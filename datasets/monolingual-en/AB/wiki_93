<doc id="22742" url="https://en.wikipedia.org/wiki?curid=22742" title="Ötzi">
Ötzi

Ötzi, also called the Iceman, is the natural mummy of a man who lived between 3400 and 3100 BCE. The mummy was found in September 1991 in the Ötztal Alps, hence the nickname "Ötzi", near Similaun mountain and Hauslabjoch on the border between Austria and Italy.

Ötzi is believed to have been murdered; an arrowhead has been found in his left shoulder, which would have caused a fatal wound. The nature of his life and the circumstances of his death are the subject of much investigation and speculation.

He is Europe's oldest known natural human mummy and has offered an unprecedented view of Chalcolithic (Copper Age) Europeans. His body and belongings are displayed in the South Tyrol Museum of Archaeology in Bolzano, South Tyrol, Italy.

Ötzi was found on 19 September 1991 by two German tourists, at an elevation of on the east ridge of the Fineilspitze in the Ötztal Alps on the Austrian–Italian border. The tourists, Helmut and Erika Simon, were walking off the path between the mountain passes Hauslabjoch and Tisenjoch. They believed that the body was of a recently deceased mountaineer. The next day, a mountain gendarme and the keeper of the nearby Similaunhütte first attempted to remove the body, which was frozen in ice below the torso, using a pneumatic drill and ice-axes, but had to give up due to bad weather. The next day, eight groups visited the site, among whom were mountaineers Hans Kammerlander and Reinhold Messner. The body was semi-officially extracted on 22 September and officially salvaged the following day. It was transported to the office of the medical examiner in Innsbruck, together with other objects found. On 24 September, the find was examined there by archaeologist Konrad Spindler of the University of Innsbruck. He dated the find to be "about four thousand years old", based on the typology of an axe among the retrieved objects.

At the Treaty of Saint-Germain-en-Laye of 1919, the border between North and South Tyrol was defined as the watershed of the rivers Inn and Etsch. Near Tisenjoch the glacier (which has since retreated) complicated establishing the watershed and the border was drawn too far north. Although Ötzi's find site drains to the Austrian side, surveys in October 1991 showed that the body had been located inside Italian territory as delineated in 1919. The province of South Tyrol claimed property rights but agreed to let Innsbruck University finish its scientific examinations. Since 1998, it has been on display at the South Tyrol Museum of Archaeology in Bolzano, the capital of South Tyrol.

The corpse has been extensively examined, measured, X-rayed, and dated. Tissues and intestinal contents have been examined microscopically, as have the items found with the body. In August 2004, frozen bodies of three Austro-Hungarian soldiers killed during the Battle of San Matteo (1918) were found on the mountain Punta San Matteo in Trentino. One body was sent to a museum in the hope that research on how the environment affected its preservation would help unravel Ötzi's past.

By current estimates (2016), at the time of his death, Ötzi was tall, weighed about , and was about 45 years of age. When his body was found, it weighed . Because the body was covered in ice shortly after his death, it had only partially deteriorated. Initial reports claimed that his penis and most of his scrotum were missing, but this was later shown to be unfounded. Analysis of pollen, dust grains and the isotopic composition of his tooth enamel indicates that he spent his childhood near the present village of Feldthurns, north of Bolzano, but later went to live in valleys about 50 kilometres farther north.
In 2009, a CAT scan revealed that the stomach had shifted upward to where his lower lung area would normally be. Analysis of the contents revealed the partly digested remains of ibex meat, confirmed by DNA analysis, suggesting he had a meal less than two hours before his death. Wheat grains were also found. It is believed that Ötzi most likely had a few slices of a dried, fatty meat, probably bacon, which came from a wild goat in South Tyrol, Italy. Analysis of Ötzi's intestinal contents showed two meals (the last one consumed about eight hours before his death), one of chamois meat, the other of red deer and herb bread; both were eaten with roots and fruits. The grain also eaten with both meals was a highly processed einkorn wheat bran, quite possibly eaten in the form of bread. In the proximity of the body, and thus possibly originating from the Iceman's provisions, chaff and grains of einkorn and barley, and seeds of flax and poppy were discovered, as well as kernels of sloes (small plum-like fruits of the blackthorn tree) and various seeds of berries growing in the wild.

Hair analysis was used to examine his diet from several months before. Pollen in the first meal showed that it had been consumed in a mid-altitude conifer forest, and other pollens indicated the presence of wheat and legumes, which may have been domesticated crops. Pollen grains of hop-hornbeam were also discovered. The pollen was very well preserved, with the cells inside remaining intact, indicating that it had been fresh (estimated about two hours old) at the time of Ötzi's death, which places the event in the spring or early summer. Einkorn wheat is harvested in the late summer, and sloes in the autumn; these must have been stored from the previous year.

High levels of both copper particles and arsenic were found in Ötzi's hair. This, along with Ötzi's copper axe blade, which is 99.7% pure copper, has led scientists to speculate that Ötzi was involved in copper smelting.

By examining the proportions of Ötzi's tibia, femur and pelvis, Christopher Ruff has determined that Ötzi's lifestyle included long walks over hilly terrain. This degree of mobility is not characteristic of other Copper Age Europeans. Ruff proposes that this may indicate that Ötzi was a high-altitude shepherd.

Using modern 3D scanning technology, a facial reconstruction has been created for the South Tyrol Museum of Archaeology in Bolzano, Italy. It shows Ötzi looking old for his 45 years, with deep-set brown eyes, a beard, a furrowed face, and sunken cheeks. He is depicted looking tired and ungroomed.

Ötzi apparently had whipworm ("Trichuris trichiura"), an intestinal parasite. During CT scans, it was observed that three or four of his right ribs had been cracked when he had been lying face down after death, or where the ice had crushed his body. One of his fingernails (of the two found) shows three Beau's lines indicating he was sick three times in the six months before he died. The last incident, two months before he died, lasted about two weeks. It was also found that his epidermis, the outer skin layer, was missing, a natural process from his mummification in ice. Ötzi's teeth showed considerable internal deterioration from cavities. These oral pathologies may have been brought about by his grain-heavy, high carbohydrate diet. DNA analysis in February 2012 revealed that Ötzi was lactose intolerant, supporting the theory that lactose intolerance was still common at that time, despite the increasing spread of agriculture and dairying.

Ötzi had a total of 61 tattoos, consisting of 19 groups of black lines ranging from 1 to 3 mm in thickness and 7 to 40 mm long. These include groups of parallel lines running along the longitudinal axis of his body and to both sides of the lumbar spine, as well as a cruciform mark behind the right knee and on the right ankle, and parallel lines around the left wrist. The greatest concentration of markings is found on his legs, which together exhibit 12 groups of lines. A microscopic examination of samples collected from these tattoos revealed that they were created from pigment manufactured out of fireplace ash or soot.

Radiological examination of Ötzi's bones showed "age-conditioned or strain-induced degeneration" corresponding to many tattooed areas, including osteochondrosis and slight spondylosis in the lumbar spine and wear-and-tear degeneration in the knee and especially in the ankle joints. It has been speculated that these tattoos may have been related to pain relief treatments similar to acupressure or acupuncture. If so, this is at least 2,000 years before their previously known earliest use in China (c. 1000 BCE). At one point, research into archaeological evidence for ancient tattooing confirmed that Ötzi was the oldest tattooed human mummy yet discovered. In 2018, however, nearly contemporaneous tattooed mummies were discovered in Egypt.

Ötzi wore a cloak made of woven grass and a coat, a belt, a pair of leggings, a loincloth and shoes, all made of leather of different skins. He also wore a bearskin cap with a leather chin strap. The shoes were waterproof and wide, seemingly designed for walking across the snow; they were constructed using bearskin for the soles, deer hide for the top panels, and a netting made of tree bark. Soft grass went around the foot and in the shoe and functioned like modern socks. The coat, belt, leggings and loincloth were constructed of vertical strips of leather sewn together with sinew. His belt had a pouch sewn to it that contained a cache of useful items: a scraper, drill, flint flake, bone awl and a dried fungus.

The shoes have since been reproduced by a Czech academic, who said that "because the shoes are actually quite complex, I'm convinced that even 5,300 years ago, people had the equivalent of a cobbler who made shoes for other people". The reproductions were found to constitute such excellent footwear that it was reported that a Czech company offered to purchase the rights to sell them. However, a more recent hypothesis by British archaeologist Jacqui Wood says that Ötzi's shoes were actually the upper part of snowshoes. According to this theory, the item currently interpreted as part of a backpack is actually the wood frame and netting of one snowshoe and animal hide to cover the face.

The leather loincloth and hide coat were made from sheepskin. Genetic analysis showed that the sheep species was nearer to modern domestic European sheep than to wild sheep; the items were made from the skins of at least four animals. Part of the coat was made from domesticated goat belonging to a mitochondrial haplogroup (a common female ancestor) that inhabits central Europe today. The coat was made from several animals from two different species and was stitched together with hides available at the time. The leggings were made from domesticated goat leather. A similar set of 6,500-year-old leggings discovered in Switzerland were made from goat leather which may indicate the goat leather was specifically chosen.

Shoelaces were made from the European genetic population of cattle. The quiver was made from wild roe deer, the fur hat was made from a genetic lineage of brown bear which lives in the region today. Writing in the journal "Scientific Reports", researchers from Ireland and Italy reported their analysis of his clothing's mitochondrial DNA, which was extracted from nine fragments from six of his garments, including his loin cloth and fur cap.

Other items found with the Iceman were a copper axe with a yew handle, a chert-bladed knife with an ash handle and a quiver of 14 arrows with viburnum and dogwood shafts. Two of the arrows, which were broken, were tipped with flint and had fletching (stabilizing fins), while the other 12 were unfinished and untipped. The arrows were found in a quiver with what is presumed to be a bow string, an unidentified tool, and an antler tool which might have been used for sharpening arrow points. There was also an unfinished yew longbow that was long.

In addition, among Ötzi's possessions were berries, two birch bark baskets, and two species of polypore mushrooms with leather strings through them. One of these, the birch fungus, is known to have anthelmintic properties, and was probably used for medicinal purposes. The other was a type of tinder fungus, included with part of what appeared to be a complex firelighting kit. The kit featured pieces of over a dozen different plants, in addition to flint and pyrite for creating sparks.

Ötzi's copper axe was of particular interest. His axe's haft is long and made from carefully worked yew with a right-angled crook at the shoulder, leading to the blade. The long axe head is made of almost pure copper, produced by a combination of casting, cold forging, polishing, and sharpening. Despite the fact that copper ore sources in the Alpines are known to have been exploited at the time, a study indicated that the copper in the axe came from southern Tuscany. It was let into the forked end of the crook and fixed there using birch-tar and tight leather lashing. The blade part of the head extends out of the lashing and shows clear signs of having been used to chop and cut. At the time, such an axe would have been a valuable possession, important both as a tool and as a status symbol for the bearer.

Ötzi's full genome has been sequenced; the report on this was published on 28 February 2012. The Y chromosome DNA of Ötzi belongs to a subclade of G defined by the SNPs M201, P287, P15, L223 and L91 (G-L91, ISOGG G2a2b, former "G2a4"). He was not typed for any of the subclades downstreaming from G-L91; however, an analysis of his BAM file revealed that he belongs to the L166 and FGC5672 subclades below L91. G-L91 is now mostly found in South Corsica.

Analysis of his mitochondrial DNA showed that Ötzi belongs to the K1 subclade, but cannot be categorized into any of the three modern branches of that subclade (K1a, K1b or K1c). The new subclade has provisionally been named "K1ö" for "Ötzi". A multiplex assay study was able to confirm that the Iceman's mtDNA belongs to a previously unknown European mtDNA clade with a very limited distribution among modern data sets.

By autosomal DNA, Ötzi is most closely related to Southern Europeans, especially to geographically isolated populations like Corsicans and Sardinians.

DNA analysis also showed him at high risk of atherosclerosis and lactose intolerance, with the presence of the DNA sequence of "Borrelia burgdorferi", possibly making him the earliest known human with Lyme disease. A later analysis suggested the sequence may have been a different "Borrelia" species.

In October 2013, it was reported that 19 modern Tyrolean men were descendants of Ötzi or of a close relative of Ötzi. Scientists from the Institute of Legal Medicine at Innsbruck Medical University had analysed the DNA of over 3,700 Tyrolean male blood donors and found 19 who shared a particular genetic mutation with the 5,300-year-old man.

In May 2012, scientists announced the discovery that Ötzi still had intact blood cells. These are the oldest complete human blood cells ever identified. In most bodies this old, the blood cells are either shrunken or mere remnants, but Ötzi's have the same dimensions as living red blood cells and resembled a modern-day sample.

In 2016, researchers reported on a study from the extraction of twelve samples from the gastrointestinal tract of Ötzi to analyze the origins of the "Helicobacter pylori" in his gut. The "H. pylori" strain found in his gastrointestinal tract was, surprisingly, the hpAsia2 strain, a strain today found primarily in South Asian and Central Asian populations, with extremely rare occurrences in modern European populations. The strain found in Ötzi's gut is most similar to three modern individuals from Northern India; the strain itself is, of course, older than the modern Northern Indian strain.

The cause of death remained uncertain until 10 years after the discovery of the body. It was initially believed that Ötzi died from exposure during a winter storm. Later it was speculated that Ötzi might have been a victim of a ritual sacrifice, perhaps for being a chieftain. This explanation was inspired by theories previously advanced for the first millennium BCE bodies recovered from peat bogs such as the Tollund Man and the Lindow Man.

In 2001, X-rays and a CT scan revealed that Ötzi had an arrowhead lodged in his left shoulder when he died and a matching small tear on his coat. The discovery of the arrowhead prompted researchers to theorize Ötzi died of blood loss from the wound, which would probably have been fatal even if modern medical techniques had been available. Further research found that the arrow's shaft had been removed before death, and close examination of the body found bruises and cuts to the hands, wrists and chest and cerebral trauma indicative of a blow to the head. One of the cuts was to the base of his thumb that reached down to the bone but had no time to heal before his death. Currently, it is believed that Ötzi bled to death after the arrow shattered the scapula and damaged nerves and blood vessels before lodging near the lung.

Recent DNA analyses claim they revealed traces of blood from at least four other people on his gear: one from his knife, two from a single arrowhead, and a fourth from his coat. Interpretations of these findings were that Ötzi killed two people with the same arrow and was able to retrieve it on both occasions, and the blood on his coat was from a wounded comrade he may have carried over his back. Ötzi's posture in death (frozen body, face down, left arm bent across the chest) could support a theory that before death occurred and rigor mortis set in, the Iceman was turned onto his stomach in the effort to remove the arrow shaft.

In 2010, it was proposed that Ötzi died at a much lower altitude and was buried higher in the mountains, as posited by archaeologist Alessandro Vanzetti of the Sapienza University of Rome and his colleagues. According to their study of the items found near Ötzi and their locations, it is possible that the iceman may have been placed above what has been interpreted as a stone burial mound but was subsequently moved with each thaw cycle that created a flowing watery mix driven by gravity before being re-frozen. While archaeobotanist Klaus Oeggl of the University of Innsbruck agrees that the natural process described probably caused the body to move from the ridge that includes the stone formation, he pointed out that the paper provided no compelling evidence to demonstrate that the scattered stones constituted a burial platform. Moreover, biological anthropologist Albert Zink argues that the iceman's bones display no dislocations that would have resulted from a downhill slide and that the intact blood clots in his arrow wound would show damage if the body had been moved up the mountain. In either case, the burial theory does not contradict the possibility of a violent cause of death.

Italian law entitled the Simons to a finders' fee from the South Tyrolean provincial government of 25% of the value of Ötzi. In 1994 the authorities offered a "symbolic" reward of 10 million lire (€5,200), which the Simons declined. In 2003, the Simons filed a lawsuit which asked a court in Bolzano to recognize their role in Ötzi's discovery and declare them his "official discoverers". The court decided in the Simons' favor in November 2003, and at the end of December that year the Simons announced that they were seeking US$300,000 as their fee. The provincial government decided to appeal.

In addition, two people came forward to claim that they were part of the same mountaineering party that came across Ötzi and discovered the body first:

In 2005 the rival claims were heard by a Bolzano court. The legal case angered Mrs. Simon, who alleged that neither woman was present on the mountain that day. In 2005, Mrs. Simon's lawyer said: "Mrs. Simon is very upset by all this and by the fact that these two new claimants have decided to appear 14 years after Ötzi was found." In 2008, however, Jarc stated for a Slovene newspaper that she wrote twice to the Bolzano court in regard to her claim but received no reply whatsoever.

In 2004, Helmut Simon died. Two years later, in June 2006, an appeals court affirmed that the Simons had indeed discovered the Iceman and were therefore entitled to a finder's fee. It also ruled that the provincial government had to pay the Simons' legal costs. After this ruling, Mrs. Erika Simon reduced her claim to €150,000. The provincial government's response was that the expenses it had incurred to establish a museum and the costs of preserving the Iceman should be considered in determining the finder's fee. It insisted it would pay no more than €50,000. In September 2006, the authorities appealed the case to Italy's highest court, the Court of Cassation.

On 29 September 2008 it was announced that the provincial government and Mrs. Simon had reached a settlement of the dispute, under which she would receive €150,000 in recognition of Ötzi's discovery by her and her late husband and the tourist income that it attracts.

Influenced by the "Curse of the pharaohs" and the media theme of cursed mummies, claims have been made that Ötzi is cursed. The allegation revolves around the deaths of several people connected to the discovery, recovery and subsequent examination of Ötzi. It is alleged that they have died under mysterious circumstances. These people include co-discoverer Helmut Simon and Konrad Spindler, the first examiner of the mummy in Austria in 1991. To date, the deaths of seven people, of which four were accidental, have been attributed to the alleged curse. In reality hundreds of people were involved in the recovery of Ötzi and are still involved in studying the body and the artifacts found with it. The fact that a small percentage of them have died over the years has not been shown to be statistically significant.






</doc>
<doc id="22743" url="https://en.wikipedia.org/wiki?curid=22743" title="Operation Deadlight">
Operation Deadlight

Operation Deadlight was the code name for the Royal Navy operation to scuttle German U-boats surrendered to the Allies after the defeat of Germany near the end of World War II.

Of the 156 U-boats that surrendered to the allies at the end of the war, 116 were scuttled as part of Operation Deadlight. The Royal Navy carried out the operation, and planned to tow the submarines to three areas about north-west of Ireland and sink them. The areas were codenamed XX, YY, and ZZ. They intended to use XX as the main scuttling area, while towing 36 boats to ZZ to use as practice targets for aerial attack. YY was to be a reserve position where, if the weather was good enough, they could divert submarines from XX to sink with naval forces. The plan was to sink those submarines not used for target practice with explosive charges, with naval gunfire as a fall-back option if that failed.

When Operation Deadlight began, the navy found that many of the U-boats were in poor condition from being moored in exposed harbours while awaiting disposal. These issues, combined with poor weather, sank 56 of the boats before they reached the scuttling areas, and those that did reach the area were generally sunk by gunfire rather than explosive charges. The first sinking took place on 17 November 1945 and the last on 11 February 1946.

Several U-boats escaped Operation Deadlight. Some were claimed as prizes by Britain, France, Norway, and the Soviet Union. Four were in East Asia when Germany surrendered and were commandeered by Japan ( was renamed "I-501", – "I-506", – "I-505", – "I-502", and a fifth boat, , had been sold to Japan in 1943 and renamed "RO-500"). Two U-boats that survived Operation Deadlight are today museum ships. was earmarked for scuttling, but American Rear Admiral Daniel V. Gallery argued successfully that she did not fall under Operation Deadlight. United States Navy Task Group 22.3, under then-Captain Gallery, had captured "U-505" in battle on 4 June 1944. Having been captured, not surrendered at the end of the war, she survived to become a war memorial at the Museum of Science and Industry in Chicago. was transferred to Norway by Britain in October 1948 and became the Norwegian "Kaura". She was returned to Germany in 1965, to become a museum ship at Laboe in October 1971.

In the late-1990s, a firm applied to the British Ministry of Defence for salvage rights to the Operation Deadlight U-boats, planning to raise up to a hundred of them. Because the U-boats were constructed in the pre-atomic age, the wrecks contain metals that are not radioactively tainted, and are therefore valuable for certain research purposes. The ministry awarded no salvage rights, due to objections from Russia and the U.S., and potentially from Great Britain. 

Between 2001 and 2003, nautical archaeologist Innes McCartney discovered and surveyed fourteen of the U-boat wrecks; including the rare Type XXI U-boat "U-2506", once under the command of Horst von Schroeter; the successful Type IXC U-boat, commanded by Adolf Piening and the "U-778", which was the most promising salvage.

In 2007, Derry City Council announced plans to raise the "U-778" to be the main exhibit of a new maritime museum. On 3 October 2007, an Irish diver, Michael Hanrahan, died whilst filming the wreck as part of the salvage project. In November 2009, a spokesman from the council's heritage museum service announced the salvage project had been cancelled for cost reasons.




</doc>
<doc id="22746" url="https://en.wikipedia.org/wiki?curid=22746" title="Order of the Eastern Star">
Order of the Eastern Star

The Order of the Eastern Star is a Masonic appendant body open to both men and women. It was established in 1850 by lawyer and educator Rob Morris, a noted Freemason, but was only adopted and approved as an appendant body of the Masonic Fraternity in 1873. The order is based on some teachings from the Bible, but is open to people of all religious beliefs. It has approximately 10,000 chapters in twenty countries and approximately 500,000 members under its General Grand Chapter.

Members of the Order of the Eastern Star are aged 18 and older; men must be Master Masons and women must have specific relationships with Masons. Originally, a woman would have to be the daughter, widow, wife, sister, or mother of a Master Mason, but the Order now allows other relatives as well as allowing Job's Daughters, Rainbow Girls, Members of the Organization of Triangles (NY only) and members of the Constellation of Junior Stars (NY only) to become members when of age.

The Order was created by Rob Morris in 1850 when he was teaching at the Eureka Masonic College in Richland, Mississippi. While confined by illness, he set down the principles of the order in his "Rosary of the Eastern Star". By 1855, he had organized a "Supreme Constellation" in New York, which chartered chapters throughout the United States.

In 1866, Dr. Morris started working with Robert Macoy, and handed the Order over to him while Morris was traveling in the Holy Land. Macoy organized the current system of Chapters, and modified Dr. Morris' "Rosary" into a "Ritual".

On December 1, 1874, Queen Esther Chapter No. 1 became the first Prince Hall Affiliate chapter of the Order of the Eastern Star when it was established in Washington, D.C. by Thornton Andrew Jackson.

The "General Grand Chapter" was formed in Indianapolis, Indiana on November 6, 1876. Committees formed at that time created the "Ritual of the Order of the Eastern Star" in more or less its current form.

The emblem of the Order is a five-pointed star with the white ray of the star pointing downwards towards the manger. The meaning of the letters FATAL surrounding the center pentagon in the emblem is only revealed to members of the Order. In the Chapter room, the downward-pointing white ray points to the West. The character-building lessons taught in the Order are stories inspired by Biblical figures:

There are 18 main officers in a full chapter:

Traditionally, a woman who is elected Associate Conductress will be elected to Conductress the following year, then the next year Associate Matron, and the next year Worthy Matron. A man elected Associate Patron will usually be elected Worthy Patron the following year. Usually the woman who is elected to become Associate Matron will let it be known who she wishes to be her Associate Patron, so the next year they will both go to the East together as Worthy Matron and Worthy Patron. There is no male counterpart to the Conductress and Associate Conductress. Only women are allowed to be Matrons, Conductresses, and the Star Points (Adah, Ruth, etc.) and only men can be Patrons.

Once a member has served a term as Worthy Matron or Worthy Patron, they may use the post-nominal letters, PM or PP respectively.

The General Grand Chapter headquarters, the International Temple, is located in the Dupont Circle neighborhood of Washington, D.C., in the Perry Belmont Mansion. The mansion was built in 1909 for the purpose of entertaining the guests of Perry Belmont. They included Britain's Prince of Wales in 1919. General Grand Chapter purchased the building in 1935. The secretary of General Grand Chapter lives there while serving his or her term of office. The mansion features works of art from around the world, most of which were given as gifts from various international Eastern Star chapters.

The Order has a charitable foundation and from 1986-2001 contributed $513,147 to Alzheimer's disease research, juvenile diabetes research, and juvenile asthma research. It also provides bursaries to students of theology and religious music, as well as other scholarships that differ by jurisdiction. In 2000 over $83,000 was donated. Many jurisdictions support a Masonic and/or Eastern Star retirement center or nursing home for older members; some homes are also open to the public. The Elizabeth Bentley OES Scholarship Fund was started in 1947.




</doc>
<doc id="22747" url="https://en.wikipedia.org/wiki?curid=22747" title="OSI model">
OSI model

The Open Systems Interconnection model (OSI model) is a conceptual model that characterises and standardises the communication functions of a telecommunication or computing system without regard to its underlying internal structure and technology. Its goal is the interoperability of diverse communication systems with standard communication protocols. The model partitions a communication system into abstraction layers.

A layer serves the layer above it and is served by the layer below it. For example, a layer that provides error-free communications across a network provides the path needed by applications above it, while it calls the next lower layer to send and receive packets that constitute the contents of that path.

The model is a product of the Open Systems Interconnection project at the International Organization for Standardization (ISO).

In the early- and mid-1970s, networking was largely either government-sponsored (NPL network in the UK, ARPANET in the US, CYCLADES in France) or vendor-developed with proprietary standards, such as IBM's Systems Network Architecture and Digital Equipment Corporation's DECnet. Public data networks were only just beginning to emerge, and these began to use the X.25 standard in the late 1970s. 

The Experimental Packet Switched System in the UK circa 1973-5 identified the need for defining higher level protocols. The UK National Computing Centre publication 'Why Distributed Computing' which came from considerable research into future configurations for computer systems, resulted in the UK presenting the case for an international standards committee to cover this area at the ISO meeting in Sydney in March 1977.

Beginning in 1977, the International Organization for Standardization (ISO) conducted a program to develop general standards and methods of networking. A similar process evolved at the International Telegraph and Telephone Consultative Committee (CCITT, from French: Comité Consultatif International Téléphonique et Télégraphique). Both bodies developed documents that defined similar networking models. The OSI model was first defined in raw form in Washington, DC in February 1978 by Hubert Zimmermann of France and the refined but still draft standard was published by the ISO in 1980. 

The drafters of the reference model had to contend with many competing priorities and interests. The rate of technological change made it necessary to define standards that new systems could converge to rather than standardizing procedures after the fact; the reverse of the traditional approach to developing standards. Although not a standard itself, it was a framework in which future standards could be defined.

In 1983, the CCITT and ISO documents were merged to form "The Basic Reference Model for Open Systems Interconnection," usually referred to as the "Open Systems Interconnection Reference Model", "OSI Reference Model", or simply "OSI model". It was published in 1984 by both the ISO, as standard ISO 7498, and the renamed CCITT (now called the Telecommunications Standardization Sector of the International Telecommunication Union or ITU-T) as standard X.200.

OSI had two major components, an abstract model of networking, called the Basic Reference Model or seven-layer model, and a set of specific protocols. The OSI reference model was a major advance in the teaching of network concepts. It promoted the idea of a consistent model of protocol layers, defining interoperability between network devices and software.

The concept of a seven-layer model was provided by the work of Charles Bachman at Honeywell Information Systems. Various aspects of OSI design evolved from experiences with the NPL network, ARPANET, CYCLADES, EIN, and the International Networking Working Group (IFIP WG6.1). In this model, a networking system was divided into layers. Within each layer, one or more entities implement its functionality. Each entity interacted directly only with the layer immediately beneath it and provided facilities for use by the layer above it.

The OSI standards documents are available from the ITU-T as the X.200-series of recommendations. Some of the protocol specifications were also available as part of the ITU-T X series. The equivalent ISO and ISO/IEC standards for the OSI model were available from ISO. Not all are free of charge.

OSI was an industry effort, attempting to get industry participants to agree on common network standards to provide multi-vendor interoperability. It was common for large networks to support multiple network protocol suites, with many devices unable to interoperate with other devices because of a lack of common protocols. For a period in the late 1980s and early 1990s, engineers, organizations and nations became polarized over the issue of which standard, the OSI model or the Internet protocol suite, would result in the best and most robust computer networks. However, while OSI developed its networking standards in the late 1980s, TCP/IP came into widespread use on multi-vendor networks for internetworking.

The OSI model is still used as a reference for teaching and documentation; however, the OSI protocols originally conceived for the model did not gain popularity. Some engineers argue the OSI reference model is still relevant to cloud computing. Others say the original OSI model doesn't fit today's networking protocols and have suggested instead a simplified approach.

Communication protocols enable an entity in one host to interact with a corresponding entity at the same layer in another host. Service definitions, like the OSI Model, abstractly describe the functionality provided to an (N)-layer by an (N-1) layer, where N is one of the seven layers of protocols operating in the local host.

At each level "N", two entities at the communicating devices (layer N "peers") exchange protocol data units (PDUs) by means of a layer N "protocol". Each PDU contains a payload, called the service data unit (SDU), along with protocol-related headers or footers.

Data processing by two communicating OSI-compatible devices proceeds as follows:

The OSI model was defined in ISO/IEC 7498 which consists of the following parts:

ISO/IEC 7498-1 is also published as ITU-T Recommendation X.200.

The recommendation X.200 describes seven layers, labelled 1 to 7. Layer 1 is the lowest layer in this model.

The physical layer is responsible for the transmission and reception of unstructured raw data between a device and a physical transmission medium. It converts the digital bits into electrical, radio, or optical signals. Layer specifications define characteristics such as voltage levels, the timing of voltage changes, physical data rates, maximum transmission distances, modulation scheme, channel access method and physical connectors. This includes the layout of pins, voltages, line impedance, cable specifications, signal timing and frequency for wireless devices. Bit rate control is done at the physical layer and may define transmission mode as simplex, half duplex, and full duplex. The components of a physical layer can be described in terms of a network topology. Physical layer specifications are included in the specifications for the ubiquitous Bluetooth, Ethernet, and USB standards. An example of a less well-known physical layer specification would be for the CAN standard.

The data link layer provides node-to-node data transfer—a link between two directly connected nodes. It detects and possibly corrects errors that may occur in the physical layer.
It defines the protocol to establish and terminate a connection between two physically connected devices. It also defines the protocol for flow control between them.

IEEE 802 divides the data link layer into two sublayers:

The MAC and LLC layers of IEEE 802 networks such as 802.3 Ethernet, 802.11 Wi-Fi, and 802.15.4 ZigBee operate at the data link layer.

The Point-to-Point Protocol (PPP) is a data link layer protocol that can operate over several different physical layers, such as synchronous and asynchronous serial lines.

The ITU-T G.hn standard, which provides high-speed local area networking over existing wires (power lines, phone lines and coaxial cables), includes a complete data link layer that provides both error correction and flow control by means of a selective-repeat sliding-window protocol.

Security, specifically (authenticated) encryption, at this layer can be applied with MACSec.

The network layer provides the functional and procedural means of transferring variable length data sequences (called packets) from one node to another connected in "different networks". A network is a medium to which many nodes can be connected, on which every node has an "address" and which permits nodes connected to it to transfer messages to other nodes connected to it by merely providing the content of a message and the address of the destination node and letting the network find the way to deliver the message to the destination node, possibly routing it through intermediate nodes. If the message is too large to be transmitted from one node to another on the data link layer between those nodes, the network may implement message delivery by splitting the message into several fragments at one node, sending the fragments independently, and reassembling the fragments at another node. It may, but does not need to, report delivery errors.

Message delivery at the network layer is not necessarily guaranteed to be reliable; a network layer protocol may provide reliable message delivery, but it need not do so.

A number of layer-management protocols, a function defined in the "management annex", ISO 7498/4, belong to the network layer. These include routing protocols, multicast group management, network-layer information and error, and network-layer address assignment. It is the function of the payload that makes these belong to the network layer, not the protocol that carries them.

The transport layer provides the functional and procedural means of transferring variable-length data sequences from a source to a destination host, while maintaining the quality of service functions.

The transport layer controls the reliability of a given link through flow control, segmentation/desegmentation, and error control. Some protocols are state- and connection-oriented. This means that the transport layer can keep track of the segments and retransmit those that fail delivery. The transport layer also provides the acknowledgement of the successful data transmission and sends the next data if no errors occurred. The transport layer creates segments out of the message received from the application layer. Segmentation is the process of dividing a long message into smaller messages.

OSI defines five classes of connection-mode transport protocols ranging from class 0 (which is also known as TP0 and provides the fewest features) to class 4 (TP4, designed for less reliable networks, similar to the Internet). Class 0 contains no error recovery and was designed for use on network layers that provide error-free connections. Class 4 is closest to TCP, although TCP contains functions, such as the graceful close, which OSI assigns to the session layer. Also, all OSI TP connection-mode protocol classes provide expedited data and preservation of record boundaries. Detailed characteristics of TP0-4 classes are shown in the following table:

An easy way to visualize the transport layer is to compare it with a post office, which deals with the dispatch and classification of mail and parcels sent. A post office inspects only the outer envelope of mail to determine its delivery. Higher layers may have the equivalent of double envelopes, such as cryptographic presentation services that can be read by the addressee only. Roughly speaking, tunnelling protocols operate at the transport layer, such as carrying non-IP protocols such as IBM's SNA or Novell's IPX over an IP network, or end-to-end encryption with IPsec. While Generic Routing Encapsulation (GRE) might seem to be a network-layer protocol, if the encapsulation of the payload takes place only at the endpoint, GRE becomes closer to a transport protocol that uses IP headers but contains complete Layer 2 frames or Layer 3 packets to deliver to the endpoint. L2TP carries PPP frames inside transport segments.

Although not developed under the OSI Reference Model and not strictly conforming to the OSI definition of the transport layer, the Transmission Control Protocol (TCP) and the User Datagram Protocol (UDP) of the Internet Protocol Suite are commonly categorized as layer-4 protocols within OSI.

Transport Layer Security (TLS) provide security at this layer.

The session layer controls the dialogues (connections) between computers. It establishes, manages and terminates the connections between the local and remote application. It provides for full-duplex, half-duplex, or simplex operation, and establishes procedures for checkpointing, suspending, restarting, and terminating a session. In the OSI model, this layer is responsible for gracefully closing a session, which is handled in the Transmission Control Protocol at the transport layer in the Internet Protocol Suite. This layer is also responsible for session checkpointing and recovery, which is not usually used in the Internet Protocol Suite. The session layer is commonly implemented explicitly in application environments that use remote procedure calls.

The presentation layer establishes context between application-layer entities, in which the application-layer entities may use different syntax and semantics if the presentation service provides a mapping between them. If a mapping is available, presentation protocol data units are encapsulated into session protocol data units and passed down the protocol stack.

This layer provides independence from data representation by translating between application and network formats. The presentation layer transforms data into the form that the application accepts. This layer formats data to be sent across a network. It is sometimes called the syntax layer. The presentation layer can include compression functions. The Presentation Layer negotiates the Transfer Syntax.

The original presentation structure used the Basic Encoding Rules of Abstract Syntax Notation One (ASN.1), with capabilities such as converting an EBCDIC-coded text file to an ASCII-coded file, or serialization of objects and other data structures from and to XML. ASN.1 effectively makes an application protocol invariant with respect to syntax.

The application layer is the OSI layer closest to the end user, which means both the OSI application layer and the user interact directly with the software application. This layer interacts with software applications that implement a communicating component. Such application programs fall outside the scope of the OSI model. Application-layer functions typically include identifying communication partners, determining resource availability, and synchronizing communication. When identifying communication partners, the application layer determines the identity and availability of communication partners for an application with data to transmit. The most important distinction in the application layer is the distinction between the application-entity and the application. For example, a reservation website might have two application-entities: one using HTTP to communicate with its users, and one for a remote database protocol to record reservations. Neither of these protocols have anything to do with reservations. That logic is in the application itself. The application layer has no means to determine the availability of resources in the network.

Cross-layer functions are services that are not tied to a given layer, but may affect more than one layer. Some orthogonal aspects, such as management and security, involve all of the layers (See ITU-T X.800 Recommendation). These services are aimed at improving the CIA triad—confidentiality, integrity, and availability—of the transmitted data. 
Cross-layer functions are the norm, in practice, because the availability of a communication service is determined by the interaction between network design and network management protocols. 

Specific examples of cross-layer functions include the following:

Neither the OSI Reference Model, nor any OSI protocol specifications, outline any programming interfaces, other than deliberately abstract service descriptions. Protocol specifications define a methodology for communication between peers, but the software interfaces are implementation-specific.

For example, the Network Driver Interface Specification (NDIS) and Open Data-Link Interface (ODI) are interfaces between the media (layer 2) and the network protocol (layer 3).

The design of protocols in the TCP/IP model of the Internet does not concern itself with strict hierarchical encapsulation and layering. RFC 3439 contains a section entitled "Layering considered harmful". TCP/IP does recognize four broad layers of functionality which are derived from the operating scope of their contained protocols: the scope of the software application; the host-to-host transport path; the internetworking range; and the scope of the direct links to other nodes on the local network.

Despite using a different concept for layering than the OSI model, these layers are often compared with the OSI layering scheme in the following manner:
These comparisons are based on the original seven-layer protocol model as defined in ISO 7498, rather than refinements in the internal organization of the network layer.

The OSI protocol suite that was specified as part of the OSI project was considered by many as too complicated and inefficient, and to a large extent unimplementable. Taking the "forklift upgrade" approach to networking, it specified eliminating all existing networking protocols and replacing them at all layers of the stack. This made implementation difficult and was resisted by many vendors and users with significant investments in other network technologies. In addition, the protocols included so many optional features that many vendors' implementations were not interoperable.

Although the OSI model is often still referenced, the Internet protocol suite has become the standard for networking. TCP/IP's pragmatic approach to computer networking and to independent implementations of simplified protocols made it a practical methodology. Some protocols and specifications in the OSI stack remain in use, one example being IS-IS, which was specified for OSI as ISO/IEC 10589:2002 and adapted for Internet use with TCP/IP as .





</doc>
<doc id="22751" url="https://en.wikipedia.org/wiki?curid=22751" title="Original Sin (2001 film)">
Original Sin (2001 film)

Original Sin is a 2001 erotic thriller film starring Antonio Banderas and Angelina Jolie. It is based on the novel "Waltz into Darkness" by Cornell Woolrich, and is a remake of the 1969 François Truffaut film "Mississippi Mermaid". The film was produced by actress Michelle Pfeiffer's production company, Via Rosa Productions.

"Original Sin" is set in the late 19th century Cuba during the Spanish rule, and flashes back and forth from the scene of a woman awaiting her execution by garrote while telling her story to a priest, to the actual events of that story.

Luis Vargas (Antonio Banderas) sends for American Julia Russell (Angelina Jolie) from Delaware to sail to his country Cuba to be his mail-order bride. Julia alights from the ship, looking nothing like the photos she had sent prior to her voyage. Julia explains she wants more than a man who is only interested in a pretty face and that is why she has been deceptive—substituting a plain-looking woman's photo in place of her own picture. Luis also admits to a deception; he has been misleading her into believing that he is a poor clerk in a coffee export house, instead of being the rich owner of that coffee export house. On hearing this, Julia says that they both have something in common and that is that both are not to be trusted. But they assure each other that they would make efforts in understanding and trusting each other in life.

Luis and Julia wed in the church within hours of her setting foot in Cuba. Luis falls desperately in love with his new wife, and they passionately have sex.

Meanwhile, Julia's sister Emily has been trying to contact her, worried about her after such a long trip to a strange land. She sends an emotional letter to Julia asking about her welfare. Luis forces Julia to write back, fearing that if Julia continues to ignore Emily's letters, Emily will assume something terrible has befallen her sister and she might send the authorities to check on her welfare. Holding off as long as possible, Julia finally pens a letter to her sister.

In order to assure that his wife has everything she wants, Luis adds Julia to his business and personal bank accounts, giving her free rein to spend as she pleases. A detective, Walter Downs (Thomas Jane), arrives from Wilmington and tells Luis that he has been hired by Emily to find her sister Julia and would like to see her on the coming Sunday. Luis informs Julia about this and she gets upset. Emily arrives in Cuba to meet Luis, and shows the letter Julia wrote to her. She informs Luis that she believes Julia to be an impostor and that her sister may be dead. Luis discovers that Julia has taken nearly all of his fortune and disconsolate, teams up with Walter to look for her.

Luis finds Julia and discovers she is actually working with Walter and that she and Luis are staying at the same hotel. Luis believes she loves him and lies to Walter, but when confronted, a fight breaks out and Luis shoots Walter. Julia coldly tells Luis to go and buy them tickets home, but the minute he leaves, Walter gets to his feet; he had loaded the gun with blanks. Julia appears to love Luis, but Walter has too much control over her, so she continues to work for him as she and Luis run off to live in secret, with the supposedly dead Walter in pursuit. Walter turns out to be Julia (Bonny's) old lover and partner Billy.

Luis throws away his promising future and opens himself to living a lie with Julia. One night, Luis follows Julia/Bonny and discovers Walter/Billy is alive and that the two are still working together; she is apparently going to poison her husband that very night. He returns home to wait for her, and when she arrives, he reveals that he knows about the plan, confesses his love for her once more and swallows the poisoned drink though she desperately tries to stop him. Julia flees with the dying Luis, with Walter close behind. They run into him at a train station; Walter is furious that Julia has betrayed him. As Walter holds a knife to her throat, Luis shoots and wounds him, with Julia finishing him off.

Back in the "mise en scene", Julia finishes her story and asks the priest to pray with her. The next morning the guards come to her cell to take her to her execution, only to find the priest kneeling in her clothing.

In Morocco, Julia is watching a card game. She walks around the table occupied by gamblers - including Luis - and thanks them for allowing her to watch. As Julia signals Luis about the other players' cards, he begins telling them the story of how they got there.

"Original Sin" holds a 12% approval rating on Rotten Tomatoes based on 91 reviews and a weighted average of 3.41/10. The site's consensus states: "Laughably melodramatic, "Original Sin" features bad acting, poor dialogue and even worse plotting."

Film critic Roger Ebert gave the movie a positive review and said about Jolie's performance, "Jolie continues to stalk through pictures entirely on her own terms. Her presence is like a dare-ya for a man. There's dialogue in this movie so overwrought, it's almost literally unspeakable, and she survives it by biting it off contemptuously and spitting it out."

Angelina Jolie was nominated for a Golden Raspberry Award for Worst Actress for her work in both this film and "".



</doc>
<doc id="22753" url="https://en.wikipedia.org/wiki?curid=22753" title="Oscar Hammerstein II">
Oscar Hammerstein II

Oscar Greeley Clendenning Hammerstein II (; July 12, 1895 – August 23, 1960) was an American lyricist, librettist, theatrical producer, and (usually uncredited) director in the musical theater for almost 40 years. He won eight Tony Awards and two Academy Awards for Best Original Song. Many of his songs are standard repertoire for vocalists and jazz musicians. He co-wrote 850 songs.

He is best known for his collaborations with composer Richard Rodgers, as the duo Rodgers and Hammerstein, whose musicals include "Oklahoma!", "Carousel", "South Pacific", "The King and I", and "The Sound of Music". Described by Stephen Sondheim as an "experimental playwright," Hammerstein helped bring the American musical to new maturity by popularizing musicals that focused on stories and character rather than the lighthearted entertainment that the musical had been known for beforehand.

He also collaborated with Jerome Kern (with whom he wrote "Show Boat"), Vincent Youmans, Rudolf Friml, Richard A. Whiting, and Sigmund Romberg.

Oscar Greeley Clendenning Hammerstein II was born in New York City, the son of Alice Hammerstein (née Nimmo) and theatrical manager William Hammerstein. His grandfather was the German theatre impresario Oscar Hammerstein I. His father was from a Jewish family, and his mother was the daughter of Scottish and English parents. He attended the Church of the Divine Paternity, now the Fourth Universalist Society in the City of New York.

Although Hammerstein's father managed the Victoria Theatre and was a producer of vaudeville shows, he was opposed to his son's desire to participate in the arts.

Hammerstein attended Columbia University (1912–1916) and studied at Columbia Law School until 1917. As a student, he maintained high grades and engaged in numerous extracurricular activities. These included playing first base on the baseball team, performing in the Varsity Show and becoming an active member of Pi Lambda Phi, a mostly Jewish fraternity.

After his father's death, in June 1914, when he was 19, he participated in his first play with the Varsity Show, entitled "On Your Way". Throughout the rest of his college career, Hammerstein wrote and performed in several Varsity Shows.<ref name="PBS-broadway/stars"></ref>

After quitting law school to pursue theatre, Hammerstein began his first professional collaboration, with Herbert Stothart, Otto Harbach and Frank Mandel. He began as an apprentice and went on to form a 20-year collaboration with Harbach. Out of this collaboration came his first musical, "Always You", for which he wrote the book and lyrics. It opened on Broadway in 1920. In 1921 Hammerstein joined The Lambs club.

Throughout the next forty years, Hammerstein teamed with many other composers, including Jerome Kern, with whom Hammerstein enjoyed a highly successful collaboration. In 1927, Kern and Hammerstein had their biggest hit, "Show Boat", which is often revived and is still considered one of the masterpieces of the American musical theatre. "Here we come to a completely new genre — the musical play as distinguished from musical comedy. Now ... the play was the thing, and everything else was subservient to that play. Now ... came complete integration of song, humor and production numbers into a single and inextricable artistic entity." Many years later, Hammerstein's wife Dorothy bristled when she overheard someone remark that Jerome Kern had written "Ol' Man River". "Indeed not," she retorted. "Jerome Kern wrote 'dum, dum, dum-dum'. My husband wrote 'Ol' Man River'."

Other Kern-Hammerstein musicals include "Sweet Adeline", "Music in the Air", "Three Sisters", and "Very Warm for May". Hammerstein also collaborated with Vincent Youmans ("Wildflower"), Rudolf Friml ("Rose-Marie"), and Sigmund Romberg ("The Desert Song" and "The New Moon").

Hammerstein's most successful and sustained collaboration began when he teamed up with Richard Rodgers to write a musical adaptation of the play "Green Grow the Lilacs". Rodgers' first partner, Lorenz Hart, originally planned to collaborate with Rodgers on this piece, but his alcoholism had become out of control, and he was unable to write. Hart was also not certain that the idea had much merit, and the two therefore separated. The adaptation became the first Rodgers and Hammerstein collaboration, entitled "Oklahoma!", which opened on Broadway in 1943. It furthered the revolution begun by "Show Boat", by thoroughly integrating all the aspects of musical theatre, with the songs and dances arising out of and further developing the plot and characters.

William A. Everett and Paul R. Laird wrote that this was a "show, that, like "Show Boat", became a milestone, so that later historians writing about important moments in twentieth-century theatre would begin to identify eras according to their relationship to "Oklahoma!"" After "Oklahoma!", Rodgers and Hammerstein were the most important contributors to the musical-play form – with such masterworks as "Carousel", "The King and I" and "South Pacific". The examples they set in creating vital plays, often rich with social thought, provided the necessary encouragement for other gifted writers to create musical plays of their own".

The partnership went on to produce these and other Broadway musicals such as "Allegro", "Me and Juliet", "Pipe Dream", "Flower Drum Song", and "The Sound of Music", as well as the musical film "State Fair" (and its stage adaptation of the same name), and the television musical "Cinderella", all featured in the revue "A Grand Night for Singing". Hammerstein also wrote the book and lyrics for "Carmen Jones", an adaptation of Georges Bizet's opera "Carmen", with an all-black cast that became a 1943 Broadway musical and a 1954 film, starring Dorothy Dandridge.

An active advocate for writers' rights within the theatre industry, Hammerstein was a member of the Dramatists Guild of America. In 1956, he was elected as the eleventh president of the non-profit organization. He continued his presidency at the Guild until 1960; he was succeeded by Alan Jay Lerner.

Hammerstein died of stomach cancer on August 23, 1960, at his home Highland Farm in Doylestown, Pennsylvania, aged 65, nine months after the opening of "The Sound of Music" on Broadway. The final song he wrote was "Edelweiss", which was added near the end of the second act during rehearsal. After Hammerstein's death, "The Sound of Music" was adapted as a 1965 film, which won the Academy Award for Best Picture.

The lights of Times Square were turned off for one minute, and London's West End lights were dimmed in recognition of his contribution to the musical. He was cremated, and his ashes were buried at the Ferncliff Cemetery in Hartsdale, New York. A memorial plaque was unveiled at Southwark Cathedral, England, on May 24, 1961. He was survived by his second wife, Dorothy, his three children, and two stepchildren.

Hammerstein married his first wife, Myra Finn, in 1917; the couple divorced in 1929. He married his second wife, the Australian-born Dorothy (Blanchard) Jacobson (1899-1987), in 1929. He had three children: William Hammerstein (1918–2001) and Alice Hammerstein Mathias by his first wife, and James Hammerstein (1931-1999) by his second wife, with whom he also had a stepson, Henry Jacobson, and a stepdaughter, Susan Blanchard.

Hammerstein was one of the most important "book writers" in Broadway history – he made the story, not the songs or the stars, central to the musical and brought musical theater to full maturity as an art form. According to Stephen Sondheim, "What few people understand is that Oscar's big contribution to the theater was as a theoretician, as a Peter Brook, as an innovator. People don't understand how experimental "Show Boat" and "Oklahoma!" felt at the time they were done. Oscar is not about the 'lark that is learning to pray' – that's easy to make fun of. He's about "Allegro"," Hammerstein's most experimental musical.

His reputation for being sentimental is based largely on the movie versions of the musicals, especially "The Sound of Music", in which a song sung by those in favor of reaching an accommodation with the Nazis, "No Way to Stop It", was cut. As recent revivals of "Show Boat", "Oklahoma!", "Carousel", and "The King and I" in London and New York show, Hammerstein was one of the more tough-minded and socially conscious American musical theater artists. According to Richard Kislan, "The shows of Rodgers and Hammerstein were the product of sincerity. In the light of criticism directed against them and their universe of sweetness and light, it is important to understand that they believed sincerely in what they wrote." According to Marc Bauch, "The Rodgers and Hammerstein musicals are romantic musical plays. Love is important."

According to "The Rodgers and Hammerstein Story" by Stanley Green, "For three minutes, on the night of September first, the entire Times Square area in New York City was blacked out in honor of the man who had done so much to light up that particular part of the world. From 8:57 to 9:00 p.m., every neon sign and every light bulb was turned off and all traffic was halted between 42nd Street and 53rd Street, and between 8th Ave and the Avenue of the Americas. A crowd of 5,000 people, many with heads bowed, assembled at the base of the statue of Father Duffy on Times Square where two trumpeters blew taps. It was the most complete blackout on Broadway since World War II, and the greatest tribute of its kind ever paid to one man."

According to "The Complete Lyrics of Oscar Hammerstein II", edited by Amy Asch, Hammerstein contributed the lyrics to 850 songs, including "Ol' Man River", "Can't Help Lovin' That Man" and "Make Believe" from "Show Boat"; "Indian Love Call" from "Rose-Marie"; "People Will Say We're in Love" and "Oklahoma" (which has been the official state song of Oklahoma since 1953) from "Oklahoma!"; "Some Enchanted Evening", from "South Pacific"; "Getting to Know You" and "Shall We Dance" from "The King and I"; and the title song as well as "Climb Ev'ry Mountain" from "The Sound of Music".

Several albums of Hammerstein's musicals were named to the "Songs of the Century" list as compiled by the Recording Industry Association of America (RIAA), the National Endowment for the Arts, and Scholastic Corporation:

Hammerstein won two Oscars for best original song—in 1941 for "The Last Time I Saw Paris" in the film "Lady Be Good", and in 1945 for "It Might as Well Be Spring" in "State Fair." In 1950, the team of Rodgers and Hammerstein received The Hundred Year Association of New York's Gold Medal Award "in recognition of outstanding contributions to the City of New York."

Hammerstein won eight Tony Awards, six for lyrics or book, and two as producer of the Best Musical ("South Pacific" and "The Sound of Music"). Rodgers and Hammerstein began writing together before the era of the Tonys: "Oklahoma!" opened in 1943 and "Carousel" in 1945, and the Tony Awards were not awarded until 1947. They won a special Pulitzer Prize in 1944 for "Oklahoma!" and, with Joshua Logan, the annual Pulitzer Prize for Drama in 1950 for "South Pacific". The Oscar Hammerstein II Center for Theater Studies at Columbia University was established in 1981 with a $1-million gift from his family.

His advice and work influenced Stephen Sondheim, a friend of the Hammerstein family from childhood. Sondheim has attributed his success in theater, and especially as a lyricist, directly to Hammerstein's influence and guidance.

The Oscar Hammerstein Award for Lifetime Achievement in Musical Theatre is presented annually. The York Theatre Company in New York City is the administrator of the award. Past awardees are composers such as Stephen Sondheim and performers such as Carol Channing. 

Oscar Hammerstein was a member of the American Theater Hall of Fame.




</doc>
<doc id="22756" url="https://en.wikipedia.org/wiki?curid=22756" title="Otto Jespersen">
Otto Jespersen

Jens Otto Harry Jespersen (; 16 July 1860 – 30 April 1943) was a Danish linguist who specialized in the grammar of the English language.

Otto Jespersen was born in Randers in Jutland. He was inspired by the work of Danish philologist Rasmus Rask as a boy, and with the help of Rask's grammars taught himself some Icelandic, Italian, and Spanish. He entered the University of Copenhagen in 1877 when he was 17, initially studying law but not forgetting his language studies. In 1881 he shifted his focus completely to languages, and in 1887 earned his master's degree in French, with English and Latin as his secondary languages. He supported himself during his studies through part-time work as a schoolteacher and as a shorthand reporter in the Danish parliament. In 1887–1888, he traveled to England, Germany and France, meeting linguists like Henry Sweet and Paul Passy and attending lectures at institutions like Oxford University. Following the advice of his mentor Vilhelm Thomsen, he returned to Copenhagen in August 1888 and began work on his doctoral dissertation on the English case system. He successfully defended his dissertation in 1891.

Jespersen was a professor of English at the University of Copenhagen from 1893 to 1925, and served as Rector of the university in 1920–21. His early work focused primarily on language teaching reform and on phonetics, but he is best known for his later work on syntax and on language development.

He advanced the theories of "Rank" and "Nexus" in Danish in two papers: "Sprogets logik" (1913) and "De to hovedarter af grammatiske forbindelser" (1921). Jespersen in this theory of ranks removes the parts of speech from the syntax, and differentiates between primaries, secondaries, and tertiaries; e.g. in ""well honed phrase"," "phrase" is a primary, this being defined by a secondary, "honed", which again is defined by a tertiary "well". The term "Nexus" is applied to sentences, structures similar to sentences and sentences in formation, in which two concepts are expressed in one unit; e.g., "it rained, he ran indoors". This term is qualified by a further concept called a "junction" which represents one idea, expressed by means of two or more elements, whereas a nexus combines two ideas. Junction and nexus proved valuable in bringing the concept of context to the forefront of the attention of the world of linguistics.

He was most widely recognized for some of his books. "Language: Its Nature, Development and Origin" (1922) is considered by many to be his masterpiece. "Modern English Grammar on Historical Principles" (1909–1949), concentrated on morphology and syntax, and "Growth and Structure of the English Language" (1905) is a comprehensive view of English by someone with another native language, and still in print, over 70 years after his death and more than 100 years after publication. Late in his life he published "Analytic Syntax" (1937), in which he presents his views on syntactic structure using an idiosyncratic shorthand notation. In "The Philosophy of Grammar" (1924) he challenged the accepted views of common concepts in Grammar and proposed corrections to the basic definitions of grammatical case, pronoun, object, voice etc., and developed further his notions of "Rank" and "Nexus". In the 21st century this book is still used as one of the basic texts in modern Structural linguistics. "Mankind, Nation and Individual: from a linguistic point of view" (1925) is one of the pioneering works on Sociolinguistics.

Jespersen visited the United States twice: he lectured at the Congress of Arts and Sciences in St. Louis in 1904, and in 1909–1910 he visited both the University of California and Columbia University. While in the U.S., he took occasion to study the country's educational system. His autobiography (see below) was published in English translation as recently as 1995.

After his retirement in 1925, Jespersen remained active in the international linguistic community. In addition to continuing to write, he convened and chaired the first International Meeting on Linguistic Research in Geneva in 1930, and acted as president of the Fourth International Congress of Linguists in Copenhagen in 1936.

Jespersen was an important figure in the international language movement. He was an early supporter of the Esperanto offshoot Ido and in 1928 published his own project Novial. He also worked with the International Auxiliary Language Association.

Jespersen received honorary degrees from Columbia University in New York (1910), St. Andrews University in Scotland (1925), and the Sorbonne in Paris (1927). He was one of the first six international scholars to be elected as honorary members of the Linguistic Society of America.






</doc>
<doc id="22758" url="https://en.wikipedia.org/wiki?curid=22758" title="List of object-oriented programming languages">
List of object-oriented programming languages

This is a list of notable programming languages with object-oriented programming (OOP) features, which are also listed in . Note that, in some contexts, the definition of an "object-oriented programming language" is not exactly the same as that of a "programming language with object-oriented features". For example, C++ is a multi-paradigm language including object-oriented paradigm; however, it is less object-oriented than some other languages such as Python and Ruby. Therefore, some people consider C++ an OOP language, while others do not or refer to it as a "semi-object-oriented programming language".




</doc>
<doc id="22759" url="https://en.wikipedia.org/wiki?curid=22759" title="OOP">
OOP

OOP, Oop, or oop may refer to:





</doc>
<doc id="22760" url="https://en.wikipedia.org/wiki?curid=22760" title="Occidental">
Occidental

Occidental may refer to:






</doc>
<doc id="22761" url="https://en.wikipedia.org/wiki?curid=22761" title="Interlingue">
Interlingue

Interlingue (; ISO 639 language codes "ie", "ile"), known until 1949 as Occidental (), is a planned international auxiliary language created by Edgar de Wahl, a Baltic German naval officer and teacher from Tallinn, Estonia, and published in 1922. The vocabulary is based on already existing words from various languages and a system of derivation using recognized prefixes and suffixes. The language is thereby naturalistic, at the same time as it is constructed to be regular. Occidental was quite popular in the years up to, during, and shortly after the Second World War, but declined thereafter until the advent of the Internet.

Interlingue is devised so that many of its derived word forms reflect the forms common to a number of Western European languages, primarily those in the Romance family, along with a certain amount of Germanic vocabulary. Many words are formed through application of de Wahl's rule, a set of rules for regular conversion of verb infinitives into derived words including from Latin double-stem verbs (e.g. "vider" to see and its derivative "vision"). The result is a language easy to understand at first sight for individuals acquainted with several Western European languages. This readability and simplified grammar along with the regular appearance of the magazine Cosmoglotta made Occidental popular in Europe during the years up to and shortly following World War II.

In "The Esperanto Book", American Esperantist Don Harlow says that Occidental had an intentional emphasis on European forms, and that some of its leading followers espoused a Eurocentric philosophy, which may have hindered its spread. Still, Occidental gained adherents in many nations including Asian nations.

Occidental survived World War II, undergoing a name change to "Interlingue", but faded into insignificance following the appearance in 1951 of a competing naturalistic project, Interlingua, which attracted among others the notable Occidentalist Ric Berger. The emergence of Interlingua occurred around the same time that Edgar de Wahl, who had opted to remain in Tallinn, was sent to a sanitarium by Soviet authorities and prohibited from corresponding with others outside the country. His death was confirmed in 1948. The proposal to change the name from Occidental to Interlingue was twofold: to attempt to demonstrate to the Soviet Union the neutrality of the language, and for a possible union or closer collaboration with Interlingua.

The activities of Occidental and its users can be seen through the magazine Cosmoglotta, which began publication in 1922 in Tallinn, Estonia under the name Kosmoglott. The language that de Wahl announced that year was a product of years of personal experimentation under the name Auli (auxiliary language), which he used during the period from 1906 to 1921 and which later on gained the nickname proto-Occidental. During the development of the language de Wahl explained his approach in a letter to an acquaintance the Baron d'Orczy written in Auli: ""My direction in the creation of a universal language seems quite regressive to you...I understand that quite well, because I am starting it right from the other end. I do not begin with the alphabet and the grammar and then have to adopt the vocabulary to it, but just the other way around: I take all international material of words, suffixes, endings, grammatical forms etc., and then I work to organize that material, put it in order, compile, interpolate, extrapolate and sift through it."" During the development of Occidental through Auli, de Wahl corresponded frequently with the Italian mathematician and creator of Latino sine flexione Giuseppe Peano and gained an appreciation for the international vocabulary in that language, writing that ""I believe the "Vocabulario commune" book by Professor Peano to already be a more valuable and scientific work than the entire scholastic literature of Ido on imaginary things evoked by the "fundamento" of Zamenhof.""

Occidental was announced in 1922 at a stage of near but not total completion. De Wahl had not intended to announce the language for another few years but did so then through the publication of Kosmoglott after hearing that the League of Nations had begun an inquiry into the question of an international language and after receiving a favorable reply the year before from Under-Secretary General Nitobe Inazō of the League of Nations which had adopted a resolution on the subject on 15 September 1921. However, the first known publication in Occidental, a booklet entitled "Transcendent algebra" by Jacob Linzbach appeared already in 1921.

Occidental began gathering followers despite a complete lack of grammars and dictionaries due to its readability. Two years later in 1924, de Wahl wrote that he was in correspondence with some 30 people "in good Occidental" despite the lack of learning material. Two Ido societies joined Occidental in the same year, one in Vienna, Austria called "IdoSocieto Progreso" (renamed as "Societé Cosmoglott Progress") and the "Societo Progreso" in Brno, Czechoslovakia, which changed its name to "Federali" ("Federation del amicos del lingue international"). The first dictionary was published the next year in 1925, the "radicarium directiv" which was a collection of Occidental root words and their equivalents in 8 languages.

For a number of years Kosmoglott was a forum for various other planned languages, while still mainly written in Occidental. Until 1924 the magazine was also affiliated with the "Academia pro Interlingua", which promoted Peano's Latino sine flexione. In 1927 the name was changed to Cosmoglotta as it began to officially promote Occidental in lieu of other languages, and in January of the same year the magazine's editorial and administrative office was moved to Vienna, Austria in the region of Mauer, now part of Liesing. Much of the early success for Occidental in this period came from the office's new central location, along with the efforts of Engelbert Pigal, also from Austria, whose article "Li Ovre de Edgar de Wahl" (The Work of Edgar de Wahl) led to interest in Occidental from users of the Ido language.

Besides the new location in a city much closer to the centre of Europe, the Vienna period was also marked by financial stability for the first time due to the support from a number of backers, particularly Hans Hörbiger, also from Vienna, and G.A. Moore from London, from which "Cosmoglotta was able to live without difficulty and gained a circle of readers despite the economic crisis". This did not last long as Hörbiger and Moore both died in 1931, and Cosmoglotta was again forced to rely on revenue from subscriptions, publications and the like.

The growing Occidental movement began a more assertive campaign for the language in the early 1930s, leveraging its at-sight readability by contacting organizations such as embassies, printing houses and the League of Nations using letters in Occidental that were often understood and responded to (in the organization's own working language), usually including the phrasing below: "Scrit in lingue international "Occidental"" ("Written in the international language Occidental")"." A large number of numbered "documents" were produced at this time as well to introduce the concept of an international language and advocate Occidental as the answer to Europe's "tower of Babel". Recordings of spoken Occidental on phonograph (gramophone) records for distribution also began to be made in this period.

The years from 1935 to 1939 were particularly active for Cosmoglotta, during which a second edition was produced along with the original Cosmoglotta. Originally entitled "Cosmoglotta-Informationes", it soon began using the name Cosmoglotta B and focused on items of more internal interest such as linguistic issues, reports of Occidental in the news, and financial updates. In early 1936, not counting the 110 issues of Cosmoglotta and any other journals and bulletins, a total of 80 publications existed in and about Occidental. 

At the same time, the years leading up to the Second World War led to difficulties for Occidental and other planned languages which were made illegal in Germany along with Austria and Czechoslovakia, forced to disband, kept under Gestapo surveillance, and had their didactic materials destroyed. The interdiction of auxiliary languages in Germany was particularly damaging as this was where most Occidentalists lived at the time. The inability to accept payment for subscriptions was a financial blow as well, a difficulty that continued after the war when Germany was divided into zones of influence and payments were still not permitted. No communication took place between Edgar de Wahl in Tallinn and the Occidental Union in Switzerland from 1939 to October 1947, first due to the war itself and thereafter from intercepted mail between Switzerland and the Soviet Union, which bewildered de Wahl who had sent multiple letters and even a large collection of translated poetry into Occidental which were never delivered. Meanwhile, de Wahl's house and his entire library had been destroyed during the bombardment of Tallinn and de Wahl himself was incarcerated for a time after refusing to leave Estonia for Germany.

In 1940 no issues of either Cosmoglotta were produced, but in 1941 Cosmoglotta B began publication once again and continued until 1950. An edition of either Cosmoglotta A or B was published every month between January 1937 and September 1939, and then (after the initial shock of the war) every month from September 1941 to June 1951. During this period only those in neutral Switzerland and Sweden were able to fully devote themselves to the language, carrying on activities in a semi-official form.

One of these activities was language standardization. De Wahl had created Occidental with a number of unchangeable features, but believed that its following of the "laws of life" gave it a firm enough base that it could follow a "natural evolution" with a flexibility which would "allow time and practice to take care of modifications that would prove to be necessary". As a result, some words had more than one permissible form and could not be resolved by decree alone, thus leaving the ultimate decision to the community by including both possible forms in the first Occidental dictionaries. One example concerned the verb "scrir" (to write) and a possible other form "scripter." De Wahl expressed a preference for "scrir", finding "scripter" to be somewhat heavy, but commented that "scripter" was certainly permissible and that Occidental might take on a similar evolution to natural languages where both forms come into common use, with the longer and more Latin-like form having a heavier and more formal character and the shorter a lighter and more everyday tone (such as English "story" vs. "history").

The decision over etymologic vs. recognizable spellings in words such as attractive (adtractiv, attractiv or atractiv) and oppression (obpression vs. oppression vs. opression) is one example where community usage quickly led to a rejection of the first option and eventually settled on the third. Much of Occidental's vocabulary was solved in this way (e.g. both ac and anc were proposed for the word "also" but ac was hardly if ever used), but not all. With questions still remaining about the official form of some words and a lack of general material destined for the general public, much time during World War II was spent on language standardization and course creation, and in August 1943 the decision was made, given the length of the war, to create an interim academy to officialize this process. The standardization process had just about begun not long before the war, and the Swiss Occidentalists, finding themselves isolated from the rest of the continent, opted to concentrate on didactic materials to have prepared by the time the war reached its end. While doing so, they frequently found themselves confronted with the decision between two forms that had remained in popular usage, but which could be confusing to a new learner of the language. During this time, the academy maintained that standardization efforts were based on actual usage, stating that "...the standardization of the language has natural limits. "Standardizing" the language does not mean arbitrarily officializing one of the possible solutions and rejecting the others as indesirable and irritating. One only standardizes solutions that have already been sanctioned through practice."

During the war, Occidentalists noticed that the language was frequently permitted to be sent by telegram within and outside of Switzerland (especially to and from Sweden) even without official recognition, surmising that censors were able to understand it and may have thought them to be written in Spanish or Romansch, a minor yet official language in Switzerland that at the time lacked a standardized orthography. This allowed a certain amount of communication to take place between the Occidentalists in Switzerland and Sweden. The other centres of Occidental activity in Europe did not fare as well, with the stocks of study materials in Vienna and Tallinn having been destroyed in bombings and numerous Occidentalists sent to concentration camps in Germany and Czechoslovakia. Contacts were reestablished shortly after the war by those who had survived it, particularly from those in France, Czechoslovakia, and Great Britain. A few months before the end of World War II in Europe Cosmoglotta had subscribers in 58 cities in Switzerland. Cosmoglotta A began publication again in 1946.

The International Auxiliary Language Association, founded in 1924 to study and determine the best planned language for international communication, was at first viewed with suspicion by the Occidental community. The co-founder of the IALA Alice Vanderbilt Morris was an Esperantist, as were many of its staff members, and many Occidentalists including Edgar de Wahl himself believed that it had been set up to eventually recommend Esperanto over other planned languages under a neutral and scientific pretext using its staff of professional linguists to bolster a final recommendation. Relations soon improved, however, as it became clear that the IALA intended to be as impartial as possible by familiarizing itself with all existing planned languages. Ric Berger detailed one visit he made in 1935 to Morris (whose husband was the US ambassador in Brussels) that vastly improved his opinion of the organization: My personal opinion was not so pessimistic, for, finding myself in Brussels in 1935, I sought out Mrs. Morris and soon obtained an audience with her where my charming host invited me to speak in Occidental. She asked her husband, the American ambassador, to come hear me to confirm what seemed to very much interest them: a language in which all words can be understood without having learned it! ... Mrs. Morris could have used her fortune to simply support Esperanto, which was her right as a convicted Esperantist. But instead of that she...decided to donate her money to a *neutral* linguistic tribunal to solve the problem *scientifically*, even if the judgement goes against her convictions.As a result, opinions of the IALA and its activities in the Occidental community began to improve and reports on its activities in Cosmoglotta became increasingly positive. In 1945 the IALA announced that it planned to create its own language and showed four possible versions under consideration, all of which were naturalistic as opposed to schematic. Occidentalists were by and large pleased that the IALA had decided to create a language so similar in appearance to Occidental, seeing it as a credible association that gave weight to their argument that an auxiliary language should proceed from study of natural languages instead of attempting to fit them into an artificial system. Ric Berger was particularly positive about the IALA's new language, calling it in 1948 "almost the same language", though not without reservations, doubting whether a project with such a similar outward appearance would be able to "suddenly cause prejudices [against planned languages] to fall and create unity among the partisans of international languages" and fearing that it might simply "disperse the partisans of the natural language with nothing to show for it" after Occidental had created "unity in the naturalistic school" for so long.

While the two languages had a 90% identical vocabulary when word endings and orthographic differences were not taken into account (e.g. "filosofie" and "philosophia" would be considered the same word), structurally and derivationally they were very different. Occidental with De Wahl's Rule had either done away with Latin double stem verbs (verbs such as act: "ager", "act"- or send: "mitter", "miss"-) or found a way to work around them, while Interlingua simply accepted them as part and parcel of a naturalistic system. The "control languages" (English, French, Italian, Spanish-Portuguese) used by Interlingua to form its vocabulary also conflicted with Occidental's Germanic and other words which would be by definition ineligible in a combined language that retained Interlingua's methodology. Occidental words such as "mann, strax, old, sestra, clocca, svimmar, trincar", etc. could only be incorporated into Interlingua if it did away with the control languages, the languages on which Interlingua itself was based. Interlingua also allowed optional verbal conjugations (such as "so," "son" and "sia" as the first-person singular, third-person plural and subjunctive form of "esser", the verb 'to be') that Occidental had never even considered and viewed as incompatible with an easy international auxiliary language.

Meanwhile, Cosmoglotta was showing financial strain with inflated printing costs after the end of World War II and the inability to collect payments from certain countries, a marked contrast to the well-funded IALA which had been based in New York since the outbreak of the war.

The beginning of the Cold War in 1947 created a particularly uncomfortable situation for the Occidental-Union, which now possessed a name that by chance coincided with that of an anti-Russian political league, and which the Occidentalists in Switzerland believed to be the reason for the interception of all of de Wahl's letters sent from Tallinn. In early 1948 the Czechoslovak Occidentalists had begun requesting approval for a new name that would allow them to continue their linguistic activities without suspicion, proposing the name Interal (International auxiliari lingue), to which the union responded that the term "Interlingue" would be more appropriate and that they were free to introduce the language as Interlingue (Occidental), or even remove the mention of Occidental in parentheses if they felt it necessary. The official vote on the name change to Interlingue took place at the plenum of the Occidental Union in 1949 and was passed with 91% support, making the official name Interlingue, with Interlingue (Occidental) also permitted, valid as of the 1st of September 1949.

The year 1951 when Interlingua was announced was consequential in weakening Interlingue-Occidental, which until then had been unchallenged in the field of naturalistic planned auxiliary languages. Vĕra Barandovská-Frank's perception of the situation at the time was as follows (translated from Esperanto): "In the field of naturalistic planned languages Occidental-Interlingue was until then unchallenged (especially after the death of Otto Jespersen, author of Novial), as all new projects were nearly imitations of it. This applied to Interlingua as well, but it carried with it a dictionary of 27 000 words put together by professional linguists that brought great respect, despite in principle only confirming the path that De Wahl had started. The Senate of the Interlingue-Union and the Interlingue-Academie took up the proposals that (1) the Interlingue-Union become a collective member of the IALA and (2) the Interlingue-Union remain favourable to the future activity of the IALA and morally support it. The first proposition was not accepted, but the second was, giving a practical collaboration and support to Interlingua.

André Martinet, the second-last director of the IALA, made similar observations to those of Matejka. He confessed that his preferred variant of Interlingua was the one closer to Interlingue than the one officialized by Gode. In these circumstances the efforts by Ric Berger to move all users of Interlingue en masse to Interlingua de IALA was a shock. His heresy caused doubt and interruptions in Interlingue circles, especially after he became involved in the publication of "Revista de Interlingua". The former idea of a natural fusion of both languages was shown to be unrealistic, with the new language becoming a rival."

While the migration of so many users to Interlingua had severely weakened the Interlingue movement, the remaining users of the language kept the language alive for a time. Besides Cosmoglotta, now publishing once every second month from 1952 and then once per quarter from 1963, bulletins in Interlingue continued to appear such as Interlingue-Postillon (1958, Germany), Novas de Oriente (1958, Japan), Amicitie european (1959, Switzerland), Teorie e practica (Switzerland-Czechoslovakia, 1967), and Novas in Interlingue (Czechoslovakia, 1971). Interlingue activity reached a low during the 1980s and early 1990s, when Cosmoglotta publication ceased for a number of years. According to Esperantist Don Harlow, "in 1985 Occidental's last periodical, Cosmoglotta, ceased publication, and its editor, Mr. Adrian Pilgrim, is quoted as having described Occidental as a "dead language."" A decade later, a documentary in 1994 by Steve Hawley and Steyger on planned languages introduced Interlingue speaker Donald Gasper as "one of the last remaining speakers of the language Occidental" (a description perhaps better suited for former Occidental-Union president Alphonse Matejka who would not pass away until 1999, as Donald Gasper was a new learner of the language).

In the year 1999 the first Yahoo! Group in Occidental was founded, and Cosmoglotta had been publishing intermittently again. An Interlingue Wikipedia was approved in 2004. In recent years official meetings between Interlingue speakers have begun taking place again: a meeting in Ulm on 10 January 2013, another in Munich in 2014 with three participants, and a third in Ulm on 16 August 2015 with five.

The most recent edition of the magazine Cosmoglotta is volume 324, for the period January to December 2018.

One of the earliest users of the language Esperanto, Edgar de Wahl encountered it for the first time in 1888 and remained a fervent supporter of the language for a number of years where he collaborated with Zamenhof on the design of the language and translated one of the first works into Esperanto: "Princidino Mary", published in 1889 originally under the name Princino Mary. He remained an Esperantist until 1894 when the vote to reform Esperanto failed, a vote in which de Wahl was one of just two that voted neither for Esperanto unchanged, nor for the reform proposed by Zamenhof, but for a completely new reform. Occidental would not be announced for a full 28 years after de Wahl had abandoned Esperanto, a period in which he spent working with other language creators (such as Rosenberger's Idiom Neutral) and trying to develop a system that combined both naturality and regularity. De Wahl's method for doing so was twofold: through de Wahl's Rule to reduce the number of irregularities in verbal derivation to a minimum, and a large number of affixes to do the same with word roots in addition to giving the resulting forms a natural appearance. The large number of suffixes can be seen through a glimpse of just those used to form nouns referring to a type of person: "-er-" ("molinero" - miller), "-or-" ("redactor" - editor), "-ari-" ("millionario" - millionaire), "-on-" ("spion", spy), "-ard" ("mentard", liar), "-astr-" ("poetastro", lousy poet), "-es" ("franceso", Frenchman), "-essa" ("reyessa", queen). In de Wahl's opinion it was always preferable to opt for a productive suffix than to be forced to coin new words from completely new radicals later on.

De Wahl published in 1922 a modification of Otto Jespersen's principle that "the best language is that which is easiest for the greatest number of people", stating that the international language should be easiest for the majority of those who need it (lit. "who must apply it"), or in other words those that need it in international relations. Along with this came a need for an international language to recognize already international vocabulary regardless of the number of people using it, particularly in specialized areas where for example the term Oenethera biennis (a type of plant) should be implemented and not modified beyond recognition even if the entire world population of botanists, those most often knowing and using the word, did not exceed 10,000. This also implied that words belonging to particular cultures should be imported without modifications, which De Wahl believed brought new ideas of value to European culture that had become "sick" after the First World War. He cited the terms "karma", "ko-tau" (kowtow), "geisha", and "mahdí" in 1924 as examples of those that should not be put in a "vocalic corset" through obligatory endings (e.g. "karmo, koŭtoŭo," "gejŝo, madho" in Esperanto")" when imported into the international language: "Such words, still not large in number, have seen a large increase in the past century, and in the future will grow in exorbitant proportion when, through international communication, the ideas of stable Oriental cultures will inundate and influence the sick Europe, which is now losing its equilibrium. And the more mutilated the words are, the more mutilated will be the ideas that they represent." In an article on the future development of language, de Wahl wrote in 1927 that due to European dominance in the sciences and other areas Occidental required a form and derivation recognizable to Europeans, but that it should also be fitted with a grammatical structure capable of taking on more analytical, non-derived forms in the future (such as the equivalents of "bake man" for "baker" or "wise way" for "wisdom") if worldwide linguistic trends began to show a preference for them.

On the subject of schematic regularity versus naturalism in an international language, De Wahl believed that there was a fine balance to be maintained between the two, where too much of the former may be convenient for the early learner but abhorrent for a speaker, and vice versa in the latter case: "Exceptions are not made to make study more difficult for foreigners, but to make speaking shorter and more fluid...It is clear that in this language as the most impersonal, abstract and businesslike one of all, regularity will be greater and more expanded than in all other national and tribal languages and idioms. But it will never be able to attain a total schematism...Also here the real solution will be a harmonization of the two contrary principles. It requires the sensitive penetration of the real necessity in the instinct of the international superpopulation."

While primarily Romance in vocabulary, de Wahl opted for a large Germanic substrate which he felt more expressive for technical and material vocabulary (self, ost for east, svimmar for to swim, moss, etc.), with Romance and Greek vocabulary more appropriate in the derivation of international words (femina for woman to form feminin, can for dog to form canin, etc.) as well as mental, corporal and natural conceptions. Minor Romance languages such as Ladin, Provençal (Occitan) and Catalan along with creoles had a large importance in the development of Occidental for de Wahl, who wrote as far back as 1912 that his language under development was more similar to Provençal than Italian or Spanish. The Swiss magazine Landbote made a similar comment in 1945 in a review of the language, commenting humoristically that "reading through the few examples of Occidental gives us the impression of a half-learned Catalan by a foreigner who doesn't much understand the grammar." 

Using internationally recognized prefixes and suffixes did not imply wholesale importing of international words. Just before the beginning of the Second World War de Wahl called criticisms of Occidental as chaotic unfounded, stating that English and French users in particular had a tendency to see Occidental as a mix of the two: "(Occidental's chaotic appearance) is not the fault of Occidental itself, but rather that of its users and especially the French and English, or those that think that the international language should be a mixture of those two languages...that is a fundamental error, especially if these forms present exceptions and irregularities in Occidental's system." Alphonse Matejka wrote in Cosmoglotta that de Wahl "always claimed a minimum of autonomy for his language and bitterly fought against all propositions that intended to augment the naturality of the language only by blindly imitating the Romance languages, or as de Wahl said crudely in one of his letters to me, "by aping French or English"".

Occidental's erring on the side of regularity led to vocabulary that was still recognizable but different from the international norm, such as ínpossibil in place of impossibil (ín + poss + ibil), scientic (scientific, from scient-ie + -ic), and descrition (description, from descri-r + -tion). This is one of the greatest differences between it and Interlingua, which has a vocabulary taken from so-called 'prototypes' (the most recent common ancestor to its source languages) while Interlingue/Occidental focused on active, on the fly derivation. After the standardization of Occidental in 1947 and the name change to Interlingue in 1949 there was a push towards greater and greater naturalistic forms inspired by the IALA's soon-to-be-published Interlingua, particularly by Ric Berger who advocated replacing the optional -i adjectival ending with -e. After advocating for the change in April 1949 he began implementing it the following month in his own writing and most of the content in Cosmoglotta, in addition to other changes such as "nostre" (our) and "vostre" (your) instead of "nor" and "vor." The following April he defended the changes, denying that they were a "concession to the IALA" but instead a simple "concession to the general tendency towards greater naturality found today in the interlinguistic movement", calling critics of the changes victims of "long-lasting habits" and an "optical illusion". Whether these experimental changes would have taken root is not known, as Berger left his position as editor of Cosmoglotta soon after and eventually joined Interlingua, while Cosmoglotta returned to publishing in the 1947 standard that continues to this day.

The symbol of Occidental was chosen in 1936 after some deliberation and many other proposed symbols, including stylized letters, a star as in Esperanto and Ido, a setting sun to represent the sun in the west (the Occident), a globe, and more. The tilde, already used by the Occidental-Union, was eventually selected based on five criteria: symbolic character, simplicity, originality, inconfusability, and for being bichromatic (having two colours) as opposed to polychromatic. Beyond the five criteria, the Occidentalists at the time referenced the lack of a fixed meaning for the tilde in the public sphere, and its similarity to a waveform, implying speech.

Interlingue is written with 26 Latin letters: a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z. The letters of the alphabet are pronounced as "a, be, ce, de, e, ef, ge, ha, i, jot, ka, el, em, en, o, pe, qu, er, es, te, u, ve, duplic ve, ix, ypsilon", and "zet".

The vowels "a, e, i, o", and "u" have a continental pronunciation and are all sounded. The "y" (initial and medial) are pronounced as in "yes", "ey" (final) as in "they", and "eu" as éh-oo.

The consonants are pronounced as in English, with the following exceptions:

Other doubled consonants are pronounced as a single consonant, unless when separated they would be pronounced differently. Ex. grammatica is pronounced as if written gramatica, but occidental and suggestion are pronounced as if written as oc followed by cidental, and sug followed by gestion.

Words are generally stressed on the vowel before the final consonant: intercalar, parol, forme. Pluralizing a noun does not change the stress: paroles, formes. The endings -bil, -ic, -im, -ul and -um do not change the stress (even when more than one is present in a single word), nor does the adverbial ending -men: rapidmen, duplic, bonissim, singul, possibil, maximum, statisticas. Two vowels together are diphthongized and do not count as two syllables for the purpose of stress: familie, potentie, unless the word is a single consonant or consonant cluster followed by two vowels: die, deo. Compound words are stressed based on the last word in the compound: hodie, substrae. In cases where the accent is irregular, it is indicated by an accent: café, ínpossibil, numeró, númere, felicitá.

The accent mark is also sometimes used to stress a word (In un casu li naves proveni de ún state = In one case the ships originate from "one" country), or over the particles ú when used as a conjunction, ó when used to mean 'either' (ó A, ó B), and é when used to mean 'both' (é A, é B). e.g. Yo ne save u il es (I don't know where he is), Yo vole trincar e lacte e bir (I want to drink both milk and beer) and O il ne save li loc, o il ne vole venir (Either he does not know the location or he does not want to come) will sometimes be seen written as Yo ne save ú il es, Yo vole trincar é lacte é bir, and Ó il ne save li loc, ó il ne vole venir.

Like English, Interlingue has a definite article and an indefinite article. The definite article (the) is "li", and the indefinite (a, an) is "un". Plural of a noun is made by adding "-s" after a vowel, or "-es" after most consonants. To avoid pronunciation and stress changes, words ending in -c, -g, and -m only add an -s: un libre, du libres, un angul, tri angules, li tric, li trics, li plug, li plugs, li album, pluri albums, li tram, du trams.

Interlingue has two forms for the personal pronouns: one for the subject form (nominative), and one for the object form (accusative or dative). In short, the personal pronouns in the subject form are:

The variants "illa" and "ella" both exist for third person singular feminine. The pronoun expressing politeness is "vu", which behaves like second person plural. The indefinite personal pronoun "one" is "on" in Occidental. If necessary, one can specify the gender of third person plural by using "illos" (masculine) or e"llas" (feminine).

In the object form the pronouns are: me, te, le, la, it, nos, vos, and les (with los and las as specific masculine and feminine forms, respectively). In the oblique case, the pronouns are me, te, il (or le), noi (or nos), voi (or vos), and ili (or les), varying by user and situation outside of the forms me and te. The possessive pronouns are mi, tui, su (his/her/its), nor, vor and lor. They may be pluralized: li mi (mine, singular), li mis (mine, plural), li nor (ours, singular), li nores (ours, plural).

Grammatical endings are used to a certain extent, though to a lesser degree than languages such as Esperanto and Ido where parts of speech are marked with obligatory endings. Only a few parts of speech (such as verb infinitives) in Interlingue have entirely obligatory endings, while many others either have endings the usage of which is optional and sometimes recommended. Some grammatical endings are:


While correlatives were not designed to match a pre-determined scheme, the majority of them do match the prefixes and suffixes shown in the chart below.
Alcun (some) and necun (no, none) are respectively the adjectives of alquel and nequel

The -qui series has optional accusative forms ending in -em: quem, alquem, nequem

The -al series is adverbialized with the -men ending: qualmen (how) talmen (that way)

The -el and -al series can take the plural ending: queles, quales

Verbs in Interlingue have three endings: -ar, -er, and -ir, and are invariable. Conjugation is carried out with a combination of endings and auxiliary verbs. The verb esser (to be) is written es in the present due to its high frequency.

Simple Verb Tenses
Compound Verb Tenses
The present participle is used to qualify nouns: un cat ama, un amant cat (a cat loves, a loving cat) and is often seen in adjectives such as fatigant (tiring, from fatigar, to tire). The gerund is used to indicate another action or state of being going on at the same time: scriente un missage, yo videt que... (writing a message, I saw that...).

Many further combinations of endings and auxiliary verbs are possible. Some examples:

Yo vell har esset amat = I would have been loved

Hante retornat al dom... = Having returned to the house... (ha + gerund)

Other notes on verbs:

The subjunctive does not exist in Interlingue: yo vole que tu ama (I want you to love). Mey is often used to express it when necessary, however, frequently after que: Yo vole que tu mey amar (I want you to love, lit. "I want that you may love").

Hay is a standalone verb that means there is or "there are". Hay du homes in li dom (there are two people in the house). As a standalone verb there is no official infinitive but users of the language often conjugate it as if there were (hayat, etc.) Other ways of expressing there is or there are: esser (esset nequó altri a far = there was nothing else to do), exister (it existe du metodes = there are two ways), trovar se (in li cité trova se tri cavalles = there are three horses in the city), etc.

The passive is formed using the verb esser: yo es amat (I am loved). Se makes the verb refer to itself (reflexive form) which often functions as a shorter way to form the passive: li frontieras esset cludet = li frontieras cludet se (the borders were closed).

The progressive tense (-nt) is not used with the same frequency as in English (what are you doing? = quo tu fa?, not quo tu es fant?). It emphasizes the continuity of the verb and is often used in storytelling (noi esset marchant vers li rivere quande... = we were walking towards the river when...)

The verb star (to stand) may be used to emphasize the completion of a verb: li dom sta constructet (the house stands constructed, i.e. it is completely built)

The verb ear (to go) may be used to emphasize the continuity of a verb: li dom ea constructet (the house is being built).

Adverbs are formed with the ending -men (rapid = quick, rapidmen = quickly). The ending may be omitted when the meaning is clear: tu deve far it rapid(men) = you must do it quick(ly).

The double negative is permitted, and was even recommended by de Wahl for its internationality and precision. De Wahl gave the following phrase as an example: "Yo ha trovat li libre, quem vu ha dat me, in null loc, quem vu ha indicat me" (lit. "I found the book you gave me nowhere you indicated me", thus "I didn't find the book anywhere you told me to look"). In this phrase, not permitting a double negative would result in ambiguity up to the word null, the only indication of a negative in the phrase, recommending Yo ne ha trovat li libre...in null loc. An obligatory double negative was never imposed and later Occidentalists found that they rarely used it, but it remained permitted and is seen from time to time.

The infinitive may also used as a mild or impersonal imperative: ne fumar - no smoking; bon comprender: un crímine es totvez un crímine - let's be clear (lit. "understand well"): a crime is still a crime.

The application of de Wahl's rule to verbs and the usage of numerous suffixes and prefixes was created to resolve irregularities that had plagued creators of language projects before Occidental, who were forced to make the choice between regularity and innatural forms, or irregularity and natural forms. The prevailing view was that natural forms needed to be sacrificed for the sake of regularity, while those that opted for naturality were forced to admit numerous irregularities when doing so (Idiom Neutral for example had a list of 81 verbs with special radicals used when forming derivatives). The rules created by Edgar de Wahl to resolve this are:


Once these rules were applied, Occidental was left with six exceptions. They are:


Suffixes are added either to the verbal root or the present theme of the verb (the infinitive minus -r). An example of the latter is the suffix -ment: move/r, move/ment (not movetment), experi/r, experi/ment (not experitment), and -ntie (English -nce): tolera/r (tolerate), tolera/ntie, existe/r (exist), existe/ntie.

The major prefixes and suffixes used in word derivation in Interlingue are added to either the present theme (infinitive minus -r), verbal root (infinitive minus two preceding vowels), or perfect theme (present theme + t or +s for verbs finishing with -d or -r) of verbs, as well as other types of speech. The below is a sample of some of the affixes used.

As an international auxiliary language, ease of learning through regular derivation and recognizable vocabulary was a key principle in Occidental's creation. The magazine Cosmoglotta often featured letters from new users and former users of other international languages (primarily Esperanto and Ido) attesting to the language's simplicity: letters from new users to demonstrate their quick command of the language, and attestations from experienced auxiliary language users to share their experiences. Because many users of Occidental had encountered the language after gaining experience in others, objective data on learnability of the language is difficult to find. One experiment to ascertain learning time was carried out however in the years 1956 to 1957 in a Swiss Catholic high school (gymnasium) in Disentis on the time required to learn the language. The experiment showed that the students participating in the study, who had previous experience with French, Latin, and Greek, mastered both written and spoken Interlingue after 30 hours of study.

Possible pronunciation: 

Translation: "Material civilization, science, and even art unify themselves more and more. The educated European feels himself almost at home in all lands that have European civilization, that is, more and more, in the entire world. Today almost all states war with the same armaments. Without pause the modes of intercommunication improve, and in consequence from that the world seems to decrease. A Parisian is now closer to an Englishman or a German than he was a hundred years before to a French peasant."






</doc>
<doc id="22763" url="https://en.wikipedia.org/wiki?curid=22763" title="Osiris">
Osiris

Osiris (, from Egyptian "wsjr", Coptic ) is the god of fertility, agriculture, the afterlife, the dead, resurrection, life, and vegetation in ancient Egyptian religion. He was classically depicted as a green-skinned deity with a pharaoh's beard, partially mummy-wrapped at the legs, wearing a distinctive atef crown, and holding a symbolic crook and flail. He was one of the first to be associated with the mummy wrap. When his brother, Set, cut him up into pieces after killing him, Isis, his wife, found all the pieces and wrapped his body up, enabling him to return to life. Osiris was at times considered the eldest son of the god Geb and the sky goddess Nut, as well as being brother and husband of Isis, with Horus being considered his posthumously begotten son. He was also associated with the epithet Khenti-Amentiu, meaning "Foremost of the Westerners", a reference to his kingship in the land of the dead. Through syncretism with Iah, he is also a god of the Moon.

Osiris can be considered the brother of Isis, Set, Nephthys, and Horus the Elder, and father of Horus the Younger. The first evidence of the worship of Osiris was found in the middle of the Fifth Dynasty of Egypt (25th century BC), although it is likely that he was worshiped much earlier; the Khenti-Amentiu epithet dates to at least the First Dynasty, and was also used as a pharaonic title. Most information available on the Osiris myth is derived from allusions contained in the Pyramid Texts at the end of the Fifth Dynasty, later New Kingdom source documents such as the Shabaka Stone and the "The Contendings of Horus and Seth", and much later, in narrative style from the writings of Greek authors including Plutarch and Diodorus Siculus.

Osiris was the judge of the dead and the underworld agency that granted all life, including sprouting vegetation and the fertile flooding of the Nile River. He was described as "He Who is Permanently Benign and Youthful" and the "Lord of Silence". The kings of Egypt were associated with Osiris in death – as Osiris rose from the dead so they would be in union with him, and inherit eternal life through a process of imitative magic.

Through the hope of new life after death, Osiris began to be associated with the cycles observed in nature, in particular vegetation and the annual flooding of the Nile, through his links with the heliacal rising of Orion and Sirius at the start of the new year. Osiris was widely worshipped until the decline of ancient Egyptian religion during the rise of Christianity in the Roman Empire.

"Osiris" is a Latin transliteration of the Ancient Greek , which in turn is the Greek adaptation of the original name in the Egyptian language. In Egyptian hieroglyphs the name appears as "wsjr", which some Egyptologists instead choose to transliterate "ꜣsjr" or "jsjrj". Since hieroglyphic writing lacks vowels, Egyptologists have vocalized the name in various ways, such as Asar, Ausar, Ausir, Wesir, Usir, or Usire.

Several proposals have been made for the etymology and meaning of the original name; as Egyptologist Mark J. Smith notes, none are fully convincing. Most take "wsjr" as the accepted transliteration, following Adolf Erman:


However, recently alternative transliterations have been proposed:


Osiris is represented in his most developed form of iconography wearing the "Atef" crown, which is similar to the White crown of Upper Egypt, but with the addition of two curling ostrich feathers at each side. He also carries the crook and flail. The crook is thought to represent Osiris as a shepherd god. The symbolism of the flail is more uncertain with shepherds whip, fly-whisk, or association with the god Andjety of the ninth nome of Lower Egypt proposed.

He was commonly depicted as a pharaoh with a complexion of either green (the color of rebirth) or black (alluding to the fertility of the Nile floodplain) in mummiform (wearing the trappings of mummification from chest downward).

The Pyramid Texts describe early conceptions of an afterlife in terms of eternal travelling with the sun god amongst the stars. Amongst these mortuary texts, at the beginning of the Fourth Dynasty, is found: ""An offering the king gives and Anubis"". By the end of the Fifth Dynasty, the formula in all tombs becomes ""An offering the king gives and Osiris"".

Osiris is the mythological father of the god Horus, whose conception is described in the Osiris myth (a central myth in ancient Egyptian belief). The myth describes Osiris as having been killed by his brother Set, who wanted Osiris' throne. His wife, Isis, finds the body of Osiris and hides it in the reeds where it is found and dismembered by Set. Isis retrieves and joins the fragmented pieces of Osiris, then briefly revives him by use of magic. This spell gives her time to become pregnant by Osiris. Isis later gives birth to Horus. As such, since Horus was born after Osiris' resurrection, Horus became thought of as a representation of new beginnings and the vanquisher of the usurper Set.

"Ptah-Seker" (who resulted from the identification of the creator god Ptah with Seker) thus gradually became identified with Osiris, the two becoming Ptah-Seker-Osiris. As the sun was thought to spend the night in the underworld, and was subsequently "reborn" every morning, Ptah-Seker-Osiris was identified as king of the underworld, god of the afterlife, life, death, and regeneration.

Osiris' soul, or rather his "Ba", was occasionally worshipped in its own right, almost as if it were a distinct god, especially in the Delta city of Mendes. This aspect of Osiris was referred to as "Banebdjedet", which is grammatically feminine (also spelt ""Banebded"" or ""Banebdjed""), literally "the "ba" of the lord of the "djed", which roughly means "The soul of the lord of the pillar of continuity". The "djed", a type of pillar, was usually understood as the backbone of Osiris.

The Nile supplying water, and Osiris (strongly connected to the vegetable regeneration) who died only to be resurrected, represented continuity and stability. As "Banebdjed", Osiris was given epithets such as "Lord of the Sky" and "Life of the (sun god) Ra". "Ba" does not mean "soul" in the western sense, and has to do with power, reputation, force of character, especially in the case of a god.

Since the "ba" was associated with power, and also happened to be a word for ram in Egyptian, Banebdjed was depicted as a ram, or as Ram-headed. A living, sacred ram was kept at Mendes and worshipped as the incarnation of the god, and upon death, the rams were mummified and buried in a ram-specific necropolis. Banebdjed was consequently said to be Horus' father, as Banebdjed was an aspect of Osiris.

Regarding the association of Osiris with the ram, the god's traditional crook and flail are the instruments of the shepherd, which has suggested to some scholars also an origin for Osiris in herding tribes of the upper Nile.

Plutarch recounts one version of the Osiris myth in which Set (Osiris' brother), along with the Queen of Ethiopia, conspired with 72 accomplices to plot the assassination of Osiris.
Set fooled Osiris into getting into a box, which Set then shut, sealed with lead, and threw into the Nile. Osiris' wife, Isis, searched for his remains until she finally found him embedded in a tamarisk tree trunk, which was holding up the roof of a palace in Byblos on the Phoenician coast. She managed to remove the coffin and retrieve her husband's body.

In one version of the myth, Isis used a spell to briefly revive Osiris so he could impregnate her. After embalming and burying Osiris, Isis conceived and gave birth to their son, Horus. Thereafter Osiris lived on as the god of the underworld. Because of his death and resurrection, Osiris was associated with the flooding and retreating of the Nile and thus with the yearly growth and death of crops along the Nile valley.

Diodorus Siculus gives another version of the myth in which Osiris was described as an ancient king who taught the Egyptians the arts of civilization, including agriculture, then travelled the world with his sister Isis, the satyrs, and the nine muses, before finally returning to Egypt. Osiris was then murdered by his evil brother Typhon, who was identified with Set. Typhon divided the body into twenty-six pieces, which he distributed amongst his fellow conspirators in order to implicate them in the murder. Isis and Hercules (Horus) avenged the death of Osiris and slew Typhon. Isis recovered all the parts of Osiris' body, except the phallus, and secretly buried them. She made replicas of them and distributed them to several locations, which then became centres of Osiris worship.

Annual ceremonies were performed in honor of Osiris in various places across Egypt. These ceremonies were fertility rites which symbolised the resurrection of Osiris. Recent scholars emphasize "the androgynous character of [Osiris'] fertility" clear from surviving material. For instance, Osiris' fertility has to come both from being castrated/cut-into-pieces and the reassembly by female Isis, whose embrace of her reassembled Osiris produces the perfect king, Horus. Further, as attested by tomb-inscriptions, both women and men could syncretize (identify) with Osiris at their death, another set of evidence that underline Osiris' androgynous nature.

Plutarch and others have noted that the sacrifices to Osiris were "gloomy, solemn, and mournful..." (Isis and Osiris, 69) and that the great mystery festival, celebrated in two phases, began at Abydos commemorating the death of the god, on the same day that grain was planted in the ground (Isis and Osiris, 13). The annual festival involved the construction of "Osiris Beds" formed in shape of Osiris, filled with soil and sown with seed. The germinating seed symbolized Osiris rising from the dead. An almost pristine example was found in the tomb of Tutankhamun.

The first phase of the festival was a public drama depicting the murder and dismemberment of Osiris, the search of his body by Isis, his triumphal return as the resurrected god, and the battle in which Horus defeated Set.

According to Julius Firmicus Maternus of the fourth century, this play was re-enacted each year by worshippers who "beat their breasts and gashed their shoulders... When they pretend that the mutilated remains of the god have been found and rejoined...they turn from mourning to rejoicing." ("De Errore Profanorum").

The passion of Osiris was reflected in his name 'Wenennefer" ("the one who continues to be perfect"), which also alludes to his post mortem power.

Much of the extant information about the rites of Osiris can be found on the Ikhernofret Stela at Abydos erected in the Twelfth Dynasty by Ikhernofret, possibly a priest of Osiris or other official (the titles of Ikhernofret are described in his stela from Abydos) during the reign of Senwosret III (Pharaoh Sesostris, about 1875 BC). The ritual reenactment of Osiris's funeral rites were held in the last month of the inundation (the annual Nile flood), coinciding with Spring, and held at Abydos which was the traditional place where the body of Osiris drifted ashore after having been drowned in the Nile.

The part of the myth recounting the chopping up of the body into 14 pieces by Set is not recounted in this particular stela. Although it is attested to be a part of the rituals by a version of the Papyrus Jumilhac, in which it took Isis 12 days to reassemble the pieces, coinciding with the festival of ploughing. Some elements of the ceremony were held in the temple, while others involved public participation in a form of theatre. The Stela of Ikhernofret recounts the programme of events of the public elements over the five days of the Festival:

Contrasting with the public "theatrical" ceremonies sourced from the I-Kher-Nefert stele (from the Middle Kingdom), more esoteric ceremonies were performed inside the temples by priests. Plutarch mentions that (for much later period) two days after the beginning of the festival "the priests bring forth a sacred chest containing a small golden coffer, into which they pour some potable water...and a great shout arises from the company for joy that Osiris is found (or resurrected). Then they knead some fertile soil with the water...and fashion therefrom a crescent-shaped figure, which they cloth and adorn, this indicating that they regard these gods as the substance of Earth and Water." ("Isis and Osiris," 39). Yet his accounts were still obscure, for he also wrote, "I pass over the cutting of the wood" – opting not to describe it, since he considered it as a most sacred ritual ("Ibid." 21).

In the Osirian temple at Denderah, an inscription (translated by Budge, Chapter XV, Osiris and the Egyptian Resurrection) describes in detail the making of wheat paste models of each dismembered piece of Osiris to be sent out to the town where each piece is discovered by Isis. At the temple of Mendes, figures of Osiris were made from wheat and paste placed in a trough on the day of the murder, then water was added for several days, until finally the mixture was kneaded into a mold of Osiris and taken to the temple to be buried (the sacred grain for these cakes were grown only in the temple fields). Molds were made from the wood of a red tree in the forms of the sixteen dismembered parts of Osiris, the cakes of "divine" bread were made from each mold, placed in a silver chest and set near the head of the god with "the inward parts of Osiris" as described in the Book of the Dead (XVII).

The idea of divine justice being exercised after death for wrongdoing during life is first encountered during the Old Kingdom in a Sixth Dynasty tomb containing fragments of what would be described later as the Negative Confessions performed in front of the 42 Assessors of Ma'at.

At death a person faced judgment by a tribunal of forty-two divine judges. If they led a life in conformance with the precepts of the goddess Ma'at, who represented truth and right living, the person was welcomed into the kingdom of Osiris. If found guilty, the person was thrown to the soul-eating demon Ammit and did not share in eternal life. The person who is taken by the devourer is subject first to terrifying punishment and then annihilated. These depictions of punishment may have influenced medieval perceptions of the inferno in hell via early Christian and Coptic texts. Purification for those who are considered justified may be found in the descriptions of "Flame Island", where they experience the triumph over evil and rebirth. For the damned, complete destruction into a state of non-being awaits, but there is no suggestion of eternal torture.

During the reign of Seti I, Osiris was also invoked in royal decrees to pursue the living when wrongdoing was observed but kept secret and not reported.

The early Ptolemaic kings promoted a new god, Serapis, who combined traits of Osiris with those of various Greek gods and was portrayed in a Hellenistic form. Serapis was often treated as the consort of Isis and became the patron deity of the Ptolemies' capital, Alexandria. Serapis's origins are not known. Some ancient authors claim the cult of Serapis was established at Alexandria by Alexander the Great himself, but most who discuss the subject of Serapis's origins give a story similar to that by Plutarch. Writing about 400 years after the fact, Plutarch claimed that Ptolemy I established the cult after dreaming of a colossal statue at Sinope in Anatolia. His councillors identified as a statue of the Greek god Pluto and said that the Egyptian name for Pluto was Serapis. This name may have been a Hellenization of "Osiris-Apis". Osiris-Apis was a patron deity of the Memphite Necropolis and the father of the Apis bull who was worshipped there, and texts from Ptolemaic times treat "Serapis" as the Greek translation of "Osiris-Apis". But little of the early evidence for Serapis's cult comes from Memphis, and much of it comes from the Mediterranean world with no reference to an Egyptian origin for Serapis, so Mark Smith expresses doubt that Serapis originated as a Greek form of Osiris-Apis's name and leaves open the possibility that Serapis originated outside Egypt.

The cult of Isis and Osiris continued at Philae until at least the 450s CE, long after the imperial decrees of the late 4th century that ordered the closing of temples to "pagan" gods. Philae was the last major ancient Egyptian temple to be closed.



</doc>
<doc id="22764" url="https://en.wikipedia.org/wiki?curid=22764" title="Orthodox Baháʼí Faith">
Orthodox Baháʼí Faith

The Orthodox Baháʼí Faith is a small Baháʼí sect that formed in 1960 by Mason Remey, and subsequently was the name used by Joel Marangella after he claimed to be Remey's successor. The basis of the dispute is over the identity of the Guardian, a term referring to the appointed head of the religion, an executive hereditary office held by Shoghi Effendi from 1921 to 1957.

Other than on the matter of leadership and organization, there are few differences between the orthodox and mainstream Baháʼís in matters of doctrine. As a group who believe that Mason Remey was the second Guardian of the Baháʼí Faith, they are considered heretical Covenant-breakers by the majority of Baháʼís who follow the leadership of the Universal House of Justice. 

Membership data of the Orthodox Baháʼís is scarce. One source estimated them at no more than 100 members as of 1988. Memorandums from an Illinois court case in 2007 state their membership in the United States at 40. Websites claiming to represent the Orthodox community indicate followers in the United States and India. Joel Marangella died in San Diego, California on Sept 1, 2013.

Following the unexpected death of the Baháʼí Faith's first Guardian Shoghi Effendi in 1957, the 27 living Hands of the Cause, having the responsibility to acknowledge any appointment of a successor, gathered and decided that he had died "without having appointed his successor," and that the Universal House of Justice would decide on the situation after its first election. Charles Mason Remey, one of the Hands, declared himself the successor to Shoghi Effendi in 1960. His claim was rejected by the 26 remaining Hands, on the basis that he was not a descendant of Baháʼu'lláh, nor was he appointed to the position by Shoghi Effendi. Remey based his claim on his being the president of the International Baháʼí Council appointed by Shoghi Effendi in 1951. The result was that Remey was unanimously expelled from the Baháʼí community by the Hands of the Cause.

In 1962 Remey asked his supporters in the United States to organize themselves and elect a "National Spiritual Assembly Under the Hereditary Guardianship" (NSAUHG), first elected in 1963. The Assembly of 9 members was incorporated in New Mexico in 1964.

In 1964 the NSAUHG filed a lawsuit against the National Spiritual Assembly (NSA) of the Baháʼís of the United States to receive the legal title to the Baháʼí House of Worship in Illinois, and all other property owned by the NSA. The NSA counter-sued, and in August 1966 Remey instructed the NSAUHG to withdraw from any action in the matter "regardless of the consequences." Later that year, Remey asked the NSAUHG to dissolve, as well as the International Baháʼí Council that he had appointed with Joel Marangella as president, residing in France. Marangella previously served on the National Spiritual Assembly of France in 1961, and was declared a Covenant-breaker when he accepted Mason Remey as the next Guardian.

Over the years following 1966 the followers of Mason Remey were not organized; with some of his followers concluding that Remey was suffering from dementia, until several of the individuals involved began forming their own groups based on different understandings of succession.

In 1962 Remey gave Marangella a sealed envelope, with instructions to open it when the time was right. In 1965 Mason Remey called for the International Baháʼí Council, of which Marangella was president, to become active. Marangella then opened the sealed letter, which was a hand-written note by Remey appointing Marangella as his successor. Marangella looks upon that time as the time of his official appointment. Remey then changed his mind, deactivated the International Baháʼí Council in 1966, and in 1969 Marangella announced that he was the third Guardian. All of the members of the 1966 NSAUHG accepted Marangella's claim. 

In 1970 Marangella appointed members to a "National Bureau of the Orthodox Baháʼís in New York", which two years later was moved to New Mexico, and subsequently changed its name to "Mother Baháʼí Council of the United States" (1978) and "Provisional National Baháʼí Council" (2000), with all members appointed by Joel Marangella.. 

Marangella died in San Diego, CA on Sept 1, 2013.



</doc>
<doc id="22770" url="https://en.wikipedia.org/wiki?curid=22770" title="1">
1

1 (one, also called unit, and unity) is a number, and a numerical digit used to represent that number in numerals. It represents a single entity, the unit of counting or measurement. For example, a line segment of "unit length" is a line segment of length 1. 1 is the first and smallest positive integer. It is also sometimes considered the first of the infinite sequence of natural numbers, followed by 2, although by other definitions 1 is the second natural number, following 0.

The fundamental mathematical property of 1 is to be a multiplicative identity, meaning that any number multiplied by 1 returns that number. Most if not all properties of 1 can be deduced from this. In advanced mathematics, a multiplicative identity is often denoted 1, even if it is not a number. 1 is by convention not considered a prime number; although universal today, this was a matter of some controversy until the mid-20th century.

The word "one" can be used as a noun, an adjective and a pronoun.

It comes from the English word "an", which comes from the Proto-Germanic root . The Proto-Germanic root comes from the Proto-Indo-European root "*oi-no-".

Compare the Proto-Germanic root to Old Frisian "an", Gothic "ains", Danish "en", Dutch "een", German "eins" and Old Norse "einn".

Compare the Proto-Indo-European root "*oi-no-" (which means "one, single") to Greek "oinos" (which means "ace" on dice), Latin "unus" (one), Old Persian , Old Church Slavonic "-inu" and "ino-", Lithuanian "vienas", Old Irish "oin" and Breton "un" (one).

One, sometimes referred to as unity, is the first non-zero natural number. It is thus the integer after zero.

Any number multiplied by one remains that number, as one is the identity for multiplication. As a result, 1 is its own factorial, its own square and square root, its own cube and cube root, and so on. One is also the result of the empty product, as any number multiplied by one is itself. It is also the only natural number that is neither composite nor prime with respect to division, but is instead considered a unit (meaning of ring theory).

The glyph used today in the Western world to represent the number 1, a vertical line, often with a serif at the top and sometimes a short horizontal line at the bottom, traces its roots back to the Brahmic script of ancient India, where it was a simple vertical line. It was transmitted to Europe via Arabic during the Middle Ages.

In some countries, the serif at the top is sometimes extended into a long upstroke, sometimes as long as the vertical line, which can lead to confusion with the glyph for seven in other countries. Whereas the number 1 is written with a long upstroke, the number 7 has a horizontal stroke through the vertical line.

While the shape of the 1 character has an ascender in most modern typefaces, in typefaces with text figures, the character usually is of x-height, as, for example, in .
Many older typewriters do not have a separate symbol for "1", and use the lowercase letter "l" instead. It is possible to find cases when the uppercase "J" is used, while it may be for decorative purposes.

Mathematically, 1 is:

Formalizations of the natural numbers have their own representations of 1. In the Peano axioms, 1 is the successor of 0. In "Principia Mathematica", it is defined as the set of all singletons (sets with one element), and in the Von Neumann cardinal assignment of natural numbers, it is defined as the set {0}.

In a multiplicative group or monoid, the identity element is sometimes denoted 1, but "e" (from the German "Einheit", "unity") is also traditional. However, 1 is especially common for the multiplicative identity of a ring, i.e., when an addition and 0 are also present. When such a ring has characteristic "n" not equal to 0, the element called 1 has the property that (where this 0 is the additive identity of the ring). Important examples are finite fields.

By definition, 1 is the magnitude, absolute value, or norm of a unit complex number, unit vector, and a unit matrix (more usually called an identity matrix). Note that the term "unit matrix" is sometimes used to mean something quite different.

By definition, 1 is the probability of an event that is almost certain to occur.

In category theory, 1 is sometimes used to denote the terminal object of a category.

In number theory, 1 is the value of Legendre's constant, which was introduced in 1808 by Adrien-Marie Legendre in expressing the asymptotic behavior of the prime-counting function. Legendre's constant was originally conjectured to be approximately 1.08366, but was proven to equal exactly 1 in 1899.

Tallying is often referred to as "base 1", since only one mark – the tally itself – is needed. This is more formally referred to as a unary numeral system. Unlike base 2 or base 10, this is not a positional notation.

Since the base 1 exponential function (1) always equals 1, its inverse does not exist (which would be called the logarithm base 1 if it did exist).

There are two ways to write the real number 1 as a recurring decimal: as 1.000..., and as 0.999...
1 is the first figurate number of every kind, such as triangular number, pentagonal number and centered hexagonal number, to name just a few.

In many mathematical and engineering problems, numeric values are typically "normalized" to fall within the unit interval from 0 to 1, where 1 usually represents the maximum possible value in the range of parameters. Likewise, vectors are often normalized into unit vectors (i.e., vectors of magnitude one), because these often have more desirable properties. Functions, too, are often normalized by the condition that they have integral one, maximum value one, or square integral one, depending on the application.

Because of the multiplicative identity, if "f"("x") is a multiplicative function, then "f"(1) must be equal to 1.

It is also the first and second number in the Fibonacci sequence (0 being the zeroth) and is the first number in many other mathematical sequences.

The definition of a field requires that 1 must not be equal to 0. Thus, there are no fields of characteristic 1. Nevertheless, abstract algebra can consider the field with one element, which is not a singleton and is not a set at all.

1 is the most common leading digit in many sets of data, a consequence of Benford's law.

1 is the only known Tamagawa number for a simply connected algebraic group over a number field.

The generating function that has all coefficients 1 is given by

formula_1

This power series converges and has finite value if and only if formula_2.

1 is by convention neither a prime number nor a composite number, but a unit (meaning of ring theory) like −1 and, in the Gaussian integers, "i" and −"i". 

The fundamental theorem of arithmetic guarantees unique factorization over the integers only up to units. For example, , but if units are included, is also equal to, say, among infinitely many similar "factorizations".

1 appears to meet the naïve definition of a prime number, being evenly divisible only by 1 and itself (also 1). As such, some mathematicians considered it a prime number as late as the middle of the 20th century, but mathematical consensus has generally and since then universally been to exclude it for a variety of reasons (such as complicating the fundamental theorem of arithmetic and other theorems related to prime numbers). 

1 is the only positive integer divisible by exactly one positive integer, whereas prime numbers are divisible by exactly two positive integers, composite numbers are divisible by more than two positive integers, and zero is divisible by all positive integers.



In the philosophy of Plotinus (and that of other neoplatonists), The One is the ultimate reality and source of all existence. Philo of Alexandria (20 BC – AD 50) regarded the number one as God's number, and the basis for all numbers ("De Allegoriis Legum," ii.12 [i.66]).










</doc>
<doc id="22773" url="https://en.wikipedia.org/wiki?curid=22773" title="Oxidative phosphorylation">
Oxidative phosphorylation

Oxidative phosphorylation (UK , US or electron transport-linked phosphorylation) is the metabolic pathway in which cells use enzymes to oxidize nutrients, thereby releasing the chemical energy of molecular oxygen, which is used to produce adenosine triphosphate (ATP). In most eukaryotes, this takes place inside mitochondria. Almost all aerobic organisms carry out oxidative phosphorylation. This pathway is so pervasive because the energy of the double bond of oxygen is so much higher than the energy of the double bond in carbon dioxide or in pairs of single bonds in organic molecules observed in alternative fermentation processes such as anaerobic glycolysis.

During oxidative phosphorylation, electrons are transferred from electron donors to electron acceptors such as oxygen in redox reactions. These redox reactions release the energy stored in the relatively weak double bond of O, which is used to form ATP. In eukaryotes, these redox reactions are catalyzed by a series of protein complexes within the inner membrane of the cell's mitochondria, whereas, in prokaryotes, these proteins are located in the cell's intermembrane space. These linked sets of proteins are called electron transport chains. In eukaryotes, five main protein complexes are involved, whereas in prokaryotes many different enzymes are present, using a variety of electron donors and acceptors.

The energy transferred by electrons flowing through this electron transport chain is used to transport protons across the inner mitochondrial membrane, in a process called "electron transport". This generates potential energy in the form of a pH gradient and an electrical potential across this membrane. This store of energy is tapped when protons flow back across the membrane and down the potential energy gradient, through a large enzyme called ATP synthase; this process is known as chemiosmosis. The ATP synthase uses the energy to transform adenosine diphosphate (ADP) into adenosine triphosphate, in a phosphorylation reaction. The reaction is driven by the proton flow, which forces the rotation of a part of the enzyme; the ATP synthase is a rotary mechanical motor.

Although oxidative phosphorylation is a vital part of metabolism, it produces reactive oxygen species such as superoxide and hydrogen peroxide, which lead to propagation of free radicals, damaging cells and contributing to disease and, possibly, aging (senescence). The enzymes carrying out this metabolic pathway are also the target of many drugs and poisons that inhibit their activities.

It is the terminal process of cellular respiration in eukaryotes and accounts for high ATP yield.

Oxidative phosphorylation works by using energy-releasing chemical reactions to drive energy-requiring reactions: The two sets of reactions are said to be "coupled". This means one cannot occur without the other. The chain of redox reactions driving the flow of electrons through the electron transport chain, from electron donors such as NADH to electron acceptors such as oxygen and hydrogen (protons), is an exergonic process – it releases energy, whereas the synthesis of ATP is an endergonic process, which requires an input of energy. Both the electron transport chain and the ATP synthase are embedded in a membrane, and energy is transferred from the electron transport chain to the ATP synthase by movements of protons across this membrane, in a process called "chemiosmosis". A current of protons is driven from the negative N-side of the membrane to the positive P-side through the proton-pumping enzymes of the electron transport chain. The movement of protons creates an electrochemical gradient across the membrane, which is often called the "proton-motive force". It has two components: a difference in proton concentration (a H gradient, ΔpH) and a difference in electric potential, with the N-side having a negative charge.

ATP synthase releases this stored energy by completing the circuit and allowing protons to flow down the electrochemical gradient, back to the N-side of the membrane. The electrochemical gradient drives the rotation of part of the enzyme's structure and couples this motion to the synthesis of ATP.

The two components of the proton-motive force are thermodynamically equivalent: In mitochondria, the largest part of energy is provided by the potential; in alkaliphile bacteria the electrical energy even has to compensate for a counteracting inverse pH difference. Inversely, chloroplasts operate mainly on ΔpH. However, they also require a small membrane potential for the kinetics of ATP synthesis. In the case of the fusobacterium "Propionigenium modestum" it drives the counter-rotation of subunits a and c of the F motor of ATP synthase.

The amount of energy released by oxidative phosphorylation is high, compared with the amount produced by anaerobic fermentation, due to the high energy of O. Glycolysis produces only 2 ATP molecules, but somewhere between 30 and 36 ATPs are produced by the oxidative phosphorylation of the 10 NADH and 2 succinate molecules made by converting one molecule of glucose to carbon dioxide and water, while each cycle of beta oxidation of a fatty acid yields about 14 ATPs. These ATP yields are theoretical maximum values; in practice, some protons leak across the membrane, lowering the yield of ATP.

The electron transport chain carries both protons and electrons, passing electrons from donors to acceptors, and transporting protons across a membrane. These processes use both soluble and protein-bound transfer molecules. In mitochondria, electrons are transferred within the intermembrane space by the water-soluble electron transfer protein cytochrome c. This carries only electrons, and these are transferred by the reduction and oxidation of an iron atom that the protein holds within a heme group in its structure. Cytochrome c is also found in some bacteria, where it is located within the periplasmic space.

Within the inner mitochondrial membrane, the lipid-soluble electron carrier coenzyme Q10 (Q) carries both electrons and protons by a redox cycle. This small benzoquinone molecule is very hydrophobic, so it diffuses freely within the membrane. When Q accepts two electrons and two protons, it becomes reduced to the "ubiquinol" form (QH); when QH releases two electrons and two protons, it becomes oxidized back to the "ubiquinone" (Q) form. As a result, if two enzymes are arranged so that Q is reduced on one side of the membrane and QH oxidized on the other, ubiquinone will couple these reactions and shuttle protons across the membrane. Some bacterial electron transport chains use different quinones, such as menaquinone, in addition to ubiquinone.

Within proteins, electrons are transferred between flavin cofactors, iron–sulfur clusters, and cytochromes. There are several types of iron–sulfur cluster. The simplest kind found in the electron transfer chain consists of two iron atoms joined by two atoms of inorganic sulfur; these are called [2Fe–2S] clusters. The second kind, called [4Fe–4S], contains a cube of four iron atoms and four sulfur atoms. Each iron atom in these clusters is coordinated by an additional amino acid, usually by the sulfur atom of cysteine. Metal ion cofactors undergo redox reactions without binding or releasing protons, so in the electron transport chain they serve solely to transport electrons through proteins. Electrons move quite long distances through proteins by hopping along chains of these cofactors. This occurs by quantum tunnelling, which is rapid over distances of less than 1.4 m.

Many catabolic biochemical processes, such as glycolysis, the citric acid cycle, and beta oxidation, produce the reduced cofactor NADH. This coenzyme contains electrons that have a high transfer potential; in other words, they will release a large amount of energy upon oxidation. However, the cell does not release this energy all at once, as this would be an uncontrollable reaction. Instead, the electrons are removed from NADH and passed to oxygen through a series of enzymes that each release a small amount of the energy. This set of enzymes, consisting of complexes I through IV, is called the electron transport chain and is found in the inner membrane of the mitochondrion. Succinate is also oxidized by the electron transport chain, but feeds into the pathway at a different point.

In eukaryotes, the enzymes in this electron transport system use the energy released from O by NADH to pump protons across the inner membrane of the mitochondrion. This causes protons to build up in the intermembrane space, and generates an electrochemical gradient across the membrane. The energy stored in this potential is then used by ATP synthase to produce ATP. Oxidative phosphorylation in the eukaryotic mitochondrion is the best-understood example of this process. The mitochondrion is present in almost all eukaryotes, with the exception of anaerobic protozoa such as "Trichomonas vaginalis" that instead reduce protons to hydrogen in a remnant mitochondrion called a hydrogenosome.

NADH-coenzyme Q oxidoreductase, also known as "NADH dehydrogenase" or "complex I", is the first protein in the electron transport chain. Complex I is a giant enzyme with the mammalian complex I having 46 subunits and a molecular mass of about 1,000 kilodaltons (kDa). The structure is known in detail only from a bacterium; in most organisms the complex resembles a boot with a large "ball" poking out from the membrane into the mitochondrion. The genes that encode the individual proteins are contained in both the cell nucleus and the mitochondrial genome, as is the case for many enzymes present in the mitochondrion.

The reaction that is catalyzed by this enzyme is the two electron oxidation of NADH by coenzyme Q10 or "ubiquinone" (represented as Q in the equation below), a lipid-soluble quinone that is found in the mitochondrion membrane:

The start of the reaction, and indeed of the entire electron chain, is the binding of a NADH molecule to complex I and the donation of two electrons. The electrons enter complex I via a prosthetic group attached to the complex, flavin mononucleotide (FMN). The addition of electrons to FMN converts it to its reduced form, FMNH. The electrons are then transferred through a series of iron–sulfur clusters: the second kind of prosthetic group present in the complex. There are both [2Fe–2S] and [4Fe–4S] iron–sulfur clusters in complex I.

As the electrons pass through this complex, four protons are pumped from the matrix into the intermembrane space. Exactly how this occurs is unclear, but it seems to involve conformational changes in complex I that cause the protein to bind protons on the N-side of the membrane and release them on the P-side of the membrane. Finally, the electrons are transferred from the chain of iron–sulfur clusters to a ubiquinone molecule in the membrane. Reduction of ubiquinone also contributes to the generation of a proton gradient, as two protons are taken up from the matrix as it is reduced to ubiquinol (QH).

Succinate-Q oxidoreductase, also known as "complex II" or "succinate dehydrogenase", is a second entry point to the electron transport chain. It is unusual because it is the only enzyme that is part of both the citric acid cycle and the electron transport chain. Complex II consists of four protein subunits and contains a bound flavin adenine dinucleotide (FAD) cofactor, iron–sulfur clusters, and a heme group that does not participate in electron transfer to coenzyme Q, but is believed to be important in decreasing production of reactive oxygen species. It oxidizes succinate to fumarate and reduces ubiquinone. As this reaction releases less energy than the oxidation of NADH, complex II does not transport protons across the membrane and does not contribute to the proton gradient.

In some eukaryotes, such as the parasitic worm "Ascaris suum", an enzyme similar to complex II, fumarate reductase (menaquinol:fumarate
oxidoreductase, or QFR), operates in reverse to oxidize ubiquinol and reduce fumarate. This allows the worm to survive in the anaerobic environment of the large intestine, carrying out anaerobic oxidative phosphorylation with fumarate as the electron acceptor. Another unconventional function of complex II is seen in the malaria parasite "Plasmodium falciparum". Here, the reversed action of complex II as an oxidase is important in regenerating ubiquinol, which the parasite uses in an unusual form of pyrimidine biosynthesis.

Electron transfer flavoprotein-ubiquinone oxidoreductase (ETF-Q oxidoreductase), also known as "electron transferring-flavoprotein dehydrogenase", is a third entry point to the electron transport chain. It is an enzyme that accepts electrons from electron-transferring flavoprotein in the mitochondrial matrix, and uses these electrons to reduce ubiquinone. This enzyme contains a flavin and a [4Fe–4S] cluster, but, unlike the other respiratory complexes, it attaches to the surface of the membrane and does not cross the lipid bilayer.

In mammals, this metabolic pathway is important in beta oxidation of fatty acids and catabolism of amino acids and choline, as it accepts electrons from multiple acetyl-CoA dehydrogenases. In plants, ETF-Q oxidoreductase is also important in the metabolic responses that allow survival in extended periods of darkness.

Q-cytochrome c oxidoreductase is also known as "cytochrome c reductase", "cytochrome bc complex", or simply "complex III". In mammals, this enzyme is a dimer, with each subunit complex containing 11 protein subunits, an [2Fe-2S] iron–sulfur cluster and three cytochromes: one cytochrome c and two b cytochromes. A cytochrome is a kind of electron-transferring protein that contains at least one heme group. The iron atoms inside complex III's heme groups alternate between a reduced ferrous (+2) and oxidized ferric (+3) state as the electrons are transferred through the protein.

The reaction catalyzed by complex III is the oxidation of one molecule of ubiquinol and the reduction of two molecules of cytochrome c, a heme protein loosely associated with the mitochondrion. Unlike coenzyme Q, which carries two electrons, cytochrome c carries only one electron.

As only one of the electrons can be transferred from the QH donor to a cytochrome c acceptor at a time, the reaction mechanism of complex III is more elaborate than those of the other respiratory complexes, and occurs in two steps called the Q cycle. In the first step, the enzyme binds three substrates, first, QH, which is then oxidized, with one electron being passed to the second substrate, cytochrome c. The two protons released from QH pass into the intermembrane space. The third substrate is Q, which accepts the second electron from the QH and is reduced to Q, which is the ubisemiquinone free radical. The first two substrates are released, but this ubisemiquinone intermediate remains bound. In the second step, a second molecule of QH is bound and again passes its first electron to a cytochrome c acceptor. The second electron is passed to the bound ubisemiquinone, reducing it to QH as it gains two protons from the mitochondrial matrix. This QH is then released from the enzyme.

As coenzyme Q is reduced to ubiquinol on the inner side of the membrane and oxidized to ubiquinone on the other, a net transfer of protons across the membrane occurs, adding to the proton gradient. The rather complex two-step mechanism by which this occurs is important, as it increases the efficiency of proton transfer. If, instead of the Q cycle, one molecule of QH were used to directly reduce two molecules of cytochrome c, the efficiency would be halved, with only one proton transferred per cytochrome c reduced.

Cytochrome c oxidase, also known as "complex IV", is the final protein complex in the electron transport chain. The mammalian enzyme has an extremely complicated structure and contains 13 subunits, two heme groups, as well as multiple metal ion cofactors – in all, three atoms of copper, one of magnesium and one of zinc.

This enzyme mediates the final reaction in the electron transport chain and transfers electrons to oxygen and hydrogen (protons), while pumping protons across the membrane. The final electron acceptor oxygen, which provides most of the energy released in the electron transfer chain and is also called the "terminal electron acceptor", is reduced to water in this step, which releases half of all the energy in aerobic respiration. Both the direct pumping of protons and the consumption of matrix protons in the reduction of oxygen contribute to the proton gradient. The reaction catalyzed is the oxidation of cytochrome c and the reduction of oxygen:

Many eukaryotic organisms have electron transport chains that differ from the much-studied mammalian enzymes described above. For example, plants have alternative NADH oxidases, which oxidize NADH in the cytosol rather than in the mitochondrial matrix, and pass these electrons to the ubiquinone pool. These enzymes do not transport protons, and, therefore, reduce ubiquinone without altering the electrochemical gradient across the inner membrane.

Another example of a divergent electron transport chain is the "alternative oxidase", which is found in plants, as well as some fungi, protists, and possibly some animals. This enzyme transfers electrons directly from ubiquinol to oxygen.

The electron transport pathways produced by these alternative NADH and ubiquinone oxidases have lower ATP yields than the full pathway. The advantages produced by a shortened pathway are not entirely clear. However, the alternative oxidase is produced in response to stresses such as cold, reactive oxygen species, and infection by pathogens, as well as other factors that inhibit the full electron transport chain. Alternative pathways might, therefore, enhance an organisms' resistance to injury, by reducing oxidative stress.

The original model for how the respiratory chain complexes are organized was that they diffuse freely and independently in the mitochondrial membrane. However, recent data suggest that the complexes might form higher-order structures called supercomplexes or "respirasomes". In this model, the various complexes exist as organized sets of interacting enzymes. These associations might allow channeling of substrates between the various enzyme complexes, increasing the rate and efficiency of electron transfer. Within such mammalian supercomplexes, some components would be present in higher amounts than others, with some data suggesting a ratio between complexes I/II/III/IV and the ATP synthase of approximately 1:1:3:7:4. However, the debate over this supercomplex hypothesis is not completely resolved, as some data do not appear to fit with this model.

In contrast to the general similarity in structure and function of the electron transport chains in eukaryotes, bacteria and archaea possess a large variety of electron-transfer enzymes. These use an equally wide set of chemicals as substrates. In common with eukaryotes, prokaryotic electron transport uses the energy released from the oxidation of a substrate to pump ions across a membrane and generate an electrochemical gradient. In the bacteria, oxidative phosphorylation in "Escherichia coli" is understood in most detail, while archaeal systems are at present poorly understood.

The main difference between eukaryotic and prokaryotic oxidative phosphorylation is that bacteria and archaea use many different substances to donate or accept electrons. This allows prokaryotes to grow under a wide variety of environmental conditions. In "E. coli", for example, oxidative phosphorylation can be driven by a large number of pairs of reducing agents and oxidizing agents, which are listed below. The midpoint potential of a chemical measures how much energy is released when it is oxidized or reduced, with reducing agents having negative potentials and oxidizing agents positive potentials.

As shown above, "E. coli" can grow with reducing agents such as formate, hydrogen, or lactate as electron donors, and nitrate, DMSO, or oxygen as acceptors. The larger the difference in midpoint potential between an oxidizing and reducing agent, the more energy is released when they react. Out of these compounds, the succinate/fumarate pair is unusual, as its midpoint potential is close to zero. Succinate can therefore be oxidized to fumarate if a strong oxidizing agent such as oxygen is available, or fumarate can be reduced to succinate using a strong reducing agent such as formate. These alternative reactions are catalyzed by succinate dehydrogenase and fumarate reductase, respectively.

Some prokaryotes use redox pairs that have only a small difference in midpoint potential. For example, nitrifying bacteria such as "Nitrobacter" oxidize nitrite to nitrate, donating the electrons to oxygen. The small amount of energy released in this reaction is enough to pump protons and generate ATP, but not enough to produce NADH or NADPH directly for use in anabolism. This problem is solved by using a nitrite oxidoreductase to produce enough proton-motive force to run part of the electron transport chain in reverse, causing complex I to generate NADH.

Prokaryotes control their use of these electron donors and acceptors by varying which enzymes are produced, in response to environmental conditions. This flexibility is possible because different oxidases and reductases use the same ubiquinone pool. This allows many combinations of enzymes to function together, linked by the common ubiquinol intermediate. These respiratory chains therefore have a modular design, with easily interchangeable sets of enzyme systems.

In addition to this metabolic diversity, prokaryotes also possess a range of isozymes – different enzymes that catalyze the same reaction. For example, in "E. coli", there are two different types of ubiquinol oxidase using oxygen as an electron acceptor. Under highly aerobic conditions, the cell uses an oxidase with a low affinity for oxygen that can transport two protons per electron. However, if levels of oxygen fall, they switch to an oxidase that transfers only one proton per electron, but has a high affinity for oxygen.

ATP synthase, also called "complex V", is the final enzyme in the oxidative phosphorylation pathway. This enzyme is found in all forms of life and functions in the same way in both prokaryotes and eukaryotes. The enzyme uses the energy stored in a proton gradient across a membrane to drive the synthesis of ATP from ADP and phosphate (P). Estimates of the number of protons required to synthesize one ATP have ranged from three to four, with some suggesting cells can vary this ratio, to suit different conditions.

This phosphorylation reaction is an equilibrium, which can be shifted by altering the proton-motive force. In the absence of a proton-motive force, the ATP synthase reaction will run from right to left, hydrolyzing ATP and pumping protons out of the matrix across the membrane. However, when the proton-motive force is high, the reaction is forced to run in the opposite direction; it proceeds from left to right, allowing protons to flow down their concentration gradient and turning ADP into ATP. Indeed, in the closely related vacuolar type H+-ATPases, the hydrolysis reaction is used to acidify cellular compartments, by pumping protons and hydrolysing ATP.

ATP synthase is a massive protein complex with a mushroom-like shape. The mammalian enzyme complex contains 16 subunits and has a mass of approximately 600 kilodaltons. The portion embedded within the membrane is called F and contains a ring of c subunits and the proton channel. The stalk and the ball-shaped headpiece is called F and is the site of ATP synthesis. The ball-shaped complex at the end of the F portion contains six proteins of two different kinds (three α subunits and three β subunits), whereas the "stalk" consists of one protein: the γ subunit, with the tip of the stalk extending into the ball of α and β subunits. Both the α and β subunits bind nucleotides, but only the β subunits catalyze the ATP synthesis reaction. Reaching along the side of the F portion and back into the membrane is a long rod-like subunit that anchors the α and β subunits into the base of the enzyme.

As protons cross the membrane through the channel in the base of ATP synthase, the F proton-driven motor rotates. Rotation might be caused by changes in the ionization of amino acids in the ring of c subunits causing electrostatic interactions that propel the ring of c subunits past the proton channel. This rotating ring in turn drives the rotation of the central axle (the γ subunit stalk) within the α and β subunits. The α and β subunits are prevented from rotating themselves by the side-arm, which acts as a stator. This movement of the tip of the γ subunit within the ball of α and β subunits provides the energy for the active sites in the β subunits to undergo a cycle of movements that produces and then releases ATP.

This ATP synthesis reaction is called the "binding change mechanism" and involves the active site of a β subunit cycling between three states. In the "open" state, ADP and phosphate enter the active site (shown in brown in the diagram). The protein then closes up around the molecules and binds them loosely – the "loose" state (shown in red). The enzyme then changes shape again and forces these molecules together, with the active site in the resulting "tight" state (shown in pink) binding the newly produced ATP molecule with very high affinity. Finally, the active site cycles back to the open state, releasing ATP and binding more ADP and phosphate, ready for the next cycle.

In some bacteria and archaea, ATP synthesis is driven by the movement of sodium ions through the cell membrane, rather than the movement of protons. Archaea such as "Methanococcus" also contain the AA synthase, a form of the enzyme that contains additional proteins with little similarity in sequence to other bacterial and eukaryotic ATP synthase subunits. It is possible that, in some species, the AA form of the enzyme is a specialized sodium-driven ATP synthase, but this might not be true in all cases.

The energy released in oxidative phosphorylation can mostly be attributed to O with its relatively weak double bond. The transport of electrons from redox pair NAD/ NADH to the final redox pair 1/2 O/ HO can be summarized as 

1/2 O + NADH + H → HO + NAD 

The potential difference between these two redox pairs is 1.14 volt, which is equivalent to -52 kcal/mol or -2600 kJ per 6 mol of O.

When one NADH is oxidized through the electron transfer chain, three ATPs are produced, which is equivalent to 7.3 kcal/mol x 3 = 21.9 kcal/mol.

The conservation of the energy can be calculated by the following formula

Efficiency = (21.9 x 100%) / 52 = 42%

So we can conclude that when NADH is oxidized, about 42% of energy is conserved in the form of three ATPs and the remaining (58%) energy is lost as heat (unless the chemical energy of ATP under physiological conditions was underestimated).

Molecular oxygen is an ideal terminal electron acceptor because it is a strong oxidizing agent. The reduction of oxygen does involve potentially harmful intermediates. Although the transfer of four electrons and four protons reduces oxygen to water, which is harmless, transfer of one or two electrons produces superoxide or peroxide anions, which are dangerously reactive.

These reactive oxygen species and their reaction products, such as the hydroxyl radical, are very harmful to cells, as they oxidize proteins and cause mutations in DNA. This cellular damage might contribute to disease and is proposed as one cause of aging.

The cytochrome c oxidase complex is highly efficient at reducing oxygen to water, and it releases very few partly reduced intermediates; however small amounts of superoxide anion and peroxide are produced by the electron transport chain. Particularly important is the reduction of coenzyme Q in complex III, as a highly reactive ubisemiquinone free radical is formed as an intermediate in the Q cycle. This unstable species can lead to electron "leakage" when electrons transfer directly to oxygen, forming superoxide. As the production of reactive oxygen species by these proton-pumping complexes is greatest at high membrane potentials, it has been proposed that mitochondria regulate their activity to maintain the membrane potential within a narrow range that balances ATP production against oxidant generation. For instance, oxidants can activate uncoupling proteins that reduce membrane potential.

To counteract these reactive oxygen species, cells contain numerous antioxidant systems, including antioxidant vitamins such as vitamin C and vitamin E, and antioxidant enzymes such as superoxide dismutase, catalase, and peroxidases, which detoxify the reactive species, limiting damage to the cell.

As oxygen is fundamental for oxidative phosphorylation, a shortage in O level likely alters ATP production rates. However, proton motive force and ATP production can be maintained by intracellular acidosis. Cytosolic protons that have accumulated with ATP hydrolysis and lactic acidosis can freely diffuse across the mitochondrial outer-membrane and acidify the inter-membrane space, hence directly contributing to the proton motive force and ATP production.

There are several well-known drugs and toxins that inhibit oxidative phosphorylation. Although any one of these toxins inhibits only one enzyme in the electron transport chain, inhibition of any step in this process will halt the rest of the process. For example, if oligomycin inhibits ATP synthase, protons cannot pass back into the mitochondrion. As a result, the proton pumps are unable to operate, as the gradient becomes too strong for them to overcome. NADH is then no longer oxidized and the citric acid cycle ceases to operate because the concentration of NAD falls below the concentration that these enzymes can use.

Many site specific inhibitors of ETC have contributed in the present knowledge of the mitochondrial respiration. Synthesis of ATP is also depend on the ETC, so all site specific inhibitors also inhibit ATP formation. Fish poison rotenone, barbitutate drug amytal and antibiotic piercidin A inhibit NADH and coenzyme Q. 

Carbon monoxide, cyanide, hydrogen sulphide and azide effectively inhibit cytochrome oxidase. Carbon monoxide reacts with reduced form of the cytochrome while cyanide and azide react with oxidised form. An antibiotic - antimycin A British antile wisite- an antidote used against war-gas re the two important inhibitors of the site between cytochrome B and C1. 

Not all inhibitors of oxidative phosphorylation are toxins. In brown adipose tissue, regulated proton channels called uncoupling proteins can uncouple respiration from ATP synthesis. This rapid respiration produces heat, and is particularly important as a way of maintaining body temperature for hibernating animals, although these proteins may also have a more general function in cells' responses to stress.

The field of oxidative phosphorylation began with the report in 1906 by Arthur Harden of a vital role for phosphate in cellular fermentation, but initially only sugar phosphates were known to be involved. However, in the early 1940s, the link between the oxidation of sugars and the generation of ATP was firmly established by Herman Kalckar, confirming the central role of ATP in energy transfer that had been proposed by Fritz Albert Lipmann in 1941. Later, in 1949, Morris Friedkin and Albert L. Lehninger proved that the coenzyme NADH linked metabolic pathways such as the citric acid cycle and the synthesis of ATP. The term "oxidative phosphorylation" was coined by in 1939.

For another twenty years, the mechanism by which ATP is generated remained mysterious, with scientists searching for an elusive "high-energy intermediate" that would link oxidation and phosphorylation reactions. This puzzle was solved by Peter D. Mitchell with the publication of the chemiosmotic theory in 1961. At first, this proposal was highly controversial, but it was slowly accepted and Mitchell was awarded a Nobel prize in 1978. Subsequent research concentrated on purifying and characterizing the enzymes involved, with major contributions being made by David E. Green on the complexes of the electron-transport chain, as well as Efraim Racker on the ATP synthase. A critical step towards solving the mechanism of the ATP synthase was provided by Paul D. Boyer, by his development in 1973 of the "binding change" mechanism, followed by his radical proposal of rotational catalysis in 1982. More recent work has included structural studies on the enzymes involved in oxidative phosphorylation by John E. Walker, with Walker and Boyer being awarded a Nobel Prize in 1997.







</doc>
<doc id="22775" url="https://en.wikipedia.org/wiki?curid=22775" title="Old fashioned (cocktail)">
Old fashioned (cocktail)

The old fashioned is a cocktail made by muddling sugar with bitters and water, adding whiskey or, less commonly, brandy, and garnishing with orange slice or zest and a cocktail cherry. It is traditionally served in an old fashioned glass (also known as rocks glass), which predated the cocktail.

Developed during the 19th century and given its name in the 1880s, it is an IBA Official Cocktail. It is also one of six basic drinks listed in David A. Embury's "The Fine Art of Mixing Drinks".

An old fashioned was one of the simpler and earlier versions of cocktails, before the development of advanced bartending techniques and recipes in the later part of the 19th century. The first documented definition of the word "cocktail" was in response to a reader's letter asking to define the word in the May 6, 1806, issue of "The Balance and Columbian Repository" in Hudson, New York. In the May 13, 1806, issue, the paper's editor wrote that it was a potent concoction of spirits, bitters, water, and sugar; it was also referred to at the time as a bittered sling and is essentially the recipe for an old fashioned.
J.E. Alexander describes the cocktail similarly in 1833, as he encountered it in New York City, as being rum, gin, or brandy, significant water, bitters, and sugar, though he includes a nutmeg garnish as well.

By the 1860s, it was common for orange curaçao, absinthe, and other liqueurs to be added to the cocktail. As cocktails became more complex, drinkers used to simpler cocktails began to ask bartenders for something akin to the pre 1850's drinks. The original concoction, albeit in different proportions, came back into vogue, and was referred to as "old-fashioned". The most popular of the in-vogue "old-fashioned" cocktails were made with whiskey, according to a Chicago barman, quoted in the "Chicago Daily Tribune" in 1882, with rye being more popular than Bourbon. The recipe he describes is a similar combination of spirits, bitters, water and sugar of seventy-six years earlier.

The Pendennis Club, a gentlemen's club founded in 1881 in Louisville, Kentucky, claims the old fashioned cocktail was invented there. The recipe was said to have been invented by a bartender at that club in honor of Colonel James E. Pepper, a prominent bourbon distiller, who brought it to the Waldorf-Astoria Hotel bar in New York City. Cocktail critic David Wonderich finds this origin story unlikely however, as the first mention in print of "old fashioned cocktails" was in the "Chicago Daily Tribune" in February 1880, before the Pendennis Club was opened; this in addition to the fact that the old fashioned was simply a re-packaging of a drink that had long existed. 

With its conception rooted in the city's history, in 2015 the city of Louisville named the old fashioned as its official cocktail. Each year, during the first two weeks of June, Louisville celebrates "Old Fashioned Fortnight" which encompasses bourbon events, cocktail specials and National Bourbon Day which is always celebrated on June 14.

George Kappeler provides several of the earliest published recipes for old fashioned cocktails in his 1895 book. Recipes are given for whiskey, brandy, Holland gin, and Old Tom gin. The whiskey old fashioned recipe specifies the following (with a jigger being ):

By the 1860s, as illustrated by Jerry Thomas' 1862 book, basic cocktail recipes included Curaçao, or other liqueurs. These liqueurs were not mentioned in the early 19th century descriptions, nor the "Chicago Daily Tribune" descriptions of the "old fashioned" cocktails of the early 1880s; they were absent from Kappeler's old fashioned recipes as well. The differences of the old fashioned cocktail recipes from the cocktail recipes of the late 19th Century are mainly preparation method, the use of sugar and water in lieu of simple or gomme syrup, and the absence of additional liqueurs. These old fashioned cocktail recipes are literally for cocktails done the old-fashioned way.

A book by David Embury published in 1948 provides a slight variation, specifying 12 parts American whiskey, 1 part simple syrup, 1-3 dashes Angostura bitters, a twist of lemon peel over the top, and serve garnished with the lemon peel. Two additional recipes from the 1900s vary in the precise ingredients, but omit the cherry which was introduced after 1930 as well as the soda water which the occasional recipe calls for. Orange bitters were a popular ingredient in the late 19th century. 

The original old fashioned recipe would have showcased the whiskey available in America in the 19th century: Irish, Bourbon or rye whiskey. But in some regions, especially Wisconsin, brandy is substituted for whiskey (sometimes called a brandy old fashioned). Eventually the use of other spirits became common, such as a gin recipe becoming popularized in the late 1940s.

Common garnishes for an old fashioned include an orange slice or a maraschino cherry or both, although these modifications came around 1930, some time after the original recipe was invented. While some recipes began making sparse use of the orange zest for flavor, the practice of muddling orange and other fruit gained prevalence as late as the 1990s.

Some modern variants have greatly sweetened the old fashioned, e.g. by adding blood orange soda to make a fizzy old fashioned, or muddled strawberries to make a strawberry old fashioned.

Modern versions may also include elaborately carved ice; though cocktail critic David Wonderich notes that this, along with essentially all other adornments or additions, goes against the simple spirit of the old fashioned.

The old fashioned is the cocktail of choice of Don Draper, the lead character on the "Mad Men" television series, set in the 1960s. The use of the drink in the series coincided with a renewed interest in this and other classic cocktails in the 2000s.

In the movie "It's a Mad, Mad, Mad, Mad World" (1963), pilot Tyler Fitzgerald (Jim Backus) directs passenger Dingy Bell (Mickey Rooney) to the aircraft's bar to "make us some old fashioneds." Annoyed by suggestions that he should limit drinking while piloting an airplane, and finding Bell's old fashioneds too sweet, Fitzgerald turns the controls over to Bell's sidekick Benjy Benjamin (Buddy Hackett) and retires to the back of the plane to "make some old fashioneds the old fashioned way, the way dear old dad used to." When Benjamin asks what if something happens, Fitzgerald replies, "What could happen to an old fashioned?"

In the television series "M*A*S*H", character Margaret Houlihan frequently orders an old fashioned, "without the fruit", while in the Officers' Club.

In the movie "Crazy, Stupid, Love", the old fashioned is the preferred cocktail of pickup artist Jacob Palmer, and he is shown drinking it both in the bar and at home.





</doc>
<doc id="22776" url="https://en.wikipedia.org/wiki?curid=22776" title="Omnipotence">
Omnipotence

Omnipotence is the quality of having unlimited power and potential. Monotheistic religions generally attribute omnipotence only to the deity of their faith. In the monotheistic philosophies of Abrahamic religions, omnipotence is often listed as one of a deity's characteristics among many, including omniscience, omnipresence, and omnibenevolence. The presence of all these properties in a single entity has given rise to considerable theological debate, prominently including the problem of theodicy, the question of why such a deity would permit the manifestation of evil. It is accepted in philosophy and science that omnipotence can never be effectively understood.

The word "omnipotence" derives from the Latin term ""omni potens"", meaning "all-powerful" or "all-potent".

The term omnipotent has been used to connote a number of different positions. These positions include, but are not limited to, the following:

Thomas Aquinas acknowledged difficulty in comprehending the deity's power: "All confess that God is omnipotent; but it seems difficult to explain in what His omnipotence precisely consists: for there may be doubt as to the precise meaning of the word 'all' when we say that God can do all things. If, however, we consider the matter aright, since power is said in reference to possible things, this phrase, 'God can do all things,' is rightly understood to mean that God can do all things that are possible; and for this reason He is said to be omnipotent." In the scholastic understanding, omnipotence is generally understood to be compatible with certain limitations or restrictions. A proposition that is necessarily true is one whose negation is self-contradictory.
Aquinas explains that:
Omnipotence is all-sufficient power. The adaptation of means to ends in the universe does not argue, as J. S. Mill would have it, that the power of the designer is limited, but only that God has willed to manifest his glory by a world so constituted rather than by another. Indeed, the production of secondary causes, capable of accomplishing certain effects, requires greater power than the direct accomplishment of these same effects. On the other hand, even though no creature existed, God's power would not be barren, for "creatures are not an end to God." Regarding the Deity's power, medieval theologians contended that there are certain things that even an omnipotent deity cannot do. The statement "a deity can do anything" is only sensible with an assumed suppressed clause, "that implies the perfection of true power". This standard scholastic answer allows that acts of creatures such as walking can be performed by humans but not by a deity. Rather than an advantage in power, human acts such as walking, sitting, or giving birth were possible only because of a "defect" in human power. The capacity to sin, for example, is not a power but a defect or infirmity. In response to questions of a deity performing impossibilities, e.g. making square circles, Aquinas says that "everything that does not imply a contradiction in terms, is numbered amongst those possible things, in respect of which God is called omnipotent: whereas whatever implies contradiction does not come within the scope of divine omnipotence, because it cannot have the aspect of possibility. Hence it is better to say that such things cannot be done, than that God cannot do them. Nor is this contrary to the word of the angel, saying: 'No word shall be impossible with God.' For whatever implies a contradiction cannot be a word, because no intellect can possibly conceive such a thing."

In recent times, C. S. Lewis has adopted a scholastic position in the course of his work "The Problem of Pain". Lewis follows Aquinas' view on contradiction:
Early Freudianism saw a feeling of omnipotence as intrinsic to early childhood. 'As Freud and Ferenczi have shown, the child lives in a sort of megalomania for a long period...the "fiction of omnipotence"'. At birth 'the baby "is" everything "as far as he knows" - "all powerful"...every step he takes towards establishing his own limits and boundaries will be painful because he'll have to lose this original God-like feeling of omnipotence'.

Freud considered that in a neurotic 'the "omnipotence" which he ascribed to his thoughts and feelings...is a frank acknowledgement of a relic of the old megalomania of infancy'. In some narcissists, the 'period of primary narcissism which subjectively did not need any objects and was entirely independent...may be retained or regressively regained..."omnipotent" behavior'.

D. W. Winnicott took a more positive view of a belief in early omnipotence, seeing it as essential to the child's well-being; and "good-enough" mothering as essential to enable the child to 'cope with the immense shock of loss of omnipotence' - as opposed to whatever 'prematurely forces it out of its narcissistic universe'.

Some monotheists reject the view that a deity is or could be omnipotent, or take the view that, by choosing to create creatures with freewill, a deity has chosen to limit divine omnipotence. In Conservative and Reform Judaism, and some movements within Protestant Christianity, including open theism, deities are said to act in the world through persuasion, and not by coercion (this is a matter of choice—a deity could act miraculously, and perhaps on occasion does so—while for process theism it is a matter of necessity—creatures have inherent powers that a deity cannot, even in principle, override). Deities are manifested in the world through inspiration and the creation of possibility, not necessarily by miracles or violations of the laws of nature.

The rejection of omnipotence often follows from either philosophical or scriptural considerations, discussed below.

Process theology rejects unlimited omnipotence on a philosophical basis, arguing that omnipotence as classically understood would be less than perfect, and is therefore incompatible with the idea of a perfect deity. The idea is grounded in Plato's oft-overlooked statement that "being is power."
From this premise, Charles Hartshorne argues further that:
The argument can be stated as follows:

For example, though someone might control a lump of jelly-pudding almost completely, the inability of that pudding to stage any resistance renders that person's power rather unimpressive. Power can only be said to be great if it is over something that has defenses and its own agenda. If a deity's power is to be great, it must therefore be over beings that have at least some of their own defenses and agenda. Thus, if a deity does not have absolute power, it must therefore embody some of the characteristics of power, and some of the characteristics of persuasion. This view is known as dipolar theism.

The most popular works espousing this point are from Harold Kushner (in Judaism). The need for a modified view of omnipotence was also articulated by Alfred North Whitehead in the early 20th century and expanded upon by the aforementioned philosopher Charles Hartshorne. Hartshorne proceeded within the context of the theological system known as process theology.

In the Authorized King James Version of the Bible, as well as several other versions, in Revelation 19:6 it is stated "...the Lord God omnipotent reigneth" (the original Greek word is παντοκράτωρ, "all-mighty").

All the above stated claims of power are each based on scriptural grounds and upon empirical human perception. This perception is limited to our senses. The power of a deity is related to its existence. There are however other ways of perception like: reason, intuition, revelation, divine inspiration, religious experience, mystical states, and historical testimony.

According to the Hindu philosophy the essence of God or Brahman can never be understood or known since Brahman is beyond both existence and non-existence, transcending and including time, causation and space, and thus can never be known in the same material sense as one traditionally 'understands' a given concept or object.

So presuming there is a god-like entity consciently taking actions, we cannot comprehend the limits of a deity's powers.

Since the current laws of physics are only known to be valid in this universe, it is possible that the laws of physics are different in parallel universes, giving a God-like entity more power. If the number of universes is unlimited, then the power of a certain God-like entity is also unlimited, since the laws of physics may be different in other universes, and accordingly making this entity omnipotent. Unfortunately concerning a multiverse there is a lack of empirical correlation. To the extreme there are theories about realms beyond this multiverse (Nirvana, Chaos, Nothingness).

Also trying to develop a theory to explain, assign or reject omnipotence on grounds of logic has little merit, since being omnipotent, in a Cartesian sense, would mean the omnipotent being is above logic, a view supported by René Descartes. He issues this idea in his Meditations on First Philosophy. This view is called universal possibilism.

Allowing assumption that a deity exists, further debate may be provoked that said deity is consciously taking actions. It could be concluded from an emanationism point of view, that all actions and creations by a deity are simply flows of divine energy (the flowing Tao in conjunction with qi is often seen as a river; Dharma (Buddhism) the law of nature discovered by Buddha has no beginning or end.)
Pantheism and pandeism see the universe/multiverse itself as God (or, at least, the current state of God), while panentheism sees the universe/multiverse as 'the body of God', making 'God' everybody and everything. So if one does something, actually 'God' is doing it. We are 'God's' means according to this view.

In the Taoist religious or philosophical tradition, the Tao is in some ways equivalent to a deity or the logos. The Tao is understood to have inexhaustible power, yet that power is simply another aspect of its weakness.





</doc>
<doc id="22780" url="https://en.wikipedia.org/wiki?curid=22780" title="Octopus">
Octopus

The octopus (plural octopuses) is a soft-bodied, eight-limbed mollusc of the order Octopoda (, ). Around 300 species are recognised, and the order is grouped within the class Cephalopoda with squids, cuttlefish, and nautiloids. Like other cephalopods, the octopus is bilaterally symmetric with two eyes and a beak, with its mouth at the center point of the eight limbs. The soft body can rapidly alter its shape, enabling octopuses to squeeze through small gaps. They trail their eight appendages behind them as they swim. The siphon is used both for respiration and for locomotion, by expelling a jet of water. Octopuses have a complex nervous system and excellent sight, and are among the most intelligent and behaviourally diverse of all invertebrates.

Octopuses inhabit various regions of the ocean, including coral reefs, pelagic waters, and the seabed; some live in the intertidal zone and others at abyssal depths. Most species grow quickly, mature early, and are short-lived. In most species, the male uses a specially adapted arm to deliver a bundle of sperm directly into the female's mantle cavity, after which he becomes senescent and dies, while the female deposits fertilised eggs in a den and cares for them until they hatch, after which she also dies. Strategies to defend themselves against predators include the expulsion of ink, the use of camouflage and threat displays, the ability to jet quickly through the water and hide, and even deceit. All octopuses are venomous, but only the blue-ringed octopuses are known to be deadly to humans.

Octopuses appear in mythology as sea monsters like the Kraken of Norway and the Akkorokamui of the Ainu, and probably the Gorgon of ancient Greece. A battle with an octopus appears in Victor Hugo's book "Toilers of the Sea", inspiring other works such as Ian Fleming's "Octopussy". Octopuses appear in Japanese erotic art, "shunga". They are eaten and considered a delicacy by humans in many parts of the world, especially the Mediterranean and the Asian seas.

The scientific Latin term "octopus" was derived from Ancient Greek ὀκτώπους, a compound form of ὀκτώ ("oktō", "eight") and πούς ("pous", "foot"), itself a variant form of ὀκτάπους, a word used for example by Alexander of Tralles (c. 525–605) for the common octopus. The standard pluralised form of "octopus" in English is "octopuses"; the Ancient Greek plural ὀκτώποδες, "octopodes" (), has also been used historically. The alternative plural "octopi" is considered grammatically incorrect because it wrongly assumes that "octopus" is a Latin second declension "-us" noun or adjective when, in either Greek or Latin, it is a third declension noun. 

"Fowler's Modern English Usage" states that the only acceptable plural in English is "octopuses", that "octopi" is misconceived, and "octopodes" pedantic; the latter is nonetheless used frequently enough to be acknowledged by the descriptivist "Merriam-Webster 11th Collegiate Dictionary" and "Webster's New World College Dictionary". The "Oxford English Dictionary" lists "octopuses", "octopi", and "octopodes", in that order, reflecting frequency of use, calling "octopodes" rare and noting that "octopi" is based on a misunderstanding. The "New Oxford American Dictionary" (3rd Edition, 2010) lists "octopuses" as the only acceptable pluralisation, and indicates that "octopodes" is still occasionally used, but that "octopi" is incorrect.

The giant Pacific octopus "(Enteroctopus dofleini)" is often cited as the largest known octopus species. Adults usually weigh around 15 kg (33 lb), with an arm span of up to 4.3 m (14 ft). The largest specimen of this species to be scientifically documented was an animal with a live mass of 71 kg (156.5 lb). Much larger sizes have been claimed for the giant Pacific octopus: one specimen was recorded as 272 kg (600 lb) with an arm span of 9 m (30 ft).
A carcass of the seven-arm octopus, "Haliphron atlanticus", weighed 61 kg (134 lb) and was estimated to have had a live mass of 75 kg (165 lb). The smallest species is "Octopus wolfi", which is around and weighs less than .

The octopus is bilaterally symmetrical along its dorso-ventral axis; the head and foot are at one end of an elongated body and function as the anterior (front) of the animal. The head includes the mouth and brain. The foot has evolved into a set of flexible, prehensile appendages, known as "arms", that surround the mouth and are attached to each other near their base by a webbed structure. The arms can be described based on side and sequence position (such as L1, R1, L2, R2) and divided into four pairs. The two rear appendages are generally used to walk on the sea floor, while the other six are used to forage for food; hence some biologists refer to the animals as having six "arms" and two "legs". The bulbous and hollow mantle is fused to the back of the head and is known as the visceral hump; it contains most of the vital organs. The mantle cavity has muscular walls and contains the gills; it is connected to the exterior by a funnel or siphon. The mouth of an octopus, located underneath the arms, has a sharp hard beak.

The skin consists of a thin outer epidermis with mucous cells and sensory cells, and a connective tissue dermis consisting largely of collagen fibres and various cells allowing colour change. Most of the body is made of soft tissue allowing it to lengthen, contract, and contort itself. The octopus can squeeze through tiny gaps; even the larger species can pass through an opening close to in diameter. Lacking skeletal support, the arms work as muscular hydrostats and contain longitudinal, transverse and circular muscles around a central axial nerve. They can extend and contract, twist to left or right, bend at any place in any direction or be held rigid.

The interior surfaces of the arms are covered with circular, adhesive suckers. The suckers allow the octopus to anchor itself or to manipulate objects. Each sucker is usually circular and bowl-like and has two distinct parts: an outer shallow cavity called an infundibulum and a central hollow cavity called an acetabulum, both of which are thick muscles covered in a protective chitinous cuticle. When a sucker attaches to a surface, the orifice between the two structures is sealed. The infundibulum provides adhesion while the acetabulum remains free, and muscle contractions allow for attachment and detachment.
The eyes of the octopus are large and are at the top of the head. They are similar in structure to those of a fish and are enclosed in a cartilaginous capsule fused to the cranium. The cornea is formed from a translucent epidermal layer and the slit-shaped pupil forms a hole in the iris and lies just behind. The lens is suspended behind the pupil and photoreceptive retinal cells cover the back of the eye. The pupil can be adjusted in size and a retinal pigment screens incident light in bright conditions.

Some species differ in form from the typical octopus body shape. Basal species, the Cirrina, have stout gelatinous bodies with webbing that reaches near the tip of their arms, and two large fins above the eyes, supported by an internal shell. Fleshy papillae or cirri are found along the bottom of the arms, and the eyes are more developed.

Octopuses have a closed circulatory system, in which the blood remains inside blood vessels. Octopuses have three hearts; a systemic heart that circulates blood around the body and two branchial hearts that pump it through each of the two gills. The systemic heart is inactive when the animal is swimming and thus it tires quickly and prefers to crawl.
Octopus blood contains the copper-rich protein haemocyanin to transport oxygen. This makes the blood very viscous and it requires considerable pressure to pump it around the body; octopuses' blood pressures can exceed . In cold conditions with low oxygen levels, haemocyanin transports oxygen more efficiently than haemoglobin. The haemocyanin is dissolved in the plasma instead of being carried within blood cells, and gives the blood a bluish colour.

The systemic heart has muscular contractile walls and consists of a single ventricle and two atria, one for each side of the body. The blood vessels consist of arteries, capillaries and veins and are lined with a cellular endothelium which is quite unlike that of most other invertebrates. The blood circulates through the aorta and capillary system, to the vena cavae, after which the blood is pumped through the gills by the auxiliary hearts and back to the main heart. Much of the venous system is contractile, which helps circulate the blood.

Respiration involves drawing water into the mantle cavity through an aperture, passing it through the gills, and expelling it through the siphon. The ingress of water is achieved by contraction of radial muscles in the mantle wall, and flapper valves shut when strong circular muscles force the water out through the siphon. Extensive connective tissue lattices support the respiratory muscles and allow them to expand the respiratory chamber. The lamella structure of the gills allows for a high oxygen uptake, up to 65% in water at . Water flow over the gills correlates with locomotion, and an octopus can propel its body when it expels water out of its siphon.

The thin skin of the octopus absorbs additional oxygen. When resting, around 41% of an octopus's oxygen absorption is through the skin. This decreases to 33% when it swims, as more water flows over the gills; skin oxygen uptake also increases. When it is resting after a meal, absorption through the skin can drop to 3% of its total oxygen uptake.

The digestive system of the octopus begins with the buccal mass which consists of the mouth with its chitinous beak, the pharynx, radula and salivary glands. The radula is a spiked, muscular tongue-like organ with multiple rows of tiny teeth. Food is broken down and is forced into the oesophagus by two lateral extensions of the esophageal side walls in addition to the radula. From there it is transferred to the gastrointestinal tract, which is mostly suspended from the roof of the mantle cavity by numerous membranes. The tract consists of a crop, where the food is stored; a stomach, where food is ground down; a caecum where the now sludgy food is sorted into fluids and particles and which plays an important role in absorption; the digestive gland, where liver cells break down and absorb the fluid and become "brown bodies"; and the intestine, where the accumulated waste is turned into faecal ropes by secretions and blown out of the funnel via the rectum.

During osmoregulation, fluid is added to the pericardia of the branchial hearts. The octopus has two nephridia (equivalent to vertebrate kidneys) which are associated with the branchial hearts; these and their associated ducts connect the pericardial cavities with the mantle cavity. Before reaching the branchial heart, each branch of the vena cava expands to form renal appendages which are in direct contact with the thin-walled nephridium. The urine is first formed in the pericardial cavity, and is modified by excretion, chiefly of ammonia, and selective absorption from the renal appendages, as it is passed along the associated duct and through the nephridiopore into the mantle cavity.

The octopus (along with cuttlefish) has the highest brain-to-body mass ratios of all invertebrates; it is also greater than that of many vertebrates. It has a highly complex nervous system, only part of which is localised in its brain, which is contained in a cartilaginous capsule. Two-thirds of an octopus's neurons are found in the nerve cords of its arms, which show a variety of complex reflex actions that persist even when they have no input from the brain. Unlike vertebrates, the complex motor skills of octopuses are not organised in their brain via an internal somatotopic map of its body, instead using a nonsomatotopic system unique to large-brained invertebrates.

Like other cephalopods, octopuses can distinguish the polarisation of light. Colour vision appears to vary from species to species, for example being present in "O. aegina" but absent in "O. vulgaris". Researchers believe that opsins in the skin can sense different wavelengths of light and help the creatures choose a coloration that camouflages them, in addition to light input from the eyes. Other researchers hypothesise that cephalopod eyes in species which only have a single photoreceptor protein may use chromatic aberration to turn monochromatic vision into colour vision, though this sacrifices image quality. This would explain pupils shaped like the letter U, the letter W, or a dumbbell, as well as explaining the need for colourful mating displays.

Attached to the brain are two special organs called statocysts (sac-like structures containing a mineralised mass and sensitive hairs), that allow the octopus to sense the orientation of its body. They provide information on the position of the body relative to gravity and can detect angular acceleration. An autonomic response keeps the octopus's eyes oriented so that the pupil is always horizontal. Octopuses may also use the statocyst to hear sound. The common octopus can hear sounds between 400 Hz and 1000 Hz, and hears best at 600 Hz.

Octopuses also have an excellent sense of touch. The octopus's suction cups are equipped with chemoreceptors so the octopus can taste what it touches. Octopus arms do not become tangled or stuck to each other because the sensors recognise octopus skin and prevent self-attachment.

The arms contain tension sensors so the octopus knows whether its arms are stretched out, but this is not sufficient for the brain to determine the position of the octopus's body or arms. As a result, the octopus does not possess stereognosis; that is, it does not form a mental image of the overall shape of the object it is handling. It can detect local texture variations, but cannot integrate the information into a larger picture. The neurological autonomy of the arms means the octopus has great difficulty learning about the detailed effects of its motions. It has a poor proprioceptive sense, and it knows what exact motions were made only by observing the arms visually.

The ink sac of an octopus is located under the digestive gland. A gland attached to the sac produces the ink, and the sac stores it. The sac is close enough to the funnel for the octopus to shoot out the ink with a water jet. Before it leaves the funnel, the ink passes through glands which mix it with mucus, creating a thick, dark blob which allows the animal to escape from a predator. The main pigment in the ink is melanin, which gives it its black colour. Cirrate octopuses lack the ink sac.

Octopuses are gonochoric and have a single, posteriorly-located gonad which is associated with the coelom. The testis in males and the ovary in females bulges into the gonocoel and the gametes are released here. The gonocoel is connected by the gonoduct to the mantle cavity, which it enters at the gonopore. An optic gland creates hormones that cause the octopus to mature and age and stimulate gamete production. The gland may be triggered by environmental conditions such as temperature, light and nutrition, which thus control the timing of reproduction and lifespan.

When octopuses reproduce, the male uses a specialised arm called a hectocotylus to transfer spermatophores (packets of sperm) from the terminal organ of the reproductive tract (the cephalopod "penis") into the female's mantle cavity. The hectocotylus in benthic octopuses is usually the third right arm, which has a spoon-shaped depression and modified suckers near the tip. In most species, fertilisation occurs in the mantle cavity.

The reproduction of octopuses has been studied in only a few species. One such species is the giant Pacific octopus, in which courtship is accompanied, especially in the male, by changes in skin texture and colour. The male may cling to the top or side of the female or position himself beside her. There is some speculation that he may first use his hectocotylus to remove any spermatophore or sperm already present in the female. He picks up a spermatophore from his spermatophoric sac with the hectocotylus, inserts it into the female's mantle cavity, and deposits it in the correct location for the species, which in the giant Pacific octopus is the opening of the oviduct. Two spermatophores are transferred in this way; these are about one metre (yard) long, and the empty ends may protrude from the female's mantle. A complex hydraulic mechanism releases the sperm from the spermatophore, and it is stored internally by the female.

About forty days after mating, the female giant Pacific octopus attaches strings of small fertilised eggs (10,000 to 70,000 in total) to rocks in a crevice or under an overhang. Here she guards and cares for them for about five months (160 days) until they hatch. In colder waters, such as those off of Alaska, it may take as much as 10 months for the eggs to completely develop. The female aerates the eggs and keeps them clean; if left untended, many eggs will not hatch. She does not feed during this time and dies soon afterwards. Males become senescent and die a few weeks after mating.

The eggs have large yolks; cleavage (division) is superficial and a germinal disc develops at the pole. During gastrulation, the margins of this grow down and surround the yolk, forming a yolk sac, which eventually forms part of the gut. The dorsal side of the disc grows upwards and forms the embryo, with a shell gland on its dorsal surface, gills, mantle and eyes. The arms and funnel develop as part of the foot on the ventral side of the disc. The arms later migrate upwards, coming to form a ring around the funnel and mouth. The yolk is gradually absorbed as the embryo develops.

Most young octopuses hatch as paralarvae and are planktonic for weeks to months, depending on the species and water temperature. They feed on copepods, arthropod larvae and other zooplankton, eventually settling on the ocean floor and developing directly into adults with no distinct metamorphoses that are present in other groups of mollusc larvae. Octopus species that produce larger eggs – including the southern blue-ringed, Caribbean reef, California two-spot, "Eledone moschata" and deep sea octopuses – do not have a paralarval stage, but hatch as benthic animals similar to the adults.

In the argonaut (paper nautilus), the female secretes a fine, fluted, papery shell in which the eggs are deposited and in which she also resides while floating in mid-ocean. In this she broods the young, and it also serves as a buoyancy aid allowing her to adjust her depth. The male argonaut is minute by comparison and has no shell.

Octopuses have a relatively short life expectancy; some species live for as little as six months. The giant Pacific octopus, one of the two largest species of octopus, may live for as much as five years. Octopus lifespan is limited by reproduction: males can live for only a few months after mating, and females die shortly after their eggs hatch. The larger Pacific striped octopus is an exception, as it can reproduce multiple times over a life of around two years. Octopus reproductive organs mature due to the hormonal influence of the optic gland but result in the inactivation of their digestive glands, typically causing the octopus to die from starvation. Experimental removal of both optic glands after spawning was found to result in the cessation of broodiness, the resumption of feeding, increased growth, and greatly extended lifespans.

Octopuses live in every ocean, and different species have adapted to different marine habitats. As juveniles, common octopuses inhabit shallow tide pools. The Hawaiian day octopus ("Octopus cyanea") lives on coral reefs; argonauts drift in pelagic waters. "Abdopus aculeatus" mostly lives in near-shore seagrass beds. Some species are adapted to the cold, ocean depths. The spoon-armed octopus ("Bathypolypus arcticus") is found at depths of , and "Vulcanoctopus hydrothermalis" lives near hydrothermal vents at . The cirrate species are often free-swimming and live in deep-water habitats. Although several species are known to live at bathyal and abyssal depths, there is only a single indisputable record of an octopus in the hadal zone; a species of "Grimpoteuthis" (dumbo octopus) photographed at . No species are known to live in fresh water.

Most species are solitary when not mating, though a few are known to occur in high densities and with frequent interactions, signaling, mate defending and eviction of individuals from dens. This is likely the result of abundant food supplies combined with limited den sites. The larger Pacific striped octopus however is social, living in groups of up to 40 individuals that share dens. Octopuses hide in dens, which are typically crevices in rocky outcrops or other hard structures, though some species burrow into sand or mud. Octopuses are not territorial but generally remain in a home range; they may leave the area in search of food. They can use navigation skills to return to a den without having to retrace their outward route. They are not known to be migratory.

Octopuses bring captured prey back to the den where they can eat it safely. Sometimes the octopus catches more prey than it can eat, and the den is often surrounded by a midden of dead and uneaten food items. Other creatures, such as fish, crabs, molluscs and echinoderms, often share the den with the octopus, either because they have arrived as scavengers, or because they have survived capture.

Nearly all octopuses are predatory; bottom-dwelling octopuses eat mainly crustaceans, polychaete worms, and other molluscs such as whelks and clams; open-ocean octopuses eat mainly prawns, fish and other cephalopods. Major items in the diet of the giant Pacific octopus include bivalve molluscs such as the cockle "Clinocardium nuttallii", clams and scallops and crustaceans such as crabs and spider crabs. Prey that it is likely to reject include moon snails because they are too large and limpets, rock scallops, chitons and abalone, because they are too securely fixed to the rock.

A benthic (bottom-dwelling) octopus typically moves among the rocks and feels through the crevices. The creature may make a jet-propelled pounce on prey and pull it towards the mouth with its arms, the suckers restraining it. Small prey may be completely trapped by the webbed structure. Octopuses usually inject crustaceans like crabs with a paralysing saliva then dismember them with their beaks. Octopuses feed on shelled molluscs either by forcing the valves apart, or by drilling a hole in the shell to inject a nerve toxin. It used to be thought that the hole was drilled by the radula, but it has now been shown that minute teeth at the tip of the salivary papilla are involved, and an enzyme in the toxic saliva is used to dissolve the calcium carbonate of the shell. It takes about three hours for "O. vulgaris" to create a hole. Once the shell is penetrated, the prey dies almost instantaneously, its muscles relax, and the soft tissues are easy for the octopus to remove. Crabs may also be treated in this way; tough-shelled species are more likely to be drilled, and soft-shelled crabs are torn apart.

Some species have other modes of feeding. "Grimpoteuthis" has a reduced or non-existent radula and swallows prey whole. In the deep-sea genus "Stauroteuthis", some of the muscle cells that control the suckers in most species have been replaced with photophores which are believed to fool prey by directing them towards the mouth, making them one of the few bioluminescent octopuses.

Octopuses mainly move about by relatively slow crawling with some swimming in a head-first position. Jet propulsion or backwards swimming, is their fastest means of locomotion, followed by swimming and crawling. When in no hurry, they usually crawl on either solid or soft surfaces. Several arms are extended forwards, some of the suckers adhere to the substrate and the animal hauls itself forwards with its powerful arm muscles, while other arms may push rather than pull. As progress is made, other arms move ahead to repeat these actions and the original suckers detach. During crawling, the heart rate nearly doubles, and the animal requires ten or fifteen minutes to recover from relatively minor exercise.

Most octopuses swim by expelling a jet of water from the mantle through the siphon into the sea. The physical principle behind this is that the force required to accelerate the water through the orifice produces a reaction that propels the octopus in the opposite direction. The direction of travel depends on the orientation of the siphon. When swimming, the head is at the front and the siphon is pointed backwards, but when jetting, the visceral hump leads, the siphon points towards the head and the arms trail behind, with the animal presenting a fusiform appearance. In an alternative method of swimming, some species flatten themselves dorso-ventrally, and swim with the arms held out sideways, and this may provide lift and be faster than normal swimming. Jetting is used to escape from danger, but is physiologically inefficient, requiring a mantle pressure so high as to stop the heart from beating, resulting in a progressive oxygen deficit.

Cirrate octopuses cannot produce jet propulsion and rely on their fins for swimming. They have neutral buoyancy and drift through the water with the fins extended. They can also contract their arms and surrounding web to make sudden moves known as "take-offs". Another form of locomotion is "pumping", which involves symmetrical contractions of muscles in their webs producing peristaltic waves. This moves the body slowly.

In 2005, "Adopus aculeatus" and veined octopus ("Amphioctopus marginatus") were found to walk on two arms, while at the same time mimicking plant matter. This form of locomotion allows these octopuses to move quickly away from a potential predator without being recognised. A study of this behaviour led to the suggestion that the two rearmost appendages may be more accurately termed "legs" rather than "arms". Some species of octopus can crawl out of the water briefly, which they may do between tide pools while hunting crustaceans or gastropods or to escape predators. "Stilt walking" is used by the veined octopus when carrying stacked coconut shells. The octopus carries the shells underneath it with two arms, and progresses with an ungainly gait supported by its remaining arms held rigid.

Octopuses are highly intelligent; the extent of their intelligence and learning capability are not well defined. Maze and problem-solving experiments have shown evidence of a memory system that can store both short- and long-term memory. It is not known precisely what contribution learning makes to adult octopus behaviour. Young octopuses learn nothing from their parents, as adults provide no parental care beyond tending to their eggs until the young octopuses hatch.

In laboratory experiments, octopuses can be readily trained to distinguish between different shapes and patterns. They have been reported to practise observational learning, although the validity of these findings is contested. Octopuses have also been observed in what has been described as play: repeatedly releasing bottles or toys into a circular current in their aquariums and then catching them. Octopuses often break out of their aquariums and sometimes into others in search of food. They have even boarded fishing boats and opened holds to eat crabs. The veined octopus collects discarded coconut shells, then uses them to build a shelter, an example of tool use.

Octopuses use camouflage when hunting and to avoid predators. To do this they use specialised skin cells which change the appearance of the skin by adjusting its colour, opacity, or reflectivity. Chromatophores contain yellow, orange, red, brown, or black pigments; most species have three of these colours, while some have two or four. Other colour-changing cells are reflective iridophores and white leucophores. This colour-changing ability is also used to communicate with or warn other octopuses.

Octopuses can create distracting patterns with waves of dark coloration across the body, a display known as the "passing cloud". Muscles in the skin change the texture of the mantle to achieve greater camouflage. In some species, the mantle can take on the spiky appearance of algae; in others, skin anatomy is limited to relatively uniform shades of one colour with limited skin texture. Octopuses that are diurnal and live in shallow water have evolved more complex skin than their nocturnal and deep-sea counterparts.

A "moving rock" trick involves the octopus mimicking a rock and then inching across the open space with a speed matching the movement in the surrounding water, allowing it to move in plain sight of a predator.

Aside from humans, octopuses may be preyed on by fishes, seabirds, sea otters, pinnipeds, cetaceans, and other cephalopods. Octopuses typically hide or disguise themselves by camouflage and mimicry; some have conspicuous warning coloration (aposematism) or deimatic behaviour. An octopus may spend 40% of its time hidden away in its den. When the octopus is approached, it may extend an arm to investigate. 66% of "Enteroctopus dofleini" in one study had scars, with 50% having amputated arms. The blue rings of the highly venomous blue-ringed octopus are hidden in muscular skin folds which contract when the animal is threatened, exposing the iridescent warning. The Atlantic white-spotted octopus ("Callistoctopus macropus") turns bright brownish red with oval white spots all over in a high contrast display. Displays are often reinforced by stretching out the animal's arms, fins or web to make it look as big and threatening as possible.

Once they have been seen by a predator, they commonly try to escape but can also use distraction with an ink cloud ejected from the ink sac. The ink is thought to reduce the efficiency of olfactory organs, which would aid evasion from predators that employ smell for hunting, such as sharks. Ink clouds of some species might act as pseudomorphs, or decoys that the predator attacks instead.

When under attack, some octopuses can perform arm autotomy, in a manner similar to the way skinks and other lizards detach their tails. The crawling arm may distract would-be predators. Such severed arms remain sensitive to stimuli and move away from unpleasant sensations. Octopuses can replace lost limbs.

Some octopuses, such as the mimic octopus, can combine their highly flexible bodies with their colour-changing ability to mimic other, more dangerous animals, such as lionfish, sea snakes, and eels.

The diseases and parasites that affect octopuses have been little studied, but cephalopods are known to be the intermediate or final hosts of various parasitic cestodes, nematodes and copepods; 150 species of protistan and metazoan parasites have been recognised. The Dicyemidae are a family of tiny worms that are found in the renal appendages of many species; it is unclear whether they are parasitic or are endosymbionts. Coccidians in the genus "Aggregata" living in the gut cause severe disease to the host. Octopuses have an innate immune system, and the haemocytes respond to infection by phagocytosis, encapsulation, infiltration or cytotoxic activities to destroy or isolate the pathogens. The haemocytes play an important role in the recognition and elimination of foreign bodies and wound repair. Captive animals have been found to be more susceptible to pathogens than wild ones. A gram-negative bacterium, "Vibrio lentus", has been found to cause skin lesions, exposure of muscle and death of octopuses in extreme cases.

The scientific name Octopoda was first coined and given as the order of octopuses in 1818 by English biologist William Elford Leach, who classified them as Octopoida the previous year. The Octopoda consists of around 300 known species and were historically divided into two suborders, the Incirrina and the Cirrina. However, more recent evidence suggests that Cirrina are merely the most basal species and are not a unique clade. The incirrate octopuses (the majority of species) lack the cirri and paired swimming fins of the cirrates. In addition, the internal shell of incirrates is either present as a pair of stylets or absent altogether. 

Cephalopods have existed for 500 million years and octopus ancestors were in the Carboniferous seas 300 million years ago. The oldest known octopus fossil is "Pohlsepia", which lived 296 million years ago. Researchers have identified impressions of eight arms, two eyes, and possibly an ink sac. Octopuses are mostly soft tissue, and so fossils are relatively rare. Octopuses, squids and cuttlefish belong to the clade Coleoidea. They are known as "soft-bodied" cephalopods, lacking the external shell of most molluscs and other cephalopods like the nautiloids and the extinct Ammonoidea. Octopuses have eight limbs like other coleoids but lack the extra specialised feeding appendages known as tentacles which are longer and thinner with suckers only at their club-like ends. The vampire squid ("Vampyroteuthis") also lacks tentacles but has sensory filaments.

The cladograms are based on Sanchez et al., 2018, who created a molecular phylogeny based on mitochondrial and nuclear DNA marker sequences.

The molecular analysis of the octopods shows that the suborder Cirrina (Cirromorphida) and the superfamily Argonautoidea are paraphyletic and are broken up; these names are shown in quotation marks and italics on the cladogram.

Octopuses and other coleoid cephalopods are capable of greater RNA editing (which involves changes to the nucleic acid sequence of the primary transcript of RNA molecules) than any other organisms. Editing is concentrated in the nervous system and affects proteins involved in neural excitability and neuronal morphology. More than 60% of RNA transcripts for coleoid brains are recoded by editing, compared to less than 1% for a human or fruit fly. Coleoids rely mostly on ADAR enzymes for RNA editing, which requires large double-stranded RNA structures to flank the editing sites. Both the structures and editing sites are conserved in the coleoid genome and the mutation rates for the sites are severely hampered. Hence, greater transcriptome plasticity has come at the cost of slower genome evolution. High levels of RNA editing do not appear to be present in more basal cephalopods or other molluscs.

Ancient seafaring people were aware of the octopus, as evidenced by certain artworks and designs. For example, a stone carving found in the archaeological recovery from Bronze Age Minoan Crete at Knossos (1900–1100 BC) has a depiction of a fisherman carrying an octopus. The terrifyingly powerful Gorgon of Greek mythology has been thought to have been inspired by the octopus or squid, the octopus itself representing the severed head of Medusa, the beak as the protruding tongue and fangs, and its tentacles as the snakes. The Kraken are legendary sea monsters of giant proportions said to dwell off the coasts of Norway and Greenland, usually portrayed in art as a giant octopus attacking ships. Linnaeus included it in the first edition of his 1735 "Systema Naturae". One translation of the Hawaiian creation myth the Kumulipo suggests that the octopus is the lone survivor of a previous age. The Akkorokamui is a gigantic octopus-like monster from Ainu folklore.

A battle with an octopus plays a significant role in Victor Hugo's book "Travailleurs de la mer" ("Toilers of the Sea"), relating to his time in exile on Guernsey.
Ian Fleming's 1966 short story collection "Octopussy and The Living Daylights", and the 1983 "James Bond" film were partly inspired by Hugo's book.

Japanese erotic art, "shunga", includes ukiyo-e woodblock prints such as Katsushika Hokusai's 1814 print "Tako to ama" (The Dream of the Fisherman's Wife), in which an ama diver is sexually intertwined with a large and a small octopus. The print is a forerunner of tentacle erotica. The biologist P. Z. Myers noted in his science blog, "Pharyngula", that octopuses appear in "extraordinary" graphic illustrations involving women, tentacles, and bare breasts.

Since it has numerous arms emanating from a common centre, the octopus is often used as a symbol for a powerful and manipulative organisation.

Octopuses generally avoid humans, but incidents have been verified. For example, a Pacific octopus, said to be nearly perfectly camouflaged, "lunged" at a diver and "wrangled" over his camera before it let go. Another diver recorded the encounter on video. 

All species are venomous, but only blue-ringed octopuses have venom that is lethal to humans. Bites are reported each year across the animals' range from Australia to the eastern Indo-Pacific Ocean. They bite only when provoked or accidentally stepped upon; bites are small and usually painless. The venom appears to be able to penetrate the skin without a puncture, given prolonged contact. It contains tetrodotoxin, which causes paralysis by blocking the transmission of nerve impulses to the muscles. This causes death by respiratory failure leading to cerebral anoxia. No antidote is known, but if breathing can be kept going artificially, patients recover within 24 hours. Bites have been recorded from captive octopuses of other species; they leave swellings which disappear in a day or two.

Octopus fisheries exist around the world with total catches varying between 245,320 and 322,999 metric tons from 1986 to 1995. The world catch peaked in 2007 at 380,000 tons, and fell by a tenth by 2012. Methods to capture octopuses include pots, traps, trawls, snares, drift fishing, spearing, hooking and hand collection. Octopus is eaten in many cultures and is a common food on the Mediterranean and Asian coasts. The arms and sometimes other body parts are prepared in various ways, often varying by species or geography. Live octopuses are eaten in several countries around the world, including the US. Animal welfare groups have objected to this practice on the basis that octopuses can experience pain. Octopuses have a food conversion efficiency greater than that of chickens, making octopus aquaculture a possibility.

In classical Greece, Aristotle (384–322 BC) commented on the colour-changing abilities of the octopus, both for camouflage and for signalling, in his "Historia animalium": "The octopus ... seeks its prey by so changing its colour as to render it like the colour of the stones adjacent to it; it does so also when alarmed." Aristotle noted that the octopus had a hectocotyl arm and suggested it might be used in sexual reproduction. This claim was widely disbelieved until the 19th century. It was described in 1829 by the French zoologist Georges Cuvier, who supposed it to be a parasitic worm, naming it as a new species, "Hectocotylus octopodis". Other zoologists thought it a spermatophore; the German zoologist Heinrich Müller believed it was "designed" to detach during copulation. In 1856 the Danish zoologist Japetus Steenstrup demonstrated that it is used to transfer sperm, and only rarely detaches.

Octopuses offer many possibilities in biological research, including their ability to regenerate limbs, change the colour of their skin, behave intelligently with a distributed nervous system, and make use of 168 kinds of protocadherins (humans have 58), the proteins that guide the connections neurons make with each other. The California two-spot octopus has had its genome sequenced, allowing exploration of its molecular adaptations. Having independently evolved mammal-like intelligence, octopuses have been compared to hypothetical intelligent extraterrestrials. Their problem-solving skills, along with their mobility and lack of rigid structure enable them to escape from supposedly secure tanks in laboratories and public aquariums.

Due to their intelligence, octopuses are listed in some countries as experimental animals on which surgery may not be performed without anesthesia, a protection usually extended only to vertebrates. In the UK from 1993 to 2012, the common octopus ("Octopus vulgaris") was the only invertebrate protected under the Animals (Scientific Procedures) Act 1986. In 2012, this legislation was extended to include all cephalopods in accordance with a general EU directive.

Some robotics research is exploring biomimicry of octopus features. Octopus arms can move and sense largely autonomously without intervention from the animal's central nervous system. In 2015 a team in Italy built soft-bodied robots able to crawl and swim, requiring only minimal computation. In 2017 a German company made an arm with a soft pneumatically controlled silicone gripper fitted with two rows of suckers. It is able to grasp objects such as a metal tube, a magazine, or a ball, and to fill a glass by pouring water from a bottle.





</doc>
<doc id="22781" url="https://en.wikipedia.org/wiki?curid=22781" title="Omniscience">
Omniscience

Omniscience () is the capacity to know everything. In monotheistic religions, such as Sikhism and the Abrahamic religions, this is an attribute of God. In Jainism, omniscience is an attribute that any individual can eventually attain. In Buddhism, there are differing beliefs about omniscience among different schools.

The word "omniscience" derives from the Latin word "sciens" ("to know" or "conscious") and the prefix "omni" ("all" or "every"), but also means "all-seeing".

The topic of omniscience has been much debated in various Indian traditions, but no more so than by the Buddhists. After Dharmakirti's excursions into the subject of what constitutes a valid cognition, Śāntarakṣita and his student Kamalaśīla thoroughly investigated the subject in the Tattvasamgraha and its commentary the Panjika. The arguments in the text can be broadly grouped into four sections:

Some modern Christian theologians argue that God's omniscience is inherent rather than total, and that God chooses to limit his omniscience in order to preserve the free will and dignity of his creatures. John Calvin, among other theologians of the 16th century, comfortable with the definition of God as being omniscient in the total sense, in order for worthy beings' abilities to choose freely, embraced the doctrine of predestination.

In Islam, God ("Allah") is attributed with absolute omniscience. He knows the past, the present and the future. It is compulsory for a Muslim to believe that Allah is indeed omniscient as stated in one of the six articles of faith which is:

It is believed that humans can only change their predestination (wealth, health, deed etc.) and not divine decree (date of birth, date of death, family etc.), thus allowing free will.

In Jainism, omniscience is considered the highest type of perception. In the words of a Jain scholar,
"The perfect manifestation of the innate nature of the self, arising on the complete annihilation of the obstructive veils, is called omniscience."

Jainism views infinite knowledge as an inherent capability of every soul. "Arihanta" is the word used by Jains to refer to those human beings who have conquered all inner passions (like attachment, greed, pride, anger) and possess "Kevala Jnana" (infinite knowledge). They are said to be of two kinds:

Whether omniscience, particularly regarding the choices that a human will make, is compatible with free will has been debated by theologians and philosophers. The argument that divine foreknowledge is not compatible with free will is known as theological fatalism. It is argued that if humans are free to choose between alternatives, God could not know what this choice will be.

A question arises: if an omniscient entity knows everything, even about its own decisions in the future, does it therefore forbid any free will to that entity? William Lane Craig states that the question subdivides into two:

However, this kind of argument fails to recognize its use of the modal fallacy. It is possible to show that the first premise of arguments like these is fallacious. 

Some philosophers, such as Patrick Grim, Linda Zagzebski, Stephan Torre and William Mander have discussed the issue of whether the apparent exclusively first-person nature of conscious experience is compatible with God's omniscience. There is a strong sense in which conscious experience is private, meaning that no outside observer can gain knowledge of what it is like to be me "as me". If a subject cannot know what it is like to be another subject in an objective manner, the question is whether that limitation applies to God as well. If it does, then God cannot be said to be omniscient since there is then a form of knowledge that God lacks access to.

The philosopher Patrick Grim most notably raised this issue. Linda Zagzebski tried to avoid it by introducing the notion of "perfect empathy", a proposed relation that God can have to subjects that would allow God to have perfect knowledge of their conscious experience. William Mander argued that God can only have such knowledge if our experiences are part of God's broader experience. Stephan Torre claimed that God can have such knowledge if self-knowledge involves the ascription of properties, either to oneself or to others. Patrick Grim saw this line of reasoning as a motivation for accepting atheism.





</doc>
<doc id="22784" url="https://en.wikipedia.org/wiki?curid=22784" title="Original Chip Set">
Original Chip Set

The Original Chip Set (OCS) is a chipset used in the earliest Commodore Amiga computers and defined the Amiga's graphics and sound capabilities. It was succeeded by the slightly improved Enhanced Chip Set (ECS) and greatly improved Advanced Graphics Architecture (AGA).

The original chipset appeared in Amiga models built between 1985 and 1990: the Amiga 1000, Amiga 2000, Amiga CDTV, and Amiga 500.

The chipset which gave the Amiga its unique graphics features consists of three main "custom" chips; "Agnus", "Denise", and "Paula". Both the original chipset and the enhanced chipset were manufactured using NMOS logic technology by Commodore's chip manufacturing subsidiary, MOS Technology. According to Jay Miner, the OCS chipset was fabricated in 5 µm manufacturing process while AGA Lisa was implemented in 1.5 µm process. All three custom chips were originally packaged in 48-pin DIPs; later versions of Agnus, known as Fat Agnus, were packaged in an 84-pin PLCC.

Agnus is the central chip in the design. It controls all access to chip RAM from both the central 68000 processor and the other custom chips, using a complicated priority system. Agnus includes sub-components known as the "blitter" (fast transfer of data in memory without the intervention of the processor) and the "Copper" (video-synchronized co-processor). The original Agnus can address of chip RAM. Later revisions, dubbed 'Fat Agnus', added pseudo-fast RAM, which for ECS was changed to 1 MB (sometimes called 'Fatter Agnus') and subsequently to 2 MB chip RAM.

Denise is the main video processor. Without using overscan, the Amiga's graphics display is 320 or 640 pixels wide by 200 (NTSC) or 256 (PAL) pixels tall. Denise also supports interlacing, which doubles the vertical resolution, at the cost of intrusive flickering on typical monitors of that era. Planar bitmap graphics are used, which splits the individual bits per pixel into separate areas of memory, called bitplanes. In normal operation, Denise allows between one and five bitplanes, giving two to 32 unique colors. These colors are selected from a palette of 4096 colors (four bits per RGB component). A 6th bitplane is available for two special video modes: Halfbrite mode and Hold-And-Modify (HAM) mode. Denise also supports eight sprites, single pixel scrolling, and a "dual-playfield" mode. Denise also handles mouse and digital joystick input.

Paula is primarily the audio chip, with four independent hardware-mixed 8-bit PCM sound channels, each of which supports 65 volume levels (no sound to maximum volume) and waveform output rates from roughly 20 samples per second to almost 29,000 samples per second. Paula also handles interrupts and various I/O functions including the floppy disk drive, the serial port, and analog joysticks.

There are many similarities both in overall functionality and in the division of functionality into the three component chips between the OCS chipset and the much earlier and simpler chipset of the Atari 8-bit family of home computers, consisting of the ANTIC, GTIA and POKEY chips; both chipsets were conceptually designed by Jay Miner, which explains the similarity.

The Agnus chip is in overall control of the entire chipset's operation. All operations are synchronised to the position of the video beam. This includes access to the built-in RAM, known as chip RAM because the chipset has access to it. Both the central 68000 processor and other members of the chipset have to arbitrate for access to chip RAM via "Agnus". In computing architecture terms, this is Direct Memory Access (DMA), where Agnus is the DMA Controller (DMAC).

Agnus has a complex and priority-based memory access policy that attempts to best coordinate requests for memory access among competing resources. For example, bitplane data fetches are prioritized over blitter transfers as the immediate display of frame buffer data is considered more important than the processing of memory by the blitter. Agnus also attempts to order accesses in such a way so as to overlap CPU bus cycles with DMA cycles. As the original 68000 processor in Amigas tended only to access memory on every second available memory cycle, Agnus operates a system where "odd" memory access cycles are allocated first and as needed to time-critical custom chip DMA while any remaining cycles are available to the CPU, thus the CPU does not generally get locked out of memory access and does not appear to slow down. However, non-time-critical custom chip access, such as "blitter" transfers, can use up any spare odd or even cycles and, if the "BLITHOG" (blitter hog) flag is set, Agnus can lock out the even cycles from the CPU in deference to the "blitter".

Agnus's timings are measured in "color clocks" of 280 ns. This is equivalent to two low resolution (140 ns) pixels or four high resolution (70 ns) pixels. Like Denise, these timings were designed for display on household TVs, and can be synchronized to an external clock source.

The "blitter" is a sub-component of Agnus. "Blit" is shorthand for "block image transfer" or bit blit. The blitter is a highly parallel memory transfer and logic operation unit. It has three modes of operation: copying blocks of memory, filling blocks (e.g. polygon filling) and line drawing.

The blitter allows rapid copying of video memory, meaning that the CPU can be freed for other tasks. The blitter was primarily used for drawing and redrawing graphics images on the screen, called "bobs", short for "blitter objects".

The blitter's block copying mode takes zero to three data sources in memory, called A, B and C, performs a programmable boolean function on the data sources and writes the result to a destination area, D. Any of these four areas can overlap. The blitter runs either from the start of the block to the end, known as "ascending" mode, or in reverse, "descending" mode.

Blocks are "rectangular"; they have a "width" in multiples of 16 bits, a height measured in "lines", and a "stride" distance to move from the end of one line to the next. This allows the blitter to operate on any video resolution up to 1,024×1,024 pixels. The copy automatically performs a per-pixel logical operation. These operations are described generically using minterms. This is most commonly used to do direct copies (D = A), or apply a pixel mask around blitted objects (D = (C AND B) OR A). The copy can also barrel shift each line by 0 to 15 pixels. This allows the blitter to draw at pixel offsets that are not exactly multiples of 16.

These functions allow the Amiga to move GUI windows around the screen rapidly as each is represented in graphical memory space as a rectangular block of memory which may be shifted to any required screen memory location at will.

The blitter's line mode draws single-pixel thick lines using Bresenham's line algorithm. It can also apply a 16-bit repeating pattern to the line. The line mode can also be used to draw rotated bobs: each line of bob data is used as line pattern while the line mode draws the tilted bob line by line.

The blitter's filling mode is used to fill per-line horizontal spans. On each span, it reads each pixel in turn from right to left. Whenever it reads a set pixel, it toggles filling mode on or off. When filling mode is on, it sets every pixel until filling mode is turned off or the line ends. Together, these modes allow the blitter to draw individual flat-shaded polygons. Later Amigas tended to use a combination of a faster CPU and blitter for many operations.

The "Copper" is another sub-component of Agnus; The name is short for "co-processor". The Copper is a programmable finite-state machine that executes a programmed instruction stream, synchronized with the video hardware.

When it is turned on, the Copper has three states; either reading an instruction, executing it, or waiting for a specific video beam position. The Copper runs a program called the "Copper list" in parallel with the main CPU. The Copper runs in sync with the video beam, and it can be used to perform various operations which require video synchronization. Most commonly it is used to control video output, but it can write to most of the chipset registers and thus can be used to initiate blits, set audio registers, or interrupt the CPU.

The Copper list has three kinds of instructions, each one being a pair of two bytes, four bytes in total:


The length of the Copper list program is limited by execution time. The Copper restarts executing the Copper list at the start of each new video frame. There is no explicit "end" instruction; instead, the WAIT instruction is used to wait for a location which is never reached.


Under normal circumstances, the Amiga generates its own video timings, but Agnus also supports synchronising the system to an external signal so as to achieve genlocking with external video hardware. There is also a 1-bit output on this connector that indicates whether the Amiga is outputting background color or not, permitting easy overlaying of Amiga video onto external video. This made the Amiga particularly attractive as a character generator for titling videos and broadcast work, as it avoided the use and expense of AB roll and chromakey units that would be required without the genlock support. The support of overscan, interlacing and genlocking capabilities, and the fact that the display timing was very close to broadcast standards (NTSC or PAL), made the Amiga the first ideal computer for video purposes, and indeed, it was used in many studios for digitizing video data (sometimes called frame-grabbing), subtitling and interactive video news.

Denise is programmed to fetch planar video data from one to five bitplanes and translate that into a color lookup. The number of bitplanes is arbitrary, thus if 32 colors are not needed, 2, 4, 8 or 16 can be used instead. The number of bitplanes (and resolution) can be changed on the fly, usually by the Copper. This allows for very economical use of RAM, and balancing of CPU processing speed vs graphical sophistication when executing from Chip RAM (as modes beyond 4bpp in lorez, or 2bpp in hires, use extra DMA channels that can slow or temporarily halt the CPU in addition to the usual non-conflicting channels). There can also be a sixth bitplane, which can be used in three special graphics modes:

In Extra-HalfBrite (EHB), if a pixel is set on the sixth bitplane, the brightness of the regular 32 color pixel is halved. Early versions of the Amiga 1000 sold in the United States did not have the Extra-HalfBrite mode.

In Hold-and-Modify (HAM) mode, each 6-bit pixel is interpreted as two control bits and four data bits. The four possible permutations of control bits are "set", "modify red", "modify green" and "modify blue". With "set", the four data bits act like a regular 16-color display look up. With one of the "modify"s, the red, green or blue component of the previous pixel is modified to the data value, and the other two components are held from the previous pixel. This allows all 4096 colors on screen at once and is an example of lossy image compression in hardware.

In dual-playfield mode, instead of acting as a single screen, two "playfields" of eight colors each (three bitplanes each) are drawn on top of each other. They are independently scrollable and the background color of the top playfield "shines through" to the underlying playfield.

There are two horizontal graphics resolutions, "lowres" with 140 ns pixels and "hires" with 70 ns pixels, with a default of 320 or 640 horizontal pixels wide without using overscan. As the pixel output is regulated by the main system clock, which is based directly on the NTSC colorburst clock, these sizes very nearly fill the width of a standard television with only a thin "underscan" border between the graphics and the screen border when compared to many other contemporary home computers, for an appearance closer to a games console but with finer detail. On top of this, Denise supports reasonably extensive overscan; technically modes with enough data for up to 400 or 800 pixels (+25%) may be specified, although this is only actually useful for scrolling and special effects that involve partial display of large graphics, as a separate hardware limit is met at 368 (or 736) pixels, which is the maximum that will fit between the end of one blanking period and the start of the next - although it is unlikely that even this many pixels will be visible on any display other than a dedicated monitor that allows adjustment of horizontal scan width, as much of the image will, by design, disappear seamlessly behind the screen bezel (or, on LCDs, be cropped off at the edge of the panel). Because of the highly regular structure of the Amiga's timing in relation to scanlines and allocation of DMA resources to various uses besides normal "playfield" graphics, increased horizontal resolution is also a tradeoff between number of pixels and how many hardware sprites are available, as increasing the DMA slots dedicated to playfield video ends up stealing some (from 1 to 7 of the total 8) the sprite engine.. Vertical resolution, without overscan, is 200 pixels for a 60 Hz NTSC Amiga or 256 for a 50 Hz PAL Amiga. This can be doubled using an interlaced display, and, as with horizontal resolution, increased using overscan, to a maximum of 241 (or 483) for NTSC, and 283 (567) for PAL (interlaced modes gaining one extra line as the maximum is determined by how many lines are taken from the available total by blanking and sync, and the total scanlines in non-interlaced modes are half the original, broadcast-spec odd-numbered interlaced counts, rounded down).

Denise can composite up to eight 16 pixel wide sprites per scan line (in automatic mode) on top, underneath, or between playfields, and detect collisions between sprites and the playfields or between sprites. These sprites have three visible colors and one transparent color. Optionally, adjacent pairs of sprites can be "attached" to make a single 15 color sprite. Using Copper or CPU register manipulations, each sprite 'channel' can be reused multiple times in a single frame to increase the total sprites per frame. Sprite "position" registers may also be changed during a scanline, increasing the total number of sprites on a single scanline. However, the sprite "data", or shape, is only fetched a single time per scanline and can't change. The first Amiga game to utilize the sprite re-position registers during a scanline was Hybris released in 1988.

Finally, Denise is responsible for handling mouse/joystick X/Y inputs.

The Paula chip, from MOS Technology, is the interrupt controller, but also includes logic for audio playback, floppy disk drive control, serial port input/output and mouse/joystick buttons two and three signals. The logic remained functionally identical across all Amiga models from Commodore.

Paula has four DMA-driven 8-bit PCM sample sound channels. Two sound channels are mixed into the left audio output, and the other two are mixed into the right output, producing stereo audio output. The only supported hardware sample format is signed linear 8-bit two's complement. Each sound channel has an independent frequency and a 6-bit volume control (64 levels). Internally, the audio hardware is implemented by four state machines, each having eight different states.

Additionally the hardware allows one channel in a channel pair to modulate the other channel's period or amplitude. It is rarely used on the Amiga due to both frequency and volume being controllable in better ways, but could be used to achieve different kinds of tremolo and vibrato, and even rudimentary FM synthesis effects.

Audio may be output using two methods. Most often, DMA-driven audio is used. As explained in the discussion of Agnus, memory access is prioritized and one DMA slot per scan line is available for each of the four sound channels. On a regular NTSC or PAL display, DMA audio playback is limited to a maximum output rate of 28867 values per channel (PAL: 28837) per second totaling 57674 (PAL: 57734) values per second on each stereo output. This rate can be increased with the ECS and AGA chipsets by using a video mode with higher horizontal scan rate.

Alternately, Paula may signal the CPU to load a new sample into any of the four audio output buffers by generating an interrupt when a new sample is needed. This allows for output rates that exceed 57 kHz per channel and increases the number of possible voices (simultaneous sounds) through software mixing.

The Amiga contains an analog low-pass filter (reconstruction filter) which is external to Paula. The filter is a 12 dB/oct Butterworth low-pass filter at approximately 3.3 kHz. The filter can only be applied globally to all four channels. In models after the Amiga 1000 (excluding the very first revision of the Amiga 500), the brightness of the power LED is used to indicate the status of the filter. The filter is active when the LED is at normal brightness, and deactivated when dimmed (on early Amiga 500 models the LED went completely off). Models released before Amiga 1200 also have a static "tone knob" type low-pass filter that is enabled regardless of the optional "LED filter". This filter is a 6 dB/oct low-pass filter with cutoff frequency at 4.5 or 5 kHz.

A software technique was later developed which can play back 14-bit audio by combining two channels set at different volumes. This results in two 14-bit channels instead of four 8-bit channels. This is achieved by playing the high byte of a 16-bit sample at maximum volume, and the low byte at minimum volume (both ranges overlap, so the low byte needs to be shifted right two bits). The bit shift operation requires a small amount of CPU or blitter overhead, whereas conventional 8-bit playback is almost entirely DMA driven. This technique was incorporated into the retargetable audio subsystem AHI, allowing compatible applications to use this mode transparently.

The floppy controller is unusually flexible. It can read and write raw bit sequences directly from and to the disk via DMA or programmed I/O at 500 (double density) or 250 kbit/s (single density or GCR). MFM or GCR were the two most commonly used formats though in theory any run-length limited code could be used. It also provides a number of convenient features, such as sync-on-word (in MFM coding, $4489 is usually used as the sync word). MFM encoding/decoding is usually done with the blitter — one pass for decode, three passes for encode. Normally the entire track is read or written in one shot, rather than sector-by-sector; this made it possible to get rid of most of the inter-sector gaps that most floppy disk formats need to safely prevent the "bleeding" of a written sector into the previously-existing header of the next sector due to speed variations of the drive. If all sectors and their headers are always written in one go, such bleeding is only an issue at the end of the track (which still must not bleed back into its beginning), so that only one gap per track is needed. This way, for the native Amiga disk format, the raw storage capacity of 3.5 inch DD disks was increased from the typical 720 KB to 880 KB, although the less-than-ideal file system of the earlier Amiga models reduced this again to approximately 830 KB of actual payload data.

In addition to the native 880 KB 3.5-inch disk format, the controller can handle many foreign formats, such as:


The Amiga 3000 introduced a special, dual-speed floppy drive that also allowed to use high density disks with double capacity without any change to Paula's floppy controller.

The serial port is rudimentary, using programmed input/output only and lacking a FIFO buffer. However, virtually any bit rate can be selected, including all standard rates, MIDI rate, as well as extremely high custom rates.





</doc>
<doc id="22786" url="https://en.wikipedia.org/wiki?curid=22786" title="Optic neuritis">
Optic neuritis

Optic neuritis is a demyelinating inflammation of the optic nerve. It is also known as optic papillitis (when the head of the optic nerve is involved), neuroretinitis when there is a combined involvement of optic disc and surrounding retina in the macular area and retrobulbar neuritis (when the posterior part of the nerve is involved). It is most often associated with multiple sclerosis, and it may lead to complete or partial loss of vision in one or both eyes. Other causes include:


Partial, transient vision loss (lasting less than one hour) can be an indication of early onset multiple sclerosis. Other possible diagnoses include diabetes mellitus, low phosphorus levels, or hyperkalaemia.

Major symptoms are sudden loss of vision (partial or complete), sudden blurred or "foggy" vision, and pain on movement of the affected eye. Early symptoms that require investigation include symptoms from multiple sclerosis (twitching, lack of coordination, slurred speech, frequent episodes of partial vision loss or blurred vision), episodes of "disturbed/blackened" rather than blurry indicate moderate stage and require immediate medical attention to prevent further loss of vision. Other early symptoms are reduced night vision, photophobia and red eyes. Many patients with optic neuritis may lose some of their color vision in the affected eye (especially red), with colors appearing subtly washed out compared to the other eye. Patients may also experience difficulties judging movement in depth which can be particular troublesome during driving or sport (Pulfrich effect). Likewise transient worsening of vision with increase of body temperature (Uhthoff's phenomenon) and glare disability are a frequent complaint. However, several case studies in children have demonstrated the absence of pain in more than half of cases (approximately 60%) in their pediatric study population, with the most common symptom reported simply as "blurriness." Other remarkable differences between the presentation of adult optic neuritis as compared to pediatric cases include more often unilateral optic neuritis in adults, while children much predominantly present with bilateral involvement.

On medical examination the head of the optic nerve can easily be visualized by a slit lamp with a high positive lens or by using direct ophthalmoscopy; however, frequently there is no abnormal appearance of the nerve head in optic neuritis (in cases of retrobulbar optic neuritis), though it may be swollen in some patients (anterior papillitis or more extensive optic neuritis). In many cases, only one eye is affected and patients may not be aware of the loss of color vision until they are asked to close or cover the healthy eye.

The optic nerve comprises axons that emerge from the retina of the eye and carry visual information to the primary visual nuclei, most of which is relayed to the occipital cortex of the brain to be processed into vision. Inflammation of the optic nerve causes loss of vision, usually because of the swelling and destruction of the myelin sheath covering the optic nerve.

The most common cause is multiple sclerosis or ischemic optic neuropathy due to thrombosis or embolism of the vessel that supplies the optic nerve. Up to 50% of patients with MS will develop an episode of optic neuritis, and 20-30% of the time optic neuritis is the presenting sign of MS. The presence of demyelinating white matter lesions on brain MRI at the time of presentation of optic neuritis is the strongest predictor for developing clinically definite MS. Almost half of the patients with optic neuritis have white matter lesions consistent with multiple sclerosis.

Some other common causes of optic neuritis include infection (e.g. a tooth abscess in the upper jaw, syphilis, Lyme disease, herpes zoster), autoimmune disorders (e.g. lupus, neurosarcoidosis, neuromyelitis optica), methanol poisoning, Vitamin B deficiency, and diabetes, or an injury to the eye. In neuromyelitis optica higher AQP4 autoantibody levels are associated with the occurrence of optic neuritis. 

Less common causes are: papilledema, brain tumor or abscess in the occipital region, cerebral trauma or hemorrhage, meningitis, arachnoidal adhesions, sinus thrombosis, liver dysfunction, or late stage kidney disease. 

The repetition of an idiopathic optic neuritis is considered a distinct clinical condition, and when it shows demyelination, it has been found to be associated to anti-MOG and AQP4-negative neuromyelitis optica

When an inflammatory recurrent optic neuritis is not demyelinating, it is called "Chronic relapsing inflammatory optic neuropathy" (CRION)

When it is anti-MOG related, it is demyelinating and it is considered inside the anti-MOG associated inflammatory demyelinating diseases.

Some reports point to the possibility to establish a difference via OCT

In most MS-associated optic neuritis, visual function spontaneously improves over 2–3 months, and there is evidence that corticosteroid treatment does not affect the long term outcome. However, for optic neuritis that is not MS-associated (or atypical optic neuritis) the evidence is less clear and therefore the threshold for treatment with intravenous corticosteroids is lower. Intravenous corticosteroids also reduce the risk of developing MS in the following two years in patients with MRI lesions; but this effect disappears by the third year of follow up.

Paradoxically, oral administration of corticosteroids in this situation may lead to more recurrent attacks than in non-treated patients (though oral steroids are generally prescribed after the intravenous course, to wean the patient off the medication). This effect of corticosteroids seems to be limited to optic neuritis and has not been observed in other diseases treated with corticosteroids.

A Cochrane Systematic Review studied the effect of corticosteroids for treating people with acute optic neuritis. Specific corticosteroids studied included intravenous and oral methylprednisone, and oral prednisone. The authors conclude that current evidence does not show a benefit of either intravenous or oral corticosteroids for rate of recovery of vision (in terms of visual acuity, contrast sensitivity, or visual fields). There is a number of reasons why this might be the case. 

Optic neuritis typically affects young adults ranging from 18–45 years of age, with a mean age of 30–35 years. There is a strong female predominance. The annual incidence is approximately 5/100,000, with a prevalence estimated to be 115/100,000.

In Charles Dickens' "Bleak House", the main character, Esther Summerville, suffers from a transient episode of visual loss, the symptoms of which are also seen in people who have optic neuritis. Legal historian Sir William Searle Holdsworth, suggested that the events in "Bleak House" took place in 1827.

In an episode of "Dr. Quinn, Medicine Woman" ("Season of Miracles", season five), Reverend Timothy Johnson is struck blind by optic neuritis on Christmas Day 1872. He remains blind for the duration of the series.



</doc>
<doc id="22787" url="https://en.wikipedia.org/wiki?curid=22787" title="List of organizations with .int domain names">
List of organizations with .int domain names

This is a list of organizations with INT domain names, in alphabetical order of the second-level domain name. The list is not comprehensive. As of June 2012, the INT domain consists of 166 subdomain delegations.

These organizations are generally international organizations established by treaty. Some however (such as YMCA) do not meet current restrictions and were grandfathered in from prior acceptance.


<nowiki>*</nowiki> Bodies which are not international treaty organizations or (arguably) related to Internet infrastructure, and so do not meet current .INT requirements.

<nowiki>**</nowiki> Domains connected with Internet infrastructure; these are not international intergovernmental organizations.

<nowiki>***</nowiki> Domains connected with treaties but which may not have the full status of intergovernmental organization (sites for treaties or treaty secretariats).


</doc>
<doc id="22788" url="https://en.wikipedia.org/wiki?curid=22788" title="Organization of American States">
Organization of American States

The Organization of American States (, , ), or the OAS or OEA, is a continental organization that was founded on 30 April 1948, for the purposes of solidarity and cooperation among its member states within the Western Hemisphere. During the Cold War, the United States hoped the OAS would be a bulwark against the spread of communism. Since the 1990s, the organization has focused on election monitoring. Headquartered in the United States' capital Washington, D.C., the OAS's members are the 35 independent states of the Americas.

As of 26 May 2015, the Secretary General of OAS is Luis Almagro.

The notion of an international union in the New World was first put forward during the liberation of the Americas by José de San Martín and Simón Bolívar who, at the 1826 Congress of Panama (still being part of Colombia), proposed creating a league of American republics, with a common military, a mutual defense pact, and a supranational parliamentary assembly. This meeting was attended by representatives of Gran Colombia (comprising the modern-day countries of Colombia, Ecuador, Panama and Venezuela), Argentina, Peru, Bolivia, The United Provinces of Central America, and Mexico but the grandly titled "Treaty of Union, League, and Perpetual Confederation" was ultimately ratified only by Gran Colombia. Bolívar's dream soon floundered with civil war in Gran Colombia, the disintegration of Central America, and the emergence of national rather than New World outlooks in the newly independent American republics. Bolívar's dream of American unity was meant to unify Hispanic American nations against external powers.

The pursuit of regional solidarity and cooperation again came to the forefront in 1889–1890, at the First International Conference of American States. Gathered together in Washington, D.C., 18 nations resolved to found the International Union of American Republics, served by a permanent secretariat called the Commercial Bureau of the American Republics (renamed the International Commercial Bureau at the Second International Conference in 1901–1902). These two bodies, in existence as of 14 April 1890, represent the point of inception to which the OAS and its General Secretariat trace their origins.

At the fourth International Conference of American States (Buenos Aires, 1910), the name of the organization was changed to the Union of American Republics and the Bureau became the Pan American Union. The Pan American Union Building was constructed in 1910, on Constitution Avenue, Northwest, Washington, D.C.

In the mid-1930s, U.S. President Franklin Delano Roosevelt organized an inter-American conference in Buenos Aires. One of the items at the conference was a "League of Nations of the Americas", an idea proposed by Colombia, Guatemala, and the Dominican Republic. At the subsequent Inter-American Conference for the Maintenance of Peace, 21 nations pledged to remain neutral in the event of a conflict between any two members. The experience of World War II convinced hemispheric governments that unilateral action could not ensure the territorial integrity of the American nations in the event of external aggression. To meet the challenges of global conflict in the postwar world and to contain conflicts within the hemisphere, they adopted a system of collective security, the Inter-American Treaty of Reciprocal Assistance (Rio Treaty) signed in 1947 in Rio de Janeiro.

The ninth International Conference of American States was held in Bogotá between March and May 1948 and led by United States Secretary of State George Marshall, a meeting which led to a pledge by members to fight communism in the western hemisphere. This was the event that saw the birth of the OAS as it stands today, with the signature by 21 American countries of the Charter of the Organization of American States on 30 April 1948 (in effect since December 1951). The meeting also adopted the American Declaration of the Rights and Duties of Man, the world's first general human rights instrument.

The transition from the Pan American Union to OAS would have been smooth if it had not been for the assassination of Colombian leader Jorge Eliécer Gaitán. The Director General of the former, Alberto Lleras Camargo, became the Organization's first Secretary General. The current Secretary General is former Uruguayan minister of foreign affairs Luis Almagro.

Significant milestones in the history of the OAS since the signing of the Charter have included the following:

In the words of Article 1 of the Charter, the goal of the member nations in creating the OAS was "to achieve an order of peace and justice, to promote their solidarity, to strengthen their collaboration, and to defend their sovereignty, their territorial integrity, and their independence." Article 2 then defines eight essential purposes:

Over the course of the 1990s, with the end of the Cold War, the return to democracy in Latin America, and the thrust toward globalization, the OAS made major efforts to reinvent itself to fit the new context. Its stated priorities now include the following:


The Organization of American States is composed of an Organization of American States General Secretariat, the Permanent Council, the Inter-American Council for Integral Development, and a number of committees.

The General Secretariat of the Organization of American States consists of six secretariats.

The various committees of the Organization of American States include:

The various commissions of the Organization of American States include: 

The OAS has two funds, one for the General Secretariat, and one for specific programs and initiatives. The General Assembly asks for contributions from each member country based on its capacity to pay. In 2018 the General Secretariat's budget was $85 million of which the US contributed $50 million. In 2017 the US contributed $17 million to the fund for specific programmes which was almost a third of the total contributions for that year.

The General Assembly is the supreme decision-making body of OAS. It convenes once every year in a regular session. In special circumstances, and with the approval of two-thirds of the member states, the Permanent Council can convene special sessions.

The Organization's member states take turns hosting the General Assembly on a rotating basis. The states are represented at its sessions by their chosen delegates: generally, their ministers of foreign affairs, or their appointed deputies. Each state has one vote, and most matters—except for those for which the Charter or the General Assembly's own rules of procedure specifically require a two-thirds majority—are settled by a simple majority vote.

The General Assembly's powers include setting the OAS's general course and policies by means of resolutions and declarations; approving its budget and determining the contributions payable by the member states; approving the reports and previous year's actions of the OAS's specialized agencies; and electing members to serve on those agencies.

All 35 independent nations of the Americas are members of the OAS. Upon foundation in 1948, there were 21 members, most of them in Latin America:

The later expansion of the OAS included Canada and the newly independent nations of the Caribbean. Members with later admission dates (sorted chronologically):

Although Canada was a founding member of the League of Nations in 1919 and has joined international organizations since that date, it chose not to join the OAS when it was first formed, despite its close relations with the United States. Canada became a Permanent Observer in the OAS on 2 February 1972. Canada signed the Charter of the Organization of American States on 13 November 1989 and this decision was ratified on 8 January 1990.

In 2004–2005, Canada was the second largest contributor to the OAS, with an annual assessed contribution representing 12.36 percent of the OAS Regular Budget (US$9.2 million) and an additional C$9 million in voluntary contributions to specific projects. Shortly after joining as a full member, Canada was instrumental in the creation of the Unit for the Promotion of Democracy, which provides support for the strengthening and consolidation of democratic processes and institutions in OAS member states.

During the 6th Conference of Foreign Ministers of the Organization of American States (OAS) in Costa Rica, from 16 to 20 August 1960, a conviction against the State of the Dominican Republic was agreed to unanimously. The penalty was motivated because the foreign ministers checked the veracity of the claim that the Rafael Trujillo regime had sponsored an attack against Rómulo Betancourt, at that time, constitutional president of Venezuela. The meeting was attended by foreign ministers from 21 American nations, including Cuba, which at that time had not yet been expelled from the inter-American system.

All countries, including the United States and Haiti, broke off diplomatic relations with the Dominican Republic. Additionally an economic blockade that affected the exports of sugar was applied, which at that time was the pillar of the Dominican economy.

It was the first application of the Inter-American Treaty of Reciprocal Assistance, which had been adopted at the OAS on July 29, 1960.

The current government of Cuba was excluded from participation in the Organization under a decision adopted by the Eighth Meeting of Consultation in Punta del Este, Uruguay, on 31 January 1962. The vote was passed by 14 in favor, with one against (Cuba) and six abstentions (Argentina, Bolivia, Brazil, Chile, Ecuador, and Mexico). The operative part of the resolution reads as follows:

This meant that the Cuban nation was still technically a member state, but that the current government was denied the right of representation and attendance at meetings and of participation in activities. The OAS's position was that although Cuba's participation was suspended, its obligations under the Charter, the American Declaration of the Rights and Duties of Man, etc. still hold: for instance, the Inter-American Commission on Human Rights continued to publish reports on Cuba's human rights situation and to hear individual cases involving Cuban nationals. However, this stance was occasionally questioned by other individual member states.

Cuba's position was stated in an official note sent to the Organization "merely as a courtesy" by Minister of Foreign Affairs Dr. Raúl Roa on 4 November 1964: "Cuba was arbitrarily excluded ... The Organization of American States has no juridical, factual, or moral jurisdiction, nor competence, over a state which it has illegally deprived of its rights."

The reincorporation of Cuba as an active member regularly arose as a topic within the inter-American system for instance, it was intimated by the outgoing ambassador of Mexico in 1998but most observers did not see it as a serious possibility while the present government remained in power. Since 1960, the Cuban administration had repeatedly characterized the OAS as the "Ministry of Colonies" of the United States of America. On 6 May 2005, President Fidel Castro reiterated that the island nation would not "be part of a disgraceful institution that has only humiliated the honor of Latin American nations." After Fidel Castro's recent retirement and the ascent of his brother Raúl to power, this official position was reasserted. Venezuelan President Hugo Chávez promised to veto any final declaration of the 2009 Summit of the Americas due to Cuba's exclusion.

On 17 April 2009, after a "trading of warm words" between the administrations of U.S. President Barack Obama and Cuban leader Raúl Castro, OAS Secretary General José Miguel Insulza said he would ask the 2009 General Assembly to annul the 1962 resolution excluding Cuba.

On 3 June 2009, foreign ministers assembled in San Pedro Sula, Honduras, for the OAS's 39th General Assembly, passed a vote to lift Cuba's suspension from the OAS. The United States had been pressuring the OAS for weeks to condition Cuba's readmission to the group on democratic principles and commitment to human rights. Ecuador's Foreign Minister Fander Falconí said there will be no such conditions. "This is a new proposal, it has no conditions—of any kind," Falconí said. "That suspension was made in the Cold War, in the language of the Cold War. What we have done here is fix a historic error." The suspension was lifted at the end of the General Assembly, but, to be readmitted to the Organization, Cuba will need to comply with all the treaties signed by the Member States, including the Inter-American Democratic Charter of 2001. A statement issued by the Cuban government on 8 June 2009 stated that while Cuba welcomed the Assembly's gesture, in light of the Organization's historical record "Cuba will not return to the OAS."

Following the expulsion of its President Manuel Zelaya, Honduras' membership of the Organization was suspended unanimously at midnight on 5 July 2009. The "de facto" government had already announced it was leaving the OAS hours earlier; this was not, however, taken into account by the OAS, which did not recognize that government as legitimate. An extraordinary meeting had been conducted by the OAS in Washington, D.C., with Zelaya in attendance. The suspension of Honduras was approved unanimously with 33 votes (Honduras did not vote). This was the first suspension carried out by the OAS since that of Cuba in 1962.

After Zelaya's return to Honduras in 2011, the country was re-admitted to the Organization on 1 June 2011 with 32 votes in favor and 1 (Ecuador) against. Venezuela expressed some reservations.

On 28 April 2017 Venezuela notified the OAS of its denunciation of the Charter of the OAS, which as per Article 143 would lead to the withdrawal of Venezuela from the OAS effective two years from the date of notification. During this period, the country did not plan on participating in the OAS.

During the 2019 Venezuelan presidential crisis, the President of the National Assembly of Venezuela Juan Guaidó, who was recognized by the National Assembly as the acting president, sent a letter to the OAS Secretary General annulling the previous denunciation of the OAS Charter, and expressing his desire for Venezuela to remain a member of the OAS. The National Assembly designated a special envoy as representative to the OAS, lawyer Gustavo Tarre Briceño, who the OAS voted to recognize as Venezuela's delegate in April.

As of 31 January 2014, there are 69 permanent observer countries including the four countries with territory or territories in the Americas—Denmark, France, the Netherlands, and the United Kingdom; as well as the European Union.

The Organization's official languages are Spanish, Portuguese, French, and English. The Charter, the basic instrument governing OAS, makes no reference to the use of official languages. These references are to be found in the Rules of Procedure governing the various OAS bodies. Article 51 of the Rules of Procedure of the General Assembly, the supreme body of the OAS, which meets once a year, states that English, French, Portuguese, and Spanish are the four official languages. Article 28 stipulates that a Style Committee shall be set up with representatives of the four official languages to review the General Assembly resolutions and declarations. Article 53 states that proposals shall be presented in the four official languages. The Rules of Procedure and Statutes of other bodies, such as the Inter-American Council for Integral Development (CIDI), the Permanent Executive Committee of the Inter-American Council for Integral Development (CEPCIDI), the Inter-American Commission of Women (CIM), the Inter-American Drug Abuse Control Commission (CICAD), the Inter-American Commission on Human Rights (IACHR) and the Inter-American Juridical Committee (CJI), technical bodies of the OAS, also mention the four official languages in which their meetings are to be conducted. Policy is therefore dictated through these instruments that require use of the four official languages at meetings.

Although a number of other languages have official status in one or more member states of OAS (Dutch in Suriname; Haitian Creole alongside French in Haiti; Quechua and Aymara in Peru, Ecuador and Bolivia; Guaraní in Paraguay), they are not official languages of the Organization.



</doc>
<doc id="22789" url="https://en.wikipedia.org/wiki?curid=22789" title="World Organisation for Animal Health">
World Organisation for Animal Health

The World Organisation for Animal Health, formerly the (OIE) is an intergovernmental organization coordinating, supporting and promoting animal disease control.

The main objective of the OIE is to control epizootic diseases and thus to prevent their spread. Other objectives consist of: transparency, scientific information, international solidarity, sanitary safety, the promotion of Veterinary Services, food safety and animal welfare. It is recognized as a reference organisation by the World Trade Organization (WTO) and in 2018 had a total of 182 member states. Its newest member state is Saint Lucia. The OIE maintains permanent relations with 45 other international and regional organisations and has Regional and sub-regional Offices on every continent. The OIE does not depend on the UN system; its autonomy is both institutional and financial and its activities are governed by its own constitutional texts. Since its first General Session held in Paris, the Office carries out its work under the authority of a Committee consisting of delegates of the contracting Governments.

The need to fight animal diseases at a global level led to the creation of the through the international Agreement signed on January 25, 1924.

In May 2003 the Office became the World Organisation for Animal Health but kept its historical acronym OIE.

In December 2016, 430 delegates to the 4th Global Conference on Animal Welfare approved a range of measures aimed to improve animal welfare. An OIE strategy document which stemmed from this conference was to be presented for adoption at the OIE World Assembly in May 2017.

In January 2017, the outgoing Obama administration designated the OIE as an organization entitled to benefits of the International Organizations Immunities Act.

The OIE's headquarters are located in Paris, in the 17th arrondissement. It was in 1939 that the OIE moved to the aristocratic district of Parc Monceau, after having occupied premises since 1927 near the Champs de Mars and the Eiffel Tower, that had been provided by the French Higher Public Health Council. In May 1938, the OIE Members gave Dr Emmanuel Leclainche, founder and first General Director of the OIE, full powers to buy a townhouse in Paris, using the reserve fund. Dr Lecleinche chose the mansion from four properties selected by a Commission comprising the President of the OIE, H.C.L.E. Berger (Netherlands), the Vice-President, Carlo Bisanti (Italy), and the accountant, Gotlieb Flückiger (Switzerland). On 22 February 1939, the OIE, represented by E. Leclainche bought the mansion from the Marquise de Montebello, at a cost of 700,000 francs.
The 13th General Session of the OIE was held from May 30 to June 5, 1939 at 12 rue de Prony after rebuilding work had been completed. Due to the Second World War, the following General Session did not take place until 1946, from 2 to 5 October. Following their entry into Paris in June 1940, the German occupying forces temporarily closed and sealed the OIE headquarters. The efforts of the President, Gotlieb Flückiger, elected in 1939, resulted in its re-opening.
12 rue de Prony was built in 1879, in the Neo-Renaissance style, by the celebrated architect Jean-Louis Pascal for the Austrian Baron, Jonas Königswater, a former banker and railway owner. A succession of major works to renovate and modernise the headquarters were undertaken by the Directors General elected after E.Leclainche: Gaston Ramon, René Vittoz, Louis Blajan, Jean Blancou and Bernard Vallat. Due to the headlong development of the organisation (tripling of the staff and the budget since 2001), additional premises have been rented at 14 rue de Prony since 2004. On 16 March 2009, the OIE purchased a large part of the building at 14 rue de Prony, adjoining its headquarters.

Timely dissemination of information is crucial to containing outbreaks. The WAHID Interface provides access to all data held within OIE's new World Animal Health Information System (WAHIS). It replaces and significantly extends the former web interface named Handistatus II System.

A comprehensive range of information is available from:




</doc>
<doc id="22790" url="https://en.wikipedia.org/wiki?curid=22790" title="Ozzie Smith">
Ozzie Smith

Osborne Earl "Ozzie" Smith (born December 26, 1954) is an American former baseball shortstop who played in Major League Baseball (MLB) for the San Diego Padres and St. Louis Cardinals from 1978 to 1996. Nicknamed "The Wizard" for his defensive brilliance, Smith set major league records for career assists (8,375) and double plays (1,590) by a shortstop (the latter since broken by Omar Vizquel), as well as the National League (NL) record with 2,511 career games at the position; Smith won the NL Gold Glove Award for play at shortstop for 13 consecutive seasons (1980–1992). A 15-time All-Star, he accumulated 2,460 hits and 580 stolen bases during his career, and he won the NL Silver Slugger Award as the best-hitting shortstop in 1987. He was elected to the Baseball Hall of Fame in his first year of eligibility in 2002. He was also elected to the St. Louis Cardinals Hall of Fame in the inaugural class of 2014.

Smith was born in Mobile, Alabama, but his family moved to Watts, Los Angeles, when he was six years old. While participating in childhood athletic activities, Smith developed quick reflexes; he went on to play baseball in high school and college, at Los Angeles' Locke High School and Cal Poly-San Luis Obispo respectively. Drafted as an amateur player by the Padres, Smith made his major league debut in 1978. He quickly established himself as an outstanding fielder, and he later became known for performing backflips on special occasions while taking his position at the beginning of a game. Smith won his first Gold Glove Award in 1980 and made his first All-Star Game appearance in 1981. When conflict with Padres' ownership developed, he was traded to the Cardinals for shortstop Garry Templeton in 1982.

Upon joining the Cardinals, Smith helped the team win the 1982 World Series. Three years later, his game-winning home run during Game 5 of the 1985 National League Championship Series prompted broadcaster Jack Buck's "Go crazy, folks!" play-by-play call. Despite a rotator cuff injury during the 1985 season, Smith posted career highs in multiple offensive categories in 1987. Smith continued to earn Gold Gloves and All-Star appearances on an annual basis until 1993. During 1995 season, Smith had shoulder surgery and was out nearly three months. After tension with his new manager Tony La Russa developed in 1996, Smith retired at season's end, and his uniform number (No. 1) was subsequently retired by the Cardinals. 

Smith served as host of the television show "This Week in Baseball" from 1997 to 1998.

Smith was born in Mobile, Alabama, the second of Clovi and Marvella Smith's six children (five boys and one girl). While the family lived in Mobile, his father worked as a sandblaster at Brookley Air Force Base. When Smith was six his family moved to the Watts section of Los Angeles. His father became a delivery truck driver for Safeway stores, while his mother became an aide at a nursing home. His mother was an influential part of his life who stressed the importance of education and encouraged him to pursue his dreams.

Smith played a variety of sports in his youth, but considered baseball to be his favorite. He developed quick reflexes through various athletic and leisure activity, such as bouncing a ball off the concrete steps in front of his house, moving in closer to reduce reaction time with each throw. When not at the local YMCA or playing sports, Smith sometimes went with friends to the neighborhood lumberyard, springboarding off inner tubes and doing flips into sawdust piles (a precursor to his famous backflips). In 1965, at age 10, he endured the Watts Riots with his family, recalling that, "We had to sleep on the floor because of all the sniping and looting going on."

While Smith was attending junior high school, his parents divorced. Continuing to pursue his interest in baseball, he would ride the bus for nearly an hour to reach Dodger Stadium, cheering for the Los Angeles Dodgers at about 25 games a year. Upon becoming a student at Locke High School, Smith played on the basketball and baseball teams. Smith was a teammate of future National Basketball Association player Marques Johnson on the basketball team, and a teammate of future fellow Hall-of-Fame player Eddie Murray on the baseball side. After high school Smith attended Cal Poly San Luis Obispo in 1974 on a partial academic scholarship, and managed to walk-on to the baseball team. In addition to his academic education, he learned to switch-hit from Cal Poly coach Berdy Harr. When Cal Poly's starting shortstop broke his leg midway through the 1974 season, Smith subsequently took over the starting role. Later named an All-American athlete, he established school records in career at bats (754) and career stolen bases (110) before graduating in 1977.

Smith was playing semi-professional baseball in Clarinda, Iowa, when in June 1976 he was selected in the seventh round of the amateur entry draft by the Detroit Tigers. The parties could not agree on a contract; Smith wanted a $10,000 ($ today) signing bonus, while the Tigers offered $8,500 ($ today). Smith returned to Cal Poly for his senior year, then in the 1977 draft was selected in the fourth round by the San Diego Padres, ultimately agreeing to a contract that included a $5,000 signing bonus ($ today). Smith spent his first year of professional baseball during 1977 with the Class A Walla Walla Padres of the Northwest League.

Smith began 1978 as a non-roster invitee to the San Diego Padres' spring training camp in Yuma, Arizona. Smith credited Padres manager Alvin Dark for giving him confidence by telling reporters the shortstop job was Smith's until he proved he can't handle it. Even though Dark was fired in the middle of training camp, Smith made his Major League Baseball (MLB) debut on April 7, 1978.

It did not take long for Smith to earn recognition in the major leagues, making what some consider his greatest fielding play only 10 games into his rookie season. The Padres played host to the Atlanta Braves on April 20, 1978, and with two out in the top of the fourth inning, Atlanta's Jeff Burroughs hit a ground ball up the middle. Smith described the play by saying, "He hit a ball back up the middle that everybody thought was going into center field. I instinctively broke to my left and dove behind second. As I was in the air, the ball took a bad hop and caromed behind me, but I was able to catch it with my bare hand. I hit the ground, bounced back up, and threw Burroughs out at first."
During a roadtrip to Houston, later in the season, Smith met a part-time usherette at the Astrodome named Denise while making his way to the team bus outside the stadium. The couple developed a relationship that was sometimes long-distance in nature, and eventually decided to marry. It was also during the 1978 season that Smith introduced a signature move. Padres promotion director Andy Strasberg knew Smith could perform backflips, but that he only did them during practice before fans entered the stadium. Strasberg asked Smith to do a backflip for fans during Fan Appreciation Day on October 1, the Padres' last home game of the season. After conferring with veteran teammate Gene Tenace, Smith went ahead with the backflip, and it proved to be wildly popular. Smith finished the 1978 season with a .258 batting average and .970 fielding percentage, placing second in National League Rookie of the Year voting to Bob Horner.

After working with a hitting instructor during the offseason, Smith failed to record a base hit in his first 32 at-bats of the 1979 season. Among players with enough at-bats to qualify for the 1979 National League Triple Crown, Smith finished the season last in batting average (.211), home runs (0), and RBI (27). Off the field, conflict developed between Padres' ownership and the combination of Smith and his agent, Ed Gottlieb. The parties entered into a contract dispute before the 1980 season, and when negotiations lasted into spring training, the Padres renewed Smith's contract at his 1979 salary of $72,500 Smith's agent told the Padres the shortstop would forgo the season to race in the Tour de France, despite the fact Smith admitted to The Break Room on 96.5 WCMF in Rochester, New York he had never heard of the Tour. Angered by the Padres' attitude during those contract talks, Gottlieb took out a help-wanted ad in the "San Diego Union", part of which read, "Padre baseball player wants part-time employment to supplement income." When Joan Kroc, wife of Padres owner Ray Kroc, publicly offered Smith a job as an assistant gardener on her estate, Smith and Gottlieb's relationship with the organization deteriorated further.

Meanwhile, Smith was winning recognition for his accomplishments on the field. In 1980, he set the single-season record for most assists by a shortstop (621), and began his string of 13 consecutive Gold Glove awards. Smith's fielding play prompted the "Yuma Daily Sun" to use the nickname "The Wizard of Oz" in a March 1981 feature article about Smith. While "The Wizard of Oz" nickname was an allusion to the 1939 motion picture of the same name, Smith also came to be known as simply "The Wizard" during his playing career, as Smith's Baseball Hall of Fame plaque would later attest. In 1981, Smith made his first All-Star Game appearance as a reserve player.

While Smith was having problems with the Padres' owners, the St. Louis Cardinals also found themselves unhappy with their shortstop, Garry Templeton. Templeton's relationship with Cardinal Nation had become increasingly strained and finally came to a head during a game at Busch Stadium on August 26, 1981, when (after being heckled for not running out a ground ball) he made obscene gestures at fans, and had to be physically pulled off the field by manager Whitey Herzog.

Given the task of overhauling the Cardinals by owner Gussie Busch (and specifically to unload Templeton), Herzog was looking to trade Templeton when he was approached by Padres General Manager Jack McKeon at the 1981 baseball winter meetings. While McKeon had previously told Herzog that Smith was untouchable in any trade, the Padres were now so angry at Smith's agent Gottlieb that McKeon was willing to deal. McKeon and Herzog agreed in principle to a six-player trade, with Templeton for Smith as the centerpiece. It was then that Padres manager Dick Williams informed Herzog that a no-trade clause had been included in Smith's 1981 contract. Upon learning of the trade, Smith's initial reaction was to invoke the clause and stay in San Diego, but he was still interested to hear what the Cardinals had to say. While the deal for the players beside Templeton and Smith went through, Herzog flew to San Diego to meet with Smith and Gottlieb over the Christmas holiday. Smith later recalled that, "Whitey told me that with me playing shortstop for the St. Louis Cardinals, we could win the pennant. He made me feel wanted, which was a feeling I was quickly losing from the Padres. The mere fact that Whitey would come all the way out there to talk to us was more than enough to convince me that St. Louis was the place I wanted to be."

On December 10, 1981, the Padres traded him along with a player to be named later and Steve Mura to the Cardinals for a player to be named later, Sixto Lezcano and Garry Templeton. The teams completed the trade on February 19, 1982, with the Padres sending Al Olmsted to the Cardinals, and St. Louis sending Luis DeLeon to the Padres. Herzog believed Smith could improve his offensive production by hitting more ground balls, and subsequently created a motivational tool designed to help Smith concentrate on that task. Approaching Smith one day during spring training, Herzog said, "Every time you hit a fly ball, you owe me a buck. Every time you hit a ground ball, I owe you a buck. We'll keep that going all year." Smith agreed to the wager, and by the end of the season had won close to $300 from Herzog. As the 1982 season got underway, Herzog's newly assembled team won 12 games in a row during the month of April, and finished the season atop the National League East division. Herzog would later say of Smith's contributions that, "If he saved two runs a game on defense, which he did many a night, it seemed to me that was just as valuable to the team as a player who drove in two runs a game on offense."

Smith became a father for the first time during the 1982 season with the birth of his son O.J., today known as Nikko, on April 28. Smith also developed a lasting friendship with teammate Willie McGee during the season, and Smith said he likes to think he "helped Willie get over some of the rough spots of adjusting to the major leagues". Smith later participated in the postseason for the first time when the Cardinals faced the Atlanta Braves in the best-of-five 1982 National League Championship Series (NLCS). Smith drove in the series' first run by hitting a sacrifice fly that scored McGee in Game 1, ultimately going five for nine in St. Louis' three-game series sweep.

Just as Herzog had predicted when he told Smith the Cardinals would win the pennant with him on the team, Smith found himself as the team's starting shortstop in the best-of-seven 1982 World Series against the Milwaukee Brewers. During the contest Smith scored three runs, had five hits, and did not commit an error in the field. When St. Louis was trailing 3–1 with one out in the sixth inning of Game 7, Smith started a rally with a base hit to left field, eventually scoring the first of the team's three runs that inning. The Cardinals scored two more runs in the 8th inning for a 6-3 win and the championship.

After the World Series championship, Smith and the Cardinals agreed on a new contract in January 1983 that paid Smith $1 million per year. Smith was voted in as the National League's starting shortstop in the All-Star Game for the first time in 1983, and at season's end won a fourth consecutive Gold Glove Award. During July of the 1984 season, Smith went on the disabled list with a broken wrist after being hit by a pitch during a game against the Padres. Smith's return to the lineup a month later was not enough to propel the Cardinals to a postseason berth.

In 1985, Smith amassed a .276 batting average, 31 stolen bases, and 591 assists in the field. The Cardinals as a team won 101 games during the season and earned another postseason berth. Facing the Los Angeles Dodgers in the now best-of-seven NLCS, a split of the first four games set the stage for Game 5 at Busch Stadium. With the score tied at two runs apiece in the bottom of the ninth inning, Dodgers manager Tommy Lasorda called upon closer Tom Niedenfuer to pitch. Smith batted left-handed against Niedenfuer with one out. Smith, who had never hit a home run in his previous 3,009 left-handed major league at-bats, pulled an inside fastball down the right-field line for a walk-off home run, ending Game 5 in a 3–2 Cardinals victory. Smith said, "I was trying to get an extra-base hit and get into scoring position. Fortunately, I was able to get the ball up." The home run not only prompted broadcaster Jack Buck's "Go crazy folks" play-by-play call, but was also later voted the greatest moment in Busch Stadium history by Cardinals fans.

After Smith's teammate Jack Clark hit a late-inning home run of his own in Game 6 to defeat the Dodgers, the Cardinals moved on to face the Kansas City Royals in the 1985 World Series. Once again sportswriters were quick to draw attention to Smith's outstanding defensive play instead of his 2 for 23 effort at the plate. After the Cardinals took a three-games-to-two advantage, a controversial Game 6 call by umpire Don Denkinger overshadowed the remainder of the Series (which the Royals won in seven games).

What was not publicly known during the regular season and playoffs was that Smith had torn his rotator cuff after suffering an impingement in his right shoulder during the July 11–14 homestand against the Padres. After suffering the impingement diving back into first base on a pickoff throw, Smith altered his throwing motion to such a degree that the rotator cuff tear subsequently developed. The 5'10" (1.78 m), 180-pound (82 kg) Smith opted to forgo surgery and instead built up his arm strength via weightlifting, playing through whatever pain he encountered. Said Smith, "I didn't tell anybody about the injury, because I wanted to keep playing and didn't want anybody thinking they could run on me or take advantage of the injury. I tried to do almost everything, except throw a baseball, left-handed: opening a door, turning on the radio—everything. It didn't get any better, but it was good enough that I didn't have to have surgery."

Because of his injury, Smith let his then four-year-old son Nikko perform his traditional Opening Day backflip before the Cardinals' first home game of the 1986 season. Smith made an "eye-popping" play later that season on August 4, during a game against the Philadelphia Phillies at Busch Stadium. In the top of the ninth inning, Phillies pinch-hitter Von Hayes hit a short fly ball to left field, which was pursued by both Smith and left fielder Curt Ford. Running with his back to home plate, Smith dove forward, simultaneously catching the ball while parallel to the ground and flying over the diving Ford, avoiding a collision by inches.

After hitting in either the second or eighth spot in the batting order for most of his time in St. Louis, Herzog made Smith the number-two hitter full-time during the 1987 season. Over the course of the year, Smith accrued a .303 batting average, 43 stolen bases, 75 RBIs, 104 runs scored, and 40 doubles, good enough to earn him the Silver Slugger Award at shortstop. In addition to winning the Gold Glove Award at shortstop for the eighth consecutive time, Smith posted a career-high on-base percentage of .392. Smith was also the leading vote-getter in the 1987 All-Star Game. The Cardinals earned a postseason berth with 95 wins, and subsequently faced the San Francisco Giants in the 1987 National League Championship Series. Smith contributed a triple during the series, and the Cardinals won the contest in seven games.

The 1987 World Series matched the Cardinals against the American League champion Minnesota Twins. The home team won every game of the contest, as Minnesota won the series. In 28 at-bats during the Series, Smith scored three runs and had two RBIs. Smith finished second in MVP balloting to Andre Dawson, who had played on the last-place Chicago Cubs, largely because Smith and teammate Jack Clark split the first-place vote. Following the 1987 season, Smith was awarded the largest contract in the National League at $2.34 million.

While the team did not see the postseason for the remainder of the decade, Smith continued to rack up All-Star appearances and Gold Gloves. Combined with the attention he received from his contract, Smith continued to be a national figure. Known as a savvy dresser, he made the April 1988 cover of "GQ" magazine. Smith was witness to change within the Cardinal organization when owner Gussie Busch died in 1989 and Herzog quit as manager during the 1990 season.

Joe Torre became Smith's new manager in 1990, but the team did not reach the postseason during Torre's nearly five-year tenure. While the Cardinals celebrated their 100th anniversary in 1992, Smith marked milestones of his own, stealing his 500th career base on April 26, then notching a triple on May 26 in front of the home crowd for his 2,000th hit. St. Louis had a one-game lead in the National League East division on June 1, 1992, but injuries took their toll on the team, including Smith's two-week illness in late July after contracting chicken pox for the first time. As a testament to his national visibility during this time, Smith appeared in a 1992 episode of "The Simpsons" titled "Homer at the Bat". Smith became a free agent for the first time in his career on November 2, 1992, only to sign a new contract with the Cardinals on December 6.

Smith won his final Gold Glove in 1992, and his 13 consecutive Gold Gloves at shortstop in the National League has yet to be matched. The 1993 season marked the only time between 1981 and 1996 that Smith failed to make the All-Star team, and Smith finished the 1993 season with a .288 batting average and .974 fielding percentage. He appeared in 98 games during the strike-shortened 1994 season, and later missed nearly three months of the 1995 season after shoulder surgery on May 31. Smith was recognized for his community service efforts with the 1994 Branch Rickey Award and the 1995 Roberto Clemente Award. In February 1994, Smith took on the role of honorary chairman and official spokesman for the Missouri Governor's Council on Physical Fitness and Health.

As Smith entered the 1996 season, he finalized a divorce from his wife Denise during the first half of the year. Meanwhile, manager Tony La Russa began his first season with the Cardinals in tandem with a new ownership group. After General Manager Walt Jocketty acquired shortstop Royce Clayton during the offseason, La Russa emphasized an open competition for the spot that would give the Cardinals the best chance to win. When spring training concluded, Smith had amassed a .288 batting average and zero errors in the field, and Clayton batted .190 with eight errors. Smith believed he had earned the position with his spring training performance, but La Russa disagreed, and awarded Clayton the majority of playing time in the platoon situation that developed, where Smith typically saw action every third game. La Russa said, "I think it's fair to say he misunderstood how he compared to Royce in spring training ... When I and the coaches evaluated the play in spring training—the whole game—Royce started very slowly offensively and you could see him start to get better. By what he was able to do defensively and on the bases, Royce deserved to play the majority of the games."

Smith missed the first month of the season with a hamstring injury, and continued to harbor ill feelings toward La Russa that had developed after spring training ended. In a closed-door meeting in mid-May, La Russa asked Smith if he would like to be traded. Instead, Smith and his agent negotiated a compromise with Cardinals management, agreeing to a buyout of special provisions in his contract in conjunction with Smith announcing his retirement. The agreement prompted a press conference at Busch Stadium on June 19, 1996, during which Smith announced he would retire from baseball at season's end.
As Smith made his final tour of the National League, he was honored by many teams, and received a standing ovation at the 1996 All-Star Game in Philadelphia. Between June 19 and September 1, Smith's batting average increased from .239 to .286. On September 2 Smith tied a career high by scoring four runs, one of which was a home run, and another on a close play at home plate in the bottom of the 10th inning against division leader Houston. The victory moved the Cardinals to within a half game of Houston in the National League Central Division, and the Cardinals went on to win the division by six games. The Cardinals held a special ceremony at Busch Stadium on September 28, 1996, before a game against the Cincinnati Reds, honoring Smith by retiring his uniform number. Noted for his ritual backflip before Opening Days, All-Star Games, and postseason games, Smith chose this occasion to perform it for one of the last times.

In the postseason, the Cardinals first faced the San Diego Padres in the 1996 National League Division Series. After sitting out Game 1, Smith got the start in Game 2 at Busch Stadium, helping his team go up two games in the series by notching a run, a hit and two walks at the plate, along with an assist and a putout in the field. The Cardinals then swept the series by winning Game 3 in San Diego.

The Cardinals faced the Atlanta Braves in the 1996 National League Championship Series. Smith started Game 1 and subsequently registered three putouts and one assist in the field, but went hitless in four at-bats in the Cardinals' 4–2 loss. The Cardinals then won Games 2, 3, and 4, contests in which Smith did not appear. Upon receiving the start in Game 5, Smith nearly duplicated his Game 1 performance with four putouts, one assist, and no hits in four at-bats as part of another Cardinals defeat. The Cardinals also failed to win Game 6 or Game 7 in Atlanta, ending their season. When the Cardinals were trailing by 10 runs during Game 7 on October 17, Smith flied out to right field while pinch-hitting in the sixth inning, marking the end of his playing career. 

Smith finished his career with distinctions ranging from the accumulation of more than 27.5 million votes in All-Star balloting, to holding the record for the most MLB at-bats without hitting a grand slam.

Upon retirement, Smith took over for Mel Allen as the host of the television series "This Week in Baseball" ("TWIB") in 1997. Smith also became color commentator for the local broadcast of Cardinals games on KPLR-TV from 1997 to 1999. When his stint on "This Week in Baseball" concluded, Smith then moved on to do work for CNN-SI beginning in 1999. After La Russa retired as manager of the Cardinals in 2011, Smith became active in the organization again, starting with his stint as a special instructor for the team's 2012 spring training camp.

On January 8, 2002 Smith learned via a phone call he had been elected to the Baseball Hall of Fame on his first ballot by receiving 91.7% of the votes cast. As it happened, the Olympic torch was passing through St. Louis on its way to Salt Lake City for the 2002 Winter Olympics, and Smith served as a torchbearer in a ceremony with St. Louis Rams' quarterback Kurt Warner that evening. Smith was inducted into the Hall of Fame during ceremonies on July 28, 2002. During his speech, he compared his baseball experiences with the characters from the novel "The Wonderful Wizard of Oz", after which his son Dustin presented his Hall of Fame plaque. Days later on August 11, Smith was back at Busch Memorial Stadium for the unveiling of a statue in his likeness made by sculptor Harry Weber. Weber chose to emphasize Smith's defensive skills by showing Smith stretched horizontal to the ground while fielding a baseball. At the ceremony Weber told Smith, "You spent half of your career up in the air. That makes it difficult for a sculptor to do something with it."

Smith has also been an entrepreneur in a variety of business ventures. Smith opened "Ozzie's" restaurant and sports bar in 1988, started a youth sports academy in 1990, became an investor in a grocery store chain in 1999, and partnered with David Slay to open a restaurant in the early 2000s. Of those businesses the youth academy remains in operation, with the restaurant having closed in 2010 after changing ownership and locations once. Aside from appearing in numerous radio and television commercials in the St. Louis area since retiring from baseball, Smith authored a children's book in 2006 and launched his own brand of salad dressing in 2008.

Besides the National Baseball Hall of Fame, Smith has been also inducted or honored in other halls of fame and recognitions. In 1999, he ranked number 87 on "The Sporting News"' list of the 100 Greatest Baseball Players, and finished third in voting at shortstop for the Major League Baseball All-Century Team. He was honored with induction into the Missouri Sports Hall of Fame, Alabama Sports Hall of Fame and the St. Louis Walk of Fame, and received an honorary Doctor of Humane Letters degree from Cal Poly. In January 2014, the Cardinals announced Smith among 22 former players and personnel to be inducted into the St. Louis Cardinals Hall of Fame Museum for the inaugural class of 2014.

Smith is the father to three children from his marriage to former wife Denise; sons Nikko and Dustin, and daughter Taryn. Smith remains a visible figure around the St. Louis area, making varied appearances like playing the role of the Wizard in the St. Louis Municipal Opera's summer 2001 production of "The Wizard of Oz". Smith cheered on his son Nikko as he cracked the top 10 finalists of the 2005 edition of "American Idol". In 2012, Smith made news headlines again, when he sold all of his Gold Gloves at auction together for more than $500,000.





</doc>
<doc id="22791" url="https://en.wikipedia.org/wiki?curid=22791" title="Boeing OC-135B Open Skies">
Boeing OC-135B Open Skies

The OC-135B Open Skies United States Air Force observation aircraft supports the Treaty on Open Skies. The aircraft, a modified WC-135B, flies unarmed observation flights over participating parties of the treaty. Three OC-135B aircraft were modified by the Aeronautical Systems Center's 4950th Test Wing at Wright-Patterson Air Force Base in Ohio. The first operationally-capable OC-135B was assigned to the 24th Reconnaissance Squadron at Offutt AFB in October 1993. It is now fitted with a basic set of navigational and sensor equipment, and was placed in inviolate storage at the Aerospace Maintenance and Regeneration Center at Davis-Monthan Air Force Base near Tucson, Arizona in 1997. Two fully operational OC-135B aircraft were delivered in 1996 with the full complement of treaty-allowed sensors, which includes an infrared line scanner, synthetic aperture radar and video scanning sensors.

The interior seats 35 people, including the cockpit crew, aircraft maintenance crew, foreign country representatives and crew members from the U.S. Department of Defense's Defense Threat Reduction Agency (DTRA). Cameras installed include one vertical and two oblique KS-87E framing cameras used for low-altitude photography approximately 3,000 feet (900 m) above the ground, and one KA-91C panoramic camera, which scans from side to side to provide a wide sweep for each picture used for high-altitude photography at approximately .

The data annotation and recording system (DARMS) processes navigational, altitude, time and camera signals to annotate each picture with correct position, altitude, time, roll angle and other information. In addition, this system records every picture taken according to camera, frame and navigational position. A keyboard with trackball is the input device for operation of this system. Two Barco VGA color monitors display camera annotation and other camera data on screen for the sensor operator and observer use.

Camera control, located in the sensor operator's console, operates and adjusts individual cameras for cloud cover, frame overlap and other functions. The sensor operator console seats four and has all the equipment listed above plus camera bay heating control, chronometers, emergency oxygen, interphone and individual lighting. The flight following console also seats four and includes most of the equipment listed above except for DARMS and camera controls.

Seven commercial Norcold Tek II coolers with individual refrigeration units maintain temperature and humidity control to maintain peak film performance. The units can be removed, if necessary, from the aircraft in order to transport film. The coolers are capable of storing of film.

The aircraft flies on its intended flight path throughout the entire mission with no reliance on ground-based navigation devices. A top-of-the-line commercial system, Litton 92 INS/GPS, which is an integrated inertial navigation system (INS) with a global positioning system (GPS), provides continuous updates. The GPS updates the INS several times per second to correct any deviations in the flight path. The INS also feeds precise latitude, longitude, time, roll angle and barometric altitude to the DARMS and camera systems. A true airspeed computer feeds true airspeed data to the INS.

A combined altitude radar altimeter provides precise height above ground information to the pilot for navigational purposes as well as a signal to DARMS for film annotation. It is accurate from above the ground level. Plus, a metric altimeter is installed on the pilot's instrument panel for altitude reference when flying in countries that use meters for altitude reference.

The aircraft are being upgraded with the Block 30 Pacer Crag Navigational System upgrade, a first step in making them compliant with ICAO mandated Global Air Traffic Management and Global Air Navigation Standards guidelines.

The OC-135B modifications center around four cameras installed in the rear of the aircraft. Since its primary mission is to take pictures, most of the installed equipment and systems provide direct support to the cameras and the camera operator. Other modifications to the aircraft also included installing an auxiliary power unit, crew luggage compartment, sensor operator console, flight following console and upgraded avionics. Though the aircraft feature a large window in the cargo door, this is simply a remnant of their previous weather reconnaissance role.

Other modifications support the aircrew. A gaseous oxygen system replaced the liquid oxygen system to be more compatible with foreign airfields, and fluorescent lighting system was added throughout the cabin to provide adequate lighting for operation and inspections. Four upgraded seats with a conference table, interphone, lighting and oxygen comprise the mission commanders' station for both countries' mission commanders. A four channel interphone system enables segregated communications between various elements on board. 
The auxiliary power unit enables the aircraft to start engines and provides electrical power and cabin heat independent of ground support equipment. It was manufactured by Allied Signal with the installation and design of the installation by E-Systems and World Auxiliary Power Company.

The aircraft are assigned to Air Combat Command at the 55th Wing, 45th Reconnaissance Squadron, Offutt Air Force Base near Omaha, Nebraska, for operations, training and maintenance. When tasked, ACC's role is to transport a DTRA observation team to an Open Skies point of entry airport, conduct the observation flight, and then return the team to the continental United States.

This article includes public domain text from the following United States Government source:


</doc>
<doc id="22792" url="https://en.wikipedia.org/wiki?curid=22792" title="Treaty on Open Skies">
Treaty on Open Skies

The Treaty on Open Skies entered into force on January 1, 2002, and currently has 35 party states. It establishes a program of unarmed aerial surveillance flights over the entire territory of its participants. The treaty is designed to enhance mutual understanding and confidence by giving all participants, regardless of size, a direct role in gathering information about military forces and activities of concern to them. Open Skies is one of the most wide-ranging international efforts to date promoting openness and transparency of military forces and activities. The concept of "mutual aerial observation" was initially proposed to Soviet Premier Nikolai Bulganin at the Geneva Conference of 1955 by President Dwight D. Eisenhower; however, the Soviets promptly rejected the concept and it lay dormant for several years. The treaty was eventually signed as an initiative of U.S. president (and former Central Intelligence Agency Director) George H. W. Bush in 1989. Negotiated by the then-members of NATO and the Warsaw Pact, the agreement was signed in Helsinki, Finland, on March 24, 1992.

This treaty is not related to civil-aviation open skies agreements.

The 34 state parties to the Open Skies Treaty are: Belarus, Belgium, Bosnia and Herzegovina, Bulgaria, Canada, Croatia, the Czech Republic, Denmark (including Greenland), Estonia, Finland, France, Georgia, Germany, Greece, Hungary, Iceland, Italy, Latvia, Lithuania, Luxembourg, the Netherlands, Norway, Poland, Portugal, Romania, the Russian Federation, Slovakia, Slovenia, Spain, Sweden, Turkey, Ukraine, the United Kingdom
. Kyrgyzstan signed the treaty but has not yet ratified it. Canada and Hungary are the Depositories of the treaty in recognition of their special contributions to the Open Skies process. Depository countries maintain treaty documents and provide administrative support.

The Open Skies treaty is one of unlimited duration, and is open to accession by other states. Republics of the former Soviet Union (U.S.S.R.) that have not already become state parties to the treaty may join it at any time. Applications from other interested countries are subject to a consensus decision by the Open Skies Consultative Commission (OSCC). Eight countries have joined into the treaty since it entered into force in 2002: Bosnia and Herzegovina, Croatia, Estonia, Finland, Latvia, Lithuania, Slovenia, and Sweden. Notably missing are Austria, Cyprus, Ireland, Switzerland, Serbia, Montenegro, Albania, North Macedonia, Moldova, Armenia and Uzbekistan. The Republic of Cyprus submitted its application to accede the Treaty in 2002; since then, however, Turkey has blocked its accession.

The Open Skies Consultative Commission is the implementing body for the Treaty on Open Skies. It comprises representatives from each state party to the treaty and meets monthly at the Vienna headquarters of the Organization for Security and Co-Operation in Europe.

The Open Skies regulations covers the territory over which the parties exercise sovereignty, including mainland, islands, and internal and territorial waters. The treaty specifies that the entire territory of a member state is open to observation. Observation flights may only be restricted for reasons of flight safety and not for reasons of national security.

Observation aircraft may be provided by either the observing party or by the observed party (the "taxi option"), at the latter's choice. All Open Skies aircraft and sensors must pass specific certification and pre-flight inspection procedures to ensure that they are compliant with treaty standards. 

The official certified U.S. Open Skies aircraft is the OC-135B Open Skies.

Canada uses a C-130 Hercules aircraft equipped with a "SAMSON" sensor pod to conduct flights over other treaty nations. The pod is a converted CC-130 fuel tank modified to carry the permitted sensors, along with associated on-board mission systems. A consortium of nations consisting of Belgium, Netherlands, Luxembourg, Canada, France, Greece, Italy, Portugal and Spain own and operate this system. The costs of maintaining the SAMSON Pod are shared, based on each nation's flight quota and actual use.

Bulgaria, Romania, Russia and Ukraine use the Antonov An-30 for their flights. The Czech Republic also used to use the An-30 for this purpose but apparently retired all of theirs from service in 2003.

Russia also uses a Tu-154M-ON monitoring aircraft. Germany formerly used this type as well until the aircraft was lost in a 1997 accident. Russia is phasing out both An-30 and Tu-154M-ON and replacing them with two Tu-214ON with the registrations RA-64519 and RA-64525. This aircraft's new sensor suite, though, is being challenged by the US.

Sweden uses a Saab 340 aircraft ("OS-100") that was certified in 2004.

Until 2008, the UK designated aircraft was an Andover C.1(PR) aircraft, registration XS596. Since then the UK has used a variety of aircraft including a Saab 340, an An-30 and an OC-135.

In 2017, the German Air Force purchased an Airbus A319 as its future Open Skies aircraft.

Open Skies aircraft may have video, optical panoramic and framing cameras for daylight photography, infra-red line scanners for a day/night capability, and synthetic aperture radar for a day/night all weather capability. Photographic image quality will permit recognition of major military equipment (e.g., permit a member state to distinguish between a tank and a truck), thus allowing significant transparency of military forces and activities. Sensor categories may be added and capabilities improved by agreement among member states. All sensors used in Open Skies must be commercially available to all signatories. Imagery resolution is limited to 30 centimetres.

Each state party is obligated to receive observation flights per its passive quota allocation. Each state party may conduct as many observation flightsits active quotaas its passive quota. During the first three years after entry into force, each state was obligated to accept no more than seventy-five percent of its passive quota. Since the overall annual passive quota for the United States is 42, this means that it was obligated to accept no more than 31 observation flights a year during this three-year period. Only two flights were requested over the United States during 2005, by the Russian Federation and Republic of Belarus Group of states parties (which functions as a single entity for quota allocation purposes). The United States is entitled to 8 of the 31 annual flights available over Russia/Belarus. Additionally, the United States is entitled to one flight over Ukraine, which is shared with Canada.

Imagery collected from Open Skies missions is available to any state party upon request for the cost of reproduction. As a result, the data available to each state party is much greater than that which it can collect itself under the treaty quota system.

At a Geneva Conference meeting with Soviet Premier Nikolai Bulganin in 1955, President Eisenhower proposed that the United States and Soviet Union conduct surveillance overflights of each other's territory to reassure each country that the other was not preparing to attack. The fears and suspicions of the Cold War led Soviet General Secretary Nikita Khrushchev to reject Eisenhower's proposal.

During a five-day summit conference held in Geneva, Switzerland, at the end of July 1955, the Soviet Union and United States held serious talks about disarmament and the United States put forward proposals for mutual reconnaissance flights over each other's air space, known as the Open Skies proposal. The United States had a large number of RB-47 and RB-36 reconnaissance aircraft at its disposal for such activities. However, the Soviets turned down this proposal. This Geneva Conference was nonetheless accepted as a turning point in the Cold War. The tensions in Europe were felt to be a stalemate. Both the Soviet Union and United States were willing to talk about their differences, rather than increase them into a state of war.

Thirty-four years later, the Open Skies concept was reintroduced by President George H. W. Bush as a means to build confidence and security between all North Atlantic Treaty Organisation (NATO) and Warsaw Pact countries. In February 1990, an international Open Skies conference involving all NATO and Warsaw Pact countries opened in Ottawa, Canada. Subsequent rounds of negotiations were held in Budapest, Hungary; Vienna, Austria; and Helsinki, Finland.

On March 24, 1992, the Open Skies Treaty was signed in Helsinki by Secretary of State James Baker and foreign ministers from 23 other countries. The treaty entered into force on January 2, 2002, after Russia and Belarus completed ratification procedures.

In November 1992, President Bush assigned responsibility for overall training, management, leadership, coordination and support for U.S. Open Skies observation missions to the On-Site Inspection Agency (OSIA), now a part of the Defense Threat Reduction Agency (DTRA). Until entry into force in January 2002, DTRA support for the treaty involved participating in training and joint trial flights (JTFs). The U.S. has conducted over 70 JTFs since 1993. By March 2003, DTRA had successfully certified 16 camera configurations on the OC-135B aircraft. They also had contributed to the certification of the Bulgarian An-30, Hungarian An-26, SAMSON POD Group (see above) C-130H, Romanian An-30, Russian An-30, and Ukrainian An-30. The United States successfully flew its first Open Skies mission over Russia in December 2002.

With entry into force of the treaty, formal observation flights began in August 2002. During the first treaty year, state parties conducted 67 observation flights. In 2004, state parties conducted 74 missions, and planned 110 missions for 2005. On March 8 and 9, 2007, Russia conducted overflights of Canada under the Treaty. The OSCC continues to address modalities for conducting observation missions and other implementation issues.

Since 2002 a total of 40 missions have taken place over the U.K. There were 24 quota missions conducted by: Russia – 20; Ukraine – three; and Sweden – one. There were 16 training flights conducted by: Benelux (joint with Estonia); Estonia (joint with Benelux); Georgia – three (one joint with Sweden); Sweden – three (one joint with Georgia); US – three; Latvia; Lithuania; Romania; Slovenia; and Yugoslavia. Also since 2002 the U.K. has undertaken a total of 51 open skies missions – 38 were quota missions to the following countries: Ukraine (five); Georgia (seven) and Russia (26); 13 missions were training missions to the following nations: Bulgaria; Yugoslavia; Estonia; Slovenia (three); Sweden (three); US; Latvia, Lithuania and the Benelux. The flights cost approximately £50,000 per operational mission, and approximately £25,000 for training missions with an approximate annual cost of £175,000.

Russian Defence Ministry spokesman stated on 4 February 2016 that Turkey had refused a Russian Open Skies mission, planned to take place in 1–5 February 2016, to fly over areas adjacent to Syria, as well as over NATO airbases. According to Russia, Turkey gave no explanation regarding the limitations, and claimed them to indicate illegal military activity in Syrian territory. The OSCC has not commented on the alleged violation of the Treaty by Turkey.

By 2016, Russian aircraft had been using equipment upgraded over initial equipment.

Both Russia and the United States have alleged that the other is violating the provisions of the treaty. U.S. Secretary of State Mike Pompeo cited for example Russia's access refusal in the Russian-controlled areas of Georgia. In 20 September 2019, the U.S. and Canada were denied access to a military exercise in central Russia.

In October 2019, according to documents from the House of Representatives, the United States' President is considering a U.S. withdrawal from the Open Skies Treaty. NATO allies and partners, in particular Ukraine, are against the move, fearing it would license Russia to reduce further or ban overflights, thus reducing their knowledge of Russian military movements.

In April 2020, it was reported that Secretary of State Mike Pompeo and Secretary of Defense Mark Esper have agreed to proceed with U.S. withdrawal from the Treaty on Open Skies. On May 21, 2020, President Trump announced that the United States would be withdrawing from the treaty due to alleged Russian violations.




</doc>
<doc id="22794" url="https://en.wikipedia.org/wiki?curid=22794" title="Limited overs cricket">
Limited overs cricket

Limited overs cricket, also known as one-day cricket, is a version of the sport of cricket in which a match is generally completed in one day, which includes List A cricket and Twenty20 cricket. The name reflects the rule that in the match each team bowls a set maximum number of overs, usually between 20 and 50, although shorter and longer forms of limited overs cricket have been played.

The concept contrasts with Test and first-class matches, which can take up to five days to complete. One-day cricket is popular with spectators as it can encourage aggressive, risky, entertaining batting, often results in cliffhanger endings, and ensures that a spectator can watch an entire match without committing to five days of continuous attendance.

Each team bats only once, and each innings is limited to a set number of overs, usually fifty in a One Day International and between forty and sixty in a List A. List A is a classification of the limited-overs (one-day) form of cricket, technically as the domestic level.

Despite its name, important one-day matches, international and domestic, often have two days set aside, the second day being a "reserve" day to allow more chance of the game being completed if a result is not possible on the first day (for instance if play is prevented or interrupted by rain).

As mentioned above, in almost all competitive one-day games, a restriction is placed on the number of overs that may be bowled by any one bowler. This is to prevent a side playing two top-class bowlers with extremely good stamina who can bowl throughout their opponents' innings. The usual limitation is set so that a side must include at least five players who bowl. For example, the usual limit for twenty-over cricket is four overs per bowler, for forty-over cricket eight per bowler and for fifty-over cricket ten per bowler. There are exceptions: Pro Cricket in the United States restricted bowlers to five overs each, thus leaving a side requiring only four bowlers.

The idea for a one-day, limited 50-over cricket tournament, was first played in the inaugural match of the All India Pooja Cricket Tournament in 1951 at Tripunithura in Kochi, Kerala. It is thought to be the brain child of KV Kelappan Thampuran, a former cricketer and the first Secretary of the Kerala Cricket Association. The one day limited over cricket game was later adapted and played between English county teams for the first instance on 2 May 1962. Leicestershire beat Derbyshire and Northamptonshire beat Nottinghamshire over 65 overs in the "Midlands Knock-Out Cup", which Northamptonshire went on to win a week later. The following year, the first full-scale one-day competition between first-class teams was played, the knock-out Gillette Cup, won by Sussex. The number of overs was reduced to 60 for the 1964 season. League one-day cricket also began in England, when the John Player Sunday League was started in 1969 with forty over matches. Both these competitions have continued every season since inauguration, though the sponsorship has changed. There is now one 50 over competition, which is called the Royal London One-Day Cup.

The first Limited Overs International (LOI) or One-Day International (ODI) match was played in Melbourne in 1971, and the quadrennial cricket World Cup began in 1975. Many of the "packaging" innovations, such as coloured clothing, were as a result of World Series Cricket, a "rebel" series set up outside the cricketing establishment by Australian entrepreneur Kerry Packer. For more details, see History of cricket.

Twenty20, a curtailed form of one-day cricket with 20 overs per side, was first played in England in 2003. It has proven very popular, and several Twenty20 matches have been played between national teams. It makes several changes to the usual laws of cricket, including the addition of a "bowl-out" (similar to a penalty shoot-out in football) to decide the result of tied matches, which was subsequently dispensed in favour of a Super Over.

100-ball cricket, another form of one-day cricket with 100 deliveries per side, will launch in England in 2021. It is designed to further shorten game time and hopes to attract a new audience. It makes further changes to the usual laws of cricket, including the addition of one 10-ball over which is bowled by each side in addition to 15 traditional 6-ball overs.

One Day International matches are usually played in brightly coloured clothing often in a "day-night" format where the first innings of the day occurs in the afternoon and the second occurs under stadium lights.

In the early days of ODI cricket, the number of overs was generally 60 overs per side, and matches were also played with 40, 45 or 55 overs per side, but now it has been uniformly fixed at 50 overs.

Every four years, the Cricket World Cup involves all the Test-playing nations and other national sides who qualify through the ICC World Cup Qualifier. It usually consists of round-robin stages, followed by semi-finals and a final. The International Cricket Council (ICC) determines the venue far in advance.

The ICC Champions Trophy also involves all the Test-playing nations, and is held between World Cups. It usually consists of a round-robin group stage, semifinals, and a final.

Each Test-playing country often hosts triangular tournaments, between the host nation and two touring sides. There is usually a round-robin group stage, and then the leading two teams play each other in a final, or sometimes a best-of-three final. When there is only one touring side, there is still often a best-of-five or best-of-seven series of limited overs matches.

The ICC World Cricket League is an ODI competition for national teams with Associate or Affiliate status.

Domestic one-day competitions exist in almost every country where cricket is played.

List A cricket is a classification of the limited-overs (one-day) form of the sport of cricket. Much as domestic first-class cricket is the level below international Test match cricket, so List A cricket is the domestic level of one-day cricket below One Day Internationals. Twenty20 matches do not qualify for the present.

Most cricketing nations have some form of domestic List A competition. The number of overs in List A cricket ranges from forty to sixty overs per side.

The Association of Cricket Statisticians and Historians created this category for the purpose of providing an equivalent to first-class cricket, to allow the generation of career records and statistics for comparable one-day matches. Only the more important one-day competitions in each country, plus matches against a touring Test team, are included. The categorisation of cricket matches as "List A" was not officially endorsed by the International Cricket Council until 2006, when the ICC announced that it and its member associations would be determining this classification in a manner similar to that done for first class matches.

Matches that qualify as List A:

Matches that do not qualify as List A:

The JLT One Day Cup is a 50 overs tournament held since 1969. The sides that compete are the following: 

In 2006 Cricket Australia introduced the KFC Twenty20 Big Bash which was amongst the state teams (as above). In 2011 this was expanded to the KFC Twenty20 Big Bash League, consisting of teams based in the capital cities of Australia. The teams are as follows:

The Dhaka Premier Division Cricket League is sponsored by Walton. It has been Bangladesh's List A competition since the 2013–14 season. Twelve teams compete; the bottom two are relegated each year and replaced by the top two from the league below. In 2017-18 the teams were:

Bangladesh's T20 competition is the Bangladesh Premier League. It has been contested annually since the 2011–12 season. Seven teams take part:


Each county has a team representing them in each league and are as followed with their home ground:

North Group:

Birmingham Bears (Edgbaston)
Derbyshire Falcons (Derby County Ground)
Durham Jets (Riverside Ground)
Lancashire Lightning (Old Trafford)
Leicestershire Foxes (Grace Road)
Northamptonshire Steelbacks (Northampton County Ground)
Nottinghamshire Outlaws (Trent Bridge)
Worcestershire Rapids (New Road)
Yorkshire Vikings (Headingley)

South Group:

Essex Eagles (Chelmsford County Ground)
Glamorgan Dragons (Sophia Gardens)
Gloucestershire (Bristol County Ground)
Hampshire Royals (Rose Bowl)
Kent Spitfires (St Lawrence Ground)
Middlesex (Lord's)
Somerset (Taunton County Ground)
Surrey (The Oval)
Sussex Sharks (Hove County Ground)




The Pakistani domestic competition changes regularly, but for 2005–06 there are plans for three one-day tournaments for men.

National Bank Cup: A two-week tournament in February and March between city teams, divided into the Gold League (with seven teams) and Silver League (with six teams). The teams play each other once, with the top two teams qualifying for the final in each individual League, so no team from the Gold League will meet a Silver League team.

Gold League teams:

Silver League teams:

National Bank Patron's Cup: A two-week tournament running just before the National Bank Cup, with one group of five teams and another group of six teams. The top two teams from each group proceed to the semi-final. The teams that compete are:

National Bank Twenty20 Cup: A tournament running one week in mid-March. The same groups apply as in the NATIONAL BANK Cup, and there will be two semi-finals and a final following the group stages. The tournament will be held in Karachi and Lahore.

Pakistan Super League
— a professional franchise Twenty20 men's cricket league. The league is headquartered in Lahore, consists of five franchises nominally representing cities in
Pakistan . It is operated by the Pakistan Cricket Board (PCB) and was established in 2016.Following are the teams:

The local competition in South Africa is the Standard Bank Cup (formerly Benson & Hedges Series) played between 6 teams:


The games are 45-overs, and based on a home-and-away round-robin match system (each team plays ten matches) with semi-finals and a final. The Eagles were the winners of the 2004/2005 and 2005/2006 competitions.

20 teams compete in the Premier Limited-Overs Tournament, which is an expansion from 16 in the last season. Games are played over 50 overs per side, and the teams are divided into two groups, where each team meets the other once over a period of a month. The four top teams from each group qualify for the quarter-finals, and there is then a direct knock-out system until a winner is found after three knock-out stages. The competing teams are:
The NAGICO Regional Super50 is the main regional one-day competition in the West Indies. In recent years, it has been run over a week's time as a group stage followed by knock-out stages. Trinidad and Tobago have won the most titles.

Current teams (2018–19):

The world record for the highest innings total in any List A limited overs match is 496 for 4 by Surrey against Gloucestershire in their Friends Provident Trophy 50-overs match at the Oval, London on 29 April 2007. That surpassed the 443 for nine by Sri Lanka against the Netherlands in their One Day International 50-overs match at Amstelveen on 4 July 2006, which was the record ODI score at the time. On 19 June 2018, England set a new international record, totalling 481 for 6 against Australia at Trent Bridge. The lowest ever total is 23 by Yorkshire against Middlesex at Headingley in 1974 in a 40-overs match. The record low score in ODIs was set by Zimbabwe, who managed just 35 against Sri Lanka in Harare on 25 April 2004.

The most runs scored by both sides in any List A limited overs match is 872: Australia, batting first, scored 434 for four in 50 overs, and yet were beaten by South Africa who scored 438 for nine with a ball to spare during their One Day International at Johannesburg in 2006.

The highest individual innings is 268 by Ali Brown for Surrey against Glamorgan in a 50-overs match at The Oval in 2002. The best bowling figures are eight for 15 by Rahul Sanghvi for Delhi against Himachal Pradesh in a 50-overs match at Una in 1997. The highest international individual innings is by Rohit Sharma who scored 264. The highest score in any formal limited overs match is believed to be United's 630 for five against Bay Area in a 45 overs match at Richmond, California in August 2006.

The most runs in an over was scored by Herschelle Gibbs of the South African cricket team when, in the 2007 Cricket World Cup in the West Indies, he hit 6 sixes in one over bowled by Daan van Bunge of the Netherlands.

This record is shared by Yuvraj Singh of India who achieved this feat in the 2007 ICC World Twenty20 in South Africa, he hit 6 sixes in an over bowled by Stuart Broad of England.

Sachin Tendulkar holds the record of being the first male cricketer to score a double century in ODIs (200 not out). He achieved this feat against South Africa on 24 February 2010, at Gwalior, India. Virender Sehwag is the second male cricketer to score a double century, when he scored 219 before being caught out against West Indies on 8 December 2011, at Indore, India. Rohit Sharma became the third male cricketer to score a double century, when he scored 209 against Australia on 2 November 2013.



</doc>
<doc id="22796" url="https://en.wikipedia.org/wiki?curid=22796" title="Organization for Security and Co-operation in Europe">
Organization for Security and Co-operation in Europe

The Organization for Security and Co-operation in Europe (OSCE) is the world's largest security-oriented intergovernmental organization. Its mandate includes issues such as arms control, promotion of human rights, freedom of the press, and fair elections. It employs around 3,460 people, mostly in its field operations but also in its secretariat in Vienna, Austria, and its institutions. It has its origins in the 1975 Conference on Security and Co-operation in Europe (CSCE) held in Helsinki, Finland.

The OSCE is concerned with early warning, conflict prevention, crisis management, and post-conflict rehabilitation. Its 57 participating countries are located in Europe, northern and central Asia, and North America. The participating states cover much of the land area of the Northern Hemisphere. It was created during the Cold War era as an East–West forum.

The Organization has its roots in the 1973 Conference on Security and Co-operation in Europe (CSCE). Talks had been mooted about a European security grouping since the 1950s but the Cold War prevented any substantial progress until the talks at Dipoli in Espoo began in November 1972. These talks were held at the suggestion of the Soviet Union which wished to use the talks to maintain its control over the communist countries in Eastern Europe, and President of Finland Urho Kekkonen hosted them in order to bolster his policy of neutrality. Western Europe, however, saw these talks as a way to reduce the tension in the region, furthering economic cooperation and obtaining humanitarian improvements for the populations of the Communist bloc.

The recommendations of the talks, in the form of "The Blue Book", gave the practical foundations for a three-stage conference called the "Helsinki process". The CSCE opened in Helsinki on 3 July 1973 with 35 states sending representatives. Stage I only took five days to agree to follow the Blue Book. Stage II was the main working phase and was conducted in Geneva from 18 September 1973 until 21 July 1975. The result of Stage II was the Helsinki Final Act which was signed by the 35 participating states during Stage III, which took place in Finlandia Hall from 30 July – 1 August 1975. It was opened by Holy See’s diplomat Cardinal Agostino Casaroli, who was chairman of the conference.

The concepts of improving relations and implementing the act were developed over a series of follow-up meetings, with major gatherings in Belgrade (4 October 19778 March 1978), Madrid (11 November 19809 September 1983) and Vienna (4 November 198619 January 1989).

The fall of the Soviet Union required a change of role for the CSCE. The Charter of Paris for a New Europe, signed on 21 November 1990, marked the beginning of this change. With the changes capped by the renaming of the CSCE to the OSCE on 1 January 1995, in accord with the results of the conference held in Budapest, Hungary, in 1994. The OSCE now had a formal secretariat, Senior Council, Parliamentary Assembly, Conflict Prevention Centre, and Office for Free Elections (later becoming the Office for Democratic Institutions and Human Rights).

In December 1996, the "Lisbon Declaration on a Common and Comprehensive Security Model for Europe for the Twenty-First Century" affirmed the universal and indivisible nature of security on the European continent.

In Istanbul on 19 November 1999, the OSCE ended a two-day summit by calling for a political settlement in Chechnya and adopting a Charter for European Security. According to then Minister of Foreign Affairs Igor Ivanov, this summit marked a turning point in Russian perception of the OSCE, from an organization that expressed Europe's collective will, to an organization that serves as a Western tool for "forced democratization".

Through its Office for Democratic Institutions and Human Rights (ODIHR), the OSCE observes and assesses elections in its member states, in order to support fair and transparent democratic processes, in keeping with the mutual standards to which the organization is committed; between 1994 and 2004 the OSCE sent teams of observers to monitor more than 150 elections, typically focusing on elections in emerging democracies. In 2004, at the invitation of the United States Government, the ODIHR deployed an assessment mission, made up of participants from six OSCE member states, which observed that year's US presidential election and produced a report. It was the first time that a US presidential election was the subject of OSCE monitoring, although the organization had previously monitored state-level American elections in Florida and California, in 2002 and 2003. The 2004 assessment took place against the backdrop of the controversial recount effort in the 2000 US presidential election, and came about largely through the initiative of 13 Democratic members of the United States House of Representatives. That group, which included Barbara Lee, of California, and Eddie Bernice Johnson, of Texas, initially addressed a request for election observers to the United Nations, in a letter to Kofi Annan, the UN Secretary-General, but the request was declined. Subsequently, the administration of President George W. Bush, through the State Department, headed by Secretary of State Colin Powell, responded to the lawmakers' concerns by inviting the OSCE election-monitoring mission.

The six official languages of the OSCE are English, French, German, Italian, Spanish and Russian.

A unique aspect of the OSCE is the non-binding status of its constitutive charter. Rather than being a formal treaty ratified by national legislatures, the Helsinki Final Act represents a political commitment by the heads of government of all signatories to build security and cooperation in Europe on the basis of its provisions. This allows the OSCE to remain a flexible "process" for the evolution of improved cooperation, which avoids disputes and/or sanctions over implementation. By agreeing to these commitments, signatories for the first time accepted that treatment of citizens "within" their borders was also a matter of legitimate international concern. This open process of the OSCE is often given credit for helping build democracy in the Soviet Union and Eastern Europe, thus leading to the end of the Cold War. Unlike most international intergovernmental organizations, however, the OSCE does not have international legal personality on account of the lack of legal effect of its charter. As a result, its headquarters’ host, Austria, had to confer legal personality on the organization in order to be able to sign a legal agreement regarding its presence in Vienna.

Political direction to the organization is given by heads of state or government during summits. Summits are not regular or scheduled but held as needed. The last summit took place in Astana (Kazakhstan), on 1 and 2 December 2010. The high-level decision-making body of the organization is the Ministerial Council, which meets at the end of every year. At ambassadorial level the Permanent Council convenes weekly in Vienna and serves as the regular negotiating and decision-making body. The chairperson of the Permanent Council is the ambassador to the Organization of the participating State which holds the chairmanship. From 1 January 2017 to 31 December 2017 the Chairperson-in-Office is Austrian Foreign Minister, Sebastian Kurz, who succeeded German Foreign Minister Frank-Walter Steinmeier.

In addition to the Ministerial Council and Permanent Council, the Forum for Security Co-operation is also an OSCE decision-making body. It deals predominantly with matters of military co-operation, such as modalities for inspections according to the Vienna Document of 1999.

The OSCE's Secretariat is located in Vienna, Austria. The current Secretary General is Thomas Greminger of Switzerland, who took over from Lamberto Zannier of Italy. The organization also has offices in Copenhagen, Geneva, The Hague, Prague and Warsaw.
, the OSCE employed 3,462 staff, including 513 in its secretariat and institutions and 2,949 in its 17 field operations.

The Parliamentary Assembly of the Organization for Security and Co-operation in Europe is made up of 323 parliamentarians from 57 member states. The Parliamentary Assembly performs its functions mainly via the Standing Committee, the Bureau, and 3 General Committees (Committee on Political Affairs and Security, Committee on Economic Affairs, Science, Technology and Environment, and Committee on Democracy, Human Rights and Humanitarian Questions). The Parliamentary Assembly passes resolutions on matters such as political and security affairs, economic and environmental issues, and democracy and human rights. Representing the collective voice of OSCE parliamentarians, these resolutions and recommendations are meant to ensure that all participating states live up to their OSCE commitments. The Parliamentary Assembly also engages in parliamentary diplomacy, and has an extensive election observation program.

The oldest OSCE institution is the Office for Democratic Institutions and Human Rights (ODIHR), established in 1991 following a decision made at the 1990 Summit of Paris. It is based in Warsaw, Poland, and is active throughout the OSCE area in the fields of election observation, democratic development, human rights, tolerance and non-discrimination, rule of law, and Roma and Sinti issues. The ODIHR has observed over 300 elections and referendums since 1995, sending more than 50,000 observers. It has operated outside its own area twice, sending a team that offered technical support to the 9 October 2004 presidential elections in Afghanistan, an OSCE Partner for Co-operation, and an election support team to assist with parliamentary and provincial council elections on 18 September 2005. ODIHR is headed by Michael Georg Link.

The Office of the OSCE Representative on Freedom of the Media, established in December 1997, acts as a watchdog to provide early warning on violations of freedom of expression in OSCE participating States. The representative also assists participating States by advocating and promoting full compliance with OSCE norms, principles and commitments regarding freedom of expression and free media. As of 2011, the current representative is expert in media law from Bosnia and Herzegovina Dunja Mijatovic.

The High Commissioner on National Minorities was created on 8 July 1992 by the Helsinki Summit Meeting of the Conference on Security and Cooperation in Europe. It is charged with identifying and seeking early resolution of ethnic tension that might endanger peace, stability or friendly relations between participating states.

Each year the OSCE holds an OSCE Asian Conference with partner nations (currently Australia, Thailand, South Korea, Japan and Afghanistan).


Almost all field operations of OSCE have been conducted in countries of former Yugoslavia and the Soviet Union.


The Secretary General is the OSCE's chief administrative officer and can, when requested by the Chairmanship, serve as the representative of the Chairperson-in-Office. Since the establishment of the office in 1992, Secretary Generals have been:

The OSCE chair is assumed at yearly intervals by one participating State, which then plays the central role in managing the Organization's work and in its external representation. The foreign minister of the country holding the chair holds the office of Chairperson-in-Office (CiO).

The responsibilities of the Chairman-in-Office (CiO) include

The CiO is assisted by the previous and incoming chairman-in-office; the three of them together constitute the Troika. The origin of the institution lies with the Charter of Paris for a New Europe (1990), the Helsinki Document 1992 formally institutionalized this function.

Chairmanship of the OSCE is held by a member state on a calendar-year basis, with the minister for foreign affairs of that state performing the function of Chairman-in-Office. The table below shows the holders since 1991.

Since 1993, the OSCE's budget by year (in millions of euro,) has been:

The OSCE considers itself a regional organization in the sense of Chapter VIII of the United Nations Charter and is an observer in the United Nations General Assembly. The Chairman-in-Office gives routine briefings to the United Nations Security Council.

The OSCE takes a comprehensive approach to the politico-military dimension of security, which includes a number of commitments by participating States and mechanisms for conflict prevention and resolution. The organization also seeks to enhance military security by promoting greater openness, transparency and co-operation.

The end of the Cold War resulted in a huge amount of surplus weapons becoming available in what is known as the international grey market for weapons. The OSCE helps to stop the - often illegal - spread of such weapons and offers assistance with their destruction. The OSCE hosts the annual exchange of information under the Conventional Forces in Europe treaty. The OSCE has also implemented two additional exchanges of information, the Vienna Document and the Global Exchange of Military Information. The Open Skies Consultative Commission, the implementing body for the Treaty on Open Skies, meets monthly at its Vienna headquarters.

The actions taken by the OSCE in border monitoring range from conflict prevention to post-conflict management, capacity building and institutional support.

With its expertise in conflict prevention, crisis management and early warning, the OSCE contributes to worldwide efforts in combating terrorism.

The OSCE works to prevent conflicts from arising and to facilitate lasting comprehensive political settlements for existing conflicts. It also helps with the process of rehabilitation in post-conflict areas.

The OSCE's Forum for Security Co-operation provides a framework for political dialogue on military reform, while practical activities are conducted by field operations, as well as the Conflict Prevention Centre.

OSCE police operations are an integral part of the organization's efforts in conflict prevention and post-conflict rehabilitation.

The OSCE was a rather small organization until selection by the international community to provide electoral organization to post war Bosnia and Herzegovina in early 1996. Ambassador Frowick was the first OSCE representative to initiate national election in September 1996, human rights issues and rule of law specifically designed to provide a foundation for judicial organization within Bosnia and Herzegovina.

The OSCE had regional offices and field offices, to include the office in Brcko in northeastern Bosnia and Herzegovina which remained in limbo until the Brcko Arbitration Agreement could be decided, finalized and implemented.

Brcko become a "special district" and remains so today.

The OSCE essentially took the place of the United Nations in Bosnia and Herzegovina in part because the Bosnian leadership felt deep contempt for the UN efforts to stop the war which began in 1991 and ended in 1995. During the time the United Nations were attempting a political solution, thousands of UN troops were posted in and around Bosnia and Herzegovina with special emphasis on Sarajevo. From 1991 to 1995, over 200,000 Bosnians were killed and over one million displaced and another million as refugees.

The OSCE continues to have a presence and a number of initiatives to bring a sustained peace to the region.

Activities in the economic and environmental dimension include the monitoring of developments related to economic and environmental security in OSCE participating States, with the aim of alerting them to any threat of conflict; assisting States in the creation of economic and environmental policies, legislation and institutions to promote security in the OSCE region.

Among the economic activities of the OSCE feature activities related to migration management, transport and energy security. Most activities are implemented in co-operation with partner organizations.

The OSCE has developed a range of activities in the environmental sphere aimed at addressing ecologic threats to security in its participating States. Among the activities feature projects in the area of hazardous waste, water management and access to information under the Aarhus Convention.

The commitments made by OSCE participating States in the human dimension aim to ensure full respect for human rights and fundamental freedoms; to abide by the rule of law; to promote the principles of democracy by building, strengthening and protecting democratic institutions; and to promote tolerance throughout the OSCE region.

Since 2003 the OSCE has had an established mechanism for combating trafficking in human beings, as defined by Article 3 of the Palermo Protocol, which is aimed at raising public awareness of the problem and building the political will within participating states to tackle it effectively.

The OSCE actions against trafficking in human beings are coordinated by the Office of the Special Representative and Co-ordinator for Combating Trafficking in Human Beings. Maria Grazia Giammarinaro, a judge in the Criminal Court of Rome, took Office as the Special Representative in March 2010. From 2006 to 2009 this Office was held by Eva Biaudet, a former Finnish Minister of Health and Social Services. Biaudet currently serves as Finnish Ombudsman for Minorities. Her predecessor was former Austrian Minister Helga Konrad, who served as the first OSCE Special Representative for Combating Trafficking in Human Beings.

The activities around Combating Trafficking in Human Beings in the OSCE Region of the Office of the Special Representative include:

The OSCE claims to promote democracy and assist the participating states in building democratic institutions.

Education programmes are an integral part of the organization's efforts in conflict prevention and post-conflict rehabilitation.

As part of its democratization activities, the OSCE carries out election assistance projects in the run-up to, during, and following elections. However, the effectiveness of such assistance is arguable—Kazakhstan, for example, despite being the former chair of the OSCE, is considered by many to be one of the least democratic countries in the world. Moreover, the recent democratic advances made in other Central Asian republics, notably Kyrgyzstan, have led to rumours of Soviet-style disruption of the Kyrgyz democratic process by, in particular, Kazakhstan and Russia. This may be in large part due to fears over the long-term stability of these countries' own quasi-dictatorships.

The equality of men and women is an integral part of sustainable democracy. The OSCE aims to provide equal opportunities for men and women and to integrate gender equality in policies and practices.

The OSCE's human rights activities focus on such priorities as freedom of movement and religion, preventing torture and trafficking in persons.

OSCE could grant consultive status to NGOs and INGOs in the form of "Researcher-in-residence programme" (run by the Prague Office of the OSCE Secretariat): accredited representatives of national and international NGOs are granted access to all records and to numerous topical compilations related to OSCE field activities.

The OSCE observes relevant media developments in its participating states with a view to addressing and providing early warning on violations of freedom of expression.

Ethnic conflict is one of the main sources of large-scale violence in Europe today. The OSCE's approach is to identify and to seek early resolution of ethnic tensions, and to set standards for the rights of persons belonging to minority groups and High Commissioner on National Minorities has been established.

The Democracy Defender Award honors a person or group for contributions to the promotion of democracy and the defense of human rights "in the spirit of Helsinki Final Act and other OSCE principles and commitments." The award was established in 2016 on the initiative of Ambassadors of 8 countries, and supported by the delegations of the 18 countries of the OSCE (22 countries in 2017).

Following an unprecedented period of activity in the 1990s and early 2000s (decade), the OSCE has in the past few years faced accusations from the CIS states (primarily Russia) of being a tool for the Western states to advance their own interests. For instance, the events in Ukraine in 2004 (the "Orange Revolution") led to allegations by Russia of OSCE involvement on behalf of the pro-Western Viktor Yushchenko. At the 2007 Munich Conference on Security Policy, Vladimir Putin made this position very clear:

Russia and its allies are advancing the concept of a comprehensive OSCE reform, which would make the Secretariat, institutions and field presences more centralized and accountable to collective consensus-based bodies and focus the work of the Organization on topical security issues (human trafficking, terrorism, non-proliferation, arms control, etc.), at the expense of the "Human Dimension", or human rights issues. The move to reduce the autonomy of the theoretically independent OSCE institutions, such as ODIHR, would effectively grant a Russian veto over any OSCE activity. Western participating States are opposing this process, which they see as an attempt to prevent the OSCE from carrying out its democratization agenda in post-Soviet countries.

Following the 2008 U.S. presidential election, OSCE's ODIHR was accused of having double standards by Russia's lawmaker Slutsky. The point was made that while numerous violations of the voting process were registered, its criticism came only from within the United States (media, human rights organizations, McCain's election staff), while the OSCE known for its bashing criticism of elections on the post-Soviet space remained silent.

In 2004 the OSCE Parliamentary Assembly sent election observers to the U.S. Presidential elections. The OSCE Parliamentary Assembly's president at the time was Democratic Congressman Alcee Hastings. Hastings had previously been impeached for corruption by the U.S. Congress. The OSCE faced criticism of partisanship and double standards due to Hastings's past and the fact that the OSCE's mandate was to promote democracy and the values of civil society.

In 2010 the Parliamentary Assembly of the Organization for Security and Co-operation in Europe was criticized from within by the Latvian delegation for lacking transparency and democracy. Spencer Oliver (b. 1938) secretary general of the OSCE Parliamentary Assembly, who has held the post since the organization's inception in 1992, faced a challenge from the Latvian Artis Pabriks. According to the rules of the OSCE Parliamentary Assembly the incumbent general secretary can only be replaced with a full consensus minus one. Pabriks called the rules "quite shocking from the perspective of an organization that's monitoring elections".

Before the U.S. presidential elections of November 2012, the OSCE announced its intention to send electoral observers to Texas and to other U.S. states. This prompted the Attorney General of Texas Greg Abbott to send letters to U.S. Secretary of State Hillary Clinton and to the OSCE, threatening to arrest OSCE officials if they should enter electoral premises in Texas and break Texas law. In reply, the U.S. Department of State stated that OSCE observers enjoyed immunities. In the event, no incidents between OSCE and Texas authorities were recorded during the elections.

On 21 March 2014 OSCE deployed its Special Monitoring Mission on a request of Ukraine's government.

On 27 April 2014 eight members of the OSCE Special Monitoring Mission (OSCE SMM) were taken hostage by the Girkin group that took over the power in the city of Slovyansk (Slavyansk). The group appointed Vyacheslav Ponomarev a mayor of the city.

During the War in Donbass, an OSCE observer allowed Russian separatists to use the organization's marked vehicle, which prompted the belief that the OSCE was biased in the war and not interested in carrying out its duties of mediating a ceasefire. The organization issued a statement regretting the incident.

The agreement called for a creation of a 40 km buffer zone, but upon Ukrainian forces withdrawing from their 20 km portion of the buffer, Russian separatists are said to have simply occupied the abandoned territory without withdrawing from their own 20 km buffer. Likewise, there are allegations of separatists using OSCE marked vehicles for transportation. Moreover, the mission also received criticism alleging that only 2 checkpoints on the Russian–Ukrainian border are currently being monitored, which has been described as "seriously inadequate" by Daniel Baer, the US ambassador to the OSCE at the time.

On the other hand, Ukraine has faced criticism following a BBC report showing an alleged violation of the Minsk agreement when Ukraine stationed tanks in the residential neighborhood Avdeevka. The mission has also been criticized for waiting months to deploy drones to help monitor the border as well as withdrawing them after only several weeks of use due to Russian electronic attacks. Ukraine has argued that approximately 80% of the OSCE observers located near Mariupol were Russian citizens and many had ties to Russian security agencies such as the FSB and GRU. The organization has also been accused of allegedly revealing the locations of Ukrainian troops to Russian forces during the conflict and that Russian OSCE observers may be directly coordinating separatist artillery strikes on Ukrainian positions.

On 1 December 2014, an OSCE observer was injured by Ukrainian counter artillery fire while observing militants firing at Ukrainian forces. The OSCE team was located next to two pro-Russian mortar teams. The OSCE team did not radio in or record the Russian mortar team firing on Ukrainian positions. Critics stated that the unorthodox behavior of being located next to an active separatist artillery position and not reporting the incident showed that the OSCE team was not acting in an impartial manner.

On 27 October 2015 a suspended OSCE monitor confirmed he was a former employee of Russia's Main Intelligence Directorate. The suspended SMM stated he had no trouble receiving the position and neither the OSCE nor Ukraine's Security Service thoroughly checked his background. Following the report the OSCE issued a comment stating the monitor has been fired due to violations of the organization's code of conduct. On 6 April 2016 photos of OSCE monitors attending the wedding of a Russian separatist were found. The wedding was hosted in June 2015. The OSCE expressed regret over the incident, issuing a statement saying "The unprofessional behavior displayed by the monitors in the picture is an individual incident that should not be abused to cast a shadow on the reputation of other mission members." The OSCE reported that the monitors were no longer with the OSCE special monitoring mission.

In April 2017 an OSCE vehicle struck a mine, killing one member and injuring two. Two armoured vehicles were on patrol was near Luhansk when one struck the mine. The dead man was an American paramedic, while the injured included a woman from Germany and a man from the Czech Republic.

On 18 July 2018 it was revealed that Russian intelligence services received inside information about the activities of the OSCE Special Monitoring Mission to Ukraine from a staff member of the OSCE. The insider information consisted of observer's preference to alcohol, women, their financial situation, and their contacts in Ukraine. The OSCE issued a statement expressing concern over the alleged security breach.

In April 2017, Turkish President Recep Tayyip Erdoğan criticized the OSCE for reporting that opposition "No" campaigners in the Turkish constitutional referendum had faced bans, police interventions and arrests. Erdoğan said: "Now the Organization for Security and Cooperation in Europe says if the result is ‘yes’, that means there are a lot of problems. Who are you? First of all, you should know your place. This is not your duty."



</doc>
<doc id="22798" url="https://en.wikipedia.org/wiki?curid=22798" title="Omri">
Omri

Omri (, "‘Omri"; fl. 9th century BC) was, according to the Hebrew Bible, the sixth king of Israel. He was a successful military campaigner who extended the northern kingdom of Israel. Other monarchs from the House of Omri are Ahab, Ahaziah, Joram, and Athaliah. Like his predecessor, king Zimri, who ruled for only seven days, Omri is the second king mentioned in the Bible without a statement of his tribal origin. One possibility, though unproven, is that he was of the tribe of Issachar. 

Nothing is said in Scripture about the lineage of Omri. His name may be Amorite, Arabic, or Hebrew in origin. Omri is credited with the construction of Samaria and establishing it as his capital. Although the Bible is silent about other actions taken during his reign, he is described as doing more evil than all the kings who preceded him. An alternative modern hypothesis maintains that, as founder of the House of Omri, an Israelite royal house, his kingdom formed the first state in the Land of Israel, and that the Kingdom of Judah only achieved statehood later.

Extrabiblical sources such as the Mesha Stele and the Black Obelisk of Shalmaneser III also mention his name; however, in the case of the Black Obelisk the reference is to the dynasty named for Omri rather than to Omri himself. A minor thesis, argued by Thomas Thompson and Niels Peter Lemche, suggests that Omri may be a dynastic name indicating the apical founder of the Kingdom of Israel rather than one denoting an actual historical king.

According to the biblical narrative, Omri was "commander of the army" of King Elah when Zimri, "commander of half the king's chariots", murdered Elah and made himself king. Instead, the troops at Gibbethon chose Omri as king, and he led them to Tirzah where they besieged it. When Zimri saw that the city was taken, he committed suicide by shutting himself in the royal palace and setting it ablaze. He died after a reign of only seven days. Although Zimri was eliminated, "half of the people" supported Tibni in opposition to Omri. It took Omri four years to subdue Tibni and at last proclaim himself undisputed king of Israel.

Initially, the capital was in Tirzah, which had been besieged and the royal palace had been burned down. The Jewish Encyclopedia suggests that "the associations of Tirzah were so repellent and sanguinary, and the location so poor for a capital, that Omri purchased a new site" for his residence. This was in Samaria, on a hill purchased from Shemer for two talents of silver, where Omri built a new capital for the kingdom. In Samaria, Omri reigned until his death and was buried there. His son Ahab became the next king.

Omri became king of Israel in the 31st year of Asa, king of Judah and reigned for 12 years, 6 years of which were in Tirzah. The biblical reference to the period of rivalry with Tibni is from the 27th year of Asa to the 31st year. There are several possible dates: William F. Albright has dated his reign to 876–869 BC, E. R. Thiele offers the dates of 888 BC to 880 BC for his rivalry with Tibni and 880–874 BC for his sole reign. while Paul L. Maier affirms that it happened between 881–873 BC.

The fortress at Jezreel was situated on one of the main east-west routes through the kingdom. Hugh Williamson believes it served not only a military function, but also a political one; a very visible example of grandiose public works used as a means of social control and to assert claims of legitimacy.

The Moabite Mesha stele (on display in the Louvre) indicates that Omri expanded his holdings to include northern Moab east of the Jordan River. It makes reference to the oppression of Moab by "Omri King of Israel". Israel would later become identified in sources as the "House of Omri" ("Bit-Humria"), with the term "Israel" being used less and less as history progressed (the other defining term for "Israel" is "Samaria", beginning in the reign of Joash). Thomas L. Thompson ("The Bible in History"), however, interprets the Mesha stele as suggesting that Omri is an eponym, or legendary founder of the kingdom rather than an historical person.

The Assyrian Black Obelisk in the British Museum has been interpreted as referring to Jehu "son of Omri", though that interpretation has been questioned.

The short-lived dynasty founded by Omri constituted a new chapter in the history of the Northern Kingdom of Israel. It ended almost fifty years of constant civil war over the throne. There was peace with the Kingdom of Judah to the south, and even cooperation between the two rival states, while relations with neighboring Sidon to the north were bolstered by marriages negotiated between the two royal courts. This state of peace with two powerful neighbors enabled the Kingdom of Israel to expand its influence and even political control in Transjordan, and these factors combined brought economic prosperity to the kingdom.

On the other hand, peace with Sidon also resulted in the penetration of Phoenician religious ideas into the kingdom and led to a "kulturkampf" between traditionalists (as personified by the prophet Elijah and his followers) and the aristocracy (as personified by Omri's son and heir Ahab and his consort Jezebel). In foreign affairs, this period paralleled the rise of the Kingdom of Aram based in Damascus, and Israel soon found itself at war in the northeast. Most threatening, however, was the ascendancy of Assyria, which was beginning to expand westward from Mesopotamia: the Battle of Qarqar (853 BC), which pitted Shalmaneser III of Assyria against a coalition of local kings, including Ahab, was the first clash between Assyria and Israel. It was the first in a series of wars that would eventually lead to the destruction of the Kingdom of Israel in 722 BC and the reduction of the Kingdom of Judah to an Assyrian tributary state.

In 841 BC, the Assyrian king Shalmaneser III campaigned along the Mediterranean coast and forced Jehu to pay tribute. Assyrian kings frequently referred to Omri's successors as belonging to the "House of Omri" ("Bit Hu-um-ri-a").



</doc>
<doc id="22799" url="https://en.wikipedia.org/wiki?curid=22799" title="Oxycodone">
Oxycodone

Oxycodone, sold under the brand name OxyContin (which is the extended release form) among others, is an opioid medication used for treatment of moderate to severe pain, and a common drug of abuse. It is usually taken by mouth, and is available in immediate-release and controlled-release formulations. Onset of pain relief typically begins within fifteen minutes and lasts for up to six hours with the immediate-release formulation. In the United Kingdom, it is available by injection. Combination products are also available with paracetamol (acetaminophen), ibuprofen, naloxone, and aspirin.
Common side effects include euphoria, constipation, nausea, vomiting, loss of appetite, drowsiness, dizziness, itching, dry mouth, and sweating. Severe side effects may include addiction, dependence, hallucinations, respiratory depression (a reduction in breathing), bradycardia, and low blood pressure. Those allergic to codeine may also be allergic to oxycodone. Use of oxycodone in early pregnancy appears relatively safe. Opioid withdrawal may occur if rapidly stopped. Oxycodone acts by activating the μ-opioid receptor. When taken by mouth, it has roughly 1.5 times the effect of the equivalent amount of morphine.
Oxycodone was first made in Germany in 1916 from thebaine. It is available as a generic medication. In 2017, it was the 52nd most commonly prescribed medication in the United States, with more than fifteen million prescriptions. A number of abuse-deterrent formulations are available, such as in combination with naloxone.
Oxycodone is used for managing moderate to severe acute or chronic pain when other treatments are not sufficient. It may improve quality of life in certain types of pain. It is unclear if use in chronic pain results in improved quality of life or ongoing pain relief.

Oxycodone is available as controlled-release tablet, intended to be taken every 12 hours. A July 1996 study independent of Purdue Pharma, the drug's originator, found the controlled-release formulation had a variable duration of action ranging from 10–12 hours. A 2006 review found that controlled-release oxycodone is comparable to immediate-release oxycodone, morphine, and hydromorphone in management of moderate to severe cancer pain, with fewer side effects than morphine. The author concluded that the controlled-release form is a valid alternative to morphine and a first-line treatment for cancer pain. In 2014, the European Association for Palliative Care recommended oxycodone by mouth as a second-line alternative to morphine by mouth for cancer pain.

In the U.S., extended-release oxycodone is approved for use in children as young as eleven years old. The approved uses is for relief of cancer pain, trauma pain, or pain due to major surgery, in children already treated with opioids, who can tolerate at least 20 mg per day of oxycodone; this provides an alternative to Duragesic (fentanyl), the only other extended-release opioid analgesic approved for children.

Oxycodone is available in a variety of formulations for by mouth or under the tongue:


In the US, oxycodone is only approved for use by mouth, available as tablets and oral solutions. Parenteral formulations of oxycodone (brand name OxyNorm) are also available in other parts of the world however, and are widely used in the European Union. In Spain, the Netherlands and the United Kingdom, oxycodone is approved for intravenous (IV) and intramuscular (IM) use. When first introduced in Germany during World War I, both IV and IM administrations of oxycodone were commonly used for postoperative pain management of Central Powers soldiers.

Serious side effects of oxycodone include reduced sensitivity to pain (beyond the pain the drug is taken to reduce), euphoria, anxiolysis, feelings of relaxation, and respiratory depression. Common side effects of oxycodone include constipation (23%), nausea (23%), vomiting (12%), somnolence (23%), dizziness (13%), itching (13%), dry mouth (6%), and sweating (5%). Less common side effects (experienced by less than 5% of patients) include loss of appetite, nervousness, abdominal pain, diarrhea, urine retention, dyspnea, and hiccups. Most side effects generally become less intense over time, although issues related to constipation are likely to continue for the duration of use. Oxycodone in combination with naloxone in managed-release tablets, has been formulated to both deter abuse and reduce "opioid-induced constipation".

The risk of experiencing severe withdrawal symptoms is high if a patient has become physically dependent and discontinues oxycodone abruptly. Medically, when the drug has been taken regularly over an extended period, it is withdrawn gradually rather than abruptly. People who regularly use oxycodone recreationally or at higher than prescribed doses are at even higher risk of severe withdrawal symptoms. The symptoms of oxycodone withdrawal, as with other opioids, may include "anxiety, panic attack, nausea, insomnia, muscle pain, muscle weakness, fevers, and other flu-like symptoms".

Withdrawal symptoms have also been reported in newborns whose mothers had been either injecting or orally taking oxycodone during pregnancy.

As with other opioids, chronic use of oxycodone (particularly with higher doses) often causes concurrent hypogonadism (low sex hormone levels).

In high doses, overdoses, or in some persons not tolerant to opioids, oxycodone can cause shallow breathing, slowed heart rate, cold/clammy skin, pauses in breathing, low blood pressure, constricted pupils, circulatory collapse, respiratory arrest, and death.

In 2011, it was the leading cause of drug-related deaths in the U.S. However, from 2012 onwards, heroin and fentanyl have become more common causes of drug-related deaths.

Oxycodone overdose has also been described to cause spinal cord infarction in high doses and ischemic damage to the brain, due to prolonged hypoxia from suppressed breathing.

Oxycodone is metabolized by the enzymes CYP3A4 and CYP2D6, and its clearance therefore can be altered by inhibitors and inducers of these enzymes, increasing and decreasing half-life, respectively. (For lists of CYP3A4 and CYP2D6 inhibitors and inducers, see here and here, respectively.) Natural genetic variation in these enzymes can also influence the clearance of oxycodone, which may be related to the wide inter-individual variability in its half-life and potency.

Ritonavir or lopinavir/ritonavir greatly increase plasma concentrations of oxycodone in healthy human volunteers due to inhibition of CYP3A4 and CYP2D6. Rifampicin greatly reduces plasma concentrations of oxycodone due to strong induction of CYP3A4. There is also a case report of fosphenytoin, a CYP3A4 inducer, dramatically reducing the analgesic effects of oxycodone in a chronic pain patient. Dosage or medication adjustments may be necessary in each case.

Oxycodone, a semi-synthetic opioid, is a highly selective full agonist of the μ-opioid receptor (MOR). This is the main biological target of the endogenous opioid neuropeptide β-endorphin. Oxycodone has low affinity for the δ-opioid receptor (DOR) and the κ-opioid receptor (KOR), where it is an agonist similarly. After oxycodone binds to the MOR, a G protein-complex is released, which inhibits the release of neurotransmitters by the cell by decreasing the amount of cAMP produced, closing calcium channels, and opening potassium channels. Opioids like oxycodone are thought to produce their analgesic effects via activation of the MOR in the midbrain periaqueductal gray (PAG) and rostral ventromedial medulla (RVM). Conversely, they are thought to produce reward and addiction via activation of the MOR in the mesolimbic reward pathway, including in the ventral tegmental area, nucleus accumbens, and ventral pallidum. Tolerance to the analgesic and rewarding effects of opioids is complex and occurs due to receptor-level tolerance (e.g., MOR downregulation), cellular-level tolerance (e.g., cAMP upregulation), and system-level tolerance (e.g., neural adaptation due to induction of ΔFosB expression).

Taken orally, 20 mg of immediate-release oxycodone is considered to be equivalent in analgesic effect to 30 mg of morphine, while extended release oxycodone is considered to be twice as potent as oral morphine.

Similarly to most other opioids, oxycodone increases prolactin secretion, but its influence on testosterone levels is unknown. Unlike morphine, oxycodone lacks immunosuppressive activity (measured by natural killer cell activity and interleukin 2 production "in vitro"); the clinical relevance of this has not been clarified.

A few of the metabolites of oxycodone have also been found to be active as MOR agonists, some of which notably have much higher affinity for (as well as higher efficacy at) the MOR in comparison. Oxymorphone possesses 3- to 5-fold higher affinity for the MOR than does oxycodone, while noroxycodone and noroxymorphone possess one-third of and 3-fold higher affinity for the MOR, respectively, and MOR activation is 5- to 10-fold less with noroxycodone but 2-fold higher with noroxymorphone relative to oxycodone. Noroxycodone, noroxymorphone, and oxymorphone also have longer biological half-lives than oxycodone.

However, despite the greater "in vitro" activity of some of its metabolites, it has been determined that oxycodone itself is responsible for 83.0% and 94.8% of its analgesic effect following oral and intravenous administration, respectively. Oxymorphone plays only a minor role, being responsible for 15.8% and 4.5% of the analgesic effect of oxycodone after oral and intravenous administration, respectively. Although the CYP2D6 genotype and the route of administration result in differential rates of oxymorphone formation, the unchanged parent compound remains the major contributor to the overall analgesic effect of oxycodone. In contrast to oxycodone and oxymorphone, noroxycodone and noroxymorphone, while also potent MOR agonists, poorly cross the blood–brain barrier into the central nervous system, and for this reason are only minimally analgesic in comparison.

In 1997, a group of Australian researchers proposed (based on a study in rats) that oxycodone acts on KORs, unlike morphine, which acts upon MORs. Further research by this group indicated the drug appears to be a high-affinity κ-opioid receptor agonist. However, this conclusion has been disputed, primarily on the basis that oxycodone produces effects that are typical of MOR agonists. In 2006, research by a Japanese group suggested the effect of oxycodone is mediated by different receptors in different situations. Specifically in diabetic mice, the KOR appears to be involved in the antinociceptive effects of oxycodone, while in nondiabetic mice, the μ-opioid receptor seems to be primarily responsible for these effects.

Oxycodone can be administered orally, intranasally, via intravenous, intramuscular, or subcutaneous injection, or rectally. The bioavailability of oral administration of oxycodone averages within a range of 60 to 87%, with rectal administration yielding the same results; intranasal varies between individuals with a mean of 46%.

After a dose of conventional (instant-release) oral oxycodone, the onset of action is 10 to 30 minutes, and peak plasma levels of the drug are attained within roughly 30 to 60 minutes; in contrast, after a dose of OxyContin (an oral controlled-release formulation), peak plasma levels of oxycodone occur in about three hours. The duration of instant-release oxycodone is 3 to 6 hours, although this can be variable depending on the individual.

Oxycodone has a volume of distribution of 2.6L/kg, in the blood it is distributed to skeletal muscle, liver, intestinal tract, lungs, spleen, and brain. Conventional oral preparations start to reduce pain within 10 to 15 minutes on an empty stomach; in contrast, OxyContin starts to reduce pain within one hour.

The metabolism of oxycodone in humans occurs in the liver mainly via the cytochrome P450 system and is extensive (about 95%) and complex, with many minor pathways and resulting metabolites. Around 10% (range 8–14%) of a dose of oxycodone is excreted essentially unchanged (unconjugated or conjugated) in the urine. The major metabolites of oxycodone are noroxycodone (70%), noroxymorphone ("relatively high concentrations"), and oxymorphone (5%). The immediate metabolism of oxycodone in humans is as follows:


In humans, N-demethylation of oxycodone to noroxycodone by CYP3A4 is the major metabolic pathway, accounting for 45% ± 21% of a dose of oxycodone, while O-demethylation of oxycodone into oxymorphone by CYP2D6 and 6-ketoreduction of oxycodone into 6-oxycodols represent relatively minor metabolic pathways, accounting for 11% ± 6% and 8% ± 6% of a dose of oxycodone, respectively.

Several of the immediate metabolites of oxycodone are subsequently conjugated with glucuronic acid and excreted in the urine. 6α-Oxycodol and 6β-oxycodol are further metabolized by N-demethylation to nor-6α-oxycodol and nor-6β-oxycodol, respectively, and by N-oxidation to 6α-oxycodol-N-oxide and 6β-oxycodol-N-oxide (which can subsequently be glucuronidated as well). Oxymorphone is also further metabolized, as follows:


The first pathway of the above three accounts for 40% of the metabolism of oxymorphone, making oxymorphone-3-glucuronide the main metabolite of oxymorphone, while the latter two pathways account for less than 10% of the metabolism of oxymorphone. After N-demethylation of oxymorphone, noroxymorphone is further glucuronidated to noroxymorphone-3-glucuronide.

Because oxycodone is metabolized by the cytochrome P450 system in the liver, its pharmacokinetics can be influenced by genetic polymorphisms and drug interactions concerning this system, as well as by liver function. Some people are fast metabolizers of oxycodone, resulting in reduced effects, while others are slow metabolizers, resulting in increased effects and toxicity. While higher CYP2D6 activity increases the effects of oxycodone (owing to increased conversion into oxymorphone), higher CYP3A4 activity has the opposite effect and decreases the effects of oxycodone (owing to increased metabolism into noroxycodone and noroxymorphone). The dose of oxycodone must be reduced in patients with reduced liver function.

The clearance of oxycodone is 0.8 L/min. Oxycodone and its metabolites are mainly excreted in urine. Therefore, oxycodone accumulates in patients with kidney impairment. Oxycodone is eliminated in the urine 10% as unchanged oxycodone, 45% ± 21% as "N"-demethylated metabolites (noroxycodone, noroxymorphone, noroxycodols), 11 ± 6% as "O"-demethylated metabolites (oxymorphone, oxymorphols), and 8% ± 6% as 6-keto-reduced metabolites (oxycodols).

Oxycodone has a half-life of 4.5 hours. It is available as a generic medication. Its manufacturer Purdue Pharma claimed in their 1992 patent application that the duration of action of OxyContin, oxycodone's controlled-release preparation, is 12 hours in "90% of patients." It has never performed any clinical studies in which OxyContin was given at more frequent intervals. In a separate filing, Purdue claims that controlled-release oxycodone "provides pain relief in said patient for at least 12 hours after administration." However, in 2016 an investigation by the "Los Angeles Times" found that "the drug weans off hours early in many people," inducing symptoms of opiate withdrawal and intense cravings for OxyContin. One doctor Lawrence Robbins told journalists that over 70% of his patients would report that OxyContin would only provide 4–7 hours of relief. Doctors in the 1990s often would switch their patients to a dosing schedule of once every eight hours when they complained that the duration of action for OxyContin was too short to be only taken twice a day.

Purdue strongly discouraged the practice: Purdue's medical director Robert Reder wrote to one doctor in 1995 that "OxyContin has been developed for [12-hour] dosing...I request that you not use a [8-hourly] dosing regimen." Purdue repeatedly released memos to its sales representatives ordering them to remind doctors not to deviate from a 12-hour dosing schedule. One such memo read, "There is no Q8 dosing with OxyContin... [8-hour dosing] needs to be nipped in the bud. NOW!!" The journalists who covered the investigation argue that Purdue Pharma has insisted on a 12-hour duration of action for nearly all patients, despite evidence to the contrary, in order to protect the reputation of OxyContin as a 12-hour drug and the willingness of health insurance and managed care companies to cover OxyContin despite its high cost relative to generic opiates such as morphine.

Purdue sales representatives were instructed to encourage doctors to write prescriptions for larger 12-hour doses instead of more frequent dosing. An August 1996 memo to Purdue sales representatives in Tennessee entitled "$$$$$$$$$$$$$ It’s Bonus Time in the Neighborhood!" reminded the representatives that their commissions would dramatically increase if they were successful in convincing doctors to prescribe larger doses. "Los Angeles Times" journalists argue using interviews from opioid addiction experts that such high doses of OxyContin spaced 12 hours apart create a combination of agony during opiate withdrawal (lower lows) and a schedule of reinforcement that relieves this agony, fostering addiction.

Oxycodone's chemical name is derived from codeine. The chemical structures are very similar, differing only in that


It is also similar to hydrocodone, differing only in that it has a hydroxyl group at carbon-14.

Oxycodone is marketed as various salts, most commonly as the hydrochloride salt. The free base conversion ratios of different salts are: hydrochloride (0.896), bitartrate (0.667), tartrate (0.750), camphosulfonate (0.576), pectinate (0.588), phenylpriopionate (0.678), sulfate (0.887), phosphate (0.763), and terephthalate (0.792). The hydrochloride salt is the basis of most American oxycodone products whilst bitartrate, tartrate, pectinate, terephthalate and phosphate salts are also available in European Union products. Methyiodide and hydroiodide are mentioned in older European Union publications.

In terms of biosynthesis, oxycodone has been found naturally in nectar extracts from the orchid family "Epipactis helleborine"; together along with another opioid: 3-{2-{3-{3-benzyloxypropyl}-3-indol, 7,8-didehydro- 4,5-epoxy-3,6-d-morphinan.

Oxycodone and/or its major metabolites may be measured in blood or urine to monitor for clearance, abuse, confirm a diagnosis of poisoning, or assist in a medicolegal death investigation. Many commercial opiate screening tests cross-react appreciably with oxycodone and its metabolites, but chromatographic techniques can easily distinguish oxycodone from other opiates.

 and (Jakob) of the University of Frankfurt in Germany published the first synthesis of oxycodone from thebaine in 1916. When Freund died, in 1920, Speyer wrote his obituary. Speyer, born to a Jewish family in Frankfurt am Main in 1878, became a victim of the Holocaust. He died on 5 May 1942, the second day of deportations from the Lodz Ghetto; his death was noted in the ghetto's chronicle.

The first clinical use of the drug was documented in 1917, the year after it was first developed. It was first introduced to the U.S. market in May 1939. In early 1928, Merck introduced a combination product containing scopolamine, oxycodone, and ephedrine under the German initials for the ingredients SEE, which was later renamed Scophedal (SCOpolamine, ePHEDrine and eukodAL) in 1942. It was last manufactured in 1987, but can be compounded. This combination is essentially an oxycodone analogue of the morphine-based "twilight sleep", with ephedrine added to reduce circulatory and respiratory effects. The drug became known as the "Miracle Drug of the 1930s" in Continental Europe and elsewhere and it was the Wehrmacht's choice for a battlefield analgesic for a time. The drug was expressly designed to provide what the patent application and package insert referred to as "very deep analgesia and profound and intense euphoria" as well as tranquillisation and anterograde amnesia useful for surgery and battlefield wounding cases. Oxycodone was allegedly chosen over morphine, hydromorphone, and hydrocodone for this product because of oxycodone having subjective elements in its side effect profile similar to cocaine.

During Operation Himmler, Skophedal was also reportedly injected in massive overdose into the prisoners dressed in Polish Army uniforms in the staged incident on 1 September 1939 which opened the Second World War.

The personal notes of Adolf Hitler's physician, Theodor Morell, indicate Hitler received repeated injections of "eukodal" (oxycodone) and Scophedal, as well as Dolantin (pethidine) codeine, and morphine less frequently; oxycodone could not be obtained after late January 1945.

In the early 1970s, the U.S. government classified oxycodone as a schedule II drug.

Purdue Pharma — a privately held company based in Stamford, Connecticut, developed the prescription painkiller OxyContin. Upon its release in 1995, OxyContin was hailed as a medical breakthrough, a long-lasting narcotic that could help patients suffering from moderate to severe pain. The drug became a blockbuster, and has reportedly generated some US$35 billion in revenue for Purdue.

In October 2017, "The New Yorker" published a story on Mortimer Sackler and Purdue Pharma regarding their ties to the production and manipulation of the oxycodone markets. The article links Raymond and Arthur Sackler's business practices with the rise of direct pharmaceutical marketing and eventually to the rise of addiction to oxycodone in the United States. The article implies that Sackler bears some responsibility for the opioid epidemic in the United States. In 2019 the New York Times ran a piece confirming Sackler told company officials in 2008 to “measure our performance by Rx’s by strength, giving higher measures to higher strengths.” This was verified with documents tied to a lawsuit – which was filed by the Massachusetts attorney general, Maura Healey – claiming that Purdue Pharma and members of the Sackler family knew that high doses of OxyContin over long periods would increase the risk of serious side effects, including addiction. Despite Purdue Pharma's proposal for a US$12 billion settlement of the lawsuit, the attorneys general of 23 states, including Massachusetts, rejected the settlement offer in September, 2019.

Oxycodone is subject to international conventions on narcotic drugs. In addition, oxycodone is subject to national laws that differ by country. The 1931 Convention for Limiting the Manufacture and Regulating the Distribution of Narcotic Drugs of the League of Nations included oxycodone. The 1961 Single Convention on Narcotic Drugs of the United Nations, which replaced the 1931 convention, categorized oxycodone in Schedule I. Global restrictions on Schedule I drugs include "limit[ing] exclusively to medical and scientific purposes the production, manufacture, export, import, distribution of, trade in, use and possession of" these drugs; "requir[ing] medical prescriptions for the supply or dispensation of [these] drugs to individuals"; and "prevent[ing] the accumulation" of quantities of these drugs "in excess of those required for the normal conduct of business".

Oxycodone is in Schedule I (derived from the Single Convention on Narcotic Drugs) of the Commonwealth's Narcotic Drugs Act 1967. In addition, it is in Schedule 8 of the Australian Standard for the Uniform Scheduling of Drugs and Poisons ("Poisons Standard"), meaning it is a "controlled drug... which should be available for use but require[s] restriction of manufacture, supply, distribution, possession and use to reduce abuse, misuse and physical or psychological dependence".

Oxycodone is a controlled substance under Schedule I of the Controlled Drugs and Substances Act (CDSA).

In February 2012, Ontario passed legislation to allow the expansion of an already existing drug-tracking system for publicly funded drugs to include those that are privately insured. This database will function to identify and monitor patient's attempts to seek prescriptions from multiple doctors or retrieve from multiple pharmacies. Other provinces have proposed similar legislation, while some, such as Nova Scotia, have legislation already in effect for monitoring prescription drug use. These changes have coincided with other changes in Ontario's legislation to target the misuse of painkillers and high addiction rates to drugs such as oxycodone. As of February 29, 2012, Ontario passed legislation delisting oxycodone from the province's public drug benefit program. This was a first for any province to delist a drug based on addictive properties. The new law prohibits prescriptions for OxyNeo except to certain patients under the Exceptional Access Program including palliative care and in other extenuating circumstances. Patients already prescribed oxycodone will receive coverage for an additional year for OxyNeo, and after that, it will be disallowed unless designated under the exceptional access program.

Much of the legislative activity has stemmed from Purdue Pharma's decision in 2011 to begin a modification of Oxycontin's composition to make it more difficult to crush for snorting or injecting. The new formulation, OxyNeo, is intended to be preventive in this regard and retain its effectiveness as a painkiller. Since introducing its "Narcotics Safety and Awareness Act", Ontario has committed to focusing on drug addiction, particularly in the monitoring and identification of problem opioid prescriptions, as well as the education of patients, doctors, and pharmacists. This Act, introduced in 2010, commits to the establishment of a unified database to fulfil this intention. Both the public and medical community have received the legislation positively, though concerns about the ramifications of legal changes have been expressed. Because laws are largely provincially regulated, many speculate a national strategy is needed to prevent smuggling across provincial borders from jurisdictions with looser restrictions.

In 2015, Purdue Pharma's abuse-resistant OxyNEO and six generic versions of OxyContin had been on the Canada-wide approved list for prescriptions since 2012. In June 2015, then federal Minister of Health Rona Ambrose announced that within three years all oxycodone products sold in Canada would need to be tamper-resistant. Some experts warned that the generic product manufacturers may not have the technology to achieve that goal, possibly giving Purdue Pharma a monopoly on this opiate.

Several class action suits across Canada have been launched against the Purdue group of companies and affiliates. Claimants argue the pharmaceutical manufacturers did not meet a standard of care and were negligent in doing so. These lawsuits reference earlier judgments in the United States, which held that Purdue was liable for wrongful marketing practices and misbranding. Since 2007, the Purdue companies have paid over CAN$650 million in settling litigation or facing criminal fines.

The drug is in Appendix III of the Narcotics Act ("Betäubungsmittelgesetz" or BtMG). The law allows only physicians, dentists, and veterinarians to prescribe oxycodone and the federal government to regulate the prescriptions (e.g., by requiring reporting).

Oxycodone is regulated under Part I of Schedule 1 of Hong Kong's Chapter 134 Dangerous Drugs Ordinance.

Oxycodone is a restricted drug in Japan. Its import and export is strictly restricted to specially designated organizations having prior permit to import it. In a high-profile case an American who was a top Toyota executive living in Tokyo, who claimed to be unaware of the law, was arrested for importing oxycodone into Japan.

Oxycodone is listed as a Class A drug in the Misuse of Drugs Act of Singapore, which means offences in relation to the drug attract the most severe level of punishment. A conviction for unauthorized manufacture of the drug attracts a minimum sentence of 10 years of imprisonment and corporal punishment of 5 strokes of the cane, and a maximum sentence of life imprisonment or 30 years of imprisonment and 15 strokes of the cane. The minimum and maximum penalties for unauthorized trafficking in the drug are respectively 5 years of imprisonment and 5 strokes of the cane, and 20 years of imprisonment and 15 strokes of the cane.

Oxycodone is a Class A drug under the Misuse of Drugs Act. For Class A drugs, which are "considered to be the most likely to cause harm", possession without a prescription is punishable by up to seven years in prison, an unlimited fine, or both. Dealing of the drug illegally is punishable by up to life imprisonment, an unlimited fine, or both. In addition, oxycodone is a Schedule 2 drug per the Misuse of Drugs Regulations 2001 which "provide certain exemptions from the provisions of the Misuse of Drugs Act 1971".

Under the Controlled Substances Act, oxycodone is a Schedule II controlled substance whether by itself or part of a multi-ingredient medication. The DEA lists oxycodone both for sale and for use in manufacturing other opioids as ACSCN 9143 and in 2013 approved the following annual aggregate manufacturing quotas: 131.5 metric tons for sale, down from 153.75 in 2012, and 10.25 metric tons for conversion, unchanged from the previous year.

Oxycodone, like other opioid analgesics, tends to induce feelings of euphoria, relaxation and reduced anxiety in those who are occasional users. These effects make it one of the most commonly abused pharmaceutical drugs in the United States.

In August 2010, Purdue Pharma reformulated their long-acting oxycodone line, marketed as OxyContin, using a polymer, Intac, to make the pills extremely difficult to crush or dissolve in water to reduce OxyContin abuse. The FDA approved relabeling the reformulated version as abuse-resistant in April 2013.

Pfizer manufactures a preparation of short-acting oxycodone, marketed as Oxecta, which contains inactive ingredients, referred to as tamper-resistant Aversion Technology. It does not deter oral abuse. Approved by the FDA in the U.S. in June 2011, the new formulation makes crushing, chewing, snorting, or injecting the opioid impractical because of a change in its chemical properties.

The non-medical use of oxycodone existed from the early 1970s, but by 2015, 91% of a national sample of injecting drug users in Australia had reported using oxycodone, and 27% had injected it in the last six months.

Opioid-related deaths in Ontario had increased by 242% from 1969 to 2014. By 2009 in Ontario there were more deaths from oxycodone overdose than from cocaine overdose. Deaths from opioid pain relievers had increased from 13.7 deaths per million residents in 1991 to 27.2 deaths per million residents in 2004. The abuse of oxycodone in Canada became a problem. Areas where oxycodone is most problematic are Atlantic Canada and Ontario, where its abuse is prevalent in rural towns, and in many smaller to medium-sized cities. Oxycodone is also widely available across Western Canada, but methamphetamine and heroin are more serious problems in the larger cities, while oxycodone is more common in rural towns. Oxycodone is diverted through doctor shopping, prescription forgery, pharmacy theft, and overprescribing.

The recent formulations of oxycodone, particularly Purdue Pharma's crush-, chew-, injection- and dissolve-resistant OxyNEO which replaced the banned OxyContin product in Canada in early 2012, have led to a decline in the abuse of this opiate but have increased the abuse of the more potent drug fentanyl. According to a Canadian Centre on Substance Abuse study quoted in Maclean's magazine, there were at least 655 fentanyl-related deaths in Canada in a five-year period.
In Alberta, the Blood Tribe police claimed that from the fall of 2014 through January 2015, oxycodone pills or a lethal fake variation referred to as Oxy 80s containing fentanyl made in illegal labs by members of organized crime were responsible for ten deaths on the Blood Reserve, which is located southwest of Lethbridge, Alberta. Province-wide, approximately 120 Albertans died from fentanyl-related overdoses in 2014.

Abuse and diversion of oxycodone in the UK commenced in the early- to mid-2000s. The first known death due to overdose in the UK occurred in 2002. However, recreational use remains relatively rare.

In the United States, more than 12 million people use opioid drugs recreationally. Opioids were responsible for 49,000 of the 72,000 drug overdose deaths in the U.S. in 2017. In September 2013, the FDA released new labeling guidelines for long acting and extended release opioids requiring manufacturers to remove moderate pain as indication for use, instead stating the drug is for "pain severe enough to require daily, around-the-clock, long term opioid treatment." The updated labeling will not restrict physicians from prescribing opioids for moderate, as needed use.

Oxycodone is the most widely recreationally used opioid in America. The U.S. Department of Health and Human Services estimates that about 11 million people in the U.S. consume oxycodone in a non-medical way annually. In 2007, about 42,800 emergency room visits occurred due to "episodes" involving oxycodone. Diverted oxycodone may be taken orally or ingested through insufflation; used intravenously, or the heated vapors inhaled. In 2008, recreational use of oxycodone and hydrocodone were involved in 14,800 deaths. Some of the cases were due to overdoses of the acetaminophen component, resulting in fatal liver damage.

Reformulated OxyContin is causing some recreational users to change to heroin, which is cheaper and easier to obtain.

The International Narcotics Control Board estimated of oxycodone were manufactured worldwide in 1998; by 2007 this figure had grown to . United States accounted for 82% of consumption in 2007 at . Canada, Germany, Australia, and France combined accounted for 13% of consumption in 2007. In 2010, of oxycodone were illegally manufactured using a fake pill imprint. This accounted for 0.8% of consumption. These illicit tablets were later seized by the U.S. Drug Enforcement Administration, according to the International Narcotics Control Board. The board also reported manufactured in 2010. This number had decreased from a record high of in 2009.

Expanded expressions for the compound oxycodone in the academic literature include "dihydrohydroxycodeinone", "Eucodal", "Eukodal", "14-hydroxydihydrocodeinone", and "Nucodan". In a UNESCO convention, the translations of "oxycodone" are "oxycodon" (Dutch), "oxycodone" (French), "oxicodona" (Spanish), (Arabic), (Chinese), and (Russian). The word "oxycodone" should not be confused with "oxandrolone", "oxazepam", "oxybutynin", "oxytocin", or "Roxanol".

Other brand names include Longtec and Shortec.


</doc>
<doc id="22800" url="https://en.wikipedia.org/wiki?curid=22800" title="Occidental College">
Occidental College

Occidental College (informally Oxy) is a private liberal arts college in Los Angeles, California. Founded in as a coeducational college in 1887 by clergy and members of the Presbyterian Church, it became non-sectarian in 1910. It is one of the oldest liberal arts colleges on the West Coast of the United States.

The college values and emphasizes diversity and global literacy, via its student demographics and academic requirements.

Occidental College was founded on April 20, 1887, by a group of Presbyterian clergy, missionaries, and laymen, including James George Bell, Lyman Stewart, and Thomas Bard. The cornerstone of the school's first building was laid in September 1887 in the Boyle Heights now East Los Angeles neighborhood of Los Angeles. The college's first term began a year later with 27 male and 13 female students, and tuition of $50 a year.

In 1896, the Boyle Heights building was destroyed by fire. The college temporarily relocated to the old St. Vincent's College campus on Hill Street before a new site was selected in Highland Park in 1898. Eventually, the college erected three main buildings: the Academy Building, the Stimson Library, and the Hall of Arts and Letters (the Hall still stands today, converted to apartments). The Highland Park site was also bisected by the tracks of the Santa Fe Railroad, and was the site of two presidential visits, first by William Howard Taft in 1909 and subsequently by Theodore Roosevelt in 1911.

In 1909, the Pomona College Board of Trustees suggested a merger between Pomona and Occidental, but the proposal came to nothing. The following year, the college severed formal ties with the Presbyterian Church and became a non-sectarian, non-denominational institution. The small size of the campus and the disruption caused by frequent freight trains pushed the college's trustees to find a new location.

In 1912, the school began construction of a new campus located in Los Angeles' Eagle Rock neighborhood. The Eagle Rock campus was designed by noted California architect Myron Hunt, also known as the planner of the California Institute of Technology (Caltech) campus and as designer of the Huntington Library and Art Gallery and the Rose Bowl. That same year, Occidental President John Willis Baer announced the trustees' decision to convert Occidental College into an all-men's institution. However, students and faculty protested, and the idea was abandoned.

In 1913, the Occidental College Board of Trustees announced plans to convert the college exclusively to a men's school. The plans were met with widespread backlash from students and faculty who protested the change. The community outcry garnered national headlines and the board later dropped the proposal.

Two weeks after Booker T. Washington came to visit Occidental, on March 27, 1914, Swan, Fowler, and Johnson Halls were dedicated at its new Eagle Rock campus. Patterson Field, today one of the oldest collegiate sports stadiums in Los Angeles, was opened in 1916. In April 1917, shortly after the United States entered World War I, the college formed a Students Army Training Corps to aid the war effort.

Under Occidental President Remsen Bird, the school opened a series of new Hunt-designed buildings, including Clapp Library (1924), Hillside Theatre and a women's dormitory (Orr Hall) in 1925, Alumni Gymnasium (1926), the Freeman Student Union (1928) and a music and speech building (1929). The Delta of California chapter of Phi Beta Kappa was established at Occidental in 1926, at a time when the only other chapters in California were at Stanford, UC Berkeley, and Pomona.
English novelist Aldous Huxley, who spoke at Occidental's convocation ceremony in the then-new Thorne Hall in 1938, lampooned President Remsen Bird as Dr. Herbert Mulge of Tarzana College in his 1939 novel, "After Many a Summer Dies the Swan". Huxley was never again invited back to campus.

During World War II, many students left Occidental to fight in the war. In July 1943, the U.S. Navy established a Navy V-12 officer training program on campus that produced hundreds of graduates before it was disbanded in 1945 at the end of the war. Occidental President Remsen Bird worked behind the scenes to help Oxy students of Japanese descent continue their education despite mandatory evacuation orders; his letters are included in the Japanese American Relocation Collection in Clapp Library.

After having its first Rhodes Scholar, Clarence Spaulding, named in 1908, Oxy seniors John Paden and Aaron Segal were awarded Rhodes Scholarships in 1958; the first and only time Occidental has produced two Rhodes Scholars in a single year. Rhodes scholars Aaron Segal and John Paden were among the 10 Occidental students who participated in Crossroads Africa that year, a forerunner to the Peace Corps that later became a national program.

In 1969, 42 students were suspended for peacefully protesting military recruiting on campus. One year later, faculty voted to suspend classes in the wake of the Kent State shootings and America's invasion of Cambodia. Subsequently, Oxy students wrote 7,000 letters to Washington D.C., protesting U.S. involvement in the war in Southeast Asia. Occidental launched one of the country's first Upward Bound programs in 1966, aimed at increasing the number of low-income, underrepresented high school students who become the first in their family to go to college.

Also in 1969, the school opened its first two co-ed dormitories, and two more followed a year later. In 1988, John Brooks Slaughter, formerly Chancellor of the University of Maryland, became Occidental's first black president. Building on faculty and student advocacy and a series of grants the college had received previously to increase the diversity of the Occidental student body, Slaughter led the process of creating a new mission statement that is still used today. Also, Slaughter led the college's community outreach expansion with the creation of the Center for Volunteerism and Community Service, the predecessor for the current Center for Community Based Learning.
In November 1990, the college, initially established as a Presbyterian institution but is no longer religiously affiliated, rededicated the campus' main chapel as the Herrick Memorial Chapel and Interfaith Center. The school also took down the crosses in the chapel in an attempt to "broaden Occidental's appeal among non-Christian students".

In July 2006, Susan Westerberg Prager became Occidental's first female president. She left her position in 2007 during the fall term. Robert Skotheim the former president of Whitman College and the Huntington Library, then served as interim president. In July 2009, Jonathan Veitch, formerly dean of The New School's Eugene Lang College, became Occidental's 15th president and the first to be a native Angeleno.

The college received some national scrutiny in 2014 when the U.S. Department of Education named Occidental College as one of 55 higher education institutions under investigation "for possible violations of federal law over the handling of sexual violence and harassment". In response to student and faculty outcry the college adopted a new interim sexual misconduct policy, hired a former assistant district attorney as a full-time, independent Title IX coordinator, and added a new 24-hour, 7-days-a-week telephone hotline. The school also created a permanent Sexual Misconduct Advisory Board made up of students, faculty, and staff. Two years later, the investigation was concluded with the Office of Civil Rights finding that "the preponderance of the evidence does not support a conclusion that the College violated Title IX, except with respect to the issue of promptness in several cases during the 2012-13 school years."

President Barack Obama attended Occidental for two years prior to transferring to Columbia University. In 2015, "birthers" falsely claimed that Obama's Occidental College transcript revealed he received financial aid as a foreign student from Indonesia after the resurgence of a fake news story from 2009.

In July 2020, Harry J. Elam, Jr., formerly vice provost for undergraduate education at Stanford University and renowned for diversity and inclusion initiatives in liberal arts curricula, became Occidental’s 16th president.

Architect Myron Hunt created the original campus master plan for Occidental's Eagle Rock campus in 1911. He structured the campus in a Mediterranean style, with covered walkways and tile roofs. The campus landscape was designed and developed by Beatrix Farrand in the late 1930s. All of the 19 buildings designed by Hunt remain in use today, including Johnson Hall, now the home for the McKinnon Center for Global Affairs.

Built on a hillside, the Eagle Rock campus covers over , some of which is undeveloped land that includes a local landmark known as Fiji Hill. There are 12 on-campus residence halls and the main dining facility is The Marketplace, which is located in the Johnson Student Center. Some buildings, such as the Hameetman Science Center (designed by Anshen + Allen, 2003), deviates from the original architecture with its large glass windows and metal balconies (its lobby houses a large Foucault pendulum). In 1979, Occidental installed "Water Forms II", a kinetic fountain designed by professor George Baker. The fountain is a campus landmark and was featured prominently in the 1984 film "".

The campus is also noted for its outdoor Remsen Bird Amphitheater, also called the Remsen Bird Hillside Theatre, where between 1960 and 1996, a season of summer plays were performed, including Shakespeare plays and musicals. However, financial cutbacks caused the theater department to end its summer festival programs. Since 1996 the Occidental Children’s Theater has instead performed there each summer.

In 1989, the college dedicated Keck Theater, a post-modern theater with a movable stage and seating arrangements for a variety of different types of shows. It was designed by the architectural firm of Kanmnitzer and Cotton. The James Barrie version of the play "Peter Pan" was the first show performed at the opening ceremony in the summer of 1989.

Occidental College was ranked as the sixth "Most Beautiful" campus by "Newsweek" in 2012. The campus is home to a 1-megawatt ground-mounted solar array, which is the largest hillside array on an American college campus and the largest of its kind in Los Angeles. The 4,886-panel installation was completed in Spring 2013 and inaugurated on the school's 126-year anniversary.

There are 34 majors offered on campus (and nine minor-only programs, including Public Health, Linguistics, and Classical Studies) and a 9:1 student–faculty ratio. The average class size is 18 students and most students take four classes per semester.

Since 1908, Occidental has graduated 10 Rhodes Scholars. The 2017 edition of the "Fiske Guide to Colleges" gave Occidental four-star ratings (out of five) in academics and quality of life. Princeton Review's The Best 381 Colleges 2017 Edition gave Occidental ratings of 91 (out of 100) in academics and quality of life and 95 in financial aid. In "Forbes" 2019 ranking of America's Top Colleges, Occidental ranks 102nd out of 650 liberal arts colleges, universities and service academies. In "U.S. News & World Report"s 2020 rankings of American liberal arts colleges, Occidental is ranked tied for 39th overall, 67th for "Best Value", and tied at 90th for "Top Performers on Social Mobility. "Kiplinger's" "Best College Values 2019" rankings places Occidental 58th among 149 liberal arts colleges.

Fall Admission Statistics

"U.S. News" deemed Occidental's admissions "more selective," with the class of 2020 acceptance rate being 37.3%. Of those admitted submitting such data, 52% were in the top 10% of their high school class. The SAT 25th - 75th percentile scores were 1810-2120. Of those admitted to the class of 2020, 50% identified as persons of color, and 13% of those admitted were international students.

Divided in three parts, the Core Program was designed by the faculty of Occidental to unify and enhance the liberal arts education offered by the school. The Core Program requires students to achieve the following:

First-year seminars (eight course hours in total) are the centerpiece of the Core Program. Students are given a variety of class choices to fulfill the seminar requirement and to satisfy the first-year writing requirement. While the classes range in topic, each is based on a curriculum of cultural studies. The classes are designed to expose students to the rigor of college academics and to the four principles of the college mission—Excellence, Equity, Community, and Service.

The Core Program's emphasis on global literacy requires students to take a minimum of three courses that touch on at least three of the following geographical areas: Africa and the Middle East; Asia and the Pacific; Europe; Latin America; the United States; and Intercultural. Students are also required to demonstrate proficiency in writing and in a foreign language and take courses in the fine arts and in the sciences, mathematics, or other courses that address formal methods of reasoning.

The final portion of the Core Program requires students to pass a senior comprehensive examination in their chosen field. Comprehensive examinations may include seminars, creative projects, fieldwork, oral exams, theses, or field research projects.

Students at Occidental can take courses at the California Institute of Technology (Caltech) in nearby Pasadena free of charge. In addition, a 3-2 engineering program allows qualified students the opportunity to study at Occidental for three years, completing their undergraduate experience with an additional two years either at Caltech or Columbia University. At the end of the five years, the student receives two degrees, a Bachelor of Arts in the Combined Plan from Occidental and a Bachelor of Science in the selected field of engineering from the engineering school.

Art majors at Occidental College can take courses at the Art Center College of Design in Pasadena, one of the country's top-ranked art schools. The program is not open to first-year students, but as with the Caltech exchange program, students receive full course credit. No additional tuition payments are required.

With a competitive GPA and LSAT scores, Columbia Law School admits students upon completion of their junior year at Occidental into its Accelerated Interdisciplinary Program in Legal Education. Admittance to the program enables students to earn a bachelor's degree from Occidental and a law degree from Columbia in six years.

Students who are interested in biotechnology and who become a biochemistry major maintaining a 3.2 GPA in the necessary courses will be guaranteed admission to the Keck master's in bioscience program. The Keck Graduate Institute is part of the Claremont Colleges consortium.

At the beginning of every school year, freshmen participate in Convocation, a formal ceremony welcoming new students to the college in which the faculty wear their full academic regalia and students don robes. Founders Day is celebrated annually at the school on April 20, the day in 1887 when Occidental's incorporation papers were officially signed by the California Secretary of State.

For the first three years at Occidental, all students are guaranteed housing on campus and for seniors it is optional. Freshmen do not get to choose where they live; the Office of Residential Education & Housing Services arranges housing by pairing students based on a short form students fill in the summer before they arrive on campus. The Occidental College dorm life consists of 13 co-ed residential housing facilities.

After a student's first year, he or she can choose to live in a number of dorms that house sophomores, juniors, and seniors; one-third of all these halls are reserved for each grade. These dorms include Newcomb Hall, Wylie Hall, Erdman Hall, Haines Hall, Rangeview Hall and Stearns Hall.

There are also themed-living communities which consist of the Multicultural Hall in Pauley (open to all years), all-women housing (Berkus House, named after alumnus Dave Berkus), the E. Norris Hall, the Queer House, and the Food Justice house.

Occidental College has various student-run clubs, organizations and ventures such as the Green Bean Coffee Lounge, organic garden, and the student-managed bike sharing and repairing program. There are also traditional groups such as glee club, Greek organizations, and student media outlets.

The campus newspaper is "The Occidental", an independent, student-run publication. It has been published continuously since 1893. As of the 2019–20 school year, "The Occidental" publishes biweekly in print and weekly online.

KOXY is a student-run campus radio station, in operation in the 1960s and 1970s, and again since 2000. It originally operated on the frequency 104.7 in and around campus from 1968 to 2009, but switched to only being available by webstream in 2009. KOXY sponsors several on-campus events.

In 2010, Occidental College launched a TV station called CatAList, launched by then-students Daniel Watson and Raffy Cortina; Cortina was also the first Occidental student to be awarded with a Student Academy Award from the Academy of Motion Picture Arts and Sciences for his short "Bottled Up". The station produces 20–30 minutes of student-run content weekly on a variety of topics.

Occidental College's Greek Council consists of eight members: local sororities Alpha Lambda Phi Alpha, Delta Omicron Tau; national sorority Kappa Alpha Theta; local fraternity Zeta Tau Zeta (co-ed), and national fraternity, Sigma Alpha Epsilon. These greek organizations are social organizations as opposed to academic greek organizations. Occidental has a fall and a spring greek recruiting period; first year students are first eligible to participate in greek recruitment during the spring of their first year. Occidental also has two cultural greek organizations: Kappa Alpha Psi and Sigma Lambda Gamma. The college is working to expand their roster of greek organizations by adding Phi Beta Sigma, Delta Sigma Theta, and Zeta Phi Beta.

There are various entities at Occidental College that promote local community involvement opportunities in Eagle Rock, Highland Park and Los Angeles. These include the Urban and Environmental Policy Institute (UEPI), the Office of Community Engagement (OCE), the Center for Community Based Learning (CCBL), the Neighborhood Partnership Program (NPP), and Upward Bound.

Occidental is one of the five schools that founded the Southern California Intercollegiate Athletic Conference (SCIAC) in 1915 and is currently a member of the SCIAC and NCAA Division III. Occidental features 21 varsity sports teams and a program of club sports and intramural competition. Approximately 25 percent of the student body participates in a varsity sports program.

During the 2006–2007 athletic season, the Tigers cross country, American football and basketball teams were Southern California Intercollegiate Athletic Conference champions. In 2014, diver Jessica Robson set the Southern California Intercollegiate Athletic Conference records for both 1m and 3m diving. The school's Blackshirts Rugby union team was also league champion for the first time in five years. In 2011, Jeremy Castro ('99) and Patrick Guthrie ('86) steered the squad to a NSCRO final, falling to Longwood University 36-27 in Virginia Beach, Virginia. In addition the college has a dance team that also performs at every home football and basketball game.

Occidental athletics date back to 1894, when the College helped organize the first intercollegiate athletic competition in Southern California. Since then, Oxy has produced more than a dozen Olympians, world-record holders, and national champions, including 1935 national girls' tennis champion Pat Henry Yeomans '38, two-time diving gold medalist Sammy Lee '43, and pole vault silver medalist Bob Gutowski '57.

Occidental has long-standing football rivalries with Pomona College and Whittier College; the Tigers have played both the Sagehens and the Poets over 100 times. In 1982, the Occidental College football team had the rare opportunity for national prominence when, due to the 1982 National Football League strike, their game with San Diego was broadcast on national television. In 2017, Occidental cancelled the remainder of its football season due to lack of healthy players, as few as 30 in some cases. The team forfeited two games and was outscored in the other three 170-19.

In 2011, Occidental College lost a basketball game to Caltech with a score of 46 to 45 giving the Caltech Beavers their first conference win in 26 years and putting an end to their 310-game losing streak.

Famous Occidental College Tigers include NFL coach Jim E. Mora, former American Football League Most Valuable Player and politician Jack Kemp, former NFL player Vance Mueller, 2011 U.S. Senior Open Champion Olin Browne, and CFL player Justin Goltz (Winnipeg Blue Bombers).

Notable graduates of Occidental College include filmmaker Terry Gilliam, football player and politician Jack Kemp, pioneering African-American physicist and inventor George Edward Alcorn Jr., former New Orleans Saints and Indianapolis Colts head coach Jim E. Mora, co-inventor of the hard disk drive William Goddard, federal judge Jacqueline Nguyen, historian and chancellor of the California State University system Glenn Dumke, former Lieutenant Governor of California Robert Finch, adventurer and writer Homer Lea, poet Robinson Jeffers, librarian and writer Lawrence Clark Powell, civil rights activist Ernesto Galarza, television director Jesus Salvador Trevino, journalist and current dean of Columbia University Graduate School of Journalism Steve Coll, actor and writer George Nader, veteran executive at Walt Disney Imagineering Joe Rohde, and CEO of Warner Music Group Stephen Cooper.

Notable alumni who did not graduate include the 44th U.S. President Barack Obama, former First Lady of Colorado Dottie Lamm, Academy Award–winning actor and filmmaker Ben Affleck, actor Luke Wilson, producer Todd Garner, and actress Emily Osment.

Notable faculty members include the American urban policy analyst Peter Dreier, former U.S. Ambassador Derek Shearer, former CNN and Fox News contributor Caroline Heldman, chemist Frank L. Lambert, and the 2005 PEN American Center Literary Award winner in poetry Martha Ronk.

Occidental's campus, architecture, and proximity to Hollywood have made it a desired location for a number of film and television productions.

Film credits include:


TV credits include:





</doc>
<doc id="22801" url="https://en.wikipedia.org/wiki?curid=22801" title="1986 United States bombing of Libya">
1986 United States bombing of Libya

The 1986 United States bombing of Libya, code-named Operation El Dorado Canyon, comprised air strikes by the United States against Libya on Tuesday 15 April 1986. The attack was carried out by the U.S. Air Force, U.S. Navy and U.S. Marine Corps via air strikes, in retaliation for the West Berlin discotheque bombing ten days earlier. There were 40 reported Libyan casualties, and one U.S. plane was shot down. One of the claimed Libyan deaths was of a baby girl, reported to be Muammar Gaddafi's daughter, Hana Gaddafi. However, there were doubts as to whether she was really killed, or whether she really even existed.

Libya represented a high priority for President Ronald Reagan shortly after his 1981 inauguration. Libyan leader Muammar Gaddafi was firmly anti-Israel and had supported violent organizations in the Palestinian territories and Syria. There were reports that Libya was attempting to become a nuclear power and Gaddafi's occupation of Chad, which was rich in uranium, was of major concern to the United States. Gaddafi's ambitions to set up a federation of Arab and Muslim states in North Africa were alarming to U.S. interests. Furthermore, then-Secretary of State Alexander Haig wanted to take proactive measures against Gaddafi because he had been using former Central Intelligence Agency (CIA) operatives to help set up terrorist camps (most notably Edwin P. Wilson and Frank E. Terpil).

After the December 1985 Rome and Vienna airport attacks, which killed 19 and wounded approximately 140, Gaddafi indicated that he would continue to support the Red Army Faction, the Red Brigades, and the Irish Republican Army as long as the European governments supported anti-Gaddafi Libyans.

After years of occasional skirmishes with Libya over Libyan territorial claims to the Gulf of Sidra, the United States contemplated a military attack to strike targets within the Libyan mainland. In March 1986, the United States, asserting the limit to territorial waters according to international law, sent a carrier task force to the region. Libya responded with aggressive counter-maneuvers on 24 March that led to a naval engagement in the Gulf of Sidra.

On 5 April 1986, Libyan agents bombed "La Belle" nightclub in West Berlin, killing three people, including a U.S. serviceman, and injuring 229 people. West Germany and the United States obtained cable transcripts from Libyan agents in East Germany who were involved in the attack.

More detailed information was retrieved years later when Stasi archives were investigated by the reunited Germany. Libyan agents who had carried out the operation from the Libyan embassy in East Germany were identified and prosecuted by Germany in the 1990s.

The attack mission against Libya had been preceded in October 1985 by an exercise in which the 20th TFW stationed at RAF Upper Heyford airbase in the UK, which was equipped with F-111Es, received a top-secret order to launch a simulated attack mission on 18 October, with ten F-111Es armed with eight 500-lb practice bombs, against a simulated airfield located in Newfoundland, Canada south of CFB Goose Bay. The mission was designated Operation Ghost Rider. The mission was a full rehearsal for a long-range strike against Libya. The mission was completed successfully, with the exception of one aircraft that had all but one of its eight bombs hang up on one of its wing racks. The lessons learned were passed on to the 48th TFW which was equipped with the newer "F" models of the F-111.

Elements of the then-secret 4450th Tactical Group (USAF) were put on standby to fly the strike mission against Libya. Over 30 F-117s had already been delivered to Tactical Air Command (USAF) and were operating from Tonopah Test Range Airport in Nevada. Commanders in the North Africa/Mediterranean theaters knew nothing about the capabilities of the F-117, or that the aircraft even existed. Within an hour of the planned launch of the F-117s, the Secretary of Defense scrubbed the stealth mission, fearing a compromise of the secret aircraft and its development program. The air strike was carried out with conventional U.S. Navy and U.S. Air Force aircraft. The F-117 would remain completely unknown to the world for several more months, before being unveiled in 1988 and featured prominently in media coverage of Operation Desert Storm.

For the Libyan raid, the United States was denied overflight rights by France, Spain, and Italy as well as the use of European continental bases, forcing the Air Force portion of the operation to be flown around France and Spain, over Portugal and through the Straits of Gibraltar, adding 1,300 miles (2,100 km) each way and requiring multiple aerial refuelings. The French refusal alone added 2,800 km and was imposed despite the fact that France had itself been the target of terrorism directed by the Gaddafi government in Libya. French president Mitterrand refused overflight clearance because the United States was interested in limited action in Libya while France was more interested in major action that would remove Gaddafi from power. Another factor in the French decision was the United States' last-minute failure to participate in a retaliatory air raid on Iranian positions after the 1983 Beirut barracks bombings.

After several unproductive days of meeting with European and Arab nations, and influenced by an American serviceman's death, Ronald Reagan, on 14 April, ordered an air raid on the following Libyan targets:

Among operational United States tactical aircraft, only the General Dynamics F-111 and the A-6 Intruder, possessed the ability to attack at night with the required precision. Although the F-111s would be required to fly from distant bases, they were essential to mission success, because the eighteen A-6 available aboard and could not carry enough bombs to simultaneously inflict the desired damage on the five targets selected.

Twenty-eight McDonnell Douglas KC-10 Extenders and Boeing KC-135 Stratotankers took off from RAF Mildenhall and RAF Fairford shortly after 19:00 on 14 April. These tankers would conduct four silent refueling operations over the round-trip route the F-111s would fly to target. Within minutes the tankers were followed by twenty-four F-111F strike aircraft of the 48th Tactical Fighter Wing, flying from RAF Lakenheath and five EF-111A Ravens of the 20th Tactical Fighter Wing from RAF Upper Heyford. Six F-111s and one EF-111 were designated spares who returned to base after the first refueling was completed without any system failures among the designated strike aircraft.

"America" was on station in the Gulf of Sidra, but "Coral Sea" was preparing to leave the Mediterranean, and made a high-speed return from Spain. Naval aviators were dismayed when pre-raid news broadcasts eliminated any element of surprise by listing their mission times and target areas. "Americas" air group would strike targets in downtown Benghazi and provide fighter and suppression support for the Air Force bombers, while "Coral Seas" planes would strike the Benina airfield outside Benghazi and provide fighter and suppression support for the Navy bombers. About 01:00 "America" launched six A-6 strike aircraft with Mark 82 bombs for the Jamahiriyah Guard barracks and six A-7 strike support aircraft. "Coral Sea", operating east of "America" simultaneously launched eight A-6 and six F/A-18. Additional fighters were launched for combat air patrol (CAP).

The raid began in the early hours of 15 April, with the stated objectives of sending a message and reducing Libya's ability to support and train terrorists. Reagan warned that "if necessary, [they] shall do it again."

Coordinated jamming by the EF-111s and EA-6B Prowlers began at 01:54 (Libyan time) as the A-7s and F/A-18s began launching AGM-88 HARM and AGM-45 Shrike for SAM suppression. The attack began at 0200 hours (Libyan time), and lasted about twelve minutes, with 60 tons of munitions dropped. The F-111 bombers' rules of engagement required target identification by both radar and Pave Tack prior to bomb release to minimize collateral damage. Of the nine F-111s targeting Bab al-Azizia, only three placed their GBU-10 Paveway II bombs on target. One F-111 was shot down by a Libyan ZSU-23-4 over the Gulf of Sidra and one F-111's bombs missed the barracks, striking diplomatic and civilian sites in Tripoli, and narrowly missing the French embassy. All three F-111s assigned to Sidi Bilal released their GBU-10 bombs on target. One of the six F-111s assigned to bomb the Tripoli airfield aborted its mission with a terrain-following radar malfunction, but the remaining five dropped BSU-49 high drag bombs destroying two Il-76 transport aircraft. "Americas" A-6s damaged the Jamahiriyah MiG assembly warehouse and destroyed four MiG shipping crates. Two A-6s from "Coral Sea" aborted their mission, but five A-6s with CBU-59 APAM cluster bombs and one with Mk 82 bombs struck Benina airfield destroying three or four MiGs, two Mil Mi-8 helicopters, one Fokker F27 Friendship transport, and one small straight-wing aircraft.

Some Libyan soldiers abandoned their positions in fright and confusion, and officers were slow to give orders. Libyan anti-aircraft fire did not begin until after the planes had passed over their targets. No Libyan fighters launched, and HARM launches and jamming prevented any of the 2K12 Kub, S-75 Dvina, S-125 Neva/Pechora, or Crotale SAM launches from homing.

Within twelve minutes, all United States aircraft were "feet wet" outbound over the Mediterranean. Navy strike aircraft had been recovered aboard their carriers by 02:53 (Libyan time) and surviving Air Force planes, with the exception of one F-111 which landed in Rota, Andalusia with an overheated engine, had returned to Britain by 10:10 (Libyan time).

The Libyan air defense network was extensive, and included:
Covering Tripoli alone were:

Forewarned by a telephone call, Libyan leader Muammar Gaddafi and his family rushed out of their residence in the Bab al-Azizia compound moments before the bombs dropped. It was long thought that the call came from Malta's Prime Minister, Karmenu Mifsud Bonnici. However, Italian Prime Minister Bettino Craxi was the person who actually warned Gaddafi, according to Giulio Andreotti, Italy's foreign minister at the time, and to Abdel Rahman Shalgham, Libya's then-ambassador to Italy. Shalgham's statement was also confirmed by Margherita Boniver, foreign affairs chief of Craxi's Socialist Party at the time.

According to medical staff in a nearby hospital, two dozen casualties were brought in wearing military uniforms, and two without uniforms. Total Libyan casualties were estimated at 60, including those at the bombed airbases. An infant girl was among the casualties; her body was shown to American reporters, who were told she was Gaddafi's recently adopted daughter Hana. However, there was and remains much skepticism over the claim. She may not have died; the adoption may have been posthumous; or he may have adopted a second daughter and given her the same name after the first one died.

Two U.S. Air Force captains—Fernando L. Ribas-Dominicci and Paul F. Lorence—were killed when their F-111 fighter-bomber was shot down over the Gulf of Sidra. In the hours following the attack, the U.S. military refused to speculate as to whether or not the fighter-bomber had been shot down, with Defense Secretary Caspar Weinberger suggesting that it could have experienced radio trouble or been diverted to another airfield. The next day, the Pentagon had announced it was no longer searching for the F-111 believed to be downed by a Libyan missile. On 25 December 1988, Gaddafi offered to release the body of Lorence to his family through Pope John Paul II. The body, returned in 1989, was identified as Ribas-Dominicci's from dental records. An autopsy conducted in Spain confirmed that he had drowned after his plane was shot down over the Gulf of Sidra. Libya denies that it held Lorence's body. However, Lorence's brother said that he and his mother saw television footage of a Libyan holding a white helmet with the name "Lorence" stenciled on the back. Furthermore, William C. Chasey, who toured the Bab al-Azizia barracks, claimed to have seen two flight suits and helmets engraved with the names "Lorence" and "Ribas-Dominicci", as well as the wreckage of their F-111.

Gaddafi announced that he had "won a spectacular military victory over the United States" and the country was officially renamed the "Great Socialist People's Libyan Arab Jamahiriyah".

Gaddafi said reconciliation between Libya and the United States was impossible so long as Reagan was in the White House; of the president he said, "He is mad. He is foolish. He is an Israeli dog." He said he had no plans to attack the United States or U.S. targets. He claimed that Reagan wanted to kill him, stating "Was Reagan trying to kill me? Of course. The attack was concentrated on my house and I was in my house", he also described how he rescued his family.
When asked that if he is in danger of losing power, he told "Really, these reports and writings are not true. As you can see I am fine, and there has been no change in our country."

The Government of Libya said that the United States had fallen prey to arrogance and madness of power and wanted to become the world's policeman. It charged that any party that did not agree to become an American vassal was an outlaw, a terrorist, and a devil.

Gaddafi quashed an internal revolt, the organization of which he blamed on the United States, although Gaddafi appeared to have left the public sphere for a time in 1986 and 1987.

The Libyan Post dedicated several postage stamps issues to the event, from 1986 until 2001. The first issue was released in 1986, 13 July (ref. Scott catalogue n.1311 – Michel catalogue n.1699). The last issue was released in 2001, 15 April (ref. Scott catalogue n.1653 – Michel catalogue n.2748–2763).

Libya responded by firing two Scud missiles at a United States Coast Guard station on the Italian island of Lampedusa which passed over the island and landed in the sea.

There was only limited change in Libyan-connected terrorism.

The Libyan government was alleged to have ordered the hijacking of Pan Am Flight 73 in Pakistan on 5 September 1986, which resulted in the deaths of 20 people. The allegation did not come to light until it was reported by "The Sunday Times" in March 2004—days after British Prime Minister Tony Blair paid the first official visit to Tripoli by a Western leader in a generation.

In May 1987, Australia expelled diplomats and broke off relations with Libya, claiming Libya sought to fuel violence in Australia and Oceania.

In late 1987 French authorities stopped a merchant vessel, the MV "Eksund", which was attempting to deliver 150 tons of Soviet arms from Libya to the Irish Republican Army (IRA).

In Beirut, Lebanon, two British hostages held by the Libyan-supported Abu Nidal Organization, Leigh Douglas and Philip Padfield, along with an American named Peter Kilburn, were shot dead in revenge. In addition, journalist John McCarthy was kidnapped, and tourist Paul Appleby was murdered in Jerusalem. Another British hostage named Alec Collett was also killed in retaliation for the bombing of Libya. Collett was shown being hanged in a video tape. His body was found in November 2009.

On 21 December 1988 Libya bombed Pan Am Flight 103, which exploded in mid-air and crashed on the town of Lockerbie in Scotland after a bomb detonated, killing all 259 people aboard, and 11 people in Lockerbie. Iran was initially thought to have been responsible for the bombing in revenge for the downing of Iran Air flight 655 by the American missile cruiser USS "Vincennes" over the Persian Gulf, but in 1991 two Libyans were charged, one of whom was convicted of the crime in a controversial judgement on 31 January 2001. The Libyan Government accepted responsibility for the Pan Am Flight 103 bombing on 29 May 2002, and offered $2.7 billion to compensate the families of the 270 victims. The convicted Libyan, Abdelbaset al-Megrahi, who was suffering from terminal prostate cancer, was released in August 2009 by the Scottish Government on compassionate grounds. He died in 2012. In May 2014 a group of relatives of the Lockerbie victims continued to campaign for al-Megrahi's name to be cleared by reopening the case.

The attack was condemned by many countries. By a vote of 79 in favor to 28 against with 33 abstentions, the United Nations General Assembly adopted resolution 41/38 which "condemns the military attack perpetrated against the Socialist People's Libyan Arab Jamahiriya on 15 April 1986, which constitutes a violation of the Charter of the United Nations and of international law."

A meeting of the Non-Aligned Movement said that it condemned the "dastardly, blatant and unprovoked act of aggression". The League of Arab States expressed that it was outraged at the United States aggression and that it reinforced an element of anarchy in international relations. The Assembly of Heads of State of the African Union in its declaration said that the deliberate attempt to kill Libyans violated the principles of international law. The Government of Iran asserted that the attack constituted a policy of aggression, gunboat diplomacy, an act of war, and called for an extensive political and economic boycott of the United States. Others saw the United States motive as an attempt to eliminate Libya's revolution.
China stated that the U.S. attack violated norms of international relations and had aggravated tension in the region. The Soviet Union said that there was a clear link between the attack and U.S. policy aimed at stirring up existing hotbeds of tension and creating new ones, and at destabilizing the international situation. West Germany stated that international disputes required diplomatic and not military solutions, and France also criticized the bombing.

Some observers held the opinion that Article 51 of the UN Charter set limitations on the use of force in exercising the legitimate right of self-defense in the absence of an act of aggression, and affirmed that there was no such act by Libya. It was charged that the United States did not exhaust the Charter provisions for settling disputes under Article 33. The "Wall Street Journal" protested that if other nations applied Article 51 as cavalierly as the United States, then "the Nicaraguan government, very reasonably predicting that the U.S. is planning an attack on its territory, has the right to bomb Washington." British Shadow Foreign Secretary Denis Healey told ABC News that, "by this same rationale of defense against future attack, Britain could bomb apartment blocks in New York and Chicago on the ground that they contained people sending money and military supplies to the Irish Republican Army."

Others asserted that Libya was innocent in the bombing of the West Berlin discotheque.

The U.S. received support from the UK, Canada, Australia, Israel, and 25 other countries. Its doctrine of declaring a war on what it called "terrorist havens" was not repeated until 1998, when President Bill Clinton ordered strikes on six terrorist camps in Afghanistan. Margaret Thatcher's approval of the use of Royal Air Force bases led to substantial public criticism, including an unprecedented story in "The Sunday Times" suggesting the Queen was upset by an "uncaring" Prime Minister. However, the Americans strongly endorsed Thatcher, and the long-standing Special Relationship between the United States and Britain was strengthened.

Although the Soviet Union was ostensibly friendly with Libya, it had, by the time of the Libya bombing, made its increasing ambivalence toward Libya apparent in public communications. Gaddafi had a history of verbally attacking the policy agendas and ideology of the Soviet Union, and he often engaged in various international interventions and meddling that conflicted with Soviet goals in a variety of spheres. During a period where the Soviet Union was apparently attempting to lead a subtle diplomatic effort that could impact its global status, close association with the whims of Gaddafi became a liability.

In the entire crisis, the Soviet Union explicitly announced that it would not provide additional help to Libya beyond resupplying basic armaments and munitions. It made no attempt to militarily intimidate the United States, despite the ongoing American operations in the Gulf of Sidra and its previous knowledge that the United States might launch an attack. The Soviet Union did not completely ignore the event, issuing a denunciation of this 'wild' and 'barbaric' act by the United States.

After the raid, Moscow did cancel a planned visit to the United States by foreign affairs minister Eduard Shevardnadze. At the same time, it clearly signaled that it did not want this action to affect negotiations about the upcoming summer summit between the United States and the Soviet Union and its plans for new arms control agreements.

Former U.S. Attorney General Ramsey Clark, acting for Libyan citizens who had been killed or injured in the bombing raid by the U.S. using British air bases, brought suit under international law against the United States and the United Kingdom in U.S. federal court. The lawsuit was dismissed as frivolous. A subsequent appeal was denied, and monetary sanctions against Clark were allowed. Saltany v. Reagan, 886 F. 2d 438 (D.C. Cir. 1989).

Every year, between at least 1994 and 2006, the United Nations General Assembly scheduled a declaration from the Organization of African Unity about the incident, but systematically deferred the discussion year after year until formally putting it aside (along with several other issues which had been similarly rescheduled for years) in 2005.

On the first anniversary of the bombing, April 1987, European and North American left-wing activists gathered to commemorate the anniversary. After a day of social and cultural networking with local Libyans, including a tour of Gaddafi's bombed house, the group gathered with other Libyans for a commemoration event.

In June 2009, during a visit to Italy, Colonel Gaddafi criticized American foreign policy and, asked as to the difference between al-Qaeda attacks and the 1986 U.S. bombing of Tripoli, he commented: "If al-Qaeda leader Osama Bin Laden has no state and is an outlaw, America is a state with international rules."

On 28 May 2008, the United States began negotiations with Libya on a comprehensive claims settlement agreement to resolve outstanding claims of American and Libyan nationals against each country in their respective courts. Gaddafi's son Saif al-Islam publicly announced that an agreement was being negotiated in July of that year. On 14 August 2008, the resulting U.S.-Libya Comprehensive Claims Settlement Agreement was signed in Tripoli by Assistant Secretary of State for Near Eastern Affairs David Welch and by Libyan Secretary for American Affairs Ahmad Fituri.

In October 2008, Libya paid US$1.5 billion (in three installments of $300 million on 9 October 2008, $600 million on 30 October 2008, and US$600 million 31 October 2008) into a fund used to compensate the following victims and their relatives:

To pay the settlement, Libya demanded US$1.5 billion from global oil companies operating in Libya's oil fields, under threat of "serious consequences" to their leases. Libya's settlement was at least partially funded by several companies, including some based in the U.S., that chose to cooperate with Libya's demand.

On 4 August 2008, President George W. Bush signed into law the Libyan Claims Resolution Act, which had unanimously passed Congress on 31 July. The Act provided for the restoration of Libya's sovereign, diplomatic, and official immunities before U.S. courts if the Secretary of State certified that the United States Government has received sufficient funds to resolve outstanding terrorism-related death and physical injury claims against Libya.

On 14 August 2008, the United States and Libya signed a comprehensive claims settlement agreement. Full diplomatic relations were restored between the two nations.

In 1986, hardcore punk band The Meatmen referred to the lack of French cooperation with the raid in their song 'French People Suck': "French people suck, I just gotta' say/made the jet fighter pilots fly out of their way." This song appears on the album "Rock & Roll Juggernaut" (Caroline Records).

In 1987, Neil Young wrote "Mideast Vacation" a song from his live album, "Life" about the bombing.

On Roger Waters' third studio album, "Amused to Death" the songs Late Home Tonight, Part I and Late Home Tonight, Part II, recalls the bombing from the perspective of two "ordinary wives' and a young American F-111 pilot.

In Nelson DeMille's book "The Lion's Game", published in 2000, there is a detailed but fictionalised description of the attack from the point of view of one of the book's main protagonists.





</doc>
<doc id="22804" url="https://en.wikipedia.org/wiki?curid=22804" title="Operational amplifier">
Operational amplifier

An operational amplifier (often op amp or opamp) is a DC-coupled high-gain electronic voltage amplifier with a differential input and, usually, a single-ended output. In this configuration, an op amp produces an output potential (relative to circuit ground) that is typically 100,000 times larger than the potential difference between its input terminals.
Operational amplifiers had their origins in analog computers, where they were used to perform mathematical operations in many linear, non-linear, and frequency-dependent circuits. 

The popularity of the op amp as a building block in analog circuits is due to its versatility. By using negative feedback, the characteristics of an op-amp circuit, its gain, input and output impedance, bandwidth etc. are determined by external components and have little dependence on temperature coefficients or engineering tolerance in the op amp itself.

Op amps are among the most widely used electronic devices today, being used in a vast array of consumer, industrial, and scientific devices. Many standard IC op amps cost only a few cents in moderate production volume; however, some integrated or hybrid operational amplifiers with special performance specifications may cost over in small quantities. Op amps may be packaged as components or used as elements of more complex integrated circuits.

The op amp is one type of differential amplifier. Other types of differential amplifier include the fully differential amplifier (similar to the op amp, but with two outputs), the instrumentation amplifier (usually built from three op amps), the isolation amplifier (similar to the instrumentation amplifier, but with tolerance to common-mode voltages that would destroy an ordinary op amp), and negative-feedback amplifier (usually built from one or more op amps and a resistive feedback network).

The amplifier's differential inputs consist of a non-inverting input (+) with voltage "V" and an inverting input (–) with voltage "V"; ideally the op amp amplifies only the difference in voltage between the two, which is called the "differential input voltage". The output voltage of the op amp "V" is given by the equation
where "A" is the open-loop gain of the amplifier (the term "open-loop" refers to the absence of a feedback loop from the output to the input).

The magnitude of "A" is typically very large (100,000 or more for integrated circuit op amps), and therefore even a quite small difference between "V" and "V" drives the amplifier output nearly to the supply voltage. Situations in which the output voltage is equal to or greater than the supply voltage are referred to as "saturation" of the amplifier. The magnitude of "A" is not well controlled by the manufacturing process, and so it is impractical to use an open-loop amplifier as a stand-alone differential amplifier.

Without negative feedback, and perhaps with positive feedback for regeneration, an op amp acts as a comparator. If the inverting input is held at ground (0 V) directly or by a resistor "R", and the input voltage "V" applied to the non-inverting input is positive, the output will be maximum positive; if "V" is negative, the output will be maximum negative. Since there is no feedback from the output to either input, this is an "open-loop" circuit acting as a comparator.

If predictable operation is desired, negative feedback is used, by applying a portion of the output voltage to the inverting input. The "closed-loop" feedback greatly reduces the gain of the circuit. When negative feedback is used, the circuit's overall gain and response becomes determined mostly by the feedback network, rather than by the op-amp characteristics. If the feedback network is made of components with values small relative to the op amp's input impedance, the value of the op amp's open-loop response "A" does not seriously affect the circuit's performance. The response of the op-amp circuit with its input, output, and feedback circuits to an input is characterized mathematically by a transfer function; designing an op-amp circuit to have a desired transfer function is in the realm of electrical engineering. The transfer functions are important in most applications of op amps, such as in analog computers. High input impedance at the input terminals and low output impedance at the output terminal(s) are particularly useful features of an op amp.

In the non-inverting amplifier on the right, the presence of negative feedback via the voltage divider "R", "R" determines the "closed-loop gain" "A" = "V" / "V". Equilibrium will be established when "V" is just sufficient to "reach around and pull" the inverting input to the same voltage as "V". The voltage gain of the entire circuit is thus 1 + "R"/"R". As a simple example, if "V" = 1 V and R = "R", "V" will be 2 V, exactly the amount required to keep "V" at 1 V. Because of the feedback provided by the "R", "R" network, this is a "closed-loop" circuit.

Another way to analyze this circuit proceeds by making the following (usually valid) assumptions:

The input signal "V" appears at both (+) and (−) pins, resulting in a current "i" through "R" equal to "V"/"R":

Since Kirchhoff's current law states that the same current must leave a node as enter it, and since the impedance into the (−) pin is near infinity, we can assume practically all of the same current "i" flows through "R", creating an output voltage

By combining terms, we determine the closed-loop gain "A":

An ideal op amp is usually considered to have the following characteristics:

These ideals can be summarized by the two "golden rules":

The first rule only applies in the usual case where the op amp is used in a closed-loop design (negative feedback, where there is a signal path of some sort feeding back from the output to the inverting input). These rules are commonly used as a good first approximation for analyzing or designing op-amp circuits.

None of these ideals can be perfectly realized. A real op amp may be modeled with non-infinite or non-zero parameters using equivalent resistors and capacitors in the op-amp model. The designer can then include these effects into the overall performance of the final circuit. Some parameters may turn out to have negligible effect on the final design while others represent actual limitations of the final performance that must be evaluated.

Real op amps differ from the ideal model in various aspects.

Real operational amplifiers suffer from several non-ideal effects:










The op-amp gain calculated at DC does not apply at higher frequencies. Thus, for high-speed operation, more sophisticated considerations must be used in an op-amp circuit design.










Modern integrated FET or MOSFET op amps approximate more closely the ideal op amp than bipolar ICs when it comes to input impedance and input bias currents. Bipolars are generally better when it comes to input "voltage" offset, and often have lower noise. Generally, at room temperature, with a fairly large signal, and limited bandwidth, FET and MOSFET op amps now offer better performance.

Sourced by many manufacturers, and in multiple similar products, an example of a bipolar transistor operational amplifier is the 741 integrated circuit designed in 1968 by David Fullagar at Fairchild Semiconductor after Bob Widlar's LM301 integrated circuit design. 
In this discussion, we use the parameters of the hybrid-pi model to characterize the small-signal, grounded emitter characteristics of a transistor. In this model, the current gain of a transistor is denoted "h", more commonly called the β.

A small-scale integrated circuit, the 741 op amp shares with most op amps an internal structure consisting of three gain stages:
Additionally, it contains current mirror (outlined red) bias circuitry and compensation capacitor (30 pF).

The input stage consists of a cascaded differential amplifier (outlined in blue) followed by a current-mirror active load. This constitutes a transconductance amplifier, turning a differential voltage signal at the bases of Q1, Q2 into a current signal into the base of Q15.

It entails two cascaded transistor pairs, satisfying conflicting requirements. 
The first stage consists of the matched NPN emitter follower pair Q1, Q2 that provide high input impedance. The second is the matched PNP common-base pair Q3, Q4 that eliminates the undesirable Miller effect; it drives an active load Q7 plus matched pair Q5, Q6.

That active load is implemented as a modified Wilson current mirror; its role is to convert the (differential) input current signal to a single-ended signal without the attendant 50% losses (increasing the op amp's open-loop gain by 3 dB). 
Thus, a small-signal differential current in Q3 versus Q4 appears summed (doubled) at the base of Q15, the input of the voltage gain stage. 
The (class-A) voltage gain stage (outlined in magenta) consists of the two NPN transistors Q15/Q19 connected in a Darlington configuration and uses the output side of current mirror Q12/Q13 as its collector (dynamic) load to achieve its high voltage gain. The output sink transistor Q20 receives its base drive from the common collectors of Q15 and Q19; the level-shifter Q16 provides base drive for the output source transistor Q14. 

The transistor Q22 prevents this stage from delivering excessive current to Q20 and thus limits the output sink current.

The output stage (Q14, Q20, outlined in cyan) is a Class AB complementary-symmetry amplifier. It provides an output drive with impedance of ~50Ω, in essence, current gain. 
Transistor Q16 (outlined in green) provides the quiescent current for the output transistors, and Q17 provides output current limiting. 
Provide appropriate quiescent current for each stage of the op amp.

The resistor (39 kΩ) connecting the (diode-connected) Q11 and Q12, and the given supply voltage ("V" − "V"), determine the current in the current mirrors, (matched pairs) Q10/Q11 and Q12/Q13. The collector current of Q11, "i" × 39 kΩ = "V" − "V" − 2 "V". For the typical "V" = ±20 V, the standing current in Q11/Q12 (as well as in Q13) would be ~1 mA. A supply current for a typical 741 of about 2 mA agrees with the notion that these two bias currents dominate the quiescent supply current.

Transistors Q11 and Q10 form a Widlar current mirror, with quiescent current in Q10 "i" such that ln("i" / "i") = "i" × 5 kΩ / 28 mV, where 5 kΩ represents the emitter resistor of Q10, and 28 mV is "V", the thermal voltage at room temperature. In this case "i" ≈ 20 μA.

The biasing circuit of this stage is set by a feedback loop that forces the collector currents of Q10 and Q9 to (nearly) match. The small difference in these currents provides the drive for the common base of Q3/Q4 (note that the base drive for input transistors Q1/Q2 is the input bias current and must be sourced externally). The summed quiescent currents of Q1/Q3 plus Q2/Q4 is mirrored from Q8 into Q9, where it is summed with the collector current in Q10, the result being applied to the bases of Q3/Q4.

The quiescent currents of Q1/Q3 (resp., Q2/Q4) "i" will thus be half of "i", of order ~10 μA. Input bias current for the base of Q1 (resp. Q2) will amount to "i" / β; typically ~50 nA, implying a current gain "h" ≈ 200 for Q1(Q2).

This feedback circuit tends to draw the common base node of Q3/Q4 to a voltage "V" − 2 "V", where "V" is the input common-mode voltage. At the same time, the magnitude of the quiescent current is relatively insensitive to the characteristics of the components Q1–Q4, such as "h", that would otherwise cause temperature dependence or part-to-part variations.

Transistor Q7 drives Q5 and Q6 into conduction until their (equal) collector currents match that of Q1/Q3 and Q2/Q4. The quiescent current in Q7 is "V" / 50 kΩ, about 35 μA, as is the quiescent current in Q15, with its matching operating point. Thus, the quiescent currents are pairwise matched in Q1/Q2, Q3/Q4, Q5/Q6, and Q7/Q15. 

Quiescent currents in Q16 and Q19 are set by the current mirror Q12/Q13, which is running at ~1 mA. Through some mechanism, the collector current in Q19 tracks that standing current.

In the circuit involving Q16 (variously named rubber diode or "V" multiplier), the 4.5 kΩ resistor must be conducting about 100 μA, with the Q16 "V" roughly 700 mV. Then the "V" must be about 0.45 V and "V" at about 1.0 V. Because the Q16 collector is driven by a current source and the Q16 emitter drives into the Q19 collector current sink, the Q16 transistor establishes a voltage difference between Q14 base and Q20 base of ~1 V, regardless of the common-mode voltage of Q14/Q20 base. The standing current in Q14/Q20 will be a factor exp(100 mV / "V") ≈ 36 smaller than the 1 mA quiescent current in the class A portion of the op amp. This (small) standing current in the output transistors establishes the output stage in class AB operation and reduces the crossover distortion of this stage. 
A small differential input voltage signal gives rise, through multiple stages of current amplification, to a much larger voltage signal on output.

The input stage with Q1 and Q3 is similar to an emitter-coupled pair (long-tailed pair), with Q2 and Q4 adding some degenerating impedance. The input impedance is relatively high because of the small current through Q1-Q4. A typical 741 op amp has a differential input impedance of about 2 MΩ. The common mode input impedance is even higher, as the input stage works at an essentially constant current.

A differential voltage "V" at the op amp inputs (pins 3 and 2, respectively) gives rise to a small differential current in the bases of Q1 and Q2 "i" ≈ "V" / (2"h""h"). This differential base current causes a change in the differential collector current in each leg by "i""h". Introducing the transconductance of Q1, "g" = "h" / "h", the (small-signal) current at the base of Q15 (the input of the voltage gain stage) is "V""g" / 2.

This portion of the op amp cleverly changes a differential signal at the op amp inputs to a single-ended signal at the base of Q15, and in a way that avoids wastefully discarding the signal in either leg. To see how, notice that a small negative change in voltage at the inverting input (Q2 base) drives it out of conduction, and this incremental decrease in current passes directly from Q4 collector to its emitter, resulting in a decrease in base drive for Q15. On the other hand, a small positive change in voltage at the non-inverting input (Q1 base) drives this transistor into conduction, reflected in an increase in current at the collector of Q3. This current drives Q7 further into conduction, which turns on current mirror Q5/Q6. Thus, the increase in Q3 emitter current is mirrored in an increase in Q6 collector current; the increased collector currents shunts more from the collector node and results in a decrease in base drive current for Q15. Besides avoiding wasting 3 dB of gain here, this technique decreases common-mode gain and feedthrough of power supply noise. 
A current signal "i" at Q15's base gives rise to a current in Q19 of order "i"β (the product of the "h" of each of Q15 and Q19, which are connected in a Darlington pair). This current signal develops a voltage at the bases of output transistors Q14/Q20 proportional to the "h" of the respective transistor.

Output transistors Q14 and Q20 are each configured as an emitter follower, so no voltage gain occurs there; instead, this stage provides current gain, equal to the "h" of Q14 (resp. Q20).

The output impedance is not zero, as it would be in an ideal op amp, but with negative feedback it approaches zero at low frequencies.

The net open-loop small-signal voltage gain of the op amp involves the product of the current gain "h" of some 4 transistors. In practice, the voltage gain for a typical 741-style op amp is of order 200,000, and the current gain, the ratio of input impedance (~2−6 MΩ) to output impedance (~50Ω) provides yet more (power) gain.

The ideal op amp has infinite common-mode rejection ratio, or zero common-mode gain.

In the present circuit, if the input voltages change in the same direction, the negative feedback makes Q3/Q4 base voltage follow (with 2"V" below) the input voltage variations. Now the output part (Q10) of Q10-Q11 current mirror keeps up the common current through Q9/Q8 constant in spite of varying voltage. Q3/Q4 collector currents, and accordingly the output current at the base of Q15, remain unchanged.

In the typical 741 op amp, the common-mode rejection ratio is 90 dB, implying an open-loop common-mode voltage gain of about 6.

The innovation of the Fairchild μA741 was the introduction of frequency compensation via an on-chip (monolithic) capacitor, simplifying application of the op amp by eliminating the need for external components for this function. 
The 30 pF capacitor stabilizes the amplifier via Miller compensation and functions in a manner similar to an op-amp integrator circuit. Also known as 'dominant pole compensation' because it introduces a pole that masks (dominates) the effects of other poles into the open loop frequency response; in a 741 op amp this pole can be as low as 10 Hz (where it causes a −3 dB loss of open loop voltage gain).

This internal compensation is provided to achieve unconditional stability of the amplifier in negative feedback configurations where the feedback network is non-reactive and the closed loop gain is unity or higher. 
By contrast, amplifiers requiring external compensation, such as the μA748, may require external compensation or closed-loop gains significantly higher than unity.

The "offset null" pins may be used to place external resistors (typically in the form of the two ends of a potentiometer, with the slider connected to "V") in parallel with the emitter resistors of Q5 and Q6, to adjust the balance of the Q5/Q6 current mirror. The potentiometer is adjusted such that the output is null (midrange) when the inputs are shorted together.

The transistors Q3, Q4 help to increase the reverse "V" rating: the base-emitter junctions of the NPN transistors Q1 and Q2 break down at around 7V, but the PNP transistors Q3 and Q4 have "V" breakdown voltages around 50V.

Variations in the quiescent current with temperature, or between parts with the same type number, are common, so crossover distortion and quiescent current may be subject to significant variation.

The output range of the amplifier is about one volt less than the supply voltage, owing in part to "V" of the output transistors Q14 and Q20.

The 25 Ω resistor at the Q14 emitter, along with Q17, acts to limit Q14 current to about 25 mA; otherwise, Q17 conducts no current.

Current limiting for Q20 is performed in the voltage gain stage: Q22 senses the voltage across Q19's emitter resistor (50Ω); as it turns on, it diminishes the drive current to Q15 base.

Later versions of this amplifier schematic may show a somewhat different method of output current limiting.

While the 741 was historically used in audio and other sensitive equipment, such use is now rare because of the improved noise performance of more modern op amps. Apart from generating noticeable hiss, 741s and other older op amps may have poor common-mode rejection ratios and so will often introduce cable-borne mains hum and other common-mode interference, such as switch 'clicks', into sensitive equipment.

The "741" has come to often mean a generic op-amp IC (such as μA741, LM301, 558, LM324, TBA221 — or a more modern replacement such as the TL071). The description of the 741 output stage is qualitatively similar for many other designs (that may have quite different input stages), except:

Op amps may be classified by their construction:
IC op amps may be classified in many ways, including:

The use of op amps as circuit blocks is much easier and clearer than specifying all their individual circuit elements (transistors, resistors, etc.), whether the amplifiers used are integrated or discrete circuits. In the first approximation op amps can be used as if they were ideal differential gain blocks; at a later stage limits can be placed on the acceptable range of parameters for each op amp.

Circuit design follows the same lines for all electronic circuits. A specification is drawn up governing what the circuit is required to do, with allowable limits. For example, the gain may be required to be 100 times, with a tolerance of 5% but drift of less than 1% in a specified temperature range; the input impedance not less than one megohm; etc.

A basic circuit is designed, often with the help of circuit modeling (on a computer). Specific commercially available op amps and other components are then chosen that meet the design criteria within the specified tolerances at acceptable cost. If not all criteria can be met, the specification may need to be modified.

A prototype is then built and tested; changes to meet or improve the specification, alter functionality, or reduce the cost, may be made.

That is, the op amp is being used as a voltage comparator. Note that a device designed primarily as a comparator may be better if, for instance, speed is important or a wide range of input voltages may be found, since such devices can quickly recover from full on or full off ("saturated") states.

A "voltage level detector" can be obtained if a reference voltage "V" is applied to one of the op amp's inputs. This means that the op amp is set up as a comparator to detect a positive voltage. If the voltage to be sensed, "E", is applied to op amp's (+) input, the result is a noninverting positive-level detector: when "E" is above "V", "V" equals +"V"; when "E" is below "V", "V" equals −"V". If "E" is applied to the inverting input, the circuit is an inverting positive-level detector: When "E" is above "V", "V" equals −"V".

A "zero voltage level detector" ("E" = 0) can convert, for example, the output of a sine-wave from a function generator into a variable-frequency square wave. If "E" is a sine wave, triangular wave, or wave of any other shape that is symmetrical around zero, the zero-crossing detector's output will be square. Zero-crossing detection may also be useful in triggering TRIACs at the best time to reduce mains interference and current spikes.

Another typical configuration of op-amps is with positive feedback, which takes a fraction of the output signal back to the non-inverting input. An important application of it is the comparator with hysteresis, the Schmitt trigger. Some circuits may use "positive" feedback and "negative" feedback around the same amplifier, for example triangle-wave oscillators and active filters.

Because of the wide slew range and lack of positive feedback, the response of all the open-loop level detectors described above will be relatively slow. External overall positive feedback may be applied, but (unlike internal positive feedback that may be applied within the latter stages of a purpose-designed comparator) this markedly affects the accuracy of the zero-crossing detection point. Using a general-purpose op amp, for example, the frequency of "E" for the sine to square wave converter should probably be below 100 Hz.

In a non-inverting amplifier, the output voltage changes in the same direction as the input voltage.

The gain equation for the op amp is

However, in this circuit "V" is a function of "V" because of the negative feedback through the "R" "R" network. "R" and "R" form a voltage divider, and as "V" is a high-impedance input, it does not load it appreciably. Consequently

where

Substituting this into the gain equation, we obtain

Solving for formula_9:

If formula_11 is very large, this simplifies to

The non-inverting input of the operational amplifier needs a path for DC to ground; if the signal source does not supply a DC path, or if that source requires a given load impedance, then the circuit will require another resistor from the non-inverting input to ground. When the operational amplifier's input bias currents are significant, then the DC source resistances driving the inputs should be balanced. The ideal value for the feedback resistors (to give minimal offset voltage) will be such that the two resistances in parallel roughly equal the resistance to ground at the non-inverting input pin. That ideal value assumes the bias currents are well matched, which may not be true for all op amps.

In an inverting amplifier, the output voltage changes in an opposite direction to the input voltage.

As with the non-inverting amplifier, we start with the gain equation of the op amp:

This time, "V" is a function of both "V" and "V" due to the voltage divider formed by "R" and "R". Again, the op-amp input does not apply an appreciable load, so

Substituting this into the gain equation and solving for formula_9:

If formula_11 is very large, this simplifies to

A resistor is often inserted between the non-inverting input and ground (so both inputs "see" similar resistances), reducing the input offset voltage due to different voltage drops due to bias current, and may reduce distortion in some op amps.

A DC-blocking capacitor may be inserted in series with the input resistor when a frequency response down to DC is not needed and any DC voltage on the input is unwanted. That is, the capacitive component of the input impedance inserts a DC zero and a low-frequency pole that gives the circuit a bandpass or high-pass characteristic.

The potentials at the operational amplifier inputs remain virtually constant (near ground) in the inverting configuration. The constant operating potential typically results in distortion levels that are lower than those attainable with the non-inverting topology.


Most single, dual and quad op amps available have a standardized pin-out which permits one type to be substituted for another without wiring changes. A specific op amp may be chosen for its open loop gain, bandwidth, noise performance, input impedance, power consumption, or a compromise between any of these factors.

1941: A vacuum tube op amp. An op amp, defined as a general-purpose, DC-coupled, high gain, inverting feedback amplifier, is first found in "Summing Amplifier" filed by Karl D. Swartzel Jr. of Bell Labs in 1941. This design used three vacuum tubes to achieve a gain of and operated on voltage rails of . It had a single inverting input rather than differential inverting and non-inverting inputs, as are common in today's op amps. Throughout World War II, Swartzel's design proved its value by being liberally used in the M9 artillery director designed at Bell Labs. This artillery director worked with the SCR584 radar system to achieve extraordinary hit rates (near 90%) that would not have been possible otherwise.

1947: An op amp with an explicit non-inverting input. In 1947, the operational amplifier was first formally defined and named in a paper by John R. Ragazzini of Columbia University. In this same paper a footnote mentioned an op-amp design by a student that would turn out to be quite significant. This op amp, designed by Loebe Julie, was superior in a variety of ways. It had two major innovations. Its input stage used a long-tailed triode pair with loads matched to reduce drift in the output and, far more importantly, it was the first op-amp design to have two inputs (one inverting, the other non-inverting). The differential input made a whole range of new functionality possible, but it would not be used for a long time due to the rise of the chopper-stabilized amplifier.

1949: A chopper-stabilized op amp. In 1949, Edwin A. Goldberg designed a chopper-stabilized op amp. This set-up uses a normal op amp with an additional AC amplifier that goes alongside the op amp. The chopper gets an AC signal from DC by switching between the DC voltage and ground at a fast rate (60 Hz or 400 Hz). This signal is then amplified, rectified, filtered and fed into the op amp's non-inverting input. This vastly improved the gain of the op amp while significantly reducing the output drift and DC offset. Unfortunately, any design that used a chopper couldn't use their non-inverting input for any other purpose. Nevertheless, the much improved characteristics of the chopper-stabilized op amp made it the dominant way to use op amps. Techniques that used the non-inverting input regularly would not be very popular until the 1960s when op-amp ICs started to show up in the field.

1953: A commercially available op amp. In 1953, vacuum tube op amps became commercially available with the release of the model K2-W from George A. Philbrick Researches, Incorporated. The designation on the devices shown, GAP/R, is an acronym for the complete company name. Two nine-pin 12AX7 vacuum tubes were mounted in an octal package and had a model K2-P chopper add-on available that would effectively "use up" the non-inverting input. This op amp was based on a descendant of Loebe Julie's 1947 design and, along with its successors, would start the widespread use of op amps in industry.

1961: A discrete IC op amp. With the birth of the transistor in 1947, and the silicon transistor in 1954, the concept of ICs became a reality. The introduction of the planar process in 1959 made transistors and ICs stable enough to be commercially useful. By 1961, solid-state, discrete op amps were being produced. These op amps were effectively small circuit boards with packages such as edge connectors. They usually had hand-selected resistors in order to improve things such as voltage offset and drift. The P45 (1961) had a gain of 94 dB and ran on ±15 V rails. It was intended to deal with signals in the range of .

1961: A varactor bridge op amp. There have been many different directions taken in op-amp design. Varactor bridge op amps started to be produced in the early 1960s. They were designed to have extremely small input current and are still amongst the best op amps available in terms of common-mode rejection with the ability to correctly deal with hundreds of volts at their inputs.

1962: An op amp in a potted module. By 1962, several companies were producing modular potted packages that could be plugged into printed circuit boards. These packages were crucially important as they made the operational amplifier into a single black box which could be easily treated as a component in a larger circuit.

1963: A monolithic IC op amp. In 1963, the first monolithic IC op amp, the μA702 designed by Bob Widlar at Fairchild Semiconductor, was released. Monolithic ICs consist of a single chip as opposed to a chip and discrete parts (a discrete IC) or multiple chips bonded and connected on a circuit board (a hybrid IC). Almost all modern op amps are monolithic ICs; however, this first IC did not meet with much success. Issues such as an uneven supply voltage, low gain and a small dynamic range held off the dominance of monolithic op amps until 1965 when the μA709 (also designed by Bob Widlar) was released.

1968: Release of the μA741. The popularity of monolithic op amps was further improved upon the release of the LM101 in 1967, which solved a variety of issues, and the subsequent release of the μA741 in 1968. The μA741 was extremely similar to the LM101 except that Fairchild's facilities allowed them to include a 30 pF compensation capacitor inside the chip instead of requiring external compensation. This simple difference has made the 741 "the" canonical op amp and many modern amps base their pinout on the 741s. The μA741 is still in production, and has become ubiquitous in electronics—many manufacturers produce a version of this classic chip, recognizable by part numbers containing "741". The same part is manufactured by several companies.

1970: First high-speed, low-input current FET design.
In the 1970s high speed, low-input current designs started to be made by using FETs. These would be largely replaced by op amps made with MOSFETs in the 1980s. 
1972: Single sided supply op amps being produced. A single sided supply op amp is one where the input and output voltages can be as low as the negative power supply voltage instead of needing to be at least two volts above it. The result is that it can operate in many applications with the negative supply pin on the op amp being connected to the signal ground, thus eliminating the need for a separate negative power supply.

The LM324 (released in 1972) was one such op amp that came in a quad package (four separate op amps in one package) and became an industry standard. In addition to packaging multiple op amps in a single package, the 1970s also saw the birth of op amps in hybrid packages. These op amps were generally improved versions of existing monolithic op amps. As the properties of monolithic op amps improved, the more complex hybrid ICs were quickly relegated to systems that are required to have extremely long service lives or other specialty systems.

Recent trends. Recently supply voltages in analog circuits have decreased (as they have in digital logic) and low-voltage op amps have been introduced reflecting this. Supplies of 5 V and increasingly 3.3 V (sometimes as low as 1.8 V) are common. To maximize the signal range modern op amps commonly have rail-to-rail output (the output signal can range from the lowest supply voltage to the highest) and sometimes rail-to-rail inputs.






</doc>
<doc id="22807" url="https://en.wikipedia.org/wiki?curid=22807" title="Oh Hell">
Oh Hell

Oh Hell is a trick-taking card game in which the object is to take exactly the number of tricks bid. Unlike contract bridge and spades, taking more tricks than bid is a loss. It was first described by B. C. Westall around 1930.

The game of Oh Hell explores the idea of taking an exact number of tricks specified by a bid before the hand. It differs from other trick-taking games in that players play a fixed number of hands. The game uses trumps, often decided by a cut of the deck after the hand's cards have been distributed.

Like many popular social card games, Oh Hell has many local variants, in both rules and names.

Prominent players of Oh Hell include former President Bill Clinton, who learned it from Steven Spielberg.

Dan Pearson of Leicestershire is also a key player.
There are many variations to this game; a common set of regulations is given here.

Oh Hell can be played with almost any number of players (3+) although 4-7 is considered optimal. The game is played using a standard 52-card deck, with ace (A) being the highest rank, two (2) the lowest. With six or more players, the game can be played with two decks combined or with a 63-card deck from six-player 500.

A game consists of a fixed number of hands, and each hand consists of dealing a certain number of cards to each player, depending on the variation and the number of players. During a hand, each player bids for a number of tricks, then attempts to take exactly that many tricks during the hand.

The dealer (initially determined by cutting cards) deals out the cards one by one, starting with the player to his left, in a clockwise direction, until the required number of cards has been dealt. After the dealing is complete, the next card is turned face up, and the suit of this card determines the trump suit for the deal, which is why only up to 12 cards are dealt in a four-player match. (If there are no unused cards, the largest hand is played without a trump suit. Alternatively, the maximal round trump suit can be determined in a variety of ways: for instance, by revealing the dealer's last card as in whist, by cutting the pack before dealing, or the dealer can decide the trump before seeing his own cards.)

Each player now bids for the number of tricks they believe they can win. The player to the left of the dealer bids first. Bidding is unrestricted except for the "screw the dealer" rule: the number of tricks bid cannot equal the number available. That is, every deal must in total be either overbid or underbid. For example, if five cards are dealt, and the first three bids are two, zero and one, then the dealer may not bid two. However, if five cards are dealt, and the first three bids are three, one and two, then the dealer is free to make any bid. (The "screw the dealer" rule is not used in the version played in West Virginia, South Carolina, rural Maryland, and Pennsylvania, with the dealer being free to make any bid.) In an alternative style of bidding, all players simultaneously hold out fingers for the number of tricks they want to bid (similar in style to a rock paper scissors shoot). The players' bids are recorded on the score sheet.

When every player has made a bid, the player to the left of the dealer makes the opening lead. Play then proceeds as usual in a trick-taking game, with each player in turn playing one card. Players must follow suit, unless they have no cards of the led suit, in which case they may play any card. The highest card of the led suit wins the trick unless ruffed, when the highest trump card wins.

In multi-deck games, the first of identical cards to be played (say two queens of clubs) wins the trick. In a more complicated variant, identical cards cancel each other, leading to the possibility (if the number of players is even) of an entire trick being canceled out.

The player who wins the trick leads to the next trick.


In this variant, all bids must add up exactly to the number of cards dealt for that round. Players must then "make it work" to move on to the next round. If anyone takes more or less than their bid, the deal moves to the left and the round is re-dealt. With four players, a second deck may be used to specify the round to be played—the value of the upcard determines the number of cards dealt and the suit determines the trump suit for the round.

This variant is played for money. Prior to dealing the first hand, players agree on the amount of money the “losers” will have to pay to the winner. The last place finisher pays the most and the second-place finisher pays the least. The sliding scale in the Prospect version keeps all the players invested in the outcome of every hand, since their finishing rank corresponds to how much money they will owe the winner.

The WPOHL (World Prospect Oh Hell League) Championship is usually held in December in Rehoboth Beach, Delaware, using “Prospect” rules. The deal begins with 10 cards, plays down to 1, then back up to 10 for a total of 19 hands per round. Depending on the size of the field, the five or ten lowest scoring players in the room are eliminated each round until there is a five-person "final table."

In the early 1990s, the International Oh-Hell League's annual Championship Tournament of All Creation was held each March in the Fire Hall of Riverton, New Jersey. This yearly tournament lasted for over 20 years. Players vied for nominal, but not actual, possession of the league trophy, a two-inch bronze reproduction of the Manneken Pis, which resided permanently in the home of tournament founder, John B. "Jack" Mathews. Alcohol was off-limits, but profanity was encouraged. A full table consisted of four players. Play began with a one-card hand, went up to 13, then back down to one for a total of 25 hands. In each hand, except the 13th when the entire deck was dealt, the first undealt card was turned over to establish the trump suit. The tournament entry fee was $5 and included complimentary hot dogs, doughnuts, and Tak-a-Boost (the official drink of the International Oh-Hell League). The tournament was a grueling all-day affair. All players had to commit to playing at least three games. Cumulative tabulation of the end scores from the first three-rounds allowed the top eight highest scoring players to move on to the semi-finals. The semi-finals portion of the tournament was where the commitment to play ended. If a semi-final eligible player could not or did not wish to continue, the player with the next highest cumulative score was offered the seat. The drawing of cards determined random seeding for the semi-final tables. The first and second place players at each semi-final table advanced to the finals, where the top-scoring player in this last game was the tournament winner. Consolation games and clean up by any remaining players were encouraged while waiting for the outcome of the tournament. For a period, the International Oh-Hell League was a registered corporation.

The Annual Cartier 'Oh Hell!' Tournament began in 1995. The tournament formula was created by Tessa Kennedy and Tomasz Starzewski. Cartier Ltd. sponsors the tournament with all money raised going to charity.

There are several alternative methods of scoring:

Oh Hell is known by many names, including:



Boerenbridge, Boerenlullen, Chinees poepen, Chinees dekken, Chinees bridgen, Koreaanse poker, 10 op en neer, jodelen, pronostieken, Slagenvragen, Hellen, Bollen, op-en-affen, Afrikaans beffen.




</doc>
<doc id="22808" url="https://en.wikipedia.org/wiki?curid=22808" title="On War">
On War

Vom Kriege () is a book on war and military strategy by Prussian general Carl von Clausewitz (1780–1831), written mostly after the Napoleonic wars, between 1816 and 1830, and published posthumously by his wife Marie von Brühl in 1832. It has been translated into English several times as On War. "On War" is an unfinished work. Clausewitz had set about revising his accumulated manuscripts in 1827, but did not live to finish the task. His wife edited his collected works and published them between 1832 and 1835.

His 10-volume collected works contain most of his larger historical and theoretical writings, though not his shorter articles and papers or his extensive correspondence with important political, military, intellectual and cultural leaders in the Prussian state. "On War" is formed by the first three volumes and represents his theoretical explorations. It is one of the most important treatises on political-military analysis and strategy ever written, and remains both controversial and influential on strategic thinking.

Clausewitz was among those intrigued by the manner in which the leaders of the French Revolution, especially Napoleon, changed the conduct of war through their ability to motivate the populace and gain access to the full resources of the state, thus unleashing war on a greater scale than had previously been seen in Europe. Clausewitz was well educated and had strong interests in art, history, science, and education. He was a professional soldier who spent a considerable part of his life fighting against Napoleon. The insights he gained from his political and military experiences, combined with a solid grasp of European history, provided the basis for his work.

A wealth of historical examples is used to illustrate its various ideas. Napoleon and Frederick the Great figure prominently for having made very efficient use of the terrain, movement and the forces at their disposal.

Clausewitz argued that war theory cannot be a strict operational advice for generals. Instead, he wanted to highlight general principles that would result from the study of history and logical thinking. He contended that military campaigns could be planned only to a very small degree because incalculable influences or events, so-called "friction", would quickly make any too-detailed planning in advance obsolete. Military leaders must be capable to make decisions under time pressure with incomplete information since in his opinion "three quarters of the things on which action is built in war" are concealed and distorted by the "fog of war".

In his 1812 "Bekenntnisschrift" ("Notes of Confession"), he presents a more existential interpretation of war by envisioning war as the highest form of self-assertion by a people. That corresponded in every respect with the spirit of the time when the French Revolution and the conflicts that arose from it had caused the evolution of conscript armies and guerrillas. The people's armies supported the idea that war is an existential struggle.

During the following years, however, Clausewitz gradually abandoned this exalted view and concluded that the war served as a mere instrument: "Thus, war is an act of violence in order to force our will upon the enemy."

Clausewitz analyzed the conflicts of his time along the line of the categories "Purpose", "Goal" and "Means". He reasoned that the "Purpose" of war is one's will to be enforced, which is determined by politics. The "Goal" of the conflict is therefore to defeat the opponent in order to exact the "Purpose". The "Goal" is pursued with the help of a strategy, that might be brought about by various "Means" such as by the defeat or the elimination of opposing armed forces or by non-military "Means" (such as propaganda, economic sanctions and political isolation). Thus, any resource of the human body and mind and all the moral and physical powers of a state might serve as "Means" to achieve the set goal.

One of Clausewitz's best-known quotes summarizes that idea: "War is a mere continuation of politics by other means."

That quote in itself allows for the interpretation that the military will take over from politics as soon as war has begun, as, for example, the German General Staff did during World War I. However, Clausewitz had postulated the "primacy of politics" and in this context elaborated: "[...], we claim that war is nothing more than a continuation of the political process by applying other means. By applying other means we simultaneously assert that the political process does not end with the conclusion of the war or is being transformed into something entirely different, but that it continues to exist and proceed in its essence, regardless of the means, it might make use of."

According to Azar Gat, the "general message" of the book was that "the conduct of war could not be reduced to universal principles [and is] dominated by political decisions and moral forces." These basic conclusions are essential to Clausewitz's theory:
Some of the key ideas (not necessarily original to Clausewitz or even to his mentor, Gerhard von Scharnhorst) discussed in "On War" include (in no particular order of importance):

Clausewitz used a dialectical method to construct his argument, which led to frequent modern misinterpretation because he explores various often-opposed ideas before he came to conclusions.

Modern perceptions of war are based on the concepts that Clausewitz put forth in "On War", but they have been diversely interpreted by various leaders (such as Moltke, Vladimir Lenin, Dwight Eisenhower, and Mao Zedong), thinkers, armies, and peoples. Modern military doctrine, organization, and norms are all still based on Napoleonic premises, but whether the premises are necessarily also "Clausewitzian" is debatable.

The "dualism" of Clausewitz's view of war (that wars can vary a great deal between the two "poles" that he proposed, based on the political objectives of the opposing sides and the context) seems to be simple enough, but few commentators have been willing to accept that crucial variability. They insist that Clausewitz "really" argued for one end of the scale or the other. "On War" has been seen by some prominent critics as an argument for "total war".

It has been blamed for the level of destruction involved in the First and the Second World Wars, but it seems rather that Clausewitz, who did not actually use the term "total war", had merely foreseen the inevitable development that started with the huge, patriotically motivated armies of the Napoleonic wars. They resulted (though the evolution of war has not yet ended) in the atomic bombing of Hiroshima and Nagasaki, with all the forces and capabilities of the state devoted to destroying forces and capabilities of the enemy state (thus "total war"). Conversely, Clausewitz has also been seen as "The preeminent military and political strategist of limited war in modern times". (Robert Osgood, 1979)

Clausewitz and his proponents have been severely criticized by other military theorists, like Antoine-Henri Jomini in the 19th century, B. H. Liddell Hart in the mid-20th century, and Martin van Creveld and John Keegan more recently. "On War" is a work rooted solely in the world of the nation state, states historian Martin van Creveld, who alleges that Clausewitz takes the state "almost for granted", as he rarely looks at anything before the Peace of Westphalia, and mediaeval warfare is effectively ignored in Clausewitz's theory. He alleges that Clausewitz does not address any form of intra/supra-state conflict, such as rebellion and revolution, because he could not theoretically account for warfare before the existence of the state.

Previous kinds of conflict were demoted to criminal activities without legitimacy and not worthy of the label "war". Van Creveld argues that "Clausewitzian war" requires the state to act in conjunction with the people and the army, the state becoming a massive engine built to exert military force against an identical opponent. He supports that statement by pointing to the conventional armies in existence throughout the 20th century. However, revolutionaries like Karl Marx and Friedrich Engels derived some inspiration from Clausewitzian ideas.







</doc>
<doc id="22810" url="https://en.wikipedia.org/wiki?curid=22810" title="Orange Alternative">
Orange Alternative

The Orange Alternative (Polish: "Pomarańczowa Alternatywa") is a Polish anti-communist underground movement, started in Wrocław, a city in south-west Poland and led by Waldemar Fydrych (sometimes misspelled as Frydrych), commonly known as "Major (Commander of Festung Breslau)" in the 1980s. Its main purpose was to offer a wider group of citizens an alternative way of opposition against the authoritarian regime by means of a peaceful protest that used absurd and nonsensical elements.

By doing this, members of the Orange Alternative could not be arrested by the police for opposition to the regime without the authorities becoming a laughing stock. The Orange Alternative has been viewed as part of the broader Solidarity movement. Academics Dennis Bos and Marjolein 't Hart have asserted it was the most effective of all Solidarity's factions in bringing about the movement's success.

Initially it painted ridiculous graffiti of dwarves on paint spots covering up anti-government slogans on city walls. Afterwards, beginning with 1985 through 1990, it organized a series of more than sixty happenings in several Polish cities, including Wrocław, Warsaw, Łódź, Lublin, and Tomaszów Mazowiecki.

It was the most picturesque element of Polish opposition to Stalinist authoritarianism. It suspended activity in 1989, but reactivated in 2001 and has been active on a small scale ever since.

A statue of a dwarf, dedicated to the memory of the movement, stands today on Świdnicka Street in Wrocław, in the place where events took place.

The Orange Alternative movement has inspired several other similar movements in authoritarian countries including Czechoslovakia and Hungary and it has also inspired and influenced the Pora and the so-called Orange Revolution movement in Ukraine, which was in turn supported by Poland.

Some utterances ascribed to Waldemar Fydrych:

The beginnings of the Orange Alternative are in a student movement called the Movement for New Culture created in 1980 at the University of Wrocław. It is in that year that Waldemar "Major" Fydrych, one of the movement's founders, proclaims the Socialist Surrealism Manifesto, which becomes the ideological backbone behind a gazette known as "The Orange Alternative". Seven out of the total fifteen issues of this gazette appear during student strikes organized in November and December 1980 as part of the Solidarity upheaval. The first number is edited jointly by Major Waldemar Fydrych and Wiesław Cupała (a.k.a. "Rittmeister") simply with an idea to have fun. The editors treat the strike and the surrounding reality as forms of Art. For the ensuing numbers, the editorial committee is joined by Piotr Adamcio, known as "Lieutenant Pablo", Andrzej Dziewit and Zenon Zegarski, nicknamed "Lieutenant Zizi Top". Although its avantgarde character, according to the student strike organizers, was a threat to the "higher aims of the strike", and notwithstanding attempts by the strike committee to censor it, the gazette became rapidly very popular among the students.

The first known actions of the Orange Alternative consisted of painting dwarf graffiti on spots created by the police's covering up anti-regime slogans on walls of the Polish cities. The first graffiti was painted by Major Waldemar Fydrych and Wiesław Cupała on the night from the 30 to 31 August 1982 on one of the residences in the Wrocław district of Biskupin and Sępolno.

Altogether more than one thousand of such graffiti were painted in the major Polish cities such as Wrocław, Kraków, Warsaw, Łódź, and Gdańsk.
Dwarves appearing in numbers all over Poland aroused the interest of both Polish pedestrians and the militia, whose intervention led to short term arrests of the graffiti artists.

During one of these incidents, Major, a detainee at a police station in Łódź, proclaimed, in reference to the Marxist and Hegelian dialectics, yet another artistic manifesto and referred to his graffiti art as "dialectic painting" stating: "The Thesis is the Anti-Regime Slogan. The Anti-thesis is the Spot and the Synthesis is the Dwarf. Quantity evolves into Quality. The more Dwarves there are, the better it is."

At the beginning of the 21st century Dwarf figurines made of bronze began to appear in Wrocław. Over time, they have become a major tourist attraction in Wrocław.

What brought the Orange Alternative the biggest fame were its street happenings which it organized throughout the second half of the 1980s. These actions gained it enormous popularity among the Polish youth, who joined the movement, seeing it an alternative to the opposition style presented by the Solidarity, which they viewed as more stiff and boring.

The first modest happening called the "Burning of Tubes" was organized as early as 1985 in Wrocław by Major Waldemar Fydrych accompanied by a small group of artists to which belonged: Krzysztof Skarbek, Piotr Petyszkowski, Andrzej Głuszek, and Sławomir Monkiewicz.

The break-through moment came in the fall of 1987, during the Open Theatre Festival in Wrocław, when the Village Voice reported the Orange Alternative's action known as "Distribution of Toilet Paper" – a happening that satirized the annoying lack of that consumer product at the time. After the publication of this article, the Orange Alternative became of interest to a number of Polish and foreign media.

The biggest happenings however took place in the years 1987 through 1989, with the "orange" wave spilling over Poland into cities such as Warsaw, Łódź, Lublin and Tomaszów Mazowiecki, following Major Fydrych's arrest on 8 March 1988.

The actions of the Orange Alternative – although its leaders and participants often expressed anarchistic viewpoints – were not inherently ideological. No serious demands were ever expressed. Rather, the slogans were surrealist in character (such as "Vivat Sorbovit", Sorbovit being a popular soft drink at that time) or "There is no freedom without dwarves". They often paraphrased slogans used by the Solidarity Union or the communists. Their role was to laugh at absurdities and pompousness of both sides of the system and provoke independent thinking.

The open street formula allowed all individuals to take part in the happenings. This openness drew thousands of pedestrians to participate in the group's actions. In such a way, the majority of the happenings could assemble thousands of participants, of whom many were accidental passers-by. The culmination point in the movement's history was the action organized on 1 June 1988, known as the "Revolution of Dwarves", during which more than 10 thousand people marched through the center of Wrocław wearing orange dwarf hats.

The happenings usually terminated with the arrest of hundreds of participants, who did not manage to escape in time from the hands of the militia. At one point, the participants were even able to provoke the Communist militia to arrest 77 Santa Clauses or, on another occasion, anyone wearing anything orange.

For each of its actions, the Orange Alternative printed leaflets and posters, featuring slogans like "Every militiaman is a piece of Art" or "Citizen, help the militia, beat yourself up".





</doc>
<doc id="22811" url="https://en.wikipedia.org/wiki?curid=22811" title="Otto IV, Holy Roman Emperor">
Otto IV, Holy Roman Emperor

Otto IV (1175 – 19 May 1218) was one of two rival kings of Germany from 1198 on, sole king from 1208 on, and Holy Roman Emperor from 1209 until he was forced to abdicate in 1215. The only German king of the Welf dynasty, he incurred the wrath of Pope Innocent III and was excommunicated in 1210.

Otto was the third son of Henry the Lion, Duke of Bavaria and Duke of Saxony, by his wife Matilda of England, the eldest daughter of King Henry II of England and Eleanor of Aquitaine. His exact birthplace is not given by any original source. He grew up in England in the care of his grandfather King Henry II. Otto was fluent in French as well as German. He became the foster son of his maternal uncle, King Richard I of England. In 1190, after he left England to join the Third Crusade, Richard appointed Otto as Earl of York. The authenticity (or authority) of this grant was doubted by the vassals of Yorkshire, who prevented Otto taking possession of his earldom. Still, he probably visited Yorkshire in 1191, and he continued to claim the revenues of the earldom after becoming king of Germany, although he never secured them. Neither did he succeed in getting the 25,000 silver marks willed to him by his uncle in 1199.

In 1195, Richard began negotiations to marry Otto to Margaret of Scotland, daughter and heir presumptive of King William the Lion of Scotland. Lothian, as Margaret's dowry, would be handed over to Richard for safekeeping and the counties of Northumberland and Cumberland (Carlisle) would be granted to Otto and turned over to the king of Scotland. The negotiations dragged on until August 1198, when the birth of a son and heir to William rendered them unnecessary. Having failed in his efforts to secure Otto an English earldom or else a Scottish kingdom, in September 1196 Richard, as duke of Aquitaine, enfeoffed Otto with the county of Poitou. There is some disagreement over whether Otto received Poitou in exchange for or in addition to the earldom of York.

Otto was in Poitou from September 1196 until mid-1197, when he joined Richard in Normandy to confer over the appointment of bishops to the vacant sees of Poitiers, Limoges and Périgueux. He then participated in the war against Philip II of France on the side of Richard. In October he returned to Poitou. The German historian Jens Ahlers, taking into account Otto's life prior to 1198, considers that he might have been the first foreign king of Germany.

After the death of Emperor Henry VI, the majority of the princes of the Empire, situated in the south, elected Henry's brother, Philip, Duke of Swabia, king in March 1198, after receiving money and promises from Philip in exchange for their support. Those princes opposed to the Hohenstaufen dynasty also decided, on the initiative of Richard of England, to elect instead a member of the House of Welf. Otto's elder brother, Henry, was on a crusade at the time, and so the choice fell to Otto. Otto, soon recognized throughout the northwest and the lower Rhine region, was elected king by his partisans in Cologne on 9 June 1198.

Otto took control of Aachen, the place of coronation, and was crowned by Adolf, Archbishop of Cologne, on 12 July 1198. This was of great symbolic importance, since the Archbishop of Cologne alone could crown the King of the Romans. The coronation was done with fake imperial regalia, because the actual materials were in the hands of the Hohenstaufen.

Otto's election pulled the empire into the conflict between England and France. Philip had allied himself with the French king, Philip II, while Otto was supported at first by Richard I, and after his death in 1199 by his brother John.

The papacy meanwhile, under Innocent III, determined to prevent the continued unification of Sicily and the Holy Roman Empire under one monarch seized the opportunity to extend its influence. Therefore, Innocent III favoured Otto, whose family had always been opposed to the house of Hohenstaufen. Otto himself also seemed willing to grant any demands that Innocent would make. The confusion in the empire allowed Innocent to drive out the imperial feudal lords from Ancona, Spoleto, and Perugia, who had been installed by Emperor Henry VI.

At the same time, Innocent encouraged the cities in Tuscany to form a league, called the League of San Genesio, against imperial interests in Italy. The cities placed themselves under Innocent's protection. In 1201, Innocent announced that he recognized Otto as the only legitimate king. In return, Otto promised to support the pope's interests in Italy. Otto also had the support of Ottokar I of Bohemia, who although at first siding with Philip of Swabia, eventually threw in his lot with Otto. Otto's cause was further strengthened by the support of Valdemar II of Denmark. Philip achieved a great deal of success in the civil war that followed, allowing him in 1204 to be again crowned king, this time by the archbishop of Cologne.

In the following years, Otto's situation worsened because after England's defeat by France he lost England's financial support. Many of his allies changed sides to Philip, including his brother Henry. Otto was defeated and wounded in battle by Philip on 27 July 1206, near Wassenberg, and as a consequence he also lost the support of the pope, who began to favour the apparent winner in the conflict. Otto was forced to retire to his possessions near Brunswick, leaving Philip virtually uncontested as German king.

Innocent III forced the two warring parties into negotiations at Cologne, and in exchange for renouncing his claim to the throne, Philip promised Otto the hand of his daughter Beatrix in marriage, together with the Duchy of Swabia and an enormous dowry. Otto refused, and as the civil war was again about to recommence, Philip was murdered on 21 June 1208.

After Philip's death, Otto made amends with the Staufen party and became engaged to Philip's daughter Beatrix. In an election in Frankfurt on 11 November 1208, he gained the support of all the electoral princes, as he promised he would not make hereditary claims to the imperial crown on behalf of any children he might father. Now fully reconciled with Innocent, Otto made preparations to be crowned Holy Roman Emperor. To secure Innocent's support, he promised to restore to the Papal States all territory that it had possessed under Louis the Pious, including the March of Ancona, the Duchy of Spoleto, the former Exarchate of Ravenna, and the Pentapolis.

Travelling down via Verona, Modena, and Bologna, he eventually arrived at Milan where he received the Iron Crown of Lombardy and the title of King of Italy in 1208. He was met at Viterbo by Pope Innocent and was taken to St. Peter's Basilica, where he was crowned emperor by Pope Innocent on 21 October 1209, before rioting broke out in Rome, forcing Otto to abandon the city.

Not content with his successes so far, Innocent also obtained from Otto further written concessions to the Papal See, including to allow all elections of German bishops to be conducted according to Church ordinances, and not to prevent any appeals to Rome. He also promised to hand over to the Church all income from any vacant sees which had been flowing into the imperial treasury.
After abandoning Rome, Otto marched north, reaching Pisa by 20 November. Here, probably advised by Peter of Celano and Dipold, Count of Acerra, he was convinced to abandon his earlier promises. Otto immediately worked to restore imperial power in Italy. After his consecration by the pope, he promised to restore the lands bequeathed to the church by the countess Matilda of Tuscany nearly a century before, and not to move against Frederick, King of Sicily. He quickly broke all his promises.

He threw out the papal troops from Ancona and Spoleto, reclaiming the territory as imperial fiefs. He then demanded that Frederick of Sicily do homage for the duchies of Calabria and Apulia, and when Frederick refused to appear, Otto declared those fiefs forfeited. Otto then marched on Rome, and commanded Innocent to annul the Concordat of Worms, and to recognise the imperial crown's right to make nominations to all vacant benefices.

Such actions infuriated Innocent, and Otto was promptly excommunicated by the pope for this on 18 November 1210. Subsequently, he tried to conquer Sicily, which was held by the Staufen king Frederick, under the guardianship of Innocent III. Parallel to this, the German nobility by this time were growing ever more frustrated with Otto. They felt that instead of wasting his time in Italy, and playing power politics with the pope, it was his first duty to defend the northern provinces of the empire against Valdemar II of Denmark, who had taken advantage of Otto's distractions by invading the northern provinces of the empire and possessing the whole Baltic coast from Holstein to Livonia. So while Otto was in southern Italy, several princes of the empire, including the archbishops of Mainz and Magdeburg, at the instigation of King Philip II of France and with the consent of the pope, elected Frederick King of the Romans at the Diet of Nuremberg in 1211.

Otto's ambassadors from Milan appeared before the Fourth Lateran Council, pleading his case for his excommunication to be lifted. Although he claimed he had repented for his offences, and declared his willingness to be obedient to the Pope in all things, Innocent III had already recognised Frederick as emperor-elect.

Otto returned to Germany to deal with the situation, hopeful to salvage something from the looming disaster. He found most of the German princes and bishops had turned against him, and that Frederick, who had made his way up the Italian peninsula, had avoided Otto's men who were guarding the passes through the Alps and had arrived at Constance. Otto soon discovered that after Beatrix died in the summer of 1212, and Frederick arrived in Germany with his army in September 1212, most of the former Staufen supporters deserted Otto for Frederick, forcing Otto to withdraw to Cologne. On 5 December 1212, Frederick was elected king for a second time by a majority of the princes.

The support that Philip II of France was giving to Frederick forced King John of England to throw his weight behind his nephew Otto. The destruction of the French fleet in 1213 by the English saw John begin preparations for an invasion of France, and Otto saw a way of both destroying Frederick's French support as well as bolstering his own prestige. He agreed to join John in the invasion, and in February 1214, as John advanced from the Loire, Otto was supposed to make a simultaneous attack from Flanders, together with the Count of Flanders. Unfortunately, the three armies could not coordinate their efforts effectively. It was not until John, who had been disappointed in his hope for an easy victory after being driven from Roche-au-Moine and had retreated to his transports, that the Imperial Army, with Otto at its head, assembled in the Low Countries.

On 27 July 1214, the opposing armies suddenly discovered they were in close proximity to each other, on the banks of the little river Marque (a tributary of the river Deûle), near the Bridge of Bouvines. Philip's army numbered some 15,000, while the allied forces possessed around 25,000 troops, and the armies clashed at the Battle of Bouvines. It was a tight battle, but it was lost when Otto was carried off the field by his wounded and terrified horse, causing his forces to abandon the field. It is said that Philip II had sent to Frederick the imperial eagle which Otto had left lying on the battlefield.

This defeat allowed Frederick to take Aachen and Cologne, as Otto was forced again to withdraw to his private possessions around Brunswick, and he was deposed in 1215. Absolved from his excommunication, he died of disease, at Harzburg castle on 19 May 1218, requesting that he be mortally expiated in atonement of his sins. Historian Kantorowicz described the death as "gruesome": "deposed, dethroned, he was flung full length on the ground by the Abbot, confessing his sins, while the reluctant priests beat him bloodily to death with rods. Such was the end of the first and last Welf Emperor." 

He is entombed in Brunswick Cathedral.

Otto was related to every other King of Germany. He married twice:

Neither marriage produced any children.




</doc>
<doc id="22812" url="https://en.wikipedia.org/wiki?curid=22812" title="Octavian (disambiguation)">
Octavian (disambiguation)

Octavian was the name of Augustus (63 BC – 14 AD) before he became Emperor of Rome.

Octavian may also refer to:



</doc>
<doc id="22816" url="https://en.wikipedia.org/wiki?curid=22816" title="Outcome-based education">
Outcome-based education

Outcome-based education or outcomes-based education (OBE), also known as standards-based education, is an educational theory that bases each part of an educational system around goals (outcomes). By the end of the educational experience, each student should have achieved the goal. There is no single specified style of teaching or assessment in OBE; instead, classes, opportunities, and assessments should all help students achieve the specified outcomes. The role of the faculty adapts into instructor, trainer, facilitator, and/or mentor based on the outcomes targeted.

Outcome-based methods have been adopted in education systems around the world, at multiple levels. 
Australia and South Africa adopted OBE policies in the early 1990s but have since been phased out. The United States has had an OBE program in place since 1994 that has been adapted over the years. In 2005, Hong Kong adopted an outcome-based approach for its universities. Malaysia implemented OBE in all of their public schools systems in 2008. The European Union has proposed an education shift to focus on outcomes, across the EU. In an international effort to accept OBE, The Washington Accord was created in 1989; it is an agreement to accept undergraduate engineering degrees that were obtained using OBE methods. As of 2017, the full signatories are Australia, Canada, Taiwan, Hong Kong, India, Ireland, Japan, Korea, Malaysia, New Zealand, Russia, Singapore, South Africa, Sri Lanka, Turkey, the United Kingdom, Pakistan, China and the United States.

OBE can primarily be distinguished from traditional education method by the way it incorporates three elements: theory of education, a systematic structure for education, and a specific approach to instructional practice. It organizes the entire educational system towards what are considered essential for the learners to successfully do at the end of their learning experiences. In this model, the term "outcome" is the core concept and sometimes used interchangeably with the terms "competency, "standards, "benchmarks", and "attainment targets". OBE also uses the same methodology formally and informally adopted in actual workplace to achieve outcomes. It focuses on the following skills when developing curricula and outcomes:


In a regional/local/foundational/electrical education system, students are given grades and rankings compared to each other. Content and performance expectations are based primarily on what was taught in the past to students of a given age of 12-18. The goal of this education was to present the knowledge and skills of an older generation to the new generation of students, and to provide students with an environment in which to learn. The process paid little attention (beyond the classroom teacher) to whether or not students learn any of the material.

The focus on outcomes creates a clear expectation of what needs to be accomplished by the end of the course. Students will understand what is expected of them and teachers will know what they need to teach during the course. Clarity is important over years of schooling and when team teaching is involved. Each team member, or year in school, will have a clear understanding of what needs to be accomplished in each class, or at each level, allowing students to progress. Those designing and planning the curriculum are expected to work backwards once an outcome has been decided upon; they must determine what knowledge and skills will be required to reach the outcome.

With a clear sense of what needs to be accomplished, instructors will be able to structure their lessons around the student’s needs. OBE does not specify a specific method of instruction, leaving instructors free to teach their students using any method. Instructors will also be able to recognize diversity among students by using various teaching and assessment techniques during their class. OBE is meant to be a student-centered learning model. Teachers are meant to guide and help the students understand the material in any way necessary, study guides, and group work are some of the methods instructors can use to facilitate students learning.

OBE can be compared across different institutions. On an individual level, institutions can look at what outcomes a student has achieved to decide what level the student would be at within a new institution. On an institutional level, institutions can compare themselves, by checking to see what outcomes they have in common, and find places where they may need improvement, based on the achievement of outcomes at other institutions. The ability to compare easily across institutions allows students to move between institutions with relative ease. The institutions can compare outcomes to determine what credits to award the student. The clearly articulated outcomes should allow institutions to assess the student’s achievements rapidly, leading to increased movement of students. These outcomes also work for school to work transitions. A potential employer can look at records of the potential employee to determine what outcomes they have achieved. They can then determine if the potential employee has the skills necessary for the job.

Student involvement in the classroom is a key part of OBE. Students are expected to do their own learning, so that they gain a full understanding of the material. Increased student involvement allows students to feel responsible for their own learning, and they should learn more through this individual learning. Other aspects of involvement are parental and community, through developing curriculum, or making changes to it. OBE outcomes are meant to be decided upon within a school system, or at a local level. Parents and community members are asked to give input in order to uphold the standards of education within a community and to ensure that students will be prepared for life after school.

The definitions of the outcomes decided upon are subject to interpretation by those implementing them. Across different programs or even different instructors outcomes could be interpreted differently, leading to a difference in education, even though the same outcomes were said to be achieved. By outlining specific outcomes, a holistic approach to learning is lost. Learning can find itself reduced to something that is specific, measurable, and observable. As a result, outcomes are not yet widely recognized as a valid way of conceptualizing what learning is about.

When determining if an outcome has been achieved, assessments may become too mechanical, looking only to see if the student has acquired the knowledge. The ability to use and apply the knowledge in different ways may not be the focus of the assessment. The focus on determining if the outcome has been achieved leads to a loss of understanding and learning for students, who may never be shown how to use the knowledge they have gained. Instructors are faced with a challenge: they must learn to manage an environment that can become fundamentally different from what they are accustomed to. In regards to giving assessments, they must be willing to put in the time required to create a valid, reliable assessment that ideally would allow students to demonstrate their understanding of the information, while remaining objective.

Education outcomes can lead to a constrained nature of teaching and assessment. Assessing liberal outcomes such as creativity, respect for self and others, responsibility, and self-sufficiency, can become problematic. There is not a measurable, observable, or specific way to determine if a student has achieved these outcomes. Due to the nature of specific outcomes, OBE may actually work against its ideals of serving and creating individuals that have achieved many outcomes.

Parental involvement, as discussed in the benefits section can also be a drawback, if parents and community members are not willing to express their opinions on the quality of the education system, the system may not see a need for improvement, and not change to meet student’s needs. Parents may also become too involved, requesting too many changes, so that important improvements get lost with other changes that are being suggested. Instructors will also find that their work is increased; they must work to first understand the outcome, then build a curriculum around each outcome they are required to meet. Instructors have found that implementing multiple outcomes is difficult to do equally, especially in primary school. Instructors will also find their work load increased if they chose to use an assessment method that evaluates students holistically.

In the early 1990s, all states and territories in Australia developed intended curriculum documents largely based on OBE for their primary and secondary schools. Criticism arose shortly after implementation. Critics argued that no evidence existed that OBE could be implemented successfully on a large scale, in either the United States or Australia. An evaluation of Australian schools found that implementing OBE was difficult. Teachers felt overwhelmed by the amount of expected achievement outcomes. Educators believed that the curriculum outcomes did not attend to the needs of the students or teachers. Critics felt that too many expected outcomes left students with shallow understanding of the material. Many of Australia’s current education policies have moved away from OBE and towards a focus on fully understanding the essential content, rather than learning more content with less understanding.

Officially, an agenda to implement Outcomes Based Education took place between 1992 and 2008 in Western Australia. Dissatisfaction with OBE escalated from 2004 when the government proposed the implementation of an alternative assessment system using OBE 'levels' for years 11 and 12. With government school teachers not permitted to publicly express dissatisfaction with the new system, a community lobby group called PLATO as formed in June 2004 by high school science teacher Marko Vojkavi. Teachers anonymously expressed their views through the website and online forums, with the website quickly became one of the most widely read educational websites in Australia with more 180,000 hits per month and contained an archive of more than 10,000 articles on the subject of OBE implementation. In 2008 it was officially abandoned by the state government with Minister for Education Mark McGowan remarking that the 1990s fad "to dispense with syllabus" was over.

In December 2012, the European Commission presented a new strategy to decrease youth unemployment rate, which at the time was close to 23% across the European Union . The European Qualifications Framework calls for a shift towards learning outcomes in primary and secondary schools throughout the EU. Students are expected to learn skills that they will need when they complete their education. It also calls for lessons to have a stronger link to employment through work-based learning (WBL). Work-based learning for students should also lead to recognition of vocational training for these students. The program also sets goals for learning foreign languages, and for teachers continued education. It also highlights the importance of using technology, especially the internet, in learning to make it relevant to students.

Hong Kong’s University Grants Committee adopted an outcomes-based approach to teaching and learning in 2005. No specific approach was created leaving universities to design the approach themselves. Universities were also left with a goal of ensuring an education for their students that will contribute to social and economic development, as defined by the community in which the university resides. With little to no direction or feedback from the outside universities will have to determine if their approach is achieving its goals on their own.

OBE has been practiced in Malaysia since the 1950s; however, as of 2008, OBE is being implemented at all levels of education, especially tertiary education. This change is a result of the belief that the education system used prior to OBE inadequately prepared graduates for life outside of school. The Ministry of Higher Education has pushed for this change because of the number of unemployed graduates. Findings in 2006 state that nearly 70% of graduates from public universities were considered unemployed. A further study of those graduates found that they felt they lacked, job experience, communication skills, and qualifications relevant to the current job market. The Malaysian Qualifications Agency (MQA) was created to oversee quality of education and to ensure outcomes were being reached. The MQA created a framework that includes eight levels of qualification within higher education, covering three sectors; skills, vocational and technical, and academic. Along with meeting the standards set by the MQA, universities set and monitor their own outcome expectations for students 

OBE was introduced to South Africa in the late 1990s by the post-apartheid government as part of its Curriculum 2005 program. , Initial support for the program derived from anti-apartheid education policies. The policy also gained support from the labor movements that borrowed ideas about competency-based education, and Vocational education from New Zealand and Australia, as well as the labor movement that critiqued the apartheid education system. With no strong alternative proposals, the idea of outcome-based education, and a national qualification framework, became the policy of the African National Congress government. This policy was believed to be a democratization of education, people would have a say in what they wanted the outcomes of education to be. It was also believed to be a way to increase education standards and increase the availability of education. The National Qualifications Framework (NQF) went into effect in 1997. In 2001 people realized that the intended effects were not being seen. By 2006 no proposals to change the system had been accepted by the government, causing a hiatus of the program. The program came to be viewed as a failure and a new curriculum improvement process was announced in 2010, slated to be implemented between 2012 and 2014.

In 1983, a report from the National Commission on Excellence in Education declared that American education standards were eroding, that young people in the United States were not learning enough. In 1989, President Bush and the nation’s governors set national goals to be achieved by the year 2000. GOALS 2000: Educate America Act was signed in March 1994. The goal of this new reform was to show that results were being achieved in schools. In 2001, the No Child Left Behind Act took the place of Goals 2000. It mandated certain measurements as a condition of receiving federal education funds. States are free to set their own standards, but the federal law mandates public reporting of math and reading test scores for disadvantaged demographic subgroups, including racial minorities, low-income students, and special education students. Various consequences for schools that do not make "adequate yearly progress" are included in the law. In 2010, President Obama proposed improvements for the program. In 2012, the U.S. Department of Education invited states to request flexibility waivers in exchange for rigorous plans designed to improve students' education in the state.

India has become the permanent signatory member of the Washington Accord on 13 June 2014. India has started implementing OBE in higher technical education like diploma and undergraduate programmes. The National Board of Accreditation, a body for promoting international quality standards for technical education in India has started accrediting only the programmes running with OBE from 2013.

The National Board of Accreditation mandates establishing a culture of outcomes-based education in institutions that offer Engineering, Pharmacy, Management programs. Outcomes analysis and using the analytical reports to find gaps and carry out continuous improvement is essential cultural shift from how the above programs are run when OBE culture is not embraced. Outcomes analysis requires huge amount of data to be churned and made available at any time, anywhere. Such an access to scalable, accurate, automated and real-time data analysis is possible only if the institute adopts either excelsheet based measurement system or some kind of home-grown or commercial software system. It is observed that excelsheet based measurement and analysis system doesn't scale when the stakeholders want to analyse longitudinal data.




</doc>
<doc id="22817" url="https://en.wikipedia.org/wiki?curid=22817" title="Olga of Kiev">
Olga of Kiev

Saint Olga (, in the baptism — Elena; born c. 890–925, in Pskov – died 969 AD in Kiev) was a regent of Kievan Rus' for her son Svyatoslav from 945 until 960. Due to the imperfect transliteration between Old East Slavic and the English language, the name Olga is synonymous with Olha. From her baptism, Olga took the name Elenа. She is known for her subjugation of the Drevlians, a tribe that had killed her husband Igor of Kiev. Even though it would be her grandson Vladimir that would convert the entire nation to Christianity, because of her efforts to spread Christianity through Rus', Olga is venerated as a saint in the Eastern Orthodox Church with the epithet "Equal to the Apostles" and her feast day is the 11th of July.

While Olga's birthdate is unknown, it could be as early as 890 AD and as late as 925 AD. According to the "Primary Chronicle" Olga was born and lived in Pskov (). Little is known about her life before her marriage to Prince Igor I of Kiev and the birth of their son, Svyatoslav. Igor was the son and heir of Rurik, founder of the Rurik dynasty. After his father's death Igor was under the guardianship of Oleg, who had consolidated power in the region, conquering neighboring tribes and establishing a capital in Kiev. This loose tribal federation became known as Kievan Rus', a territory covering what are now parts of Russia, Ukraine, and Belarus.

The Drevlians were a neighboring tribe with which the growing Kievan Rus' empire had a complex relationship. The Drevlians had joined Kievan Rus' in military campaigns against the Byzantine Empire and paid tribute to Igor's predecessors. They stopped paying tribute upon Oleg's death and instead gave money to a local warlord. In 945, Igor set out to the Drevlian capital, Iskorosten (today known as Korosten in northern Ukraine), to force the tribe to pay tribute to Kievan Rus'. Confronted by Igor's larger army, the Drevlians backed down and paid him. As Igor and his army rode home, however, he decided the payment was not enough and returned, with only a small escort, seeking more tribute. Upon his arrival in their territory, the Drevlians murdered Igor. According to the Byzantine chronicler Leo the Deacon, Igor's death was caused by a gruesome act of torture in which he was "captured by them, tied to tree trunks, and torn in two." D. Sullivan has suggested that Leo may have invented this sensationalist version of Igor's death, taking inspiration from Diodorus Siculus' account of a similar killing method used by the robber Sinis, who lived near the Isthmus of Corinth and was killed by Theseus.

After Igor's death in 945, Olga ruled Kievan Rus as regent on behalf of their son Svyatoslav. 
Little is known about Olga's tenure as ruler of Kiev, but the "Primary Chronicle" does give an account of her accession to the throne and her bloody revenge on the Drevlians for the murder of her husband as well as some insight into her role as civil leader of the Kievan people.

According to archeologist , Knyaginya Olga, like all the other rulers before Vladimir the Great, was also using the bident as her personal symbol.

After Igor's death at the hands of the Drevlians, Olga assumed the throne because her three-year-old son Svyatoslav was too young to rule. The Drevlians, emboldened by their success in ambushing and killing the king, sent a messenger to Olga proposing that she marry his murderer, Prince Mal. Twenty Drevlian negotiators boated to Kiev to pass along their king's message and to ensure Olga's compliance. They arrived in her court and told the queen why they were in Kiev: "to report that they had slain her husband...and that Olga should come and marry their Prince Mal." Olga responded:Your proposal is pleasing to me, indeed, my husband cannot rise again from the dead. But I desire to honor you tomorrow in the presence of my people. Return now to your boat, and remain there with an aspect of arrogance. I shall send for you on the morrow, and you shall say, "We will not ride on horses nor go on foot, carry us in our boat." And you shall be carried in your boat.When the Drevlians returned the next day, they waited outside Olga's court to receive the honor she had promised. When they repeated the words she had told them to say, the people of Kiev rose up, carrying the Drevlians in their boat. The ambassadors believed this was a great honor, as if they were being carried by palanquin. The people brought them into the court where they were dropped into a trench that had been dug the day before under Olga's orders where the ambassadors were buried alive. It is written that Olga bent down to watch them as they were buried and "inquired whether they found the honor to their taste."

Olga then sent a message to the Drevlians that they should send "their distinguished men to her in Kiev, so that she might go to their Prince with due honor." The Drevlians, unaware of the fate of the first diplomatic party, gathered another party of men to send "the best men who governed the land of Dereva." When they arrived, Olga commanded her people to draw them a bath and invited the men to appear before her after they had bathed. When the Drevlians entered the bathhouse, Olga had it set on fire from the doors, so that all the Drevlians within burned to death.

Olga sent another message to the Drevlians, this time ordering them to "prepare great quantities of mead in the city where you killed my husband, that I may weep over his grave and hold a funeral feast for him." When Olga and a small group of attendants arrived at Igor's tomb, she did indeed weep and hold a funeral feast. The Drevlians sat down to join them and began to drink heavily. When the Drevlians were drunk, she ordered her followers to kill them, "and went about herself egging on her retinue to the massacre of the Drevlians." According to the "Primary Chronicle", five thousand Drevlians were killed on this night, but Olga returned to Kiev to prepare an army to finish off the survivors.

The initial conflict between the armies of the two nations went very well for the forces of Kievan Rus', who won the battle handily and drove the survivors back into their cities. Olga then led her army to Iskorosten (what is today Korosten), the city where her husband had been slain, and laid siege to the city. The siege lasted for a year without success, when Olga thought of a plan to trick the Drevlians. She sent them a message: "Why do you persist in holding out? All your cities have surrendered to me and submitted to tribute, so that the inhabitants now cultivate their fields and their lands in peace. But you had rather tide of hunger, without submitting to tribute." The Drevlians responded that they would submit to tribute, but that they were afraid she was still intent on avenging her husband. Olga answered that the murder of the messengers sent to Kiev, as well as the events of the feast night, had been enough for her. She then asked them for a small request: "Give me three pigeons...and three sparrows from each house." The Drevlians rejoiced at the prospect of the siege ending for so small a price, and did as she asked.

Olga then instructed her army to attach a piece of sulphur bound with small pieces of cloth to each bird. At nightfall, Olga told her soldiers to set the pieces aflame and release the birds. They returned to their nests within the city, which subsequently set the city ablaze. As the "Primary Chronicle" tells it: "There was not a house that was not consumed, and it was impossible to extinguish the flames, because all the houses caught fire at once." As the people fled the burning city, Olga ordered her soldiers to catch them, killing some of them and giving the others as slaves to her followers. She left the remnant to pay tribute.

Olga remained regent ruler of Kievan Rus with the support of the army and her people. She changed the system of tribute gathering (poliudie) in the first legal reform recorded in Eastern Europe. She continued to evade proposals of marriage, defended the city during the Siege of Kiev in 968, and saved the power of the throne for her son.

After her dramatic subjugation of the Drevlians, the "Primary Chronicle" recounts how Olga "passed through the land of Dereva, accompanied by her son and her retinue, establishing laws and tribute. Her trading posts and hunting-reserves are there still." As queen, Olga established trading-posts and collected tribute along the Msta and the Luga rivers. She established hunting grounds, boundary posts, towns, and trading-posts across the empire. Olga's work helped to centralize state rule with these trade centers, called "pogosti", which served as administrative centers in addition to their mercantile roles. Olga's network of "pogosti" would prove important in the ethnic and cultural unification of the Russian nation, and her border posts began the establishment of national boundaries for the kingdom.

During her son's prolonged military campaigns, she remained in charge of Kiev, residing in the castle of Vyshgorod with her grandsons.

The "Primary Chronicle" does not go into additional detail about Olga's time as regent, but does tell the story of her conversion to Christianity and subsequent effect on the acceptance of Christianity in Eastern Europe.
In the 950s, Olga traveled to Constantinople, the capital of the Byzantine Empire, to visit Emperor Constantine VII. Once in Constantinople, Olga converted to Christianity with the assistance of the Emperor and the Patriarch. While the "Primary Chronicle" does not divulge Olga's motivation for her visit or conversion, it does go into great detail on the conversion process, in which she was baptized and instructed in the ways of Christianity:The reigning Emperor was named Constantine, son of Leo. Olga came before him, and when he saw that she was very fair of countenance and wise as well, the Emperor wondered at her intellect. He conversed with her and remarked that she was worthy to reign with him in his city. When Olga heard his words, she replied that she was still a pagan, and that if he desired to baptize her, he should perform this function himself; otherwise, she was unwilling to accept baptism. The Emperor, with the assistance of the Patriarch, accordingly baptized her. When Olga was enlightened, she rejoiced in soul and body. The Patriarch, who instructed her in the faith, said to her, "Blessed art thou among the women of Rus', for thou hast loved the light, and quit the darkness. The sons of Rus' shall bless thee to the last generation of thy descendants." He taught her the doctrine of the Church, and instructed her in prayer and fasting, in almsgiving, and in the maintenance of chastity. She bowed her head, and like a sponge absorbing water, she eagerly drank in his teachings. The Princess bowed before the Patriarch, saying, "Through thy prayers, Holy Father, may I be preserved from the crafts and assaults of the devil!" At her baptism she was christened Helena, after the ancient Empress, mother of Constantine the Great. The Patriarch then blessed her and dismissed her.While the "Primary Chronicle" notes that Olga was christened with the name "Helena" after the ancient Saint Helena (the mother of Constantine the Great), Jonathan Shepard argues that Olga's baptismal name comes from the contemporary emperor's wife, Helena.

The observation that Olga was "worthy to reign with him in his city" suggests that the emperor was interested in marrying her. While the "Chronicle" explains Constantine's desire to take Olga as his wife as stemming from the fact that she was "fair of countenance and wise as well," marrying Olga could certainly have helped him gain power over Rus'. The "Chronicle" recounts that Olga asked the emperor to baptize her knowing that his baptismal sponsorship, by the rules of spiritual kinship, would make marriage between them a kind of spiritual incest. Though her desire to become Christian may have been genuine, this request was also a way for her to maintain political independence. After the baptism, when Constantine repeated his marriage proposal, Olga answered that she could not marry him since Church law forbade a goddaughter to marry her godfather:After her baptism, the Emperor summoned Olga and made known to her that he wished her to become his wife. But she replied, "How can you marry me, after yourself baptizing me and calling me your daughter? For among Christians that is unlawful, as you yourself must know." Then the Emperor said, "Olga, you have outwitted me." He gave her many gifts of gold, silver, silks, and various vases, and dismissed her, still calling her his daughter.Francis Butler argues that the story of the proposal was a literary embellishment, describing an event that is highly unlikely to have ever actually occurred. In fact, at the time of her baptism, Constantine already had an empress. In addition to uncertainty over the truth of the "Chronicle" telling of events in Constantinople, there is controversy over the details of her conversion to Christianity. According to Russian sources, she was baptized in Constantinople in 957. Byzantine sources, however, indicate that she was a Christian prior to her 957 visit. It seems likely that she was baptized in Kiev around 955 and, following a second christening in Constantinople, took the Christian name Helen. Olga was not the first person from Rus' to convert from her pagan ways-- there were Christians in Igor's court who had taken oaths at the St. Elias Church in Kiev for the Rus'–Byzantine Treaty in 945--but she was the most powerful Rus' individual to undergo baptism during her life.

"The Primary" "Chronicle" reports that Olga received the Patriarch's blessing for her journey home, and that once she arrived, she unsuccessfully attempted to convert her son to Christianity:Now Olga dwelt with her son Svyatoslav, and she urged him to be baptized, but he would not listen to her suggestion, though when any man wished to be baptized, he was not hindered, but only mocked. For to the infidels, the Christian faith is foolishness. They do not comprehend it, because they walk in darkness and do not see the glory of God. Their hearts are hardened, and they can neither hear with their ears nor see with their eyes. For Solomon has said, "The deeds of the unrighteous are far from wisdom. Inasmuch as I have called you, and ye heard me not, I sharpened my words, and ye understood not. But ye have set at nought all my counsel, and would have none of my reproach. For they have hated knowledge, and the fear of Jehovah they have not chosen. They would none of my counsel, but despised all my reproof."This passage highlights the hostility towards Christianity in Kievan Rus' in the tenth century. In the "Chronicle," Svyatoslav declares that his followers would "laugh" if he were to accept Christianity. While Olga tried to convince her son that his followers would follow his example if he converted, her efforts were in vain. However, her son agreed not to persecute those in his kingdom who did convert, which marked a crucial turning point for Christianity in the area. Despite the resistance of her people to Christianity, Olga built churches in Kiev, Pskov, and elsewhere.

Seven Latin sources document Olga's embassy to Holy Roman Emperor Otto I in 959. The continuation of Regino of Prüm mentions that the envoys requested the emperor to appoint a bishop and priests for their nation. The chronicler accuses the envoys of lies, commenting that their trick was not exposed until later. Thietmar of Merseburg says that the first archbishop of Magdeburg, Saint Adalbert of Magdeburg, before being promoted to this high rank, was sent by Emperor Otto to the country of the Rus' ("Rusciae") as a simple bishop but was expelled by pagan allies of Svyatoslav I. The same data is repeated in the annals of Quedlinburg and Hildesheim. 

In 2018, Russian historian and writer Boris Akunin pointed out the importance of a 2-year gap between invitation and arrival of bishops: "The failure of Olga's Byzantine trip has inflicted a severe blow to her party. The Grand Knyaginya made a second attempt to find a Christian patron, now in the West. But it seems, in the period between the sending of the embassy to Emperor Otto in 959 and the arrival of Adalbert in Kiev in 961, a bloodless coup took place. Pagan party prevailed, the young Sviatoslav pushed his mother into the background, and that's why the German bishops had to return empty-handed."

According to Russian historian Vladimir Petrukhin, Olga invited the Roman Catholic bishops because she wanted to motivate Byzantine Orthodox priests to catechize the Rus' people more enthusiastically, by introducing competition.

According to the "Primary Chronicle", Olga died from illness in 969, soon after the Pechenegs' siege of the city. When Svyatoslav announced plans to move his throne to the Danube region, the ailing Olga convinced him to stay with her during her final days. Only three days later, she passed away and her family and all of Kievan Rus’ wept:Svyatoslav announced to his mother and his boyars, "I do not care to remain in Kiev, but should prefer to live in Perya-slavets on the Danube, since that is the centre of my realm, where all riches are concentrated; gold, silks, wine, and various fruits from Greece, silver and horses from Hungary and Bohemia, and from Rus' furs, wax, honey, and slaves." But Olga made reply, "You behold me in my weakness. Why do you desire to depart from me?" For she was already in precarious health. She thus remonstrated with him and begged him first to bury her and then to go wheresoever he would. Three days later Olga died. Her son wept for her with great mourning, as did likewise her grandsons and all the people. They thus carried her out, and buried her in her tomb. Olga had given command not to hold a funeral feast for her, for she had a priest who performed the last rites over the sainted Princess.Although he disapproved of his mother's Christian tradition, Svyatoslav heeded Olga's request that her priest, Gregory, conduct a Christian funeral without the ritual pagan burial feast. Her tomb remained in Kiev for over two centuries, but was destroyed by the Mongolian-Tatar armies of Batu Khan in 1240.

At the time of her death, it seemed that Olga's attempt to make Kievan Rus' a Christian territory had been a failure. Nonetheless, Olga's Christianizing mission would be brought to fruition by her grandson, Vladimir, who officially adopted Christianity in 988. The "Primary Chronicle" highlights Olga's holiness in contrast to the pagans around her during her life as well as the significance of her decision to convert to Christianity:Olga was the precursor of the Christian land, even as the day-spring precedes the sun and as the dawn precedes the day. For she shone like the moon by night, and she was radiant among the infidels like a pearl in the mire, since the people were soiled, and not yet purified of their sin by holy baptism. But she herself was cleansed by this sacred purification…. She was the first from Rus' to enter the kingdom of God, and the sons of Rus' thus praise her as their leader, for since her death she has interceded with God in their behalf.In 1547, nearly 600 years after her 969 death, the Russian Orthodox Church named Olga a saint. Because of her proselytizing influence, the Eastern Orthodox Church, the Ruthenian Greek Catholic Church, and the Ukrainian Greek Catholic Church call Saint Olga by the honorific Isapóstolos, "Equal to the Apostles". She is also a saint in the Roman Catholic Church. Olga's feast day is July 11, the date of her death. In keeping with her own biography, she is the patron of widows and converts.

Olga is venerated as Saint in East Slavic-speaking countries where churches uses the Byzantine Rite: Eastern Orthodox Church (especially in Russian Orthodox Church), Greek Catholic Church (especially in the Ukrainian Greek Catholic Church), in churches with Byzantine Rite Lutheranism, and in the Roman Catholic Church in Russia (Latin rite).


As an important figure in the history of Christianity, Olga's image as a saint lives on. But the question of Olga as a historical figure and character in the "Primary Chronicle" has been taken up in recent years.

Olga's historical characterization as a vengeful princess, juxtaposed with her estimation within the Orthodox tradition as a saint, has produced a variety of modern interpretations of her story. Scholars tend to be more conservative with their interpretations, focusing on what the "Primary Chronicle" makes explicit: Olga's role in the spread of Christianity to Eastern Europe and Russia. These texts, generally speaking, focus on Olga's role as advisor to her son, whose decision not to persecute Christians in the Kievan Rus' was a pivotal moment in the religious history of Russia and its neighboring lands. Academic work on Olga tends not to dwell on the narrative twists and turns of her story, instead focusing on extracting historical facts from the story.

Modern publications, however, have focused on her as an historical character. Journalists have penned articles with titles ranging from "Saint Olga of Kiev is the Best Warrior Princess You Never Knew" to "Meet the Murderous Viking Princess Who Brought the Faith to Eastern Europe." These texts, written for a broader audience, tend to focus on Olga's exploits as a sort of historical drama. Her Viking heritage is always brought up, and often used as an explanation for her fiery spirit and military accomplishments. Authors focus on the most dramatic details of her story: her murder of two Drevlian negotiating groups, her wily deception of the Drevlian ruler, and her ultimate conquest of his people. A number of sources make her out to be a proto-feminist figure, a woman who did not allow contemporaneous expectations of gender roles to lock her out of the leadership role. Because there is little evidence to support the idea that Olga's rule was ever questioned by her people, this characterization of her rule is a medievalism — that is, an assumption made about history based not on facts but on preconceptions about the past, in this case the rigid relationship between gender and medieval rulership.

Though a number of these contemporary sources refer to Olga as a "warrior princess", there is little evidence to suggest she actually participated in the fighting and killing of her enemies. Based on historical precedent, it is more likely that she was a commander of troops, a sort of general or commander-in-chief, than a warrior of particular skill. These assertions have still made their way into the public imagination, however, as evidenced by the appropriation of her image in the Eastern European heavy metal scene.

This duality of Olga's character — on the one hand a venerated saint, on the other a bloodthirsty commander of troops — has made her an attractive figure for subversive artists. Her image has been taken up in the heavy metal scene in some cases, most notably as the muse and cover figure for A Perfect Absolution, a concept album by French band Gorod about Olga of Kiev.

According to Russian politician Vladimir Medinsky, the influence of Olga's image as a ruler is underappreciated among the feminists: "Logically, Olga should have been a feminist icon. At least, Russian feminist icon. To stand up and throw a boat full of guys into a pit. And to bury them. Unfortunately (or maybe, in this case, fortunately), Russian ladies, the regular consumers of "Cosmopolitan" and "Sex & the City", do not know the history well. Seriously speaking, the fact of the second ruler of Russia being female is surprisingly poorly mastered by the public consciousness... For sure, the memory about Olga will be refreshed, alongside Catherine the Great's." Medinsky also pointed out Olga's successful political PR: "For politicians, it is very important to be perceived as wise and cunning. In their case, these two qualities merge and make them seem exceptional. And in this sense, Olga is the most successful politician of Ancient Russia. Her image of both clever and cunning has survived through the ages."

According to Russian historian Boris Akunin, the facts about Olga can be relatively clearly separated from the legends. For him, it's only plausible she murdered the envoys who wanted to replace her husband Igor with their Prince Mal, as Iskorosten was just two days' ride from Kiev, so it was impossible to conceal the first public murder. He also considers it obvious that she reconquered the Drevlians. Still, her large-scale administrative-economic reforms have some controversial implications: "Olga has secured for herself "traps" () (hunting lands) and "camps" () (guesting places). She was generally very concerned about the separation of her personal property from the state. It gave the Grand Knyazes the opportunity to dispose of the funds more voluntary, but at the same time it has inserted a time bomb into the centralized state: after a period of time, the division of the country into "Grand Kniaz's" and "non-Grand-Kniaz's" parts will become one of the reasons for Kievan Rus' collapse. However, Olga had secured her family's power and wealth for the next 100 years."




</doc>
<doc id="22818" url="https://en.wikipedia.org/wiki?curid=22818" title="Olympus Mons">
Olympus Mons

Olympus Mons (; Latin for Mount Olympus) is a very large shield volcano on the planet Mars. The volcano has a height of over 21 km (13.6 mi or 72,000 ft) as measured by the Mars Orbiter Laser Altimeter (MOLA). Olympus Mons is about two and a half times Mount Everest's height above sea level. It is one of the largest volcanoes, the tallest planetary mountain, and the second tallest mountain currently discovered in the Solar System, comparable to Rheasilvia on Vesta. It is often cited as the largest volcano in the Solar System. However, by some metrics, other volcanoes are considerably larger. Alba Mons, northeast of Olympus Mons, has roughly 19 times the surface area, but is only about one third the height. Pele, the largest known volcano on Io, is also much larger, at roughly 4 times the surface area, but is considerably flatter. Additionally, Tharsis Rise, a large volcanic structure on Mars of which Olympus Mons is a part, has been interpreted as an enormous spreading volcano. If this is confirmed, Tharsis would be by far the largest volcano in the Solar System. Olympus Mons is the youngest of the large volcanoes on Mars, having formed during Mars's Hesperian Period. It had been known to astronomers since the late 19th century as the albedo feature Nix Olympica (Latin for "Olympic Snow"). Its mountainous nature was suspected well before space probes confirmed its identity as a mountain.

The volcano is located in Mars' western hemisphere at approximately , just off the northwestern edge of the Tharsis bulge. The western portion of the volcano lies in the Amazonis quadrangle (MC-8) and the central and eastern portions in the adjoining Tharsis quadrangle (MC-9).

Two impact craters on Olympus Mons have been assigned provisional names by the International Astronomical Union. They are the -diameter Karzok crater () and the -diameter Pangboche crater (). The craters are notable for being two of several suspected source areas for shergottites, the most abundant class of Martian meteorites.

As a shield volcano, Olympus Mons resembles the shape of the large volcanoes making up the Hawaiian Islands. The edifice is about wide. Because the mountain is so large, with complex structure at its edges, allocating a height to it is difficult. Olympus Mons stands above the Mars global datum, and its local relief, from the foot of the cliffs which form its northwest margin to its peak, is over (a little over twice the height of Mauna Kea as measured from its base on the ocean floor). The total elevation change from the plains of Amazonis Planitia, over to the northwest, to the summit approaches . The summit of the mountain has six nested calderas (collapsed craters) forming an irregular depression × across and up to deep. The volcano's outer edge consists of an escarpment, or cliff, up to tall (although obscured by lava flows in places), a feature unique among the shield volcanoes of Mars, which may have been created by enormous flank landslides. Olympus Mons covers an area of about , which is approximately the size of Italy or the Philippines, and it is supported by a thick lithosphere. The extraordinary size of Olympus Mons is likely because Mars lacks mobile tectonic plates. Unlike on Earth, the crust of Mars remains fixed over a stationary hotspot, and a volcano can continue to discharge lava until it reaches an enormous height.

Being a shield volcano, Olympus Mons has a very gently sloping profile. The average slope on the volcano's flanks is only 5°. Slopes are steepest near the middle part of the flanks and grow shallower toward the base, giving the flanks a concave upward profile. The shape of Olympus Mons is distinctly asymmetrical—its flanks are shallower and extend farther from the summit in the northwestern direction than they do to the southeast. The volcano's shape and profile have been likened to a "circus tent" held up by a single pole that is shifted off center.

Due to the size and shallow slopes of Olympus Mons, an observer standing on the Martian surface would be unable to view the entire profile of the volcano, even from a great distance. The curvature of the planet and the volcano itself would obscure such a synoptic view. Similarly, an observer near the summit would be unaware of standing on a very high mountain, as the slope of the volcano would extend far beyond the horizon, a mere 3 kilometers away.

The typical atmospheric pressure at the top of Olympus Mons is 72 pascals, about 12% of the average Martian surface pressure of 600 pascals. Both are exceedingly low by terrestrial standards; by comparison, the atmospheric pressure at the summit of Mount Everest is 32,000 pascals, or about 32% of Earth's sea level pressure. Even so, high-altitude orographic clouds frequently drift over the Olympus Mons summit, and airborne Martian dust is still present. Although the average Martian surface atmospheric pressure is less than one percent of Earth's, the much lower gravity of Mars increases the atmosphere's scale height; in other words, Mars's atmosphere is expansive and does not drop off in density with height as sharply as Earth's.

The composition of Olympus Mons is approximately 44% silicates, 17.5% iron oxides (which give the planet its red coloration) 7% aluminum, 6% magnesium, 6% calcium, and particularly high proportions of sulfur oxide with 7%. These results point to the surface being largely composed of basalts and other mafic rocks, which would have erupted as low viscosity lava flows and hence lead to the low gradients on the surface of the planet.

Olympus Mons is an unlikely landing location for automated space probes in the near future. The high elevations preclude parachute-assisted landings because the atmosphere is insufficiently dense to slow the spacecraft down. Moreover, Olympus Mons stands in one of the dustiest regions of Mars. A mantle of fine dust obscures the underlying bedrock, possibly making rock samples hard to come by and likely posing a significant obstacle for rovers.
Olympus Mons is the result of many thousands of highly fluid, basaltic lava flows that poured from volcanic vents over a long period of time (the Hawaiian Islands exemplify similar shield volcanoes on a smaller scale – see Mauna Kea). Like the basalt volcanoes on Earth, Martian basaltic volcanoes are capable of erupting enormous quantities of ash. Due to the reduced gravity of Mars compared to Earth, there are lesser buoyant forces on the magma rising out of the crust. In addition, the magma chambers are thought to be much larger and deeper than the ones found on Earth. The flanks of Olympus Mons are made up of innumerable lava flows and channels. Many of the flows have levees along their margins (pictured). The cooler, outer margins of the flow solidify, leaving a central trough of molten, flowing lava. Partially collapsed lava tubes are visible as chains of pit craters, and broad lava fans formed by lava emerging from intact, subsurface tubes are also common. In places along the volcano's base, solidified lava flows can be seen spilling out into the surrounding plains, forming broad aprons, and burying the basal escarpment. Crater counts from high-resolution images taken by the Mars Express orbiter in 2004 indicate that lava flows on the northwestern flank of Olympus Mons range in age from 115 million years old (Mya) to only 2 Mya. These ages are very recent in geological terms, suggesting that the mountain may still be volcanically active, though in a very quiescent and episodic fashion.

The caldera complex at the peak of the volcano is made of at least six overlapping calderas and caldera segments (pictured). Calderas are formed by roof collapse following depletion and withdrawal of the subsurface magma chamber after an eruption. Each caldera thus represents a separate pulse of volcanic activity on the mountain. The largest and oldest caldera segment appears to have formed as a single, large lava lake. Using geometric relationships of caldera dimensions from laboratory models, scientists have estimated that the magma chamber associated with the largest caldera on Olympus Mons lies at a depth of about below the caldera floor. Crater size-frequency distributions on the caldera floors indicate the calderas range in age from 350 Mya to about 150 Mya. All probably formed within 100 million years of each other.

Olympus Mons is asymmetrical structurally as well as topographically. The longer, more shallow northwestern flank displays extensional features, such as large slumps and normal faults. In contrast, the volcano's steeper southeastern side has features indicating compression, including step-like terraces in the volcano's mid-flank region (interpreted as thrust faults) and a number of wrinkle ridges located at the basal escarpment. Why opposite sides of the mountain should show different styles of deformation may lie in how large shield volcanoes grow laterally and in how variations within the volcanic substrate have affected the mountain's final shape.

Large shield volcanoes grow not only by adding material to their flanks as erupted lava, but also by spreading laterally at their bases. As a volcano grows in size, the stress field underneath the volcano changes from compressional to extensional. A subterranean rift may develop at the base of the volcano, causing the underlying crust to spread apart. If the volcano rests on sediments containing mechanically weak layers (e.g., beds of water-saturated clay), detachment zones (decollements) may develop in the weak layers. The extensional stresses in the detachment zones can produce giant landslides and normal faults on the volcano's flanks, leading to the formation of a basal escarpment. Further from the volcano, these detachment zones can express themselves as a succession of overlapping, gravity driven thrust faults. This mechanism has long been cited as an explanation of the Olympus Mons aureole deposits (discussed below).

Olympus Mons lies at the edge of the Tharsis bulge, an ancient vast volcanic plateau likely formed by the end of the Noachian Period. During the Hesperian, when Olympus Mons began to form, the volcano was located on a shallow slope that descended from the high in Tharsis into the northern lowland basins. Over time, these basins received large volumes of sediment eroded from Tharsis and the southern highlands. The sediments likely contained abundant Noachian-aged phyllosilicates (clays) formed during an early period on Mars when surface water was abundant, and were thickest in the northwest where basin depth was greatest. As the volcano grew through lateral spreading, low-friction detachment zones preferentially developed in the thicker sediment layers to the northwest, creating the basal escarpment and widespread lobes of aureole material (Lycus Sulci). Spreading also occurred to the southeast; however, it was more constrained in that direction by the Tharsis rise, which presented a higher-friction zone at the volcano's base. Friction was higher in that direction because the sediments were thinner and probably consisted of coarser grained material resistant to sliding. The competent and rugged basement rocks of Tharsis acted as an additional source of friction. This inhibition of southeasterly basal spreading in Olympus Mons could account for the structural and topographic asymmetry of the mountain. Numerical models of particle dynamics involving lateral differences in friction along the base of Olympus Mons have been shown to reproduce the volcano's present shape and asymmetry fairly well.

It has been speculated that the detachment along the weak layers was aided by the presence of high-pressure water in the sediment pore spaces, which would have interesting astrobiological implications. If water-saturated zones still exist in sediments under the volcano, they would likely have been kept warm by a high geothermal gradient and residual heat from the volcano's magma chamber. Potential springs or seeps around the volcano would offer exciting possibilities for detecting microbial life.

Olympus Mons and a few other volcanoes in the Tharsis region stand high enough to reach above the frequent Martian dust-storms recorded by telescopic observers as early as the 19th century. The astronomer Patrick Moore pointed out that Schiaparelli (1835–1910) "had found that his "Nodus Gordis" and "Olympic Snow" [Nix Olympica] were almost the only features to be seen" during dust storms, and "guessed correctly that they must be high".

The Mariner 9 spacecraft arrived in orbit around Mars in 1971 during a global dust-storm. The first objects to become visible as the dust began to settle, the tops of the Tharsis volcanoes, demonstrated that the altitude of these features greatly exceeded that of any mountain found on Earth, as astronomers expected. Observations of the planet from Mariner 9 confirmed that Nix Olympica was a volcano. Ultimately, astronomers adopted the name "Olympus Mons" for the albedo feature known as Nix Olympica.

Olympus Mons is located between the northwestern edge of the Tharsis region and the eastern edge of Amazonis Planitia. It stands about from the other three large Martian shield volcanoes, collectively called the Tharsis Montes (Arsia Mons, Pavonis Mons, and Ascraeus Mons). The Tharsis Montes are slightly smaller than Olympus Mons.

A wide, annular depression or moat about deep surrounds the base of Olympus Mons and is thought to be due to the volcano's immense weight pressing down on the Martian crust. The depth of this depression is greater on the northwest side of the mountain than on the southeast side.

Olympus Mons is partially surrounded by a region of distinctive grooved or corrugated terrain known as the Olympus Mons aureole. The aureole consists of several large lobes. Northwest of the volcano, the aureole extends a distance of up to and is known as Lycus Sulci (). East of Olympus Mons, the aureole is partially covered by lava flows, but where it is exposed it goes by different names (Gigas Sulci, for example). The origin of the aureole remains debated, but it was likely formed by huge landslides or gravity-driven thrust sheets that sloughed off the edges of the Olympus Mons shield.



</doc>
<doc id="22820" url="https://en.wikipedia.org/wiki?curid=22820" title="Odobenidae">
Odobenidae

Odobenidae is a family of pinnipeds. The only living species is the walrus. In the past, however, the group was much more diverse, and includes more than a dozen fossil genera.

All genera, except "Odobenus", are extinct.

In re-analyzing "Pelagiarctos", Boessenecker et al. (2013) proposed the phylogenetic relationships of Odobenidae as follows (this analysis excluded "Archaeodobenus, Titanotaria, Nanodobenus," and "Pliopedia;" and included "Enaliarctos, Pteronarctos, Allodesmus, Desmatophoca, Callorhinus, Monachus," and "Erignathus"):


</doc>
<doc id="22826" url="https://en.wikipedia.org/wiki?curid=22826" title="Object database">
Object database

An object database is a database management system in which information is represented in the form of objects as used in object-oriented programming. Object databases are different from relational databases which are table-oriented. Object-relational databases are a hybrid of both approaches.

Object databases have been considered since the early 1980s.

Object-oriented database management systems (OODBMSs) also called ODBMS (Object Database Management System) combine database capabilities with object-oriented programming language capabilities.
OODBMSs allow object-oriented programmers to develop the product, store them as objects, and replicate or modify existing objects to make new objects within the OODBMS. Because the database is integrated with the programming language, the programmer can maintain consistency within one environment, in that both the OODBMS and the programming language will use the same model of representation. Relational DBMS projects, by way of contrast, maintain a clearer division between the database model and the application.

As the usage of web-based technology increases with the implementation of Intranets and extranets, companies have a vested interest in OODBMSs to display their complex data. Using a DBMS that has been specifically designed to store data as objects gives an advantage to those companies that are geared towards multimedia presentation or organizations that utilize computer-aided design (CAD).

Some object-oriented databases are designed to work well with object-oriented programming languages such as Delphi, Ruby, Python, JavaScript, Perl, Java, C#, Visual Basic .NET, C++, Objective-C and Smalltalk; others such as JADE have their own programming languages. OODBMSs use exactly the same model as object-oriented programming languages.

Object database management systems grew out of research during the early to mid-1970s into having intrinsic database management support for graph-structured objects. The term "object-oriented database system" first appeared around 1985. Notable research projects included Encore-Ob/Server (Brown University), EXODUS (University of Wisconsin–Madison), IRIS (Hewlett-Packard), ODE (Bell Labs), ORION (Microelectronics and Computer Technology Corporation or MCC), Vodak (GMD-IPSI), and Zeitgeist (Texas Instruments). The ORION project had more published papers than any of the other efforts. Won Kim of MCC compiled the best of those papers in a book published by The MIT Press.

Early commercial products included Gemstone (Servio Logic, name changed to GemStone Systems), Gbase (Graphael), and Vbase (Ontologic). Additional commercial products entered the market in the late 1980s through the mid 1990s. These included ITASCA (Itasca Systems), Jasmine (Fujitsu, marketed by Computer Associates), Matisse (Matisse Software), Objectivity/DB (Objectivity, Inc.), ObjectStore (Progress Software, acquired from eXcelon which was originally Object Design, Incorporated), ONTOS (Ontos, Inc., name changed from Ontologic), O (O Technology, merged with several companies, acquired by Informix, which was in turn acquired by IBM), POET (now FastObjects from Versant which acquired Poet Software), Versant Object Database (Versant Corporation), VOSS (Logic Arts) and JADE (Jade Software Corporation). Some of these products remain on the market and have been joined by new open source and commercial products such as InterSystems Caché.

Object database management systems added the concept of persistence to object programming languages. The early commercial products were integrated with various languages: GemStone (Smalltalk), Gbase (LISP), Vbase (COP) and VOSS (Virtual Object Storage System for Smalltalk). For much of the 1990s, C++ dominated the commercial object database management market. Vendors added Java in the late 1990s and more recently, C#.

Starting in 2004, object databases have seen a second growth period when open source object databases emerged that were widely affordable and easy to use, because they are entirely written in OOP languages like Smalltalk, Java, or C#, such as Versant's db4o (db4objects), DTS/S1 from Obsidian Dynamics and Perst (McObject), available under dual open source and commercial licensing.


Object databases based on persistent programming acquired a niche in application areas such as
engineering and spatial databases, telecommunications, and scientific areas such as high energy physics and molecular biology.

Another group of object databases focuses on embedded use in devices, packaged software, and real-time systems.

Most object databases also offer some kind of query language, allowing objects to be found using a declarative programming approach. It is in the area of object query languages, and the integration of the query and navigational interfaces, that the biggest differences between products are found. An attempt at standardization was made by the ODMG with the Object Query Language, OQL.

Access to data can be faster because an object can be retrieved directly without a search, by following pointers.

Another area of variation between products is in the way that the schema of a database is defined. A general characteristic, however, is that the programming language and the database schema use the same type definitions.

Multimedia applications are facilitated because the class methods associated with the data are responsible for its correct interpretation.

Many object databases, for example Gemstone or VOSS, offer support for versioning. An object can be viewed as the set of all its versions. Also, object versions can be treated as objects in their own right. Some object databases also provide systematic support for triggers and constraints which are the basis of active databases.

The efficiency of such a database is also greatly improved in areas which demand massive amounts of data about one item. For example, a banking institution could get the user's account information and provide them efficiently with extensive information such as transactions, account information entries etc.

The Object Data Management Group was a consortium of object database and object-relational mapping vendors, members of the academic community, and interested parties. Its goal was to create a set of specifications that would allow for portable applications that store objects in database management systems. It published several versions of its specification. The last release was ODMG 3.0. By 2001, most of the major object database and object-relational mapping vendors claimed conformance to the ODMG Java Language Binding. Compliance to the other components of the specification was mixed. In 2001, the ODMG Java Language Binding was submitted to the Java Community Process as a basis for the Java Data Objects specification. The ODMG member companies then decided to concentrate their efforts on the Java Data Objects specification. As a result, the ODMG disbanded in 2001.

Many object database ideas were also absorbed into and have been implemented in varying degrees in object-relational database products.

In 2005 Cook, Rai, and Rosenberger proposed to drop all standardization efforts to introduce additional object-oriented query APIs but rather use the OO programming language itself, i.e., Java and .NET, to express queries. As a result, Native Queries emerged. Similarly, Microsoft announced Language Integrated Query (LINQ) and DLINQ, an implementation of LINQ, in September 2005, to provide close, language-integrated database query capabilities with its programming languages C# and VB.NET 9.

In February 2006, the Object Management Group (OMG) announced that they had been granted the right to develop new specifications based on the ODMG 3.0 specification and the formation of the Object Database Technology Working Group (ODBT WG). The ODBT WG planned to create a set of standards that would incorporate advances in object database technology (e.g., replication), data management (e.g., spatial indexing), and data formats (e.g., XML) and to include new features into these standards that support domains where object databases are being adopted (e.g., real-time systems). The work of the ODBT WG was suspended in March 2009 when, subsequent to the economic turmoil in late 2008, the ODB vendors involved in this effort decided to focus their resources elsewhere.

In January 2007 the World Wide Web Consortium gave final recommendation status to the XQuery language. XQuery uses XML as its data model. Some of the ideas developed originally for object databases found their way into XQuery, but XQuery is not intrinsically object-oriented. Because of the popularity of XML, XQuery engines compete with object databases as a vehicle for storage of data that is too complex or variable to hold conveniently in a relational database. XQuery also allows modules to be written to provide encapsulation features that have been provided by Object-Oriented systems.

XQuery v1 and XPath v2 are extremely complex (no FOSS software is implementing these standards more than 10 years after their publication) when compared to XPath v1 and XSLT v1, and XML did not fit all community demands as an open format. Since the early 2000s JSON has gained community adoption and popularity in applications, surpassing XML in the 2010s. JSONiq, a query-analog of XQuery for JSON (sharing XQuery's core expressions and operations), demonstrated the functional equivalence of the JSON and XML formats. In this context, the main strategy of OODBMS maintainers was to retrofit JSON to their databases (by using it as the internal data type).

In January 2016, with the PostgreSQL 9.5 release was the first FOSS OODBMS to offer an efficient JSON internal datatype (JSONB) with a complete set of functions and operations, for all basic relational and non-relational manipulations.

An object database stores complex data and relationships between data directly, without mapping to relational rows and columns, and this makes them suitable for applications dealing with very complex data. Objects have a many-to-many relationship and are accessed by the use of pointers. Pointers are linked to objects to establish relationships. Another benefit of an OODBMS is that it can be programmed with small procedural differences without affecting the entire system.




</doc>
<doc id="22827" url="https://en.wikipedia.org/wiki?curid=22827" title="Ovo-lacto vegetarianism">
Ovo-lacto vegetarianism

An ovo-lacto vegetarian or lacto-ovo vegetarian is a vegetarian who consumes some animal products, such as eggs and dairy. Unlike pescatarians, they do not consume fish or other seafood. A typical ovo-lacto vegetarian diet may include fruits, vegetables, grains, nuts, seeds, herbs, roots, fungi, milk, cheese, yogurt, kefir, and eggs.

The terminology stems from the Latin "" meaning "milk" (as in 'lactation'), "" meaning "egg", and the English term "vegetarian", so as giving the definition of a vegetarian diet containing milk and eggs.

In the Western world, ovo-lacto vegetarians are the most common type of vegetarian. Generally speaking, when one uses the term "vegetarian", an ovo-lacto vegetarian is assumed. Ovo-lacto vegetarians are often well-catered to in restaurants and shops, especially in some parts of Europe and metropolitan cities in North America.

Jainism prohibits causing harm to anything with a soul or potential life. Traditionally this includes eggs and certain kinds of vegetables, as well as animals, but dairy products are permitted. Jains are therefore lacto vegetarians, not ovo-lacto vegetarians.

In Hinduism, many individuals are either raised as ovo-lacto vegetarians or lacto vegetarians.

The Bible Christian Church was a Christian vegetarian sect founded by William Cowherd in 1809. Cowherd was one of the philosophical forerunners of the Vegetarian Society founded in 1847. The Bible Christian Church promoted the use of eggs, dairy and honey as God's given food per "the promised land flowing with milk and honey" (Exodus 3:8).

Many Seventh-day Adventist followers are ovo-lacto vegetarians. For over 130 years, Seventh-day Adventists have recommended a vegetarian diet which may include milk products and eggs.

In India, eggs are not universally considered vegetarian; those who do practice ovo-vegetarianism are often described as "eggetarians". To accommodate this, products containing eggs are specially marked to differentiate them from otherwise vegetarian food products. Some manufacturers specifically advise that their products contain eggs but not meat or animal products to avoid diminishing interest among those who practice ovo-vegetarianism.



</doc>
<doc id="22829" url="https://en.wikipedia.org/wiki?curid=22829" title="Orgy of the Dead">
Orgy of the Dead

Orgy of the Dead is a 1965 erotic horror film directed by Stephen C. Apostolof (under the alias A. C. Stephen). The screenplay was written by cult film director Edward D. Wood Jr., who adapted the screenplay into a novel.

The film belongs to the genre of nudie cuties, narrative-based films featuring female nudity. It was an evolution of earlier films, which featured striptease and burlesque shows. These predecessors mostly depicted actual stage performances, sometimes attached to a frame story.

The film has "little to no" story line. About 70 minutes of the film's running time features topless female dancers without any dialogue. 

The film opens to two muscle-bound men dressed in loincloths approaching a crypt. They open the doors, revealing a coffin. They remove the lid and exit the crypt, then the inhabitant of the coffin (Criswell) sits up to deliver an opening narration.

A lone Chevrolet Corvair drives down a California desert road. Its passengers, Bob and Shirley, are arguing over the decision to use this night to search for a cemetery. Bob is a horror writer who hopes that the scene of a cemetery at night will bring him inspiration. The conversation ends when Bob accidentally drives the car off the road and over a cliff.

The next scene opens to a nocturnal image of a fog-shrouded cemetery. The lonely figure of the Emperor walks towards a marble altar, sits, and then summons his "Princess of the Night", the Black Ghoul, who appears and bows before him. The Emperor warns that if the night's entertainment fails to please him, he will banish the souls of the entertainers to eternal damnation, indicating that he is an all-powerful demonic being.

As the full moon appears, the Black Ghoul summons the first dancer of the night, a Native American woman. The Black Ghoul explains that this woman loved flames, and that her lovers and she died in flames. The woman dances and strips before the flames of the cemetery. The Black Ghoul then introduces the second dancer of the night, a street walker in life. While the woman dances, Bob and Shirley make their way to the cemetery and start observing the dance from a distance. Shirley suspects that they are observing a college initiation, though Bob seriously doubts her theory.

The Emperor himself summons the third dancer, a woman who worshiped gold above else. The Golden Girl dances in her turn, and the Emperor instructs his loin-clothed servants to reward her with gold. The supposed reward is soon revealed to be a punishment, as the servants place her in a cauldron with liquid gold. What emerges from the cauldron is a golden statue of the living woman who entered. The servants transport the immobile statue to a nearby crypt.

A werewolf and a mummy appear and seize the intruding young couple. They are brought before the Emperor, who decides to postpone deciding their fate. The intruders are tied up, side by side, and allowed to continue watching the dances. The Black Ghoul next introduces the fourth dancer, a "Cat Woman" (Texas Starr). She is depicted as a woman dressed in a leopard costume, which exposes her chest area. As she dances, a servant follows her around and thrashes her with a bullwhip, offering a sadomasochistic show for the spectators.

The Emperor next calls for a Slave Girl to be whipped for his amusement. The slave wears a tunic and is chained to a wall. Following her torture session, the Slave Girl breaks free and becomes the fifth dancer of the night. Later, the Black Ghoul exhibits a fascination with Shirley and scratches a mark on her. She draws a knife and seems about to kill Shirley, when the Emperor decides it is not yet time for the intruders to properly join them. The female ghoul reluctantly obeys.

The Emperor is puzzled when a human skull appears instead of the next dancer. The Black Ghoul explains it is the symbol of the sixth dancer, who loved bullfighting and matadors. She used to dance over their demise, and now it is time to dance over her own. The dancer of apparent Spanish/Mexican heritage (Stephanie Jones) appears to perform. The Emperor and Ghoul briefly discuss the past of the dancer, who came to them on the Day of the Dead. The seventh dancer appears dressed in Polynesian garments. The Black Ghoul describes her as a worshiper of snakes, smoke, and flames. A rattlesnake is depicted along with her dance. The camera shifts to the mummy and the werewolf. The mummy voices his dislike of snakes and recalls the death of Cleopatra. He informs his companion that ancient Egypt had many snakes and they were the stuff of nightmares.

The Emperor next expresses his boredom and demands "unusual" entertainment, while the Black Ghoul notes that the night is almost over. She reminds her superior that they will be gone at the first sight of the morning sun. They proceed to argue over the fate of Shirley. The argument ends with the introduction of the eighth dancer, a woman who murdered her husband on their wedding night. She dances with the skeleton of her spouse. The argument over Shirley then resumes, as the Ghoul claims her for her own. The Emperor feels the need to assert his own authority over the Black Ghoul.

The ninth dancer was a zombie in life and remains zombie-like in death. The tenth and final dancer is introduced as one who died for feathers, fur, and fluff. She starts her dance in clothing matching this style. When the final dance ends, the Emperor finally offers Shirley to the Ghoul. The Ghoul briefly dances herself as she prepares to claim her prize, but dawn arrives and with it, sunlight. The Emperor and all his undead are reduced to bones. The final scene portrays Bob and Shirley waking up at the scene of the accident, surrounded by paramedics, suggesting it was all a dream. Criswell appears in his coffin to offer parting words to the audience.


Apostolof was attracted to the project, because the film was "relatively very inexpensive" to produce and direct. 

The film's graveyard prologue is a recreation of the opening scene from Ed Wood's then-unreleased 1958 film "Night of the Ghouls". Originally, Wood titled the film's script "Night of the Ghouls", as he did not expect the 1958 film to ever be released. The film also had a working title "Ghoulies".

The action begins when a young couple, Bob (William Bates) and Shirley (sexploitation actress Pat Barrington, billed as Pat Barringer) survive a car crash only to find themselves tied to posts in a misty cemetery where they are forced to watch dead spirits dance for the Emperor of the Night played by Criswell (best known for "Plan 9 from Outer Space"). Criswell reprises his role from the earlier film. Wood convinced Apostolof to cast his friend Criswell in the film. His lines were written on cue cards, which he had difficulty reading because he wasn't wearing his glasses. 

Ten striptease performances by topless dancers outfitted in various motifs comprise most of this movie. The Wolf Man (wearing a very obvious mask, with the actor's bare neck visible below the bottom of the mask) and the Mummy are also tossed in for comic relief. Barrington doubles as the blond Golden Girl (inspired by Shirley Eaton in "Goldfinger") while her red-headed "Shirley" character watches her perform. The dancing has been described as awkward and wooden, probably exacerbated by Apostolof firing the dance coordinator during the shooting of the film. 

Criswell's undead consort, the sexy Black Ghoul, was allegedly written for Maila Nurmi, a.k.a. Vampira, but was instead played by Fawn Silver, who wore a black bouffant wig. The Black Ghoul appears to have "pasty white skin", with red fingernails and lipstick. She wears a black dress, implying the role of a funerary garment. Black, red, and white are the main colors associated with her.

Wood served as writer, production manager, casting agent, and even held up cue cards on the film, although he did not direct. Apostolof paid Wood $400 for the script. Several gaffes typical for Wood-associated projects are present, such as day-for-night issues, poor integration of stock footage, obviously fake props and absurd dialogue (for example, at one point, Criswell declares something to be "more than a fact").

Wood allegedly stole money from the film's budget to purchase alcohol for himself. The incident caused a falling-out between Apostolof and Wood, and the two men would not collaborate again until 1972. 

An article on the making of this film was published in "Femme Fatales", 7:1 (June 1998).

Film journalists Andrew J. Rausch and Charles E. Pratt describe the film's attempt in eroticism as juvenile, with a puerile focus on jiggling female breasts. They write that the viewer "just has to sit back in awe and speculate how something like this could come from the mind of a grown man".

TV Guide criticized the film's "boring" striptease performances and music, but praised Wood's dialog as "priceless". 

Allmovie critic Mark Deming stated that the film "moves like molasses on a cold morning", and that "Orgy of the Dead" "is that rare film that would have been improved if Ed Wood had directed it".

In September 2017, the film was restored in 2K and released on DVD and Blu-ray by Vinegar Syndrome.





</doc>
<doc id="22830" url="https://en.wikipedia.org/wiki?curid=22830" title="Ostwald process">
Ostwald process

The Ostwald process is a chemical process used for making nitric acid (HNO). Wilhelm Ostwald developed the process, and he patented it in 1902. The Ostwald process is a mainstay of the modern chemical industry, and it provides the main raw material for the most common type of fertilizer production. Historically and practically, the Ostwald process is closely associated with the Haber process, which provides the requisite raw material, ammonia (NH).

Ammonia is converted to nitric acid in 2 stages. It is oxidized by heating with oxygen in the presence of a catalyst such as platinum with 10% rhodium, platinum metal on fused silica wool, copper or nickel, to form nitric oxide (nitrogen(II) oxide) and water (as steam). This reaction is strongly exothermic, making it a useful heat source once initiated:

Stage two encompasses two reactions and is carried out in an absorption apparatus containing water. Initially nitric oxide is oxidized again to yield nitrogen dioxide (nitrogen(IV) oxide). This gas is then readily absorbed by the water, yielding the desired product (nitric acid, albeit in a dilute form), while reducing a portion of it back to nitric oxide:

The NO is recycled, and the acid is concentrated to the required strength by distillation.

Alternatively, if the last step is carried out in air:

Typical conditions for the first stage, which contribute to an overall yield of about 98%, are:

A complication that needs to be taken into consideration involves a side-reaction in the first step that reverts the nitric oxide back to nitrogen:

This is a secondary reaction that is minimised by reducing the time the gas mixtures are in contact with the catalyst.

The overall reaction is the sum of the first equation, 3 times the second equation, and 2 times the last equation; all divided by 2:

Alternatively, if the last step is carried out in air, the overall reaction is the sum of equation 1, 2 times the equation 2, and equation 4; all divided by 2.

Without considering the state of water,



</doc>
<doc id="22831" url="https://en.wikipedia.org/wiki?curid=22831" title="Oliver Heaviside">
Oliver Heaviside

Oliver Heaviside FRS (; 18 May 1850 – 3 February 1925) was an English self-taught electrical engineer, mathematician, and physicist who adapted complex numbers to the study of electrical circuits, invented mathematical techniques for the solution of differential equations (equivalent to Laplace transforms), reformulated Maxwell's field equations in terms of electric and magnetic forces and energy flux, and independently co-formulated vector analysis. Although at odds with the scientific establishment for most of his life, Heaviside changed the face of telecommunications, mathematics, and science.

Heaviside was born in Camden Town, London, at 55 Kings Street (now Plender Street). He was a short and red-headed child, and suffered from scarlet fever when young, which left him with a hearing impairment. A small legacy enabled the family to move to a better part of Camden when he was thirteen and he was sent to Camden House Grammar School. He was a good student, placed fifth out of five hundred students in 1865, but his parents could not keep him at school after he was 16, so he continued studying for a year by himself and had no further formal education.

Heaviside's uncle by marriage was Sir Charles Wheatstone (1802–1875), an internationally celebrated expert in telegraphy and electromagnetism, and the original co-inventor of the first commercially successful telegraph in the mid-1830s. Wheatstone took a strong interest in his nephew's education and in 1867 sent him north to work with his own, older brother Arthur, who was managing one of Wheatstone's telegraph companies in Newcastle-upon-Tyne.

Two years later he took a job as a telegraph operator with the Danish Great Northern Telegraph Company laying a cable from Newcastle to Denmark using British contractors. He soon became an electrician. Heaviside continued to study while working, and by the age of 22 he published an article in the prestigious "Philosophical Magazine" on 'The Best Arrangement of Wheatstone's Bridge for measuring a Given Resistance with a Given Galvanometer and Battery' which received positive comments from physicists who had unsuccessfully tried to solve this algebraic problem, including Sir William Thomson, to whom he gave a copy of the paper, and James Clerk Maxwell. When he published an article on the duplex method of using a telegraph cable, he poked fun at R. S. Culley, the engineer in chief of the Post Office telegraph system, who had been dismissing duplex as impractical. Later in 1873 his application to join the Society of Telegraph Engineers was turned down with the comment that "they didn't want telegraph clerks". This riled Heaviside, who asked Thomson to sponsor him, and along with support of the society's president he was admitted "despite the P.O. snobs".

In 1873 Heaviside had encountered Maxwell's newly published, and later famous, two-volume "Treatise on Electricity and Magnetism". In his old age Heaviside recalled:
Undertaking research from home, he helped develop transmission line theory (also known as the ""telegrapher's equations""). Heaviside showed mathematically that uniformly distributed inductance in a telegraph line would diminish both attenuation and distortion, and that, if the inductance were great enough and the insulation resistance not too high, the circuit would be distortionless in that currents of all frequencies would have equal speeds of propagation. Heaviside's equations helped further the implementation of the telegraph.

From 1882 to 1902, except for three years, he contributed regular articles to the trade paper "The Electrician", which wished to improve its standing, for which he was paid £40 per year. This was hardly enough to live on, but his demands were very small and he was doing what he most wanted to. Between 1883 and 1887 these averaged 2–3 articles per month and these articles later formed the bulk of his "Electromagnetic Theory" and "Electrical Papers".

In 1880, Heaviside researched the skin effect in telegraph transmission lines. That same year he patented, in England, the coaxial cable. In 1884 he recast Maxwell's mathematical analysis from its original cumbersome form (they had already been recast as quaternions) to its modern vector terminology, thereby reducing twelve of the original twenty equations in twenty unknowns down to the four differential equations in two unknowns we now know as Maxwell's equations. The four re-formulated Maxwell's equations describe the nature of electric charges (both static and moving), magnetic fields, and the relationship between the two, namely electromagnetic fields.

Between 1880 and 1887, Heaviside developed the operational calculus using "p" for the differential operator, (which Boole had previously denoted by "D"), giving a method of solving differential equations by direct solution as algebraic equations. This later caused a great deal of controversy, owing to its lack of rigour. He famously said, "Mathematics is an experimental science, and definitions do not come first, but later on. They make themselves, when the nature of the subject has developed itself." On another occasion he asked somewhat more defensively, "Shall I refuse my dinner because I do not fully understand the process of digestion?"

In 1887, Heaviside worked with his brother Arthur on a paper entitled "The Bridge System of Telephony". However the paper was blocked by Arthur's superior, William Henry Preece of the Post Office, because part of the proposal was that loading coils (inductors) should be added to telephone and telegraph lines to increase their self-induction and correct the distortion which they suffered. Preece had recently declared self-inductance to be the great enemy of clear transmission. Heaviside was also convinced that Preece was behind the sacking of the editor of "The Electrician" which brought his long-running series of articles to a halt (until 1891). There was a long history of animosity between Preece and Heaviside. Heaviside considered Preece to be mathematically incompetent, an assessment supported by the biographer Paul J. Nahin: "Preece was a powerful government official, enormously ambitious, and in some remarkable ways, an utter blockhead." Preece's motivations in suppressing Heaviside's work were more to do with protecting Preece's own reputation and avoiding having to admit error than any perceived faults in Heaviside's work.

The importance of Heaviside's work remained undiscovered for some time after publication in "The Electrician", and so its rights lay in the public domain. In 1897, AT&T employed one of its own scientists, George A. Campbell, and an external investigator Michael I. Pupin to find some respect in which Heaviside's work was incomplete or incorrect. Campbell and Pupin extended Heaviside's work, and AT&T filed for patents covering not only their research, but also the technical method of constructing the coils previously invented by Heaviside. AT&T later offered Heaviside money in exchange for his rights; it is possible that the Bell engineers' respect for Heaviside influenced this offer. However, Heaviside refused the offer, declining to accept any money unless the company were to give him full recognition. Heaviside was chronically poor, making his refusal of the offer even more striking.

But this setback had the effect of turning Heaviside's attention towards electromagnetic radiation, and in two papers of 1888 and 1889, he calculated the deformations of electric and magnetic fields surrounding a moving charge, as well as the effects of it entering a denser medium. This included a prediction of what is now known as Cherenkov radiation, and inspired his friend George FitzGerald to suggest what now is known as the Lorentz–FitzGerald contraction.

In 1889, Heaviside first published a correct derivation of the magnetic force on a moving charged particle, which is the magnetic component of what is now called the Lorentz force.

In the late 1880s and early 1890s, Heaviside worked on the concept of electromagnetic mass. Heaviside treated this as material mass, capable of producing the same effects. Wilhelm Wien later verified Heaviside's expression (for low velocities).

In 1891 the British Royal Society recognized Heaviside's contributions to the mathematical description of electromagnetic phenomena by naming him a Fellow of the Royal Society, and the following year devoting more than fifty pages of the "Philosophical Transactions" of the Society to his vector methods and electromagnetic theory. In 1905 Heaviside was given an honorary doctorate by the University of Göttingen.

In 1896, FitzGerald and John Perry obtained a civil list pension of £120 per year for Heaviside, who was now living in Devon, and persuaded him to accept it, after he had rejected other charitable offers from the Royal Society.

In 1902, Heaviside proposed the existence of what is now known as the Kennelly–Heaviside layer of the ionosphere. Heaviside's proposal included means by which radio signals are transmitted around the Earth's curvature. The existence of the ionosphere was confirmed in 1923. The predictions by Heaviside, combined with Planck's radiation theory, probably discouraged further attempts to detect radio waves from the Sun and other astronomical objects. For whatever reason, there seem to have been no attempts for 30 years, until Jansky's development of radio astronomy in 1932.

In later years his behavior became quite eccentric. According to associate B. A. Behrend, he became a recluse who was so averse to meeting people that he delivered the manuscripts of his "Electrician" papers to a grocery store, where the editors picked them up. Though he had been an active cyclist in his youth, his health seriously declined in his sixth decade. During this time Heaviside would sign letters with the initials ""W.O.R.M."" after his name. Heaviside also reportedly started painting his fingernails pink and had granite blocks moved into his house for furniture. In 1922, he became the first recipient of the Faraday Medal, which was established that year.

On Heaviside's religious views, he was a Unitarian, but not a religious one. He was even said to have made fun of people who put their faith in a supreme being.

Heaviside died on 3 February 1925, at Torquay in Devon after falling from a ladder, and is buried near the eastern corner of Paignton cemetery. He is buried with his father, Thomas Heaviside (1813–1896) and his mother, Rachel Elizabeth Heaviside. The gravestone was cleaned thanks to an anonymous donor sometime in 2005. Most of his recognition was gained posthumously.

In July 2014, academics at Newcastle University, UK and the Newcastle Electromagnetics Interest Group founded the Heaviside Memorial Project in a bid to fully restore the monument through public subscription. The restored memorial was ceremonially unveiled on 30 August 2014 by Alan Heather, a distant relative of Heaviside. The unveiling was attended by the Mayor of Torbay, the MP for Torbay, an ex-curator of the Science Museum (representing the Institution of Engineering and Technology), the Chairman of the Torbay Civic Society, and delegates from Newcastle University.

A collection of Heaviside's notebooks, papers, correspondence, notes and annotated pamphlets on telegraphy is held at the Institution of Engineering and Technology (IET) Archive Centre.

Heaviside did much to develop and advocate vector methods and vector calculus. Maxwell's formulation of electromagnetism consisted of 20 equations in 20 variables. Heaviside employed the curl and divergence operators of the vector calculus to reformulate 12 of these 20 equations into four equations in four variables (B, E, J, and ρ), the form by which they have been known ever since (see Maxwell's equations). Less well known is that Heaviside's equations and Maxwell's are not exactly the same, and in fact it is easier to modify the former to make them compatible with quantum physics. The possibility of gravitational waves was also discussed by Heaviside using the analogy between the inverse-square law in gravitation and electricity.

He invented the Heaviside step function, using it to calculate the current when an electric circuit is switched on. He was the first to use the unit impulse function now usually known as the Dirac delta function. He invented his operational calculus method for solving linear differential equations. This resembles the currently used Laplace transform method based on the "Bromwich integral" named after Bromwich who devised a rigorous mathematical justification for Heaviside's operator method using contour integration. Heaviside was familiar with the Laplace transform method but considered his own method more direct.

Heaviside developed the transmission line theory (also known as the "telegrapher's equations"), which had the effect of increasing the transmission rate over transatlantic cables by a factor of ten. It originally took ten minutes to transmit each character, and this immediately improved to one character per minute. Closely related to this was his discovery that telephone transmission could be greatly improved by placing electrical inductance in series with the cable. Heaviside also independently discovered the Poynting vector.

Heaviside advanced the idea that the Earth's uppermost atmosphere contained an ionized layer known as the ionosphere; in this regard, he predicted the existence of what later was dubbed the Kennelly–Heaviside layer. In 1947 Edward Victor Appleton received the Nobel Prize in Physics for proving that this layer really existed.

Heaviside coined the following terms of art in electromagnetic theory:
Heaviside is sometimes also credited with coining "susceptance" (the imaginary part of admittance, reciprocal of reactance), but this is actually due to Charles Proteus Steinmetz.



Sorted by date.



</doc>
<doc id="22832" url="https://en.wikipedia.org/wiki?curid=22832" title="Book of Omni">
Book of Omni

The Book of Omni () is one of the books that make up the Book of Mormon. The book contains only one chapter although it covers more than two centuries of Nephite history (from "ca" 323 BC to 130 BC, according to footnotes).

Nephi created a book of metal plates for engraving a record. Omni is a descendant of Nephi and he receives the plates through his ancestors. In turn he passes them to his son Amaron. The plates then pass to Amaron's brother Chemish, then to Chemish's son Abinadom, and finally to Abinadom's son Amaleki (). The people of the country, called the Nephites, are in general decline. As each descendant receives the book, they generally write less and less than his predecessor, until the final author, Amaleki. As the last historian of a 400-year civilization, he writes an eloquent, lengthy dirge of his people.

The initial author was Omni, but several others were charged with keeping the record as time passed, though few made significant contributions. Verse 5 explains that "the more wicked part of the Nephites were destroyed." There is little detail about the destruction, except to say that the Lord did visit them in great judgment because of their wickedness.

Abinadom speaks of many wars between the people of Nephi and the Lamanites.

Amaleki speaks of the then current Nephite king, named Mosiah. As had happened previously, the Lord told the king (who appears to be a spiritual leader [prophet] as well as a secular leader) to lead the righteous Nephites out of the land of Nephi, their ancestral home for the previous 400 years, to a new place. At the end of their journey they discover the Mulekite people whose ancestors had also come from Jerusalem, but after it was attacked by the Babylonians. These people, however, did not bring religious or historical records with them which had two results—they had lost their religion, and they were unable to preserve their language from generation to generation. These people are known as the people of Zarahemla (the name of their then current king and also the name given to the land). Mosiah arranges for the people of Zarahemla to be taught the Nephite language, and Zarahemla is able to recount to him their oral history.

The two groups of people united themselves with Mosiah as their king, and they are all known as Nephites.

The first mention of the Jaredites is found here as well. A large stone is found with writing on it. Mosiah is able to "interpret the engravings by the gift and power of God." It tells of a man named Coriantumr and the downfall of his people. Their history is recounted more fully in the Book of Ether.

Mosiah, the king dies and his son, Benjamin, becomes king. There is a war between the Nephites led by Benjamin and the Lamanites, which by this time is nothing new.

It is apparent that many of the Nephites were reluctant to leave their long-time homeland. Ameliki describes how some of the Nephites wished to return to the land of Nephi, apparently in an attempt to reclaim it. At the time Ameliki stops writing, he has not received word of them, including his brother who is among them.

Amaleki closes with some words about Christ, asserting that his words are true and that it is his intent to help others come unto Christ. He states at the close of the book that, having no descendants to carry on the record-keeping, he will give the records to King Benjamin.

The Book of Omni is the last of the books contained on the Small Plates of Nephi, one of two major divisions of the gold plates.

From First Nephi to the end of Omni, the narratives are first person perspectives (though there are many quotations). The book immediately following Omni, the Words of Mormon, is an editorial insertion that explains why the first person narrative was inserted into the Book of Mormon and how the subsequent narratives differ, being mostly third person narration by Mormon summarizing other lengthier accounts taken from the Large Plates of Nephi. This third person record extends from Mosiah to Fourth Nephi.



</doc>
<doc id="22834" url="https://en.wikipedia.org/wiki?curid=22834" title="Ozone layer">
Ozone layer

The ozone layer or ozone shield is a region of Earth's stratosphere that absorbs most of the Sun's ultraviolet radiation. It contains a high concentration of ozone (O) in relation to other parts of the atmosphere, although still small in relation to other gases in the stratosphere. The ozone layer contains less than 10 parts per million of ozone, while the average ozone concentration in Earth's atmosphere as a whole is about 0.3 parts per million. The ozone layer is mainly found in the lower portion of the stratosphere, from approximately above Earth, although its thickness varies seasonally and geographically.

The ozone layer was discovered in 1913 by the French physicists Charles Fabry and Henri Buisson. Measurements of the sun showed that the radiation sent out from its surface and reaching the ground on Earth is usually consistent with the spectrum of a black body with a temperature in the range of 5,500–6,000 K (5,227 to 5,727 °C), except that there was no radiation below a wavelength of about 310 nm at the ultraviolet end of the spectrum. It was deduced that the missing radiation was being absorbed by something in the atmosphere. Eventually the spectrum of the missing radiation was matched to only one known chemical, ozone. Its properties were explored in detail by the British meteorologist G. M. B. Dobson, who developed a simple spectrophotometer (the Dobsonmeter) that could be used to measure stratospheric ozone from the ground. Between 1928 and 1958, Dobson established a worldwide network of ozone monitoring stations, which continue to operate to this day. The "Dobson unit", a convenient measure of the amount of ozone overhead, is named in his honor.

The ozone layer absorbs 97 to 99 percent of the Sun's medium-frequency ultraviolet light (from about 200 nm to 315 nm wavelength), which otherwise would potentially damage exposed life forms near the surface.

In 1976, atmospheric research revealed that the ozone layer was being depleted by chemicals released by industry, mainly chlorofluorocarbons (CFCs). Concerns that increased UV radiation due to ozone depletion threatened life on Earth, including increased skin cancer in humans and other ecological problems, led to bans on the chemicals, and the latest evidence is that ozone depletion has slowed or stopped. The United Nations General Assembly has designated September 16 as the International Day for the Preservation of the Ozone Layer.

Venus also has a thin ozone layer at an altitude of 100 kilometers above the planet's surface.

The photochemical mechanisms that give rise to the ozone layer were discovered by the British physicist Sydney Chapman in 1930. Ozone in the Earth's stratosphere is created by ultraviolet light striking ordinary oxygen molecules containing two oxygen atoms (O), splitting them into individual oxygen atoms (atomic oxygen); the atomic oxygen then combines with unbroken O to create ozone, O. The ozone molecule is unstable (although, in the stratosphere, long-lived) and when ultraviolet light hits ozone it splits into a molecule of O and an individual atom of oxygen, a continuing process called the ozone-oxygen cycle. Chemically, this can be described as:
About 90 percent of the ozone in the atmosphere is contained in the stratosphere. Ozone concentrations are greatest between about , where they range from about 2 to 8 parts per million. If all of the ozone were compressed to the pressure of the air at sea level, it would be only thick.

Although the concentration of the ozone in the ozone layer is very small, it is vitally important to life because it absorbs biologically harmful ultraviolet (UV) radiation coming from the sun. Extremely short or vacuum UV (10–100 nm) is screened out by nitrogen. UV radiation capable of penetrating nitrogen is divided into three categories, based on its wavelength; these are referred to as UV-A (400–315 nm), UV-B (315–280 nm), and UV-C (280–100 nm).

UV-C, which is very harmful to all living things, is entirely screened out by a combination of dioxygen (< 200 nm) and ozone (> about 200 nm) by around altitude. UV-B radiation can be harmful to the skin and is the main cause of sunburn; excessive exposure can also cause cataracts, immune system suppression, and genetic damage, resulting in problems such as skin cancer. The ozone layer (which absorbs from about 200 nm to 310 nm with a maximal absorption at about 250 nm) is very effective at screening out UV-B; for radiation with a wavelength of 290 nm, the intensity at the top of the atmosphere is 350 million times stronger than at the Earth's surface. Nevertheless, some UV-B, particularly at its longest wavelengths, reaches the surface, and is important for the skin's production of vitamin D.

Ozone is transparent to most UV-A, so most of this longer-wavelength UV radiation reaches the surface, and it constitutes most of the UV reaching the Earth. This type of UV radiation is significantly less harmful to DNA, although it may still potentially cause physical damage, premature aging of the skin, indirect genetic damage, and skin cancer.

The thickness of the ozone layer varies worldwide and is generally thinner near the equator and thicker near the poles. Thickness refers to how much ozone is in a column over a given area and varies from season to season. The reasons for these variations are due to atmospheric circulation patterns and solar intensity.

The majority of ozone is produced over the tropics and is transported towards the poles by stratospheric wind patterns. In the northern hemisphere these patterns, known as the Brewer-Dobson circulation, make the ozone layer thickest in the spring and thinnest in the fall. When ozone is produced by solar UV radiation in the tropics, it is done so by circulation lifting ozone-poor air out of the troposphere and into the stratosphere where the sun photolyzes oxygen molecules and turns them into ozone. Then, the ozone-rich air is carried to higher latitudes and drops into lower layers of the atmosphere.

Research has found that the ozone levels in the United States are highest in the spring months of April and May and lowest in October. While the total amount of ozone increases moving from the tropics to higher latitudes, the concentrations are greater in high northern latitudes than in high southern latitudes, due to the ozone hole phenomenon. The highest amounts of ozone are found over the Arctic during the spring months of March and April, but the Antarctic has their lowest amounts of ozone during their summer months of September and October,

The ozone layer can be depleted by free radical catalysts, including nitric oxide (NO), nitrous oxide (NO), hydroxyl (OH), atomic chlorine (Cl), and atomic bromine (Br). While there are natural sources for all of these species, the concentrations of chlorine and bromine increased markedly in recent decades because of the release of large quantities of man-made organohalogen compounds, especially chlorofluorocarbons (CFCs) and bromofluorocarbons. These highly stable compounds are capable of surviving the rise to the stratosphere, where Cl and Br radicals are liberated by the action of ultraviolet light. Each radical is then free to initiate and catalyze a chain reaction capable of breaking down over 100,000 ozone molecules. By 2009, nitrous oxide was the largest ozone-depleting substance (ODS) emitted through human activities.

The breakdown of ozone in the stratosphere results in reduced absorption of ultraviolet radiation. Consequently, unabsorbed and dangerous ultraviolet radiation is able to reach the Earth's surface at a higher intensity. Ozone levels have dropped by a worldwide average of about 4 percent since the late 1970s. For approximately 5 percent of the Earth's surface, around the north and south poles, much larger seasonal declines have been seen, and are described as "ozone holes". The discovery of the annual depletion of ozone above the Antarctic was first announced by Joe Farman, Brian Gardiner and Jonathan Shanklin, in a paper which appeared in "Nature" on May 16, 1985.

To support successful regulation attempts, the ozone case was communicated to lay persons "with easy-to-understand bridging metaphors derived from the popular culture" and related to "immediate risks with everyday relevance". The specific metaphors used in the discussion (ozone shield, ozone hole) proved quite useful and, compared to global climate change, the ozone case was much more seen as a "hot issue" and imminent risk. Lay people were cautious about a depletion of the ozone layer and the risks of skin cancer.

In 1978, the United States, Canada and Norway enacted bans on CFC-containing aerosol sprays that damage the ozone layer. The European Community rejected an analogous proposal to do the same. In the U.S., chlorofluorocarbons continued to be used in other applications, such as refrigeration and industrial cleaning, until after the discovery of the Antarctic ozone hole in 1985. After negotiation of an international treaty (the Montreal Protocol), CFC production was capped at 1986 levels with commitments to long-term reductions. This allowed for a ten-year phase-in for developing countries (identified in Article 5 of the protocol). Since that time, the treaty was amended to ban CFC production after 1995 in the developed countries, and later in developing countries. Today, all of the world's 197 countries have signed the treaty. Beginning January 1, 1996, only recycled and stockpiled CFCs were available for use in developed countries like the US. This production phaseout was possible because of efforts to ensure that there would be substitute chemicals and technologies for all ODS uses.

On August 2, 2003, scientists announced that the global depletion of the ozone layer may be slowing down because of the international regulation of ozone-depleting substances. In a study organized by the American Geophysical Union, three satellites and three ground stations confirmed that the upper-atmosphere ozone-depletion rate slowed significantly during the previous decade. Some breakdown can be expected to continue because of ODSs used by nations which have not banned them, and because of gases which are already in the stratosphere. Some ODSs, including CFCs, have very long atmospheric lifetimes, ranging from 50 to over 100 years. It has been estimated that the ozone layer will recover to 1980 levels near the middle of the 21st century. A gradual trend toward "healing" was reported in 2016.

Compounds containing C–H bonds (such as hydrochlorofluorocarbons, or HCFCs) have been designed to replace CFCs in certain applications. These replacement compounds are more reactive and less likely to survive long enough in the atmosphere to reach the stratosphere where they could affect the ozone layer. While being less damaging than CFCs, HCFCs can have a negative impact on the ozone layer, so they are also being phased out. These in turn are being replaced by hydrofluorocarbons (HFCs) and other compounds that do not destroy stratospheric ozone at all.

The residual effects of CFCs accumulating within the atmosphere lead to a concentration gradient between the atmosphere and the ocean. This organohalogen compound is able to dissolve into the ocean's surface waters and is able to act as a time-dependent tracer. This tracer helps scientists study ocean circulation by tracing biological, physical and chemical pathways 

As ozone in the atmosphere prevents most energetic ultraviolet radiation reaching the surface of the Earth, astronomical data in these wavelengths have to be gathered from satellites orbiting above the atmosphere and ozone layer. Most of the light from young hot stars is in the ultraviolet and so study of these wavelengths is important for studying the origins of galaxies. The Galaxy Evolution Explorer, GALEX, is an orbiting ultraviolet space telescope launched on April 28, 2003, which operated until early 2012.







</doc>
<doc id="22841" url="https://en.wikipedia.org/wiki?curid=22841" title="Public Enemy (band)">
Public Enemy (band)

Public Enemy is an American hip hop group consisting of Chuck D and DJ Lord. Formed in Long Island, New York, in 1985, they are known for their music with a heavy political message alongside strong criticism of the media of the United States, with many of their works also revolving around frustrations and concerns of the African American community.

Public Enemy's debut album, "Yo! Bum Rush the Show", was released in 1987 to critical acclaim and their second album, "It Takes a Nation of Millions to Hold Us Back" (1988), was the first hip hop album to top "The Village Voice"s Pazz & Jop critics' poll. Their next three albums, "Fear of a Black Planet" (1990), "Apocalypse 91... The Enemy Strikes Black" (1991) and "Muse Sick-n-Hour Mess Age" (1994), were also well received by music critics. Public Enemy has gone through lineup changes over the years, with Chuck D being the only constant member of the group. Professor Griff left in 1989 but rejoined in 1998, though he parted ways with the group again some years later. DJ Lord also joined Public Enemy in 1998 as the replacement of the group's original DJ Terminator X. Flavor Flav, one of the co-founders of Public Enemy, was a member of the group for three-and-a-half decades, until he was fired in 2020. This turned out to be a publicity stunt that was called an April Fools' Day prank. Public Enemy minus Flavor Flav also tour and record music under the name of Public Enemy Radio which consists of the lineup of Chuck D, Jahi, DJ Lord and the S1Ws.

Public Enemy's first four albums during the late 1980s and early 1990s were all certified either gold or platinum and were, according to music critic Robert Hilburn in 1998, "the most acclaimed body of work ever by a hip hop act". Critic Stephen Thomas Erlewine called them "the most influential and radical band of their time". They were inducted into Rock and Roll Hall of Fame in 2013. They were honored with the Grammy Lifetime Achievement Award at the 62nd Grammy Awards.

Public Enemy was formed in 1985 by Carlton Ridenhour (Chuck D) and William Drayton (Flavor Flav), who met at Long Island's Adelphi University in the mid-1980s. Developing his talents as an MC with Flav while delivering furniture for his father's business, Chuck D and Spectrum City, as the group was called, released the record "Check Out the Radio", backed by "Lies", a social commentary—both of which would influence RUSH Productions' Run–D.M.C. and Beastie Boys. Chuck D put out a tape to promote WBAU (the radio station where he was working at the time) and to fend off a local MC who wanted to battle him. He called the tape "Public Enemy #1" because he felt like he was being persecuted by people in the local scene. This was the first reference to the notion of a public enemy in any of Chuck D's songs. The single was created by Chuck D with a contribution by Flavor Flav, though this was before the group "Public Enemy" was officially assembled. Around 1986, Bill Stephney, the former Program Director at WBAU, was approached by Ali Hafezi and offered a position with the label. Stephney accepted, and his first assignment was to help fledgling producer Rick Rubin sign Chuck D, whose song "Public Enemy Number One" Rubin had heard from Andre "Doctor Dré" Brown.

According to the book "The History of Rap Music" by Cookie Lommel, "Stephney thought it was time to mesh the hard-hitting style of Run DMC with politics that addressed black youth. Chuck recruited Spectrum City, which included Hank Shocklee, his brother Keith Shocklee, and Eric "Vietnam" Sadler, collectively known as the Bomb Squad, to be his production team and added another Spectrum City partner, Professor Griff, to become the group's Minister of Information. With the addition of Flavor Flav and another local mobile DJ named Terminator X, the group Public Enemy was born." According to Chuck, The S1W, which stands for Security of the First World, "represents that the black man can be just as intelligent as he is strong. It stands for the fact that we're not third-world people, we're first-world people; we're the original people." Hank Shocklee came up with the name Public Enemy based on "underdog love and their developing politics" and the idea from Def Jam staffer Bill Stephney following the Howard Beach racial incident, Bernhard Goetz, and the death of Michael Stewart: "The Black man is definitely the public enemy."

Public Enemy started out as opening act for the Beastie Boys during the latter's "Licensed to Ill" popularity, and in 1987 released their debut album "Yo! Bum Rush the Show".

Their debut album, "Yo! Bum Rush the Show", was released in 1987 to critical acclaim. The album was the group's first step toward stardom. In October 1987, music critic Simon Reynolds dubbed Public Enemy "a superlative "rock" band". They released their second album "It Takes a Nation of Millions to Hold Us Back" in 1988, which performed better in the charts than their previous release, and included the hit single "Don't Believe the Hype" in addition to "Bring the Noise". "Nation of Millions ... " was the first hip hop album to be voted album of the year in "The Village Voice"s influential Pazz & Jop critics' poll.

In 1989, the group returned to the studio to record "Fear of a Black Planet", which continued their politically charged themes. The album was supposed to be released in late 1989, but was pushed back to April 1990. It was the most successful of any of their albums and, in 2005, was selected for preservation in the National Recording Registry. It included the singles "Welcome to the Terrordome", written after the band was criticized by Jews for Professor Griff's anti-semitic comments, "911 Is a Joke", which criticized emergency response units for taking longer to arrive at emergencies in the black community than those in the white community, and "Fight the Power". "Fight the Power" is regarded as one of the most popular and influential songs in hip hop history. It was the theme song of Spike Lee's "Do the Right Thing".

The group's next release, "Apocalypse 91... The Enemy Strikes Black", continued this trend, with songs like "Can't Truss It", which addressed the history of slavery and how the black community can fight back against oppression; "I Don't Wanna be Called Yo Nigga", a track that takes issue with the use of the word "nigga" outside of its original derogatory context. The album also included the controversial song and video "By the Time I Get to Arizona", which chronicled the black community's frustration that some US states did not recognize Martin Luther King Jr.'s birthday as a national holiday. The video featured members of Public Enemy taking out their frustrations on politicians in the states not recognizing the holiday. In 1992, the group was one of the first rap acts to perform at the Reading Festival in the UK, headlining the second day of the three-day festival.

After a 1994 motorcycle accident shattered his left leg and kept him in the hospital for a full month, Terminator X relocated to his 15-acre farm in Vance County, North Carolina. By 1998, he was ready to retire from the group and focus full-time on raising African black ostriches on his farm. In late 1998, the group started looking for Terminator X's permanent replacement. Following several months of searching for a DJ, Professor Griff saw DJ Lord at a Vestax Battle and approached him about becoming the DJ for Public Enemy. DJ Lord joined as the group's full-time DJ just in time for Public Enemy's 40th World Tour. Since 1999, he has been the official DJ for Public Enemy on albums and world tours while winning numerous turntablist competitions, including multiple DMC finals.

In 2007, the group released an album entitled "How You Sell Soul to a Soulless People Who Sold Their Soul?". Public Enemy's single from the album was "Harder Than You Think". Four years after "How You Sell Soul ... ", in January 2011, Public Enemy released the album "Beats and Places", a compilation of remixes and "lost" tracks. On July 13, 2012, "Most of My Heroes Still Don't Appear on No Stamp" was released and was exclusively available on iTunes. In July 2012, on UK television an advert for the London 2012 Summer Paralympics featured a short remix of the song "Harder Than You Think". The advert caused the song to reach No. 4 in the UK Singles Chart on September 2, 2012. On July 30, 2012, Public Enemy performed a free concert with Salt-N-Pepa and Kid 'n Play at Wingate Park in Brooklyn, New York as part of the Martin Luther King Jr. Concert Series. On August 26, 2012, Public Enemy performed at South West Four music festival in Clapham Common in London. On October 1, 2012 "The Evil Empire of Everything" was released. On June 29, 2013, they performed at Glastonbury Festival 2013. On September 14, 2013 they performed at Riot Fest & Carnival 2013 in Chicago, Illinois. On September 20, 2013 they performed at Riot Fest & Side Show in Byers, Colorado.

In 2014 Chuck D launched PE 2.0 with Oakland rapper Jahi as a spiritual successor and "next generation" of Public Enemy. Jahi met Chuck D backstage during a soundcheck at the 1999 Rock & Roll Hall of Fame and later appeared as a support act on Public Enemy's 20th Anniversary Tour in 2007. PE 2.0's task is twofold, Jahi says, to "take select songs from the PE catalog and cover or revisit them" as well as new material with members of the original Public Enemy including DJ Lord, Davy DMX, Professor Griff and Chuck D. PE 2.0's first album "People Get Ready" was released on October 7, 2014. "InsPirEd" PE 2.0's second album and part two of a proposed trilogy was released a year later on October 11, 2015.
"Man Plans God Laughs", Public Enemy's thirteenth album, was released in July 2015. On June 29, 2017, Public Enemy released their fourteenth album, "Nothing Is Quick in the Desert". The album was available for free download through Bandcamp until July 4, 2017.

In late February 2020 it was announced that Public Enemy (billed as Public Enemy Radio) would perform at a campaign rally in Los Angeles, CA on March 1, 2020 for Bernie Sanders, who was campaigning to be the nominee of the Democratic Party in the 2020 presidential election. Days following the announcement, Flavor Flav took issue toward the group being associated with the Sanders campaign and issued a cease and desist letter asking the campaign to not use the group's name or logo. "While Chuck is certainly free to express his political views as he sees fit -- his voice alone does not speak for Public Enemy. The planned performance will only be Chuck D of Public Enemy, it will not be a performance by Public Enemy. Those who truly know what Public Enemy stands for know what time it is, there is no Public Enemy without Flavor Flav." Flavor Flav's statement read. Chuck D responded to the statement by saying "Flavor chooses to dance for his money and not do benevolent work like this. He has a year to get his act together and get himself straight or he’s out.” A lawyer for Chuck D added "Chuck could perform as Public Enemy if he ever wanted to; he is the sole owner of the Public Enemy trademark. He originally drew the logo himself in the mid-80’s, is also the creative visionary and the group’s primary songwriter, having written Flavor’s most memorable lines.” Prior to the group's performance at the Sanders rally, Chuck D issued a statement saying Flavor Flav had been fired from the group. "Public Enemy and Public Enemy Radio will be moving forward without Flavor Flav. We thank him for his years of service and wish him well." According to reports, Chuck D and Flavor Flav had been at odds for a while. In 2017, Flavor Flav sued Chuck D over claims his earnings from Public Enemy “diminished to almost nothing". Flavor Flav issued a statement shortly before his firing saying "I don’t want our family and our movement broken up. I am a little worried about my partner Chuck, I hope he is ok and that Public Enemy can get back to doing the good works we have done for 30 years…not for money but for people like me who have been denied their rights to participate because of bullshit policies. I have nothing personal against Bernie but I have issues with how he and his people have handled this". Following his firing his lawyer released statement taking shots at Chuck D and claiming that "masses of clock wearing fans" left the Sanders rally when Public Enemy Radio performed.

Following the news of Flavor Flav being fired, on March 2, 2021, it was announced that the group would be releasing their first album under the Public Enemy Radio name titled "Loud Is Not Enough" which is due for release in April 2020. The album will feature the lineup of Chuck D, DJ Lord, Jahi and the S1Ws and according to a statement from the group it will be “taking it back to hip hop’s original DJ-and-turntablist foundation.”

A Reuters report dated April 1, 2020 claimed that the breakup was a hoax to generate publicity and provide a commentary on disinformation. According to the report, Chuck D said "he and Flav concocted a fake split to grab attention and highlight media bias towards reporting bad news about hip hop." However, Flavor Flav denied these claims, writing on his Twitter, "I am not a part of your hoax"; referring to the COVID-19 pandemic, he continued, "There are more serious things in the world right now than April Fool's jokes and dropping records. The world needs better than this." Flav finished his Twitter post with the sentence "you say we are leaders so act like one."

On June 19, 2020, Public Enemy (with Flavor Flav), released the single and music video for their anti-Donald Trump song “State of the Union (STFU)”. “Our collective voices keep getting louder. The rest of the planet is on our side. But it’s not enough to talk about change. You have to show up and demand change. Folks gotta vote like their lives depend on it, cause it does.” Chuck D said.

Public Enemy made contributions to the hip-hop world with sonic experimentation as well as political and cultural consciousness, which infused itself into skilled and poetic rhymes. Critic Stephen Thomas Erlewine wrote that "PE brought in elements of free jazz, hard funk, even musique concrète, via [its] producing team the Bomb Squad, creating a dense, ferocious sound unlike anything that came before." Terminator X's innovative scratching tricks can be heard on the songs "Rebel Without a Pause", "Night of the Living Baseheads", and "Shut 'Em Down".

Public Enemy held a strong, pro-black, political stance. Before PE, politically motivated hip-hop was defined by a few tracks by Ice-T, Grandmaster Flash and the Furious Five, Kurtis Blow and Boogie Down Productions. Other politically motivated opinions were shared by prototypical artists Gil Scott-Heron and the Last Poets. PE was a revolutionary hip-hop act whose entire image rested on a specified political stance. With the successes of Public Enemy, many hip-hop artists began to celebrate Afrocentric themes, such as Kool Moe Dee, Gang Starr, X Clan, Eric B. & Rakim, Queen Latifah, the Jungle Brothers, and A Tribe Called Quest.

Public Enemy was one of the first hip-hop groups to do well internationally. PE changed the Internet's music distribution capability by being one of the first groups to release MP3-only albums, a format virtually unknown at the time.

Public Enemy helped to create and define "rap metal" by collaborating with Living Colour in 1988 ("Funny Vibe"), with Sonic Youth on the 1990 song "Kool Thing", and with New York thrash metal outfit Anthrax in 1991. The single "Bring the Noise" was a mix of semi-militant black power lyrics, grinding guitars, and sporadic humor. The two bands, cemented by a mutual respect and the personal friendship between Chuck D and Anthrax's Scott Ian, introduced a hitherto alien genre to rock fans, and the two seemingly disparate groups toured together. Flavor Flav's pronouncement on stage that "They said this tour would never happen" (as heard on Anthrax's "" CD) has become a legendary comment in both rock and hip-hop circles. Metal guitarist Vernon Reid (of Living Colour) contributed to Public Enemy's recordings, and PE sampled Slayer's "Angel of Death" half-time riff on "She Watch Channel Zero?!"

Members of the Bomb Squad produced or remixed works for other acts, like Bell Biv DeVoe, Ice Cube, Vanessa Williams, Sinéad O'Connor, Blue Magic, Peter Gabriel, L.L. Cool J, Paula Abdul, Jasmine Guy, Jody Watley, Eric B & Rakim, Third Bass, Big Daddy Kane, EPMD, and Chaka Khan. According to Chuck D, "We had tight dealings with MCA Records and were talking about taking three guys that were left over from New Edition and coming up with an album for them. The three happened to be Ricky Bell, Michael Bivins, and Ronnie DeVoe, later to become Bell Biv DeVoe. Ralph Tresvant had been slated to do a solo album for years, Bobby Brown had left New Edition and experienced some solo success beginning in 1988, and Johnny Gill had just been recruited to come in, but [he] had come off a solo career and could always go back to that. At MCA, Hiram Hicks, who was their manager, and Louil Silas, who was running the show, were like, 'Yo, these kids were left out in the cold. Can y'all come up with something for them?' It was a task that Hank, Keith, Eric, and I took on to try to put some kind of hip-hop-flavored R&B shit down for them. Subsequently, what happened in the four weeks of December [1989] was that the Bomb Squad knocked out a large piece of the production and arrangement on Bell Biv DeVoe's three-million selling album "Poison". In January [1990], they knocked out "Fear of a Black Planet" in four weeks, and PE knocked out Ice Cube's album "AmeriKKKa's Most Wanted" in four to five weeks in February." They have also produced local talent such as Son of Bazerk, Young Black Teenagers, Kings of Pressure, and True Mathematics—and gave producer Kip Collins his start in the business.

Poet and hip-hop artist Saul Williams uses a sample from Public Enemy's "Welcome to the Terrordome" in his song "Tr[n]igger" on the "Niggy Tardust" album. He also used a line from the song in his poem, "amethyst rocks".

The Manic Street Preachers track "Repeat (Stars And Stripes)" is a remix of the band's own anti-monarchy tirade by Public Enemy production team The Bomb Squad of whom James Dean Bradfield and Richey Edwards were big fans. The song samples "Countdown to Armageddon" from "It Takes a Nation of Millions to Hold Us Back". The band had previously sampled Public Enemy on their 1991 single Motown Junk.

The revolutionary influence of the band is seen throughout hip-hop and is recognized in society and politics. The band "rewrote the rules of hip-hop", changing the image, sound and message forever. Pro-black lyrics brought political and social themes to hardcore hip hop, with stirring ideas of racial equality, and retribution against police brutality, aimed at disenfranchised blacks, but appealing to all the poor and underrepresented. Before Public Enemy, hip hop music was seen as "throwaway entertainment", with trite sexist and homophobic lyrics. Public Enemy brought social relevance and strength to hip hop. They also brought black activist Louis Farrakhan to greater popularity, and they gave impetus to the Million Man March in 1995.

The influence of the band goes also beyond hip-hop in a unique way, indeed the group was cited as an influence by artists as diverse as Autechre (selected in the All Tomorrow's Parties in 2003), Nirvana (It Takes a Nation of Millions to Hold Us Back being cited by Kurt Cobain among his favorite albums), Nine Inch Nails (mentioned the band in Pretty Hate Machine credits), Björk (included Rebel Without a Pause in her The Breezeblock Mix in July 2007), Tricky (did a cover of Black Steel in the Hour of Chaos and appears in Do You Wanna Go Our Way ??? video), The Prodigy (included Public Enemy No. 1 in The Dirtchamber Sessions Volume One), Ben Harper, Underground Resistance (cited by both Mad Mike and Jeff Mills), Orlando Voorn, M.I.A., Amon Tobin, Mathew Jonson, Aphex Twin (Welcome To The Terrordome being the first track played after the introduction at the Coachella festival in April 2008), Rage Against the Machine (sampling the track in their song "Renegades of Funk") and My Bloody Valentine who was influenced by the Bomb Squad's production for their sound.

The 1991 song "By the Time I Get to Arizona" from "Apocalypse 91... The Enemy Strikes Black" referenced the controversy a year earlier when Arizona cancelled a state holiday for Martin Luther King Jr., and the NFL switched Super Bowl XXVII from Arizona to California, costing the state an estimated loss of over $100 million. A video of "By the Time I Get to Arizona", which was shown only once on MTV, depicted Chuck D killing Arizona officials with machine guns and a car bomb. This violent behaviour attracted negative media attention, and was described by one newspaper columnist as being the opposite of what King died for.

In 1989, in an interview with Public Enemy for the "Washington Times", the interviewing journalist, David Mills, lifted some quotations from a UK magazine in which the band were asked their opinion on the Arab–Israeli conflict. Professor Griff's comments apparently sympathized with the Palestinians and he was accused of anti-Semitism. According to Rap Attack 2, he suggested that "Jews are responsible for the majority of the wickedness in the world" (p. 177). (In turn a quote from" The International Jew") Shortly after, Ridenhour expressed an apology on his behalf. At a June 21, 1989, press conference, Ridenhour announced Griff's dismissal from the group, and a June 28 statement by Russell Simmons, president of Def Jam Recordings and Rush Artists Management, stated that Chuck D. had disbanded Public Enemy "for an indefinite period of time". By August 10, however, Ridenhour denied that he had disbanded the group, and stated that Griff had been re-hired as "Supreme Allied Chief of Community Relations" (in contrast to his previous position with the group as Minister of Information). Griff later denied holding anti-Semitic views and apologized for the remarks. Several people who had worked with Public Enemy expressed concern about Ridenhour's leadership abilities and role as a social spokesman.

In his 2009 book, entitled "Analytixz", Griff criticized his 1989 statement: "to say the Jews are responsible for the majority of wickedness that went on around the globe I would have to know about the majority of wickedness that went on around the globe, which is impossible ... I'm not the best knower. Then, not only knowing that, I would have to know who is at the crux of all of the problems in the world and then blame Jewish people, which is not correct." Griff also said that not only were his words taken out of context, but that the recording has never been released to the public for an unbiased listen.

The controversy and apologies on behalf of Griff spurred Chuck D to reference the negative press they were receiving. In 1990, Public Enemy issued the single "Welcome to the Terrordome", which contains the lyrics: "Crucifixion ain't no fiction / So-called chosen frozen / Apologies made to whoever pleases / Still they got me like Jesus". These lyrics have been cited by some in the media as anti-Semitic, making supposed references to the concept of the "chosen people" with the lyric "so-called chosen" and Jewish deicide with the last line.

In 1999 the group released an album entitled "There's a Poison Goin' On". The title of the last song on the album is called "Swindler's Lust". The Anti-Defamation League (ADL) claimed that the title of the song was a word play on the title of the Steven Spielberg movie "Schindler's List" about the genocide of Jews in World War II. Similarly in 2000 a Public Enemy spin off group under the name Confrontation Camp, a name according to the ADL, that is a pun on the term concentration camp, released an album. The group consisted of Kyle Jason, Chuck D (under the name Mistachuck) and Professor Griff.

In a letter to the editor, Leo Haber alludes to criticism by "The New York Times" writer Peter Watrous of the group's supposed homophobia.

Zoe Williams defended Public Enemy against charges of homophobia by stating that:






Grammy Awards

American Music Awards
Rock and Roll Hall of Fame

Public Enemy was inducted into the Rock and Roll Hall of Fame in 2013.




</doc>
<doc id="22860" url="https://en.wikipedia.org/wiki?curid=22860" title="Paleolithic">
Paleolithic

The Paleolithic or Palaeolithic or Palæolithic (), also called the Old Stone Age, is a period in human prehistory distinguished by the original development of stone tools that covers  99% of the time period of human technological prehistory. It extends from the earliest known use of stone tools by hominins  3.3 million years ago, to the end of the Pleistocene  11,650 cal BP.

The Paleolithic Age in Europe preceded the Mesolithic Age, although the date of the transition varies geographically by several thousand years. During the Paleolithic Age, hominins grouped together in small societies such as bands and subsisted by gathering plants, fishing, and hunting or scavenging wild animals. The Paleolithic Age is characterized by the use of knapped stone tools, although at the time humans also used wood and bone tools. Other organic commodities were adapted for use as tools, including leather and vegetable fibers; however, due to rapid decomposition, these have not survived to any great degree.

About 50,000 years ago a marked increase in the diversity of artifacts occurred. In Africa, bone artifacts and the first art appear in the archaeological record. The first evidence of human fishing is also noted, from artifacts in places such as Blombos cave in South Africa. Archaeologists classify artifacts of the last 50,000 years into many different categories, such as projectile points, engraving tools, knife blades, and drilling and piercing tools.

Humankind gradually evolved from early members of the genus "Homo"—such as "Homo habilis", who used simple stone tools—into anatomically modern humans as well as behaviourally modern humans by the Upper Paleolithic. During the end of the Paleolithic Age, specifically the Middle or Upper Paleolithic Age, humans began to produce the earliest works of art and to engage in religious or spiritual behavior such as burial and ritual. Conditions during the Paleolithic Age went through a set of glacial and interglacial periods in which the climate periodically fluctuated between warm and cool temperatures. Archaeological and genetic data suggest that the source populations of Paleolithic humans survived in sparsely-wooded areas and dispersed through areas of high primary productivity while avoiding dense forest-cover.

By  BP, the first humans set foot in Australia. By  BP, humans lived at 61°N latitude in Europe. By  BP, Japan was reached, and by  BP humans were present in Siberia, above the Arctic Circle. At the end of the Upper Paleolithic Age a group of humans crossed Beringia and quickly expanded throughout the Americas.

The term "Palaeolithic" was coined by archaeologist John Lubbock in 1865. It derives from Greek: παλαιός, "palaios", "old"; and λίθος, "lithos", "stone", meaning "old age of the stone" or "Old Stone Age".

The Paleolithic coincides almost exactly with the Pleistocene epoch of geologic time, which lasted from 2.6 million years ago to about 12,000 years ago. This epoch experienced important geographic and climatic changes that affected human societies.

During the preceding Pliocene, continents had continued to drift from possibly as far as from their present locations to positions only from their current location. South America became linked to North America through the Isthmus of Panama, bringing a nearly complete end to South America's distinctive marsupial fauna. The formation of the isthmus had major consequences on global temperatures, because warm equatorial ocean currents were cut off, and the cold Arctic and Antarctic waters lowered temperatures in the now-isolated Atlantic Ocean.

Most of Central America formed during the Pliocene to connect the continents of North and South America, allowing fauna from these continents to leave their native habitats and colonize new areas. Africa's collision with Asia created the Mediterranean, cutting off the remnants of the Tethys Ocean. During the Pleistocene, the modern continents were essentially at their present positions; the tectonic plates on which they sit have probably moved at most from each other since the beginning of the period.

Climates during the Pliocene became cooler and drier, and seasonal, similar to modern climates. Ice sheets grew on Antarctica. The formation of an Arctic ice cap around 3 million years ago is signaled by an abrupt shift in oxygen isotope ratios and ice-rafted cobbles in the North Atlantic and North Pacific Ocean beds. Mid-latitude glaciation probably began before the end of the epoch. The global cooling that occurred during the Pliocene may have spurred on the disappearance of forests and the spread of grasslands and savannas.
The Pleistocene climate was characterized by repeated glacial cycles during which continental glaciers pushed to the 40th parallel in some places. Four major glacial events have been identified, as well as many minor intervening events. A major event is a general glacial excursion, termed a "glacial". Glacials are separated by "interglacials". During a glacial, the glacier experiences minor advances and retreats. The minor excursion is a "stadial"; times between stadials are "interstadials". Each glacial advance tied up huge volumes of water in continental ice sheets deep, resulting in temporary sea level drops of or more over the entire surface of the Earth. During interglacial times, such as at present, drowned coastlines were common, mitigated by isostatic or other emergent motion of some regions.

The effects of glaciation were global. Antarctica was ice-bound throughout the Pleistocene and the preceding Pliocene. The Andes were covered in the south by the Patagonian ice cap. There were glaciers in New Zealand and Tasmania. The now decaying glaciers of Mount Kenya, Mount Kilimanjaro, and the Ruwenzori Range in east and central Africa were larger. Glaciers existed in the mountains of Ethiopia and to the west in the Atlas mountains. In the northern hemisphere, many glaciers fused into one. The Cordilleran Ice Sheet covered the North American northwest; the Laurentide covered the east. The Fenno-Scandian ice sheet covered northern Europe, including Great Britain; the Alpine ice sheet covered the Alps. Scattered domes stretched across Siberia and the Arctic shelf. The northern seas were frozen. During the late Upper Paleolithic (Latest Pleistocene)  BP, the Beringia land bridge between Asia and North America was blocked by ice, which may have prevented early Paleo-Indians such as the Clovis culture from directly crossing Beringia to reach the Americas.

According to Mark Lynas (through collected data), the Pleistocene's overall climate could be characterized as a continuous El Niño with trade winds in the south Pacific weakening or heading east, warm air rising near Peru, warm water spreading from the west Pacific and the Indian Ocean to the east Pacific, and other El Niño markers.

The Paleolithic is often held to finish at the end of the ice age (the end of the Pleistocene epoch), and Earth's climate became warmer. This may have caused or contributed to the extinction of the Pleistocene megafauna, although it is also possible that the late Pleistocene extinctions were (at least in part) caused by other factors such as disease and overhunting by humans. New research suggests that the extinction of the woolly mammoth may have been caused by the combined effect of climatic change and human hunting. Scientists suggest that climate change during the end of the Pleistocene caused the mammoths' habitat to shrink in size, resulting in a drop in population. The small populations were then hunted out by Paleolithic humans. The global warming that occurred during the end of the Pleistocene and the beginning of the Holocene may have made it easier for humans to reach mammoth habitats that were previously frozen and inaccessible. Small populations of woolly mammoths survived on isolated Arctic islands, Saint Paul Island and Wrangel Island, until  BP and  BP respectively. The Wrangel Island population became extinct around the same time the island was settled by prehistoric humans. There is no evidence of prehistoric human presence on Saint Paul island (though early human settlements dating as far back as 6500 BP were found on the nearby Aleutian Islands).

Nearly all of our knowledge of Paleolithic human culture and way of life comes from archaeology and ethnographic comparisons to modern hunter-gatherer cultures such as the !Kung San who live similarly to their Paleolithic predecessors. The economy of a typical Paleolithic society was a hunter-gatherer economy. Humans hunted wild animals for meat and gathered food, firewood, and materials for their tools, clothes, or shelters.

Human population density was very low, around only one person per square mile. This was most likely due to low body fat, infanticide, women regularly engaging in intense endurance exercise, late weaning of infants, and a nomadic lifestyle. Like contemporary hunter-gatherers, Paleolithic humans enjoyed an abundance of leisure time unparalleled in both Neolithic farming societies and modern industrial societies. At the end of the Paleolithic, specifically the Middle or Upper Paleolithic, humans began to produce works of art such as cave paintings, rock art and jewellery and began to engage in religious behavior such as burial and ritual.

At the beginning of the Paleolithic, hominins were found primarily in eastern Africa, east of the Great Rift Valley. Most known hominin fossils dating earlier than one million years before present are found in this area, particularly in Kenya, Tanzania, and Ethiopia.

By  BP, groups of hominins began leaving Africa and settling southern Europe and Asia. Southern Caucasus was occupied by  BP, and northern China was reached by  BP. By the end of the Lower Paleolithic, members of the hominin family were living in what is now China, western Indonesia, and, in Europe, around the Mediterranean and as far north as England, France, southern Germany, and Bulgaria. Their further northward expansion may have been limited by the lack of control of fire: studies of cave settlements in Europe indicate no regular use of fire prior to  BP.

East Asian fossils from this period are typically placed in the genus "Homo erectus". Very little fossil evidence is available at known Lower Paleolithic sites in Europe, but it is believed that hominins who inhabited these sites were likewise "Homo erectus". There is no evidence of hominins in America, Australia, or almost anywhere in Oceania during this time period.

Fates of these early colonists, and their relationships to modern humans, are still subject to debate. According to current archaeological and genetic models, there were at least two notable expansion events subsequent to peopling of Eurasia  BP. Around 500,000 BP a group of early humans, frequently called "Homo heidelbergensis", came to Europe from Africa and eventually evolved into "Homo neanderthalensis" (Neanderthals). In the Middle Paleolithic, Neanderthals were present in the region now occupied by Poland.

Both "Homo erectus" and "Homo neanderthalensis" became extinct by the end of the Paleolithic. Descended from "Homo sapiens", the anatomically modern "Homo sapiens sapiens" emerged in eastern Africa  BP, left Africa around 50,000 BP, and expanded throughout the planet. Multiple hominid groups coexisted for some time in certain locations. "Homo neanderthalensis" were still found in parts of Eurasia  BP years, and engaged in an unknown degree of interbreeding with "Homo sapiens sapiens". DNA studies also suggest an unknown degree of interbreeding between "Homo sapiens sapiens" and "Homo sapiens denisova".

Hominin fossils not belonging either to "Homo neanderthalensis" or to "Homo sapiens" species, found in the Altai Mountains and Indonesia, were radiocarbon dated to  BP and  BP respectively.

For the duration of the Paleolithic, human populations remained low, especially outside the equatorial region. The entire population of Europe between 16,000 and 11,000 BP likely averaged some 30,000 individuals, and between 40,000 and 16,000 BP, it was even lower at 4,000–6,000 individuals.

Paleolithic humans made tools of stone, bone, and wood. The early paleolithic hominins, "Australopithecus", were the first users of stone tools. Excavations in Gona, Ethiopia have produced thousands of artifacts, and through radioisotopic dating and magnetostratigraphy, the sites can be firmly dated to 2.6 million years ago. Evidence shows these early hominins intentionally selected raw materials with good flaking qualities and chose appropriate sized stones for their needs to produce sharp-edged tools for cutting.

The earliest Paleolithic stone tool industry, the Oldowan, began around 2.6 million years ago. It contained tools such as choppers, burins, and stitching awls. It was completely replaced around 250,000 years ago by the more complex Acheulean industry, which was first conceived by "Homo ergaster" around 1.8–1.65 million years ago. The Acheulean implements completely vanish from the archaeological record around 100,000 years ago and were replaced by more complex Middle Paleolithic tool kits such as the Mousterian and the Aterian industries.

Lower Paleolithic humans used a variety of stone tools, including hand axes and choppers. Although they appear to have used hand axes often, there is disagreement about their use. Interpretations range from cutting and chopping tools, to digging implements, to flaking cores, to the use in traps, and as a purely ritual significance, perhaps in courting behavior. William H. Calvin has suggested that some hand axes could have served as "killer Frisbees" meant to be thrown at a herd of animals at a waterhole so as to stun one of them. There are no indications of hafting, and some artifacts are far too large for that. Thus, a thrown hand axe would not usually have penetrated deeply enough to cause very serious injuries. Nevertheless, it could have been an effective weapon for defense against predators. Choppers and scrapers were likely used for skinning and butchering scavenged animals and sharp-ended sticks were often obtained for digging up edible roots. Presumably, early humans used wooden spears as early as 5 million years ago to hunt small animals, much as their relatives, chimpanzees, have been observed to do in Senegal, Africa. Lower Paleolithic humans constructed shelters, such as the possible wood hut at Terra Amata.

Fire was used by the Lower Paleolithic hominins "Homo erectus" and "Homo ergaster" as early as 300,000 to 1.5 million years ago and possibly even earlier by the early Lower Paleolithic (Oldowan) hominin "Homo habilis" or by robust "Australopithecines" such as "Paranthropus". However, the use of fire only became common in the societies of the following Middle Stone Age and Middle Paleolithic. Use of fire reduced mortality rates and provided protection against predators. Early hominins may have begun to cook their food as early as the Lower Paleolithic ( million years ago) or at the latest in the early Middle Paleolithic ( years ago). Some scientists have hypothesized that hominins began cooking food to defrost frozen meat, which would help ensure their survival in cold regions.

The Lower Paleolithic "Homo erectus" possibly invented rafts ( BP) to travel over large bodies of water, which may have allowed a group of "Homo erectus" to reach the island of Flores and evolve into the small hominin "Homo floresiensis". However, this hypothesis is disputed within the anthropological community. The possible use of rafts during the Lower Paleolithic may indicate that Lower Paleolithic hominins such as "Homo erectus" were more advanced than previously believed, and may have even spoken an early form of modern language. Supplementary evidence from Neanderthal and modern human sites located around the Mediterranean Sea, such as Coa de sa Multa ( BP), has also indicated that both Middle and Upper Paleolithic humans used rafts to travel over large bodies of water (i.e. the Mediterranean Sea) for the purpose of colonizing other bodies of land.

By around 200,000 BP, Middle Paleolithic stone tool manufacturing spawned a tool making technique known as the prepared-core technique, that was more elaborate than previous Acheulean techniques. This technique increased efficiency by allowing the creation of more controlled and consistent flakes. It allowed Middle Paleolithic humans to create stone tipped spears, which were the earliest composite tools, by hafting sharp, pointy stone flakes onto wooden shafts. In addition to improving tool making methods, the Middle Paleolithic also saw an improvement of the tools themselves that allowed access to a wider variety and amount of food sources. For example, microliths or small stone tools or points were invented around 70,000–65,000 BP and were essential to the invention of bows and spear throwers in the following Upper Paleolithic.

Harpoons were invented and used for the first time during the late Middle Paleolithic ( BP); the invention of these devices brought fish into the human diets, which provided a hedge against starvation and a more abundant food supply. Thanks to their technology and their advanced social structures, Paleolithic groups such as the Neanderthals—who had a Middle Paleolithic level of technology—appear to have hunted large game just as well as Upper Paleolithic modern humans. and the Neanderthals in particular may have likewise hunted with projectile weapons. Nonetheless, Neanderthal use of projectile weapons in hunting occurred very rarely (or perhaps never) and the Neanderthals hunted large game animals mostly by ambushing them and attacking them with mêlée weapons such as thrusting spears rather than attacking them from a distance with projectile weapons.

During the Upper Paleolithic, further inventions were made, such as the net or  BP) bolas, the spear thrower ( BP), the bow and arrow ( or  BP) and the oldest example of ceramic art, the Venus of Dolní Věstonice ( BP). Early dogs were domesticated, sometime between 30,000 and 14,000 BP, presumably to aid in hunting. However, the earliest instances of successful domestication of dogs may be much more ancient than this. Evidence from canine DNA collected by Robert K. Wayne suggests that dogs may have been first domesticated in the late Middle Paleolithic around 100,000 BP or perhaps even earlier.

Archaeological evidence from the Dordogne region of France demonstrates that members of the European early Upper Paleolithic culture known as the Aurignacian used calendars ( BP). This was a lunar calendar that was used to document the phases of the moon. Genuine solar calendars did not appear until the Neolithic. Upper Paleolithic cultures were probably able to time the migration of game animals such as wild horses and deer. This ability allowed humans to become efficient hunters and to exploit a wide variety of game animals. Recent research indicates that the Neanderthals timed their hunts and the migrations of game animals long before the beginning of the Upper Paleolithic.

The social organization of the earliest Paleolithic (Lower Paleolithic) societies remains largely unknown to scientists, though Lower Paleolithic hominins such as "Homo habilis" and "Homo erectus" are likely to have had more complex social structures than chimpanzee societies. Late Oldowan/Early Acheulean humans such as "Homo ergaster"/"Homo erectus" may have been the first people to invent central campsites or home bases and incorporate them into their foraging and hunting strategies like contemporary hunter-gatherers, possibly as early as 1.7 million years ago; however, the earliest solid evidence for the existence of home bases or central campsites (hearths and shelters) among humans only dates back to 500,000 years ago.

Similarly, scientists disagree whether Lower Paleolithic humans were largely monogamous or polygynous. In particular, the Provisional model suggests that bipedalism arose in pre-Paleolithic australopithecine societies as an adaptation to monogamous lifestyles; however, other researchers note that sexual dimorphism is more pronounced in Lower Paleolithic humans such as "Homo erectus" than in modern humans, who are less polygynous than other primates, which suggests that Lower Paleolithic humans had a largely polygynous lifestyle, because species that have the most pronounced sexual dimorphism tend more likely to be polygynous.

Human societies from the Paleolithic to the early Neolithic farming tribes lived without states and organized governments. For most of the Lower Paleolithic, human societies were possibly more hierarchical than their Middle and Upper Paleolithic descendants, and probably were not grouped into bands, though during the end of the Lower Paleolithic, the latest populations of the hominin "Homo erectus" may have begun living in small-scale (possibly egalitarian) bands similar to both Middle and Upper Paleolithic societies and modern hunter-gatherers.

Middle Paleolithic societies, unlike Lower Paleolithic and early Neolithic ones, consisted of bands that ranged from 20–30 or 25–100 members and were usually nomadic. These bands were formed by several families. Bands sometimes joined together into larger "macrobands" for activities such as acquiring mates and celebrations or where resources were abundant. By the end of the Paleolithic era ( BP), people began to settle down into permanent locations, and began to rely on agriculture for sustenance in many locations. Much evidence exists that humans took part in long-distance trade between bands for rare commodities (such as ochre, which was often used for religious purposes such as ritual) and raw materials, as early as 120,000 years ago in Middle Paleolithic. Inter-band trade may have appeared during the Middle Paleolithic because trade between bands would have helped ensure their survival by allowing them to exchange resources and commodities such as raw materials during times of relative scarcity (i.e. famine, drought). Like in modern hunter-gatherer societies, individuals in Paleolithic societies may have been subordinate to the band as a whole. Both Neanderthals and modern humans took care of the elderly members of their societies during the Middle and Upper Paleolithic.

Some sources claim that most Middle and Upper Paleolithic societies were possibly fundamentally egalitarian and may have rarely or never engaged in organized violence between groups (i.e. war).
Some Upper Paleolithic societies in resource-rich environments (such as societies in Sungir, in what is now Russia) may have had more complex and hierarchical organization (such as tribes with a pronounced hierarchy and a somewhat formal division of labor) and may have engaged in endemic warfare. Some argue that there was no formal leadership during the Middle and Upper Paleolithic. Like contemporary egalitarian hunter-gatherers such as the Mbuti pygmies, societies may have made decisions by communal consensus decision making rather than by appointing permanent rulers such as chiefs and monarchs. Nor was there a formal division of labor during the Paleolithic. Each member of the group was skilled at all tasks essential to survival, regardless of individual abilities. Theories to explain the apparent egalitarianism have arisen, notably the Marxist concept of primitive communism. Christopher Boehm (1999) has hypothesized that egalitarianism may have evolved in Paleolithic societies because of a need to distribute resources such as food and meat equally to avoid famine and ensure a stable food supply. Raymond C. Kelly speculates that the relative peacefulness of Middle and Upper Paleolithic societies resulted from a low population density, cooperative relationships between groups such as reciprocal exchange of commodities and collaboration on hunting expeditions, and because the invention of projectile weapons such as throwing spears provided less incentive for war, because they increased the damage done to the attacker and decreased the relative amount of territory attackers could gain. However, other sources claim that most Paleolithic groups may have been larger, more complex, sedentary and warlike than most contemporary hunter-gatherer societies, due to occupying more resource-abundant areas than most modern hunter-gatherers who have been pushed into more marginal habitats by agricultural societies.

Anthropologists have typically assumed that in Paleolithic societies, women were responsible for gathering wild plants and firewood, and men were responsible for hunting and scavenging dead animals. However, analogies to existent hunter-gatherer societies such as the Hadza people and the Aboriginal Australians suggest that the sexual division of labor in the Paleolithic was relatively flexible. Men may have participated in gathering plants, firewood and insects, and women may have procured small game animals for consumption and assisted men in driving herds of large game animals (such as woolly mammoths and deer) off cliffs. Additionally, recent research by anthropologist and archaeologist Steven Kuhn from the University of Arizona is argued to support that this division of labor did not exist prior to the Upper Paleolithic and was invented relatively recently in human pre-history.<ref name="NG2006/12/061207"></ref> Sexual division of labor may have been developed to allow humans to acquire food and other resources more efficiently. Possibly there was approximate parity between men and women during the Middle and Upper Paleolithic, and that period may have been the most gender-equal time in human history. Archaeological evidence from art and funerary rituals indicates that a number of individual women enjoyed seemingly high status in their communities, and it is likely that both sexes participated in decision making. The earliest known Paleolithic shaman ( BP) was female. Jared Diamond suggests that the status of women declined with the adoption of agriculture because women in farming societies typically have more pregnancies and are expected to do more demanding work than women in hunter-gatherer societies. Like most contemporary hunter-gatherer societies, Paleolithic and the Mesolithic groups probably followed mostly matrilineal and ambilineal descent patterns; patrilineal descent patterns were probably rarer than in the Neolithic.

Early examples of artistic expression, such as the Venus of Tan-Tan and the patterns found on elephant bones from Bilzingsleben in Thuringia, may have been produced by Acheulean tool users such as "Homo erectus" prior to the start of the Middle Paleolithic period. However, the earliest undisputed evidence of art during the Paleolithic comes from Middle Paleolithic/Middle Stone Age sites such as Blombos Cave–South Africa–in the form of bracelets, beads, rock art, and ochre used as body paint and perhaps in ritual. Undisputed evidence of art only becomes common in the Upper Paleolithic.

Lower Paleolithic Acheulean tool users, according to Robert G. Bednarik, began to engage in symbolic behavior such as art around 850,000 BP. They decorated themselves with beads and collected exotic stones for aesthetic, rather than utilitarian qualities. According to him, traces of the pigment ochre from late Lower Paleolithic Acheulean archaeological sites suggests that Acheulean societies, like later Upper Paleolithic societies, collected and used ochre to create rock art. Nevertheless, it is also possible that the ochre traces found at Lower Paleolithic sites is naturally occurring.

Upper Paleolithic humans produced works of art such as cave paintings, Venus figurines, animal carvings, and rock paintings. Upper Paleolithic art can be divided into two broad categories: figurative art such as cave paintings that clearly depicts animals (or more rarely humans); and nonfigurative, which consists of shapes and symbols. Cave paintings have been interpreted in a number of ways by modern archaeologists. The earliest explanation, by the prehistorian Abbe Breuil, interpreted the paintings as a form of magic designed to ensure a successful hunt. However, this hypothesis fails to explain the existence of animals such as saber-toothed cats and lions, which were not hunted for food, and the existence of half-human, half-animal beings in cave paintings. The anthropologist David Lewis-Williams has suggested that Paleolithic cave paintings were indications of shamanistic practices, because the paintings of half-human, half-animal paintings and the remoteness of the caves are reminiscent of modern hunter-gatherer shamanistic practices. Symbol-like images are more common in Paleolithic cave paintings than are depictions of animals or humans, and unique symbolic patterns might have been trademarks that represent different Upper Paleolithic ethnic groups. Venus figurines have evoked similar controversy. Archaeologists and anthropologists have described the figurines as representations of goddesses, pornographic imagery, apotropaic amulets used for sympathetic magic, and even as self-portraits of women themselves.

R. Dale Guthrie has studied not only the most artistic and publicized paintings, but also a variety of lower-quality art and figurines, and he identifies a wide range of skill and ages among the artists. He also points out that the main themes in the paintings and other artifacts (powerful beasts, risky hunting scenes and the over-sexual representation of women) are to be expected in the fantasies of adolescent males during the Upper Paleolithic.

The "Venus" figurines have been theorized, not universally, as representing a mother goddess; the abundance of such female imagery has inspired the theory that religion and society in Paleolithic (and later Neolithic) cultures were primarily interested in, and may have been directed by, women. Adherents of the theory include archaeologist Marija Gimbutas and feminist scholar Merlin Stone, the author of the 1976 book "When God Was a Woman". Other explanations for the purpose of the figurines have been proposed, such as Catherine McCoid and LeRoy McDermott's hypothesis that they were self-portraits of woman artists and R.Dale Gutrie's hypothesis that served as "stone age pornography".

The origins of music during the Paleolithic are unknown. The earliest forms of music probably did not use musical instruments other than the human voice or natural objects such as rocks. This early music would not have left an archaeological footprint. Music may have developed from rhythmic sounds produced by daily chores, for example, cracking open nuts with stones. Maintaining a rhythm while working may have helped people to become more efficient at daily activities. An alternative theory originally proposed by Charles Darwin explains that music may have begun as a hominin mating strategy. Bird and other animal species produce music such as calls to attract mates. This hypothesis is generally less accepted than the previous hypothesis, but nonetheless provides a possible alternative.

Upper Paleolithic (and possibly Middle Paleolithic) humans used flute-like bone pipes as musical instruments, and music may have played a large role in the religious lives of Upper Paleolithic hunter-gatherers. As with modern hunter-gatherer societies, music may have been used in ritual or to help induce trances. In particular, it appears that animal skin drums may have been used in religious events by Upper Paleolithic shamans, as shown by the remains of drum-like instruments from some Upper Paleolithic graves of shamans and the ethnographic record of contemporary hunter-gatherer shamanic and ritual practices.

According to James B. Harrod humankind first developed religious and spiritual beliefs during the Middle Paleolithic or Upper Paleolithic. Controversial scholars of prehistoric religion and anthropology, James Harrod and Vincent W. Fallio, have recently proposed that religion and spirituality (and art) may have first arisen in Pre-Paleolithic chimpanzees or Early Lower Paleolithic (Oldowan) societies. According to Fallio, the common ancestor of chimpanzees and humans experienced altered states of consciousness and partook in ritual, and ritual was used in their societies to strengthen social bonding and group cohesion.

Middle Paleolithic humans' use of burials at sites such as Krapina, Croatia ( BP) and Qafzeh, Israel ( BP) have led some anthropologists and archaeologists, such as Philip Lieberman, to believe that Middle Paleolithic humans may have possessed a belief in an afterlife and a "concern for the dead that transcends daily life". Cut marks on Neanderthal bones from various sites, such as Combe-Grenal and Abri Moula in France, suggest that the Neanderthals—like some contemporary human cultures—may have practiced ritual defleshing for (presumably) religious reasons. According to recent archaeological findings from "Homo heidelbergensis" sites in Atapuerca, humans may have begun burying their dead much earlier, during the late Lower Paleolithic; but this theory is widely questioned in the scientific community.

Likewise, some scientists have proposed that Middle Paleolithic societies such as Neanderthal societies may also have practiced the earliest form of totemism or animal worship, in addition to their (presumably religious) burial of the dead. In particular, Emil Bächler suggested (based on archaeological evidence from Middle Paleolithic caves) that a bear cult was widespread among Middle Paleolithic Neanderthals. A claim that evidence was found for Middle Paleolithic animal worship  BCE originates from the Tsodilo Hills in the African Kalahari desert has been denied by the original investigators of the site. Animal cults in the Upper Paleolithic, such as the bear cult, may have had their origins in these hypothetical Middle Paleolithic animal cults. Animal worship during the Upper Paleolithic was intertwined with hunting rites. For instance, archaeological evidence from art and bear remains reveals that the bear cult apparently involved a type of sacrificial bear ceremonialism, in which a bear was shot with arrows, finished off by a shot or thrust in the lungs, and ritually worshipped near a clay bear statue covered by a bear fur with the skull and the body of the bear buried separately. Barbara Ehrenreich controversially theorizes that the sacrificial hunting rites of the Upper Paleolithic (and by extension Paleolithic cooperative big-game hunting) gave rise to war or warlike raiding during the following Epipaleolithic and Mesolithic or late Upper Paleolithic.

The existence of anthropomorphic images and half-human, half-animal images in the Upper Paleolithic may further indicate that Upper Paleolithic humans were the first people to believe in a pantheon of gods or supernatural beings, though such images may instead indicate shamanistic practices similar to those of contemporary tribal societies. The earliest known undisputed burial of a shaman (and by extension the earliest undisputed evidence of shamans and shamanic practices) dates back to the early Upper Paleolithic era ( BP) in what is now the Czech Republic. However, during the early Upper Paleolithic it was probably more common for all members of the band to participate equally and fully in religious ceremonies, in contrast to the religious traditions of later periods when religious authorities and part-time ritual specialists such as shamans, priests and medicine men were relatively common and integral to religious life. Additionally, it is also possible that Upper Paleolithic religions, like contemporary and historical animistic and polytheistic religions, believed in the existence of a single creator deity in addition to other supernatural beings such as animistic spirits.

Religion was possibly apotropaic; specifically, it may have involved sympathetic magic. The Venus figurines, which are abundant in the Upper Paleolithic archaeological record, provide an example of possible Paleolithic sympathetic magic, as they may have been used for ensuring success in hunting and to bring about fertility of the land and women. The Upper Paleolithic Venus figurines have sometimes been explained as depictions of an earth goddess similar to Gaia, or as representations of a goddess who is the ruler or mother of the animals. James Harrod has described them as representative of female (and male) shamanistic spiritual transformation processes.

Paleolithic hunting and gathering people ate varying proportions of vegetables (including tubers and roots), fruit, seeds (including nuts and wild grass seeds) and insects, meat, fish, and shellfish. However, there is little direct evidence of the relative proportions of plant and animal foods. Although the term "paleolithic diet", without references to a specific timeframe or locale, is sometimes used with an implication that most humans shared a certain diet during the entire era, that is not entirely accurate. The Paleolithic was an extended period of time, during which multiple technological advances were made, many of which had impact on human dietary structure. For example, humans probably did not possess the control of fire until the Middle Paleolithic, or tools necessary to engage in extensive fishing. On the other hand, both these technologies are generally agreed to have been widely available to humans by the end of the Paleolithic (consequently, allowing humans in some regions of the planet to rely heavily on fishing and hunting). In addition, the Paleolithic involved a substantial geographical expansion of human populations. During the Lower Paleolithic, ancestors of modern humans are thought to have been constrained to Africa east of the Great Rift Valley. During the Middle and Upper Paleolithic, humans greatly expanded their area of settlement, reaching ecosystems as diverse as New Guinea and Alaska, and adapting their diets to whatever local resources were available.

Another view is that until the Upper Paleolithic, humans were frugivores (fruit eaters) who supplemented their meals with carrion, eggs, and small prey such as baby birds and mussels, and only on rare occasions managed to kill and consume big game such as antelopes. This view is supported by studies of higher apes, particularly chimpanzees. Chimpanzees are the closest to humans genetically, sharing more than 96% of their DNA code with humans, and their digestive tract is functionally very similar to that of humans. Chimpanzees are primarily frugivores, but they could and would consume and digest animal flesh, given the opportunity. In general, their actual diet in the wild is about 95% plant-based, with the remaining 5% filled with insects, eggs, and baby animals. In some ecosystems, however, chimpanzees are predatory, forming parties to hunt monkeys. Some comparative studies of human and higher primate digestive tracts do suggest that humans have evolved to obtain greater amounts of calories from sources such as animal foods, allowing them to shrink the size of the gastrointestinal tract relative to body mass and to increase the brain mass instead.

Anthropologists have diverse opinions about the proportions of plant and animal foods consumed. Just as with still existing hunters and gatherers, there were many varied "diets" in different groups, and also varying through this vast amount of time. Some paleolithic hunter-gatherers consumed a significant amount of meat and possibly obtained most of their food from hunting, while others are shown as a primarily plant-based diet. Most, if not all, are believed to have been opportunistic omnivores. One hypothesis is that carbohydrate tubers (plant underground storage organs) may have been eaten in high amounts by pre-agricultural humans. It is thought that the Paleolithic diet included as much as per day of fruit and vegetables. The relative proportions of plant and animal foods in the diets of Paleolithic people often varied between regions, with more meat being necessary in colder regions (which weren't populated by anatomically modern humans until  BP). It is generally agreed that many modern hunting and fishing tools, such as fish hooks, nets, bows, and poisons, weren't introduced until the Upper Paleolithic and possibly even Neolithic. The only hunting tools widely available to humans during any significant part of the Paleolithic were hand-held spears and harpoons. There's evidence of Paleolithic people killing and eating seals and elands as far as  BP. On the other hand, buffalo bones found in African caves from the same period are typically of very young or very old individuals, and there's no evidence that pigs, elephants, or rhinos were hunted by humans at the time.

Paleolithic peoples suffered less famine and malnutrition than the Neolithic farming tribes that followed them. This was partly because Paleolithic hunter-gatherers accessed a wider variety of natural foods, which allowed them a more nutritious diet and a decreased risk of famine. Many of the famines experienced by Neolithic (and some modern) farmers were caused or amplified by their dependence on a small number of crops. It is thought that wild foods can have a significantly different nutritional profile than cultivated foods. The greater amount of meat obtained by hunting big game animals in Paleolithic diets than Neolithic diets may have also allowed Paleolithic hunter-gatherers to enjoy a more nutritious diet than Neolithic agriculturalists. It has been argued that the shift from hunting and gathering to agriculture resulted in an increasing focus on a limited variety of foods, with meat likely taking a back seat to plants. It is also unlikely that Paleolithic hunter-gatherers were affected by modern diseases of affluence such as type 2 diabetes, coronary heart disease, and cerebrovascular disease, because they ate mostly lean meats and plants and frequently engaged in intense physical activity, and because the average lifespan was shorter than the age of common onset of these conditions.

Large-seeded legumes were part of the human diet long before the Neolithic Revolution, as evident from archaeobotanical finds from the Mousterian layers of Kebara Cave, in Israel.<ref name="doi10.1016/j.jas.2004.11.006"></ref> There is evidence suggesting that Paleolithic societies were gathering wild cereals for food use at least as early as 30,000 years ago. However, seeds—such as grains and beans—were rarely eaten and never in large quantities on a daily basis.<ref name="doi:10.1080/11026480510032043"></ref> Recent archaeological evidence also indicates that winemaking may have originated in the Paleolithic, when early humans drank the juice of naturally fermented wild grapes from animal-skin pouches. Paleolithic humans consumed animal organ meats, including the livers, kidneys, and brains. Upper Paleolithic cultures appear to have had significant knowledge about plants and herbs and may have, albeit very rarely, practiced rudimentary forms of horticulture. In particular, bananas and tubers may have been cultivated as early as 25,000 BP in southeast Asia. Late Upper Paleolithic societies also appear to have occasionally practiced pastoralism and animal husbandry, presumably for dietary reasons. For instance, some European late Upper Paleolithic cultures domesticated and raised reindeer, presumably for their meat or milk, as early as 14,000 BP. Humans also probably consumed hallucinogenic plants during the Paleolithic. The Aboriginal Australians have been consuming a variety of native animal and plant foods, called bushfood, for an estimated 60,000 years, since the Middle Paleolithic.
In February 2019, scientists reported evidence, based on isotope studies, that at least some Neanderthals may have eaten meat. People during the Middle Paleolithic, such as the Neanderthals and Middle Paleolithic Homo sapiens in Africa, began to catch shellfish for food as revealed by shellfish cooking in Neanderthal sites in Italy about 110,000 years ago and in Middle Paleolithic "Homo sapiens" sites at Pinnacle Point, Africa around 164,000 BP.<ref name="NYTIMES/10/08/07"></ref> Although fishing only became common during the Upper Paleolithic, fish have been part of human diets long before the dawn of the Upper Paleolithic and have certainly been consumed by humans since at least the Middle Paleolithic. For example, the Middle Paleolithic "Homo sapiens" in the region now occupied by the Democratic Republic of the Congo hunted large -long catfish with specialized barbed fishing points as early as 90,000 years ago. The invention of fishing allowed some Upper Paleolithic and later hunter-gatherer societies to become sedentary or semi-nomadic, which altered their social structures. Example societies are the Lepenski Vir as well as some contemporary hunter-gatherers, such as the Tlingit. In some instances (at least the Tlingit), they developed social stratification, slavery, and complex social structures such as chiefdoms.

Anthropologists such as Tim White suggest that cannibalism was common in human societies prior to the beginning of the Upper Paleolithic, based on the large amount of “butchered human" bones found in Neanderthal and other Lower/Middle Paleolithic sites. Cannibalism in the Lower and Middle Paleolithic may have occurred because of food shortages. However, it may have been for religious reasons, and would coincide with the development of religious practices thought to have occurred during the Upper Paleolithic. Nonetheless, it remains possible that Paleolithic societies never practiced cannibalism, and that the damage to recovered human bones was either the result of excarnation or predation by carnivores such as saber-toothed cats, lions, and hyenas.

A modern-day diet known as the Paleolithic diet exists, based on restricting consumption to the foods presumed to be available to anatomically modern humans prior to the advent of settled agriculture.



</doc>
<doc id="22873" url="https://en.wikipedia.org/wiki?curid=22873" title="Presidential Medal of Freedom">
Presidential Medal of Freedom

The Presidential Medal of Freedom is an award bestowed by the president of the United States to recognize people who have made "an especially meritorious contribution to the security or national interests of the United States, world peace, cultural or other significant public or private endeavors". The Presidential Medal of Freedom and the Congressional Gold Medal are the highest civilian awards of the United States. The award is not limited to U.S. citizens and, while it is a civilian award, it can also be awarded to military personnel and worn on the uniform.
It was established in 1963 by President John F. Kennedy, superseding the Medal of Freedom that was established by President Harry S. Truman in 1945 to honor civilian service during World War II.

Similar in name to the Medal of Freedom, but much closer in meaning and precedence to the Medal for Merit, the Presidential Medal of Freedom is currently the supreme civilian decoration in precedence in the United States, whereas the Medal of Freedom was inferior in precedence to the Medal for Merit; the Medal of Freedom was awarded by any of three Cabinet secretaries, whereas the Medal for Merit was awarded by the president, as is the Presidential Medal of Freedom.

President John F. Kennedy established the current decoration in 1963 through , with unique and distinctive insignia, vastly expanded purpose, and far higher prestige. It was the first U.S. civilian neck decoration and, in the grade of Awarded With Distinction, is the only U.S. sash and star decoration (the Chief Commander degree of the Legion of Merit—which may only be awarded to foreign heads of state—is a star decoration but without a sash). The executive order calls for the medal to be awarded annually on or around July 4, and at other convenient times as chosen by the president, but it has not been awarded every year (e.g., 2001, 2010). Recipients are selected by the president, either on the president's own initiative or based on recommendations. The order establishing the medal also expanded the size and the responsibilities of the Distinguished Civilian Service Awards Board so it could serve as a major source of such recommendations.

The medal may be awarded to an individual more than once; Colin Powell received two awards, his second being With Distinction; Ellsworth Bunker received both of his awards With Distinction. It may also be awarded posthumously (after the death of the recipient); examples (in chronological order) include John Wayne, John F. Kennedy, Pope John XXIII, Lyndon Johnson, Paul "Bear" Bryant, Thurgood Marshall, Cesar Chavez, Walter Reuther, Roberto Clemente, Jack Kemp, Harvey Milk, James Chaney, Andrew Goodman, Michael Schwerner, Elouise Cobell, Grace Hopper, Antonin Scalia, Elvis Presley and Babe Ruth. (Chaney, Goodman and Schwerner, civil rights workers murdered in 1964, were awarded their medals in 2014, 50 years later.)

In 2015, in response to questions about the medal awarded to Bill Cosby in 2002, President Barack Obama stated that there was no precedent to revoke Presidential Medals of Freedom.

The badge of the Presidential Medal of Freedom is in the form of a golden star with white enamel, with a red enamel pentagon behind it; the central disc bears thirteen gold stars on a blue enamel background (taken from the Great Seal of the United States) within a golden ring. Golden North American bald eagles with spread wings stand between the points of the star. It is worn around the neck on a blue ribbon with white edge stripes.

A special rarely given grade of the medal, known as the Presidential Medal of Freedom with Distinction, has a larger execution of the same medal design worn as a star on the left chest along with a sash over the right shoulder (similar to how the insignia of a Grand Cross is worn), with its rosette (blue with white edge, bearing the central disc of the medal at its center) resting on the left hip. When the medal With Distinction is awarded, the star may be presented descending from a neck ribbon and can be identified by its larger size than the standard medal (compare the size of medals in pictures below).

Both medals may also be worn in miniature form on a ribbon on the left chest, with a silver North American bald eagle with spread wings on the ribbon, or a golden North American bald eagle for a medal awarded With Distinction. In addition, the medal is accompanied by a service ribbon for wear on military service uniform, a miniature medal pendant for wear on mess dress or civilian formal wear, and a lapel badge for wear on civilian clothes (all shown in the accompanying photograph of the full presentation set).




</doc>
<doc id="22915" url="https://en.wikipedia.org/wiki?curid=22915" title="Planet">
Planet

A planet is an astronomical body orbiting a star or stellar remnant that is massive enough to be rounded by its own gravity, is not massive enough to cause thermonuclear fusion, and has cleared its neighbouring region of planetesimals.

The term "planet" is ancient, with ties to history, astrology, science, mythology, and religion. Apart from Earth itself, five planets in the Solar System are often visible to the naked eye. These were regarded by many early cultures as divine, or as emissaries of deities. As scientific knowledge advanced, human perception of the planets changed, incorporating a number of disparate objects. In 2006, the International Astronomical Union (IAU) officially adopted a resolution defining planets within the Solar System. This definition is controversial because it excludes many objects of planetary mass based on where or what they orbit. Although eight of the planetary bodies discovered before 1950 remain "planets" under the current definition, some celestial bodies, such as Ceres, Pallas, Juno and Vesta (each an object in the solar asteroid belt), and Pluto (the first trans-Neptunian object discovered), that were once considered "planets" by the scientific community, are no longer viewed as planets under the current definition of "planet".

The planets were thought by Ptolemy to orbit Earth in deferent and epicycle motions. Although the idea that the planets orbited the Sun had been suggested many times, it was not until the 17th century that this view was supported by evidence from the first telescopic astronomical observations, performed by Galileo Galilei. About the same time, by careful analysis of pre-telescopic observational data collected by Tycho Brahe, Johannes Kepler found the planets' orbits were elliptical rather than circular. As observational tools improved, astronomers saw that, like Earth, each of the planets rotated around an axis tilted with respect to its orbital pole, and some shared such features as ice caps and seasons. Since the dawn of the Space Age, close observation by space probes has found that Earth and the other planets share characteristics such as volcanism, hurricanes, tectonics, and even hydrology.

Planets in the Solar System are divided into two main types: large low-density giant planets, and smaller rocky terrestrials. There are eight planets in the Solar System. In order of increasing distance from the Sun, they are the four terrestrials, Mercury, Venus, Earth, and Mars, then the four giant planets, Jupiter, Saturn, Uranus, and Neptune. Six of the planets are orbited by one or more natural satellites.

Several thousands of planets around other stars ("extrasolar planets" or "exoplanets") have been discovered in the Milky Way. As of , known extrasolar planets in planetary systems (including multiple planetary systems), ranging in size from just above the size of the Moon to gas giants about twice as large as Jupiter have been discovered, out of which more than 100 planets are the same size as Earth, nine of which are at the same relative distance from their star as Earth from the Sun, i.e. in the circumstellar habitable zone. On December 20, 2011, the Kepler Space Telescope team reported the discovery of the first Earth-sized extrasolar planets, Kepler-20e and Kepler-20f, orbiting a Sun-like star, Kepler-20. A 2012 study, analyzing gravitational microlensing data, estimates an average of at least 1.6 bound planets for every star in the Milky Way.
Around one in five Sun-like stars is thought to have an Earth-sized planet in its habitable zone.

The idea of planets has evolved over its history, from the divine lights of antiquity to the earthly objects of the scientific age. The concept has expanded to include worlds not only in the Solar System, but in hundreds of other extrasolar systems. The ambiguities inherent in defining planets have led to much scientific controversy.

The five classical planets of the Solar System, being visible to the naked eye, have been known since ancient times and have had a significant impact on mythology, religious cosmology, and ancient astronomy. In ancient times, astronomers noted how certain lights moved across the sky, as opposed to the "fixed stars", which maintained a constant relative position in the sky. Ancient Greeks called these lights (, "wandering stars") or simply (, "wanderers"), from which today's word "planet" was derived. In ancient Greece, China, Babylon, and indeed all pre-modern civilizations, it was almost universally believed that Earth was the center of the Universe and that all the "planets" circled Earth. The reasons for this perception were that stars and planets appeared to revolve around Earth each day and the apparently common-sense perceptions that Earth was solid and stable and that it was not moving but at rest.

The first civilization known to have a functional theory of the planets were the Babylonians, who lived in Mesopotamia in the first and second millennia BC. The oldest surviving planetary astronomical text is the Babylonian Venus tablet of Ammisaduqa, a 7th-century BC copy of a list of observations of the motions of the planet Venus, that probably dates as early as the second millennium BC. The MUL.APIN is a pair of cuneiform tablets dating from the 7th century BC that lays out the motions of the Sun, Moon, and planets over the course of the year. The Babylonian astrologers also laid the foundations of what would eventually become Western astrology. The "Enuma anu enlil", written during the Neo-Assyrian period in the 7th century BC, comprises a list of omens and their relationships with various celestial phenomena including the motions of the planets. Venus, Mercury, and the outer planets Mars, Jupiter, and Saturn were all identified by Babylonian astronomers. These would remain the only known planets until the invention of the telescope in early modern times.

The ancient Greeks initially did not attach as much significance to the planets as the Babylonians. The Pythagoreans, in the 6th and 5th centuries BC appear to have developed their own independent planetary theory, which consisted of the Earth, Sun, Moon, and planets revolving around a "Central Fire" at the center of the Universe. Pythagoras or Parmenides is said to have been the first to identify the evening star (Hesperos) and morning star (Phosphoros) as one and the same (Aphrodite, Greek corresponding to Latin Venus), though this had long been known by the Babylonians. In the 3rd century BC, Aristarchus of Samos proposed a heliocentric system, according to which Earth and the planets revolved around the Sun. The geocentric system remained dominant until the Scientific Revolution.

By the 1st century BC, during the Hellenistic period, the Greeks had begun to develop their own mathematical schemes for predicting the positions of the planets. These schemes, which were based on geometry rather than the arithmetic of the Babylonians, would eventually eclipse the Babylonians' theories in complexity and comprehensiveness, and account for most of the astronomical movements observed from Earth with the naked eye. These theories would reach their fullest expression in the "Almagest" written by Ptolemy in the 2nd century CE. So complete was the domination of Ptolemy's model that it superseded all previous works on astronomy and remained the definitive astronomical text in the Western world for 13 centuries. To the Greeks and Romans there were seven known planets, each presumed to be circling Earth according to the complex laws laid out by Ptolemy. They were, in increasing order from Earth (in Ptolemy's order and using modern names): the Moon, Mercury, Venus, the Sun, Mars, Jupiter, and Saturn.

Cicero, in his "De Natura Deorum", enumerated the planets known during the 1st century BCE using the names for them in use at the time:

In 499 CE, the Indian astronomer Aryabhata propounded a planetary model that explicitly incorporated Earth's rotation about its axis, which he explains as the cause of what appears to be an apparent westward motion of the stars. He also believed that the orbits of planets are elliptical.
Aryabhata's followers were particularly strong in South India, where his principles of the diurnal rotation of Earth, among others, were followed and a number of secondary works were based on them.

In 1500, Nilakantha Somayaji of the Kerala school of astronomy and mathematics, in his "Tantrasangraha", revised Aryabhata's model. In his "Aryabhatiyabhasya", a commentary on Aryabhata's "Aryabhatiya", he developed a planetary model where Mercury, Venus, Mars, Jupiter and Saturn orbit the Sun, which in turn orbits Earth, similar to the Tychonic system later proposed by Tycho Brahe in the late 16th century. Most astronomers of the Kerala school who followed him accepted his planetary model.

In the 11th century, the transit of Venus was observed by Avicenna, who established that Venus was, at least sometimes, below the Sun. In the 12th century, Ibn Bajjah observed "two planets as black spots on the face of the Sun", which was later identified as a transit of Mercury and Venus by the Maragha astronomer Qotb al-Din Shirazi in the 13th century. Ibn Bajjah could not have observed a transit of Venus, because none occurred in his lifetime.

With the advent of the Scientific Revolution, use of the term "planet" changed from something that moved across the sky (in relation to the star field); to a body that orbited Earth (or that was believed to do so at the time); and by the 18th century to something that directly orbited the Sun when the heliocentric model of Copernicus, Galileo and Kepler gained sway.

Thus, Earth became included in the list of planets, whereas the Sun and Moon were excluded. At first, when the first satellites of Jupiter and Saturn were discovered in the 17th century, the terms "planet" and "satellite" were used interchangeably – although the latter would gradually become more prevalent in the following century. Until the mid-19th century, the number of "planets" rose rapidly because any newly discovered object directly orbiting the Sun was listed as a planet by the scientific community.

In the 19th century astronomers began to realize that recently discovered bodies that had been classified as planets for almost half a century (such as Ceres, Pallas, Juno, and Vesta) were very different from the traditional ones. These bodies shared the same region of space between Mars and Jupiter (the asteroid belt), and had a much smaller mass; as a result they were reclassified as "asteroids". In the absence of any formal definition, a "planet" came to be understood as any "large" body that orbited the Sun. Because there was a dramatic size gap between the asteroids and the planets, and the spate of new discoveries seemed to have ended after the discovery of Neptune in 1846, there was no apparent need to have a formal definition.

In the 20th century, Pluto was discovered. After initial observations led to the belief that it was larger than Earth, the object was immediately accepted as the ninth planet. Further monitoring found the body was actually much smaller: in 1936, Ray Lyttleton suggested that Pluto may be an escaped satellite of Neptune, and Fred Whipple suggested in 1964 that Pluto may be a comet. As it was still larger than all known asteroids and the population of dwarf planets & other trans-Neptunian objects was not well observed, it kept its status until 2006.

In 1992, astronomers Aleksander Wolszczan and Dale Frail announced the discovery of planets around a pulsar, PSR B1257+12. This discovery is generally considered to be the first definitive detection of a planetary system around another star. Then, on October 6, 1995, Michel Mayor and Didier Queloz of the Geneva Observatory announced the first definitive detection of an exoplanet orbiting an ordinary main-sequence star (51 Pegasi).

The discovery of extrasolar planets led to another ambiguity in defining a planet: the point at which a planet becomes a star. Many known extrasolar planets are many times the mass of Jupiter, approaching that of stellar objects known as brown dwarfs. Brown dwarfs are generally considered stars due to their ability to fuse deuterium, a heavier isotope of hydrogen. Although objects more massive than 75 times that of Jupiter fuse hydrogen, objects of only 13 Jupiter masses can fuse deuterium. Deuterium is quite rare, and most brown dwarfs would have ceased fusing deuterium long before their discovery, making them effectively indistinguishable from supermassive planets.

With the discovery during the latter half of the 20th century of more objects within the Solar System and large objects around other stars, disputes arose over what should constitute a planet. There were particular disagreements over whether an object should be considered a planet if it was part of a distinct population such as a belt, or if it was large enough to generate energy by the thermonuclear fusion of deuterium.

A growing number of astronomers argued for Pluto to be declassified as a planet, because many similar objects approaching its size had been found in the same region of the Solar System (the Kuiper belt) during the 1990s and early 2000s. Pluto was found to be just one small body in a population of thousands.

Some of them, such as Quaoar, Sedna, and Eris, were heralded in the popular press as the tenth planet, failing to receive widespread scientific recognition. The announcement of Eris in 2005, an object then thought of as 27% more massive than Pluto, created the necessity and public desire for an official definition of a planet.

Acknowledging the problem, the IAU set about creating the definition of planet, and produced one in August 2006. The number of planets dropped to the eight significantly larger bodies that had cleared their orbit (Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune), and a new class of dwarf planets was created, initially containing three objects (Ceres, Pluto and Eris).

There is no official definition of extrasolar planets. In 2003, the International Astronomical Union (IAU) Working Group on Extrasolar Planets issued a position statement, but this position statement was never proposed as an official IAU resolution and was never voted on by IAU members. The positions statement incorporates the following guidelines, mostly focused upon the boundary between planets and brown dwarfs:

This working definition has since been widely used by astronomers when publishing discoveries of exoplanets in academic journals. Although temporary, it remains an effective working definition until a more permanent one is formally adopted. It does not address the dispute over the lower mass limit, and so it steered clear of the controversy regarding objects within the Solar System. This definition also makes no comment on the planetary status of objects orbiting brown dwarfs, such as 2M1207b.

One definition of a sub-brown dwarf is a planet-mass object that formed through cloud collapse rather than accretion. This formation distinction between a sub-brown dwarf and a planet is not universally agreed upon; astronomers are divided into two camps as whether to consider the formation process of a planet as part of its division in classification. One reason for the dissent is that often it may not be possible to determine the formation process. For example, a planet formed by accretion around a star may get ejected from the system to become free-floating, and likewise a sub-brown dwarf that formed on its own in a star cluster through cloud collapse may get captured into orbit around a star.

One study suggests that objects above formed through gravitational instability and should not be thought of as planets.

The 13 Jupiter-mass cutoff represents an average mass rather than a precise threshold value. Large objects will fuse most of their deuterium and smaller ones will fuse only a little, and the 13 value is somewhere in between. In fact, calculations show that an object fuses 50% of its initial deuterium content when the total mass ranges between 12 and 14 . The amount of deuterium fused depends not only on mass but also on the composition of the object, on the amount of helium and deuterium present. As of 2011 the Extrasolar Planets Encyclopaedia included objects up to 25 Jupiter masses, saying, "The fact that there is no special feature around in the observed mass spectrum reinforces the choice to forget this mass limit". 
As of 2016 this limit was increased to 60 Jupiter masses based on a study of mass–density relationships. The Exoplanet Data Explorer includes objects up to 24 Jupiter masses with the advisory: "The 13 Jupiter-mass distinction by the IAU Working Group is physically unmotivated for planets with rocky cores, and observationally problematic due to the sin i ambiguity."
The NASA Exoplanet Archive includes objects with a mass (or minimum mass) equal to or less than 30 Jupiter masses.

Another criterion for separating planets and brown dwarfs, rather than deuterium fusion, formation process or location, is whether the core pressure is dominated by coulomb pressure or electron degeneracy pressure.

The matter of the lower limit was addressed during the 2006 meeting of the IAU's General Assembly. After much debate and one failed proposal, a large majority of those remaining at the meeting voted to pass a resolution. The 2006 resolution defines planets within the Solar System as follows:

Under this definition, the Solar System is considered to have eight planets. Bodies that fulfill the first two conditions but not the third (such as Ceres, Pluto, and Eris) are classified as dwarf planets, provided they are not also natural satellites of other planets. Originally an IAU committee had proposed a definition that would have included a much larger number of planets as it did not include (c) as a criterion. After much discussion, it was decided via a vote that those bodies should instead be classified as dwarf planets.

This definition is based in theories of planetary formation, in which planetary embryos initially clear their orbital neighborhood of other smaller objects. As described by astronomer Steven Soter:

The 2006 IAU definition presents some challenges for exoplanets because the language is specific to the Solar System and because the criteria of roundness and orbital zone clearance are not presently observable. Astronomer Jean-Luc Margot proposed a mathematical criterion that determines whether an object can clear its orbit during the lifetime of its host star, based on the mass of the planet, its semimajor axis, and the mass of its host star. This formula produces a value π that is greater than 1 for planets. The eight known planets and all known exoplanets have π values above 100, while Ceres, Pluto, and Eris have π values of 0.1 or less. Objects with π values of 1 or more are also expected to be approximately spherical, so that objects that fulfill the orbital zone clearance requirement automatically fulfill the roundness requirement.

The table below lists Solar System bodies once considered to be planets.

Beyond the scientific community, Pluto still holds cultural significance for many in the general public due to its historical classification as a planet from 1930 to 2006.

The names for the planets in the Western world are derived from the naming practices of the Romans, which ultimately derive from those of the Greeks and the Babylonians. In ancient Greece, the two great luminaries the Sun and the Moon were called "Helios" and "Selene"; the farthest planet (Saturn) was called "Phainon", the shiner; followed by "Phaethon" (Jupiter), "bright"; the red planet (Mars) was known as "Pyroeis", the "fiery"; the brightest (Venus) was known as "Phosphoros", the light bringer; and the fleeting final planet (Mercury) was called "Stilbon", the gleamer. The Greeks also made each planet sacred to one among their pantheon of gods, the Olympians: Helios and Selene were the names of both planets and gods; Phainon was sacred to Cronus, the Titan who fathered the Olympians; Phaethon was sacred to Zeus, Cronus's son who deposed him as king; Pyroeis was given to Ares, son of Zeus and god of war; Phosphoros was ruled by Aphrodite, the goddess of love; and Hermes, messenger of the gods and god of learning and wit, ruled over Stilbon.

The Greek practice of grafting their gods' names onto the planets was almost certainly borrowed from the Babylonians. The Babylonians named Phosphoros after their goddess of love, "Ishtar"; Pyroeis after their god of war, "Nergal", Stilbon after their god of wisdom Nabu, and Phaethon after their chief god, "Marduk". There are too many concordances between Greek and Babylonian naming conventions for them to have arisen separately. The translation was not perfect. For instance, the Babylonian Nergal was a god of war, and thus the Greeks identified him with Ares. Unlike Ares, Nergal was also god of pestilence and the underworld.

Today, most people in the western world know the planets by names derived from the Olympian pantheon of gods. Although modern Greeks still use their ancient names for the planets, other European languages, because of the influence of the Roman Empire and, later, the Catholic Church, use the Roman (Latin) names rather than the Greek ones. The Romans, who, like the Greeks, were Indo-Europeans, shared with them a common pantheon under different names but lacked the rich narrative traditions that Greek poetic culture had given their gods. During the later period of the Roman Republic, Roman writers borrowed much of the Greek narratives and applied them to their own pantheon, to the point where they became virtually indistinguishable. When the Romans studied Greek astronomy, they gave the planets their own gods' names: "Mercurius" (for Hermes), "Venus" (Aphrodite), "Mars" (Ares), "Iuppiter" (Zeus) and "Saturnus" (Cronus). When subsequent planets were discovered in the 18th and 19th centuries, the naming practice was retained with "Neptūnus" (Poseidon). Uranus is unique in that it is named for a Greek deity rather than his Roman counterpart.

Some Romans, following a belief possibly originating in Mesopotamia but developed in Hellenistic Egypt, believed that the seven gods after whom the planets were named took hourly shifts in looking after affairs on Earth. The order of shifts went Saturn, Jupiter, Mars, Sun, Venus, Mercury, Moon (from the farthest to the closest planet). Therefore, the first day was started by Saturn (1st hour), second day by Sun (25th hour), followed by Moon (49th hour), Mars, Mercury, Jupiter and Venus. Because each day was named by the god that started it, this is also the order of the days of the week in the Roman calendar after the Nundinal cycle was rejected – and still preserved in many modern languages. In English, "Saturday, Sunday," and "Monday" are straightforward translations of these Roman names. The other days were renamed after "Tiw" (Tuesday), "Wóden" (Wednesday), "Thunor" (Thursday), and "Fríge" (Friday), the Anglo-Saxon gods considered similar or equivalent to Mars, Mercury, Jupiter, and Venus, respectively.

Earth is the only planet whose name in English is not derived from Greco-Roman mythology. Because it was only generally accepted as a planet in the 17th century, there is no tradition of naming it after a god. (The same is true, in English at least, of the Sun and the Moon, though they are no longer generally considered planets.) The name originates from the 8th century Anglo-Saxon word "erda", which means ground or soil and was first used in writing as the name of the sphere of Earth perhaps around 1300. As with its equivalents in the other Germanic languages, it derives ultimately from the Proto-Germanic word "ertho", "ground", as can be seen in the English "earth", the German "Erde", the Dutch "aarde", and the Scandinavian "jord". Many of the Romance languages retain the old Roman word "terra" (or some variation of it) that was used with the meaning of "dry land" as opposed to "sea". The non-Romance languages use their own native words. The Greeks retain their original name, "Γή" "(Ge)".

Non-European cultures use other planetary-naming systems. India uses a system based on the Navagraha, which incorporates the seven traditional planets ("Surya" for the Sun, "Chandra" for the Moon, "Budha" for Mercury, "Shukra" for Venus, "Mangala" for Mars, "" for Jupiter, and "Shani" for Saturn) and the ascending and descending lunar nodes "Rahu" and "Ketu".

China and the countries of eastern Asia historically subject to Chinese cultural influence (such as Japan, Korea and Vietnam) use a naming system based on the five Chinese elements: water (Mercury), metal (Venus), fire (Mars), wood (Jupiter) and earth (Saturn).

In traditional Hebrew astronomy, the seven traditional planets have (for the most part) descriptive names – the Sun is חמה "Ḥammah" or "the hot one," the Moon is לבנה "Levanah" or "the white one," Venus is כוכב נוגה "Kokhav Nogah" or "the bright planet," Mercury is כוכב "Kokhav" or "the planet" (given its lack of distinguishing features), Mars is מאדים "Ma'adim" or "the red one," and Saturn is שבתאי "Shabbatai" or "the resting one" (in reference to its slow movement compared to the other visible planets). The odd one out is Jupiter, called צדק "Tzedeq" or "justice". Steiglitz suggests that this may be a euphemism for the original name of כוכב בעל "Kokhav Ba'al" or "Baal's planet", seen as idolatrous and euphemized in a similar manner to Ishbosheth from II Samuel.

In Arabic, Mercury is عُطَارِد ("ʿUṭārid", cognate with Ishtar / Astarte), Venus is الزهرة ("az-Zuhara", "the bright one", an epithet of the goddess Al-'Uzzá), Earth is الأرض ("al-ʾArḍ", from the same root as eretz), Mars is اَلْمِرِّيخ ("al-Mirrīkh", meaning "featherless arrow" due to its retrograde motion), Jupiter is المشتري ("al-Muštarī", "the reliable one", from Akkadian) and Saturn is زُحَل ("Zuḥal", "withdrawer").

It is not known with certainty how planets are formed. The prevailing theory is that they are formed during the collapse of a nebula into a thin disk of gas and dust. A protostar forms at the core, surrounded by a rotating protoplanetary disk. Through accretion (a process of sticky collision) dust particles in the disk steadily accumulate mass to form ever-larger bodies. Local concentrations of mass known as planetesimals form, and these accelerate the accretion process by drawing in additional material by their gravitational attraction. These concentrations become ever denser until they collapse inward under gravity to form protoplanets. After a planet reaches a mass somewhat larger than Mars' mass, it begins to accumulate an extended atmosphere, greatly increasing the capture rate of the planetesimals by means of atmospheric drag. Depending on the accretion history of solids and gas, a giant planet, an ice giant, or a terrestrial planet may result. 

When the protostar has grown such that it ignites to form a star, the surviving disk is removed from the inside outward by photoevaporation, the solar wind, Poynting–Robertson drag and other effects. Thereafter there still may be many protoplanets orbiting the star or each other, but over time many will collide, either to form a single larger planet or release material for other larger protoplanets or planets to absorb. Those objects that have become massive enough will capture most matter in their orbital neighbourhoods to become planets. Protoplanets that have avoided collisions may become natural satellites of planets through a process of gravitational capture, or remain in belts of other objects to become either dwarf planets or small bodies.

The energetic impacts of the smaller planetesimals (as well as radioactive decay) will heat up the growing planet, causing it to at least partially melt. The interior of the planet begins to differentiate by mass, developing a denser core. Smaller terrestrial planets lose most of their atmospheres because of this accretion, but the lost gases can be replaced by outgassing from the mantle and from the subsequent impact of comets. (Smaller planets will lose any atmosphere they gain through various escape mechanisms.)

With the discovery and observation of planetary systems around stars other than the Sun, it is becoming possible to elaborate, revise or even replace this account. The level of metallicity—an astronomical term describing the abundance of chemical elements with an atomic number greater than 2 (helium)—is now thought to determine the likelihood that a star will have planets. Hence, it is thought that a metal-rich population I star will likely have a more substantial planetary system than a metal-poor, population II star.

There are eight planets in the Solar System, which are in increasing distance from the Sun:


Jupiter is the largest, at 318 Earth masses, whereas Mercury is the smallest, at 0.055 Earth masses.

The planets of the Solar System can be divided into categories based on their composition:

An exoplanet (extrasolar planet) is a planet outside the Solar System. 

In early 1992, radio astronomers Aleksander Wolszczan and Dale Frail announced the discovery of two planets orbiting the pulsar PSR 1257+12. This discovery was confirmed, and is generally considered to be the first definitive detection of exoplanets. These pulsar planets are believed to have formed from the unusual remnants of the supernova that produced the pulsar, in a second round of planet formation, or else to be the remaining rocky cores of giant planets that survived the supernova and then decayed into their current orbits.

The first confirmed discovery of an extrasolar planet orbiting an ordinary main-sequence star occurred on 6 October 1995, when Michel Mayor and Didier Queloz of the University of Geneva announced the detection of an exoplanet around 51 Pegasi. From then until the Kepler mission most known extrasolar planets were gas giants comparable in mass to Jupiter or larger as they were more easily detected. The catalog of Kepler candidate planets consists mostly of planets the size of Neptune and smaller, down to smaller than Mercury.

There are types of planets that do not exist in the Solar System: super-Earths and mini-Neptunes, which could be rocky like Earth or a mixture of volatiles and gas like Neptune—a radius of 1.75 times that of Earth is a possible dividing line between the two types of planet. There are hot Jupiters that orbit very close to their star and may evaporate to become chthonian planets, which are the leftover cores. Another possible type of planet is carbon planets, which form in systems with a higher proportion of carbon than in the Solar System.

A 2012 study, analyzing gravitational microlensing data, estimates an average of at least 1.6 bound planets for every star in the Milky Way.

On December 20, 2011, the Kepler Space Telescope team reported the discovery of the first Earth-size exoplanets, Kepler-20e and Kepler-20f, orbiting a Sun-like star, Kepler-20.

Around 1 in 5 Sun-like stars have an "Earth-sized" planet in the habitable zone, so the nearest would be expected to be within 12 light-years distance from Earth.
The frequency of occurrence of such terrestrial planets is one of the variables in the Drake equation, which estimates the number of intelligent, communicating civilizations that exist in the Milky Way.

There are exoplanets that are much closer to their parent star than any planet in the Solar System is to the Sun, and there are also exoplanets that are much farther from their star. Mercury, the closest planet to the Sun at 0.4 AU, takes 88 days for an orbit, but the shortest known orbits for exoplanets take only a few hours, see Ultra-short period planet. The Kepler-11 system has five of its planets in shorter orbits than Mercury's, all of them much more massive than Mercury. Neptune is 30 AU from the Sun and takes 165 years to orbit, but there are exoplanets that are hundreds of AU from their star and take more than a thousand years to orbit, e.g. 1RXS1609 b.

A planetary-mass object (PMO), planemo, or planetary body is a celestial object with a mass that falls within the range of the definition of a planet: massive enough to achieve hydrostatic equilibrium (to be rounded under its own gravity), but not enough to sustain core fusion like a star. By definition, all planets are "planetary-mass objects", but the purpose of this term is to refer to objects that do not conform to typical expectations for a planet. These include dwarf planets, which are rounded by their own gravity but not massive enough to clear their own orbit, the larger moons, and free-floating planemos, which may have been ejected from a system (rogue planets) or formed through cloud-collapse rather than accretion (sometimes called sub-brown dwarfs).

A dwarf planet is a planetary-mass object that is neither a true planet nor a natural satellite; it is in direct orbit of a star, and is massive enough for its gravity to compress it into a hydrostatically equilibrious shape (usually a spheroid), but has not cleared the neighborhood of other material around its orbit. Alan Stern, who proposed the term 'dwarf planet', has argued that location should not matter and that only geophysical attributes should be taken into account, and that dwarf planets are thus a subtype of planet. However, the IAU accepted the term (rather than the more neutral 'planetoid') but decided to classify dwarf planets as a separate category of object. The number of dwarf planets in the Solar System is unknown. IAU has recognized three (Ceres, Pluto and Eris) and assigned the naming of two additional candidates, Haumea and Makemake, to the IAU dwarf-planet naming committee, though only Pluto has been demonstrated to meet the definition.

Several computer simulations of stellar and planetary system formation have suggested that some objects of planetary mass would be ejected into interstellar space. Some scientists have argued that such objects found roaming in deep space should be classed as "planets", although others have suggested that they should be called low-mass brown dwarfs.

Stars form via the gravitational collapse of gas clouds, but smaller objects can also form via cloud-collapse. Planetary-mass objects formed this way are sometimes called sub-brown dwarfs. Sub-brown dwarfs may be free-floating such as Cha 110913-773444 and OTS 44, or orbiting a larger object such as 2MASS J04414489+2301513.

Binary systems of sub-brown dwarfs are theoretically possible; Oph 162225-240515 was initially thought to be a binary system of a brown dwarf of 14 Jupiter masses and a sub-brown dwarf of 7 Jupiter masses, but further observations revised the estimated masses upwards to greater than 13 Jupiter masses, making them brown dwarfs according to the IAU working definitions.

In close binary star systems one of the stars can lose mass to a heavier companion. Accretion-powered pulsars may drive mass loss. The shrinking star can then become a planetary-mass object. An example is a Jupiter-mass object orbiting the pulsar PSR J1719-1438. These shrunken white dwarfs may become a helium planet or carbon planet.

Some large satellites (moons) are of similar size or larger than the planet Mercury, e.g. Jupiter's Galilean moons and Titan. Alan Stern has argued that location should not matter and that only geophysical attributes should be taken into account in the definition of a planet, and proposes the term "satellite planet" for a planet-sized satellite.

Rogue planets in stellar clusters have similar velocities to the stars and so can be recaptured. They are typically captured into wide orbits between 100 and 10 AU. The capture efficiency decreases with increasing cluster volume, and for a given cluster size it increases with the host/primary mass. It is almost independent of the planetary mass. Single and multiple planets could be captured into arbitrary unaligned orbits, non-coplanar with each other or with the stellar host spin, or pre-existing planetary system.

Although each planet has unique physical characteristics, a number of broad commonalities do exist among them. Some of these characteristics, such as rings or natural satellites, have only as yet been observed in planets in the Solar System, whereas others are also commonly observed in extrasolar planets.

According to current definitions, all planets must revolve around stars; thus, any potential "rogue planets" are excluded. In the Solar System, all the planets orbit the Sun in the same direction as the Sun rotates (counter-clockwise as seen from above the Sun's north pole). At least one extrasolar planet, WASP-17b, has been found to orbit in the opposite direction to its star's rotation. The period of one revolution of a planet's orbit is known as its sidereal period or "year". A planet's year depends on its distance from its star; the farther a planet is from its star, not only the longer the distance it must travel, but also the slower its speed, because it is less affected by its star's gravity. No planet's orbit is perfectly circular, and hence the distance of each varies over the course of its year. The closest approach to its star is called its periastron (perihelion in the Solar System), whereas its farthest separation from the star is called its apastron (aphelion). As a planet approaches periastron, its speed increases as it trades gravitational potential energy for kinetic energy, just as a falling object on Earth accelerates as it falls; as the planet reaches apastron, its speed decreases, just as an object thrown upwards on Earth slows down as it reaches the apex of its trajectory.

Each planet's orbit is delineated by a set of elements:

Planets also have varying degrees of axial tilt; they lie at an angle to the plane of their stars' equators. This causes the amount of light received by each hemisphere to vary over the course of its year; when the northern hemisphere points away from its star, the southern hemisphere points towards it, and vice versa. Each planet therefore has seasons, changes to the climate over the course of its year. The time at which each hemisphere points farthest or nearest from its star is known as its solstice. Each planet has two in the course of its orbit; when one hemisphere has its summer solstice, when its day is longest, the other has its winter solstice, when its day is shortest. The varying amount of light and heat received by each hemisphere creates annual changes in weather patterns for each half of the planet. Jupiter's axial tilt is very small, so its seasonal variation is minimal; Uranus, on the other hand, has an axial tilt so extreme it is virtually on its side, which means that its hemispheres are either perpetually in sunlight or perpetually in darkness around the time of its solstices. Among extrasolar planets, axial tilts are not known for certain, though most hot Jupiters are believed to have negligible to no axial tilt as a result of their proximity to their stars.

The planets rotate around invisible axes through their centres. A planet's rotation period is known as a stellar day. Most of the planets in the Solar System rotate in the same direction as they orbit the Sun, which is counter-clockwise as seen from above the Sun's north pole, the exceptions being Venus and Uranus, which rotate clockwise, though Uranus's extreme axial tilt means there are differing conventions on which of its poles is "north", and therefore whether it is rotating clockwise or anti-clockwise. Regardless of which convention is used, Uranus has a retrograde rotation relative to its orbit.

The rotation of a planet can be induced by several factors during formation. A net angular momentum can be induced by the individual angular momentum contributions of accreted objects. The accretion of gas by the giant planets can also contribute to the angular momentum. Finally, during the last stages of planet building, a stochastic process of protoplanetary accretion can randomly alter the spin axis of the planet. There is great variation in the length of day between the planets, with Venus taking 243 days to rotate, and the giant planets only a few hours. The rotational periods of extrasolar planets are not known. However, for "hot" Jupiters, their proximity to their stars means that they are tidally locked (i.e., their orbits are in sync with their rotations). This means, they always show one face to their stars, with one side in perpetual day, the other in perpetual night.

The defining dynamic characteristic of a planet is that it has "cleared its neighborhood". A planet that has cleared its neighborhood has accumulated enough mass to gather up or sweep away all the planetesimals in its orbit. In effect, it orbits its star in isolation, as opposed to sharing its orbit with a multitude of similar-sized objects. This characteristic was mandated as part of the IAU's official definition of a planet in August, 2006. This criterion excludes such planetary bodies as Pluto, Eris and Ceres from full-fledged planethood, making them instead dwarf planets. Although to date this criterion only applies to the Solar System, a number of young extrasolar systems have been found in which evidence suggests orbital clearing is taking place within their circumstellar discs.

A planet's defining physical characteristic is that it is massive enough for the force of its own gravity to dominate over the electromagnetic forces binding its physical structure, leading to a state of hydrostatic equilibrium. This effectively means that all planets are spherical or spheroidal. Up to a certain mass, an object can be irregular in shape, but beyond that point, which varies depending on the chemical makeup of the object, gravity begins to pull an object towards its own centre of mass until the object collapses into a sphere.

Mass is also the prime attribute by which planets are distinguished from stars. The upper mass limit for planethood is roughly 13 times Jupiter's mass for objects with solar-type isotopic abundance, beyond which it achieves conditions suitable for nuclear fusion. Other than the Sun, no objects of such mass exist in the Solar System; but there are exoplanets of this size. The 13-Jupiter-mass limit is not universally agreed upon and the Extrasolar Planets Encyclopaedia includes objects up to 60 Jupiter masses, and the Exoplanet Data Explorer up to 24 Jupiter masses.

The smallest known planet is PSR B1257+12A, one of the first extrasolar planets discovered, which was found in 1992 in orbit around a pulsar. Its mass is roughly half that of the planet Mercury. The smallest known planet orbiting a main-sequence star other than the Sun is Kepler-37b, with a mass (and radius) slightly higher than that of the Moon.

Every planet began its existence in an entirely fluid state; in early formation, the denser, heavier materials sank to the centre, leaving the lighter materials near the surface. Each therefore has a differentiated interior consisting of a dense planetary core surrounded by a mantle that either is or was a fluid. The terrestrial planets are sealed within hard crusts, but in the giant planets the mantle simply blends into the upper cloud layers. The terrestrial planets have cores of elements such as iron and nickel, and mantles of silicates. Jupiter and Saturn are believed to have cores of rock and metal surrounded by mantles of metallic hydrogen. Uranus and Neptune, which are smaller, have rocky cores surrounded by mantles of water, ammonia, methane and other ices. The fluid action within these planets' cores creates a geodynamo that generates a magnetic field.

All of the Solar System planets except Mercury have substantial atmospheres because their gravity is strong enough to keep gases close to the surface. The larger giant planets are massive enough to keep large amounts of the light gases hydrogen and helium, whereas the smaller planets lose these gases into space. The composition of Earth's atmosphere is different from the other planets because the various life processes that have transpired on the planet have introduced free molecular oxygen.

Planetary atmospheres are affected by the varying insolation or internal energy, leading to the formation of dynamic weather systems such as hurricanes, (on Earth), planet-wide dust storms (on Mars), a greater-than-Earth-sized anticyclone on Jupiter (called the Great Red Spot), and holes in the atmosphere (on Neptune). At least one extrasolar planet, HD 189733 b, has been claimed to have such a weather system, similar to the Great Red Spot but twice as large.

Hot Jupiters, due to their extreme proximities to their host stars, have been shown to be losing their atmospheres into space due to stellar radiation, much like the tails of comets. These planets may have vast differences in temperature between their day and night sides that produce supersonic winds, although the day and night sides of HD 189733 b appear to have very similar temperatures, indicating that that planet's atmosphere effectively redistributes the star's energy around the planet.

One important characteristic of the planets is their intrinsic magnetic moments, which in turn give rise to magnetospheres. The presence of a magnetic field indicates that the planet is still geologically alive. In other words, magnetized planets have flows of electrically conducting material in their interiors, which generate their magnetic fields. These fields significantly change the interaction of the planet and solar wind. A magnetized planet creates a cavity in the solar wind around itself called the magnetosphere, which the wind cannot penetrate. The magnetosphere can be much larger than the planet itself. In contrast, non-magnetized planets have only small magnetospheres induced by interaction of the ionosphere with the solar wind, which cannot effectively protect the planet.

Of the eight planets in the Solar System, only Venus and Mars lack such a magnetic field. In addition, the moon of Jupiter Ganymede also has one. Of the magnetized planets the magnetic field of Mercury is the weakest, and is barely able to deflect the solar wind. Ganymede's magnetic field is several times larger, and Jupiter's is the strongest in the Solar System (so strong in fact that it poses a serious health risk to future manned missions to its moons). The magnetic fields of the other giant planets are roughly similar in strength to that of Earth, but their magnetic moments are significantly larger. The magnetic fields of Uranus and Neptune are strongly tilted relative the rotational axis and displaced from the centre of the planet.

In 2004, a team of astronomers in Hawaii observed an extrasolar planet around the star HD 179949, which appeared to be creating a sunspot on the surface of its parent star. The team hypothesized that the planet's magnetosphere was transferring energy onto the star's surface, increasing its already high 7,760 °C temperature by an additional 400 °C.

Several planets or dwarf planets in the Solar System (such as Neptune and Pluto) have orbital periods that are in resonance with each other or with smaller bodies (this is also common in satellite systems). All except Mercury and Venus have natural satellites, often called "moons". Earth has one, Mars has two, and the giant planets have numerous moons in complex planetary-type systems. Many moons of the giant planets have features similar to those on the terrestrial planets and dwarf planets, and some have been studied as possible abodes of life (especially Europa).

The four giant planets are also orbited by planetary rings of varying size and complexity. The rings are composed primarily of dust or particulate matter, but can host tiny 'moonlets' whose gravity shapes and maintains their structure. Although the origins of planetary rings is not precisely known, they are believed to be the result of natural satellites that fell below their parent planet's Roche limit and were torn apart by tidal forces.

No secondary characteristics have been observed around extrasolar planets. The sub-brown dwarf Cha 110913-773444, which has been described as a rogue planet, is believed to be orbited by a tiny protoplanetary disc and the sub-brown dwarf OTS 44 was shown to be surrounded by a substantial protoplanetary disk of at least 10 Earth masses.



</doc>
<doc id="22918" url="https://en.wikipedia.org/wiki?curid=22918" title="Paramount Pictures">
Paramount Pictures

Paramount Pictures Corporation (commonly known as Paramount Pictures and also known simply as Paramount) is an American film studio and subsidiary of ViacomCBS. It is the fifth oldest film studio in the world, the second oldest film studio in the United States, and the sole member of the "Big Five" film studios still located in the city limits of Los Angeles.

In 1916, film producer Adolph Zukor put 22 actors and actresses under contract and honored each with a star on the logo. In 2014, Paramount Pictures became the first major Hollywood studio to distribute all of its films in digital form only. The company's headquarters and studios are located at 5555 Melrose Avenue, Hollywood, California, United States.

Paramount Pictures is a member of the Motion Picture Association (MPA).

Paramount is the fifth oldest surviving film studio in the world after the French studios Gaumont Film Company (1895) and Pathé (1896), followed by the Nordisk Film company (1906), and Universal Studios (1912). It is the last major film studio still headquartered in the Hollywood district of Los Angeles.

Paramount Pictures dates its existence from the 1912 founding date of the Famous Players Film Company. Hungarian-born founder Adolph Zukor, who had been an early investor in nickelodeons, saw that movies appealed mainly to working-class immigrants. With partners Daniel Frohman and Charles Frohman he planned to offer feature-length films that would appeal to the middle class by featuring the leading theatrical players of the time (leading to the slogan "Famous Players in Famous Plays"). By mid-1913, Famous Players had completed five films, and Zukor was on his way to success. Its first film was "Les Amours de la reine Élisabeth", which starred Sarah Bernhardt.

That same year, another aspiring producer, Jesse L. Lasky, opened his Lasky Feature Play Company with money borrowed from his brother-in-law, Samuel Goldfish, later known as Samuel Goldwyn. The Lasky company hired as their first employee a stage director with virtually no film experience, Cecil B. DeMille, who would find a suitable site in Hollywood, near Los Angeles, for his first feature film, "The Squaw Man".

Starting in 1914, both Lasky and Famous Players released their films through a start-up company, Paramount Pictures Corporation, organized early that year by a Utah theatre owner, W. W. Hodkinson, who had bought and merged several smaller firms. Hodkinson and actor, director, producer Hobart Bosworth had started production of a series of Jack London movies. Paramount was the first successful nationwide distributor; until this time, films were sold on a statewide or regional basis which had proved costly to film producers. Also, Famous Players and Lasky were privately owned while Paramount was a corporation.

In 1916, Zukor maneuvered a three-way merger of his Famous Players, the Lasky Company, and Paramount. Zukor and Lasky bought Hodkinson out of Paramount, and merged the three companies into one. The new company Lasky and Zukor founded, Famous Players-Lasky Corporation, grew quickly, with Lasky and his partners Goldwyn and DeMille running the production side, Hiram Abrams in charge of distribution, and Zukor making great plans. With only the exhibitor-owned First National as a rival, Famous Players-Lasky and its "Paramount Pictures" soon dominated the business.
Because Zukor believed in stars, he signed and developed many of the leading early stars, including Mary Pickford, Marguerite Clark, Pauline Frederick, Douglas Fairbanks, Gloria Swanson, Rudolph Valentino, and Wallace Reid. With so many important players, Paramount was able to introduce "block booking", which meant that an exhibitor who wanted a particular star's films had to buy a year's worth of other Paramount productions. It was this system that gave Paramount a leading position in the 1920s and 1930s, but which led the government to pursue it on antitrust grounds for more than twenty years.

The driving force behind Paramount's rise was Zukor. Through the teens and twenties, he built the Publix Theatres Corporation, a chain of nearly 2,000 screens, ran two production studios (in Astoria, New York, now the Kaufman Astoria Studios, and Hollywood, California), and became an early investor in radio, taking a 50% interest in the new Columbia Broadcasting System in 1928 (selling it within a few years; this would not be the last time Paramount and CBS crossed paths).

In 1926, Zukor hired independent producer B. P. Schulberg, an unerring eye for new talent, to run the new West Coast operations. They purchased the Robert Brunton Studios, a 26-acre facility at 5451 Marathon Street for US$1 million. In 1927, Famous Players-Lasky took the name Paramount Famous Lasky Corporation. Three years later, because of the importance of the Publix Theatres, it became Paramount Publix Corporation.

In 1928, Paramount began releasing "Inkwell Imps," animated cartoons produced by Max and Dave Fleischer's Fleischer Studios in New York City. The Fleischers, veterans in the animation industry, were among the few animation producers capable of challenging the prominence of Walt Disney. The Paramount newsreel series Paramount News ran from 1927 to 1957. Paramount was also one of the first Hollywood studios to release what were known at that time as "talkies", and in 1929, released their first musical, "Innocents of Paris". Richard A. Whiting and Leo Robin composed the score for the film; Maurice Chevalier starred and sung the most famous song from the film, "Louise".

By acquiring the successful Balaban & Katz chain in 1926, Zukor gained the services of Barney Balaban (who would eventually become Paramount's president in 1936), his brother A. J. Balaban (who would eventually supervise all stage production nationwide and produce talkie shorts), and their partner Sam Katz (who would run the Paramount-Publix theatre chain in New York City from the thirty-five-story Paramount Theatre Building on Times Square).

Balaban and Katz had developed the Wonder Theater concept, first publicized around 1918 in Chicago. The Chicago Theater was created as a very ornate theater and advertised as a "wonder theater." When Publix acquired Balaban, they embarked on a project to expand the wonder theaters, and starting building in New York in 1927. While Balaban and Public were dominant in Chicago, Loew's was the big player in New York, and did not want the Publix theaters to overshadow theirs. The two companies brokered a non-competition deal for New York and Chicago, and Loew's took over the New York area projects, developing five wonder theaters. Publix continued Balaban's wonder theater development in its home area.

Eventually, Zukor shed most of his early partners; the Frohman brothers, Hodkinson and Goldwyn were out by 1917 while Lasky hung on until 1932, when, blamed for the near-collapse of Paramount in the Depression years, he too was tossed out. Zukor's over-expansion and use of overvalued Paramount stock for purchases led the company into receivership in 1933. A bank-mandated reorganization team, led by John Hertz and Otto Kahn kept the company intact, and, miraculously, Zukor was kept on. In 1935, Paramount-Publix went bankrupt. In June 1935 John E. Otterson and in 1936 Barney Balaban became president, and Zukor was bumped up to chairman of the board. In this role, Zukor reorganized the company as Paramount Pictures, Inc. and was able to successfully bring the studio out of bankruptcy.

As always, Paramount films continued to emphasize stars; in the 1920s there were Gloria Swanson, Wallace Reid, Rudolph Valentino, Florence Vidor, Thomas Meighan, Pola Negri, Bebe Daniels, Antonio Moreno, Richard Dix, Esther Ralston, Emil Jannings, George Bancroft, Betty Compson, Clara Bow, Adolphe Menjou, and Charles Buddy Rogers. By the late 1920s and the early 1930s, talkies brought in a range of powerful draws: Richard Arlen, Nancy Carroll, Maurice Chevalier, Gary Cooper, Marlene Dietrich, Charles Ruggles, Ruth Chatterton, William Powell, Mae West, Sylvia Sidney, Bing Crosby, Claudette Colbert, the Marx Brothers, W.C. Fields, Fredric March, Jack Oakie, Jeanette MacDonald (whose first two films were shot at Paramount's Astoria, New York, studio), Carole Lombard, George Raft, Miriam Hopkins, Cary Grant and Stuart Erwin, among them. In this period Paramount can truly be described as a movie factory, turning out sixty to seventy pictures a year. Such were the benefits of having a huge theater chain to fill, and of block booking to persuade other chains to go along. In 1933, Mae West would also add greatly to Paramount's success with her suggestive movies "She Done Him Wrong" and "I'm No Angel". However, the sex appeal West gave in these movies would also lead to the enforcement of the Production Code, as the newly formed organization the Catholic Legion of Decency threatened a boycott if it was not enforced.

Paramount cartoons produced by Fleischer Studios continued to be successful, with characters such as Betty Boop and Popeye the Sailor becoming widely successful. One Fleischer series, "Screen Songs", featured live-action music stars under contract to Paramount hosting sing-alongs of popular songs. The animation studio would rebound with Popeye, and in 1935, polls showed that Popeye was even more popular than Mickey Mouse. After an unsuccessful expansion into feature films, as well as the fact that Max and Dave Fleischer were no longer speaking to one another, Fleischer Studios was acquired by Paramount, which renamed the operation Famous Studios. That incarnation of the animation studio continued cartoon production until 1967, but has been historically dismissed as having largely failed to maintain the artistic acclaim the Fleischer brothers achieved under their management.

In 1940, Paramount agreed to a government-instituted consent decree: block booking and "pre-selling" (the practice of collecting up-front money for films not yet in production) would end. Immediately, Paramount cut back on production, from 71 films to a more modest 19 annually in the war years. Still, with more new stars like Bob Hope, Alan Ladd, Veronica Lake, Paulette Goddard, and Betty Hutton, and with war-time attendance at astronomical numbers, Paramount and the other integrated studio-theatre combines made more money than ever. At this, the Federal Trade Commission and the Justice Department decided to reopen their case against the five integrated studios. Paramount also had a monopoly over Detroit movie theaters through subsidiary company United Detroit Theaters. This led to the Supreme Court decision "United States v. Paramount Pictures, Inc." (1948) holding that movie studios could not also own movie theater chains. This decision broke up Adolph Zukor's creation, with the theater chain being split into a new company, United Paramount Theaters, and effectively brought an end to the classic Hollywood studio system.

With the separation of production and exhibition forced by the U.S. Supreme Court, Paramount Pictures Inc. was split in two. Paramount Pictures Corporation was formed to be the production distribution company, with the 1,500-screen theater chain handed to the new United Paramount Theaters on December 31, 1949. Leonard Goldenson, who had headed the chain since 1938, remained as the new company's president. The Balaban and Katz theatre division was spun off with UPT; its trademark eventually became the property of the Balaban and Katz Historical Foundation. The Foundation has recently acquired ownership of the Famous Players Trademark. Cash-rich and controlling prime downtown real estate, Goldenson began looking for investments. Barred from film-making by prior antitrust rulings, he acquired the struggling ABC television network in February 1953, leading it first to financial health, and eventually, in the mid-1970s, to first place in the national Nielsen ratings, before selling out to Capital Cities in 1985 (Capital Cities would eventually sell out, in turn, to The Walt Disney Company in 1996). United Paramount Theaters was renamed ABC Theaters in 1965 and was sold to businessman Henry Plitt in 1977. The movie theater chain was renamed Plitt Theaters. In 1985, Cineplex Odeon Corporation merged with Plitt. In later years, Paramount's TV division would develop a strong relationship with ABC, providing many hit series to the network.

Paramount Pictures had been an early backer of television, launching experimental stations in 1939 in Los Angeles and Chicago. The Los Angeles station eventually became KTLA, the first commercial station on the West Coast. The Chicago station got a commercial license as WBKB in 1943, but was sold to UPT along with Balaban & Katz in 1948 and was eventually resold to CBS as WBBM-TV.

In 1938, Paramount bought a stake in television manufacturer DuMont Laboratories. Through this stake, it became a minority owner of the DuMont Television Network. Also Paramount launched its own network, Paramount Television Network, in 1948 through its television unit, Television Productions, Inc.

Paramount management planned to acquire additional owned-and-operated stations ("O&Os"); the company applied to the FCC for additional stations in San Francisco, Detroit, and Boston. The FCC, however, denied Paramount's applications. A few years earlier, the federal regulator had placed a five-station cap on all television networks: no network was allowed to own more than five VHF television stations. Paramount was hampered by its minority stake in the DuMont Television Network. Although both DuMont and Paramount executives stated that the companies were separate, the FCC ruled that Paramount's partial ownership of DuMont meant that DuMont and Paramount were in theory branches of the same company. Since DuMont owned three television stations and Paramount owned two, the federal agency ruled neither network could acquire additional television stations. The FCC requested that Paramount relinquish its stake in DuMont, but Paramount refused. According to television historian William Boddy, "Paramount's checkered antitrust history" helped convince the FCC that Paramount controlled DuMont. Both DuMont and Paramount Television Network suffered as a result, with neither company able to acquire five O&Os. Meanwhile, CBS, ABC, and NBC had each acquired the maximum of five stations by the mid-1950s.

When ABC accepted a merger offer from UPT in 1953, DuMont quickly realized that ABC now had more resources than it could possibly hope to match. It quickly reached an agreement in principle to merge with ABC. However, Paramount vetoed the offer due to antitrust concerns. For all intents and purposes, this was the end of DuMont, though it lingered on until 1956.

In 1951, Paramount bought a stake in International Telemeter, an experimental pay TV service which operated with a coin inserted into a box. The service began operating in Palm Springs, California on November 27, 1953, but due to pressure from the FCC, the service ended on May 15, 1954.

With the loss of the theater chain, Paramount Pictures went into a decline, cutting studio-backed production, releasing its contract players, and making production deals with independents. By the mid-1950s, all the great names were gone; only Cecil B. DeMille, associated with Paramount since 1913, kept making pictures in the grand old style. Despite Paramount's losses, DeMille would, however, give the studio some relief and create his most successful film at Paramount, a 1956 remake of his 1923 film "The Ten Commandments". DeMille died in 1959. Like some other studios, Paramount saw little value in its film library, and sold 764 of its pre-1948 films to MCA Inc./EMKA, Ltd. (known today as Universal Television) in February 1958.

By the early 1960s, Paramount's future was doubtful. The high-risk movie business was wobbly; the theater chain was long gone; investments in DuMont and in early pay-television came to nothing; and the Golden Age of Hollywood had just ended, even the flagship Paramount building in Times Square was sold to raise cash, as was KTLA (sold to Gene Autry in 1964 for a then-phenomenal $12.5 million). Their only remaining successful property at that point was Dot Records, which Paramount had acquired in 1957, and even its profits started declining by the middle of the 1960s. Founding father Adolph Zukor (born in 1873) was still chairman emeritus; he referred to chairman Barney Balaban (born 1888) as "the boy." Such aged leadership was incapable of keeping up with the changing times, and in 1966, a sinking Paramount was sold to Charles Bluhdorn's industrial conglomerate, Gulf + Western Industries Corporation. Bluhdorn immediately put his stamp on the studio, installing a virtually unknown producer named Robert Evans as head of production. Despite some rough times, Evans held the job for eight years, restoring Paramount's reputation for commercial success with "The Odd Couple", "Rosemary's Baby", "Love Story", "The Godfather", "Chinatown", and "3 Days of the Condor".

Gulf + Western Industries also bought the neighboring Desilu television studio (once the lot of RKO Pictures) from Lucille Ball in 1967. Using some of Desilu's established shows such as "", "", and "Mannix" as a foot in the door at the networks, the newly reincorporated Paramount Television eventually became known as a specialist in half-hour situation comedies.

In 1968, Paramount formed Films Distributing Corp to distribute sensitive film product, including "Sin With a Stranger", which was one of the first films to receive an X rating in the United States when the MPAA introduced their new rating system.

In 1970, Paramount teamed with Universal Studios to form Cinema International Corporation, a new company that would distribute films by the two studios outside the United States. Metro-Goldwyn-Mayer would become a partner in the mid-1970s. Both Paramount and CIC entered the video market with Paramount Home Video (now Paramount Home Entertainment) and CIC Video, respectively.

Robert Evans abandoned his position as head of production in 1974; his successor, Richard Sylbert, proved to be too literary and too tasteful for Gulf + Western's Bluhdorn. By 1976, a new, television-trained team was in place headed by Barry Diller and his "Killer-Dillers", as they were called by admirers or "Dillettes" as they were called by detractors. These associates, made up of Michael Eisner, Jeffrey Katzenberg, Dawn Steel and Don Simpson would each go on and head up major movie studios of their own later in their careers.

The Paramount specialty was now simpler. "High concept" pictures such as "Saturday Night Fever" and "Grease" hit big, hit hard and hit fast all over the world, and Diller's television background led him to propose one of his longest-standing ideas to the board: Paramount Television Service, a fourth commercial network. Paramount Pictures purchased the Hughes Television Network (HTN) including its satellite time in planning for PTVS in 1976. Paramount sold HTN to Madison Square Garden in 1979. But Diller believed strongly in the concept, and so took his fourth-network idea with him when he moved to 20th Century Fox in 1984, where Fox's then freshly installed proprietor, Rupert Murdoch was a more interested listener.

However, the television division would be playing catch-up for over a decade after Diller's departure in 1984 before launching its own television network – UPN – in 1995. Lasting eleven years before being merged with The WB network to become The CW in 2006, UPN would feature many of the shows it originally produced for other networks, and would take numerous gambles on series such as "" and "" that would have otherwise either gone direct-to-cable or become first-run syndication to independent stations across the country (as "" and "" were).

Paramount Pictures was not connected to either Paramount Records (1910s-1935) or ABC-Paramount Records (1955–66) until it purchased the rights to use the name (but not the latter's catalog) in the late 1960s. The Paramount name was used for soundtrack albums and some pop re-issues from the Dot Records catalog which Paramount had acquired in 1957. By 1970, Dot had become an all-country label and in 1974, Paramount sold all of its record holdings to ABC Records, which in turn was sold to MCA (now Universal Music Group) in 1979.

Paramount's successful run of pictures extended into the 1980s and 1990s, generating hits like "Airplane!", "American Gigolo", "Ordinary People", "An Officer and a Gentleman", "Flashdance", "Terms of Endearment", "Footloose", "Pretty in Pink", "Top Gun", "Crocodile Dundee", "Fatal Attraction", "Ghost", the "Friday the 13th" slasher series, as well as teaming up with Lucasfilm to create the "Indiana Jones" franchise. Other examples are the "Star Trek" film series and a string of films starring comedian Eddie Murphy like "Trading Places", "Coming to America" and "Beverly Hills Cop" and its sequels. While the emphasis was decidedly on the commercial, there were occasional less commercial but more artistic and intellectual efforts like "I'm Dancing as Fast as I Can", "Atlantic City", "Reds", "Witness", "Children of a Lesser God" and "The Accused". During this period, responsibility for running the studio passed from Eisner and Katzenberg to Frank Mancuso, Sr. (1984) and Ned Tanen (1984) to Stanley R. Jaffe (1991) and Sherry Lansing (1992). More so than most, Paramount's slate of films included many remakes and television spin-offs; while sometimes commercially successful, there have been few compelling films of the kind that once made Paramount the industry leader.

On August 25, 1983, Paramount Studios caught fire. Two or three sound stages and four outdoor sets were destroyed.

When Charles Bluhdorn died unexpectedly, his successor Martin Davis dumped all of G+W's industrial, mining, and sugar-growing subsidiaries and refocused the company, renaming it Paramount Communications in 1989. With the influx of cash from the sale of G+W's industrial properties in the mid-1980s, Paramount bought a string of television stations and KECO Entertainment's theme park operations, renaming them Paramount Parks. These parks included Paramount's Great America, Paramount Canada's Wonderland, Paramount's Carowinds, Paramount's Kings Dominion, and Paramount's Kings Island.

In 1993, Sumner Redstone's entertainment conglomerate Viacom made a bid for a merger with Paramount Communications; this quickly escalated into a bidding war with Barry Diller's QVC. But Viacom prevailed, ultimately paying $10 billion for the Paramount holdings. Viacom and Paramount had planned to merge as early as 1989.

Paramount is the last major film studio located in Hollywood proper. When Paramount moved to its present home in 1927, it was in the heart of the film community. Since then, former next-door neighbor RKO closed up shop in 1957 (Paramount ultimately absorbed their former lot); Warner Bros. (whose old Sunset Boulevard studio was sold to Paramount in 1949 as a home for KTLA) moved to Burbank in 1930; Columbia joined Warners in Burbank in 1973 then moved again to Culver City in 1989; and the Pickford-Fairbanks-Goldwyn-United Artists lot, after a lively history, has been turned into a post-production and music-scoring facility for Warners, known simply as "The Lot". For a time the semi-industrial neighborhood around Paramount was in decline, but has now come back. The recently refurbished studio has come to symbolize Hollywood for many visitors, and its studio tour is a popular attraction.

In 1983, Gulf and Western began a restructuring process that would transform the corporation from a bloated conglomerate consisting of subsidiaries from unrelated industries to a more focused entertainment and publishing company. The idea was to aid financial markets in measuring the company's success, which, in turn, would help place better value on its shares. Though its Paramount division did very well in recent years, Gulf and Western's success as a whole was translating poorly with investors. This process eventually led Davis to divest many of the company's subsidiaries. Its sugar plantations in Florida and the Dominican Republic were sold in 1985; the consumer and industrial products branch was sold off that same year. In 1989, Davis renamed the company Paramount Communications Incorporated after its primary asset, Paramount Pictures. In addition to the Paramount film, television, home video, and music publishing divisions, the company continued to own the Madison Square Garden properties (which also included MSG Network), a 50% stake in USA Networks (the other 50% was owned by MCA/Universal Studios) and Simon & Schuster, Prentice Hall, Pocket Books, Allyn & Bacon, Cineamerica (a joint venture with Warner Communications), and Canadian cinema chain Famous Players Theatres.

That same year, the company launched a $12.2 billion hostile bid to acquire Time Inc. in an attempt to end a stock-swap merger deal between Time and Warner Communications, which also renamed itself after a film studio it owned upon selling off its non-entertainment assets. (The original name of Warner Communications was Kinney National Company.) This caused Time to raise its bid for Warner to $14.9 billion in cash and stock. Gulf and Western responded by filing a lawsuit in a Delaware court to block the Time-Warner merger. The court ruled twice in favor of Time, forcing Gulf and Western to drop both the Time acquisition and the lawsuit, and allowing the formation of Time Warner.

Paramount used cash acquired from the sale of Gulf and Western's non-entertainment properties to take over the TVX Broadcast Group chain of television stations (which at that point consisted mainly of large-market stations which TVX had bought from Taft Broadcasting, plus two mid-market stations which TVX owned prior to the Taft purchase), and the KECO Entertainment chain of theme parks from Taft successor Great American Broadcasting. Both of these companies had their names changed to reflect new ownership: TVX became known as the Paramount Stations Group, while KECO was renamed to Paramount Parks.

Paramount Television launched Wilshire Court Productions in conjunction with USA Networks, before the latter was renamed NBCUniversal Cable, in 1989. Wilshire Court Productions (named for a side street in Los Angeles) produced television films that aired on the USA Networks, and later for other networks. USA Networks launched a second channel, the Sci-Fi Channel (now known as Syfy), in 1992. As its name implied, it focused on films and television series within the science fiction genre. Much of the initial programming was owned either by Paramount or Universal. Paramount bought one more television station in 1993: Cox Enterprises' WKBD-TV in Detroit, Michigan, at the time an affiliate of the Fox Broadcasting Company.

In February 1994, Viacom acquired 50.1% of Paramount Communications Inc. shares for $9.75 billion, following a five-month battle with QVC, and completed the merger in July. At the time, Paramount's holdings included Paramount Pictures, Madison Square Garden, the New York Rangers, the New York Knicks, and the Simon & Schuster publishing house. The deal had been planned as early as 1989, when the company was still known as Gulf and Western. Though Davis was named a member of the board of National Amusements, which controlled Viacom, he ceased to manage the company.

Under Viacom, the Paramount Stations Group continued to build with more station acquisitions, eventually leading to Viacom's acquisition of its former parent, the CBS network, in 1999. Around the same time, Viacom bought out Spelling Entertainment, incorporating its library into that of Paramount itself.

Viacom split into two companies in 2006, one retaining the Viacom name (which continues to own Paramount Pictures), while another was named CBS Corporation (which now controls Paramount Television Group, which was renamed CBS Paramount Television, now known as CBS Television Studios and worldwide distribution unit is now CBS Television Distribution and CBS Studios International, in 2006, Simon & Schuster [except for Prentice Hall and other educational units, which Viacom sold to Pearson PLC in 1998, and what's left of the original Paramount Stations Group, now known as CBS Television Stations). National Amusements retains majority control of the two.

Together, these two companies own many of the former media assets of Gulf and Western and its Paramount successor today. Meanwhile, the Madison Square Garden properties (including Madison Square Gardens, the MSG Network, Knicks and Rangers) were sold to Cablevision for $1.075 billion not long after the Viacom takeover. CBS retained ownership of the Paramount Parks chain for a few months after becoming part of the new CBS Corporation, but sold the parks to Cedar Fair in the summer of 2006, and thus National Amusements got out of the theme park ownership business entirely. Over the next few years, Cedar Fair purged references to Viacom-owned properties from the former Paramount Parks, a task completed in 2010. Viacom also sold its stake in the USA Networks to Universal in 1997, and the channels came under the ownership of Universal's successor, NBCUniversal, which still retained those holdings as of late July 2013.

During this time period, Paramount Pictures went under the guidance of Jonathan Dolgen, chairman and Sherry Lansing, president. During their administration over Paramount, the studio had an extremely successful period of films with two of Paramount's ten highest-grossing films being produced during this period. The most successful of these films, "Titanic", a joint partnership with 20th Century Fox, and Lightstorm Entertainment became the highest-grossing film up to that time, grossing over $1.8 billion worldwide. Also during this time, three Paramount Pictures films won the Academy Award for Best Picture; "Titanic, Braveheart", and "Forrest Gump".

Paramount's most important property, however, was "Star Trek". Studio executives had begun to call it "the franchise" in the 1980s due to its reliable revenue, and other studios envied its "untouchable and unduplicatable" success. By 1998 "Star Trek" TV shows, movies, books, videotapes, and licensing provided so much of the studio's profit that "it is not possible to spend any reasonable amount of time at Paramount and not be aware of [its] presence"; filming for "Star Trek: Voyager" and "Star Trek: Deep Space Nine" required up to nine of the largest of the studio's 36 sound stages.

In 1995, Viacom and Chris-Craft Industries' United Television launched United Paramount Network (UPN) with "Star Trek: Voyager" as its flagship series, fulfilling Barry Diller's plan for a Paramount network from 25 years earlier. In 1999, Viacom bought out United Television's interests, and handed responsibility for the start-up network to the newly acquired CBS unit, which Viacom bought in 1999 – an ironic confluence of events as Paramount had once invested in CBS, and Viacom had once been the syndication arm of CBS as well. During this period the studio acquired some 30 TV stations to support the UPN network as well acquiring and merging in the assets of Republic Pictures, Spelling Television and Viacom Television, almost doubling the size of the studio's TV library. The TV division produced the dominant prime time show for the decade in "Frasier" as well as such long running hits as NCIS and "Becker" and the dominant prime time magazine show "Entertainment Tonight." Paramount also gained the ownership rights to the Rysher library, after Viacom acquired the rights from Cox Enterprises.

During this period, Paramount and its related subsidiaries and affiliates, operating under the name "Viacom Entertainment Group" also included the fourth largest group of theme parks in the United States and Canada which in addition to traditional rides and attractions launched numerous successful location-based entertainment units including a long running "Star Trek" attraction at the Las Vegas Hilton. Famous Music – the company's celebrated music publishing arm almost doubled in size and developed artists including Pink, Bush, Green Day as well as catalog favorites including Duke Ellington and Henry Mancini. The Paramount/Viacom licensing group under the leadership of Tom McGrath created the "Cheers" franchise bars and restaurants and a chain of restaurants borrowing from the studio's Academy Award-winning film "Forrest Gump" – "The Bubba Gump Shrimp Company". Through the combined efforts of Famous Music and the studio over ten "Broadway" musicals were created including Irving Berlin's "White Christmas", "Footloose, Saturday Night Fever", Andrew Lloyd Webber's "Sunset Boulevard" among others. The company's international arm, United International Pictures (UIP), was the dominant distributor internationally for ten straight years representing Paramount, Universal and MGM. Simon and Schuster became part of the Viacom Entertainment Group emerging as the US' dominant trade book publisher.

In 2002, Paramount; along with Buena Vista Distribution, 20th Century Fox, Columbia TriStar Pictures Entertainment, MGM/UA Entertainment, Universal Studios, DreamWorks Pictures, Artisan Entertainment, Lions Gate Entertainment, and Warner Bros. formed the Digital Cinema Initiatives. Operating under a waiver from the antitrust law, the studios combined under the leadership of Paramount Chief Operating Officer Tom McGrath to develop technical standards for the eventual introduction of digital film projection – replacing the now 100-year-old film technology. DCI was created "to establish and document voluntary specifications for an open architecture for digital cinema that ensures a uniform and high level of technical performance, reliability and quality control." McGrath also headed up Paramount's initiative for the creation and launch of the Blu-ray Disc.

Reflecting in part the troubles of the broadcasting business, in 2006 Viacom wrote off over $18 billion from its radio acquisitions and, early that year, announced that it would split itself in two. The split was completed in January 2006.

With the announcement of the split of Viacom, Dolgen and Lansing were replaced by former television executives Brad Grey and Gail Berman. The Viacom Inc. board split the company into CBS Corporation and a separate company under the Viacom name. The board scheduled the division for the first quarter of 2006. Under the plan, CBS Corp. would comprise CBS and UPN networks, Viacom Television Stations Group, Infinity Broadcasting, Viacom Outdoor, Paramount Television, KingWorld, Showtime, Simon and Schuster, Paramount Parks, and CBS News. The revamped Viacom would include "MTV, VH1, Nickelodeon, BET and several other cable networks as well as the Paramount movie studio". Paramount's home entertainment unit continues to distribute the Paramount TV library through CBS DVD, as both Viacom and CBS Corporation are controlled by Sumner Redstone's National Amusements.

In 2009, CBS stopped using the Paramount name in its series and changed the name of the production arm to CBS Television Studios, eliminating the Paramount name from television, to distance itself from the latter.

On December 11, 2005, the Paramount Motion Pictures Group announced that it had purchased DreamWorks SKG (which was co-founded by former Paramount executive Jeffrey Katzenberg) in a deal worth $1.6 billion. The announcement was made by Brad Grey, chairman and CEO of Paramount Pictures who noted that enhancing Paramount's pipeline of pictures is a "key strategic objective in restoring Paramount's stature as a leader in filmed entertainment." The agreement does not include DreamWorks Animation SKG Inc., the most profitable part of the company that went public the previous year.

Grey also broke up the famous United International Pictures (UIP) international distribution company with 15 countries being taken over by Pararmount or Universal by December 31, 2006 with the joint venture continuing in 20 markets. In Australia, Brazil, France, Ireland, Mexico, New Zealand and the U.K., Paramount took over UIP. While in Austria, Belgium, Germany, Italy, the Netherlands, Russia, Spain and Switzerland, Universal took over and Paramount would build its own distribution operations there. In 2007 and 2008, Paramount may sub-distribute films via Universal's countries and vice versa. Paramount's international distribution unit would be headquartered in Los Angeles and have a European hub. In Italy, Paramount distributed through Universal. With Universal indicated that it was pulling out of the UIP Korea and started its own operation there in November 2016, Paramount agreed to have CJ Entertainment distribute there. UIP president and chief operating officer Andrew Cripps was hired as Paramount Pictures International head. Paramount Pictures International distributed films that made the 1 billion mark in July 2007; the fifth studio that year to do so and it its first year.

On October 6, 2008, DreamWorks executives announced that they were leaving Paramount and relaunching an independent DreamWorks. The DreamWorks trademarks remained with DreamWorks Animation when that company was spun off before the Paramount purchase, and DreamWorks Animation transferred the license to the name to the new company.

DreamWorks films, acquired by Paramount but still distributed internationally by Universal, are included in Paramount's market share. Grey also launched a Digital Entertainment division to take advantage of emerging digital distribution technologies. This led to Paramount becoming the second movie studio to sign a deal with Apple Inc. to sell its films through the iTunes Store.

Also, in 2007, Paramount sold another one of its "heritage" units, Famous Music, to Sony/ATV Music Publishing (best known for publishing many songs by The Beatles, and for being co-owned by Michael Jackson), ending a nearly-eight-decade run as a division of Paramount, being the studio's music publishing arm since the period when the entire company went by the name "Famous Players."

In early 2008, Paramount partnered with Los Angeles-based developer FanRocket to make short scenes taken from its film library available to users on Facebook. The application, called VooZoo, allows users to send movie clips to other Facebook users and to post clips on their profile pages. Paramount engineered a similar deal with Makena Technologies to allow users of vMTV and There.com to view and send movie clips.

In March 2010, Paramount founded Insurge Pictures, an independent distributor of "micro budget" films. The distributor planned ten movies with budgets of $100,000 each. The first release was "The Devil Inside", a movie with a budget of about US$1 million. In March 2015, following waning box office returns, Paramount shuttered Insurge Pictures and moved its operations to the main studio.

In July 2011, in the wake of critical and box office success of the animated feature, "Rango", and the departure of DreamWorks Animation upon completion of their distribution contract in 2012, Paramount announced the formation of a new division, devoted to the creation of animated productions. It marks Paramount's return to having its own animated division for the first time since 1967, when Paramount Cartoon Studios shut down (it was formerly Famous Studios until 1956).

In December 2013, Walt Disney Studios (via its parent company's purchase of Lucasfilm a year earlier) gained Paramount's remaining distribution and marketing rights to future "Indiana Jones" films. Paramount will permanently retain the distribution rights to the first four films, and will receive "financial participation" from any additional films.

In February 2016, Viacom CEO and newly appointed chairman Philippe Dauman announced that the conglomerate is in talks to find an investor to purchase a minority stake in Paramount. Sumner Redstone and his daughter Shari are reportedly opposed with the deal. On July 13, 2016, Wanda Group was in talks to acquire a 49% stake of Paramount. The talks with Wanda were dropped. On January 19, 2017, Shanghai Film Group Corp. and Huahua Media said they would finance at least 25% of all Paramount Pictures movies over a three-year period. Shanghai Film Group and Huahua Media, in the deal, would help distribute and market Paramount's features in China. At the time, the "Wall Street Journal" wrote that "nearly every major Hollywood studio has a co-financing deal with a Chinese company."

On March 27, 2017, Jim Gianopulos was named as a chairman and CEO of Paramount Pictures, replacing Brad Grey. 
In July 2017, Paramount Players was formed by the studio with the hiring of Brian Robbins, founder of AwesomenessTV, Tollin/Robbins Productions and Varsity Pictures, as the division's president. The division was expected to produce films based on the Viacom Media Networks properties including MTV, Nickelodeon, BET and Comedy Central. In June 2017, Paramount Pictures signed a deal with 20th Century Fox for distribution of its films in Italy, which took effect on September. Prior to the deal, Paramount's films in Italy were distributed by Universal Pictures.

On December 7, 2017, it was reported that Paramount sold the international distribution rights of "Annihilation" to Netflix. Netflix subsequently bought the worldwide rights to "The Cloverfield Paradox" for $50 million. On November 16, 2018, Paramount signed a multi-picture film deal with Netflix as part of Viacom's growth strategy, making Paramount the first major film studio to do so. A sequel to Awesomeness Films' "To All the Boys I've Loved Before" is currently in development at the studio for Netflix.

In April 2018, Paramount posted its first quarterly profit since 2015. Bob Bakish, CEO of parent Viacom, said in a statement that turnaround efforts "have firmly taken hold as the studio improved margins and returned to profitability. This month's outstanding box-office performance of "A Quiet Place", the first film produced and released under the new team at Paramount, is a clear sign of our progress."

On September 29, 2016, National Amusements sent a letter to both CBS Corporation and Viacom, encouraging the two companies to merge back into one company. On December 12, the deal was called off. On May 30, 2019, CNBC reported that CBS and Viacom would explore merger discussions in mid-June 2019. Reports say that CBS and Viacom reportedly set August 8 as an informal deadline for reaching an agreement to recombine the two media companies. CBS announced to acquire Viacom as part of the re-merger for up to $15.4 billion. On August 2, 2019, the two companies agreed to merge back into one entity, which named ViacomCBS and the deal was closed on December 4, 2019.

In December 2019, ViacomCBS agreed to purchase a 49% stake in Miramax that was owned by beIN Media Group, with Paramount gaining the distribution of the studio's 700-film library as well as its future releases. Also, Paramount will produce television series based on Miramax's IPs. The deal officially closed on April 3, 2020.

In 2006, Paramount became the parent of DreamWorks Pictures. Soros Strategic Partners and Dune Entertainment II soon afterwards acquired controlling interest in live-action films released through DreamWorks, with the release of "Just Like Heaven" on September 16, 2005. The remaining live-action films released until March 2006 remained under direct Paramount control. However, Paramount still owns distribution and other ancillary rights to Soros and Dune films.

On February 8, 2010, Viacom repurchased Soros' controlling stake in DreamWorks' library of films released before 2005 for around $400 million. Even as DreamWorks switched distribution of live-action films not part of existing franchises to Walt Disney Studios Motion Pictures and later Universal Pictures, Paramount continues to own the films released before the merger, and the films that Paramount themselves distributed, including sequel rights such as that of "Little Fockers" (2010), distributed by Paramount and DreamWorks. It was a sequel to two existing DreamWorks films, "Meet the Parents" (2000) and "Meet the Fockers" (2004). (Paramount only owned the international distribution rights to "Little Fockers", whereas Universal Pictures handled domestic distribution).

Paramount also owned distribution rights to the DreamWorks Animation library of films made before 2013, and their previous distribution deal with future DWA titles expired at the end of 2012, with "Rise of the Guardians". 20th Century Fox took over distribution for post-2012 titles beginning with "The Croods" (2013) and ending with "" (2017). Universal Pictures subsequently took over distribution for DreamWorks Animation's films beginning with "" (2019) due to NBCUniversal's acquisition of the company in 2016, though Paramount's rights to the 2006-2012 DWA library would've expired 16 years after each film's initial theatrical release date. However, in July 2014, DreamWorks Animation purchased Paramount's distribution rights to the pre-2013 library, with 20th Century Fox distributing the library until January 2018, which Universal then assumed ownership of distribution rights.

Another asset of the former DreamWorks owned by Paramount is the pre-2008 DreamWorks Television library, which is currently distributed by Paramount's sister company CBS Television Distribution; it includes "Spin City", "High Incident", "Freaks and Geeks", "Undeclared" and "On the Lot".

Independent company Hollywood Classics represents Paramount with the theatrical distribution of all the films produced by the various motion picture divisions of CBS over the years, as a result of the Viacom/CBS merger.

Paramount has outright video distribution to the aforementioned CBS library with some exceptions; less-demanded content is usually released manufactured-on-demand by CBS themselves or licensed to Visual Entertainment Inc. Until 2009, the video rights to "My Fair Lady" were with original theatrical distributor Warner Bros., under license from CBS (the video license to that film has now reverted to Paramount).




In March 2012, Paramount licensed their name and logo to a luxury hotel investment group which subsequently named the company Paramount Hotels and Resorts. The investors plan to build 50 hotels throughout the world based on the themes of Hollywood and the California lifestyle. Among the features are private screening rooms and the Paramount library available in the hotel rooms. In April 2013, Paramount Hotels and Dubai-based DAMAC Properties announced the building of the first resort: "DAMAC Towers by Paramount."

The distinctively pyramidal Paramount mountain has been the mainstay of the company's production logo since its inception and is the oldest surviving Hollywood film logo. In the sound era, the logo was accompanied by a fanfare called "Paramount on Parade" after the film of the same name, released in 1930. The words to the fanfare, originally sung in the 1930 film, were "Proud of the crowd that will never be loud, it's Paramount on Parade."

Legend has it that the mountain is based on a doodle made by W. W. Hodkinson during a meeting with Adolph Zukor. It is said to be based on the memories of his childhood in Utah. Some claim that Utah's Ben Lomond is the mountain Hodkinson doodled, and that Peru's Artesonraju is the mountain in the live-action logo, while others claim that the Italian side of Monviso inspired the logo. Some editions of the logo bear a striking resemblance to the Pfeifferhorn, another Wasatch Range peak, and to the Matterhorn on the border between Switzerland and Italy. Mount Huntington in Alaska also bears a striking resemblance.

The motion picture logo has gone through many changes over the years:

Paramount Studios offers tours of their studios, including a Studio Tour, a VIP Tour and an After Dark Tour. The 2-hour Studio Tour offers a behind-the-scenes look at the current operations of the studio. Most of the buildings on the tour are named for historical Paramount executives or the artists that worked at Paramount over the years. Many of the stars' dressing rooms have been converted into working offices. The stages where "Samson and Delilah", "Sunset Blvd.", "White Christmas", "Rear Window", "Sabrina", "Breakfast at Tiffany's", and many other classic films were shot are still in use today. The studio's backlot set, "New York Street", features numerous blocks of façades that depict a number of New York locales: "Washington Square" (where some scenes in "The Heiress", starring Olivia de Havilland, were shot), "Brooklyn," "Financial District," and others. The 4.5-hour VIP tour takes you to additional areas not covered by the standard tour, and includes meetings with archivists and tradesman. The After Dark Tour involves a tour of the Hollywood Forever Cemetery.

A few years after the ruling of the "United States v. Paramount Pictures, Inc." case in 1948, Music Corporation of America (MCA) approached Paramount offering $50 million for 750 sound feature films released prior to December 1, 1949 with payment to be spread over a period of several years. Paramount saw this as a bargain since the fleeting movie studio saw very little value in its library of old films at the time. To address any antitrust concerns, MCA set up EMKA, Ltd. as a dummy corporation to sell these films to television. EMKA's/Universal Television's library includes the five Paramount Marx Brothers films, most of the Bob Hope–Bing Crosby "Road to..." pictures, and other classics such as "Trouble in Paradise", "Shanghai Express", "She Done Him Wrong", "Sullivan's Travels", "The Palm Beach Story", "For Whom the Bell Tolls", "Double Imdemnity", "The Lost Weekend", and "The Heiress".

The studio has produced many critically acclaimed films such as "Titanic", "Footloose", "Breakfast at Tiffany's", "Braveheart", "Ghost", "The Truman Show", "Mean Girls", "Psycho", "Rocketman", "Ferris Bueller's Day Off", "The Curious Case of Benjamin Button", "Days of Thunder", "Rosemary's Baby", "Nebraska", "Sunset Boulevard", "Forrest Gump", "Super 8", "Coming to America", "World War Z", "Babel", "The Conversation", "The Fighter", "Interstellar", "", "Terms of Endearment", "The Wolf of Wall Street" and "A Quiet Place"; as well as commercially successful franchises and/or properties such as: the "Godfather" films, "Star Trek", "", "SpongeBob SquarePants", the "Grease" films, "Sonic the Hedgehog", the "Top Gun" films, "The Italian Job", the "Transformers" films, the "Teenage Mutant Ninja Turtles" films, the "Tomb Raider" films, the "Friday the 13th" films, the "Cloverfield" films, the "G.I. Joe" films, the "Beverly Hills Cop" films, the "Terminator" films, the "Pet Sematary" films, the "Without a Paddle" films, "Jackass", the "Odd Couple" films, "South Park", the "Crocodile Dundee" films, the "Charolette's Web" films, the "Wayne's World" films, "Beavis & Butthead", "Jimmy Neutron", the "War of the Worlds" films, the "Naked Gun" films, the "Anchorman" films, "Dora the Explorer", the "Addams Family" films, "Rugrats", the "Zoolander" films, "Æon Flux", the "Ring" films, the "Bad News Bears" films, "The Wild Thornberrys", and the "Paranormal Activity" films; as well as the first four films of the Marvel Cinematic Universe, the "Indiana Jones" films, and various DreamWorks Animation properties (such as "Shrek", the "Madagascar" sequels, the first two "Kung Fu Panda" films, and the first "How to Train Your Dragon") before both studios were respectively acquired by Disney (via Marvel Studios and Lucasfilm) and Universal Studios.

—Includes theatrical reissue(s).

On July 31, 2018, Paramount was targeted by the National Hispanic Media Coalition and the National Latino Media Council, which have both claimed that the studio has the worst track record of hiring Latino and Hispanic talent both in front of and behind the camera (the last Paramount film directed by a Spanish director was "Rings" in 2017). In response to the controversy, Paramount released the statement: "We recently met with NHMC in a good faith effort to see how we could partner as we further drive Paramount's culture of diversity, inclusion, and belonging. Under our new leadership team, we continue to make progress — including ensuring representation in front of and behind the camera in upcoming films such as "Dora the Explorer", "Instant Family," "Bumblebee," and "Limited Partners" – and welcome the opportunity to build and strengthen relationships with the Latino creative community further."

The NHMC protested at the Paramount Pictures lot on August 25. More than 60 protesters attended, while chanting "Latinos excluded, time to be included!". NHMC president and CEO Alex Nogales vowed to continue the boycott until the studio signed a memorandum of understanding.

On October 17, the NHMC protested at the Paramount film lot for the second time in two months, with 75 protesters attending. The leaders delivered a petition signed by 12,307 people and addressed it to Jim Gianopulos.




</doc>
<doc id="22921" url="https://en.wikipedia.org/wiki?curid=22921" title="Psychology">
Psychology

Psychology is the science of mind and behavior. Psychology includes the study of conscious and unconscious phenomena, as well as feeling and thought. It is an academic discipline of immense scope. Psychologists seek an understanding of the emergent properties of brains, and all the variety of phenomena linked to those emergent properties, joining this way the broader neuro-scientific group of researchers. As a social science, it aims to understand individuals and groups by establishing general principles and researching specific cases.

In this field, a professional practitioner or researcher is called a psychologist and can be classified as a social, behavioral, or cognitive scientist. Psychologists attempt to understand the role of mental functions in individual and social behavior, while also exploring the physiological and biological processes that underlie cognitive functions and behaviors.

Psychologists explore behavior and mental processes, including perception, cognition, attention, emotion, intelligence, subjective experiences, motivation, brain functioning, and personality. This extends to interaction between people, such as interpersonal relationships, including psychological resilience, family resilience, and other areas. Psychologists of diverse orientations also consider the unconscious mind. Psychologists employ empirical methods to infer causal and correlational relationships between psychosocial variables. In addition, or in opposition, to employing empirical and deductive methods, some—especially clinical and counseling psychologists—at times rely upon symbolic interpretation and other inductive techniques. Psychology has been described as a "hub science" in that medicine tends to draw psychological research via neurology and psychiatry, whereas social sciences most commonly draws directly from sub-disciplines within psychology.

While psychological knowledge is often applied to the assessment and treatment of mental health problems, it is also directed towards understanding and solving problems in several spheres of human activity. By many accounts psychology ultimately aims to benefit society. The majority of psychologists are involved in some kind of therapeutic role, practicing in clinical, counseling, or school settings. Many do scientific research on a wide range of topics related to mental processes and behavior, and typically work in university psychology departments or teach in other academic settings (e.g., medical schools, hospitals). Some are employed in industrial and organizational settings, or in other areas such as human development and aging, sports, health, and the media, as well as in forensic investigation and other aspects of law.

The word "psychology" derives from Greek roots meaning study of the psyche, or soul (ψυχή "psychē", "breath, spirit, soul" and -λογία "-logia", "study of" or "research"). The Latin word "psychologia" was first used by the Croatian humanist and Latinist Marko Marulić in his book, "Psichiologia de ratione animae humanae" in the late 15th century or early 16th century. The earliest known reference to the word "psychology" in English was by Steven Blankaart in 1694 in "The Physical Dictionary" which refers to "Anatomy, which treats the Body, and Psychology, which treats of the Soul."

In 1890, William James defined "psychology" as "the science of mental life, both of its phenomena and their conditions". This definition enjoyed widespread currency for decades. However, this meaning was contested, notably by radical behaviorists such as John B. Watson, who in his 1913 manifesto defined the discipline of psychology as the acquisition of information useful to the control of behavior. Also since James defined it, the term more strongly connotes techniques of scientific experimentation. Folk psychology refers to the understanding of ordinary people, as contrasted with that of psychology professionals.

The ancient civilizations of Egypt, Greece, China, India, and Persia all engaged in the philosophical study of psychology. In Ancient Egypt the Ebers Papyrus mentioned depression and thought disorders. Historians note that Greek philosophers, including Thales, Plato, and Aristotle (especially in his "De Anima" treatise), addressed the workings of the mind. As early as the 4th century BC, Greek physician Hippocrates theorized that mental disorders had physical rather than supernatural causes.

In China, psychological understanding grew from the philosophical works of Laozi and Confucius, and later from the doctrines of Buddhism. This body of knowledge involves insights drawn from introspection and observation, as well as techniques for focused thinking and acting. It frames the universe as a division of, and interaction between, physical reality and mental reality, with an emphasis on purifying the mind in order to increase virtue and power. An ancient text known as "The Yellow Emperor's Classic of Internal Medicine" identifies the brain as the nexus of wisdom and sensation, includes theories of personality based on yin–yang balance, and analyzes mental disorder in terms of physiological and social disequilibria. Chinese scholarship focused on the brain advanced in the Qing Dynasty with the work of Western-educated Fang Yizhi (1611–1671), Liu Zhi (1660–1730), and Wang Qingren (1768–1831). Wang Qingren emphasized the importance of the brain as the center of the nervous system, linked mental disorder with brain diseases, investigated the causes of dreams and insomnia, and advanced a theory of hemispheric lateralization in brain function.

Distinctions in types of awareness appear in the ancient thought of India, influenced by Hinduism. A central idea of the Upanishads is the distinction between a person's transient mundane self and their eternal unchanging soul. Divergent Hindu doctrines, and Buddhism, have challenged this hierarchy of selves, but have all emphasized the importance of reaching higher awareness. Yoga is a range of techniques used in pursuit of this goal. Much of the Sanskrit corpus was suppressed under the British East India Company followed by the British Raj in the 1800s. However, Indian doctrines influenced Western thinking via the Theosophical Society, a New Age group which became popular among Euro-American intellectuals.

Psychology was a popular topic in Enlightenment Europe. In Germany, Gottfried Wilhelm Leibniz (1646–1716) applied his principles of calculus to the mind, arguing that mental activity took place on an indivisible continuum—most notably, that among an infinity of human perceptions and desires, the difference between conscious and unconscious awareness is only a matter of degree. Christian Wolff identified psychology as its own science, writing "Psychologia empirica" in 1732 and "Psychologia rationalis" in 1734. This notion advanced further under Immanuel Kant, who established the idea of anthropology, with psychology as an important subdivision. However, Kant explicitly and notoriously rejected the idea of experimental psychology, writing that "the empirical doctrine of the soul can also never approach chemistry even as a systematic art of analysis or experimental doctrine, for in it the manifold of inner observation can be separated only by mere division in thought, and cannot then be held separate and recombined at will (but still less does another thinking subject suffer himself to be experimented upon to suit our purpose), and even observation by itself already changes and displaces the state of the observed object." In 1783, Ferdinand Ueberwasser (1752-1812) designated himself "Professor of Empirical Psychology and Logic" and gave lectures on scientific psychology, though these developments were soon overshadowed by the Napoleonic Wars, after which the Old University of Münster was discontinued by Prussian authorities. Having consulted philosophers Hegel and Herbart, however, in 1825 the Prussian state established psychology as a mandatory discipline in its rapidly expanding and highly influential educational system. However, this discipline did not yet embrace experimentation. In England, early psychology involved phrenology and the response to social problems including alcoholism, violence, and the country's well-populated mental asylums.

Gustav Fechner began conducting psychophysics research in Leipzig in the 1830s, articulating the principle (Weber–Fechner law) that human perception of a stimulus varies logarithmically according to its intensity. Fechner's 1860 "Elements of Psychophysics" challenged Kant's stricture against quantitative study of the mind. In Heidelberg, Hermann von Helmholtz conducted parallel research on sensory perception, and trained physiologist Wilhelm Wundt. Wundt, in turn, came to Leipzig University, establishing the psychological laboratory which brought experimental psychology to the world. Wundt focused on breaking down mental processes into the most basic components, motivated in part by an analogy to recent advances in chemistry, and its successful investigation of the elements and structure of material. Paul Flechsig and Emil Kraepelin soon created another influential psychology laboratory at Leipzig, this one focused on more on experimental psychiatry.

Psychologists in Germany, Denmark, Austria, England, and the United States soon followed Wundt in setting up laboratories. G. Stanley Hall who studied with Wundt, formed a psychology lab at Johns Hopkins University in Maryland, which became internationally influential. Hall, in turn, trained Yujiro Motora, who brought experimental psychology, emphasizing psychophysics, to the Imperial University of Tokyo. Wundt's assistant, Hugo Münsterberg, taught psychology at Harvard to students such as Narendra Nath Sen Gupta—who, in 1905, founded a psychology department and laboratory at the University of Calcutta. Wundt students Walter Dill Scott, Lightner Witmer, and James McKeen Cattell worked on developing tests for mental ability. Catell, who also studied with eugenicist Francis Galton, went on to found the Psychological Corporation. Wittmer focused on mental testing of children; Scott, on selection of employees.

Another student of Wundt, Edward Titchener, created the psychology program at Cornell University and advanced a doctrine of "structuralist" psychology. Structuralism sought to analyze and classify different aspects of the mind, primarily through the method of introspection. William James, John Dewey and Harvey Carr advanced a more expansive doctrine called functionalism, attuned more to human–environment actions. In 1890, James wrote an influential book, "The Principles of Psychology", which expanded on the realm of structuralism, memorably described the human "stream of consciousness", and interested many American students in the emerging discipline. Dewey integrated psychology with social issues, most notably by promoting the cause progressive education to assimilate immigrants and inculcate moral values in children.

A different strain of experimentalism, with more connection to physiology, emerged in South America, under the leadership of Horacio G. Piñero at the University of Buenos Aires. Russia, too, placed greater emphasis on the biological basis for psychology, beginning with Ivan Sechenov's 1873 essay, "Who Is to Develop Psychology and How?" Sechenov advanced the idea of brain reflexes and aggressively promoted a deterministic viewpoint on human behavior.

Wolfgang Kohler, Max Wertheimer and Kurt Koffka co-founded the school of Gestalt psychology (not to be confused with the Gestalt therapy of Fritz Perls). This approach is based upon the idea that individuals experience things as unified wholes. Rather than breaking down thoughts and behavior into smaller elements, as in structuralism, the Gestaltists maintained that whole of experience is important, and differs from the sum of its parts. Other 19th-century contributors to the field include the German psychologist Hermann Ebbinghaus, a pioneer in the experimental study of memory, who developed quantitative models of learning and forgetting at the University of Berlin, and the Russian-Soviet physiologist Ivan Pavlov, who discovered in dogs a learning process that was later termed "classical conditioning" and applied to human beings.

One of the earliest psychology societies was "La Société de Psychologie Physiologique" in France, which lasted 1885–1893. The first meeting of the International Congress of Psychology sponsored by the International Union of Psychological Science took place in Paris, in August 1889, amidst the World's Fair celebrating the centennial of the French Revolution. William James was one of three Americans among the four hundred attendees. The American Psychological Association (APA) was founded soon after, in 1892. The International Congress continued to be held, at different locations in Europe, with wider international participation. The Sixth Congress, Geneva 1909, included presentations in Russian, Chinese, and Japanese, as well as Esperanto. After a hiatus for World War I, the Seventh Congress met in Oxford, with substantially greater participation from the war-victorious Anglo-Americans. In 1929, the Congress took place at Yale University in New Haven, Connecticut, attended by hundreds of members of the APA. Tokyo Imperial University led the way in bringing new psychology to the East, and from Japan these ideas diffused into China.

American psychology gained status during World War I, during which a standing committee headed by Robert Yerkes administered mental tests ("Army Alpha" and "Army Beta") to almost 1.8 million soldiers. Subsequent funding for behavioral research came in large part from the Rockefeller family, via the Social Science Research Council. Rockefeller charities funded the National Committee on Mental Hygiene, which promoted the concept of mental illness and lobbied for psychological supervision of child development. Through the Bureau of Social Hygiene and later funding of Alfred Kinsey, Rockefeller foundations established sex research as a viable discipline in the U.S. Under the influence of the Carnegie-funded Eugenics Record Office, the Draper-funded Pioneer Fund, and other institutions, the eugenics movement also had a significant impact on American psychology; in the 1910s and 1920s, eugenics became a standard topic in psychology classes.

During World War II and the Cold War, the U.S. military and intelligence agencies established themselves as leading funders of psychology—through the armed forces and in the new Office of Strategic Services intelligence agency. University of Michigan psychologist Dorwin Cartwright reported that university researchers began large-scale propaganda research in 1939–1941, and "the last few months of the war saw a social psychologist become chiefly responsible for determining the week-by-week-propaganda policy for the United States Government." Cartwright also wrote that psychologists had significant roles in managing the domestic economy. The Army rolled out its new General Classification Test and engaged in massive studies of troop morale. In the 1950s, the Rockefeller Foundation and Ford Foundation collaborated with the Central Intelligence Agency (CIA) to fund research on psychological warfare. In 1965, public controversy called attention to the Army's Project Camelot—the "Manhattan Project" of social science—an effort which enlisted psychologists and anthropologists to analyze foreign countries for strategic purposes.

In Germany after World War I, psychology held institutional power through the military, and subsequently expanded along with the rest of the military under the Third Reich. Under the direction of Hermann Göring's cousin Matthias Göring, the Berlin Psychoanalytic Institute was renamed the Göring Institute. Freudian psychoanalysts were expelled and persecuted under the anti-Jewish policies of the Nazi Party, and all psychologists had to distance themselves from Freud and Adler. The Göring Institute was well-financed throughout the war with a mandate to create a "New German Psychotherapy". This psychotherapy aimed to align suitable Germans with the overall goals of the Reich; as described by one physician: "Despite the importance of analysis, spiritual guidance and the active cooperation of the patient represent the best way to overcome individual mental problems and to subordinate them to the requirements of the "Volk" and the "Gemeinschaft"." Psychologists were to provide "Seelenführung", leadership of the mind, to integrate people into the new vision of a German community. Harald Schultz-Hencke melded psychology with the Nazi theory of biology and racial origins, criticizing psychoanalysis as a study of the weak and deformed. Johannes Heinrich Schultz, a German psychologist recognized for developing the technique of autogenic training, prominently advocated sterilization and euthanasia of men considered genetically undesirable, and devised techniques for facilitating this process. After the war, some new institutions were created and some psychologists were discredited due to Nazi affiliation. Alexander Mitscherlich founded a prominent applied psychoanalysis journal called "Psyche" and with funding from the Rockefeller Foundation established the first clinical psychosomatic medicine division at Heidelberg University. In 1970, psychology was integrated into the required studies of medical students.

After the Russian Revolution, psychology was heavily promoted by the Bolsheviks as a way to engineer the "New Man" of socialism. Thus, university psychology departments trained large numbers of students, for whom positions were made available at schools, workplaces, cultural institutions, and in the military. An especial focus was pedology, the study of child development, regarding which Lev Vygotsky became a prominent writer. The Bolsheviks also promoted free love and embraced the doctrine of psychoanalysis as an antidote to sexual repression. Although pedology and intelligence testing fell out of favor in 1936, psychology maintained its privileged position as an instrument of the Soviet Union. Stalinist purges took a heavy toll and instilled a climate of fear in the profession, as elsewhere in Soviet society. Following World War II, Jewish psychologists past and present (including Lev Vygotsky, A.R. Luria, and Aron Zalkind) were denounced; Ivan Pavlov (posthumously) and Stalin himself were aggrandized as heroes of Soviet psychology. Soviet academics was speedily liberalized during the Khrushchev Thaw, and cybernetics, linguistics, genetics, and other topics became acceptable again. There emerged a new field called "engineering psychology" which studied mental aspects of complex jobs (such as pilot and cosmonaut). Interdisciplinary studies became popular and scholars such as Georgy Shchedrovitsky developed systems theory approaches to human behavior.

Twentieth-century Chinese psychology originally modeled the U.S., with translations from American authors like William James, the establishment of university psychology departments and journals, and the establishment of groups including the Chinese Association of Psychological Testing (1930) and the Chinese Psychological Society (1937). Chinese psychologists were encouraged to focus on education and language learning, with the aspiration that education would enable modernization and nationalization. John Dewey, who lectured to Chinese audiences in 1918–1920, had a significant influence on this doctrine. Chancellor T'sai Yuan-p'ei introduced him at Peking University as a greater thinker than Confucius. Kuo Zing-yang who received a PhD at the University of California, Berkeley, became President of Zhejiang University and popularized behaviorism. After the Chinese Communist Party gained control of the country, the Stalinist Soviet Union became the leading influence, with Marxism–Leninism the leading social doctrine and Pavlovian conditioning the approved concept of behavior change. Chinese psychologists elaborated on Lenin's model of a "reflective" consciousness, envisioning an "active consciousness" () able to transcend material conditions through hard work and ideological struggle. They developed a concept of "recognition" () which referred the interface between individual perceptions and the socially accepted worldview (failure to correspond with party doctrine was "incorrect recognition"). Psychology education was centralized under the Chinese Academy of Sciences, supervised by the State Council. In 1951, the Academy created a Psychology Research Office, which in 1956 became the Institute of Psychology. Most leading psychologists were educated in the United States, and the first concern of the Academy was re-education of these psychologists in the Soviet doctrines. Child psychology and pedagogy for nationally cohesive education remained a central goal of the discipline.

In 1920, Édouard Claparède and Pierre Bovet created a new applied psychology organization called the International Congress of Psychotechnics Applied to Vocational Guidance, later called the International Congress of Psychotechnics and then the International Association of Applied Psychology. The IAAP is considered the oldest international psychology association. Today, at least 65 international groups deal with specialized aspects of psychology. In response to male predominance in the field, female psychologists in the U.S. formed National Council of Women Psychologists in 1941. This organization became the International Council of Women Psychologists after World War II, and the International Council of Psychologists in 1959. Several associations including the Association of Black Psychologists and the Asian American Psychological Association have arisen to promote non-European racial groups in the profession.

The world federation of national psychological societies is the International Union of Psychological Science (IUPsyS), founded in 1951 under the auspices of UNESCO, the United Nations cultural and scientific authority. Psychology departments have since proliferated around the world, based primarily on the Euro-American model. Since 1966, the Union has published the "International Journal of Psychology". IAAP and IUPsyS agreed in 1976 each to hold a congress every four years, on a staggered basis.

The International Union recognizes 66 national psychology associations and at least 15 others exist. The American Psychological Association is the oldest and largest. Its membership has increased from 5,000 in 1945 to 100,000 in the present day. The APA includes 54 divisions, which since 1960 have steadily proliferated to include more specialties. Some of these divisions, such as the Society for the Psychological Study of Social Issues and the American Psychology–Law Society, began as autonomous groups.

The Interamerican Society of Psychology, founded in 1951, aspires to promote psychology and coordinate psychologists across the Western Hemisphere. It holds the Interamerican Congress of Psychology and had 1,000 members in year 2000. The European Federation of Professional Psychology Associations, founded in 1981, represents 30 national associations with a total of 100,000 individual members. At least 30 other international groups organize psychologists in different regions.

In some places, governments legally regulate who can provide psychological services or represent themselves as a "psychologist". The APA defines a psychologist as someone with a doctoral degree in psychology.

Early practitioners of experimental psychology distinguished themselves from parapsychology, which in the late nineteenth century enjoyed great popularity (including the interest of scholars such as William James), and indeed constituted the bulk of what people called "psychology". Parapsychology, hypnotism, and psychism were major topics of the early International Congresses. But students of these fields were eventually ostractized, and more or less banished from the Congress in 1900–1905. Parapsychology persisted for a time at Imperial University, with publications such as "Clairvoyance and Thoughtography" by Tomokichi Fukurai, but here too it was mostly shunned by 1913.

As a discipline, psychology has long sought to fend off accusations that it is a "soft" science. Philosopher of science Thomas Kuhn's 1962 critique implied psychology overall was in a pre-paradigm state, lacking the agreement on overarching theory found in mature sciences such as chemistry and physics. Because some areas of psychology rely on research methods such as surveys and questionnaires, critics asserted that psychology is not an objective science. Skeptics have suggested that personality, thinking, and emotion, cannot be directly measured and are often inferred from subjective self-reports, which may be problematic. Experimental psychologists have devised a variety of ways to indirectly measure these elusive phenomenological entities.

Divisions still exist within the field, with some psychologists more oriented towards the unique experiences of individual humans, which cannot be understood only as data points within a larger population. Critics inside and outside the field have argued that mainstream psychology has become increasingly dominated by a "cult of empiricism" which limits the scope of its study by using only methods derived from the physical sciences. Feminist critiques along these lines have argued that claims to scientific objectivity obscure the values and agenda of (historically mostly male) researchers. Jean Grimshaw, for example, argues that mainstream psychological research has advanced a patriarchal agenda through its efforts to control behavior.

Psychologists generally consider the organism the basis of the mind, and therefore a vitally related area of study. Psychiatrists and neuropsychologists work at the interface of mind and body.
Biological psychology, also known as physiological psychology, or neuropsychology is the study of the biological substrates of behavior and mental processes. Key research topics in this field include comparative psychology, which studies humans in relation to other animals, and perception which involves the physical mechanics of sensation as well as neural and mental processing. For centuries, a leading question in biological psychology has been whether and how mental functions might be localized in the brain. From Phineas Gage to H.M. and Clive Wearing, individual people with mental issues traceable to physical damage have inspired new discoveries in this area. Modern neuropsychology could be said to originate in the 1870s, when in France Paul Broca traced production of speech to the left frontal gyrus, thereby also demonstrating hemispheric lateralization of brain function. Soon after, Carl Wernicke identified a related area necessary for the understanding of speech.

The contemporary field of behavioral neuroscience focuses on physical causes underpinning behavior. For example, physiological psychologists use animal models, typically rats, to study the neural, genetic, and cellular mechanisms that underlie specific behaviors such as learning and memory and fear responses. Cognitive neuroscientists investigate the neural correlates of psychological processes in humans using neural imaging tools, and neuropsychologists conduct psychological assessments to determine, for instance, specific aspects and extent of cognitive deficit caused by brain damage or disease. The biopsychosocial model is an integrated perspective toward understanding consciousness, behavior, and social interaction. It assumes that any given behavior or mental process affects and is affected by dynamically interrelated biological, psychological, and social factors.

Evolutionary psychology examines cognition and personality traits from an evolutionary perspective. This perspective suggests that psychological adaptations evolved to solve recurrent problems in human ancestral environments. Evolutionary psychology offers complementary explanations for the mostly proximate or developmental explanations developed by other areas of psychology: that is, it focuses mostly on ultimate or "why?" questions, rather than proximate or "how?" questions. "How?" questions are more directly tackled by behavioral genetics research, which aims to understand how genes and environment impact behavior.

The search for biological origins of psychological phenomena has long involved debates about the importance of race, and especially the relationship between race and intelligence. The idea of white supremacy and indeed the modern concept of race itself arose during the process of world conquest by Europeans. Carl von Linnaeus's four-fold classification of humans classifies Europeans as intelligent and severe, Americans as contented and free, Asians as ritualistic, and Africans as lazy and capricious. Race was also used to justify the construction of socially specific mental disorders such as "drapetomania" and "dysaesthesia aethiopica"—the behavior of uncooperative African slaves. After the creation of experimental psychology, "ethnical psychology" emerged as a subdiscipline, based on the assumption that studying primitive races would provide an important link between animal behavior and the psychology of more evolved humans.

Psychologists take human behavior as a main area of study. Much of the research in this area began with tests on mammals, based on the idea that humans exhibit similar fundamental tendencies. Behavioral research ever aspires to improve the effectiveness of techniques for behavior modification.

Early behavioral researchers studied stimulus–response pairings, now known as classical conditioning. They demonstrated that behaviors could be linked through repeated association with stimuli eliciting pain or pleasure. Ivan Pavlov—known best for inducing dogs to salivate in the presence of a stimulus previously linked with food—became a leading figure in the Soviet Union and inspired followers to use his methods on humans. In the United States, Edward Lee Thorndike initiated "connectionism" studies by trapping animals in "puzzle boxes" and rewarding them for escaping. Thorndike wrote in 1911: "There can be no moral warrant for studying man's nature unless the study will enable us to control his acts." From 1910–1913 the American Psychological Association went through a sea change of opinion, away from mentalism and towards "behavioralism", and in 1913 John B. Watson coined the term behaviorism for this school of thought. Watson's famous Little Albert experiment in 1920 demonstrated that repeated use of upsetting loud noises could instill phobias (aversions to other stimuli) in an infant human. Karl Lashley, a close collaborator with Watson, examined biological manifestations of learning in the brain.

Embraced and extended by Clark L. Hull, Edwin Guthrie, and others, behaviorism became a widely used research paradigm. A new method of "instrumental" or "operant" conditioning added the concepts of reinforcement and punishment to the model of behavior change. Radical behaviorists avoided discussing the inner workings of the mind, especially the unconscious mind, which they considered impossible to assess scientifically. Operant conditioning was first described by Miller and Kanorski and popularized in the U.S. by B.F. Skinner, who emerged as a leading intellectual of the behaviorist movement.

Noam Chomsky delivered an influential critique of radical behaviorism on the grounds that it could not adequately explain the complex mental process of language acquisition. Martin Seligman and colleagues discovered that the conditioning of dogs led to outcomes ("learned helplessness") that opposed the predictions of behaviorism. Skinner's behaviorism did not die, perhaps in part because it generated successful practical applications. Edward C. Tolman advanced a hybrid "cognitive behaviorial" model, most notably with his 1948 publication discussing the cognitive maps used by rats to guess at the location of food at the end of a modified maze.

The Association for Behavior Analysis International was founded in 1974 and by 2003 had members from 42 countries. The field has been especially influential in Latin America, where it has a regional organization known as ALAMOC: "La Asociación Latinoamericana de Análisis y Modificación del Comportamiento". Behaviorism also gained a strong foothold in Japan, where it gave rise to the Japanese Society of Animal Psychology (1933), the Japanese Association of Special Education (1963), the Japanese Society of Biofeedback Research (1973), the Japanese Association for Behavior Therapy (1976), the Japanese Association for Behavior Analysis (1979), and the Japanese Association for Behavioral Science Research (1994). Today the field of behaviorism is also commonly referred to as behavior modification or behavior analysis.

Cognitive psychology studies cognition, the mental processes underlying mental activity. Perception, attention, reasoning, thinking, problem solving, memory, learning, language, and emotion are areas of research. Classical cognitive psychology is associated with a school of thought known as cognitivism, whose adherents argue for an information processing model of mental function, informed by functionalism and experimental psychology.
Starting in the 1950s, the experimental techniques developed by Wundt, James, Ebbinghaus, and others re-emerged as experimental psychology became increasingly cognitivist—concerned with information and its processing—and, eventually, constituted a part of the wider cognitive science. Some called this development the cognitive revolution because it rejected the anti-mentalist dogma of behaviorism as well as the strictures of psychoanalysis.

Social learning theorists, such as Albert Bandura, argued that the child's environment could make contributions of its own to the behaviors of an observant subject.
Technological advances also renewed interest in mental states and representations. English neuroscientist Charles Sherrington and Canadian psychologist Donald O. Hebb used experimental methods to link psychological phenomena with the structure and function of the brain. The rise of computer science, cybernetics and artificial intelligence suggested the value of comparatively studying information processing in humans and machines. Research in cognition had proven practical since World War II, when it aided in the understanding of weapons operation.

A popular and representative topic in this area is cognitive bias, or irrational thought. Psychologists (and economists) have classified and described a sizeable catalogue of biases which recur frequently in human thought. The availability heuristic, for example, is the tendency to overestimate the importance of something which happens to come readily to mind.

Elements of behaviorism and cognitive psychology were synthesized to form cognitive behavioral therapy, a form of psychotherapy modified from techniques developed by American psychologist Albert Ellis and American psychiatrist Aaron T. Beck.

On a broader level, cognitive science is an interdisciplinary enterprise of cognitive psychologists, cognitive neuroscientists, researchers in artificial intelligence, linguists, human–computer interaction, computational neuroscience, logicians and social scientists. The discipline of cognitive science covers cognitive psychology as well as philosophy of mind, computer science, and neuroscience. Computer simulations are sometimes used to model phenomena of interest.

Social psychology is the study of how humans think about each other and how they relate to each other. Social psychologists study such topics as the influence of others on an individual's behavior (e.g. conformity, persuasion), and the formation of beliefs, attitudes, and stereotypes about other people. Social cognition fuses elements of social and cognitive psychology in order to understand how people process, remember, or distort social information. The study of group dynamics reveals information about the nature and potential optimization of leadership, communication, and other phenomena that emerge at least at the microsocial level. In recent years, many social psychologists have become increasingly interested in implicit measures, mediational models, and the interaction of both person and social variables in accounting for behavior. The study of human society is therefore a potentially valuable source of information about the causes of psychiatric disorder. Some sociological concepts applied to psychiatric disorders are the social role, sick role, social class, life event, culture, migration, social, and total institution.

Psychoanalysis comprises a method of investigating the mind and interpreting experience; a systematized set of theories about human behavior; and a form of psychotherapy to treat psychological or emotional distress, especially conflict originating in the unconscious mind. This school of thought originated in the 1890s with Austrian medical doctors including Josef Breuer (physician), Alfred Adler (physician), Otto Rank (psychoanalyst), and most prominently Sigmund Freud (neurologist). Freud's psychoanalytic theory was largely based on interpretive methods, introspection and clinical observations. It became very well known, largely because it tackled subjects such as sexuality, repression, and the unconscious. These subjects were largely taboo at the time, and Freud provided a catalyst for their open discussion in polite society. Clinically, Freud helped to pioneer the method of free association and a therapeutic interest in dream interpretation.

Swiss psychiatrist Carl Jung, influenced by Freud, elaborated a theory of the collective unconscious—a primordial force present in all humans, featuring archetypes which exerted a profound influence on the mind. Jung's competing vision formed the basis for analytical psychology, which later led to the archetypal and process-oriented schools. Other well-known psychoanalytic scholars of the mid-20th century include Erik Erikson, Melanie Klein, D.W. Winnicott, Karen Horney, Erich Fromm, John Bowlby, and Sigmund Freud's daughter, Anna Freud. Throughout the 20th century, psychoanalysis evolved into diverse schools of thought which could be called Neo-Freudian. Among these schools are ego psychology, object relations, and interpersonal, Lacanian, and relational psychoanalysis.

Psychologists such as Hans Eysenck and philosophers including Karl Popper criticized psychoanalysis. Popper argued that psychoanalysis had been misrepresented as a scientific discipline, whereas Eysenck said that psychoanalytic tenets had been contradicted by experimental data. By the end of 20th century, psychology departments in American universities mostly marginalized Freudian theory, dismissing it as a "desiccated and dead" historical artifact. However, researchers in the emerging field of neuro-psychoanalysis today defend some of Freud's ideas on scientific grounds, while scholars of the humanities maintain that Freud was not a "scientist at all, but ... an interpreter".

Humanistic psychology developed in the 1950s as a movement within academic psychology, in reaction to both behaviorism and psychoanalysis. The humanistic approach sought to glimpse the whole person, not just fragmented parts of the personality or isolated cognitions. Humanism focused on uniquely human issues, such as free will, personal growth, self-actualization, self-identity, death, aloneness, freedom, and meaning. It emphasized subjective meaning, rejection of determinism, and concern for positive growth rather than pathology. Some founders of the humanistic school of thought were American psychologists Abraham Maslow, who formulated a hierarchy of human needs, and Carl Rogers, who created and developed client-centered therapy. Later, positive psychology opened up humanistic themes to scientific modes of exploration.

The "American Association for Humanistic Psychology", formed in 1963, declared:

Humanistic psychology is primarily an orientation toward the whole of psychology rather than a distinct area or school. It stands for respect for the worth of persons, respect for differences of approach, open-mindedness as to acceptable methods, and interest in exploration of new aspects of human behavior. As a "third force" in contemporary psychology, it is concerned with topics having little place in existing theories and systems: e.g., love, creativity, self, growth, organism, basic need-gratification, self-actualization, higher values, being, becoming, spontaneity, play, humor, affection, naturalness, warmth, ego-transcendence, objectivity, autonomy, responsibility, meaning, fair-play, transcendental experience, peak experience, courage, and related concepts.

In the 1950s and 1960s, influenced by philosophers Søren Kierkegaard and Martin Heidegger and, psychoanalytically trained American psychologist Rollo May pioneered an existential branch of psychology, which included existential psychotherapy: a method based on the belief that inner conflict within a person is due to that individual's confrontation with the givens of existence. Swiss psychoanalyst Ludwig Binswanger and American psychologist George Kelly may also be said to belong to the existential school. Existential psychologists differed from more "humanistic" psychologists in their relatively neutral view of human nature and their relatively positive assessment of anxiety. Existential psychologists emphasized the humanistic themes of death, free will, and meaning, suggesting that meaning can be shaped by myths, or narrative patterns, and that it can be encouraged by an acceptance of the free will requisite to an authentic, albeit often anxious, regard for death and other future prospects.

Austrian existential psychiatrist and Holocaust survivor Viktor Frankl drew evidence of meaning's therapeutic power from reflections garnered from his own internment. He created a variation of existential psychotherapy called logotherapy, a type of existentialist analysis that focuses on a "will to meaning" (in one's life), as opposed to Adler's Nietzschean doctrine of "will to power" or Freud's "will to pleasure".

Personality psychology is concerned with enduring patterns of behavior, thought, and emotion—commonly referred to as personality—in individuals. Theories of personality vary across different psychological schools and orientations. They carry different assumptions about such issues as the role of the unconscious and the importance of childhood experience. According to Freud, personality is based on the dynamic interactions of the id, ego, and super-ego. In order to develop a taxonomy of personality constructs, trait theorists, in contrast, attempt to describe the personality sphere in terms of a discrete number of key traits using the statistical data-reduction method of factor analysis. Although the number of proposed traits has varied widely, an early biologically-based model proposed by Hans Eysenck, the 3rd mostly highly cited psychologist of the 20th Century (after Freud, and Piaget respectively), suggested that at least three major trait constructs are necessary to describe human personality structure: extraversion–introversion, neuroticism-stability, and psychoticism-normality. Raymond Cattell, the 7th most highly cited psychologist of the 20th Century (based on the scientific peer-reviewed journal literature) empirically derived a theory of 16 personality factors at the primary-factor level, and up to 8 broader second-stratum factors (at the Eysenckian level of analysis), rather than the "Big Five" dimensions. Dimensional models of personality are receiving increasing support, and a version of dimensional assessment has been included in the DSM-V. However, despite a plethora of research into the various versions of the "Big Five" personality dimensions, it appears necessary to move on from static conceptualizations of personality structure to a more dynamic orientation, whereby it is acknowledged that personality constructs are subject to learning and change across the lifespan.

An early example of personality assessment was the Woodworth Personal Data Sheet, constructed during World War I. The popular, although psychometrically inadequate Myers–Briggs Type Indicator sought to assess individuals' "personality types" according to the personality theories of Carl Jung. Behaviorist resistance to introspection led to the development of the Strong Vocational Interest Blank and Minnesota Multiphasic Personality Inventory (MMPI), in an attempt to ask empirical questions that focused less on the psychodynamics of the respondent. However, the MMPI has been subjected to critical scrutiny, given that it adhered to archaic psychiatric nosology, and since it required individuals to provide subjective, introspective responses to the hundreds of items pertaining to psychopathology.

Study of the unconscious mind, a part of the psyche outside the awareness of the individual which nevertheless influenced thoughts and behavior was a hallmark of early psychology. In one of the first psychology experiments conducted in the United States, C.S. Peirce and Joseph Jastrow found in 1884 that subjects could choose the minutely heavier of two weights even if consciously uncertain of the difference. Freud popularized this concept, with terms like Freudian slip entering popular culture, to mean an uncensored intrusion of unconscious thought into one's speech and action. His 1901 text "The Psychopathology of Everyday Life" catalogues hundreds of everyday events which Freud explains in terms of unconscious influence. Pierre Janet advanced the idea of a subconscious mind, which could contain autonomous mental elements unavailable to the scrutiny of the subject.

Behaviorism notwithstanding, the unconscious mind has maintained its importance in psychology. Cognitive psychologists have used a "filter" model of attention, according to which much information processing takes place below the threshold of consciousness, and only certain processes, limited by nature and by simultaneous quantity, make their way through the filter. Copious research has shown that subconscious "priming" of certain ideas can covertly influence thoughts and behavior. A significant hurdle in this research is proving that a subject's conscious mind has not grasped a certain stimulus, due to the unreliability of self-reporting. For this reason, some psychologists prefer to distinguish between "implicit" and "explicit" memory. In another approach, one can also describe a subliminal stimulus as meeting an "objective" but not a "subjective" threshold.

The automaticity model, which became widespread following exposition by John Bargh and others in the 1980s, describes sophisticated processes for executing goals which can be selected and performed over an extended duration without conscious awareness. Some experimental data suggests that the brain begins to consider taking actions before the mind becomes aware of them. This influence of unconscious forces on people's choices naturally bears on philosophical questions free will. John Bargh, Daniel Wegner, and Ellen Langer are some prominent contemporary psychologists who describe free will as an illusion.

Psychologists such as William James initially used the term "motivation" to refer to intention, in a sense similar to the concept of "will" in European philosophy. With the steady rise of Darwinian and Freudian thinking, instinct also came to be seen as a primary source of motivation. According to drive theory, the forces of instinct combine into a single source of energy which exerts a constant influence. Psychoanalysis, like biology, regarded these forces as physical demands made by the organism on the nervous system. However, they believed that these forces, especially the sexual instincts, could become entangled and transmuted within the psyche. Classical psychoanalysis conceives of a struggle between the pleasure principle and the reality principle, roughly corresponding to id and ego. Later, in "Beyond the Pleasure Principle", Freud introduced the concept of the "death drive", a compulsion towards aggression, destruction, and psychic repetition of traumatic events. Meanwhile, behaviorist researchers used simple dichotomous models (pleasure/pain, reward/punishment) and well-established principles such as the idea that a thirsty creature will take pleasure in drinking. Clark Hull formalized the latter idea with his drive reduction model.

Hunger, thirst, fear, sexual desire, and thermoregulation all seem to constitute fundamental motivations for animals. Humans also seem to exhibit a more complex set of motivations—though theoretically these could be explained as resulting from primordial instincts—including desires for belonging, self-image, self-consistency, truth, love, and control.

Motivation can be modulated or manipulated in many different ways. Researchers have found that eating, for example, depends not only on the organism's fundamental need for homeostasis—an important factor causing the experience of hunger—but also on circadian rhythms, food availability, food palatability, and cost. Abstract motivations are also malleable, as evidenced by such phenomena as "goal contagion": the adoption of goals, sometimes unconsciously, based on inferences about the goals of others. Vohs and Baumeister suggest that contrary to the need-desire-fulfilment cycle of animal instincts, human motivations sometimes obey a "getting begets wanting" rule: the more you get a reward such as self-esteem, love, drugs, or money, the more you want it. They suggest that this principle can even apply to food, drink, sex, and sleep.

Mainly focusing on the development of the human mind through the life span, developmental psychology seeks to understand how people come to perceive, understand, and act within the world and how these processes change as they age. This may focus on cognitive, affective, moral, social, or neural development. Researchers who study children use a number of unique research methods to make observations in natural settings or to engage them in experimental tasks. Such tasks often resemble specially designed games and activities that are both enjoyable for the child and scientifically useful, and researchers have even devised clever methods to study the mental processes of infants. In addition to studying children, developmental psychologists also study aging and processes throughout the life span, especially at other times of rapid change (such as adolescence and old age). Developmental psychologists draw on the full range of psychological theories to inform their research.

All researched psychological traits are influenced by both genes and environment, to varying degrees. These two sources of influence are often confounded in observational research of individuals or families. An example is the transmission of depression from a depressed mother to her offspring. Theory may hold that the offspring, by virtue of having a depressed mother in his or her (the offspring's) environment, is at risk for developing depression. However, risk for depression is also influenced to some extent by genes. The mother may both carry genes that contribute to her depression but will also have passed those genes on to her offspring thus increasing the offspring's risk for depression. Genes and environment in this simple transmission model are completely confounded. Experimental and quasi-experimental behavioral genetic research uses genetic methodologies to disentangle this confound and understand the nature and origins of individual differences in behavior. Traditionally this research has been conducted using twin studies and adoption studies, two designs where genetic and environmental influences can be partially un-confounded. More recently, the availability of microarray molecular genetic or genome sequencing technologies allows researchers to measure participant DNA variation directly, and test whether individual genetic variants within genes are associated with psychological traits and psychopathology through methods including genome-wide association studies. One goal of such research is similar to that in positional cloning and its success in Huntington's: once a causal gene is discovered biological research can be conducted to understand how that gene influences the phenotype. One major result of genetic association studies is the general finding that psychological traits and psychopathology, as well as complex medical diseases, are highly polygenic, where a large number (on the order of hundreds to thousands) of genetic variants, each of small effect, contribute to individual differences in the behavioral trait or propensity to the disorder. Active research continues to understand the genetic and environmental bases of behavior and their interaction.

Psychology encompasses many subfields and includes different approaches to the study of mental processes and behavior:

Psychological testing has ancient origins, such as examinations for the Chinese civil service dating back to 2200 BC. Written exams began during the Han dynasty (202 BC – AD 200). By 1370, the Chinese system required a stratified series of tests, involving essay writing and knowledge of diverse topics. The system was ended in 1906. In Europe, mental assessment took a more physiological approach, with theories of physiognomy—judgment of character based on the face—described by Aristotle in 4th century BC Greece. Physiognomy remained current through the Enlightenment, and added the doctrine of phrenology: a study of mind and intelligence based on simple assessment of neuroanatomy.

When experimental psychology came to Britain, Francis Galton was a leading practitioner, and, with his procedures for measuring reaction time and sensation, is considered an inventor of modern mental testing (also known as "psychometrics"). James McKeen Cattell, a student of Wundt and Galton, brought the concept to the United States, and in fact coined the term "mental test". In 1901, Cattell's student Clark Wissler published discouraging results, suggesting that mental testing of Columbia and Barnard students failed to predict their academic performance. In response to 1904 orders from the Minister of Public Instruction, French psychologists Alfred Binet and Théodore Simon elaborated a new test of intelligence in 1905–1911, using a range of questions diverse in their nature and difficulty. Binet and Simon introduced the concept of mental age and referred to the lowest scorers on their test as "idiots". Henry H. Goddard put the Binet-Simon scale to work and introduced classifications of mental level such as "imbecile" and "feebleminded". In 1916 (after Binet's death), Stanford professor Lewis M. Terman modified the Binet-Simon scale (renamed the Stanford–Binet scale) and introduced the intelligence quotient as a score report. From this test, Terman concluded that mental retardation "represents the level of intelligence which is very, very common among Spanish-Indians and Mexican families of the Southwest and also among negroes. Their dullness seems to be racial."

Following the Army Alpha and Army Beta tests for soldiers in World War I, mental testing became popular in the US, where it was soon applied to school children. The federally created National Intelligence Test was administered to 7 million children in the 1920s, and in 1926 the College Entrance Examination Board created the Scholastic Aptitude Test to standardize college admissions. The results of intelligence tests were used to argue for segregated schools and economic functions—i.e. the preferential training of Black Americans for manual labor. These practices were criticized by black intellectuals such a Horace Mann Bond and Allison Davis. Eugenicists used mental testing to justify and organize compulsory sterilization of individuals classified as mentally retarded. In the United States, tens of thousands of men and women were sterilized. Setting a precedent which has never been overturned, the U.S. Supreme Court affirmed the constitutionality of this practice in the 1907 case "Buck v. Bell".

Today mental testing is a routine phenomenon for people of all ages in Western societies. Modern testing aspires to criteria including standardization of procedure, consistency of results, output of an interpretable score, statistical norms describing population outcomes, and, ideally, effective prediction of behavior and life outcomes outside of testing situations.

The provision of psychological health services is generally called "clinical psychology" in the U.S. The definitions of this term are various and may include school psychology and counseling psychology. Practitioners typically includes people who have graduated from doctoral programs in clinical psychology but may also include others. In Canada, the above groups usually fall within the larger category of "professional psychology". In Canada and the US, practitioners get bachelor's degrees and doctorates, then spend one year in an internship and one year in postdoctoral education. In Mexico and most other Latin American and European countries, psychologists do not get bachelor's and doctorate degrees; instead, they take a three-year professional course following high school. Clinical psychology is at present the largest specialization within psychology. It includes the study and application of psychology for the purpose of understanding, preventing, and relieving psychologically based distress, dysfunction or mental illness and to promote subjective well-being and personal development. Central to its practice are psychological assessment and psychotherapy although clinical psychologists may also engage in research, teaching, consultation, forensic testimony, and program development and administration.

Credit for the first psychology clinic in the United States typically goes to Lightner Witmer, who established his practice in Philadelphia in 1896. Another modern psychotherapist was Morton Prince. For the most part, in the first part of the twentieth century, most mental health care in the United States was performed by specialized medical doctors called psychiatrists. Psychology entered the field with its refinements of mental testing, which promised to improve diagnosis of mental problems. For their part, some psychiatrists became interested in using psychoanalysis and other forms of psychodynamic psychotherapy to understand and treat the mentally ill. In this type of treatment, a specially trained therapist develops a close relationship with the patient, who discusses wishes, dreams, social relationships, and other aspects of mental life. The therapist seeks to uncover repressed material and to understand why the patient creates defenses against certain thoughts and feelings. An important aspect of the therapeutic relationship is transference, in which deep unconscious feelings in a patient reorient themselves and become manifest in relation to the therapist.

Psychiatric psychotherapy blurred the distinction between psychiatry and psychology, and this trend continued with the rise of community mental health facilities and behavioral therapy, a thoroughly non-psychodynamic model which used behaviorist learning theory to change the actions of patients. A key aspect of behavior therapy is empirical evaluation of the treatment's effectiveness. In the 1970s, cognitive-behavior therapy arose, using similar methods and now including the cognitive constructs which had gained popularity in theoretical psychology. A key practice in behavioral and cognitive-behavioral therapy is exposing patients to things they fear, based on the premise that their responses (fear, panic, anxiety) can be deconditioned.

Mental health care today involves psychologists and social workers in increasing numbers. In 1977, National Institute of Mental Health director Bertram Brown described this shift as a source of "intense competition and role confusion". Graduate programs issuing doctorates in psychology (PhD or PsyD) emerged in the 1950s and underwent rapid increase through the 1980s. This degree is intended to train practitioners who might conduct scientific research.

Some clinical psychologists may focus on the clinical management of patients with brain injury—this area is known as clinical neuropsychology. In many countries, clinical psychology is a regulated mental health profession. The emerging field of "disaster psychology" (see crisis intervention) involves professionals who respond to large-scale traumatic events.

The work performed by clinical psychologists tends to be influenced by various therapeutic approaches, all of which involve a formal relationship between professional and client (usually an individual, couple, family, or small group). Typically, these approaches encourage new ways of thinking, feeling, or behaving. Four major theoretical perspectives are psychodynamic, cognitive behavioral, existential–humanistic, and systems or family therapy. There has been a growing movement to integrate the various therapeutic approaches, especially with an increased understanding of issues regarding culture, gender, spirituality, and sexual orientation. With the advent of more robust research findings regarding psychotherapy, there is evidence that most of the major therapies have equal effectiveness, with the key common element being a strong therapeutic alliance. Because of this, more training programs and psychologists are now adopting an eclectic therapeutic orientation.

Diagnosis in clinical psychology usually follows the "Diagnostic and Statistical Manual of Mental Disorders" (DSM), a handbook first published by the American Psychiatric Association in 1952. New editions over time have increased in size and focused more on medical language. The study of mental illnesses is called abnormal psychology.

Educational psychology is the study of how humans learn in educational settings, the effectiveness of educational interventions, the psychology of teaching, and the social psychology of schools as organizations. The work of child psychologists such as Lev Vygotsky, Jean Piaget, and Jerome Bruner has been influential in creating teaching methods and educational practices. Educational psychology is often included in teacher education programs in places such as North America, Australia, and New Zealand.

School psychology combines principles from educational psychology and clinical psychology to understand and treat students with learning disabilities; to foster the intellectual growth of gifted students; to facilitate prosocial behaviors in adolescents; and otherwise to promote safe, supportive, and effective learning environments. School psychologists are trained in educational and behavioral assessment, intervention, prevention, and consultation, and many have extensive training in research.

Industrialists soon brought the nascent field of psychology to bear on the study of scientific management techniques for improving workplace efficiency. This field was at first called "economic psychology" or "business psychology"; later, "industrial psychology", "employment psychology", or "psychotechnology". An important early study examined workers at Western Electric's Hawthorne plant in Cicero, Illinois from 1924–1932. With funding from the Laura Spelman Rockefeller Fund and guidance from Australian psychologist Elton Mayo, Western Electric experimented on thousands of factory workers to assess their responses to illumination, breaks, food, and wages. The researchers came to focus on workers' responses to observation itself, and the term Hawthorne effect is now used to describe the fact that people work harder when they think they're being watched.

The name industrial and organizational psychology (I–O) arose in the 1960s and became enshrined as the Society for Industrial and Organizational Psychology, Division 14 of the American Psychological Association, in 1973. The goal is to optimize human potential in the workplace. Personnel psychology, a subfield of I–O psychology, applies the methods and principles of psychology in selecting and evaluating workers. I–O psychology's other subfield, organizational psychology, examines the effects of work environments and management styles on worker motivation, job satisfaction, and productivity. The majority of I–O psychologists work outside of academia, for private and public organizations and as consultants. A psychology consultant working in business today might expect to provide executives with information and ideas about their industry, their target markets, and the organization of their company.

One role for psychologists in the military is to evaluate and counsel soldiers and other personnel. In the U.S., this function began during World War I, when Robert Yerkes established the School of Military Psychology at Fort Oglethorpe in Georgia, to provide psychological training for military staff military. Today, U.S Army psychology includes psychological screening, clinical psychotherapy, suicide prevention, and treatment for post-traumatic stress, as well as other aspects of health and workplace psychology such as smoking cessation.

Psychologists may also work on a diverse set of campaigns known broadly as psychological warfare. Psychological warfare chiefly involves the use of propaganda to influence enemy soldiers and civilians. In the case of so-called black propaganda the propaganda is designed to seem like it originates from a different source. The CIA's MKULTRA program involved more individualized efforts at mind control, involving techniques such as hypnosis, torture, and covert involuntary administration of LSD. The U.S. military used the name Psychological Operations (PSYOP) until 2010, when these were reclassified as Military Information Support Operations (MISO), part of Information Operations (IO). Psychologists are sometimes involved in assisting the interrogation and torture of suspects, though this has sometimes been denied by those involved and sometimes opposed by others.

Medical facilities increasingly employ psychologists to perform various roles. A prominent aspect of health psychology is the psychoeducation of patients: instructing them in how to follow a medical regimen. Health psychologists can also educate doctors and conduct research on patient compliance.

Psychologists in the field of public health use a wide variety of interventions to influence human behavior. These range from public relations campaigns and outreach to governmental laws and policies. Psychologists study the composite influence of all these different tools in an effort to influence whole populations of people.

Black American psychologists Kenneth and Mamie Clark studied the psychological impact of segregation and testified with their findings in the desegregation case "Brown v. Board of Education" (1954).

Positive psychology is the study of factors which contribute to human happiness and well-being, focusing more on people who are currently healthy. In 2010, "Clinical Psychological Review" published a special issue devoted to positive psychological interventions, such as gratitude journaling and the physical expression of gratitude. Positive psychological interventions have been limited in scope, but their effects are thought to be superior to that of placebos, especially with regard to helping people with body image problems.

Quantitative psychological research lends itself to the statistical testing of hypotheses. Although the field makes abundant use of randomized and controlled experiments in laboratory settings, such research can only assess a limited range of short-term phenomena. Thus, psychologists also rely on creative statistical methods to glean knowledge from clinical trials and population data. These include the Pearson product–moment correlation coefficient, the analysis of variance, multiple linear regression, logistic regression, structural equation modeling, and hierarchical linear modeling. The measurement and operationalization of important constructs is an essential part of these research designs

A true experiment with random allocation of subjects to conditions allows researchers to make strong inferences about causal relationships. In an experiment, the researcher alters parameters of influence, called independent variables, and measures resulting changes of interest, called dependent variables. Prototypical experimental research is conducted in a laboratory with a carefully controlled environment.

Repeated-measures experiments are those which take place through intervention on multiple occasions. In research on the effectiveness of psychotherapy, experimenters often compare a given treatment with placebo treatments, or compare different treatments against each other. Treatment type is the independent variable. The dependent variables are outcomes, ideally assessed in several ways by different professionals. Using crossover design, researchers can further increase the strength of their results by testing both of two treatments on two groups of subjects.

Quasi-experimental design refers especially to situations precluding random assignment to different conditions. Researchers can use common sense to consider how much the nonrandom assignment threatens the study's validity. For example, in research on the best way to affect reading achievement in the first three grades of school, school administrators may not permit educational psychologists to randomly assign children to phonics and whole language classrooms, in which case the psychologists must work with preexisting classroom assignments. Psychologists will compare the achievement of children attending phonics and whole language classes.

Experimental researchers typically use a statistical hypothesis testing model which involves making predictions before conducting the experiment, then assessing how well the data supports the predictions. (These predictions may originate from a more abstract scientific hypothesis about how the phenomenon under study actually works.) Analysis of variance (ANOVA) statistical techniques are used to distinguish unique results of the experiment from the null hypothesis that variations result from random fluctuations in data. In psychology, the widely used standard ascribes statistical significance to results which have less than 5% probability of being explained by random variation.

Statistical surveys are used in psychology for measuring attitudes and traits, monitoring changes in mood, checking the validity of experimental manipulations, and for other psychological topics. Most commonly, psychologists use paper-and-pencil surveys. However, surveys are also conducted over the phone or through e-mail. Web-based surveys are increasingly used to conveniently reach many subjects.

Neuropsychological tests, such as the Wechsler scales and Wisconsin Card Sorting Test, are mostly questionnaires or simple tasks used which assess a specific type of mental function in the respondent. These can be used in experiments, as in the case of lesion experiments evaluating the results of damage to a specific part of the brain.

Observational studies analyze uncontrolled data in search of correlations; multivariate statistics are typically used to interpret the more complex situation. Cross-sectional observational studies use data from a single point in time, whereas longitudinal studies are used to study trends across the life span. Longitudinal studies track the same people, and therefore detect more individual, rather than cultural, differences. However, they suffer from lack of controls and from confounding factors such as "selective attrition" (the bias introduced when a certain type of subject disproportionately leaves a study).

Exploratory data analysis refers to a variety of practices which researchers can use to visualize and analyze existing sets of data. In Peirce's three modes of inference, exploratory data analysis corresponds to abduction, or hypothesis formation. Meta-analysis is the technique of integrating the results from multiple studies and interpreting the statistical properties of the pooled dataset.

A classic and popular tool used to relate mental and neural activity is the electroencephalogram (EEG), a technique using amplified electrodes on a person's scalp to measure voltage changes in different parts of the brain. Hans Berger, the first researcher to use EEG on an unopened skull, quickly found that brains exhibit signature "brain waves": electric oscillations which correspond to different states of consciousness. Researchers subsequently refined statistical methods for synthesizing the electrode data, and identified unique brain wave patterns such as the delta wave observed during non-REM sleep.

Newer functional neuroimaging techniques include functional magnetic resonance imaging and positron emission tomography, both of which track the flow of blood through the brain. These technologies provide more localized information about activity in the brain and create representations of the brain with widespread appeal. They also provide insight which avoids the classic problems of subjective self-reporting. It remains challenging to draw hard conclusions about where in the brain specific thoughts originate—or even how usefully such localization corresponds with reality. However, neuroimaging has delivered unmistakable results showing the existence of correlations between mind and brain. Some of these draw on a systemic neural network model rather than a localized function model.

Psychiatric interventions such as transcranial magnetic stimulation and drugs also provide information about brain–mind interactions. Psychopharmacology is the study of drug-induced mental effects.

Computational modeling is a tool used in mathematical psychology and cognitive psychology to simulate behavior. This method has several advantages. Since modern computers process information quickly, simulations can be run in a short time, allowing for high statistical power. Modeling also allows psychologists to visualize hypotheses about the functional organization of mental events that couldn't be directly observed in a human. Computational neuroscience uses mathematical models to simulate the brain. Another method is symbolic modeling, which represents many mental objects using variables and rules. Other types of modeling include dynamic systems and stochastic modeling.

Animal experiments aid in investigating many aspects of human psychology, including perception, emotion, learning, memory, and thought, to name a few. In the 1890s, Russian physiologist Ivan Pavlov famously used dogs to demonstrate classical conditioning. Non-human primates, cats, dogs, pigeons, rats, and other rodents are often used in psychological experiments. Ideally, controlled experiments introduce only one independent variable at a time, in order to ascertain its unique effects upon dependent variables. These conditions are approximated best in laboratory settings. In contrast, human environments and genetic backgrounds vary so widely, and depend upon so many factors, that it is difficult to control important variables for human subjects. There are pitfalls in generalizing findings from animal studies to humans through animal models.

Comparative psychology refers to the scientific study of the behavior and mental processes of non-human animals, especially as these relate to the phylogenetic history, adaptive significance, and development of behavior. Research in this area explores the behavior of many species, from insects to primates. It is closely related to other disciplines that study animal behavior such as ethology. Research in comparative psychology sometimes appears to shed light on human behavior, but some attempts to connect the two have been quite controversial, for example the Sociobiology of E.O. Wilson. Animal models are often used to study neural processes related to human behavior, e.g. in cognitive neuroscience.

Research designed to answer questions about the current state of affairs such as the thoughts, feelings, and behaviors of individuals is known as "descriptive research". Descriptive research can be qualitative or quantitative in orientation. "Qualitative research" is descriptive research that is focused on observing and describing events as they occur, with the goal of capturing all of the richness of everyday behavior and with the hope of discovering and understanding phenomena that might have been missed if only more cursory examinations have been made.

Qualitative psychological research methods include interviews, first-hand observation, and participant observation. Creswell (2003) identifies five main possibilities for qualitative research, including narrative, phenomenology, ethnography, case study, and grounded theory. Qualitative researchers sometimes aim to enrich interpretations or critiques of symbols, subjective experiences, or social structures. Sometimes hermeneutic and critical aims can give rise to quantitative research, as in Erich Fromm's study of Nazi voting or Stanley Milgram's studies of obedience to authority.

Just as Jane Goodall studied chimpanzee social and family life by careful observation of chimpanzee behavior in the field, psychologists conduct naturalistic observation of ongoing human social, professional, and family life. Sometimes the participants are aware they are being observed, and other times the participants do not know they are being observed. Strict ethical guidelines must be followed when covert observation is being carried out.

Program Evaluation is a systematic method for collecting, analyzing, and using information to answer questions about projects, policies and programs, particularly about their effectiveness and efficiency. In both the public and private sectors, stakeholders often want to know whether the programs they are funding, implementing, voting for, receiving or objecting to are producing the intended effect. While program evaluation first focuses around this definition, important considerations often include how much the program costs per participant, how the program could be improved, whether the program is worthwhile, whether there are better alternatives, if there are unintended outcomes, and whether the program goals are appropriate and useful.

The field of metascience has revealed significant problems with the methodology of psychological research. Psychological research suffers from high bias, low reproducibility, and widespread misuse of statistics. These finding have led to calls for reform from within and from outside the scientific community.

In 1959, statistician Theodore Sterling examined the results of psychological studies and discovered that 97% of them supported their initial hypotheses, implying a possible publication bias. Similarly, Fanelli (2010) found that 91.5% of psychiatry/psychology studies confirmed the effects they were looking for, and concluded that the odds of this happening (a positive result) was around five times higher than in fields such as space- or geosciences. Fanelli argues that this is because researchers in "softer" sciences have fewer constraints to their conscious and unconscious biases.

Over the subsequent few years, a replication crisis in psychology was identified, where it was publicly noted that many notable findings in the field had not been replicated and with some researchers being accused of outright fraud in their results. More systematic efforts to assess the extent of the problem, such as the Reproducibility Project of the Center for Open Science, found that as many as two-thirds of highly publicized findings in psychology had failed to be replicated, with reproducibility being generally stronger in studies and journals representing cognitive psychology than social psychology topics, and the subfields of differential psychology (including general intelligence and Big Five personality traits research), behavioral genetics (except for candidate gene and candidate gene-by-environment interaction research on behavior and mental illness), and the related field of behavioral economics being largely unaffected by the replication crisis. Other subfields of psychology that have been implicated by the replication crisis are clinical psychology, developmental psychology (particularly cognitive and personality development), and a field closely related to psychology that has also been implicated is educational research.

Focus on the replication crisis has led to other renewed efforts in the discipline to re-test important findings, and in response to concerns about publication bias and "p"-hacking, more than 140 psychology journals have adopted result-blind peer review where studies are accepted not on the basis of their findings and after the studies are completed, but before the studies are conducted and upon the basis of the methodological rigor of their experimental designs and the theoretical justifications for their statistical analysis techniques before data collection or analysis is done. In addition, large-scale collaborations between researchers working in multiple labs in different countries and that regularly make their data openly available for different researchers to assess have become much more common in the field. Early analysis of such reforms has estimated that 61 percent of result-blind studies have led to null results, in contrast to an estimated 5 to 20 percent in earlier research.

Some critics view statistical hypothesis testing as misplaced. Psychologist and statistician Jacob Cohen wrote in 1994 that psychologists routinely confuse statistical significance with practical importance, enthusiastically reporting great certainty in unimportant facts. Some psychologists have responded with an increased use of effect size statistics, rather than sole reliance on p-values.

In 2008, Arnett pointed out that most articles in American Psychological Association journals were about US populations when U.S. citizens are only 5% of the world's population. He complained that psychologists had no basis for assuming psychological processes to be universal and generalizing research findings to the rest of the global population. In 2010, Henrich, Heine, and Norenzayan reported a systemic bias in conducting psychology studies with participants from "WEIRD" (western, educated, industrialized, rich and democratic) societies. Although only 1/8 people worldwide live in regions that fall into the WEIRD classification, the researchers claimed that 60–90% of psychology studies are performed on participants from these areas. The article gave examples of results that differ significantly between people from WEIRD and tribal cultures, including the Müller-Lyer illusion. Arnett (2008), Altmaier and Hall (2008), and Morgan-Consoli et al. (2018) saw the Western bias in research and theory as a serious problem considering psychologists are increasingly applying psychological principles developed in WEIRD regions in their research, clinical work, and consultation with populations around the world. In 2018, Rad, Martingano & Ginges showed that nearly a decade after Henrich et al.'s paper, over 80% of the samples used in studies published in the journal, Psychological Science, were from the WEIRD population. Moreover, their analysis showed that several studies did not fully disclose the origin of their samples, and the authors offer a set of recommendations to editors and reviewers to reduce the WEIRD bias. 

From an anthropological perspective, scholars applied the WEIRD model to European history, arguing that a powerful Christian Church forced a radical change away from incest and cousin marriages that undermined the role of clans and created individualism in Europe by 1500 CE. They argue that a distinctive Western psychology thus emerged that valued agency, autonomy, and kindness toward stranger. Historians were not involved in that project, and have since pointed out its historical fallacies regarding an all-powerful Church at too early a point in time, and a rejection of cousin marriage that did not happen.

Some observers perceive a gap between scientific theory and its application—in particular, the application of unsupported or unsound clinical practices. Critics say there has been an increase in the number of mental health training programs that do not instill scientific competence. Practices such as "facilitated communication for infantile autism"; memory-recovery techniques including body work; and other therapies, such as rebirthing and reparenting, may be dubious or even dangerous, despite their popularity. In 1984, Allen Neuringer made a similar point regarding the experimental analysis of behavior. Psychologists, sometimes divided along the lines of laboratory vs. clinic, continue to debate these issues.

Ethical standards in the discipline have changed over time. Some famous past studies are today considered unethical and in violation of established codes the Canadian Code of Conduct for Research Involving Humans, and the Belmont Report).

The most important contemporary standards are informed and voluntary consent. After World War II, the Nuremberg Code was established because of Nazi abuses of experimental subjects. Later, most countries (and scientific journals) adopted the Declaration of Helsinki. In the U.S., the National Institutes of Health established the Institutional Review Board in 1966, and in 1974 adopted the National Research Act (HR 7724). All of these measures encouraged researchers to obtain informed consent from human participants in experimental studies. A number of influential studies led to the establishment of this rule; such studies included the MIT and Fernald School radioisotope studies, the Thalidomide tragedy, the Willowbrook hepatitis study, and Stanley Milgram's studies of obedience to authority.

University psychology departments have ethics committees dedicated to the rights and well-being of research subjects. Researchers in psychology must gain approval of their research projects before conducting any experiment to protect the interests of human participants and laboratory animals.

The ethics code of the American Psychological Association originated in 1951 as "Ethical Standards of Psychologists". This code has guided the formation of licensing laws in most American states. It has changed multiple times over the decades since its adoption. In 1989, the APA revised its policies on advertising and referral fees to negotiate the end of an investigation by the Federal Trade Commission. The 1992 incarnation was the first to distinguish between "aspirational" ethical standards and "enforceable" ones. Members of the public have a five-year window to file ethics complaints about APA members with the APA ethics committee; members of the APA have a three-year window.

Some of the ethical issues considered most important are the requirement to practice only within the area of competence, to maintain confidentiality with the patients, and to avoid sexual relations with them. Another important principle is informed consent, the idea that a patient or research subject must understand and freely choose a procedure they are undergoing. Some of the most common complaints against clinical psychologists include sexual misconduct, and involvement in child custody evaluations.

Current ethical guidelines state that using non-human animals for scientific purposes is only acceptable when the harm (physical or psychological) done to animals is outweighed by the benefits of the research. Keeping this in mind, psychologists can use certain research techniques on animals that could not be used on humans.




</doc>
<doc id="22923" url="https://en.wikipedia.org/wiki?curid=22923" title="PhpWiki">
PhpWiki

PhpWiki is a web-based wiki software application.
It began as a clone of WikiWikiWeb and was the first wiki written in PHP.
PhpWiki has been used to edit and format paper books for publication.

The first version, by Steve Wainstead, was released in December 1999. It was the first Wiki written in PHP to be publicly released. This version required PHP 3.x and only supported DBM files. 
It was a feature-for-feature reimplementation of the original WikiWikiWeb at c2.com.

In early 2000 Arno Hollosi added a second database library to allow running PhpWiki on MySQL.
From then on more features were added and contributions to the software increased, adding features such as a templating system, color diffs, rewrites of the rendering engine and much more. Arno was interested in running a wiki for the game Go.

Jeff Dairiki was the next major contributor, and soon headed the project for the next few years, followed up by Reini Urban up to 1.4, and then Marc-Etienne Vargenau since 1.5.

With version 1.4.0 Wikicreole 1.0 including additions and MediaWiki markup syntax are supported. In version 1.5.0 PHP 4 support was deprecated.



</doc>
<doc id="22926" url="https://en.wikipedia.org/wiki?curid=22926" title="Poetry">
Poetry

Poetry (derived from the Greek "poiesis", "making") is a form of literature that uses aesthetic and often rhythmic qualities of language—such as phonaesthetics, sound symbolism, and metre—to evoke meanings in addition to, or in place of, the prosaic ostensible meaning.

Poetry has a long history – dating back to prehistoric times with hunting poetry in Africa, and to panegyric and elegiac court poetry of the empires of the Nile, Niger, and Volta River valleys. Some of the earliest written poetry in Africa occurs among the Pyramid Texts written during the 25th century BCE. The earliest surviving Western Asian epic poetry, the "Epic of Gilgamesh", was written in Sumerian.

Early poems in the Eurasian continent evolved from folk songs such as the Chinese "Shijing"; or from a need to retell oral epics, as with the Sanskrit "Vedas", the Zoroastrian "Gathas", and the Homeric epics, the "Iliad" and the "Odyssey". Ancient Greek attempts to define poetry, such as Aristotle's "Poetics", focused on the uses of speech in rhetoric, drama, song, and comedy. Later attempts concentrated on features such as repetition, verse form, and rhyme, and emphasized the aesthetics which distinguish poetry from more objectively-informative prosaic writing.

Poetry uses forms and conventions to suggest differential interpretations of words, or to evoke emotive responses. Devices such as assonance, alliteration, onomatopoeia, and rhythm may convey musical or incantatory effects. The use of ambiguity, symbolism, irony, and other stylistic elements of poetic diction often leaves a poem open to multiple interpretations. Similarly, figures of speech such as metaphor, simile, and metonymy establish a resonance between otherwise disparate images—a layering of meanings, forming connections previously not perceived. Kindred forms of resonance may exist, between individual verses, in their patterns of rhyme or rhythm.

Some poetry types are unique to particular cultures and genres and respond to characteristics of the language in which the poet writes. Readers accustomed to identifying poetry with Dante, Goethe, Mickiewicz, or Rumi may think of it as written in lines based on rhyme and regular meter. There are, however, traditions, such as Biblical poetry, that use other means to create rhythm and euphony. Much modern poetry reflects a critique of poetic tradition, testing the principle of euphony itself or altogether forgoing rhyme or set rhythm.
In an increasingly globalized world, poets often adapt forms, styles, and techniques from diverse cultures and languages.

A Western cultural tradition (which extends at least from Homer to Rilke) associates the production of poetry with inspiration – often by a Muse (either classical or contemporary).

Some scholars believe that the art of poetry may predate literacy.
Others, however, suggest that poetry did not necessarily predate writing.

The oldest surviving epic poem, the "Epic of Gilgamesh", dates from the 3rd millenniumBCE in Sumer (in Mesopotamia, now Iraq), and was written in cuneiform script on clay tablets and, later, on papyrus. A tablet #2461 dating to 2000BCE describes an annual rite in which the king symbolically married and mated with the goddess Inanna to ensure fertility and prosperity; some have labelled it the world's oldest love poem. An example of Egyptian epic poetry is "The Story of Sinuhe" (c. 1800 BCE).

Other ancient epic poetry includes the Greek epics, the "Iliad" and the "Odyssey"; the Avestan books, the "Gathic Avesta" and the "Yasna"; the Roman national epic, Virgil's "Aeneid" (written between 29 and 19 BCE); and the Indian epics, the "Ramayana" and the "Mahabharata". Epic poetry, including the "Odyssey", the "Gathas", and the Indian "Vedas", appears to have been composed in poetic form as an aid to memorization and oral transmission in ancient societies.

Other forms of poetry developed directly from folk songs. The earliest entries in the oldest extant collection of Chinese poetry, the "Shijing", were initially lyrics.

The efforts of ancient thinkers to determine what makes poetry distinctive as a form, and what distinguishes good poetry from bad, resulted in "poetics"—the study of the aesthetics of poetry. Some ancient societies, such as China's through her "Shijing" ("Classic of Poetry"), developed canons of poetic works that had ritual as well as aesthetic importance. More recently, thinkers have struggled to find a definition that could encompass formal differences as great as those between Chaucer's "Canterbury Tales" and Matsuo Bashō's "Oku no Hosomichi", as well as differences in content spanning Tanakh religious poetry, love poetry, and rap.

Classical thinkers in the West employed classification as a way to define and assess the quality of poetry. Notably, the existing fragments of Aristotle's "Poetics" describe three genres of poetry—the epic, the comic, and the tragic—and develop rules to distinguish the highest-quality poetry in each genre, based on the perceived underlying purposes of the genre. Later aestheticians identified three major genres: epic poetry, lyric poetry, and dramatic poetry, treating comedy and tragedy as subgenres of dramatic poetry.

Aristotle's work was influential throughout the Middle East during the Islamic Golden Age, as well as in Europe during the Renaissance. Later poets and aestheticians often distinguished poetry from, and defined it in opposition to prose, which they generally understood as writing with a proclivity to logical explication and a linear narrative structure.

This does not imply that poetry is illogical or lacks narration, but rather that poetry is an attempt to render the beautiful or sublime without the burden of engaging the logical or narrative thought-process. English Romantic poet John Keats termed this escape from logic "Negative capability". This "romantic" approach views form as a key element of successful poetry because form is abstract and distinct from the underlying notional logic. This approach remained influential into the 20th century.

During this period, there was also substantially more interaction among the various poetic traditions, in part due to the spread of European colonialism and the attendant rise in global trade. In addition to a boom in translation, during the Romantic period numerous ancient works were rediscovered.

Some 20th-century literary theorists rely less on the ostensible opposition of prose and poetry, instead focusing on the poet as simply one who creates using language, and poetry as what the poet creates. The underlying concept of the poet as creator is not uncommon, and some modernist poets essentially do not distinguish between the creation of a poem with words, and creative acts in other media. Yet other modernists challenge the very attempt to define poetry as misguided.

The rejection of traditional forms and structures for poetry that began in the first half of the 20th century coincided with a questioning of the purpose and meaning of traditional definitions of poetry and of distinctions between poetry and prose, particularly given examples of poetic prose and prosaic poetry. Numerous modernist poets have written in non-traditional forms or in what traditionally would have been considered prose, although their writing was generally infused with poetic diction and often with rhythm and tone established by non-metrical means. While there was a substantial formalist reaction within the modernist schools to the breakdown of structure, this reaction focused as much on the development of new formal structures and syntheses as on the revival of older forms and structures.

Postmodernism goes beyond modernism's emphasis on the creative role of the poet, to emphasize the role of the reader of a text (hermeneutics), and to highlight the complex cultural web within which a poem is read. Today, throughout the world, poetry often incorporates poetic form and diction from other cultures and from the past, further confounding attempts at definition and classification that once made sense within a tradition such as the Western canon.

The early 21st-century poetic tradition appears to continue to strongly orient itself to earlier precursor poetic traditions such as those initiated by Whitman, Emerson, and Wordsworth. The literary critic Geoffrey Hartman (1929–2016) used the phrase "the anxiety of demand" to describe the contemporary response to older poetic traditions as "being fearful that the fact no longer has a form", building on a trope introduced by Emerson. Emerson had maintained that in the debate concerning poetic structure where either "form" or "fact" could predominate, that one need simply "Ask the fact for the form." This has been challenged at various levels by other literary scholars such as Bloom (1930–2019), who has stated: "The generation of poets who stand together now, mature and ready to write the major American verse of the twenty-first century, may yet be seen as what Stevens called 'a great shadow's last embellishment,' the shadow being Emerson's."

Prosody is the study of the meter, rhythm, and intonation of a poem. Rhythm and meter are different, although closely related. Meter is the definitive pattern established for a verse (such as iambic pentameter), while rhythm is the actual sound that results from a line of poetry. Prosody also may be used more specifically to refer to the scanning of poetic lines to show meter.

The methods for creating poetic rhythm vary across languages and between poetic traditions. Languages are often described as having timing set primarily by accents, syllables, or moras, depending on how rhythm is established, though a language can be influenced by multiple approaches. Japanese is a mora-timed language. Latin, Catalan, French, Leonese, Galician and Spanish are called syllable-timed languages. Stress-timed languages include English, Russian and, generally, German. Varying intonation also affects how rhythm is perceived. Languages can rely on either pitch or tone. Some languages with a pitch accent are Vedic Sanskrit or Ancient Greek. Tonal languages include Chinese, Vietnamese and most Subsaharan languages.

Metrical rhythm generally involves precise arrangements of stresses or syllables into repeated patterns called feet within a line. In Modern English verse the pattern of stresses primarily differentiate feet, so rhythm based on meter in Modern English is most often founded on the pattern of stressed and unstressed syllables (alone or elided). In the classical languages, on the other hand, while the metrical units are similar, vowel length rather than stresses define the meter. Old English poetry used a metrical pattern involving varied numbers of syllables but a fixed number of strong stresses in each line.

The chief device of ancient Hebrew Biblical poetry, including many of the psalms, was "parallelism", a rhetorical structure in which successive lines reflected each other in grammatical structure, sound structure, notional content, or all three. Parallelism lent itself to antiphonal or call-and-response performance, which could also be reinforced by intonation. Thus, Biblical poetry relies much less on metrical feet to create rhythm, but instead creates rhythm based on much larger sound units of lines, phrases and sentences. Some classical poetry forms, such as Venpa of the Tamil language, had rigid grammars (to the point that they could be expressed as a context-free grammar) which ensured a rhythm.

Classical Chinese poetics, based on the tone system of Middle Chinese, recognized two kinds of tones: the level (平 "píng") tone and the oblique (仄 "zè") tones, a category consisting of the rising (上 "sháng") tone, the departing (去 "qù") tone and the entering (入 "rù") tone. Certain forms of poetry placed constraints on which syllables were required to be level and which oblique.

The formal patterns of meter used in Modern English verse to create rhythm no longer dominate contemporary English poetry. In the case of free verse, rhythm is often organized based on looser units of cadence rather than a regular meter. Robinson Jeffers, Marianne Moore, and William Carlos Williams are three notable poets who reject the idea that regular accentual meter is critical to English poetry. Jeffers experimented with sprung rhythm as an alternative to accentual rhythm.

In the Western poetic tradition, meters are customarily grouped according to a characteristic metrical foot and the number of feet per line. The number of metrical feet in a line are described using Greek terminology: tetrameter for four feet and hexameter for six feet, for example. Thus, "iambic pentameter" is a meter comprising five feet per line, in which the predominant kind of foot is the "iamb". This metric system originated in ancient Greek poetry, and was used by poets such as Pindar and Sappho, and by the great tragedians of Athens. Similarly, "dactylic hexameter", comprises six feet per line, of which the dominant kind of foot is the "dactyl". Dactylic hexameter was the traditional meter of Greek epic poetry, the earliest extant examples of which are the works of Homer and Hesiod. Iambic pentameter and dactylic hexameter were later used by a number of poets, including William Shakespeare and Henry Wadsworth Longfellow, respectively. The most common metrical feet in English are:

There are a wide range of names for other types of feet, right up to a choriamb, a four syllable metric foot with a stressed syllable followed by two unstressed syllables and closing with a stressed syllable. The choriamb is derived from some ancient Greek and Latin poetry. Languages which utilize vowel length or intonation rather than or in addition to syllabic accents in determining meter, such as Ottoman Turkish or Vedic, often have concepts similar to the iamb and dactyl to describe common combinations of long and short sounds.

Each of these types of feet has a certain "feel," whether alone or in combination with other feet. The iamb, for example, is the most natural form of rhythm in the English language, and generally produces a subtle but stable verse. Scanning meter can often show the basic or fundamental pattern underlying a verse, but does not show the varying degrees of stress, as well as the differing pitches and lengths of syllables.

There is debate over how useful a multiplicity of different "feet" is in describing meter. For example, Robert Pinsky has argued that while dactyls are important in classical verse, English dactylic verse uses dactyls very irregularly and can be better described based on patterns of iambs and anapests, feet which he considers natural to the language. Actual rhythm is significantly more complex than the basic scanned meter described above, and many scholars have sought to develop systems that would scan such complexity. Vladimir Nabokov noted that overlaid on top of the regular pattern of stressed and unstressed syllables in a line of verse was a separate pattern of accents resulting from the natural pitch of the spoken words, and suggested that the term "scud" be used to distinguish an unaccented stress from an accented stress.

Different traditions and genres of poetry tend to use different meters, ranging from the Shakespearean iambic pentameter and the Homeric dactylic hexameter to the anapestic tetrameter used in many nursery rhymes. However, a number of variations to the established meter are common, both to provide emphasis or attention to a given foot or line and to avoid boring repetition. For example, the stress in a foot may be inverted, a caesura (or pause) may be added (sometimes in place of a foot or stress), or the final foot in a line may be given a feminine ending to soften it or be replaced by a spondee to emphasize it and create a hard stop. Some patterns (such as iambic pentameter) tend to be fairly regular, while other patterns, such as dactylic hexameter, tend to be highly irregular. Regularity can vary between language. In addition, different patterns often develop distinctively in different languages, so that, for example, iambic tetrameter in Russian will generally reflect a regularity in the use of accents to reinforce the meter, which does not occur, or occurs to a much lesser extent, in English.

Some common metrical patterns, with notable examples of poets and poems who use them, include:

Rhyme, alliteration, assonance and consonance are ways of creating repetitive patterns of sound. They may be used as an independent structural element in a poem, to reinforce rhythmic patterns, or as an ornamental element. They can also carry a meaning separate from the repetitive sound patterns created. For example, Chaucer used heavy alliteration to mock Old English verse and to paint a character as archaic.

Rhyme consists of identical ("hard-rhyme") or similar ("soft-rhyme") sounds placed at the ends of lines or at predictable locations within lines ("internal rhyme"). Languages vary in the richness of their rhyming structures; Italian, for example, has a rich rhyming structure permitting maintenance of a limited set of rhymes throughout a lengthy poem. The richness results from word endings that follow regular forms. English, with its irregular word endings adopted from other languages, is less rich in rhyme. The degree of richness of a language's rhyming structures plays a substantial role in determining what poetic forms are commonly used in that language.

Alliteration is the repetition of letters or letter-sounds at the beginning of two or more words immediately succeeding each other, or at short intervals; or the recurrence of the same letter in accented parts of words. Alliteration and assonance played a key role in structuring early Germanic, Norse and Old English forms of poetry. The alliterative patterns of early Germanic poetry interweave meter and alliteration as a key part of their structure, so that the metrical pattern determines when the listener expects instances of alliteration to occur. This can be compared to an ornamental use of alliteration in most Modern European poetry, where alliterative patterns are not formal or carried through full stanzas. Alliteration is particularly useful in languages with less rich rhyming structures.

Assonance, where the use of similar vowel sounds within a word rather than similar sounds at the beginning or end of a word, was widely used in skaldic poetry but goes back to the Homeric epic. Because verbs carry much of the pitch in the English language, assonance can loosely evoke the tonal elements of Chinese poetry and so is useful in translating Chinese poetry. Consonance occurs where a consonant sound is repeated throughout a sentence without putting the sound only at the front of a word. Consonance provokes a more subtle effect than alliteration and so is less useful as a structural element.

In many languages, including modern European languages and Arabic, poets use rhyme in set patterns as a structural element for specific poetic forms, such as ballads, sonnets and rhyming couplets. However, the use of structural rhyme is not universal even within the European tradition. Much modern poetry avoids traditional rhyme schemes. Classical Greek and Latin poetry did not use rhyme. Rhyme entered European poetry in the High Middle Ages, in part under the influence of the Arabic language in Al Andalus (modern Spain). Arabic language poets used rhyme extensively from the first development of literary Arabic in the sixth century, as in their long, rhyming qasidas. Some rhyming schemes have become associated with a specific language, culture or period, while other rhyming schemes have achieved use across languages, cultures or time periods. Some forms of poetry carry a consistent and well-defined rhyming scheme, such as the chant royal or the rubaiyat, while other poetic forms have variable rhyme schemes.

Most rhyme schemes are described using letters that correspond to sets of rhymes, so if the first, second and fourth lines of a quatrain rhyme with each other and the third line do not rhyme, the quatrain is said to have an "aa-ba" rhyme scheme. This rhyme scheme is the one used, for example, in the rubaiyat form. Similarly, an "a-bb-a" quatrain (what is known as "enclosed rhyme") is used in such forms as the Petrarchan sonnet. Some types of more complicated rhyming schemes have developed names of their own, separate from the "a-bc" convention, such as the ottava rima and terza rima. The types and use of differing rhyming schemes are discussed further in the main article.

Poetic form is more flexible in modernist and post-modernist poetry and continues to be less structured than in previous literary eras. Many modern poets eschew recognizable structures or forms and write in free verse. But poetry remains distinguished from prose by its form; some regard for basic formal structures of poetry will be found in even the best free verse, however much such structures may appear to have been ignored. Similarly, in the best poetry written in classic styles there will be departures from strict form for emphasis or effect.

Among major structural elements used in poetry are the line, the stanza or verse paragraph, and larger combinations of stanzas or lines such as cantos. Also sometimes used are broader visual presentations of words and calligraphy. These basic units of poetic form are often combined into larger structures, called "poetic forms" or poetic modes (see the following section), as in the sonnet.

Poetry is often separated into lines on a page, in a process known as lineation. These lines may be based on the number of metrical feet or may emphasize a rhyming pattern at the ends of lines. Lines may serve other functions, particularly where the poem is not written in a formal metrical pattern. Lines can separate, compare or contrast thoughts expressed in different units, or can highlight a change in tone. See the article on line breaks for information about the division between lines.

Lines of poems are often organized into stanzas, which are denominated by the number of lines included. Thus a collection of two lines is a couplet (or distich), three lines a triplet (or tercet), four lines a quatrain, and so on. These lines may or may not relate to each other by rhyme or rhythm. For example, a couplet may be two lines with identical meters which rhyme or two lines held together by a common meter alone.

Other poems may be organized into verse paragraphs, in which regular rhymes with established rhythms are not used, but the poetic tone is instead established by a collection of rhythms, alliterations, and rhymes established in paragraph form. Many medieval poems were written in verse paragraphs, even where regular rhymes and rhythms were used.

In many forms of poetry, stanzas are interlocking, so that the rhyming scheme or other structural elements of one stanza determine those of succeeding stanzas. Examples of such interlocking stanzas include, for example, the ghazal and the villanelle, where a refrain (or, in the case of the villanelle, refrains) is established in the first stanza which then repeats in subsequent stanzas. Related to the use of interlocking stanzas is their use to separate thematic parts of a poem. For example, the strophe, antistrophe and epode of the ode form are often separated into one or more stanzas.

In some cases, particularly lengthier formal poetry such as some forms of epic poetry, stanzas themselves are constructed according to strict rules and then combined. In skaldic poetry, the dróttkvætt stanza had eight lines, each having three "lifts" produced with alliteration or assonance. In addition to two or three alliterations, the odd-numbered lines had partial rhyme of consonants with dissimilar vowels, not necessarily at the beginning of the word; the even lines contained internal rhyme in set syllables (not necessarily at the end of the word). Each half-line had exactly six syllables, and each line ended in a trochee. The arrangement of dróttkvætts followed far less rigid rules than the construction of the individual dróttkvætts.

Even before the advent of printing, the visual appearance of poetry often added meaning or depth. Acrostic poems conveyed meanings in the initial letters of lines or in letters at other specific places in a poem. In Arabic, Hebrew and Chinese poetry, the visual presentation of finely calligraphed poems has played an important part in the overall effect of many poems.

With the advent of printing, poets gained greater control over the mass-produced visual presentations of their work. Visual elements have become an important part of the poet's toolbox, and many poets have sought to use visual presentation for a wide range of purposes. Some Modernist poets have made the placement of individual lines or groups of lines on the page an integral part of the poem's composition. At times, this complements the poem's rhythm through visual caesuras of various lengths, or creates juxtapositions so as to accentuate meaning, ambiguity or irony, or simply to create an aesthetically pleasing form. In its most extreme form, this can lead to concrete poetry or asemic writing.

Poetic diction treats the manner in which language is used, and refers not only to the sound but also to the underlying meaning and its interaction with sound and form. Many languages and poetic forms have very specific poetic dictions, to the point where distinct grammars and dialects are used specifically for poetry. Registers in poetry can range from strict employment of ordinary speech patterns, as favoured in much late-20th-century prosody, through to highly ornate uses of language, as in medieval and Renaissance poetry.

Poetic diction can include rhetorical devices such as simile and metaphor, as well as tones of voice, such as irony. Aristotle wrote in the "Poetics" that "the greatest thing by far is to be a master of metaphor." Since the rise of Modernism, some poets have opted for a poetic diction that de-emphasizes rhetorical devices, attempting instead the direct presentation of things and experiences and the exploration of tone. On the other hand, Surrealists have pushed rhetorical devices to their limits, making frequent use of catachresis.

Allegorical stories are central to the poetic diction of many cultures, and were prominent in the West during classical times, the late Middle Ages and the Renaissance. "Aesop's Fables", repeatedly rendered in both verse and prose since first being recorded about 500 BCE, are perhaps the richest single source of allegorical poetry through the ages. Other notables examples include the "Roman de la Rose", a 13th-century French poem, William Langland's "Piers Ploughman" in the 14th century, and Jean de la Fontaine's "Fables" (influenced by Aesop's) in the 17th century. Rather than being fully allegorical, however, a poem may contain symbols or allusions that deepen the meaning or effect of its words without constructing a full allegory.

Another element of poetic diction can be the use of vivid imagery for effect. The juxtaposition of unexpected or impossible images is, for example, a particularly strong element in surrealist poetry and haiku. Vivid images are often endowed with symbolism or metaphor. Many poetic dictions use repetitive phrases for effect, either a short phrase (such as Homer's "rosy-fingered dawn" or "the wine-dark sea") or a longer refrain. Such repetition can add a somber tone to a poem, or can be laced with irony as the context of the words changes.

Specific poetic forms have been developed by many cultures. In more developed, closed or "received" poetic forms, the rhyming scheme, meter and other elements of a poem are based on sets of rules, ranging from the relatively loose rules that govern the construction of an elegy to the highly formalized structure of the ghazal or villanelle. Described below are some common forms of poetry widely used across a number of languages. Additional forms of poetry may be found in the discussions of the poetry of particular cultures or periods and in the glossary.

Among the most common forms of poetry, popular from the Late Middle Ages on, is the sonnet, which by the 13th century had become standardized as fourteen lines following a set rhyme scheme and logical structure. By the 14th century and the Italian Renaissance, the form had further crystallized under the pen of Petrarch, whose sonnets were translated in the 16th century by Sir Thomas Wyatt, who is credited with introducing the sonnet form into English literature. A traditional Italian or Petrarchan sonnet follows the rhyme scheme "ABBA, ABBA, CDECDE", though some variation, perhaps the most common being CDCDCD, especially within the final six lines (or "sestet"), is common. The English (or Shakespearean) sonnet follows the rhyme scheme "ABAB, CDCD, EFEF, GG", introducing a third quatrain (grouping of four lines), a final couplet, and a greater amount of variety with regard to rhyme than is usually found in its Italian predecessors. By convention, sonnets in English typically use iambic pentameter, while in the Romance languages, the hendecasyllable and Alexandrine are the most widely used meters.

Sonnets of all types often make use of a "volta", or "turn," a point in the poem at which an idea is turned on its head, a question is answered (or introduced), or the subject matter is further complicated. This "volta" can often take the form of a "but" statement contradicting or complicating the content of the earlier lines. In the Petrarchan sonnet, the turn tends to fall around the division between the first two quatrains and the sestet, while English sonnets usually place it at or near the beginning of the closing couplet.

Sonnets are particularly associated with high poetic diction, vivid imagery, and romantic love, largely due to the influence of Petrarch as well as of early English practitioners such as Edmund Spenser (who gave his name to the Spenserian sonnet), Michael Drayton, and Shakespeare, whose sonnets are among the most famous in English poetry, with twenty being included in the "Oxford Book of English Verse". However, the twists and turns associated with the "volta" allow for a logical flexibility applicable to many subjects. Poets from the earliest centuries of the sonnet to the present have utilized the form to address topics related to politics (John Milton, Percy Bysshe Shelley, Claude McKay), theology (John Donne, Gerard Manley Hopkins), war (Wilfred Owen, e.e. cummings), and gender and sexuality (Carol Ann Duffy). Further, postmodern authors such as Ted Berrigan and John Berryman have challenged the traditional definitions of the sonnet form, rendering entire sequences of "sonnets" that often lack rhyme, a clear logical progression, or even a consistent count of fourteen lines.

"Shi" () Is the main type of Classical Chinese poetry. Within this form of poetry the most important variations are "folk song" styled verse ("yuefu"), "old style" verse ("gushi"), "modern style" verse ("jintishi"). In all cases, rhyming is obligatory. The Yuefu is a folk ballad or a poem written in the folk ballad style, and the number of lines and the length of the lines could be irregular. For the other variations of "shi" poetry, generally either a four line (quatrain, or "jueju") or else an eight-line poem is normal; either way with the even numbered lines rhyming. The line length is scanned by an according number of characters (according to the convention that one character equals one syllable), and are predominantly either five or seven characters long, with a caesura before the final three syllables. The lines are generally end-stopped, considered as a series of couplets, and exhibit verbal parallelism as a key poetic device. The "old style" verse ("Gushi") is less formally strict than the "jintishi", or regulated verse, which, despite the name "new style" verse actually had its theoretical basis laid as far back as Shen Yue (441–513 CE), although not considered to have reached its full development until the time of Chen Zi'ang (661–702 CE). A good example of a poet known for his "Gushi" poems is Li Bai (701–762 CE). Among its other rules, the jintishi rules regulate the tonal variations within a poem, including the use of set patterns of the four tones of Middle Chinese. The basic form of jintishi (sushi) has eight lines in four couplets, with parallelism between the lines in the second and third couplets. The couplets with parallel lines contain contrasting content but an identical grammatical relationship between words. Jintishi often have a rich poetic diction, full of allusion, and can have a wide range of subject, including history and politics. One of the masters of the form was Du Fu (712–770 CE), who wrote during the Tang Dynasty (8th century).

The villanelle is a nineteen-line poem made up of five triplets with a closing quatrain; the poem is characterized by having two refrains, initially used in the first and third lines of the first stanza, and then alternately used at the close of each subsequent stanza until the final quatrain, which is concluded by the two refrains. The remaining lines of the poem have an a-b alternating rhyme. The villanelle has been used regularly in the English language since the late 19th century by such poets as Dylan Thomas, W. H. Auden, and Elizabeth Bishop.

A limerick is a poem that consists of five lines and is often humorous. Rhythm is very important in limericks for the first, second and fifth lines must have seven to ten syllables. However, the third and fourth lines only need five to seven. All of the lines must rhyme and have the same rhythm.

Tanka is a form of unrhymed Japanese poetry, with five sections totalling 31 "on" (phonological units identical to morae), structured in a 5-7-5-7-7 pattern. There is generally a shift in tone and subject matter between the upper 5-7-5 phrase and the lower 7-7 phrase. Tanka were written as early as the Asuka period by such poets as Kakinomoto no Hitomaro ("fl." late 7th century), at a time when Japan was emerging from a period where much of its poetry followed Chinese form. Tanka was originally the shorter form of Japanese formal poetry (which was generally referred to as "waka"), and was used more heavily to explore personal rather than public themes. By the tenth century, tanka had become the dominant form of Japanese poetry, to the point where the originally general term "waka" ("Japanese poetry") came to be used exclusively for tanka. Tanka are still widely written today.

Haiku is a popular form of unrhymed Japanese poetry, which evolved in the 17th century from the "hokku", or opening verse of a renku. Generally written in a single vertical line, the haiku contains three sections totalling 17 "on" (morae), structured in a 5-7-5 pattern. Traditionally, haiku contain a kireji, or cutting word, usually placed at the end of one of the poem's three sections, and a kigo, or season-word. The most famous exponent of the haiku was Matsuo Bashō (1644–1694). An example of his writing:

The "khlong" (, ) is among the oldest Thai poetic forms. This is reflected in its requirements on the tone markings of certain syllables, which must be marked with "mai ek" (, , ) or "mai tho" (, , ). This was likely derived from when the Thai language had three tones (as opposed to today's five, a split which occurred during the Ayutthaya Kingdom period), two of which corresponded directly to the aforementioned marks. It is usually regarded as an advanced and sophisticated poetic form.

In "khlong", a stanza ("bot", , ) has a number of lines ("bat", , , from Pali and Sanskrit "pāda"), depending on the type. The "bat" are subdivided into two "wak" (, , from Sanskrit "varga"). The first "wak" has five syllables, the second has a variable number, also depending on the type, and may be optional. The type of "khlong" is named by the number of "bat" in a stanza; it may also be divided into two main types: "khlong suphap" (, ) and "khlong dan" (, ). The two differ in the number of syllables in the second "wak" of the final "bat" and inter-stanza rhyming rules.

The "khlong si suphap" (, ) is the most common form still currently employed. It has four "bat" per stanza ("si" translates as "four"). The first "wak" of each "bat" has five syllables. The second "wak" has two or four syllables in the first and third "bat", two syllables in the second, and four syllables in the fourth. "Mai ek" is required for seven syllables and "Mai tho" is required for four, as shown below. "Dead word" syllables are allowed in place of syllables which require "mai ek", and changing the spelling of words to satisfy the criteria is usually acceptable.

Odes were first developed by poets writing in ancient Greek, such as Pindar, and Latin, such as Horace. Forms of odes appear in many of the cultures that were influenced by the Greeks and Latins. The ode generally has three parts: a strophe, an antistrophe, and an epode. The antistrophes of the ode possess similar metrical structures and, depending on the tradition, similar rhyme structures. In contrast, the epode is written with a different scheme and structure. Odes have a formal poetic diction and generally deal with a serious subject. The strophe and antistrophe look at the subject from different, often conflicting, perspectives, with the epode moving to a higher level to either view or resolve the underlying issues. Odes are often intended to be recited or sung by two choruses (or individuals), with the first reciting the strophe, the second the antistrophe, and both together the epode. Over time, differing forms for odes have developed with considerable variations in form and structure, but generally showing the original influence of the Pindaric or Horatian ode. One non-Western form which resembles the ode is the qasida in Persian poetry.

The ghazal (also ghazel, gazel, gazal, or gozol) is a form of poetry common in Arabic, Bengali, Persian and Urdu. In classic form, the ghazal has from five to fifteen rhyming couplets that share a refrain at the end of the second line. This refrain may be of one or several syllables and is preceded by a rhyme. Each line has an identical meter. The ghazal often reflects on a theme of unattainable love or divinity.

As with other forms with a long history in many languages, many variations have been developed, including forms with a quasi-musical poetic diction in Urdu. Ghazals have a classical affinity with Sufism, and a number of major Sufi religious works are written in ghazal form. The relatively steady meter and the use of the refrain produce an incantatory effect, which complements Sufi mystical themes well. Among the masters of the form is Rumi, a 13th-century Persian poet.
One of the most famous poet in this type of poetry is Hafez, whose poems often include the theme of exposing hypocrisy. His life and poems have been the subject of much analysis, commentary and interpretation, influencing post-fourteenth century Persian writing more than any other author. The West-östlicher Diwan of Johann Wolfgang von Goethe, a collection of lyrical poems, is inspired by the Persian poet Hafez.

In addition to specific forms of poems, poetry is often thought of in terms of different genres and subgenres. A poetic genre is generally a tradition or classification of poetry based on the subject matter, style, or other broader literary characteristics. Some commentators view genres as natural forms of literature. Others view the study of genres as the study of how different works relate and refer to other works.

Narrative poetry is a genre of poetry that tells a story. Broadly it subsumes epic poetry, but the term "narrative poetry" is often reserved for smaller works, generally with more appeal to human interest. Narrative poetry may be the oldest type of poetry. Many scholars of Homer have concluded that his "Iliad" and "Odyssey" were composed of compilations of shorter narrative poems that related individual episodes. Much narrative poetry—such as Scottish and English ballads, and Baltic and Slavic heroic poems—is performance poetry with roots in a preliterate oral tradition. It has been speculated that some features that distinguish poetry from prose, such as meter, alliteration and kennings, once served as memory aids for bards who recited traditional tales.

Notable narrative poets have included Ovid, Dante, Juan Ruiz, William Langland, Chaucer, Fernando de Rojas, Luís de Camões, Shakespeare, Alexander Pope, Robert Burns, Adam Mickiewicz, Alexander Pushkin, Edgar Allan Poe, Alfred Tennyson, and Anne Carson.

Lyric poetry is a genre that, unlike epic and dramatic poetry, does not attempt to tell a story but instead is of a more personal nature. Poems in this genre tend to be shorter, melodic, and contemplative. Rather than depicting characters and actions, it portrays the poet's own feelings, states of mind, and perceptions. Notable poets in this genre include Christine de Pizan, John Donne, Charles Baudelaire, Gerard Manley Hopkins, Antonio Machado, and Edna St. Vincent Millay.

Epic poetry is a genre of poetry, and a major form of narrative literature. This genre is often defined as lengthy poems concerning events of a heroic or important nature to the culture of the time. It recounts, in a continuous narrative, the life and works of a heroic or mythological person or group of persons. Examples of epic poems are Homer's "Iliad" and "Odyssey", Virgil's Aeneid, the "Nibelungenlied", Luís de Camões' "Os Lusíadas", the "Cantar de Mio Cid", the "Epic of Gilgamesh", the "Mahabharata", Valmiki's "Ramayana", Ferdowsi's "Shahnama", Nizami (or Nezami)'s Khamse (Five Books), and the "Epic of King Gesar". While the composition of epic poetry, and of long poems generally, became less common in the west after the early 20th century, some notable epics have continued to be written. Derek Walcott won a Nobel prize to a great extent on the basis of his epic, "Omeros".

Poetry can be a powerful vehicle for satire. The Romans had a strong tradition of satirical poetry, often written for political purposes. A notable example is the Roman poet Juvenal's satires.

The same is true of the English satirical tradition. John Dryden (a Tory), the first Poet Laureate, produced in 1682 "Mac Flecknoe", subtitled "A Satire on the True Blue Protestant Poet, T.S." (a reference to Thomas Shadwell). Another master of 17th-century English satirical poetry was John Wilmot, 2nd Earl of Rochester. Satirical poets outside England include Poland's Ignacy Krasicki, Azerbaijan's Sabir, Portugal's Manuel Maria Barbosa du Bocage, and Korea's Kim Kirim, especially noted for his "Gisangdo".

An elegy is a mournful, melancholy or plaintive poem, especially a lament for the dead or a funeral song. The term "elegy," which originally denoted a type of poetic meter (elegiac meter), commonly describes a poem of mourning. An elegy may also reflect something that seems to the author to be strange or mysterious. The elegy, as a reflection on a death, on a sorrow more generally, or on something mysterious, may be classified as a form of lyric poetry.

Notable practitioners of elegiac poetry have included Propertius, Jorge Manrique, Jan Kochanowski, Chidiock Tichborne, Edmund Spenser, Ben Jonson, John Milton, Thomas Gray, Charlotte Turner Smith, William Cullen Bryant, Percy Bysshe Shelley, Johann Wolfgang von Goethe, Evgeny Baratynsky, Alfred Tennyson, Walt Whitman, Antonio Machado, Juan Ramón Jiménez, Giannina Braschi, William Butler Yeats, Rainer Maria Rilke, and Virginia Woolf.

The fable is an ancient literary genre, often (though not invariably) set in verse. It is a succinct story that features anthropomorphised animals, legendary creatures, plants, inanimate objects, or forces of nature that illustrate a moral lesson (a "moral"). Verse fables have used a variety of meter and rhyme patterns.

Notable verse fabulists have included Aesop, Vishnu Sarma, Phaedrus, Marie de France, Robert Henryson, Biernat of Lublin, Jean de La Fontaine, Ignacy Krasicki, Félix María de Samaniego, Tomás de Iriarte, Ivan Krylov and Ambrose Bierce.

Dramatic poetry is drama written in verse to be spoken or sung, and appears in varying, sometimes related forms in many cultures. Greek tragedy in verse dates to the 6th century B.C., and may have been an influence on the development of Sanskrit drama, just as Indian drama in turn appears to have influenced the development of the "bianwen" verse dramas in China, forerunners of Chinese Opera. East Asian verse dramas also include Japanese Noh. Examples of dramatic poetry in Persian literature include Nizami's two famous dramatic works, "Layla and Majnun" and "Khosrow and Shirin", Ferdowsi's tragedies such as "Rostam and Sohrab", Rumi's "Masnavi", Gorgani's tragedy of "Vis and Ramin", and Vahshi's tragedy of "Farhad".

Speculative poetry, also known as fantastic poetry (of which weird or macabre poetry is a major sub-classification), is a poetic genre which deals thematically with subjects which are "beyond reality", whether via extrapolation as in science fiction or via weird and horrific themes as in horror fiction. Such poetry appears regularly in modern science fiction and horror fiction magazines. Edgar Allan Poe is sometimes seen as the "father of speculative poetry". Poe's most remarkable achievement in the genre was his anticipation, by three-quarters of a century, of the Big Bang theory of the universe's origin, in his then much-derided 1848 essay (which, due to its very speculative nature, he termed a "prose poem"), "".

Prose poetry is a hybrid genre that shows attributes of both prose and poetry. It may be indistinguishable from the micro-story ( the "short short story", "flash fiction"). While some examples of earlier prose strike modern readers as poetic, prose poetry is commonly regarded as having originated in 19th-century France, where its practitioners included Aloysius Bertrand, Charles Baudelaire, Arthur Rimbaud and Stéphane Mallarmé. Since the late 1980s especially, prose poetry has gained increasing popularity, with entire journals, such as "The Prose Poem: An International Journal", "Contemporary Haibun Online", and "Haibun Today" devoted to that genre and its hybrids. Latin American poets of the 20th century who wrote prose poems include Octavio Paz, Alejandra Pizarnik, and Giannina Braschi

Light poetry, or light verse, is poetry that attempts to be humorous. Poems considered "light" are usually brief, and can be on a frivolous or serious subject, and often feature word play, including puns, adventurous rhyme and heavy alliteration. Although a few free verse poets have excelled at light verse outside the formal verse tradition, light verse in English usually obeys at least some formal conventions. Common forms include the limerick, the clerihew, and the double dactyl.

While light poetry is sometimes condemned as doggerel, or thought of as poetry composed casually, humor often makes a serious point in a subtle or subversive way. Many of the most renowned "serious" poets have also excelled at light verse. Notable writers of light poetry include Lewis Carroll, Ogden Nash, X. J. Kennedy, Willard R. Espy, and Wendy Cope.

Slam poetry as a genre originated in 1986 in Chicago, Illinois, when Marc Kelly Smith organized the first slam. Slam performers comment emotively, aloud before an audience, on personal, social, or other matters. Slam focuses on the aesthetics of word play, intonation, and voice inflection. Slam poetry is often competitive, at dedicated "poetry slam" contests.






</doc>
<doc id="22934" url="https://en.wikipedia.org/wiki?curid=22934" title="Probability">
Probability

Probability is the branch of mathematics concerning numerical descriptions of how likely an event is to occur or how likely it is that a proposition is true. The probability of an event is a number between 0 and 1, where, roughly speaking, 0 indicates impossibility of the event and 1 indicates certainty. The higher the probability of an event, the more likely it is that the event will occur. A simple example is the tossing of a fair (unbiased) coin. Since the coin is fair, the two outcomes ("heads" and "tails") are both equally probable; the probability of "heads" equals the probability of "tails"; and since no other outcomes are possible, the probability of either "heads" or "tails" is 1/2 (which could also be written as 0.5 or 50%).

These concepts have been given an axiomatic mathematical formalization in probability theory, which is used widely in such areas of study as mathematics, statistics, finance, gambling, science (in particular physics), artificial intelligence/machine learning, computer science, game theory, and philosophy to, for example, draw inferences about the expected frequency of events. Probability theory is also used to describe the underlying mechanics and regularities of complex systems.

When dealing with experiments that are random and well-defined in a purely theoretical setting (like tossing a fair coin), probabilities can be numerically described by the number of desired outcomes divided by the total number of all outcomes. For example, tossing a fair coin twice will yield "head-head", "head-tail", "tail-head", and "tail-tail" outcomes. The probability of getting an outcome of "head-head" is 1 out of 4 outcomes, or, in numerical terms, 1/4, 0.25 or 25%. However, when it comes to practical application, there are two major competing categories of probability interpretations, whose adherents possess different views about the fundamental nature of probability:


The word "probability" derives from the Latin "probabilitas", which can also mean "probity", a measure of the authority of a witness in a legal case in Europe, and often correlated with the witness's nobility. In a sense, this differs much from the modern meaning of "probability", which, in contrast, is a measure of the weight of empirical evidence, and is arrived at from inductive reasoning and statistical inference.

The scientific study of probability is a modern development of mathematics. Gambling shows that there has been an interest in quantifying the ideas of probability for millennia, but exact mathematical descriptions arose much later. There are reasons for the slow development of the mathematics of probability. Whereas games of chance provided the impetus for the mathematical study of probability, are still obscured by the superstitions of gamblers.

According to Richard Jeffrey, "Before the middle of the seventeenth century, the term 'probable' (Latin "probabilis") meant "approvable", and was applied in that sense, univocally, to opinion and to action. A probable action or opinion was one such as sensible people would undertake or hold, in the circumstances." However, in legal contexts especially, 'probable' could also apply to propositions for which there was good evidence.

The earliest known forms of probability and statistics were developed by Middle Eastern mathematicians studying cryptography between the 8th and 13th centuries. Al-Khalil (717–786) wrote the "Book of Cryptographic Messages" which contains the first use of permutations and combinations to list all possible Arabic words with and without vowels. Al-Kindi (801–873) made the earliest known use of statistical inference in his work on cryptanalysis and frequency analysis. An important contribution of Ibn Adlan (1187–1268) was on sample size for use of frequency analysis.

The sixteenth century Italian polymath Gerolamo Cardano demonstrated the efficacy of defining odds as the ratio of favourable to unfavourable outcomes (which implies that the probability of an event is given by the ratio of favourable outcomes to the total number of possible outcomes).
Aside from the elementary work by Cardano, the doctrine of probabilities dates to the correspondence of Pierre de Fermat and Blaise Pascal (1654). Christiaan Huygens (1657) gave the earliest known scientific treatment of the subject. Jakob Bernoulli's "Ars Conjectandi" (posthumous, 1713) and Abraham de Moivre's "Doctrine of Chances" (1718) treated the subject as a branch of mathematics. See Ian Hacking's "The Emergence of Probability" and James Franklin's "The Science of Conjecture" for histories of the early development of the very concept of mathematical probability.

The theory of errors may be traced back to Roger Cotes's "Opera Miscellanea" (posthumous, 1722), but a memoir prepared by Thomas Simpson in 1755 (printed 1756) first applied the theory to the discussion of errors of observation. The reprint (1757) of this memoir lays down the axioms that positive and negative errors are equally probable, and that certain assignable limits define the range of all errors. Simpson also discusses continuous errors and describes a probability curve.

The first two laws of error that were proposed both originated with Pierre-Simon Laplace. The first law was published in 1774 and stated that the frequency of an error could be expressed as an exponential function of the numerical magnitude of the error, disregarding sign. The second law of error was proposed in 1778 by Laplace and stated that the frequency of the error is an exponential function of the square of the error. The second law of error is called the normal distribution or the Gauss law. "It is difficult historically to attribute that law to Gauss, who in spite of his well-known precocity had probably not made this discovery before he was two years old."

Daniel Bernoulli (1778) introduced the principle of the maximum product of the probabilities of a system of concurrent errors.

Adrien-Marie Legendre (1805) developed the method of least squares, and introduced it in his "Nouvelles méthodes pour la détermination des orbites des comètes" ("New Methods for Determining the Orbits of Comets"). In ignorance of Legendre's contribution, an Irish-American writer, Robert Adrain, editor of "The Analyst" (1808), first deduced the law of facility of error,
where formula_2 is a constant depending on precision of observation, and formula_3 is a scale factor ensuring that the area under the curve equals 1. He gave two proofs, the second being essentially the same as John Herschel's (1850). Gauss gave the first proof that seems to have been known in Europe (the third after Adrain's) in 1809. Further proofs were given by Laplace (1810, 1812), Gauss (1823), James Ivory (1825, 1826), Hagen (1837), Friedrich Bessel (1838), W.F. Donkin (1844, 1856), and Morgan Crofton (1870). Other contributors were Ellis (1844), De Morgan (1864), Glaisher (1872), and Giovanni Schiaparelli (1875). Peters's (1856) formula for "r", the probable error of a single observation, is well known.

In the nineteenth century authors on the general theory included Laplace, Sylvestre Lacroix (1816), Littrow (1833), Adolphe Quetelet (1853), Richard Dedekind (1860), Helmert (1872), Hermann Laurent (1873), Liagre, Didion, and Karl Pearson. Augustus De Morgan and George Boole improved the exposition of the theory.

Andrey Markov introduced the notion of Markov chains (1906), which played an important role in stochastic processes theory and its applications. The modern theory of probability based on the measure theory was developed by Andrey Kolmogorov (1931).

On the geometric side (see integral geometry) contributors to "The Educational Times" were influential (Miller, Crofton, McColl, Wolstenholme, Watson, and Artemas Martin).

Like other theories, the theory of probability is a representation of its concepts in formal terms—that is, in terms that can be considered separately from their meaning. These formal terms are manipulated by the rules of mathematics and logic, and any results are interpreted or translated back into the problem domain.

There have been at least two successful attempts to formalize probability, namely the Kolmogorov formulation and the Cox formulation. In Kolmogorov's formulation (see probability space), sets are interpreted as events and probability itself as a measure on a class of sets. In Cox's theorem, probability is taken as a primitive (that is, not further analyzed) and the emphasis is on constructing a consistent assignment of probability values to propositions. In both cases, the laws of probability are the same, except for technical details.

There are other methods for quantifying uncertainty, such as the Dempster–Shafer theory or possibility theory, but those are essentially different and not compatible with the laws of probability as usually understood.

Probability theory is applied in everyday life in risk assessment and modeling. The insurance industry and markets use actuarial science to determine pricing and make trading decisions. Governments apply probabilistic methods in environmental regulation, entitlement analysis (Reliability theory of aging and longevity), and financial regulation.

A good example of the use of probability theory in equity trading is the effect of the perceived probability of any widespread Middle East conflict on oil prices, which have ripple effects in the economy as a whole. An assessment by a commodity trader that a war is more likely can send that commodity's prices up or down, and signals other traders of that opinion. Accordingly, the probabilities are neither assessed independently nor necessarily very rationally. The theory of behavioral finance emerged to describe the effect of such groupthink on pricing, on policy, and on peace and conflict.

In addition to financial assessment, probability can be used to analyze trends in biology (e.g. disease spread) as well as ecology (e.g. biological Punnett squares). As with finance, risk assessment can be used as a statistical tool to calculate the likelihood of undesirable events occurring and can assist with implementing protocols to avoid encountering such circumstances. Probability is used to design games of chance so that casinos can make a guaranteed profit, yet provide payouts to players that are frequent enough to encourage continued play.

The discovery of rigorous methods to assess and combine probability assessments has changed society.

Another significant application of probability theory in everyday life is reliability. Many consumer products, such as automobiles and consumer electronics, use reliability theory in product design to reduce the probability of failure. Failure probability may influence a manufacturer's decisions on a product's warranty.

The cache language model and other statistical language models that are used in natural language processing are also examples of applications of probability theory.

Consider an experiment that can produce a number of results. The collection of all possible results is called the sample space of the experiment. The power set of the sample space is formed by considering all different collections of possible results. For example, rolling a die can produce six possible results. One collection of possible results gives an odd number on the die. Thus, the subset {1,3,5} is an element of the power set of the sample space of dice rolls. These collections are called "events". In this case, {1,3,5} is the event that the die falls on some odd number. If the results that actually occur fall in a given event, the event is said to have occurred.

A probability is a way of assigning every event a value between zero and one, with the requirement that the event made up of all possible results (in our example, the event {1,2,3,4,5,6}) is assigned a value of one. To qualify as a probability, the assignment of values must satisfy the requirement that if you look at a collection of mutually exclusive events (events with no common results, e.g., the events {1,6}, {3}, and {2,4} are all mutually exclusive), the probability that at least one of the events will occur is given by the sum of the probabilities of all the individual events.

The probability of an event "A" is written as formula_4, formula_5, or formula_6. This mathematical definition of probability can extend to infinite sample spaces, and even uncountable sample spaces, using the concept of a measure.

The "opposite" or "complement" of an event "A" is the event [not "A"] (that is, the event of "A" not occurring), often denoted as formula_7, or formula_8; its probability is given by . As an example, the chance of not rolling a six on a six-sided die is formula_9. See Complementary event for a more complete treatment.

If two events "A" and "B" occur on a single performance of an experiment, this is called the intersection or joint probability of "A" and "B", denoted as formula_10.

If two events, "A" and "B" are independent then the joint probability is
for example, if two coins are flipped the chance of both being heads is formula_12.

In probability theory, a pairwise independent collection of events is such that any two of the events are independent. Denote by <math>\


</doc>
