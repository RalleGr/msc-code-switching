<doc id="14147" url="https://en.wikipedia.org/wiki?curid=14147" title="Harmonic analysis">
Harmonic analysis

Harmonic analysis is a branch of mathematics concerned with the representation of functions or signals as the superposition of basic waves, and the study of and generalization of the notions of Fourier series and Fourier transforms (i.e. an extended form of Fourier analysis). In the past two centuries, it has become a vast subject with applications in areas as diverse as number theory, representation theory, signal processing, quantum mechanics, tidal analysis and neuroscience.

The term "harmonics" originated as the Ancient Greek word "harmonikos", meaning "skilled in music". In physical eigenvalue problems, it began to mean waves whose frequencies are integer multiples of one another, as are the frequencies of the harmonics of music notes, but the term has been generalized beyond its original meaning.

The classical Fourier transform on R is still an area of ongoing research, particularly concerning Fourier transformation on more general objects such as tempered distributions. For instance, if we impose some requirements on a distribution "f", we can attempt to translate these requirements in terms of the Fourier transform of "f". The Paley–Wiener theorem is an example of this. The Paley–Wiener theorem immediately implies that if "f" is a nonzero distribution of compact support (these include functions of compact support), then its Fourier transform is never compactly supported. This is a very elementary form of an uncertainty principle in a harmonic-analysis setting.

Fourier series can be conveniently studied in the context of Hilbert spaces, which provides a connection between harmonic analysis and functional analysis.

Many applications of harmonic analysis in science and engineering begin with the idea or hypothesis that a phenomenon or signal is composed of a sum of individual oscillatory components. Ocean tides and vibrating strings are common and simple examples. The theoretical approach is often to try to describe the system by a differential equation or system of equations to predict the essential features, including the amplitude, frequency, and phases of the oscillatory components. The specific equations depend on the field, but theories generally try to select equations that represent major principles that are applicable.

The experimental approach is usually to acquire data that accurately quantifies the phenomenon. For example, in a study of tides, the experimentalist would acquire samples of water depth as a function of time at closely enough spaced intervals to see each oscillation and over a long enough duration that multiple oscillatory periods are likely included. In a study on vibrating strings, it is common for the experimentalist to acquire a sound waveform sampled at a rate at least twice that of the highest frequency expected and for a duration many times the period of the lowest frequency expected.

For example, the top signal at the right is a sound waveform of a bass guitar playing an open string corresponding to an A note with a fundamental frequency of 55 Hz. The waveform appears oscillatory, but it is more complex than a simple sine wave, indicating the presence of additional waves. The different wave components contributing to the sound can be revealed by applying a mathematical analysis technique known as the Fourier transform, the result of which is shown in the lower figure. Note that there is a prominent peak at 55 Hz, but that there are other peaks at 110 Hz, 165 Hz, and at other frequencies corresponding to integer multiples of 55 Hz. In this case, 55 Hz is identified as the fundamental frequency of the string vibration, and the integer multiples are known as harmonics.

One of the most modern branches of harmonic analysis, having its roots in the mid-20th century, is analysis on topological groups. The core motivating ideas are the various Fourier transforms, which can be generalized to a transform of functions defined on Hausdorff locally compact topological groups.

The theory for abelian locally compact groups is called Pontryagin duality.

Harmonic analysis studies the properties of that duality and Fourier transform and attempts to extend those features to different settings, for instance, to the case of non-abelian Lie groups.

For general non-abelian locally compact groups, harmonic analysis is closely related to the theory of unitary group representations. For compact groups, the Peter–Weyl theorem explains how one may get harmonics by choosing one irreducible representation out of each equivalence class of representations. This choice of harmonics enjoys some of the useful properties of the classical Fourier transform in terms of carrying convolutions to pointwise products, or otherwise showing a certain understanding of the underlying group structure. See also: Non-commutative harmonic analysis.

If the group is neither abelian nor compact, no general satisfactory theory is currently known ("satisfactory" means at least as strong as the Plancherel theorem). However, many specific cases have been analyzed, for example SL. In this case, representations in infinite dimensions play a crucial role.





</doc>
<doc id="14148" url="https://en.wikipedia.org/wiki?curid=14148" title="Home run">
Home run

In baseball, a home run (abbreviated HR) is scored when the ball is hit in such a way that the batter is able to circle the bases and reach home
safely in one play without any errors being committed by the defensive team in the process. In modern baseball, the feat is typically achieved by hitting the ball over the outfield fence between the foul poles (or making contact with either foul pole) without first touching the ground, resulting in an automatic home run. There is also the "inside-the-park" home run where the batter reaches home safely while the baseball is in play on the field. 

When a home run is scored, the batter is also credited with a hit and a run scored, and an RBI for each runner that scores, including himself. Likewise, the pitcher is recorded as having given up a hit and a run, with additional runs charged for each runner that scores other than the batter.

Home runs are among the most popular aspects of baseball and, as a result, prolific home run hitters are usually the most popular among fans and consequently the highest paid by teams—hence the old saying, "Home run hitters drive Cadillacs, and singles hitters drive Fords" (coined, circa 1948, by veteran pitcher Fritz Ostermueller, by way of mentoring his young teammate, Ralph Kiner).

Nicknames for a home run include "homer", "big fly", "dinger", "long ball", "jack", "shot"/"moon shot", "bomb" and "blast", while a player hitting a home run may be said to have "gone deep" or "gone yard".

In modern times a home run is most often scored when the ball is hit over the outfield wall between the foul poles (in fair territory) before it touches the ground (in flight), and without being caught or deflected back onto the field by a fielder. A batted ball is also a home run if it touches either foul pole or its attached screen before touching the ground, as the foul poles are by definition in fair territory. Additionally, many major-league ballparks have ground rules stating that a batted ball in flight that strikes a specified location or fixed object is a home run; this usually applies to objects that are beyond the outfield wall but are located such that it may be difficult for an umpire to judge.

In professional baseball, a batted ball that goes over the outfield wall "after" touching the ground (i.e. a ball that bounces over the outfield wall) becomes an automatic double. This is colloquially referred to as a "ground rule double" even though it is uniform across all of Major League Baseball, per MLB rules 5.05(a)(6) through 5.05(a)(9).

A fielder is allowed to reach over the wall to attempt to catch the ball as long as his feet are on or over the field during the attempt, and if the fielder successfully catches the ball while it is in flight the batter is out, even if the ball had already passed the vertical plane of the wall. However, since the fielder is not part of the field, a ball that bounces off a fielder (including his glove) and over the wall without touching the ground is still a home run. A fielder may not deliberately throw his glove, cap, or any other equipment or apparel to stop or deflect a fair ball, and an umpire may award a home run to the batter if a fielder does so on a ball that, in the umpire's judgment, would have otherwise been a home run (this is rare in modern professional baseball).

A home run accomplished in any of the above manners is an automatic home run. The ball is dead, even if it rebounds back onto the field (e.g., from striking a foul pole), and the batter and any preceding runners cannot be put out at any time while running the bases. However, if one or more runners fail to touch a base or one runner passes another before reaching home plate, that runner or runners can be called out on appeal, though in the case of not touching a base a runner can go back and touch it if doing so won't cause them to be passed by another preceding runner and they have not yet touched the next base (or home plate in the case of missing third base). This stipulation is in Approved Ruling (2) of Rule 7.10(b).

An inside-the-park home run occurs when a batter hits the ball into play and is able to circle the bases before the fielders can put him out. Unlike with an outside-the-park home run, the batter-runner and all preceding runners are liable to be put out by the defensive team at any time while running the bases. This can only happen if the ball does not leave the ballfield.

In the early days of baseball, outfields were relatively much more spacious, reducing the likelihood of an over-the-fence home run, while increasing the likelihood of an inside-the-park home run, as a ball getting past an outfielder had more distance that it could roll before a fielder could track it down.

Modern outfields are much less spacious and more uniformly designed than in the game's early days, therefore inside-the-park home runs are now a rarity. They usually occur when a fast runner hits the ball deep into the outfield and the ball bounces in an unexpected direction away from the nearest outfielder (e.g., off a divot in the grass or off the outfield wall), the nearest outfielder is injured on the play and cannot get to the ball, or an outfielder misjudges the flight of the ball in a way that he cannot quickly recover from the mistake (e.g., by diving and missing). The speed of the runner is crucial as even triples are relatively rare in most modern ballparks.

If any defensive play on an inside-the-park home run is labeled an error by the official scorer, a home run is not scored; instead, it is scored as a single, double, or triple, and the batter-runner and any applicable preceding runners are said to have taken all additional bases on error. All runs scored on such a play, however, still count.

An example of an unexpected bounce occurred during the 2007 Major League Baseball All-Star Game at AT&T Park in San Francisco on July 10, 2007. Ichiro Suzuki of the American League team hit a fly ball that caromed off the right-center field wall in the opposite direction from where National League right fielder Ken Griffey, Jr. was expecting it to go. By the time the ball was relayed, Ichiro had already crossed the plate standing up. This was the first inside-the-park home run in All-Star Game history, and led to Suzuki being named the game's Most Valuable Player.

Home runs are often characterized by the number of runners on base at the time. A home run hit with the bases empty is seldom called a "one-run homer", but rather a solo home run, solo homer, or "solo shot". With one runner on base, two runs are scored (the baserunner and the batter) and thus the home run is often called a two-run homer or two-run shot. Similarly, a home runs with two runners on base is a three-run homer or three-run shot.

The term "four-run homer" is seldom used; instead, it is nearly always called a "grand slam". Hitting a grand slam is the best possible result for the batter's turn at bat and the worst possible result for the pitcher and his team.

A grand slam occurs when the bases are "loaded" (that is, there are base runners standing at first, second, and third base) and the batter hits a home run. According to "The Dickson Baseball Dictionary", the term originated in the card game of contract bridge. An inside-the-park grand slam is a grand slam that is also an inside-the-park home run, a home run without the ball leaving the field, and it is very rare, due to the relative rarity of loading the bases along with the significant rarity (nowadays) of inside-the-park home runs.

On July 25, 1956, Roberto Clemente became the only MLB player to have ever scored a walk-off inside-the-park grand slam in a 9–8 Pittsburgh Pirates win over the Chicago Cubs, at Forbes Field.

On April 23, 1999, Fernando Tatís made history by hitting two grand slams in one inning, both against Chan Ho Park of the Los Angeles Dodgers. With this feat, Tatís also set a Major League record with 8 RBI in one inning.

On July 29, 2003 against the Texas Rangers, Bill Mueller of the Boston Red Sox became the only player in major league history to hit two grand slams in one game from opposite sides of the plate; he hit three home runs in that game, and his two grand slams were in consecutive at-bats.

On August 25, 2011 the New York Yankees became the first team to hit three grand slams in one game vs the Oakland A's. The Yankees eventually won the game 22–9, after trailing 7–1.

These types of home runs are characterized by the specific game situation in which they occur, and can theoretically occur on either an outside-the-park or inside-the-park home run.

A walk-off home run is a home run hit by the home team in the bottom of the ninth inning, any extra inning, or other scheduled final inning, which gives the home team the lead and thereby ends the game. The term is attributed to Hall of Fame relief pitcher Dennis Eckersley, so named because after the run is scored, the losing team has to "walk off" the field.

Two World Series have ended via the "walk-off" home run. The first was the 1960 World Series when Bill Mazeroski of the Pittsburgh Pirates hit a 9th inning solo home run in the 7th game of the series off New York Yankees pitcher Ralph Terry to give the Pirates the World Championship. The second time was the 1993 World Series when Joe Carter of the Toronto Blue Jays hit a 9th inning 3-run home run off Philadelphia Phillies pitcher Mitch Williams in Game 6 of the series, to help the Toronto Blue Jays capture their second World Series Championship in a row.

Such a home run can also be called a "sudden death" or "sudden victory" home run. That usage has lessened as "walk-off home run" has gained favor. Along with Mazeroski's 1960 shot, the most famous walk-off or sudden-death homer would probably be the "Shot Heard 'Round the World" hit by Bobby Thomson to win the 1951 National League pennant for the New York Giants, along with many other game-ending home runs that famously ended some of the most important and suspenseful baseball games.

A walk-off home run over the fence is an exception to baseball's one-run rule. Normally if the home team is tied or behind in the ninth or extra innings the game ends as soon as the home team scores enough runs to achieve a lead. If the home team has two outs in the inning, and the game is tied, the game will officially end either the moment the batter successfully reaches 1st base or the moment the runner touches home plate—whichever happens last. However, this is superseded by the "ground rule", which provides automatic doubles (when a ball-in-play hits the ground first then leaves the playing field) and home runs (when a ball-in-play leaves the playing field without ever touching the ground). In the latter case, all base runners including the batter are allowed to cross the plate.

A leadoff home run is a home run hit by the first batter of a team, the leadoff hitter of the first inning of the game. In MLB, Rickey Henderson holds the career record with 81 lead-off home runs.

In 1996, Brady Anderson set a Major League record by hitting a lead-off home run in four consecutive games.

When two consecutive batters each hit a home run, this is described as back-to-back home runs. It is still considered back-to-back even if both batters hit their home runs off different pitchers. A third batter hitting a home run is commonly referred to as back-to-back-to-back.

Four home runs in a row by consecutive batters has only occurred ten times in the history of Major League Baseball. Following convention, this is called back-to-back-to-back-to-back. The most recent occurrence was on August 16, 2020 when the Chicago White Sox hit four in a row against the St. Louis Cardinals. Yoan Moncada, Yasmani Grandal, José Abreu and Eloy Jiménez hit consecutive home runs during the fifth inning off relief pitcher Roel Ramírez, who was making his major league debut.

On June 9, 2019, the Washington Nationals hit four in a row against the San Diego Padres in Petco Park as Howie Kendrick, Trea Turner, Adam Eaton and Anthony Rendon homered off pitcher Craig Stammen. Stammen became the fifth pitcher to surrender back-to-back-to-back-to-back home runs, following Paul Foytack on July 31, 1963, Chase Wright on April 22, 2007, Dave Bush on August 10, 2010, and Michael Blazek on July 27, 2017.

On August 14, 2008, the Chicago White Sox defeated the Kansas City Royals 9–2. In this game, Jim Thome, Paul Konerko, Alexei Ramírez, and Juan Uribe hit back-to-back-to-back-to-back home runs in that order. Thome, Konerko, and Ramirez blasted their homers off of Joel Peralta, while Uribe did it off of Rob Tejeda. The next batter, veteran backstop Toby Hall, tried aimlessly to hit the ball as far as possible, but his effort resulted in a strike out.

On April 22, 2007 the Boston Red Sox were trailing the New York Yankees 3–0 when Manny Ramirez, J. D. Drew, Mike Lowell and Jason Varitek hit back-to-back-to-back-to-back home runs to put them up 4–3. They eventually went on to win the game 7–6 after a three-run home run by Mike Lowell in the bottom of the 7th inning. On September 18, 2006 trailing 9–5 to the San Diego Padres in the 9th inning, Jeff Kent, J. D. Drew, Russell Martin, and Marlon Anderson of the Los Angeles Dodgers hit back-to-back-to-back-to-back home runs to tie the game. After giving up a run in the top of the 10th, the Dodgers won the game in the bottom of the 10th, on a walk-off two run home run by Nomar Garciaparra. J. D. Drew has been part of two different sets of back-to-back-to-back-to-back home runs. In both occurrences, his homer was the second of the four.

On September 30, 1997, in the sixth inning of Game One of the American League Division Series between the New York Yankees and Cleveland Indians, Tim Raines, Derek Jeter and Paul O'Neill hit back-to-back-to-back home runs for the Yankees. Raines' home run tied the game. New York went on to win 8–6. This was the first occurrence of three home runs in a row ever in postseason play. The Boston Red Sox repeated the feat in Game Four of the 2007 American League Championship Series, also against the Indians. The Indians returned the favor in Game One of the 2016 American League Division Series.

Twice in MLB history have two brothers hit back-to-back home runs. On April 23, 2013, brothers Melvin Upton, Jr. (formerly B.J. Upton) and Justin Upton hit back-to-back home runs. The first time was on September 15, 1938, when Lloyd Waner and Paul Waner performed the feat.

Simple back-to-back home runs are a relatively frequent occurrence. If a pitcher gives up a homer, he might have his concentration broken and might alter his normal approach in an attempt to "make up for it" by striking out the next batter with some fastballs. Sometimes the next batter will be expecting that and will capitalize on it. A notable back-to-back home run of that type in World Series play involved "Babe Ruth's called shot" in 1932, which was accompanied by various Ruthian theatrics, yet the pitcher, Charlie Root, was allowed to stay in the game. He delivered just one more pitch, which Lou Gehrig drilled out of the park for a back-to-back shot, after which Root was removed from the game.

In Game 3 of the 1976 NLCS, George Foster and Johnny Bench hit back-to-back homers in the last of the ninth off Ron Reed to tie the game. The Series-winning run was scored later in the inning.

Another notable pair of back-to-back home runs occurred on September 14, 1990, when Ken Griffey, Sr. and Ken Griffey, Jr. hit back-to-back home runs, off Kirk McCaskill, the only father-and-son duo to do so in Major League history.

On May 2, 2002, Bret Boone and Mike Cameron of the Seattle Mariners hit back-to-back home runs off of starter Jon Rauch in the first inning of a game against the Chicago White Sox. The Mariners batted around in the inning, and Boone and Cameron came up to bat against reliever Jim Parque with two outs, again hitting back-to-back home runs and becoming the only pair of teammates to hit back-to-back home runs twice in the same inning.

On June 19, 2012, José Bautista and Colby Rasmus hit back-to-back home runs and back-to-back-to-back home runs with Edwin Encarnación for a lead change in each instance.

On July 23, 2017, Whit Merrifield, Jorge Bonifacio, and Eric Hosmer of the Kansas City Royals hit back-to-back-to-back home runs in the fourth inning against the Chicago White Sox. The Royals went on to win the game 5–4.

On June 20, 2018, George Springer, Alex Bregman, and José Altuve of the Houston Astros hit back-to-back-to-back home runs in the sixth inning against the Tampa Bay Rays. The Astros went on to win the game 5–1.

On April 3, 2018, the St. Louis Cardinals began the game against the Milwaukee Brewers with back-to-back homers from Dexter Fowler and Tommy Pham. Then in the bottom of the ninth, with two outs and the Cardinals leading 4–3, Christian Yelich homered to tie the game; and Ryan Braun hit the next pitch for a walk-off homer. This is the only major league game to begin and end with back-to-back homers.

On May 5, 2019, Eugenio Suarez, Jesse Winker and Derek Dietrich of the Cincinnati Reds, hit back-to-back-to-back home runs on three straight pitches against Jeff Samardzija of the San Francisco Giants in the bottom of the first inning.

The record for consecutive home runs by a batter under any circumstances is four. Of the sixteen players (through 2012) who have hit four in one game, six have hit them consecutively. Twenty-eight other batters have hit four consecutive across two games.

Bases on balls do not count as at-bats, and Ted Williams holds the record for consecutive home runs across the most games, four in four games played, during September 17–22, 1957, for the Red Sox. Williams hit a pinch-hit homer on the 17th; walked as a pinch-hitter on the 18th; there was no game on the 19th; hit another pinch-homer on the 20th; homered and then was lifted for a pinch-runner after at least one walk, on the 21st; and homered after at least one walk on the 22nd. All in all, he had four walks interspersed among his four homers.

In World Series play, Reggie Jackson hit a record three in one Series game, the final game (Game 6) in 1977. But those three were a part of a much more impressive feat. He walked on four pitches in the second inning of game 6. Then he hit his three home runs on the first pitch of his next three at bats, off of three different pitchers (4th inning- Hooten, 5th inning- Sosa, 8th inning- Hough). He had also hit one in his last at bat of the previous game, giving him four home runs on four consecutive swings. The four in a row set the record for consecutive homers across two Series games.

In Game 3 of the World Series in 2011, Albert Pujols hit three home runs to tie the record with Babe Ruth and Reggie Jackson. The St. Louis Cardinals went on to win the World Series in Game 7 at Busch Stadium. In Game 1 of the World Series in 2012, Pablo Sandoval of the San Francisco Giants hit three home runs on his first three at-bats of the Series.

Nomar Garciaparra holds the record for consecutive home runs in the shortest time in terms of innings: three homers in two innings, on July 23, 2002, for the Boston Red Sox.

An offshoot of hitting for the cycle, a "home run cycle" is when a player hits a solo home run, two-run home run, three-run home run, and grand slam all in one game. This is an extremely rare feat, as it requires the batter not only to hit four home runs in the game, but also to hit the home runs with a specific number of runners already on base. This is largely dependent on circumstances outside of the player's control, such as teammates' ability to get on base, and the order in which the player comes to bat in any particular inning. A further variant of the home run cycle would be the "natural home run cycle", should a batter hit the home runs in the specific order listed above.

A home run cycle has never occurred in MLB, which has only had 18 instances of a player hitting four home runs in a game. Though multiple home run cycles have been recorded in collegiate baseball, the only known home run cycle in a professional baseball game belongs to Tyrone Horne, playing for the Arkansas Travelers in a Double-A level Minor League Baseball game against the San Antonio Missions on July 27, 1998.

Major league players have come close to hitting a home run cycle, a notable example being Scooter Gennett of the Cincinnati Reds on June 6, 2017, when he hit four home runs against the St. Louis Cardinals. He hit a grand slam in the third inning, a two-run home run in the fourth inning, a solo home run in the sixth inning, and a two-run home run in the eighth inning. He had an opportunity for a three-run home run in the first inning, but drove in one run with a single in that at bat.

In the early days of the game, when the ball was less lively and the ballparks generally had very large outfields, most home runs were of the inside-the-park variety. The first home run ever hit in the National League was by Ross Barnes of the Chicago White Stockings (now known as the Chicago Cubs), in 1876. The home "run" was literally descriptive. Home runs over the fence were rare, and only in ballparks where a fence was fairly close. Hitters were discouraged from trying to hit home runs, with the conventional wisdom being that if they tried to do so they would simply fly out. This was a serious concern in the 19th century, because in baseball's early days a ball caught after one bounce was still an out. The emphasis was on place-hitting and what is now called "manufacturing runs" or "small ball".

The home run's place in baseball changed dramatically when the live-ball era began after World War I. First, the materials and manufacturing processes improved significantly, making the now-mass-produced, cork-centered ball somewhat more lively. Batters such as Babe Ruth and Rogers Hornsby took full advantage of rules changes that were instituted during the 1920s, particularly prohibition of the spitball, and the requirement that balls be replaced when worn or dirty. These changes resulted in the baseball being easier to see and hit, and easier to hit out of the park. Meanwhile, as the game's popularity boomed, more outfield seating was built, shrinking the size of the outfield and increasing the chances of a long fly ball resulting in a home run. The teams with the sluggers, typified by the New York Yankees, became the championship teams, and other teams had to change their focus from the "inside game" to the "power game" in order to keep up.

Before , Major League Baseball considered a fair ball that bounced over an outfield fence to be a home run. The rule was changed to require the ball to clear the fence on the fly, and balls that reached the seats on a bounce became automatic doubles (often referred to as a ground rule double). The last "bounce" home run in MLB was hit by Al López of the Brooklyn Robins on September 12, 1930, at Ebbets Field. A carryover of the old rule is that if a player deflects a ball over the outfield fence in fair territory without it touching the ground, it is a home run, per MLB rule 5.05(a)(9). Additionally, MLB rule 5.05(a)(5) still stipulates that a ball hit over a fence in fair territory that is less that from home plate "shall entitle the batter to advance to second base only", as some early ballparks had short dimensions.

Also until circa 1931, the ball had to go not only over the fence in fair territory, but it had to land in the bleachers in fair territory or still be visibly fair when disappearing from view. The rule stipulated "fair when last seen" by the umpires. Photos from that era in ballparks, such as the Polo Grounds and Yankee Stadium, show ropes strung from the foul poles to the back of the bleachers, or a second "foul pole" at the back of the bleachers, in a straight line with the foul line, as a visual aid for the umpire. Ballparks still use a visual aid much like the ropes; a net or screen attached to the foul poles on the fair side has replaced ropes. As with American football, where a touchdown once required a literal "touch down" of the ball in the end zone but now only requires the "breaking of the [vertical] plane" of the goal line, in baseball the ball need only "break the plane" of the fence in fair territory (unless the ball is caught by a player who is in play, in which case the batter is called out).

Babe Ruth's 60th home run in 1927 was somewhat controversial, because it landed barely in fair territory in the stands down the right field line. Ruth lost a number of home runs in his career due to the when-last-seen rule. Bill Jenkinson, in "The Year Babe Ruth Hit 104 Home Runs", estimates that Ruth lost at least 50 and as many as 78 in his career due to this rule.

Further, the rules once stipulated that an over-the-fence home run in a sudden-victory situation would only count for as many bases as was necessary to "force" the winning run home. For example, if a team trailed by two runs with the bases loaded, and the batter hit a fair ball over the fence, it only counted as a triple, because the runner immediately ahead of him had technically already scored the game-winning run. That rule was changed in the 1920s as home runs became increasingly frequent and popular. Babe Ruth's career total of 714 would have been one higher had that rule not been in effect in the early part of his career.

Major League Baseball keeps running totals of all-time home runs by team, including teams no longer active (prior to 1900) as well as by individual players. Gary Sheffield hit the 250,000th home run in MLB history with a grand slam on September 8, 2008. Sheffield had hit MLB's 249,999th home run against Gio González in his previous at-bat.

The all-time, verified professional baseball record for career home runs for one player, excluding the U.S. Negro Leagues during the era of segregation, is held by Sadaharu Oh. Oh spent his entire career playing for the Yomiuri Giants in Japan's Nippon Professional Baseball, later managing the Giants, the Fukuoka SoftBank Hawks and the 2006 World Baseball Classic Japanese team. Oh holds the all-time home run world record, having hit 868 home runs in his career.

In Major League Baseball, the career record is 762, held by Barry Bonds, who broke Hank Aaron's record on August 7, 2007, when he hit his 756th home run at AT&T Park off pitcher Mike Bacsik. Only eight other major league players have hit as many as 600: Hank Aaron (755), Babe Ruth (714), Alex Rodriguez (696), Willie Mays (660), Albert Pujols (654), Ken Griffey, Jr. (630), Jim Thome (612), and Sammy Sosa (609); Pujols holds the record for active MLB players.

The single season record is 73, set by Barry Bonds in 2001. Other notable single season records were achieved by Babe Ruth who hit 60 in 1927, Roger Maris, with 61 home runs in 1961, and Mark McGwire, who hit 70 in 1998.

Negro League slugger Josh Gibson's Baseball Hall of Fame plaque says he hit "almost 800" home runs in his career. The "Guinness Book of World Records" lists Gibson's lifetime home run total at 800. Ken Burns' award-winning series, "Baseball", states that his actual total may have been as high as 950. Gibson's true total is not known, in part due to inconsistent record keeping in the Negro Leagues. The 1993 edition of the MacMillan "Baseball Encyclopedia" attempted to compile a set of Negro League records, and subsequent work has expanded on that effort. Those records demonstrate that Gibson and Ruth were of comparable power. The 1993 book had Gibson hitting 146 home runs in the 501 "official" Negro League games they were able to account for in his 17-year career, about 1 homer every 3.4 games. Babe Ruth, in 22 seasons (several of them in the dead-ball era), hit 714 in 2503 games, or 1 homer every 3.5 games. The large gap in the numbers for Gibson reflect the fact that Negro League clubs played relatively far fewer league games and many more "barnstorming" or exhibition games during the course of a season, than did the major league clubs of that era.

Other legendary home run hitters include Jimmie Foxx, Mel Ott, Ted Williams, Mickey Mantle (who on September 10, 1960, mythically hit "the longest home run ever" at an estimated distance of , although this was measured after the ball stopped rolling), Reggie Jackson, Harmon Killebrew, Ernie Banks, Mike Schmidt, Dave Kingman, Sammy Sosa (who hit 60 or more home runs in a season 3 times), Ken Griffey, Jr. and Eddie Mathews. In 1987, Joey Meyer of the Denver Zephyrs hit the longest verifiable home run in professional baseball history. The home run was measured at a distance of and was hit inside Denver's Mile High Stadium. Major League Baseball's longest verifiable home run distance is about , by Babe Ruth, to straightaway center field at Tiger Stadium (then called Navin Field and before the double-deck), which landed nearly across the intersection of Trumbull and Cherry.

The location of where Hank Aaron's record 755th home run landed has been monumented in Milwaukee. The spot sits outside Miller Park, where the Milwaukee Brewers currently play. Similarly, the point where Aaron's 715th homer landed, upon breaking Ruth's career record in 1974, is marked in the Turner Field parking lot. A red-painted seat in Fenway Park marks the landing place of the 502-ft home run Ted Williams hit in 1946, the longest measured homer in Fenway's history; a red stadium seat mounted on the wall of the Mall of America in Bloomington, Minnesota, marks the landing spot of Harmon Killebrew's record 520-foot shot in old Metropolitan Stadium.

May 2019 saw 1,135 MLB home runs, the highest ever number of home runs in a single month in Major League Baseball history. During this month, 44.5% of all runs came during a homer, breaking the previous record of 42.3%.

Replays "to get the call right" have been used extremely sporadically in the past, but the use of instant replay to determine "boundary calls"—home runs and foul balls—was not officially allowed until 2008.

In a game on May 31, 1999, involving the St. Louis Cardinals and Florida Marlins, a hit by Cliff Floyd of the Marlins was initially ruled a double, then a home run, then was changed back to a double when umpire Frank Pulli decided to review video of the play. The Marlins protested that video replay was not allowed, but while the National League office agreed that replay was not to be used in future games, it declined the protest on the grounds it was a judgment call, and the play stood.

In November 2007, the general managers of Major League Baseball voted in favor of implementing instant replay reviews on boundary home run calls. The proposal limited the use of instant replay to determining whether a boundary/home run call is:

On August 28, 2008, instant replay review became available in MLB for reviewing calls in accordance with the above proposal. It was first utilized on September 3, 2008 in a game between the New York Yankees and the Tampa Bay Rays at Tropicana Field. Alex Rodriguez of the Yankees hit what appeared to be a home run, but the ball hit a catwalk behind the foul pole. It was at first called a home run, until Tampa Bay manager Joe Maddon argued the call, and the umpires decided to review the play. After 2 minutes and 15 seconds, the umpires came back and ruled it a home run.

About two weeks later, on September 19, also at Tropicana Field, a boundary call was overturned for the first time. In this case, Carlos Peña of the Rays was given a ground rule double in a game against the Minnesota Twins after an umpire believed a fan reached into the field of play to catch a fly ball in right field. The umpires reviewed the play, determined the fan did not reach over the fence, and reversed the call, awarding Peña a home run.

Aside from the two aforementioned reviews at Tampa Bay, replay was used four more times in the 2008 MLB regular season: twice at Houston, once at Seattle, and once at San Francisco. The San Francisco incident is perhaps the most unusual. Bengie Molina, the Giants' catcher, hit what was first called a single. Molina then was replaced in the game by Emmanuel Burriss, a pinch-runner, before the umpires re-evaluated the call and ruled it a home run. In this instance though, Molina was not allowed to return to the game to complete the run, as he had already been replaced. Molina was credited with the home run, and two RBIs, but not for the run scored which went to Burriss instead.

On October 31, 2009, in the fourth inning of Game 3 of the World Series, Alex Rodriguez hit a long fly ball that hit a camera protruding over the wall and into the field of play in deep right field. The ball ricocheted off the camera and re-entered the field, initially ruled a double. However, after the umpires consulted with each other after watching the instant replay, the hit was ruled a home run, marking the first time an instant replay home run was hit in a playoff game.


Career achievements

Other sports

Boundary (cricket)#Scoring runs (A "six" in cricket is similar to a home run).



</doc>
<doc id="14149" url="https://en.wikipedia.org/wiki?curid=14149" title="Harappa">
Harappa

Harappa (; Urdu/) is an archaeological site in Punjab, Pakistan, about west of Sahiwal. The site takes its name from a modern village located near the former course of the Ravi River which now runs to the north. The current village of Harappa is less than from the ancient site. Although modern Harappa has a legacy railway station from the British Raj period, it is a small crossroads town of 15,000 people today.

The site of the ancient city contains the ruins of a Bronze Age fortified city, which was part of the Indus Valley Civilisation centred in Sindh and the Punjab, and then the Cemetery H culture. The city is believed to have had as many as 23,500 residents and occupied about with clay brick houses at its greatest extent during the Mature Harappan phase (2600 BC – 1900 BC), which is considered large for its time. Per archaeological convention of naming a previously unknown civilisation by its first excavated site, the Indus Valley Civilisation is also called the Harappan Civilisation.

The ancient city of Harappa was heavily damaged under British rule, when bricks from the ruins were used as track ballast in the construction of the Lahore–Multan Railway. In 2005, a controversial amusement park scheme at the site was abandoned when builders unearthed many archaeological artifacts during the early stages of building work. A plea from the Pakistani archaeologist Mohit Prem Kumar to the Ministry of Culture resulted in a restoration of the site.

The Harappan Civilisation has its earliest roots in cultures such as that of Mehrgarh, approximately 6000  BC. The two greatest cities, Mohenjo-daro and Harappa, emerged circa 2600 BC along the Indus River valley in Punjab and Sindh. The civilisation, with a possible writing system, urban centers, and diversified social and economic system, was rediscovered in the 1920s also after excavations at Mohenjo-daro in Sindh near Larkana, and Harappa, in west Punjab south of Lahore. A number of other sites stretching from the Himalayan foothills in east Punjab, India in the north, to Gujarat in the south and east, and to Pakistani Balochistan in the west have also been discovered and studied. Although the archaeological site at Harappa was damaged in 1857 when engineers constructing the Lahore-Multan railroad used brick from the Harappa ruins for track ballast, an abundance of artifacts have nevertheless been found. The bricks discovered were made of red sand, clay, stones and were baked at very high temperature. As early as 1826 Harappa, located in west Punjab, attracted the attention of Daya Ram Sahni, who gets credit for preliminary excavations of Harappa.
Because of the reducing sea-levels certain regions in late Harappan period were abandoned 
. Towards the end Harappan civilization lost features such as writing and hydraulic engineering.. As a result the ganges valley settlement gained prominence and ganges cities developed.

The Indus Valley civilisation was mainly an urban culture sustained by surplus agricultural production and commerce, the latter including trade with Sumer in southern Mesopotamia. Both Mohenjo-Daro and Harappa are generally characterized as having "differentiated living quarters, flat-roofed brick houses, and fortified administrative or religious centers." Although such similarities have given rise to arguments for the existence of a standardized system of urban layout and planning, the similarities are largely due to the presence of a semi-orthogonal type of civic layout, and a comparison of the layouts of Mohenjo-Daro and Harappa shows that they are in fact, arranged in a quite dissimilar fashion.

The weights and measures of the Indus Valley Civilisation, on the other hand, were highly standardized, and conform to a set scale of gradations. Distinctive seals were used, among other applications, perhaps for identification of property and shipment of goods. Although copper and bronze were in use, iron was not yet employed. "Cotton was woven and dyed for clothing; wheat, rice, and a variety of vegetables and fruits were cultivated; and a number of animals, including the humped bull, were domesticated," as well as "fowl for fighting". Wheel-made pottery—some of it adorned with animal and geometric motifs—has been found in profusion at all the major Indus sites. A centralized administration for each city, though not the whole civilisation, has been inferred from the revealed cultural uniformity; however, it remains uncertain whether authority lay with a commercial oligarchy. Harappans had many trade routes along the Indus River that went as far as the Persian Gulf, Mesopotamia, and Egypt. Some of the most valuable things traded were carnelian and lapis lazuli.

What is clear is that Harappan society was not entirely peaceful, with the human skeletal remains demonstrating some of the highest rates of injury (15.5%) found in South Asian prehistory. Paleopathological analysis demonstrated that leprosy and tuberculosis were present at Harappa, with the highest prevalence of both disease and trauma present in the skeletons from Area G (an ossuary located south-east of the city walls). Furthermore, rates of cranio-facial trauma and infection increased through time demonstrating that the civilisation collapsed amid illness and injury. The bioarchaeologists who examined the remains have suggested that the combined evidence for differences in mortuary treatment and epidemiology indicate that some individuals and communities at Harappa were excluded from access to basic resources like health and safety, a basic feature of hierarchical societies worldwide.

The harappans had traded with Babylonia among other areas. Cotton textiles and agricultural products were the primary trading objects. The Harappan merchants also had procurement colonies in Mesopotamia which also served as trading centers.

The excavators of the site have proposed the following chronology of Harappa's occupation:

By far the most exquisite and obscure artifacts unearthed to date are the small, square steatite (soapstone) seals engraved with human or animal motifs. A large number of seals have been found at such sites as Mohenjo-Daro and Harappa. Many bear pictographic inscriptions generally thought to be a form of writing or script. Despite the efforts of philologists from all parts of the world, and despite the use of modern cryptographic analysis, the signs remain undeciphered. It is also unknown if they reflect proto-Dravidian or other non-Vedic language(s). The ascribing of Indus Valley Civilisation iconography and epigraphy to historically known cultures is extremely problematic, in part due to the rather tenuous archaeological evidence for such claims, as well as the projection of modern South Asian political concerns onto the archaeological record of the area. This is especially evident in the radically varying interpretations of Harappan material culture as seen from both Pakistan- and India-based scholars.

In February 2006 a school teacher in the village of Sembian-Kandiyur in Tamil Nadu discovered a stone celt (tool) with an inscription estimated to be up to 3,500 years old.

The area of late Harappan period consisted of areas of Daimabad, Maharashtra and Badakshan regions of Afghanistan. The area covered by this civilization would have been very large with distance of around 1500 miles. 

Clay and stone tablets unearthed at Harappa, which were carbon dated 3300–3200 BC., contain trident-shaped and plant-like markings. "It is a big question as to if we can call what we have found true writing, but we have found symbols that have similarities to what became Indus script" said Dr. Richard Meadow of Harvard University, Director of the Harappa Archeological Research Project. This primitive writing is placed slightly earlier than primitive writings of the Sumerians of Mesopotamia, dated c.3100 BC. These markings have similarities to what later became Indus Script.





</doc>
<doc id="14153" url="https://en.wikipedia.org/wiki?curid=14153" title="Hendecasyllable">
Hendecasyllable

In poetry, a hendecasyllable is a line of eleven syllables. The term "hendecasyllabic" is used to refer to two different poetic meters, the older of which is quantitative and used chiefly in classical (Ancient Greek and Latin) poetry and the newer of which is accentual and used in medieval and modern poetry. It is often referred to when an iambic parameter contains 11 syllables.

The classical hendecasyllable is a quantitative meter used in Ancient Greece in Aeolic verse and in scolia, and later by the Roman poets Catullus and Martial. Each line has eleven syllables; hence the name, which comes from the Greek word for eleven. The heart of the line is the choriamb (¯ ˘ ˘ ¯). There are three different versions.
The pattern of the Phalaecian (Latin: "hendecasyllabus phalaecius") is as follows (using "¯" for a long syllable, "˘" for a short and "−̆" for an "anceps" or variable syllable):

Another form of hendecasyllabic verse is the "Alcaic" (Latin: "hendecasyllabus alcaicus"; used in the Alcaic stanza), which has the pattern:

The third form of hendecasyllabic verse is the "Sapphic" (Latin: "hendecasyllabus sapphicus"; so named for its use in the Sapphic stanza), with the pattern:

Forty-three of Catullus's poems are hendecasyllabic; for an example, see Catullus 1.

The metre has been imitated in English, notably by Alfred Tennyson, Swinburne, and Robert Frost, cf. "For Once Then Something." Contemporary American poets Annie Finch ("Lucid Waking") and Patricia Smith ("The Reemergence of the Noose") have published recent examples. Poets wanting to capture the hendecasyllabic rhythm in English have simply transposed the pattern into its accentual-syllabic equivalent: ¯ ˘|¯ ˘|¯ ˘ ˘|¯ ˘|¯ ˘|, or trochee/trochee/dactyl/trochee/trochee, so that the long/short pattern becomes a stress/unstress pattern. Tennyson, however, maintained the quantitative features of the metre:

The hendecasyllable () is the principal metre in Italian poetry. Its defining feature is a constant stress on the tenth syllable, so that the number of syllables in the verse may vary, equaling eleven in the usual case where the final word is stressed on the penultimate syllable. The verse also has a stress preceding the caesura, on either the fourth or sixth syllable. The first case is called "endecasillabo a minore", or lesser hendecasyllable, and has the first hemistich equivalent to a "quinario"; the second is called "endecasillabo a maiore", or greater hendecasyllable, and has a "settenario" as the first hemistich.

There is a strong tendency for hendecasyllabic lines to end with feminine rhymes (causing the total number of syllables to be eleven, hence the name), but ten-syllable lines (""Ciò che 'n grembo a Benaco star non può"") and twelve-syllable lines (""Ergasto mio, perché solingo e tacito"") are encountered as well. Lines of ten or twelve syllables are more common in rhymed verse; "versi sciolti", which rely more heavily on a pleasant rhythm for effect, tend toward a stricter eleven-syllable format. As a novelty, lines longer than twelve syllables can be created by the use of certain verb forms and affixed enclitic pronouns (""Ottima è l'acqua; ma le piante abbeverinosene."").

Additional accents beyond the two mandatory ones provide rhythmic variation and allow the poet to express thematic effects. A line in which accents fall consistently on even-numbered syllables (""Al còr gentìl rempàira sèmpre amóre"") is called iambic ("giambico") and may be a greater or lesser hendecasyllable. This line is the simplest, commonest and most musical but may become repetitive, especially in longer works. Lesser hendecasyllables often have an accent on the seventh syllable (""fàtta di giòco in figùra d'amóre""). Such a line is called dactylic ("dattilico") and its less pronounced rhythm is considered particularly appropriate for representing dialogue. Another kind of greater hendecasyllable has an accent on the third syllable (""Se Mercé fosse amìca a' miei disìri"") and is known as anapestic ("anapestico"). This sort of line has a crescendo effect and gives the poem a sense of speed and fluidity.

It is considered improper for the lesser hendecasyllable to use a word accented on its antepenultimate syllable ("parola sdrucciola") for its mid-line stress. A line like ""Più non sfavìllano quegli òcchi néri"", which delays the caesura until after the sixth syllable, is not considered a valid hendecasylable.

Most classical Italian poems are composed in hendecasyllables, including the major works of Dante, Francesco Petrarca, Ludovico Ariosto, and Torquato Tasso. The rhyme systems used include terza rima, ottava, sonnet and canzone, and some verse forms use a mixture of hendecasyllables and shorter lines. From the early 16th century onward, hendecasyllables are often used without a strict system, with few or no rhymes, both in poetry and in drama. This is known as "verso sciolto". An early example is "Le Api" ("the bees") by Giovanni di Bernardo Rucellai, written around 1517 and published in 1525, which begins:

Like other early Italian-language tragedies, the "Sophonisba" of Gian Giorgio Trissino (1515) is in blank hendecasyllables. Later examples can be found in the "Canti" of Giacomo Leopardi, where hendecasyllables are alternated with "settenari".

The hendecasyllabic metre (Polish: "jedenastozgłoskowiec") was very popular in Polish poetry, especially in the seventeenth and eighteenth centuries, owing to strong Italian literary influence. It was used by Jan Kochanowski, Piotr Kochanowski (who translated "Jerusalem Delivered" by Torquato Tasso), Sebastian Grabowiecki, Wespazjan Kochowski and Stanisław Herakliusz Lubomirski. The greatest Polish Romantic poet, Adam Mickiewicz, set his poem Grażyna in this measure. The Polish hendecasyllable is widely used when translating English 
blank verse.

The eleven-syllable line is normally defined by primary stresses on the fourth and tenth syllables and a caesura after the fifth syllable. Only rarely it is fully iambic. 

A popular form of Polish literature that employs the hendacasyllable is the Sapphic stanza: 11/11/11/5. 

The Polish hendecasyllable is often combined with an 8-syllable line: 11a/8b/11a/8b. Such a stanza was used by Mickiewicz in his ballads, as in the following example.

The hendecasyllable (Portuguese: "hendecassílabo") is a common meter in Portuguese poetry. The best-known Portuguese poem composed in hendecasyllables is Luís de Camões' "Lusiads", which begins as follows:

In Portuguese, the hendecasyllable meter is often called "decasyllable" ("decassílabo"), even when the work in question uses overwhelmingly feminine rhymes (as is the case with the "Lusiads"). This is due to Portuguese prosody considering verses to end at the last stressed syllable, thus the aforementioned verses are effectively decasyllabic according to Portuguese scansion.

The hendecasyllable ("endecasílabo") is less pervasive in Spanish poetry than in Italian or Portuguese, but it is commonly used with Italianate verse forms like sonnets and ottava rima. An example of the latter is Alonso de Ercilla's epic "La Araucana", which opens as follows:

Spanish dramatists often use hendecasyllables in tandem with shorter lines like heptasyllables, as can be seen in Rosaura's opening speech from Calderón's "La vida es sueño":

The term "hendecasyllable" is sometimes used to describe a line of iambic pentameter with a feminine ending, as in the first line of John Keats's "Endymion:" "A thing of beauty is a joy for ever."





</doc>
<doc id="14155" url="https://en.wikipedia.org/wiki?curid=14155" title="Hebrides">
Hebrides

The Hebrides (; , ; ) comprise a widespread and diverse archipelago off the west coast of mainland Scotland. There are two main groups: the Inner and Outer Hebrides.

These islands have a long history of occupation dating back to the Mesolithic, and the culture of the residents has been affected by the successive influences of Celtic-speaking, Norse-speaking, and English-speaking peoples. This diversity is reflected in the names given to the islands, which are derived from the languages that have been spoken there in historic and perhaps prehistoric times.

The Hebrides are the source of much of Scottish Gaelic literature and Gaelic music. Today the economy of the islands is dependent on crofting, fishing, tourism, the oil industry, and renewable energy. The Hebrides have lower biodiversity than mainland Scotland, but there is a significant presence of seals and seabirds.

The earliest written references that have survived relating to the islands were made circa 77 AD by Pliny the Elder in his "Natural History", where he states that there are 30 ', and makes a separate reference to ', which Watson (1926) concludes is unequivocally the Outer Hebrides. Writing about 80 years later, in 140-150 AD, Ptolemy, drawing on the earlier naval expeditions of , writes that there are five ' (possibly meaning the Inner Hebrides) and '. Later texts in classical Latin, by writers such as , use the forms ' and '.

The name ' recorded by Ptolemy may be pre-Celtic. Islay is Ptolemy's , the use of the "p" hinting at a Brythonic or Pictish tribal name, , although the root is not Gaelic. Woolf (2012) has suggested that ' may be "an Irish attempt to reproduce the word ' phonetically rather than by translating it" and that the tribe's name may come from the root ' meaning "horse". Watson (1926) also notes the possible relationship between ' and the ancient Irish Ulaid tribal name ' and the personal name of a king recorded in the "".
The names of other individual islands reflect their complex linguistic history. The majority are Norse or Gaelic but the roots of several other Hebrides may have a pre-Celtic origin. Adomnán, the 7th century abbot of Iona, records Colonsay as "Colosus" and Tiree as "Ethica", both of which may be pre-Celtic names. The etymology of Skye is complex and may also include a pre-Celtic root. Lewis is "" in Old Norse and although various suggestions have been made as to a Norse meaning (such as "song house") the name is not of Gaelic origin and the Norse credentials are questionable.

The earliest comprehensive written list of Hebridean island names was undertaken by Donald Monro in 1549, which in some cases also provides the earliest written form of the island name. The derivations of all of the inhabited islands of the Hebrides and some of the larger uninhabited ones are listed below.

Lewis and Harris is the largest island in Scotland and the third largest of the British Isles, after Great Britain and Ireland. It incorporates Lewis in the north and Harris in the south, both of which are frequently referred to as individual islands, although they are joined by a land border. Remarkably, the island does not have a common name in either English or Gaelic and is referred to as "Lewis and Harris", "Lewis with Harris", "Harris with Lewis" etc. For this reason it is treated as two separate islands below. The derivation of Lewis may be pre-Celtic (see above) and the origin of Harris is no less problematic. In the Ravenna Cosmography, "Erimon" may refer to Harris (or possibly the Outer Hebrides as a whole). This word may derive from the ( "desert". The origin of Uist () is similarly unclear.

There are various examples of Inner Hebridean island names that were originally Gaelic but have become completely replaced. For example, Adomnán records "Sainea", "Elena", "Ommon" and "Oideacha" in the Inner Hebrides, which names must have passed out of usage in the Norse era and whose locations are not clear. One of the complexities is that an island may have had a Celtic name, which was replaced by a similar-sounding Norse name, but then reverted to an essentially Gaelic name with a Norse "øy" or "ey" ending. See for example Rona below.

The names of uninhabited islands follow the same general patterns as the inhabited islands. The following are the ten largest in the Hebrides and their outliers.

The etymology of St Kilda, a small archipelago west of the Outer Hebrides, and its main island Hirta, is very complex. No saint is known by the name of Kilda, and various theories have been proposed for the word's origin, which dates from the late 16th century. Haswell-Smith (2004) notes that the full name "St Kilda" first appears on a Dutch map dated 1666, and that it may have been derived from Norse ' ("sweet wellwater") or from a mistaken Dutch assumption that the spring ' was dedicated to a saint (' is a tautological placename, consisting of the Gaelic and Norse words for "well", i.e. "well well"). The origin of the Gaelic for "Hirta"—', ', or '—which long pre-dates the use of "St Kilda", is similarly open to interpretation. Watson (1926) offers the Old Irish ', a word meaning "death", possibly relating to the dangerous seas. Maclean (1977), drawing on an Icelandic saga describing an early 13th-century voyage to Ireland that mentions a visit to the islands of ', speculates that the shape of Hirta resembles a stag, "" being "stags" in Norse.

The etymology of small islands may be no less complex. In relation to , R. L. Stevenson believed that "black and dismal" was a translation of the name, noting that "as usual, in Gaelic, it is not the only one."

The Hebrides were settled during the Mesolithic era around 6500 BC or earlier, after the climatic conditions improved enough to sustain human settlement. Occupation at a site on is dated to 8590 ±95 uncorrected radiocarbon years BP, which is amongst the oldest evidence of occupation in Scotland. There are many examples of structures from the Neolithic period, the finest example being the standing stones at Callanish, dating to the 3rd millennium BC. Cladh Hallan, a Bronze Age settlement on South Uist is the only site in the UK where prehistoric mummies have been found.

In 55 BC, the Greek historian Diodorus Siculus wrote that there was an island called "Hyperborea" (which means "beyond the North Wind"), where a round temple stood from which the moon appeared only a little distance above the earth every 19 years. This may have been a reference to the stone circle at Callanish.

A traveller called Demetrius of Tarsus related to Plutarch the tale of an expedition to the west coast of Scotland in or shortly before AD 83. He stated it was a gloomy journey amongst uninhabited islands, but he had visited one which was the retreat of holy men. He mentioned neither the druids nor the name of the island.

The first written records of native life begin in the 6th century AD, when the founding of the kingdom of Dál Riata took place. This encompassed roughly what is now Argyll and Bute and Lochaber in Scotland and County Antrim in Ireland. The figure of Columba looms large in any history of Dál Riata, and his founding of a monastery on Iona ensured that the kingdom would be of great importance in the spread of Christianity in northern Britain. However, Iona was far from unique. Lismore in the territory of the Cenél Loairn, was sufficiently important for the death of its abbots to be recorded with some frequency and many smaller sites, such as on Eigg, Hinba, and Tiree, are known from the annals.

North of Dál Riata, the Inner and Outer Hebrides were nominally under Pictish control, although the historical record is sparse. Hunter (2000) states that in relation to King Bridei I of the Picts in the sixth century: "As for Shetland, Orkney, Skye and the Western Isles, their inhabitants, most of whom appear to have been Pictish in culture and speech at this time, are likely to have regarded Bridei as a fairly distant presence.”

Viking raids began on Scottish shores towards the end of the 8th century and the Hebrides came under Norse control and settlement during the ensuing decades, especially following the success of Harald Fairhair at the Battle of in 872. In the Western Isles Ketill Flatnose may have been the dominant figure of the mid 9th century, by which time he had amassed a substantial island realm and made a variety of alliances with other Norse leaders. These princelings nominally owed allegiance to the Norwegian crown, although in practice the latter's control was fairly limited. Norse control of the Hebrides was formalised in 1098 when Edgar of Scotland formally signed the islands over to Magnus III of Norway. The Scottish acceptance of Magnus III as King of the Isles came after the Norwegian king had conquered Orkney, the Hebrides and the Isle of Man in a swift campaign earlier the same year, directed against the local Norwegian leaders of the various island petty kingdoms. By capturing the islands Magnus imposed a more direct royal control, although at a price. His skald Bjorn Cripplehand recorded that in Lewis "fire played high in the heaven" as "flame spouted from the houses" and that in the Uists "the king dyed his sword red in blood".

The Hebrides were now part of the Kingdom of the Isles, whose rulers were themselves vassals of the Kings of Norway. This situation lasted until the partitioning of the Western Isles in 1156, at which time the Outer Hebrides remained under Norwegian control while the Inner Hebrides broke out under Somerled, the Norse-Gael kinsman of the Manx royal house.

Following the ill-fated 1263 expedition of Haakon IV of Norway, the Outer Hebrides and the Isle of Man were yielded to the Kingdom of Scotland as a result of the 1266 Treaty of Perth. Although their contribution to the islands can still be found in personal and place names, the archaeological record of the Norse period is very limited. The best known find is the Lewis chessmen, which date from the mid 12th century.

As the Norse era drew to a close, the Norse-speaking princes were gradually replaced by Gaelic-speaking clan chiefs including the MacLeods of Lewis and Harris, Clan Donald and MacNeil of Barra. This transition did little to relieve the islands of internecine strife although by the early 14th century the MacDonald Lords of the Isles, based on Islay, were in theory these chiefs' feudal superiors and managed to exert some control.

The Lords of the Isles ruled the Inner Hebrides as well as part of the Western Highlands as subjects of the King of Scots until John MacDonald, fourth Lord of the Isles, squandered the family's powerful position. A rebellion by his nephew, Alexander of Lochalsh provoked an exasperated James IV to forfeit the family's lands in 1493.

In 1598, King James VI authorised some "Gentleman Adventurers" from Fife to civilise the "most barbarous Isle of Lewis". Initially successful, the colonists were driven out by local forces commanded by Murdoch and Neil MacLeod, who based their forces on in . The colonists tried again in 1605 with the same result, but a third attempt in 1607 was more successful and in due course Stornoway became a Burgh of Barony. By this time, Lewis was held by the Mackenzies of Kintail (later the Earls of Seaforth), who pursued a more enlightened approach, investing in fishing in particular. The Seaforths' royalist inclinations led to Lewis becoming garrisoned during the Wars of the Three Kingdoms by Cromwell's troops, who destroyed the old castle in Stornoway.

With the implementation of the Treaty of Union in 1707, the Hebrides became part of the new Kingdom of Great Britain, but the clans' loyalties to a distant monarch were not strong. A considerable number of islesmen "came out" in support of the Jacobite Earl of Mar in the 1715 and again in the 1745 rising including Macleod of Dunvegan and MacLea of Lismore. The aftermath of the decisive Battle of Culloden, which effectively ended Jacobite hopes of a Stuart restoration, was widely felt. The British government's strategy was to estrange the clan chiefs from their kinsmen and turn their descendants into English-speaking landlords whose main concern was the revenues their estates brought rather than the welfare of those who lived on them. This may have brought peace to the islands, but in the following century it came at a terrible price. In the wake of the rebellion, the clan system was broken up and islands of the Hebrides became a series of landed estates.

The early 19th century was a time of improvement and population growth. Roads and quays were built; the slate industry became a significant employer on Easdale and surrounding islands; and the construction of the Crinan and Caledonian canals and other engineering works such as Clachan Bridge improved transport and access. However, in the mid-19th century, the inhabitants of many parts of the Hebrides were devastated by the Clearances, which destroyed communities throughout the Highlands and Islands as the human populations were evicted and replaced with sheep farms. The position was exacerbated by the failure of the islands' kelp industry that thrived from the 18th century until the end of the Napoleonic Wars in 1815 and large scale emigration became endemic.

As , a Gaelic poet from South Uist, wrote for his countrymen who were obliged to leave the Hebrides in the late 18th century, emigration was the only alternative to "sinking into slavery" as the Gaels had been unfairly dispossessed by rapacious landlords. In the 1880s, the "Battle of the Braes" involved a demonstration against unfair land regulation and eviction, stimulating the calling of the Napier Commission. Disturbances continued until the passing of the 1886 Crofters' Act.

The Hebrides have a diverse geology ranging in age from Precambrian strata that are amongst the oldest rocks in Europe to Paleogene igneous intrusions. Raised shore platforms in the Hebrides are identified as strandflats formed possibly in Pliocene times and later modified by the Quaternary glaciations.

The Hebrides can be divided into two main groups, separated from one another by the Minch to the north and the Sea of the Hebrides to the south. The Inner Hebrides lie closer to mainland Scotland and include Islay, Jura, Skye, Mull, Raasay, Staffa and the Small Isles. There are 36 inhabited islands in this group. The Outer Hebrides are a chain of more than 100 islands and small skerries located about west of mainland Scotland. There are 15 inhabited islands in this archipelago. The main islands include Barra, Benbecula, Berneray, Harris, Lewis, North Uist, South Uist, and St Kilda. In total, the islands have an area of approximately and a population of 44,759.

A complication is that there are various descriptions of the scope of the Hebrides. The "Collins Encyclopedia of Scotland" describes the Inner Hebrides as lying "east of the Minch", which would include any and all offshore islands. There are various islands that lie in the sea lochs such as and that might not ordinarily be described as "Hebridean", but no formal definitions exist.

In the past, the Outer Hebrides were often referred to as the "Long Isle" (). Today, they are also known as the "Western Isles", although this phrase can also be used to refer to the Hebrides in general.

The Hebrides have a cool temperate climate that is remarkably mild and steady for such a northerly latitude, due to the influence of the Gulf Stream. In the Outer Hebrides the average temperature for the year is 6 °C (44 °F) in January and 14 °C (57 °F) in summer. The average annual rainfall in Lewis is and sunshine hours range from 1,100 – 1,200 "per annum" (13%). The summer days are relatively long, and May to August is the driest period.

The residents of the Hebrides have spoken a variety of different languages during the long period of human occupation.

It is assumed that Pictish must once have predominated in the northern Inner Hebrides and Outer Hebrides. The Scottish Gaelic language arrived from Ireland due to the growing influence of the kingdom of Dál Riata from the 6th century AD onwards, and became the dominant language of the southern Hebrides at that time. For a few centuries, the military might of the ' meant that Old Norse was prevalent in the Hebrides. North of , the place names that existed prior to the 9th century have been all but obliterated. The Old Norse name for the Hebrides during the Viking occupation was ', which means "Southern Isles"; in contrast to the ", or "Northern Isles" of Orkney and Shetland.

South of , Gaelic place names are more common, and after the 13th century, Gaelic became the main language of the entire Hebridean archipelago. Due to Scots and English being favoured in government and the educational system, the Hebrides have been in a state of diglossia since at least the 17th century. The Highland Clearances of the 19th century accelerated the language shift away from Scottish Gaelic, as did increased migration and the continuing lower status of Gaelic speakers. Nevertheless, as late as the end of the 19th century, there were significant populations of monolingual Gaelic speakers, and the Hebrides still contain the highest percentages of Gaelic speakers in Scotland. This is especially true of the Outer Hebrides, where a slim majority speak the language. The Scottish Gaelic college, , is based on Skye and Islay.

Ironically, given the status of the Western Isles as the last Gaelic-speaking stronghold in Scotland, the Gaelic language name for the islands – " – means "isles of the foreigners"; from the time when they were under Norse colonisation.

For those who remained, new economic opportunities emerged through the export of cattle, commercial fishing and tourism. Nonetheless emigration and military service became the choice of many and the archipelago's populations continued to dwindle throughout the late 19th century and for much of the 20th century. Lengthy periods of continuous occupation notwithstanding, many of the smaller islands were abandoned.

There were, however, continuing gradual economic improvements, among the most visible of which was the replacement of the traditional thatched blackhouse with accommodation of a more modern design and with the assistance of Highlands and Islands Enterprise many of the islands' populations have begun to increase after decades of decline. The discovery of substantial deposits of North Sea oil in 1965 and the renewables sector have contributed to a degree of economic stability in recent decades. For example, the Arnish yard has had a chequered history but has been a significant employer in both the oil and renewables industries.

The widespread immigration of mainlanders, particularly non-Gaelic speakers, has been a subject of controversy.

Many contemporary Gaelic musicians have roots in the Hebrides, including Julie Fowlis (North Uist), Catherine-Ann MacPhee (Barra), Kathleen MacInnes (South Uist), and Ishbel MacAskill (Lewis). All of these singers have repertoire based on the Hebridean tradition, such as ' and ' (waulking songs). This tradition includes many songs composed by little-known or anonymous poets before 1800, such as "", "" and "". Several of Runrig's songs are inspired by the archipelago; Calum and were raised on North Uist and Donnie Munro on Skye.

The Gaelic poet spent much of his life in the Hebrides and often referred to them in his poetry, including in ' and '. The best known Gaelic poet of her era, (Mary MacPherson, 1821–98), embodied the spirit of the land agitation of the 1870s and 1880s. This, and her powerful evocation of the Hebrides—she was from Skye—has made her among the most enduring Gaelic poets. Allan MacDonald (1859–1905), who spent his adult life on Eriskay and South Uist, composed hymns and verse in honour of the Blessed Virgin, the Christ Child, and the Eucharist. In his secular poetry, MacDonald praised the beauty of Eriskay and its people. In his verse drama, ' ("The Old Wives' Parliament"), he lampooned the gossiping of his female parishioners and local marriage customs.

In the 20th century, Murdo Macfarlane of Lewis wrote ', a well-known poem about the Gaelic revival in the Outer Hebrides. Sorley MacLean, the most respected 20th-century Gaelic writer, was born and raised on Raasay, where he set his best known poem, ', about the devastating effect of the Highland Clearances. , raised on South Uist and described by MacLean as "one of the few really significant living poets in Scotland, writing in any language" (West Highland Free Press, October 1992) wrote the Scottish Gaelic-language novel "" which was voted in the Top Ten of the 100 Best-Ever Books from Scotland.




In some respects the Hebrides lack biodiversity in comparison to mainland Britain; for example, there are only half as many mammalian species. However, these islands provide breeding grounds for many important seabird species including the world's largest colony of northern gannets. Avian life includes the corncrake, red-throated diver, rock dove, kittiwake, tystie, Atlantic puffin, goldeneye, golden eagle and white-tailed sea eagle. The latter was re-introduced to Rùm in 1975 and has successfully spread to various neighbouring islands, including Mull. There is a small population of red-billed chough concentrated on the islands of Islay and Colonsay.

Red deer are common on the hills and the grey seal and common seal are present around the coasts of Scotland. Colonies of seals are found on Oronsay and the Treshnish Isles. The rich freshwater streams contain brown trout, Atlantic salmon and water shrew. Offshore, minke whales, Killer whales, basking sharks, porpoises and dolphins are among the sealife that can be seen.

Heather moor containing ling, bell heather, cross-leaved heath, bog myrtle and fescues is abundant and there is a diversity of Arctic and alpine plants including Alpine pearlwort and mossy cyphal.

Loch Druidibeg on South Uist is a national nature reserve owned and managed by Scottish Natural Heritage. The reserve covers 1,677 hectares across the whole range of local habitats. Over 200 species of flowering plants have been recorded on the reserve, some of which are nationally scarce. South Uist is considered the best place in the UK for the aquatic plant slender naiad, which is a European Protected Species.

Hedgehogs are not native to the Outer Hebrides—they were introduced in the 1970s to reduce garden pests—and their spread poses a threat to the eggs of ground nesting wading birds. In 2003, Scottish Natural Heritage undertook culls of hedgehogs in the area although these were halted in 2007 due to protests. Trapped animals were relocated to the mainland.





</doc>
<doc id="14158" url="https://en.wikipedia.org/wiki?curid=14158" title="HMS Dreadnought">
HMS Dreadnought

Several ships and one submarine of the Royal Navy have borne the name HMS "Dreadnought" in the expectation that they would "dread nought", i.e. "fear nothing". The 1906 ship was one of the Royal Navy's most famous vessels; battleships built after her were referred to as 'dreadnoughts', and earlier battleships became known as pre-dreadnoughts.


Also

Citations

References



</doc>
<doc id="14159" url="https://en.wikipedia.org/wiki?curid=14159" title="Hartmann Schedel">
Hartmann Schedel

Hartmann Schedel (13 February 1440 – 28 November 1514) was a German historian, physician, humanist, and one of the first cartographers to use the printing press. He was born and died in Nuremberg. Matheolus Perusinus served as his tutor. 

Schedel is best known for his writing the text for the "Nuremberg Chronicle", known as "Schedelsche Weltchronik" (English: "Schedel's World Chronicle"), published in 1493 in Nuremberg. It was commissioned by Sebald Schreyer (1446–1520) and Sebastian Kammermeister (1446–1503). Maps in the "Chronicle" were the first ever illustrations of many cities and countries.

With the invention of the printing press by Johannes Gutenberg in 1447, it became feasible to print books and maps for a larger customer basis. Because they had to be handwritten, books were previously rare and very expensive.

Schedel was also a notable collector of books, art and old master prints. An album he had bound in 1504, which once contained five engravings by Jacopo de' Barbari, provides important evidence for dating de' Barbari's work.





</doc>
<doc id="14160" url="https://en.wikipedia.org/wiki?curid=14160" title="Hexameter">
Hexameter

Hexameter is a metrical line of verses consisting of six feet. It was the standard epic metre in classical Greek and Latin literature, such as in the "Iliad", "Odyssey" and "Aeneid". Its use in other genres of composition include Horace's satires, Ovid's "Metamorphoses," and the Hymns of Orpheus. According to Greek mythology, hexameter was invented by Phemonoe, daughter of Apollo and the first Pythia of Delphi.
In classical hexameter, the six feet follow these rules:

A short syllable (υ) is a syllable with a short vowel and no consonant at the end. A long syllable (–) is a syllable that either has a long vowel, one or more consonants at the end (or a long consonant), or both. Spaces between words are not counted in syllabification, so for instance "cat" is a long syllable in isolation, but "cat attack" would be syllabified as short-short-long: "ca", "ta", "tack" (υ υ –).

Variations of the sequence from line to line, as well as the use of caesura (logical full stops within the line) are essential in avoiding what may otherwise be a monotonous sing-song effect.

Although the rules seem simple, it is hard to use classical hexameter in English, because English is a stress-timed language that condenses vowels and consonants between stressed syllables, while hexameter relies on the regular timing of the phonetic sounds. Languages having the latter properties (i.e., languages that are not stress-timed) include Ancient Greek, Latin, Lithuanian and Hungarian.

While the above classical hexameter has never enjoyed much popularity in English, where the standard metre is iambic pentameter, English poems have frequently been written in iambic hexameter. There are numerous examples from the 16th century and a few from the 17th; the most prominent of these is Michael Drayton's "Poly-Olbion" (1612) in couplets of iambic hexameter. An example from Drayton (marking the feet):

In the 17th century the iambic hexameter, also called alexandrine, was used as a substitution in the heroic couplet, and as one of the types of permissible lines in lyrical stanzas and the Pindaric odes of Cowley and Dryden.

Several attempts were made in the 19th century to naturalise the dactylic hexameter to English, by Henry Wadsworth Longfellow, Arthur Hugh Clough and others, none of them particularly successful. Gerard Manley Hopkins wrote many of his poems in six-foot iambic and sprung rhythm lines. In the 20th century a loose ballad-like six-foot line with a strong medial pause was used by William Butler Yeats. The iambic six-foot line has also been used occasionally, and an accentual six-foot line has been used by translators from the Latin and many poets.

In the late 18th century the hexameter was adapted to the Lithuanian language by Kristijonas Donelaitis. His poem ""Metai" (The Seasons)" is considered the most successful hexameter text in Lithuanian as yet.

Hungarian is extremely suitable to hexameter (and other forms of poetry based on quantitative metre). It has been applied to Hungarian since 1541, introduced by the grammarian János Sylvester. It can even occur spontaneously: A student may extricate oneself from failing to remember a poem by saying "I'm stuck here, unfortunately the rest won't come into my mind," which is a hexameter in Hungarian:
Sándor Weöres included an ordinary nameplate text ("Gyula Tóth tinsmith and plumber") in one of his poems (this time, a pentameter):
An inscription on a bar of chocolate went as follows ("milk chocolate with apricot and biscuit pieces"), another hexameter, noticed by the poet Dániel Varró:

Due to this feature, hexameter has been widely used both in translated (Greek and Roman) and in original Hungarian poetry up to the twentieth century (e.g. by Miklós Radnóti).





</doc>
<doc id="14162" url="https://en.wikipedia.org/wiki?curid=14162" title="Timeline of Polish history">
Timeline of Polish history

This is a timeline of Polish history, comprising important legal and territorial changes and political events in Poland and its predecessor states. To read about the background to these events, see History of Poland. See also the list of Polish monarchs and list of Prime Ministers of Poland.





</doc>
<doc id="14168" url="https://en.wikipedia.org/wiki?curid=14168" title="Himalia">
Himalia

Himalia may refer to:



</doc>
<doc id="14169" url="https://en.wikipedia.org/wiki?curid=14169" title="Heracleidae">
Heracleidae

In Greek mythology, the Heracleidae (; ) or Heraclids were the numerous descendants of Heracles (Hercules), especially applied in a narrower sense to the descendants of Hyllus, the eldest of his four sons by Deianira (Hyllus was also sometimes thought of as Heracles' son by Melite). Other Heracleidae included Macaria, Lamos, Manto, Bianor, Tlepolemus, and Telephus. These Heraclids were a group of Dorian kings who conquered the Peloponnesian kingdoms of Mycenae, Sparta and Argos; according to the literary tradition in Greek mythology, they claimed a right to rule through their ancestor. Since Karl Otfried Müller's "Die Dorier" (1830, English translation 1839), I. ch. 3, their rise to dominance has been associated with a "Dorian invasion".

Though details of genealogy differ from one ancient author to another, the cultural significance of the mythic theme, that the descendants of Heracles, exiled after his death, returned some generations later to reclaim land that their ancestors had held in Mycenaean Greece, was to assert the primal legitimacy of a traditional ruling clan that traced its origin, thus its legitimacy, to Heracles.

Heracles, whom Zeus had originally intended to be ruler of Argos, Lacedaemon and Messenian Pylos, had been supplanted by the cunning of Hera, and his intended possessions had fallen into the hands of Eurystheus, king of Mycenae. After the death of Heracles, his children, after many wanderings, found refuge from Eurystheus at Athens. Eurystheus, on his demand for their surrender being refused, attacked Athens, but was defeated and slain. Hyllus and his brothers then invaded Peloponnesus, but after a year's stay were forced by a pestilence to quit. They withdrew to Thessaly, where Aegimius, the mythical ancestor of the Dorians, whom Heracles had assisted in war against the Lapithae, adopted Hyllus and made over to him a third part of his territory.

After the death of Aegimius, his two sons, Pamphylus and Dymas, voluntarily submitted to Hyllus (who was, according to the Dorian tradition in Herodotus V. 72, really an Achaean), who thus became ruler of the Dorians, the three branches of that race being named after these three heroes. Desiring to reconquer his paternal inheritance, Hyllus consulted the Delphic oracle, which told him to wait for "the third fruit", (or "the third crop") and then enter Peloponnesus by "a narrow passage by sea". Accordingly, after three years, Hyllus marched across the isthmus of Corinth to attack Atreus, the successor of Eurystheus, but was slain in single combat by Echemus, king of Tegea. This second attempt was followed by a third under Cleodaeus and a fourth under Aristomachus, both unsuccessful.

At last, Temenus, Cresphontes and Aristodemus, the sons of Aristomachus, complained to the oracle that its instructions had proved fatal to those who had followed them. They received the answer that by the "third fruit" the "third generation" was meant, and that the "narrow passage" was not the isthmus of Corinth, but the straits of Rhium. They accordingly built a fleet at Naupactus, but before they set sail, Aristodemus was struck by lightning (or shot by Apollo) and the fleet destroyed, because one of the Heracleidae had slain an Acarnanian soothsayer.

The oracle, being again consulted by Temenus, bade him offer an expiatory sacrifice and banish the murderer for ten years, and look out for a man with three eyes to act as guide. On his way back to Naupactus, Temenus fell in with Oxylus, an Aetolian, who had lost one eye, riding on a horse (thus making up the three eyes) and immediately pressed him into his service. According to another account, a mule on which Oxylus rode had lost an eye. The Heracleidae repaired their ships, sailed from Naupactus to Antirrhium, and thence to Rhium in Peloponnesus. A decisive battle was fought with Tisamenus, son of Orestes, the chief ruler in the peninsula, who was defeated and slain. This conquest was traditionally dated eighty years after the Trojan War.

The Heracleidae, who thus became practically masters of Peloponnesus, proceeded to distribute its territory among themselves by lot. Argos fell to Temenus, Lacedaemon to Procles and Eurysthenes, the twin sons of Aristodemus; and Messenia to Cresphontes (tradition maintains that Cresphontes cheated in order to obtain Messenia, which had the best land of all.) The fertile district of Elis had been reserved by agreement for Oxylus. The Heracleidae ruled in Lacedaemon until 221 BCE, but disappeared much earlier in the other countries.

This conquest of Peloponnesus by the Dorians, commonly called the "Dorian invasion" or the "Return of the Heraclidae", is represented as the recovery by the descendants of Heracles of the rightful inheritance of their hero ancestor and his sons. The Dorians followed the custom of other Greek tribes in claiming as ancestor for their ruling families one of the legendary heroes, but the traditions must not on that account be regarded as entirely mythical. They represent a joint invasion of Peloponnesus by Aetolians and Dorians, the latter having been driven southward from their original northern home under pressure from the Thessalians. It is noticeable that there is no mention of these Heraclidae or their invasion in Homer or Hesiod. Herodotus (vi. 52) speaks of poets who had celebrated their deeds, but these were limited to events immediately succeeding the death of Heracles.

At Sparta, the Heraclids formed two dynasties ruling jointly: the Agiads and the Eurypontids.

At Corinth the Heraclids ruled as the Bacchiadae dynasty before the aristocratic revolution, which brought a Bacchiad aristocracy into power. The kings were as follows:

The Greek tragedians amplified the story, probably drawing inspiration from local legends which glorified the services rendered by Athens to the rulers of Peloponnesus.

The Heracleidae feature as the main subjects of Euripides' play, "Heracleidae". J. A. Spranger found the political subtext of "Heracleidae", never far to seek, so particularly apt in Athens towards the end of the peace of Nicias, in 419 BCE, that he suggested the date as that of the play's first performance.

In the tragedy, Iolaus, Heracles' old comrade, and Heracles' children, Macaria and her brothers and sisters have hidden from Eurystheus in Athens, ruled by King Demophon; as the first scene makes clear, they expect that the blood relationship of the kings with Heracles and their father's past indebtedness to Theseus will finally provide them sanctuary. As Eurystheus prepares to attack, an oracle tells Demophon that only the sacrifice of a noble woman to Persephone can guarantee an Athenian victory. Macaria volunteers for the sacrifice and a spring is named the Macarian spring in her honor.




</doc>
<doc id="14170" url="https://en.wikipedia.org/wiki?curid=14170" title="HIV">
HIV

The human immunodeficiency viruses (HIV) are two species of "Lentivirus" (a subgroup of retrovirus) that infect humans. Over time, they cause acquired immunodeficiency syndrome (AIDS), a condition in which progressive failure of the immune system allows life-threatening opportunistic infections and cancers to thrive. Without treatment, average survival time after infection with HIV is estimated to be 9 to 11 years, depending on the HIV subtype. In most cases, HIV is a sexually transmitted infection and occurs by contact with or transfer of blood, pre-ejaculate, semen, and vaginal fluids. Research has shown (for both same-sex and opposite-sex couples) that HIV is untransmittable through condomless sexual intercourse if the HIV-positive partner has a consistently undetectable viral load. Non-sexual transmission can occur from an infected mother to her infant during pregnancy, during childbirth by exposure to her blood or vaginal fluid, and through breast milk. Within these bodily fluids, HIV is present as both free virus particles and virus within infected immune cells.

HIV infects vital cells in the human immune system, such as helper T cells (specifically CD4 T cells), macrophages, and dendritic cells. HIV infection leads to low levels of CD4 T cells through a number of mechanisms, including pyroptosis of abortively infected T cells, apoptosis of uninfected bystander cells, direct viral killing of infected cells, and killing of infected CD4 T cells by CD8 cytotoxic lymphocytes that recognize infected cells. When CD4 T cell numbers decline below a critical level, cell-mediated immunity is lost, and the body becomes progressively more susceptible to opportunistic infections, leading to the development of AIDS.

HIV is a member of the genus "Lentivirus", part of the family "Retroviridae". Lentiviruses have many morphologies and biological properties in common. Many species are infected by lentiviruses, which are characteristically responsible for long-duration illnesses with a long incubation period. Lentiviruses are transmitted as single-stranded, positive-sense, enveloped RNA viruses. Upon entry into the target cell, the viral RNA genome is converted (reverse transcribed) into double-stranded DNA by a virally encoded enzyme, reverse transcriptase, that is transported along with the viral genome in the virus particle. The resulting viral DNA is then imported into the cell nucleus and integrated into the cellular DNA by a virally encoded enzyme, integrase, and host co-factors. Once integrated, the virus may become latent, allowing the virus and its host cell to avoid detection by the immune system, for an indeterminate amount of time. The HIV virus can remain dormant in the human body for up to ten years after primary infection; during this period the virus does not cause symptoms. Alternatively, the integrated viral DNA may be transcribed, producing new RNA genomes and viral proteins, using host cell resources, that are packaged and released from the cell as new virus particles that will begin the replication cycle anew.

Two types of HIV have been characterized: HIV-1 and HIV-2. HIV-1 is the virus that was initially discovered and termed both lymphadenopathy associated virus (LAV) and human T-lymphotropic virus 3 (HTLV-III). HIV-1 is more virulent and more infective than HIV-2, and is the cause of the majority of HIV infections globally. The lower infectivity of HIV-2, compared to HIV-1, implies that fewer of those exposed to HIV-2 will be infected per exposure. Due to its relatively poor capacity for transmission, HIV-2 is largely confined to West Africa.

HIV is different in structure from other retroviruses. It is roughly spherical with a diameter of about 120 nm, around 60 times smaller than a red blood cell. It is composed of two copies of positive-sense single-stranded RNA that codes for the virus's nine genes enclosed by a conical capsid composed of 2,000 copies of the viral protein p24. The single-stranded RNA is tightly bound to nucleocapsid proteins, p7, and enzymes needed for the development of the virion such as reverse transcriptase, proteases, ribonuclease and integrase. A matrix composed of the viral protein p17 surrounds the capsid ensuring the integrity of the virion particle.

This is, in turn, surrounded by the viral envelope, that is composed of the lipid bilayer taken from the membrane of a human host cell when the newly formed virus particle buds from the cell. The viral envelope contains proteins from the host cell and relatively few copies of the HIV envelope protein, which consists of a cap made of three molecules known as glycoprotein (gp) 120, and a stem consisting of three gp41 molecules that anchor the structure into the viral envelope. The envelope protein, encoded by the HIV "env" gene, allows the virus to attach to target cells and fuse the viral envelope with the target cell's membrane releasing the viral contents into the cell and initiating the infectious cycle.

As the sole viral protein on the surface of the virus, the envelope protein is a major target for HIV vaccine efforts. Over half of the mass of the trimeric envelope spike is N-linked glycans. The density is high as the glycans shield the underlying viral protein from neutralisation by antibodies. This is one of the most densely glycosylated molecules known and the density is sufficiently high to prevent the normal maturation process of glycans during biogenesis in the endoplasmic and Golgi apparatus. The majority of the glycans are therefore stalled as immature 'high-mannose' glycans not normally present on human glycoproteins that are secreted or present on a cell surface. The unusual processing and high density means that almost all broadly neutralising antibodies that have so far been identified (from a subset of patients that have been infected for many months to years) bind to, or are adapted to cope with, these envelope glycans.

The molecular structure of the viral spike has now been determined by X-ray crystallography and cryogenic electron microscopy. These advances in structural biology were made possible due to the development of stable recombinant forms of the viral spike by the introduction of an intersubunit disulphide bond and an isoleucine to proline mutation (radical replacement of an amino acid) in gp41. The so-called SOSIP trimers not only reproduce the antigenic properties of the native viral spike, but also display the same degree of immature glycans as presented on the native virus. Recombinant trimeric viral spikes are promising vaccine candidates as they display less non-neutralising epitopes than recombinant monomeric gp120, which act to suppress the immune response to target epitopes. 

The RNA genome consists of at least seven structural landmarks (LTR, TAR, RRE, PE, SLIP, CRS, and INS), and nine genes ("gag", "pol", and "env", "tat", "rev", "nef", "vif", "vpr", "vpu", and sometimes a tenth "tev", which is a fusion of "tat", "env" and "rev"), encoding 19 proteins. Three of these genes, "gag", "pol", and "env", contain information needed to make the structural proteins for new virus particles. For example, "env" codes for a protein called gp160 that is cut in two by a cellular protease to form gp120 and gp41. The six remaining genes, "tat", "rev", "nef", "vif", "vpr", and "vpu" (or "vpx" in the case of HIV-2), are regulatory genes for proteins that control the ability of HIV to infect cells, produce new copies of virus (replicate), or cause disease.

The two "tat" proteins (p16 and p14) are transcriptional transactivators for the LTR promoter acting by binding the TAR RNA element. The TAR may also be processed into microRNAs that regulate the apoptosis genes "ERCC1" and "IER3". The "rev" protein (p19) is involved in shuttling RNAs from the nucleus and the cytoplasm by binding to the RRE RNA element. The "vif" protein (p23) prevents the action of APOBEC3G (a cellular protein that deaminates cytidine to uridine in the single-stranded viral DNA and/or interferes with reverse transcription). The "vpr" protein (p14) arrests cell division at G2/M. The "nef" protein (p27) down-regulates CD4 (the major viral receptor), as well as the MHC class I and class II molecules.

"Nef" also interacts with SH3 domains. The "vpu" protein (p16) influences the release of new virus particles from infected cells. The ends of each strand of HIV RNA contain an RNA sequence called a long terminal repeat (LTR). Regions in the LTR act as switches to control production of new viruses and can be triggered by proteins from either HIV or the host cell. The Psi element is involved in viral genome packaging and recognized by "gag" and "rev" proteins. The SLIP element () is involved in the frameshift in the "gag"-"pol" reading frame required to make functional "pol".

The term viral tropism refers to the cell types a virus infects. HIV can infect a variety of immune cells such as CD4 T cells, macrophages, and microglial cells. HIV-1 entry to macrophages and CD4 T cells is mediated through interaction of the virion envelope glycoproteins (gp120) with the CD4 molecule on the target cells' membrane and also with chemokine co-receptors.

Macrophage-tropic (M-tropic) strains of HIV-1, or non-syncytia-inducing strains (NSI; now called R5 viruses) use the "β"-chemokine receptor, CCR5, for entry and are thus able to replicate in both macrophages and CD4 T cells. This CCR5 co-receptor is used by almost all primary HIV-1 isolates regardless of viral genetic subtype. Indeed, macrophages play a key role in several critical aspects of HIV infection. They appear to be the first cells infected by HIV and perhaps the source of HIV production when CD4 cells become depleted in the patient. Macrophages and microglial cells are the cells infected by HIV in the central nervous system. In the tonsils and adenoids of HIV-infected patients, macrophages fuse into multinucleated giant cells that produce huge amounts of virus.

T-tropic strains of HIV-1, or syncytia-inducing strains (SI; now called X4 viruses) replicate in primary CD4 T cells as well as in macrophages and use the "α"-chemokine receptor, CXCR4, for entry.

Dual-tropic HIV-1 strains are thought to be transitional strains of HIV-1 and thus are able to use both CCR5 and CXCR4 as co-receptors for viral entry.

The "α"-chemokine SDF-1, a ligand for CXCR4, suppresses replication of T-tropic HIV-1 isolates. It does this by down-regulating the expression of CXCR4 on the surface of HIV target cells. M-tropic HIV-1 isolates that use only the CCR5 receptor are termed R5; those that use only CXCR4 are termed X4, and those that use both, X4R5. However, the use of co-receptors alone does not explain viral tropism, as not all R5 viruses are able to use CCR5 on macrophages for a productive infection and HIV can also infect a subtype of myeloid dendritic cells, which probably constitute a reservoir that maintains infection when CD4 T cell numbers have declined to extremely low levels.

Some people are resistant to certain strains of HIV. For example, people with the CCR5-Δ32 mutation are resistant to infection by the R5 virus, as the mutation leaves HIV unable to bind to this co-receptor, reducing its ability to infect target cells.

Sexual intercourse is the major mode of HIV transmission. Both X4 and R5 HIV are present in the seminal fluid, which enables the virus to be transmitted from a male to his sexual partner. The virions can then infect numerous cellular targets and disseminate into the whole organism. However, a selection process leads to a predominant transmission of the R5 virus through this pathway. In patients infected with subtype B HIV-1, there is often a co-receptor switch in late-stage disease and T-tropic variants that can infect a variety of T cells through CXCR4. These variants then replicate more aggressively with heightened virulence that causes rapid T cell depletion, immune system collapse, and opportunistic infections that mark the advent of AIDS. HIV-positive patients acquire an enormously broad spectrum of opportunistic infections, which was particularly problematic prior to the onset of HAART therapies; however, the same infections are reported among HIV-infected patients examined post-mortem following the onset of antiretroviral therapies. Thus, during the course of infection, viral adaptation to the use of CXCR4 instead of CCR5 may be a key step in the progression to AIDS. A number of studies with subtype B-infected individuals have determined that between 40 and 50 percent of AIDS patients can harbour viruses of the SI and, it is presumed, the X4 phenotypes.

HIV-2 is much less pathogenic than HIV-1 and is restricted in its worldwide distribution to West Africa. The adoption of "accessory genes" by HIV-2 and its more promiscuous pattern of co-receptor usage (including CD4-independence) may assist the virus in its adaptation to avoid innate restriction factors present in host cells. Adaptation to use normal cellular machinery to enable transmission and productive infection has also aided the establishment of HIV-2 replication in humans. A survival strategy for any infectious agent is not to kill its host, but ultimately become a commensal organism. Having achieved a low pathogenicity, over time, variants that are more successful at transmission will be selected.

The HIV virion enters macrophages and CD4 T cells by the adsorption of glycoproteins on its surface to receptors on the target cell followed by fusion of the viral envelope with the target cell membrane and the release of the HIV capsid into the cell.

Entry to the cell begins through interaction of the trimeric envelope complex (gp160 spike) on the HIV viral envelope and both CD4 and a chemokine co-receptor (generally either CCR5 or CXCR4, but others are known to interact) on the target cell surface. Gp120 binds to integrin αβ activating LFA-1, the central integrin involved in the establishment of virological synapses, which facilitate efficient cell-to-cell spreading of HIV-1. The gp160 spike contains binding domains for both CD4 and chemokine receptors.

The first step in fusion involves the high-affinity attachment of the CD4 binding domains of gp120 to CD4. Once gp120 is bound with the CD4 protein, the envelope complex undergoes a structural change, exposing the chemokine receptor binding domains of gp120 and allowing them to interact with the target chemokine receptor. This allows for a more stable two-pronged attachment, which allows the N-terminal fusion peptide gp41 to penetrate the cell membrane. Repeat sequences in gp41, HR1, and HR2 then interact, causing the collapse of the extracellular portion of gp41 into a hairpin shape. This loop structure brings the virus and cell membranes close together, allowing fusion of the membranes and subsequent entry of the viral capsid.

After HIV has bound to the target cell, the HIV RNA and various enzymes, including reverse transcriptase, integrase, ribonuclease, and protease, are injected into the cell. During the microtubule-based transport to the nucleus, the viral single-strand RNA genome is transcribed into double-strand DNA, which is then integrated into a host chromosome.

HIV can infect dendritic cells (DCs) by this CD4-CCR5 route, but another route using mannose-specific C-type lectin receptors such as DC-SIGN can also be used. DCs are one of the first cells encountered by the virus during sexual transmission. They are currently thought to play an important role by transmitting HIV to T cells when the virus is captured in the mucosa by DCs. The presence of FEZ-1, which occurs naturally in neurons, is believed to prevent the infection of cells by HIV.
HIV-1 entry, as well as entry of many other retroviruses, has long been believed to occur exclusively at the plasma membrane. More recently, however, productive infection by pH-independent, clathrin-mediated endocytosis of HIV-1 has also been reported and was recently suggested to constitute the only route of productive entry.

Shortly after the viral capsid enters the cell, an enzyme called reverse transcriptase liberates the positive-sense single-stranded RNA genome from the attached viral proteins and copies it into a complementary DNA (cDNA) molecule. The process of reverse transcription is extremely error-prone, and the resulting mutations may cause drug resistance or allow the virus to evade the body's immune system. The reverse transcriptase also has ribonuclease activity that degrades the viral RNA during the synthesis of cDNA, as well as DNA-dependent DNA polymerase activity that creates a sense DNA from the "antisense" cDNA. Together, the cDNA and its complement form a double-stranded viral DNA that is then transported into the cell nucleus. The integration of the viral DNA into the host cell's genome is carried out by another viral enzyme called integrase.

The integrated viral DNA may then lie dormant, in the latent stage of HIV infection. To actively produce the virus, certain cellular transcription factors need to be present, the most important of which is NF-"κ"B (nuclear factor kappa B), which is upregulated when T cells become activated. This means that those cells most likely to be targeted, entered and subsequently killed by HIV are those actively fighting infection.

During viral replication, the integrated DNA provirus is transcribed into RNA, some of which then undergo RNA splicing to produce mature messenger RNAs (mRNAs). These mRNAs are exported from the nucleus into the cytoplasm, where they are translated into the regulatory proteins Tat (which encourages new virus production) and Rev. As the newly produced Rev protein is produced it moves to the nucleus, where it binds to full-length, unspliced copies of virus RNAs and allows them to leave the nucleus. Some of these full-length RNAs function as new copies of the virus genome, while others function as mRNAs that are translated to produce the structural proteins Gag and Env. Gag proteins bind to copies of the virus RNA genome to package them into new virus particles.

HIV-1 and HIV-2 appear to package their RNA differently. HIV-1 will bind to any appropriate RNA. HIV-2 will preferentially bind to the mRNA that was used to create the Gag protein itself.

Two RNA genomes are encapsidated in each HIV-1 particle (see Structure and genome of HIV). Upon infection and replication catalyzed by reverse transcriptase, recombination between the two genomes can occur. Recombination occurs as the single-strand, positive-sense RNA genomes are reverse transcribed to form DNA. During reverse transcription, the nascent DNA can switch multiple times between the two copies of the viral RNA. This form of recombination is known as copy-choice. Recombination events may occur throughout the genome. Anywhere from two to 20 recombination events per genome may occur at each replication cycle, and these events can rapidly shuffle the genetic information that is transmitted from parental to progeny genomes.

Viral recombination produces genetic variation that likely contributes to the evolution of resistance to anti-retroviral therapy. Recombination may also contribute, in principle, to overcoming the immune defenses of the host. Yet, for the adaptive advantages of genetic variation to be realized, the two viral genomes packaged in individual infecting virus particles need to have arisen from separate progenitor parental viruses of differing genetic constitution. It is unknown how often such mixed packaging occurs under natural conditions.

Bonhoeffer "et al." suggested that template switching by reverse transcriptase acts as a repair process to deal with breaks in the single-stranded RNA genome. In addition, Hu and Temin suggested that recombination is an adaptation for repair of damage in the RNA genomes. Strand switching (copy-choice recombination) by reverse transcriptase could generate an undamaged copy of genomic DNA from two damaged single-stranded RNA genome copies. This view of the adaptive benefit of recombination in HIV could explain why each HIV particle contains two complete genomes, rather than one. Furthermore, the view that recombination is a repair process implies that the benefit of repair can occur at each replication cycle, and that this benefit can be realized whether or not the two genomes differ genetically. On the view that recombination in HIV is a repair process, the generation of recombinational variation would be a consequence, but not the cause of, the evolution of template switching.

HIV-1 infection causes chronic inflammation and production of reactive oxygen species. Thus, the HIV genome may be vulnerable to oxidative damages, including breaks in the single-stranded RNA. For HIV, as well as for viruses in general, successful infection depends on overcoming host defensive strategies that often include production of genome-damaging reactive oxygen species. Thus, Michod "et al." suggested that recombination by viruses is an adaptation for repair of genome damages, and that recombinational variation is a byproduct that may provide a separate benefit.

The final step of the viral cycle, assembly of new HIV-1 virions, begins at the plasma membrane of the host cell. The Env polyprotein (gp160) goes through the endoplasmic reticulum and is transported to the Golgi apparatus where it is cleaved by furin resulting in the two HIV envelope glycoproteins, gp41 and gp120. These are transported to the plasma membrane of the host cell where gp41 anchors gp120 to the membrane of the infected cell. The Gag (p55) and Gag-Pol (p160) polyproteins also associate with the inner surface of the plasma membrane along with the HIV genomic RNA as the forming virion begins to bud from the host cell. The budded virion is still immature as the gag polyproteins still need to be cleaved into the actual matrix, capsid and nucleocapsid proteins. This cleavage is mediated by the packaged viral protease and can be inhibited by antiretroviral drugs of the protease inhibitor class. The various structural components then assemble to produce a mature HIV virion. Only mature virions are then able to infect another cell.

The classical process of infection of a cell by a virion can be called "cell-free spread" to distinguish it from a more recently recognized process called "cell-to-cell spread". In cell-free spread (see figure), virus particles bud from an infected T cell, enter the blood or extracellular fluid and then infect another T cell following a chance encounter. HIV can also disseminate by direct transmission from one cell to another by a process of cell-to-cell spread, for which two pathways have been described. Firstly, an infected T cell can transmit virus directly to a target T cell via a virological synapse. Secondly, an antigen-presenting cell (APC), such as a macrophage or dendritic cell, can transmit HIV to T cells by a process that either involves productive infection (in the case of macrophages) or capture and transfer of virions "in trans" (in the case of dendritic cells). Whichever pathway is used, infection by cell-to-cell transfer is reported to be much more efficient than cell-free virus spread. A number of factors contribute to this increased efficiency, including polarised virus budding towards the site of cell-to-cell contact, close apposition of cells, which minimizes fluid-phase diffusion of virions, and clustering of HIV entry receptors on the target cell towards the contact zone. Cell-to-cell spread is thought to be particularly important in lymphoid tissues where CD4 T cells are densely packed and likely to interact frequently. Intravital imaging studies have supported the concept of the HIV virological synapse "in vivo". The many spreading mechanisms available to HIV contribute to the virus' ongoing replication in spite of anti-retroviral therapies.

HIV differs from many viruses in that it has very high genetic variability. This diversity is a result of its fast replication cycle, with the generation of about 10 virions every day, coupled with a high mutation rate of approximately 3 x 10 per nucleotide base per cycle of replication and recombinogenic properties of reverse transcriptase.

This complex scenario leads to the generation of many variants of HIV in a single infected patient in the course of one day. This variability is compounded when a single cell is simultaneously infected by two or more different strains of HIV. When simultaneous infection occurs, the genome of progeny virions may be composed of RNA strands from two different strains. This hybrid virion then infects a new cell where it undergoes replication. As this happens, the reverse transcriptase, by jumping back and forth between the two different RNA templates, will generate a newly synthesized retroviral DNA sequence that is a recombinant between the two parental genomes. This recombination is most obvious when it occurs between subtypes.

The closely related simian immunodeficiency virus (SIV) has evolved into many strains, classified by the natural host species. SIV strains of the African green monkey (SIVagm) and sooty mangabey (SIVsmm) are thought to have a long evolutionary history with their hosts. These hosts have adapted to the presence of the virus, which is present at high levels in the host's blood, but evokes only a mild immune response, does not cause the development of simian AIDS, and does not undergo the extensive mutation and recombination typical of HIV infection in humans.

In contrast, when these strains infect species that have not adapted to SIV ("heterologous" or similar hosts such as rhesus or cynomologus macaques), the animals develop AIDS and the virus generates genetic diversity similar to what is seen in human HIV infection. Chimpanzee SIV (SIVcpz), the closest genetic relative of HIV-1, is associated with increased mortality and AIDS-like symptoms in its natural host. SIVcpz appears to have been transmitted relatively recently to chimpanzee and human populations, so their hosts have not yet adapted to the virus. This virus has also lost a function of the "nef" gene that is present in most SIVs. For non-pathogenic SIV variants, "nef" suppresses T cell activation through the CD3 marker. "Nef"'s function in non-pathogenic forms of SIV is to downregulate expression of inflammatory cytokines, MHC-1, and signals that affect T cell trafficking. In HIV-1 and SIVcpz, "nef" does not inhibit T-cell activation and it has lost this function. Without this function, T cell depletion is more likely, leading to immunodeficiency.

Three groups of HIV-1 have been identified on the basis of differences in the envelope ("env") region: M, N, and O. Group M is the most prevalent and is subdivided into eight subtypes (or clades), based on the whole genome, which are geographically distinct. The most prevalent are subtypes B (found mainly in North America and Europe), A and D (found mainly in Africa), and C (found mainly in Africa and Asia); these subtypes form branches in the phylogenetic tree representing the lineage of the M group of HIV-1. Co-infection with distinct subtypes gives rise to circulating recombinant forms (CRFs). In 2000, the last year in which an analysis of global subtype prevalence was made, 47.2% of infections worldwide were of subtype C, 26.7% were of subtype A/CRF02_AG, 12.3% were of subtype B, 5.3% were of subtype D, 3.2% were of CRF_AE, and the remaining 5.3% were composed of other subtypes and CRFs. Most HIV-1 research is focused on subtype B; few laboratories focus on the other subtypes. The existence of a fourth group, "P", has been hypothesised based on a virus isolated in 2009. The strain is apparently derived from gorilla SIV (SIVgor), first isolated from western lowland gorillas in 2006.

HIV-2's closest relative is SIVsm, a strain of SIV found in sooty mangabees. Since HIV-1 is derived from SIVcpz, and HIV-2 from SIVsm, the genetic sequence of HIV-2 is only partially homologous to HIV-1 and more closely resembles that of SIVsm.

Many HIV-positive people are unaware that they are infected with the virus. For example, in 2001 less than 1% of the sexually active urban population in Africa had been tested, and this proportion is even lower in rural populations. Furthermore, in 2001 only 0.5% of pregnant women attending urban health facilities were counselled, tested or receive their test results. Again, this proportion is even lower in rural health facilities. Since donors may therefore be unaware of their infection, donor blood and blood products used in medicine and medical research are routinely screened for HIV.

HIV-1 testing is initially done using an enzyme-linked immunosorbent assay (ELISA) to detect antibodies to HIV-1. Specimens with a non-reactive result from the initial ELISA are considered HIV-negative, unless new exposure to an infected partner or partner of unknown HIV status has occurred. Specimens with a reactive ELISA result are retested in duplicate. If the result of either duplicate test is reactive, the specimen is reported as repeatedly reactive and undergoes confirmatory testing with a more specific supplemental test (e.g., a polymerase chain reaction (PCR), western blot or, less commonly, an immunofluorescence assay (IFA)). Only specimens that are repeatedly reactive by ELISA and positive by IFA or PCR or reactive by western blot are considered HIV-positive and indicative of HIV infection. Specimens that are repeatedly ELISA-reactive occasionally provide an indeterminate western blot result, which may be either an incomplete antibody response to HIV in an infected person or nonspecific reactions in an uninfected person.

Although IFA can be used to confirm infection in these ambiguous cases, this assay is not widely used. In general, a second specimen should be collected more than a month later and retested for persons with indeterminate western blot results. Although much less commonly available, nucleic acid testing (e.g., viral RNA or proviral DNA amplification method) can also help diagnosis in certain situations. In addition, a few tested specimens might provide inconclusive results because of a low quantity specimen. In these situations, a second specimen is collected and tested for HIV infection.

Modern HIV testing is extremely accurate, when the window period is taken into consideration. A single screening test is correct more than 99% of the time. The chance of a false-positive result in a standard two-step testing protocol is estimated to be about 1 in 250,000 in a low risk population. Testing post-exposure is recommended immediately and then at six weeks, three months, and six months.

The latest recommendations of the US Centers for Disease Control and Prevention (CDC) show that HIV testing must start with an immunoassay combination test for HIV-1 and HIV-2 antibodies and p24 antigen. A negative result rules out HIV exposure, while a positive one must be followed by an HIV-1/2 antibody differentiation immunoassay to detect which antibodies are present. This gives rise to four possible scenarios:

HIV/AIDS research includes all medical research that attempts to prevent, treat, or cure HIV/AIDS, as well as fundamental research about the nature of HIV as an infectious agent and AIDS as the disease caused by HIV.

Many governments and research institutions participate in HIV/AIDS research. This research includes behavioral health interventions, such as research into sex education, and drug development, such as research into microbicides for sexually transmitted diseases, HIV vaccines, and anti-retroviral drugs. Other medical research areas include the topics of pre-exposure prophylaxis, post-exposure prophylaxis, circumcision and HIV, and accelerated aging effects.

The management of HIV/AIDS normally includes the use of multiple antiretroviral drugs. In many parts of the world, HIV has become a chronic condition in which progression to AIDS is increasingly rare.

HIV latency, and the consequent viral reservoir in CD4 T cells, dendritic cells, as well as macrophages, is the main barrier to eradication of the virus.

It is important to note that although HIV is highly virulent, transmission does not occur through sex when an HIV-positive person has a consistently undetectable viral load (<50 copies/ml) due to anti-retroviral treatment. Previously it was said the chance of transmission was "very low" or "negligible" (The "Swiss Statement"). However, following multiple studies, it is now clear that the chance of passing on HIV through sex is effectively zero where the HIV-positive person has a consistently undetectable viral load; this is known as U=U, "Undetectable=Untransmittable", also phrased as "can't pass it on". The studies demonstrating U=U are: Opposites Attract, PARTNER 1, PARTNER 2, (for male-male couples) and HPTN052 (for heterosexual couples) when "the partner living with HIV had a durably suppressed viral load." In these studies, couples where one partner was HIV positive and one partner was HIV negative were enrolled and regular HIV testing completed. In total from the four studies, 4097 couples were enrolled over four continents and 151,880 acts of condomless sex were reported; there were zero phylogenetically linked transmissions of HIV where the positive partner had an undetectable viral load. Following this, the U=U consensus statement advocating the use of "zero risk" was signed by hundreds of individuals and organisations, including the US CDC, British HIV Association and "The Lancet" medical journal. The importance of the final results of the PARTNER 2 study were described by the medical director of the Terrence Higgins Trust as "impossible to overstate," while lead author Alison Rodger declared that the message that "undetectable viral load makes HIV untransmittable ... can help end the HIV pandemic by preventing HIV transmission. The authors summarised their findings in "The Lancet" as follows:

This result is consistent with the conclusion presented by Anthony S. Fauci, the Director of the National Institute of Allergy and Infectious Diseases for the U.S. National Institutes of Health, and his team in a viewpoint published in the "Journal of the American Medical Association", that U=U is an effective HIV prevention method when an undetectable viral load is maintained.

Genital herpes (HSV-2) reactivation in those infected with the virus have an associated increase in CCR-5 enriched CD4+ T cells as well as inflammatory dendritic cells in the submucosa of the genital skin. Tropism of HIV for CCR-5 positive cells explains the two to threefold increase in HIV acquisition among persons with genital herpes. Daily antiviral (e.g. acyclovir) medication do not reduce the sub-clinical post reactivation inflammation and therefore does not confer reduced risk of HIV acquisition.

The first news story on "an exotic new disease" appeared May 18, 1981 in the gay newspaper "New York Native".

AIDS was first clinically observed in 1981 in the United States. The initial cases were a cluster of injection drug users and gay men with no known cause of impaired immunity who showed symptoms of "Pneumocystis" pneumonia (PCP or PJP, the latter term recognizing that the causative agent is now called "Pneumocystis jirovecii"), a rare opportunistic infection that was known to occur in people with very compromised immune systems. Soon thereafter, additional gay men developed a previously rare skin cancer called Kaposi's sarcoma (KS). Many more cases of PJP and KS emerged, alerting U.S. Centers for Disease Control and Prevention (CDC) and a CDC task force was formed to monitor the outbreak. The earliest retrospectively described case of AIDS is believed to have been in Norway beginning in 1966.

In the beginning, the CDC did not have an official name for the disease, often referring to it by way of the diseases that were associated with it, for example, lymphadenopathy, the disease after which the discoverers of HIV originally named the virus. They also used "Kaposi's Sarcoma and Opportunistic Infections", the name by which a task force had been set up in 1981. In the general press, the term "GRID", which stood for gay-related immune deficiency, had been coined. The CDC, in search of a name and looking at the infected communities, coined "the 4H disease", as it seemed to single out homosexuals, heroin users, hemophiliacs, and Haitians. However, after determining that AIDS was not isolated to the gay community, it was realized that the term GRID was misleading and "AIDS" was introduced at a meeting in July 1982. By September 1982 the CDC started using the name AIDS.
In 1983, two separate research groups led by American Robert Gallo and French investigators and Luc Montagnier independently declared that a novel retrovirus may have been infecting AIDS patients, and published their findings in the same issue of the journal "Science". Gallo claimed that a virus his group had isolated from a person with AIDS was strikingly similar in shape to other human T-lymphotropic viruses (HTLVs) his group had been the first to isolate. Gallo admitted in 1987 that the virus he claimed to have discovered in 1984 was in reality a virus sent to him from France the year before. Gallo's group called their newly-isolated virus HTLV-III. Montagnier's group isolated a virus from a patient presenting with swelling of the lymph nodes of the neck and physical weakness, two classic symptoms of primary HIV infection. Contradicting the report from Gallo's group, Montagnier and his colleagues showed that core proteins of this virus were immunologically different from those of HTLV-I. Montagnier's group named their isolated virus lymphadenopathy-associated virus (LAV). As these two viruses turned out to be the same, in 1986 LAV and HTLV-III were renamed HIV.

Another group working contemporaneously with the Montagnier and Gallo groups was that of Dr Jay Levy at the University of California, San Francisco. He independently discovered the AIDS virus in 1983 and named it the AIDS associated retrovirus (ARV). This virus was very different from the virus reported by the Montagnier and Gallo groups. The ARV strains indicated, for the first time, the heterogeneity of HIV isolates and several of these remain classic examples of the AIDS virus found in the United States.

Both HIV-1 and HIV-2 are believed to have originated in non-human primates in West-central Africa, and are believed to have transferred to humans (a process known as zoonosis) in the early 20th century.

HIV-1 appears to have originated in southern Cameroon through the evolution of SIVcpz, a simian immunodeficiency virus (SIV) that infects wild chimpanzees (HIV-1 descends from the SIVcpz endemic in the chimpanzee subspecies "Pan troglodytes troglodytes"). The closest relative of HIV-2 is SIVsmm, a virus of the sooty mangabey ("Cercocebus atys atys"), an Old World monkey living in littoral West Africa (from southern Senegal to western Côte d'Ivoire). New World monkeys such as the owl monkey are resistant to HIV-1 infection, possibly because of a genomic fusion of two viral resistance genes.

HIV-1 is thought to have jumped the species barrier on at least three separate occasions, giving rise to the three groups of the virus, M, N, and O.
There is evidence that humans who participate in bushmeat activities, either as hunters or as bushmeat vendors, commonly acquire SIV. However, SIV is a weak virus, and it is typically suppressed by the human immune system within weeks of infection. It is thought that several transmissions of the virus from individual to individual in quick succession are necessary to allow it enough time to mutate into HIV. Furthermore, due to its relatively low person-to-person transmission rate, it can only spread throughout the population in the presence of one or more high-risk transmission channels, which are thought to have been absent in Africa prior to the 20th century.

Specific proposed high-risk transmission channels, allowing the virus to adapt to humans and spread throughout the society, depend on the proposed timing of the animal-to-human crossing. Genetic studies of the virus suggest that the most recent common ancestor of the HIV-1 M group dates back to circa 1910. Proponents of this dating link the HIV epidemic with the emergence of colonialism and growth of large colonial African cities, leading to social changes, including different patterns of sexual contact (especially multiple, concurrent partnerships), the spread of prostitution, and the concomitant high frequency of genital ulcer diseases (such as syphilis) in nascent colonial cities. While transmission rates of HIV during vaginal intercourse are typically low, they are increased manyfold if one of the partners suffers from a sexually transmitted infection resulting in genital ulcers. Early 1900s colonial cities were notable for their high prevalence of prostitution and genital ulcers to the degree that as of 1928 as many as 45% of female residents of eastern Leopoldville were thought to have been prostitutes and as of 1933 around 15% of all residents of the same city were infected by one of the forms of syphilis.

The earliest, well-documented case of HIV in a human dates back to 1959 in the Belgian Congo. The virus may have been present in the United States as early as the mid-to-late 1950s, as a sixteen-year-old male presented with symptoms in 1966 and died in 1969.

An alternative view—unsupported by evidence—holds that unsafe medical practices in Africa during years following World War II, such as unsterile reuse of single-use syringes during mass vaccination, antibiotic, and anti-malaria treatment campaigns, were the initial vector that allowed the virus to adapt to humans and spread.





</doc>
<doc id="14173" url="https://en.wikipedia.org/wiki?curid=14173" title="HOL">
HOL

Hol or HOL may refer to:








</doc>
<doc id="14174" url="https://en.wikipedia.org/wiki?curid=14174" title="Hostile witness">
Hostile witness

A hostile witness, also known as an adverse witness or an unfavorable witness, is a witness at trial whose testimony on direct examination is either openly antagonistic or appears to be contrary to the legal position of the party who called the witness.

During direct examination, if the examining attorney who called the witness finds that their testimony is antagonistic or contrary to the legal position of their client, the attorney may request that the judge declare the witness "hostile". If the request is granted, the attorney may proceed to ask the witness leading questions. Leading questions either suggest the answer ("You saw my client sign the contract, correct?") or challenge (impeach) the witness's testimony. As a rule, leading questions are generally allowed only during cross-examination, but a hostile witness is an exception to this rule.

In cross-examination conducted by the opposing party's attorney, a witness is presumed to be hostile and the examining attorney is not required to seek the judge's permission before asking leading questions. Attorneys can influence a hostile witness's responses by using Gestalt psychology to influence the way the witness perceives the situation, and utility theory to understand their likely responses. The attorney will integrate a hostile witness's expected responses into the larger case strategy through pretrial planning and through adapting as necessary during the course of the trial.

In the state of New South Wales, the term 'unfavourable witness' is defined by section 38 of the Evidence Act which permits the prosecution to cross-examine their own witness. For example, if the prosecution calls all material witnesses relevant to a case before the court, and any evidence given is not favourable to, or supports the prosecution case, or a witness has given a prior inconsistent statement, then the prosecution may seek leave of the court, via section 192, to test the witness in relation to their evidence.

In New Zealand, section 94 of the Evidence Act 2006 permits a party to cross-examine their own witness if the presiding judge determines the witness to be hostile and gives permission.



</doc>
<doc id="14179" url="https://en.wikipedia.org/wiki?curid=14179" title="Henry I of England">
Henry I of England

Henry I (c. 1068 – 1 December 1135), also known as Henry Beauclerc, was King of England from 1100 to his death in 1135. He was the fourth son of William the Conqueror and was educated in Latin and the liberal arts. On William's death in 1087, Henry's elder brothers Robert Curthose and William Rufus inherited Normandy and England, respectively, but Henry was left landless. He purchased the County of Cotentin in western Normandy from Robert, but his brothers deposed him in 1091. He gradually rebuilt his power base in the Cotentin and allied himself with William against Robert. 

Present at the place where his brother William died in a hunting accident in 1100, Henry seized the English throne, promising at his coronation to correct many of William's less popular policies. He married Matilda of Scotland and they had two surviving children, William Adelin and Empress Matilda; he also had many illegitimate children by his many mistresses. Robert, who invaded in 1101, disputed Henry's control of England; this military campaign ended in a negotiated settlement that confirmed Henry as king. The peace was short-lived, and Henry invaded the Duchy of Normandy in 1105 and 1106, finally defeating Robert at the Battle of Tinchebray. Henry kept Robert imprisoned for the rest of his life. Henry's control of Normandy was challenged by Louis VI of France, Baldwin VII of Flanders and Fulk V of Anjou, who promoted the rival claims of Robert's son, William Clito, and supported a major rebellion in the Duchy between 1116 and 1119. Following Henry's victory at the Battle of Brémule, a favourable peace settlement was agreed with Louis in 1120.

Considered by contemporaries to be a harsh but effective ruler, Henry skillfully manipulated the barons in England and Normandy. In England, he drew on the existing Anglo-Saxon system of justice, local government and taxation, but also strengthened it with additional institutions, including the royal exchequer and itinerant justices. Normandy was also governed through a growing system of justices and an exchequer. Many of the officials who ran Henry's system were "new men" of obscure backgrounds, rather than from families of high status, who rose through the ranks as administrators. Henry encouraged ecclesiastical reform, but became embroiled in a serious dispute in 1101 with Archbishop Anselm of Canterbury, which was resolved through a compromise solution in 1105. He supported the Cluniac order and played a major role in the selection of the senior clergy in England and Normandy.

Henry's son William drowned in the "White Ship" disaster of 1120, throwing the royal succession into doubt. Henry took a second wife, Adeliza of Louvain, in the hope of having another son, but their marriage was childless. In response to this, he declared his daughter Matilda his heir and married her to Geoffrey of Anjou. The relationship between Henry and the couple became strained, and fighting broke out along the border with Anjou. Henry died on 1 December 1135 after a week of illness. Despite his plans for Matilda, the King was succeeded by his nephew, Stephen of Blois, resulting in a period of civil war known as the Anarchy.
Henry was probably born in England in 1068, in either the summer or the last weeks of the year, possibly in the town of Selby in Yorkshire. His father was William the Conqueror, the Duke of Normandy who had invaded England in 1066 to become the King of England, establishing lands stretching into Wales. The invasion had created an Anglo-Norman ruling class, many with estates on both sides of the English Channel. These Anglo-Norman barons typically had close links to the kingdom of France, which was then a loose collection of counties and smaller polities, under only the nominal control of the king. Henry's mother, Matilda of Flanders, was the granddaughter of Robert II of France, and she probably named Henry after her uncle, King Henry I of France.

Henry was the youngest of William and Matilda's four sons. Physically he resembled his older brothers Robert Curthose, Richard and William Rufus, being, as historian David Carpenter describes, "short, stocky and barrel-chested," with black hair. As a result of their age differences and Richard's early death, Henry would have probably seen relatively little of his older brothers. He probably knew his sister Adela well, as the two were close in age. There is little documentary evidence for his early years; historians Warren Hollister and Kathleen Thompson suggest he was brought up predominantly in England, while Judith Green argues he was initially brought up in the Duchy. He was probably educated by the Church, possibly by Bishop Osmund, the King's chancellor, at Salisbury Cathedral; it is uncertain if this indicated an intent by his parents for Henry to become a member of the clergy. It is also uncertain how far Henry's education extended, but he was probably able to read Latin and had some background in the liberal arts. He was given military training by an instructor called Robert Achard, and Henry was knighted by his father on 24 May 1086.

In 1087, William was fatally injured during a campaign in the Vexin. Henry joined his dying father near Rouen in September, where the King partitioned his possessions among his sons. The rules of succession in western Europe at the time were uncertain; in some parts of France, primogeniture, in which the eldest son would inherit a title, was growing in popularity. In other parts of Europe, including Normandy and England, the tradition was for lands to be divided up, with the eldest son taking patrimonial lands – usually considered to be the most valuable – and younger sons given smaller, or more recently acquired, partitions or estates.

In dividing his lands, William appears to have followed the Norman tradition, distinguishing between Normandy, which he had inherited, and England, which he had acquired through war. William's second son, Richard, had died in a hunting accident, leaving Henry and his two brothers to inherit William's estate. Robert, the eldest, despite being in armed rebellion against his father at the time of his death, received Normandy. England was given to William Rufus, who was in favour with the dying king. Henry was given a large sum of money, usually reported as £5,000, with the expectation that he would also be given his mother's modest set of lands in Buckinghamshire and Gloucestershire. William's funeral at Caen was marred by angry complaints from a local man, and Henry may have been responsible for resolving the dispute by buying off the protester with silver.

Robert returned to Normandy, expecting to have been given both the Duchy and England, to find that William Rufus had crossed the Channel and been crowned king. The two brothers disagreed fundamentally over the inheritance, and Robert soon began to plan an invasion of England to seize the kingdom, helped by a rebellion by some of the leading nobles against William Rufus. Henry remained in Normandy and took up a role within Robert's court, possibly either because he was unwilling to side openly with William Rufus, or because Robert might have taken the opportunity to confiscate Henry's inherited money if he had tried to leave. William Rufus sequestered Henry's new estates in England, leaving Henry landless.

In 1088, Robert's plans for the invasion of England began to falter, and he turned to Henry, proposing that his brother lend him some of his inheritance, which Henry refused. Henry and Robert then came to an alternative arrangement, in which Robert would make Henry the count of western Normandy, in exchange for £3,000. Henry's lands were a new countship created by a delegation of the ducal authority in the Cotentin, but it extended across the Avranchin, with control over the bishoprics of both. This also gave Henry influence over two major Norman leaders, Hugh d'Avranches and Richard de Redvers, and the abbey of Mont Saint-Michel, whose lands spread out further across the Duchy. Robert's invasion force failed to leave Normandy, leaving William Rufus secure in England.

Henry quickly established himself as count, building up a network of followers from western Normandy and eastern Brittany, whom historian John Le Patourel has characterised as "Henry's gang". His early supporters included Roger of Mandeville, Richard of Redvers, Richard d'Avranches and Robert Fitzhamon, along with the Churchman Roger of Salisbury. Robert attempted to go back on his deal with Henry and re-appropriate the county, but Henry's grip was already sufficiently firm to prevent this. Robert's rule of the Duchy was chaotic, and parts of Henry's lands became almost independent of central control from Rouen.

During this period, neither William nor Robert seems to have trusted Henry. Waiting until the rebellion against William Rufus was safely over, Henry returned to England in July 1088. He met with the King but was unable to persuade him to grant him their mother's estates, and travelled back to Normandy in the autumn. While he had been away, however, Odo, Bishop of Bayeux, who regarded Henry as a potential competitor, had convinced Robert that Henry was conspiring against the duke with William Rufus. On landing, Odo seized Henry and imprisoned him in Neuilly-la-Forêt, and Robert took back the county of the Cotentin. Henry was held there over the winter, but in the spring of 1089 the senior elements of the Normandy nobility prevailed upon Robert to release him.

Although no longer formally the Count of Cotentin, Henry continued to control the west of Normandy. The struggle between his brothers continued. William Rufus continued to put down resistance to his rule in England, but began to build a number of alliances against Robert with barons in Normandy and neighbouring Ponthieu. Robert allied himself with Philip I of France. In late 1090 William Rufus encouraged Conan Pilatus, a powerful burgher in Rouen, to rebel against Robert; Conan was supported by most of Rouen and made appeals to the neighbouring ducal garrisons to switch allegiance as well.

Robert issued an appeal for help to his barons, and Henry was the first to arrive in Rouen in November. Violence broke out, leading to savage, confused street fighting as both sides attempted to take control of the city. Robert and Henry left the castle to join the battle, but Robert then retreated, leaving Henry to continue the fighting. The battle turned in favour of the ducal forces and Henry took Conan prisoner. Henry was angry that Conan had turned against his feudal lord. He had him taken to the top of Rouen Castle and then, despite Conan's offers to pay a huge ransom, threw him off the top of the castle to his death. Contemporaries considered Henry to have acted appropriately in making an example of Conan, and Henry became famous for his exploits in the battle.

In the aftermath, Robert forced Henry to leave Rouen, probably because Henry's role in the fighting had been more prominent than his own, and possibly because Henry had asked to be formally reinstated as the count of the Cotentin. In early 1091, William Rufus invaded Normandy with a sufficiently large army to bring Robert to the negotiating table. The two brothers signed a treaty at Rouen, granting William Rufus a range of lands and castles in Normandy. In return, William Rufus promised to support Robert's attempts to regain control of the neighbouring county of Maine, once under Norman control, and help in regaining control over the Duchy, including Henry's lands. They nominated each other as heirs to England and Normandy, excluding Henry from any succession while either one of them lived.

War now broke out between Henry and his brothers. Henry mobilised a mercenary army in the west of Normandy, but as William Rufus and Robert's forces advanced, his network of baronial support melted away. Henry focused his remaining forces at Mont Saint-Michel, where he was besieged, probably in March 1091. The site was easy to defend, but lacked fresh water. The chronicler William of Malmesbury suggested that when Henry's water ran short, Robert allowed his brother fresh supplies, leading to remonstrations between Robert and William Rufus. The events of the final days of the siege are unclear: the besiegers had begun to argue about the future strategy for the campaign, but Henry then abandoned Mont Saint-Michel, probably as part of a negotiated surrender. He left for Brittany and crossed over into France.

Henry's next steps are not well documented; one chronicler, Orderic Vitalis, suggests that he travelled in the French Vexin, along the Normandy border, for over a year with a small band of followers. By the end of the year, Robert and William Rufus had fallen out once again, and the Treaty of Rouen had been abandoned. In 1092, Henry and his followers seized the Normandy town of Domfront. Domfront had previously been controlled by Robert of Bellême, but the inhabitants disliked his rule and invited Henry to take over the town, which he did in a bloodless coup. Over the next two years, Henry re-established his network of supporters across western Normandy, forming what Judith Green terms a "court in waiting". By 1094, he was allocating lands and castles to his followers as if he were the Duke of Normandy. William Rufus began to support Henry with money, encouraging his campaign against Robert, and Henry used some of this to construct a substantial castle at Domfront.

William Rufus crossed into Normandy to take the war to Robert in 1094, and when progress stalled, called upon Henry for assistance. Henry responded, but travelled to London instead of joining the main campaign further east in Normandy, possibly at the request of the King, who in any event abandoned the campaign and returned to England. Over the next few years, Henry appears to have strengthened his power base in western Normandy, visiting England occasionally to attend at William Rufus's court. In 1095 Pope Urban II called the First Crusade, encouraging knights from across Europe to join. Robert joined the Crusade, borrowing money from William Rufus to do so, and granting the King temporary custody of his part of the Duchy in exchange. The King appeared confident of regaining the remainder of Normandy from Robert, and Henry appeared ever closer to William Rufus. They campaigned together in the Norman Vexin between 1097 and 1098.

On the afternoon of 2 August 1100, the King went hunting in the New Forest, accompanied by a team of huntsmen and a number of the Norman nobility, including Henry. An arrow, possibly shot by the baron Walter Tirel, hit and killed William Rufus. Numerous conspiracy theories have been put forward suggesting that the King was killed deliberately; most modern historians reject these, as hunting was a risky activity, and such accidents were common. Chaos broke out, and Tirel fled the scene for France, either because he had shot the fatal arrow, or because he had been incorrectly accused and feared that he would be made a scapegoat for the King's death.

Henry rode to Winchester, where an argument ensued as to who now had the best claim to the throne. William of Breteuil championed the rights of Robert, who was still abroad, returning from the Crusade, and to whom Henry and the barons had given homage in previous years. Henry argued that, unlike Robert, he had been born to a reigning king and queen, thereby giving him a claim under the right of porphyrogeniture. Tempers flared, but Henry, supported by Henry de Beaumont and Robert of Meulan, held sway and persuaded the barons to follow him. He occupied Winchester Castle and seized the royal treasury.

Henry was hastily crowned king in Westminster Abbey on 5 August by Maurice, the Bishop of London, as Anselm, the Archbishop of Canterbury, had been exiled by William Rufus, and Thomas, the Archbishop of York, was in the north of England at Ripon. In accordance with English tradition and in a bid to legitimise his rule, Henry issued a coronation charter laying out various commitments. The new king presented himself as having restored order to a trouble-torn country. He announced that he would abandon William Rufus's policies towards the Church, which had been seen as oppressive by the clergy; he promised to prevent royal abuses of the barons' property rights, and assured a return to the gentler customs of Edward the Confessor; he asserted that he would "establish a firm peace" across England and ordered "that this peace shall henceforth be kept".

In addition to his existing circle of supporters, many of whom were richly rewarded with new lands, Henry quickly co-opted many of the existing administration into his new royal household. William Giffard, William Rufus's chancellor, was made the Bishop of Winchester, and the prominent sheriffs Urse d'Abetot, Haimo Dapifer and Robert Fitzhamon continued to play a senior role in government. By contrast, the unpopular Ranulf Flambard, the Bishop of Durham and a key member of the previous regime, was imprisoned in the Tower of London and charged with corruption. The late king had left many Church positions unfilled, and Henry set about nominating candidates to these, in an effort to build further support for his new government. The appointments needed to be consecrated, and Henry wrote to Anselm, apologising for having been crowned while the Archbishop was still in France and asking him to return at once.

On 11 November 1100 Henry married Matilda, the daughter of Malcolm III of Scotland. Henry was now around 31 years old, but late marriages for noblemen were not unusual in the 11th century. The pair had probably first met earlier the previous decade, possibly being introduced through Bishop Osmund of Salisbury. Historian Warren Hollister argues that Henry and Matilda were emotionally close, but their union was also certainly politically motivated. Matilda had originally been named Edith, an Anglo-Saxon name, and was a member of the West Saxon royal family, being the niece of Edgar the Ætheling, the great-granddaughter of Edmund Ironside and a descendant of Alfred the Great. For Henry, marrying Matilda gave his reign increased legitimacy, and for Matilda, an ambitious woman, it was an opportunity for high status and power in England.

Matilda had been educated in a sequence of convents, however, and may well have taken the vows to formally become a nun, which formed an obstacle to the marriage progressing. She did not wish to be a nun and appealed to Anselm for permission to marry Henry, and the Archbishop established a council at Lambeth Palace to judge the issue. Despite some dissenting voices, the council concluded that although Matilda had lived in a convent, she had not actually become a nun and was therefore free to marry, a judgement that Anselm then affirmed, allowing the marriage to proceed. Matilda proved an effective queen for Henry, acting as a regent in England on occasion, addressing and presiding over councils, and extensively supporting the arts. The couple soon had two children, Matilda, born in 1102, and William Adelin, born in 1103; it is possible that they also had a second son, Richard, who died young. Following the birth of these children, Matilda preferred to remain based in Westminster while Henry travelled across England and Normandy, either for religious reasons or because she enjoyed being involved in the machinery of royal governance.

Henry had a considerable sexual appetite and enjoyed a substantial number of sexual partners, resulting in many illegitimate children, at least nine sons and 13 daughters, many of whom he appears to have recognised and supported. It was normal for unmarried Anglo-Norman noblemen to have sexual relations with prostitutes and local women, and kings were also expected to have mistresses. Some of these relationships occurred before Henry was married, but many others took place after his marriage to Matilda. Henry had a wide range of mistresses from a range of backgrounds, and the relationships appear to have been conducted relatively openly. He may have chosen some of his noble mistresses for political purposes, but the evidence to support this theory is limited.

By early 1101, Henry's new regime was established and functioning, but many of the Anglo-Norman elite still supported his brother Robert, or would be prepared to switch sides if Robert appeared likely to gain power in England. In February, Flambard escaped from the Tower of London and crossed the Channel to Normandy, where he injected fresh direction and energy to Robert's attempts to mobilise an invasion force. By July, Robert had formed an army and a fleet, ready to move against Henry in England. Raising the stakes in the conflict, Henry seized Flambard's lands and, with the support of Anselm, Flambard was removed from his position as bishop. The King held court in April and June, where the nobility renewed their oaths of allegiance to him, but their support still appeared partial and shaky.

With the invasion imminent, Henry mobilised his forces and fleet outside Pevensey, close to Robert's anticipated landing site, training some of them personally in how to counter cavalry charges. Despite English levies and knights owing military service to the Church arriving in considerable numbers, many of his barons did not appear. Anselm intervened with some of the doubters, emphasising the religious importance of their loyalty to Henry. Robert unexpectedly landed further up the coast at Portsmouth on 20 July with a modest force of a few hundred men, but these were quickly joined by many of the barons in England. However, instead of marching into nearby Winchester and seizing Henry's treasury, Robert paused, giving Henry time to march west and intercept the invasion force.

The two armies met at Alton, Hampshire, where peace negotiations began, possibly initiated by either Henry or Robert, and probably supported by Flambard. The brothers then agreed to the Treaty of Alton, under which Robert released Henry from his oath of homage and recognised him as king; Henry renounced his claims on western Normandy, except for Domfront, and agreed to pay Robert £2,000 a year for life; if either brother died without a male heir, the other would inherit his lands; the barons whose lands had been seized by either the King or the Duke for supporting his rival would have them returned, and Flambard would be reinstated as bishop; the two brothers would campaign together to defend their territories in Normandy. Robert remained in England for a few months more with Henry before returning to Normandy.

Despite the treaty, Henry set about inflicting severe penalties on the barons who had stood against him during the invasion. William de Warenne, the Earl of Surrey, was accused of fresh crimes, which were not covered by the Alton amnesty, and was banished from England. In 1102 Henry then turned against Robert of Bellême and his brothers, the most powerful of the barons, accusing him of 45 different offences. Robert escaped and took up arms against Henry. Henry besieged Robert's castles at Arundel, Tickhill and Shrewsbury Castles, pushing down into the south-west to attack Bridgnorth. His power base in England broken, Robert accepted Henry's offer of banishment and left the country for Normandy.

Henry's network of allies in Normandy became stronger during 1103. He arranged the marriages of his illegitimate daughters, Juliane and Matilda, to Eustace of Breteuil and Rotrou III, Count of Perche, respectively, the latter union securing the Norman border. Henry attempted to win over other members of the Norman nobility and gave other English estates and lucrative offers to key Norman lords. Duke Robert continued to fight Robert of Bellême, but the Duke's position worsened, until by 1104, he had to ally himself formally with Bellême to survive. Arguing that the Duke had broken the terms of their treaty, the King crossed over the Channel to Domfront, where he met with senior barons from across Normandy, eager to ally themselves with him. He confronted the Duke and accused him of siding with his enemies, before returning to England.

Normandy continued to disintegrate into chaos. In 1105, Henry sent his friend Robert Fitzhamon and a force of knights into the Duchy, apparently to provoke a confrontation with Duke Robert. Fitzhamon was captured, and Henry used this as an excuse to invade, promising to restore peace and order. Henry had the support of most of the neighbouring counts around Normandy's borders, and King Philip of France was persuaded to remain neutral. Henry occupied western Normandy, and advanced east on Bayeux, where Fitzhamon was held. The city refused to surrender, and Henry besieged it, burning it to the ground. Terrified of meeting the same fate, the town of Caen switched sides and surrendered, allowing Henry to advance on Falaise, Calvados, which he took with some casualties. His campaign stalled, and the King instead began peace discussions with Robert. The negotiations were inconclusive and the fighting dragged on until Christmas, when Henry returned to England.

Henry invaded again in July 1106, hoping to provoke a decisive battle. After some initial tactical successes, he turned south-west towards the castle of Tinchebray. He besieged the castle and Duke Robert, supported by Robert of Bellême, advanced from Falaise to relieve it. After attempts at negotiation failed, the Battle of Tinchebray took place, probably on 28 September. The battle lasted around an hour, and began with a charge by Duke Robert's cavalry; the infantry and dismounted knights of both sides then joined the battle. Henry's reserves, led by Elias I, Count of Maine, and Alan IV, Duke of Brittany, attacked the enemy's flanks, routing first Bellême's troops and then the bulk of the ducal forces. Duke Robert was taken prisoner, but Bellême escaped.

Henry mopped up the remaining resistance in Normandy, and Duke Robert ordered his last garrisons to surrender. Reaching Rouen, Henry reaffirmed the laws and customs of Normandy and took homage from the leading barons and citizens. The lesser prisoners taken at Tinchebray were released, but the Duke and several other leading nobles were imprisoned indefinitely. The Duke's son, William Clito, was only three years old and was released to the care of Helias of Saint-Saens, a Norman baron. Henry reconciled himself with Robert of Bellême, who gave up the ducal lands he had seized and rejoined the royal court. Henry had no way of legally removing the Duchy from his brother, and initially Henry avoided using the title "duke" at all, emphasising that, as the King of England, he was only acting as the guardian of the troubled Duchy.

Henry inherited the kingdom of England from William Rufus, giving him a claim of suzerainty over Wales and Scotland, and acquired the Duchy of Normandy, a complex entity with troubled borders. The borders between England and Scotland were still uncertain during Henry's reign, with Anglo-Norman influence pushing northwards through Cumbria, but his relationship with King David I of Scotland was generally good, partially due to Henry's marriage to his sister. In Wales, Henry used his power to coerce and charm the indigenous Welsh princes, while Norman Marcher Lords pushed across the valleys of South Wales. Normandy was controlled via various interlocking networks of ducal, ecclesiastical and family contacts, backed by a growing string of important ducal castles along the borders. Alliances and relationships with neighbouring counties along the Norman border were particularly important to maintaining the stability of the Duchy.

Henry ruled through the various barons and lords in England and Normandy, whom he manipulated skilfully for political effect. Political friendships, termed "amicitia" in Latin, were important during the 12th century, and Henry maintained a wide range of these, mediating between his friends in various factions across his realm when necessary, and rewarding those who were loyal to him. He also had a reputation for punishing those barons who stood against him, and he maintained an effective network of informers and spies who reported to him on events. Henry was a harsh, firm ruler, but not excessively so by the standards of the day. Over time, he increased the degree of his control over the barons, removing his enemies and bolstering his friends until the "reconstructed baronage", as historian Warren Hollister describes it, was predominantly loyal and dependent on the King.

Henry's itinerant royal court comprised various parts. At the heart was his domestic household, called the "domus"; a wider grouping was termed the "familia regis", and formal gatherings of the court were termed "curia". The "domus" was divided into several parts. The chapel, headed by the chancellor, looked after the royal documents, the chamber dealt with financial affairs and the master-marshal was responsible for travel and accommodation. The "familia regis" included Henry's mounted household troops, up to several hundred strong, who came from a wider range of social backgrounds, and could be deployed across England and Normandy as required. Initially Henry continued his father's practice of regular crown-wearing ceremonies at his "curia", but they became less frequent as the years passed. Henry's court was grand and ostentatious, financing the construction of large new buildings and castles with a range of precious gifts on display, including his private menagerie of exotic animals, which he kept at Woodstock Palace. Despite being a lively community, Henry's court was more tightly controlled than those of previous kings. Strict rules controlled personal behaviour and prohibited members of the court from pillaging neighbouring villages, as had been the norm under William Rufus.

Henry was responsible for a substantial expansion of the royal justice system. In England, Henry drew on the existing Anglo-Saxon system of justice, local government and taxes, but strengthened it with additional central governmental institutions. Roger of Salisbury began to develop the royal exchequer after 1110, using it to collect and audit revenues from the King's sheriffs in the shires. Itinerant justices began to emerge under Henry, travelling around the country managing eyre courts, and many more laws were formally recorded. Henry gathered increasing revenue from the expansion of royal justice, both from fines and from fees. The first Pipe Roll that is known to have survived dates from 1130, recording royal expenditures. Henry reformed the coinage in 1107, 1108 and in 1125, inflicting harsh corporal punishments to English coiners who had been found guilty of debasing the currency. In Normandy, he restored law and order after 1106, operating through a body of Norman justices and an exchequer system similar to that in England. Norman institutions grew in scale and scope under Henry, although less quickly than in England. Many of the officials that ran Henry's system were termed "new men", relatively low-born individuals who rose through the ranks as administrators, managing justice or the royal revenues.

Henry's ability to govern was intimately bound up with the Church, which formed the key to the administration of both England and Normandy, and this relationship changed considerably over the course of his reign. William the Conqueror had reformed the English Church with the support of his Archbishop of Canterbury, Lanfranc, who became a close colleague and advisor to the King. Under William Rufus this arrangement had collapsed, the King and Archbishop Anselm had become estranged and Anselm had gone into exile. Henry also believed in Church reform, but on taking power in England he became embroiled in the investiture controversy.

The argument concerned who should invest a new bishop with his staff and ring: traditionally, this had been carried out by the King in a symbolic demonstration of royal power, but Pope Urban II had condemned this practice in 1099, arguing that only the papacy could carry out this task, and declaring that the clergy should not give homage to their local temporal rulers. Anselm returned to England from exile in 1100 having heard Urban's pronouncement, and informed Henry that he would be complying with the Pope's wishes. Henry was in a difficult position. On one hand, the symbolism and homage was important to him; on the other hand, he needed Anselm's support in his struggle with his brother Duke Robert.

Anselm stuck firmly to the letter of the papal decree, despite Henry's attempts to persuade him to give way in return for a vague assurance of a future royal compromise. Matters escalated, with Anselm going back into exile and Henry confiscating the revenues of his estates. Anselm threatened excommunication, and in July 1105 the two men finally negotiated a solution. A distinction was drawn between the secular and ecclesiastical powers of the prelates, under which Henry gave up his right to invest his clergy, but retained the custom of requiring them to come and do homage for the temporalities, the landed properties they held in England. Despite this argument, the pair worked closely together, combining to deal with Duke Robert's invasion of 1101, for example, and holding major reforming councils in 1102 and 1108.

A long-running dispute between the Archbishops of Canterbury and York flared up under Anselm's successor, Ralph d'Escures. Canterbury, traditionally the senior of the two establishments, had long argued that the Archbishop of York should formally promise to obey their Archbishop, but York argued that the two episcopates were independent within the English Church and that no such promise was necessary. Henry supported the primacy of Canterbury, to ensure that England remained under a single ecclesiastical administration, but the Pope preferred the case of York. The matter was complicated by Henry's personal friendship with Thurstan, the Archbishop of York, and the King's desire that the case should not end up in a papal court, beyond royal control. Henry needed the support of the Papacy in his struggle with Louis of France, however, and therefore allowed Thurstan to attend the Council of Rheims in 1119, where Thurstan was then consecrated by the Pope with no mention of any duty towards Canterbury. Henry believed that this went against assurances Thurstan had previously made and exiled him from England until the King and Archbishop came to a negotiated solution the following year.

Even after the investiture dispute, Henry continued to play a major role in the selection of new English and Norman bishops and archbishops. He appointed many of his officials to bishoprics and, as historian Martin Brett suggests, "some of his officers could look forward to a mitre with all but absolute confidence". Henry's chancellors, and those of his queens, became bishops of Durham, Hereford, London, Lincoln, Winchester and Salisbury. Henry increasingly drew on a wider range of these bishops as advisors – particularly Roger of Salisbury – breaking with the earlier tradition of relying primarily on the Archbishop of Canterbury. The result was a cohesive body of administrators through which Henry could exercise careful influence, holding general councils to discuss key matters of policy. This stability shifted slightly after 1125, when he began to inject a wider range of candidates into the senior positions of the Church, often with more reformist views, and the impact of this generation would be felt in the years after Henry's death.

Like other rulers of the period, Henry donated to the Church and patronised various religious communities, but contemporary chroniclers did not consider him an unusually pious king. His personal beliefs and piety may, however, have developed during the course of his life. Henry had always taken an interest in religion, but in his later years he may have become much more concerned about spiritual affairs. If so, the major shifts in his thinking would appear to have occurred after 1120, when his son William Adelin died, and 1129, when his daughter's marriage teetered on the verge of collapse.

As a proponent of religious reform, Henry gave extensively to reformist groups within the Church. He was a keen supporter of the Cluniac order, probably for intellectual reasons. He donated money to the abbey at Cluny itself, and after 1120 gave generously to Reading Abbey, a Cluniac establishment. Construction on Reading began in 1121, and Henry endowed it with rich lands and extensive privileges, making it a symbol of his dynastic lines. He also focused effort on promoting the conversion of communities of clerks into Augustinian canons, the foundation of leper hospitals, expanding the provision of nunneries, and the charismatic orders of the Savigniacs and Tironensians. He was an avid collector of relics, sending an embassy to Constantinople in 1118 to collect Byzantine items, some of which were donated to Reading Abbey.

Normandy faced an increased threat from France, Anjou and Flanders after 1108. King Louis VI succeeded to the French throne in 1108 and began to reassert central royal power. Louis demanded Henry give homage to him and that two disputed castles along the Normandy border be placed into the control of neutral castellans. Henry refused, and Louis responded by mobilising an army. After some arguments, the two kings negotiated a truce and retreated without fighting, leaving the underlying issues unresolved. Fulk V assumed power in Anjou in 1109 and began to rebuild Angevin authority. He inherited the county of Maine, but refused to recognise Henry as his feudal lord and instead allied himself with Louis. Robert II of Flanders also briefly joined the alliance, before his death in 1111.

In 1108, Henry betrothed his six-year-old daughter, Matilda, to Henry V, the future Holy Roman Emperor. For King Henry, this was a prestigious match; for Henry V, it was an opportunity to restore his financial situation and fund an expedition to Italy, as he received a dowry of £6,666 from England and Normandy. Raising this money proved challenging, and required the implementation of a special "aid", or tax, in England. Matilda was crowned German queen in 1110.

Henry responded to the French and Angevin threat by expanding his own network of supporters beyond the Norman borders. Some Norman barons deemed unreliable were arrested or dispossessed, and Henry used their forfeited estates to bribe his potential allies in the neighbouring territories, in particular Maine. Around 1110, Henry attempted to arrest the young William Clito, but William's mentors moved him to the safety of Flanders before he could be taken. At about this time, Henry probably began to style himself as the duke of Normandy. Robert of Bellême turned against Henry once again, and when he appeared at Henry's court in 1112 in a new role as a French ambassador, he was arrested and imprisoned.

Rebellions broke out in France and Anjou between 1111 and 1113, and Henry crossed into Normandy to support his nephew, Count Theobald II of Blois, who had sided against Louis in the uprising. In a bid to isolate Louis diplomatically, Henry betrothed his young son, William Adelin, to Fulk's daughter Matilda, and married his illegitimate daughter Matilda to Duke Conan III of Brittany, creating alliances with Anjou and Brittany respectively. Louis backed down and in March 1113 met with Henry near Gisors to agree a peace settlement, giving Henry the disputed fortresses and confirming Henry's overlordship of Maine, Bellême and Brittany.

Meanwhile, the situation in Wales was deteriorating. Henry had conducted a campaign in South Wales in 1108, pushing out royal power in the region and colonising the area around Pembroke with Flemings. By 1114, some of the resident Norman lords were under attack, while in Mid-Wales, Owain ap Cadwgan blinded one of the political hostages he was holding, and in North Wales Gruffudd ap Cynan threatened the power of the Earl of Chester. Henry sent three armies into Wales that year, with Gilbert Fitz Richard leading a force from the south, Alexander, King of Scotland, pressing from the north and Henry himself advancing into Mid-Wales. Owain and Gruffudd sued for peace, and Henry accepted a political compromise. He reinforced the Welsh Marches with his own appointees, strengthening the border territories.

Concerned about the succession, Henry sought to persuade Louis VI to accept his son, William Adelin, as the legitimate future Duke of Normandy, in exchange for his son's homage. Henry crossed into Normandy in 1115 and assembled the Norman barons to swear loyalty; he also almost successfully negotiated a settlement with Louis, affirming William's right to the Duchy in exchange for a large sum of money. However, Louis, backed by his ally Baldwin of Flanders, instead declared that he considered William Clito the legitimate heir to the Duchy.

War broke out after Henry returned to Normandy with an army to support Theobald of Blois, who was under attack from Louis. Henry and Louis raided each other's towns along the border, and a wider conflict then broke out, probably in 1116. Henry was pushed onto the defensive as French, Flemish and Angevin forces began to pillage the Normandy countryside. Amaury III of Montfort and many other barons rose up against Henry, and there was an assassination plot from within his own household. Henry's wife, Matilda, died in early 1118, but the situation in Normandy was sufficiently pressing that Henry was unable to return to England for her funeral.

Henry responded by mounting campaigns against the rebel barons and deepening his alliance with Theobald. Baldwin of Flanders was wounded in battle and died in September 1118, easing the pressure on Normandy from the north-east. Henry attempted to crush a revolt in the city of Alençon, but was defeated by Fulk and the Angevin army. Forced to retreat from Alençon, Henry's position deteriorated alarmingly, as his resources became overstretched and more barons abandoned his cause. Early in 1119, Eustace of Breteuil and Henry's daughter, Juliana, threatened to join the baronial revolt. Hostages were exchanged in a bid to avoid conflict, but relations broke down and both sides mutilated their captives. Henry attacked and took the town of Breteuil, Eure, despite Juliana's attempt to kill her father with a crossbow. In the aftermath, Henry dispossessed the couple of almost all of their lands in Normandy.

Henry's situation improved in May 1119 when he enticed Fulk to switch sides by finally agreeing to marry William Adelin to Fulk's daughter, Matilda, and paying Fulk a large sum of money. Fulk left for the Levant, leaving the County of Maine in Henry's care, and the King was free to focus on crushing his remaining enemies. During the summer Henry advanced into the Norman Vexin, where he encountered Louis's army, resulting in the Battle of Brémule. Henry appears to have deployed scouts and then organised his troops into several carefully formed lines of dismounted knights. Unlike Henry's forces, the French knights remained mounted; they hastily charged the Anglo-Norman positions, breaking through the first rank of the defences but then becoming entangled in Henry's second line of knights. Surrounded, the French army began to collapse. In the melee, Henry was hit by a sword blow, but his armour protected him. Louis and William Clito escaped from the battle, leaving Henry to return to Rouen in triumph.

The war slowly petered out after this battle, and Louis took the dispute over Normandy to Pope Callixtus II's council in Reims that October. Henry faced a number of French complaints concerning his acquisition and subsequent management of Normandy, and despite being defended by Geoffrey, the Archbishop of Rouen, Henry's case was shouted down by the pro-French elements of the council. Callixtus declined to support Louis, however, and merely advised the two rulers to seek peace. Amaury de Montfort came to terms with Henry, but Henry and William Clito failed to find a mutually satisfactory compromise. In June 1120, Henry and Louis formally made peace on terms advantageous to the King of England: William Adelin gave homage to Louis, and in return Louis confirmed William's rights to the Duchy.

Henry's succession plans were thrown into chaos by the sinking of the "White Ship" on 25 November 1120. Henry had left the port of Barfleur for England in the early evening, leaving William Adelin and many of the younger members of the court to follow on that night in a separate vessel, the "White Ship". Both the crew and passengers were drunk and, just outside the harbour, the ship hit a submerged rock. The ship sank, killing as many as 300 people, with only one survivor, a butcher from Rouen. Henry's court was initially too scared to report William's death to the King. When he was finally told, he collapsed with grief.

The disaster left Henry with no legitimate son, his various nephews now the closest possible male heirs. Henry announced he would take a new wife, Adeliza of Louvain, opening up the prospect of a new royal son, and the two were married at Windsor Castle in January 1121. Henry appears to have chosen her because she was attractive and came from a prestigious noble line. Adeliza seems to have been fond of Henry and joined him in his travels, probably to maximise the chances of her conceiving a child. The "White Ship" disaster initiated fresh conflict in Wales, where the drowning of Richard, Earl of Chester, encouraged a rebellion led by Maredudd ap Bleddyn. Henry intervened in North Wales that summer with an army and, although he was hit by a Welsh arrow, the campaign reaffirmed royal power across the region.

Henry's alliance with Anjou – which had been based on his son William marrying Fulk's daughter Matilda – began to disintegrate. Fulk returned from the Levant and demanded that Henry return Matilda and her dowry, a range of estates and fortifications in Maine. Matilda left for Anjou, but Henry argued that the dowry had in fact originally belonged to him before it came into the possession of Fulk, and so declined to hand the estates back to Anjou. Fulk married his daughter Sibylla to William Clito, and granted them Maine. Once again, conflict broke out, as Amaury de Montfort allied himself with Fulk and led a revolt along the Norman-Anjou border in 1123. Amaury was joined by several other Norman barons, headed by Waleran de Beaumont, one of the sons of Henry's old ally, Robert of Meulan.

Henry dispatched Robert of Gloucester and Ranulf le Meschin to Normandy and then intervened himself in late 1123. He began the process of besieging the rebel castles, before wintering in the Duchy. In the spring of 1124, campaigning began again. In the battle of Bourgthéroulde, Odo Borleng, castellan of Bernay, Eure, led the King's army and received intelligence that the rebels were departing from the rebel base in Beaumont-le-Roger allowing him to ambush them as they traversed through the Brotonne forest. Waleran charged the royal forces, but his knights were cut down by Odo's archers and the rebels were quickly overwhelmed. Waleran was captured, but Amaury escaped. Henry mopped up the remainder of the rebellion, blinding some of the rebel leaders – considered, at the time, a more merciful punishment than execution – and recovering the last rebel castles. He paid Pope Callixtus a large amount of money, in exchange for the Papacy annulling the marriage of William Clito and Sibylla on the grounds of consanguinity.

Henry and Adeliza did not conceive any children, generating prurient speculation as to the possible explanation, and the future of the dynasty appeared at risk. Henry may have begun to look among his nephews for a possible heir. He may have considered Stephen of Blois as a possible option and, perhaps in preparation for this, he arranged a beneficial marriage for Stephen to a wealthy heiress, Matilda. Theobald of Blois, his close ally, may have also felt that he was in favour with Henry. William Clito, who was King Louis's preferred choice, remained opposed to Henry and was therefore unsuitable. Henry may have also considered his own illegitimate son, Robert of Gloucester, as a possible candidate, but English tradition and custom would have looked unfavourably on this.

Henry's plans shifted when the Empress Matilda's husband, the Emperor Henry, died in 1125. The King recalled his daughter to England the next year and declared that, should he die without a male heir, she was to be his rightful successor. The Anglo-Norman barons were gathered together at Westminster at Christmas 1126, where they swore to recognise Matilda and any future legitimate heir she might have. Putting forward a woman as a potential heir in this way was unusual: opposition to Matilda continued to exist within the English court, and Louis was vehemently opposed to her candidacy.

Fresh conflict broke out in 1127, when the childless Charles I, Count of Flanders, was murdered, creating a local succession crisis. Backed by King Louis, William Clito was chosen by the Flemings to become their new ruler. This development potentially threatened Normandy, and Henry began to finance a proxy war in Flanders, promoting the claims of William's Flemish rivals. In an effort to disrupt the French alliance with William, Henry mounted an attack into France in 1128, forcing Louis to cut his aid to William. William died unexpectedly in July, removing the last major challenger to Henry's rule and bringing the war in Flanders to a halt. Without William, the baronial opposition in Normandy lacked a leader. A fresh peace was made with France, and Henry was finally able to release the remaining prisoners from the revolt of 1123, including Waleran of Meulan, who was rehabilitated into the royal court.

Meanwhile, Henry rebuilt his alliance with Fulk of Anjou, this time by marrying Matilda to Fulk's eldest son, Geoffrey. The pair were betrothed in 1127 and married the following year. It is unknown whether Henry intended Geoffrey to have any future claim on England or Normandy, and he was probably keeping his son-in-law's status deliberately uncertain. Similarly, although Matilda was granted a number of Normandy castles as part of her dowry, it was not specified when the couple would actually take possession of them. Fulk left Anjou for Jerusalem in 1129, declaring Geoffrey the Count of Anjou and Maine. The marriage proved difficult, as the couple did not particularly like each other and the disputed castles proved a point of contention, resulting in Matilda returning to Normandy later that year. Henry appears to have blamed Geoffrey for the separation, but in 1131 the couple were reconciled. Much to the pleasure and relief of Henry, Matilda then gave birth to a sequence of two sons, Henry and Geoffrey, in 1133 and 1134.

Relations between Henry, Matilda, and Geoffrey became increasingly strained during the King's final years. Matilda and Geoffrey suspected that they lacked genuine support in England. In 1135 they urged Henry to hand over the royal castles in Normandy to Matilda whilst he was still alive, and insisted that the Norman nobility swear immediate allegiance to her, thereby giving the couple a more powerful position after Henry's death. Henry angrily declined to do so, probably out of concern that Geoffrey would try to seize power in Normandy. A fresh rebellion broke out amongst the barons in southern Normandy, led by William III, Count of Ponthieu, whereupon Geoffrey and Matilda intervened in support of the rebels.

Henry campaigned throughout the autumn, strengthening the southern frontier, and then travelled to Lyons-la-Forêt in November to enjoy some hunting, still apparently healthy. There he fell ill – according to the chronicler Henry of Huntingdon, he ate too many ("a surfeit of") lampreys against his physician's advice – and his condition worsened over the course of a week. Once the condition appeared terminal, Henry gave confession and summoned Archbishop Hugh of Amiens, who was joined by Robert of Gloucester and other members of the court. In accordance with custom, preparations were made to settle Henry's outstanding debts and to revoke outstanding sentences of forfeiture. The King died on 1 December 1135, and his corpse was taken to Rouen accompanied by the barons, where it was embalmed; his entrails were buried locally at the priory of Notre-Dame du Pré, and the preserved body was taken on to England, where it was interred at Reading Abbey.

Despite Henry's efforts, the succession was disputed. When news began to spread of the King's death, Geoffrey and Matilda were in Anjou supporting the rebels in their campaign against the royal army, which included a number of Matilda's supporters such as Robert of Gloucester. Many of these barons had taken an oath to stay in Normandy until the late king was properly buried, which prevented them from returning to England. The Norman nobility discussed declaring Theobald of Blois king. Theobald's younger brother, Stephen of Blois, quickly crossed from Boulogne to England, however, accompanied by his military household. Hugh Bigod dubiously testified that Henry, on his deathbed, had released the barons from their oath to Matilda, and with the help of his brother, Henry of Blois, Stephen seized power in England and was crowned king on 22 December. Matilda did not give up her claim to England and Normandy, appealing at first to the Pope against the decision to allow the coronation of Stephen, and then invading England to start a prolonged civil war, known as the Anarchy, between 1135 and 1153.

Historians have drawn on a range of sources on Henry, including the accounts of chroniclers; other documentary evidence, including early pipe rolls; and surviving buildings and architecture. The three main chroniclers to describe the events of Henry's life were William of Malmesbury, Orderic Vitalis, and Henry of Huntingdon, but each incorporated extensive social and moral commentary into their accounts and borrowed a range of literary devices and stereotypical events from other popular works. Other chroniclers include Eadmer, Hugh the Chanter, Abbot Suger, and the authors of the Welsh "Brut". Not all royal documents from the period have survived, but there are a number of royal acts, charters, writs, and letters, along with some early financial records. Some of these have since been discovered to be forgeries, and others had been subsequently amended or tampered with.

Late medieval historians seized on the accounts of selected chroniclers regarding Henry's education and gave him the title of Henry "Beauclerc", a theme echoed in the analysis of Victorian and Edwardian historians such as Francis Palgrave and Henry Davis. The historian Charles David dismissed this argument in 1929, showing the more extreme claims for Henry's education to be without foundation. Modern histories of Henry commenced with Richard Southern's work in the early 1960s, followed by extensive research during the rest of the 20th century into a wide number of themes from his reign in England, and a much more limited number of studies of his rule in Normandy. Only two major, modern biographies of Henry have been produced, C. Warren Hollister's posthumous volume in 2001, and Judith Green's 2006 work.

Interpretation of Henry's personality by historians has altered over time. Earlier historians such as Austin Poole and Richard Southern considered Henry as a cruel, draconian ruler. More recent historians, such as Hollister and Green, view his implementation of justice much more sympathetically, particularly when set against the standards of the day, but even Green has noted that Henry was "in many respects highly unpleasant", and Alan Cooper has observed that many contemporary chroniclers were probably too scared of the King to voice much criticism. Historians have also debated the extent to which Henry's administrative reforms genuinely constituted an introduction of what Hollister and John Baldwin have termed systematic, "administrative kingship", or whether his outlook remained fundamentally traditional.

Henry's burial at Reading Abbey is marked by a local cross and a plaque, but Reading Abbey was slowly demolished during the Dissolution of the Monasteries in the 16th century. The exact location is uncertain, but the most likely location of the tomb itself is now in a built-up area of central Reading, on the site of the former abbey choir. A plan to locate his remains was announced in March 2015, with support from English Heritage and Philippa Langley, who aided with the successful discovery and exhumation of Richard III.

In addition to Matilda and William, Henry possibly had a short-lived son, Richard, with his first wife, Matilda of Scotland. Henry and his second wife, Adeliza of Louvain, had no children.

Henry had a number of illegitimate children by various mistresses.





</doc>
<doc id="14183" url="https://en.wikipedia.org/wiki?curid=14183" title="Hentai">
Hentai

Outside of Japan, hentai ( or ) is anime and manga pornography. In Japanese, however, "hentai" is not a genre of media but any type of perverse or bizarre sexual desire or act. For example, outside of Japan a work of animation depicting lesbian sex might be described as "yuri hentai", but in Japan it would just be described as "yuri".

The word is short for "hentai seiyoku" (), a perverse sexual desire. The word "hentai" in Japanese means "transformation" or "metamorphosis"; the implication of perversion or paraphilia was derived from there, and both meanings can be easily distinguished in context.

"Hentai" is a kanji compound of ("hen"; "change", "weird", or "strange") and ("tai"; "appearance" or "condition"). It also means "perversion" or "abnormality", especially when used as an adjective. It is the shortened form of the phrase which means "sexual perversion". The character "hen" is catch-all for queerness as a peculiarity—it does not carry an explicit sexual reference. While the term has expanded in use to cover a range of publications including homosexual publications, it remains primarily a heterosexual term, as terms indicating homosexuality entered Japan as foreign words. Japanese pornographic works are often simply tagged as , meaning "prohibited to those not yet 18 years old", and . Less official terms also in use include , , and the English initialism AV (for "adult video"). Usage of the term "hentai" does not define a genre in Japan.

"Hentai" is defined differently in English. The "Oxford Dictionary Online" defines it as "a subgenre of the Japanese genres of manga and anime, characterized by overtly sexualized characters and sexually explicit images and plots." The origin of the word in English is unknown, but AnimeNation's John Oppliger points to the early 1990s, when a "Dirty Pair" erotic "doujinshi" (self-published work) titled "H-Bomb" was released, and when many websites sold access to images culled from Japanese erotic visual novels and games. The earliest English use of the term traces back to the rec.arts.anime boards; with a 1990 post concerning Happosai of "Ranma ½" and the first discussion of the meaning in 1991. A 1995 glossary on the rec.arts.anime boards contained reference to the Japanese usage and the evolving definition of hentai as "pervert" or "perverted sex". "The Anime Movie Guide", published in 1997, defines as the initial sound of hentai (i.e., the name of the letter "H", as pronounced in Japanese); it included that ecchi was "milder than hentai". A year later it was defined as a genre in "Good Vibrations Guide to Sex". At the beginning of 2000, "hentai" was listed as the 41st most-popular search term of the internet, while "anime" ranked 99th. The attribution has been applied retroactively to works such as "Urotsukidōji", "La Blue Girl", and "Cool Devices". "Urotsukidōji" had previously been described with terms such as "Japornimation", and "erotic grotesque", prior to being identified as hentai.

The history of the word "hentai" has its origins in science and psychology. By the middle of the Meiji era, the term appeared in publications to describe unusual or abnormal traits, including paranormal abilities and psychological disorders. A translation of German sexologist Richard von Krafft-Ebing's text "Psychopathia Sexualis" originated the concept of "hentai seiyoku", as a "perverse or abnormal sexual desire". Though it was popularized outside psychology, as in the case of Mori Ōgai's 1909 novel "Vita Sexualis". Continued interest in "hentai seiyoku" resulted in numerous journals and publications on sexual advice which circulated in the public, served to establish the sexual connotation of "hentai" as perverse. Any perverse or abnormal act could be hentai, such as committing "shinjū" (love suicide). It was Nakamura Kokyo's journal "Abnormal Psychology" which started the popular sexology boom in Japan which would see the rise of other popular journals like "Sexuality and Human Nature", "Sex Research" and "Sex". Originally, Tanaka Kogai wrote articles for "Abnormal Psychology", but it would be Tanaka's own journal "Modern Sexuality" which would become one of the most popular sources of information about erotic and neurotic expression. "Modern Sexuality" was created to promote fetishism, S&M, and necrophilia as a facet of modern life. The ero-guro movement and depiction of perverse, abnormal and often erotic undertones were a response to interest in "hentai seiyoku".

Following World War II, Japan took a new interest in sexualization and public sexuality. Mark McLelland puts forth the observation that the term "hentai" found itself shortened to "H" and that the English pronunciation was "etchi", referring to lewdness and which did not carry the stronger connotation of abnormality or perversion. By the 1950s, the "hentai seiyoku" publications became their own genre and included fetish and homosexual topics. By the 1960s, the homosexual content was dropped in favor of subjects like sadomasochism and stories of lesbianism targeted to male readers. The late 1960s brought a sexual revolution which expanded and solidified the normalizing the terms identity in Japan that continues to exist today through publications such as "Bessatsu Takarajima"s "Hentai-san ga iku" series.

With the usage of "hentai" as any erotic depiction, the history of these depictions is split into their media. Japanese artwork and comics serve as the first example of hentai material, coming to represent the iconic style after the publication of Azuma Hideo's "Cybele" in 1979. Japanese animation (anime) had its first hentai, in both definitions, with the 1984 release of Wonderkid's "Lolita Anime", overlooking the erotic and sexual depictions in 1969's "One Thousand and One Arabian Nights" and the bare-breasted Cleopatra in 1970's "Cleopatra" film. Erotic games, another area of contention, has its first case of the art style depicting sexual acts in 1985's "Tenshitachi no Gogo". In each of these mediums, the broad definition and usage of the term complicates its historic examination.

Depictions of sex and abnormal sex can be traced back through the ages, predating the term "hentai". "Shunga", a Japanese term for erotic art, is thought to have existed in some form since the Heian period. From the 16th to the 19th centuries, "shunga" works were suppressed by "shōguns". A well-known example is "The Dream of the Fisherman's Wife", which depicts a woman being stimulated by two octopuses. "Shunga" production fell with the introduction of pornographic photographs in the late 19th century.

To define erotic manga, a definition for manga is needed. While the "Hokusai Manga" uses the term "manga" in its title, it does not depict the story-telling aspect common to modern manga, as the images are unrelated. Due to the influence of pornographic photographs in the 19th and 20th centuries, the manga artwork was depicted by realistic characters. Osamu Tezuka helped define the modern look and form of manga, and was later proclaimed as the "God of Manga". His debut work "New Treasure Island" was released in 1947 as a comic book through Ikuei Publishing and sold over 400,000 copies, though it was the popularity of Tezuka's "Astro Boy", "Metropolis", and "Jungle Emperor" manga that would come to define the media. This story-driven manga style is distinctly unique from comic strips like "Sazae-san", and story-driven works came to dominate "shōjo" and "shōnen" magazines.

Adult themes in manga have existed since the 1940s, but some of these depictions were more realistic than the cartoon-cute characters popularized by Tezuka. Early well-known ""ero-gekiga"" releases were "Ero Mangatropa" (1973), "Erogenica" (1975), and "Alice" (1977). The distinct shift in the style of Japanese pornographic comics from realistic to cartoon-cute characters is accredited to Hideo Azuma, "The Father of Lolicon". In 1979, he penned "Cybele", which offered the first depictions of sexual acts between cute, unrealistic Tezuka-style characters. This would start a pornographic manga movement. The lolicon boom of the 1980s saw the rise of magazines such as the anthologies "Lemon People" and "Petit Apple Pie".

The publication of erotic materials in the United States can be traced back to at least 1990, when IANVS Publications printed its first "Anime Shower Special". In March 1994, Antarctic Press released "Bondage Fairies", an English translation of "Insect Hunter".

Because there are fewer animation productions, most erotic works are retroactively tagged as "hentai" since the coining of the term in English. "Hentai" is typically defined as consisting of excessive nudity, and graphic sexual intercourse whether or not it is perverse. The term "ecchi" is typically related to fanservice, with no sexual intercourse being depicted.

Two early works escape being defined as hentai, but contain erotic themes. This is likely due to the obscurity and unfamiliarity of the works, arriving in the United States and fading from public focus a full 20 years before importation and surging interests coined the Americanized term "hentai". The first is the 1969 film "One Thousand and One Arabian Nights", which faithfully includes erotic elements of the original story. In 1970, "", was the first animated film to carry an X rating, but it was mislabeled as erotica in the United States.

The "Lolita Anime" series is typically identified as the first erotic anime and original video animation (OVA); it was released in 1984 by Wonder Kids. Containing eight episodes, the series focused on underage sex and rape, and included one episode containing BDSM bondage. Several sub-series were released in response, including a second "Lolita Anime" series released by Nikkatsu. It has not been officially licensed or distributed outside of its original release.

The "Cream Lemon" franchise of works ran from 1984 to 2005, with a number of them entering the American market in various forms. "The Brothers Grime" series released by Excalibur Films contained "Cream Lemon" works as early as 1986. However, they were not billed as anime and were introduced during the same time that the first underground distribution of erotic works began.

The American release of licensed erotic anime was first attempted in 1991 by Central Park Media, with "I Give My All", but it never occurred. In December 1992, "Devil Hunter Yohko" was the first risque ("ecchi") title that was released by A.D. Vision. While it contains no sexual intercourse, it pushes the limits of the "ecchi" category with sexual dialogue, nudity and one scene in which the heroine is about to be raped.

It was Central Park Media's 1993 release of "Urotsukidoji" which brought the first hentai film to American viewers. Often cited for inventing the tentacle rape subgenre, it contains extreme depictions of violence and monster sex. As such, it is acknowledged for being the first to depict tentacle sex on screen. When the film premiered in the United States, it was described as being "drenched in graphic scenes of perverse sex and ultra-violence".

Following this release, a wealth of pornographic content began to arrive in the United States, with companies such as A.D. Vision, Central Park Media and Media Blasters releasing licensed titles under various labels. A.D. Vision's label SoftCel Pictures released 19 titles in 1995 alone. Another label, Critical Mass, was created in 1996 to release an unedited edition of "Violence Jack". When A.D. Vision's hentai label SoftCel Pictures shut down in 2005, most of its titles were acquired by Critical Mass. Following the bankruptcy of Central Park Media in 2009, the licenses for all Anime 18-related products and movies were transferred to Critical Mass.

The term "eroge" (erotic game) literally defines any erotic game, but has become synonymous with video games depicting the artistic styles of anime and manga. The origins of "eroge" began in the early 1980s, while the computer industry in Japan was struggling to define a computer standard with makers like NEC, Sharp, and Fujitsu competing against one another. The PC98 series, despite lacking in processing power, CD drives and limited graphics, came to dominate the market, with the popularity of "eroge" games contributing to its success.

Because of vague definitions of what constitutes an "erotic game", there are several possible candidates for the first "eroge". If the definition applies to adult themes, the first game was "Softporn Adventure". Released in America in 1981 for the Apple II, this was a text-based comedic game from On-Line Systems. If "eroge" is defined as the first graphical depictions or Japanese adult themes, it would be Koei's 1982 release of "Night Life". Sexual intercourse is depicted through simple graphic outlines. Notably, "Night Life" was not intended to be erotic so much as an instructional guide "to support married life". A series of "undressing" games appeared as early as 1983, such as "Strip Mahjong". The first anime-styled erotic game was "Tenshitachi no Gogo", released in 1985 by JAST. In 1988, ASCII released the first erotic role-playing game, "Chaos Angel". In 1989, AliceSoft released the turn-based role-playing game "Rance" and ELF released "Dragon Knight".

In the late 1980s, "eroge" began to stagnate under high prices and the majority of games containing uninteresting plots and mindless sex. ELF's 1992 release of "Dōkyūsei" came as customer frustration with "eroge" was mounting and spawned a new genre of games called dating sims. "Dōkyūsei" was unique because it had no defined plot and required the player to build a relationship with different girls in order to advance the story. Each girl had her own story, but the prospect of consummating a relationship required the girl growing to love the player; there was no easy sex.

The term "visual novel" is vague, with Japanese and English definitions classifying the genre as a type of interactive fiction game driven by narration and limited player interaction. While the term is often retroactively applied to many games, it was Leaf that coined the term with their "Leaf Visual Novel Series" (LVNS) with the 1996 release of "Shizuku" and "Kizuato". The success of these two dark "eroge" games would be followed by the third and final installment of the LVNS, the 1997 romantic "eroge" "To Heart". "Eroge" visual novels took a new emotional turn with Tactics' 1998 release "". Key's 1999 release of "Kanon" proved to be a major success and would go on to have numerous console ports, two manga series and two anime series.

Japanese laws have impacted depictions of works since the Meiji Restoration, but these predate the common definition of hentai material. Since becoming law in 1907, Article 175 of the Criminal Code of Japan forbids the publication of obscene materials. Specifically, depictions of male–female sexual intercourse and pubic hair are considered obscene, but bare genitalia is not. As censorship is required for published works, the most common representations are the blurring dots on pornographic videos and "bars" or "lights" on still images. In 1986, Toshio Maeda sought to get past censorship on depictions of sexual intercourse, by creating tentacle sex. This led to the large number of works containing sexual intercourse with monsters, demons, robots, and aliens, whose genitals look different from men's. While Western views attribute hentai to any explicit work, it was the products of this censorship which became not only the first titles legally imported to America and Europe, but the first successful ones. While uncut for American release, the United Kingdom's release of "Urotsukidoji" removed many scenes of the violence and tentacle rape scenes.

It was also because of this law that the artists began to depict the characters with a minimum of anatomical details and without pubic hair, by law, prior to 1991. Part of the ban was lifted when Nagisa Oshima prevailed over the obscenity charges at his trial for his film "In the Realm of the Senses". Though not enforced, the lifting of this ban did not apply to anime and manga as they were not deemed artistic exceptions.

Alterations of material or censorship and banning of works are common. The US release of "La Blue Girl" altered the age of the heroine from 16 to 18, removed sex scenes with a dwarf ninja named Nin-nin, and removed the Japanese blurring dots. "La Blue Girl" was outright rejected by UK censors who refused to classify it and prohibited its distribution. In 2011, the Liberal Democratic Party of Japan sought a ban on the subgenre "lolicon".

The most prolific consumers of hentai are men. "Eroge" games in particular combine three favored media—cartoons, pornography and gaming—into an experience. The hentai genre engages a wide audience that expands yearly, and desires better quality and storylines, or works which push the creative envelope. Nobuhiro Komiya, a manga censor, states that the unusual and extreme depictions in hentai are not about perversion so much as they are an example of the profit-oriented industry. Anime depicting normal sexual situations enjoy less market success than those that break social norms, such as sex at schools or bondage.

According to clinical psychologist Megha Hazuria Gorem, "Because toons are a kind of final fantasy, you can make the person look the way you want him or her to look. Every fetish can be fulfilled." Sexologist Narayan Reddy noted of "eroge", "Animators make new games because there is a demand for them, and because they depict things that the gamers do not have the courage to do in real life, or that might just be illegal, these games are an outlet for suppressed desire."

The hentai genre can be divided into numerous subgenres, the broadest of which encompasses heterosexual and homosexual acts. Hentai that features mainly heterosexual interactions occur in both male-targeted ("ero" or "dansei-muke") and female-targeted ("ladies' comics") form. Those that feature mainly homosexual interactions are known as "yaoi" or "Boys' Love" (male–male) and "yuri" (female–female). Both "yaoi" and, to a lesser extent, "yuri", are generally aimed at members of the opposite sex from the persons depicted. While "yaoi" and "yuri" are not always explicit, their pornographic history and association remain. "Yaoi" pornographic usage has remained strong in textual form through fanfiction. The definition of "yuri" has begun to be replaced by the broader definitions of "lesbian-themed animation or comics".

Hentai is perceived as "dwelling" on sexual fetishes. These include dozens of fetish and paraphilia related subgenres, which can be further classified with additional terms, such as heterosexual or homosexual types.

Many works are focused on depicting the mundane and the impossible across every conceivable act and situation, no matter how fantastical. One subgenre of hentai is "futanari" (hermaphroditism), which most often features a female with a penis or penis-like appendage in place of, or in addition to, a vulva. Futanari characters are primarily depicted as having sex with other women and will almost always be submissive with a male; exceptions include Yonekura Kengo's work, which features female empowerment and domination over males.




</doc>
<doc id="14186" url="https://en.wikipedia.org/wiki?curid=14186" title="Henry VII of England">
Henry VII of England

Henry VII (; 28 January 145721 April 1509) was the King of England and Lord of Ireland from his seizure of the crown on 22 August 1485 to his death. He was the first monarch of the House of Tudor.

Henry attained the throne when his forces defeated King Richard III at the Battle of Bosworth Field, the culmination of the Wars of the Roses. He was the last king of England to win his throne on the field of battle. He cemented his claim by marrying Elizabeth of York, daughter of Richard's brother Edward IV. Henry was successful in restoring the power and stability of the English monarchy after the civil war.

Henry is credited with a number of administrative, economic and diplomatic initiatives. His supportive policy toward England's wool industry and his standoff with the Low Countries had long-lasting benefit to the whole English economy. He paid very close attention to detail, and instead of spending lavishly he concentrated on raising new revenues. New taxes stabilised the government's finances. After his death, a commission found widespread abuses in the tax collection process. Henry reigned for nearly 24 years and was peacefully succeeded by his son, Henry VIII.

Henry VII was born at Pembroke Castle on 28 January 1457 to Margaret Beaufort, Countess of Richmond. His father, Edmund Tudor, 1st Earl of Richmond, died three months before his birth.

Henry's paternal grandfather, Owen Tudor, originally from the Tudors of Penmynydd, Isle of Anglesey in Wales, had been a page in the court of Henry V. He rose to become one of the "Squires to the Body to the King" after military service at the Battle of Agincourt. Owen is said to have secretly married the widow of Henry V, Catherine of Valois. One of their sons was Edmund Tudor, father of Henry VII. Edmund was created Earl of Richmond in 1452, and "formally declared legitimate by Parliament".

Henry's main claim to the English throne derived from his mother through the House of Beaufort. Henry's mother, Lady Margaret Beaufort, was a great-granddaughter of John of Gaunt, the Duke of Lancaster and fourth son of Edward III, and his third wife Katherine Swynford. Katherine was Gaunt's mistress for about 25 years. When they married in 1396 they already had four children, including Henry's great-grandfather John Beaufort. Thus, Henry's claim was somewhat tenuous; it was from a woman, and by illegitimate descent. In theory, the Portuguese and Castilian royal families had a better claim as descendants of Catherine of Lancaster, the daughter of John of Gaunt and his second wife Constance of Castile.

Gaunt's nephew Richard II legitimised Gaunt's children by Katherine Swynford by Letters Patent in 1397. In 1407, Henry IV, Gaunt's son by his first wife, issued new Letters Patent confirming the legitimacy of his half-siblings but also declaring them ineligible for the throne. Henry IV's action was of doubtful legality, as the Beauforts were previously legitimised by an Act of Parliament, but it further weakened Henry's claim.

Nonetheless, by 1483 Henry was the senior male Lancastrian claimant remaining after the deaths in battle, by murder or execution of Henry VI, his son Edward of Westminster, Prince of Wales, and the other Beaufort line of descent through Lady Margaret's uncle, the 2nd Duke of Somerset.

Henry also made some political capital out of his Welsh ancestry in attracting military support and safeguarding his army's passage through Wales on its way to the Battle of Bosworth. He came from an old, established Anglesey family that claimed descent from Cadwaladr, in legend, the last ancient British king, and on occasion Henry displayed the red dragon of Cadwaladr. He took it, as well as the standard of St. George, on his procession through London after the victory at Bosworth. A contemporary writer and Henry's biographer, Bernard André, also made much of Henry's Welsh descent.

His hereditary connections to Welsh aristocracy were not strong. He was descended by the paternal line, through several generations, from Ednyfed Fychan, the seneschal (steward) of Gwynedd and through this seneschal's wife from Rhys ap Tewdwr, the King of Deheubarth in South Wales. His more immediate ancestor, Tudur ap Goronwy, had aristocratic land rights, but his sons, who were first cousins to Owain Glyndŵr, sided with Owain in his revolt. One son was executed and the family land was forfeited. Another son, Henry's great-grandfather, became a butler to the Bishop of Bangor. Owen Tudor, the son of the butler, like the children of other rebels, was provided for by Henry V, a circumstance that precipitated his access to Queen Catherine of Valois. Notwithstanding this lineage, to the bards of Wales, Henry was a candidate for Y Mab Darogan, "The Son of Prophecy" who would free the Welsh from oppression.
In 1456, Henry's father Edmund Tudor was captured while fighting for Henry VI in South Wales against the Yorkists. He died in Carmarthen Castle, three months before Henry was born. Henry's uncle Jasper Tudor, the Earl of Pembroke and Edmund's younger brother, undertook to protect the young widow, who was 13 years old when she gave birth to Henry. When Edward IV became King in 1461, Jasper Tudor went into exile abroad. Pembroke Castle, and later the Earldom of Pembroke, were granted to the Yorkist William Herbert, who also assumed the guardianship of Margaret Beaufort and the young Henry.

Henry lived in the Herbert household until 1469, when Richard Neville, Earl of Warwick (the "Kingmaker"), went over to the Lancastrians. Herbert was captured fighting for the Yorkists and executed by Warwick. When Warwick restored Henry VI in 1470, Jasper Tudor returned from exile and brought Henry to court. When the Yorkist Edward IV regained the throne in 1471, Henry fled with other Lancastrians to Brittany, where he spent most of the next 14 years under the protection of Francis II, Duke of Brittany. In November 1476, Henry's protector fell ill and his principal advisers were more amenable to negotiating with the English king. Henry was handed over and escorted to the Breton port of Saint-Malo. While there, he feigned stomach cramps and in the confusion fled into a monastery. Following the Battle of Tewkesbury in 1471, Edward IV prepared to order Henry's extraction and probable execution. The townspeople took exception to his behaviour and Francis recovered from his illness. Thus, a small band of scouts rescued Henry.

By 1483, Henry's mother was actively promoting him as an alternative to Richard III, despite her being married to Lord Stanley, a Yorkist. At Rennes Cathedral on Christmas Day 1483, Henry pledged to marry Elizabeth of York, the eldest daughter of Edward IV, who was also Edward's heir since the presumed death of her brothers, the Princes in the Tower, King Edward V and his brother Richard of Shrewsbury, Duke of York. With money and supplies borrowed from his host, Francis II, Duke of Brittany, Henry tried to land in England, but his conspiracy unravelled resulting in the execution of his primary co-conspirator, the Duke of Buckingham. Now supported by Francis II's prime minister, Pierre Landais, Richard III attempted to extradite Henry from Brittany, but Henry escaped to France. He was welcomed by the French, who readily supplied him with troops and equipment for a second invasion.

Henry gained the support of the Woodvilles, in-laws of the late Edward IV, and sailed with a small French and Scottish force, landing at Mill Bay near Dale, Pembrokeshire. He marched toward England accompanied by his uncle Jasper and the Earl of Oxford. Wales was traditionally a Lancastrian stronghold, and Henry owed the support he gathered to his Welsh birth and ancestry, being directly descended, through his father, from Rhys ap Gruffydd. He amassed an army of about 5,000 soldiers.

Henry devised a plan to seize the throne by engaging Richard quickly because Richard had reinforcements in Nottingham and Leicester. Though outnumbered, Henry's Lancastrian forces decisively defeated Richard's Yorkist army at the Battle of Bosworth Field on 22 August 1485. Several of Richard's key allies, such as the Earl of Northumberland and William and Thomas Stanley, crucially switched sides or left the battlefield. Richard III's death at Bosworth Field effectively ended the Wars of the Roses.

As king, Henry was styled "by the Grace of God, King of England and France and Lord of Ireland". On his succession, Henry became entitled to bear the Royal Arms of England. After his marriage, Henry used as his emblem the red and white rose, which became known as the Tudor rose.

To secure his hold on the throne, Henry declared himself king by right of conquest retroactively from 21 August 1485, the day before Bosworth Field. Thus, anyone who had fought for Richard against him would be guilty of treason and Henry could legally confiscate the lands and property of Richard III, while restoring his own. Henry spared Richard's nephew and designated heir, the Earl of Lincoln, and made Margaret Plantagenet, a Yorkist heiress, Countess of Salisbury "suo jure". He took care not to address the baronage or summon Parliament until after his coronation, which took place in Westminster Abbey on 30 October 1485. After his coronation Henry issued an edict that any gentleman who swore fealty to him would, notwithstanding any previous attainder, be secure in his property and person.

Henry honoured his pledge of December 1483 to marry Elizabeth of York. They were third cousins, as both were great-great-grandchildren of John of Gaunt. Henry and Elizabeth were married on 18 January 1486 . The marriage unified the warring houses and gave his children a strong claim to the throne. The unification of the houses of York and Lancaster by this marriage is symbolised by the heraldic emblem of the Tudor rose, a combination of the white rose of York and the red rose of Lancaster. It also ended future discussion as to whether the descendants of the fourth son of Edward III, Edmund, Duke of York, through marriage to Philippa, heiress of the second son, Lionel, Duke of Clarence, had a superior or inferior claim to those of the third son John of Gaunt, who had held the throne for three generations.

In addition, Henry had Parliament repeal "Titulus Regius", the statute that declared Edward IV's marriage invalid and his children illegitimate, thus legitimising his wife. Amateur historians Bertram Fields and Sir Clements Markham have claimed that he may have been involved in the murder of the Princes in the Tower, as the repeal of "Titulus Regius" gave the Princes a stronger claim to the throne than his own. Alison Weir, however, points out that the Rennes ceremony, two years earlier, was possible only if Henry and his supporters were certain that the Princes were already dead.

Henry married Elizabeth of York with the hope of uniting the Yorkist and Lancastrian sides of the Plantagenet dynastic disputes, and he was largely successful. However, such a level of paranoia persisted that anyone (John de la Pole, Earl of Lincoln, for example) with blood ties to the Plantagenets was suspected of coveting the throne.
Henry secured his crown principally by dividing and undermining the power of the nobility, especially through the aggressive use of bonds and recognisances to secure loyalty. He also enacted laws against livery and maintenance, the great lords' practice of having large numbers of "retainers" who wore their lord's badge or uniform and formed a potential private army.

While he was still in Leicester, after the battle of Bosworth Field, Henry was already taking precautions to prevent any rebellions against his reign. Before leaving Leicester to go to London, Henry dispatched Robert Willoughby to Sheriff Hutton in Yorkshire, to have the ten-year-old Edward, Earl of Warwick, arrested and taken to the Tower of London. Edward was the son of George, Duke of Clarence, and as such he presented a threat as a potential rival to the new King Henry VII for the throne of England. However, Henry was threatened by several active rebellions over the next few years. The first was the rebellion of the Stafford brothers and Viscount Lovell of 1486, which collapsed without fighting.

In 1487, Yorkists led by Lincoln rebelled in support of Lambert Simnel, a boy who was claimed to be the Earl of Warwick, son of Edward IV's brother Clarence (who had last been seen as a prisoner in the Tower). The rebellion began in Ireland, where the traditionally Yorkist nobility, headed by the powerful Gerald FitzGerald, 8th Earl of Kildare, proclaimed Simnel king and provided troops for his invasion of England. The rebellion was defeated and Lincoln killed at the Battle of Stoke. Henry showed remarkable clemency to the surviving rebels: he pardoned Kildare and the other Irish nobles, and he made the boy, Simnel, a servant in the royal kitchen where he was in charge of roasting meats on a spit.

In 1490, a young Fleming, Perkin Warbeck, appeared and claimed to be Richard, the younger of the "Princes in the Tower". Warbeck won the support of Edward IV's sister Margaret of Burgundy. He led attempted invasions of Ireland in 1491 and England in 1495, and persuaded James IV of Scotland to invade England in 1496. In 1497 Warbeck landed in Cornwall with a few thousand troops, but was soon captured and executed.

When the King's agents searched the property of William Stanley (Chamberlain of the Household with direct access to Henry VII) they found a bag of coins amounting to around £10,000 and a collar of livery with Yorkist garnishings. Stanley was accused of supporting Warbeck's cause, arrested and later executed. In response to this threat within his own household, the King instituted more rigid security for access to his person.

In 1499, Henry had the Earl of Warwick executed. However, he spared Warwick's elder sister Margaret. She survived until 1541, when she was executed by Henry VIII.

For most of Henry VII's reign Edward Story was Bishop of Chichester. Story's register still exists and, according to the 19th-century historian W.R.W. Stephens, "affords some illustrations of the avaricious and parsimonious character of the king". It seems that the king was skillful at extracting money from his subjects on many pretexts, including that of war with France or war with Scotland. The money so extracted added to the king's personal fortune rather than being used for the stated purpose.

Unlike his predecessors, Henry VII came to the throne without personal experience in estate management or financial administration. But during his reign he became a fiscally prudent monarch who restored the fortunes of an effectively bankrupt exchequer. Henry VII introduced stability to the financial administration of England by keeping the same financial advisors throughout his reign. For instance, except for the first few months of the reign, Lord Dynham and Thomas Howard, 2nd Duke of Norfolk were the only two office holders in the position of Lord High Treasurer of England throughout his reign.

Henry VII improved tax collection within the realm by introducing ruthlessly efficient mechanisms of taxation. He was supported in this effort by his chancellor, Archbishop John Morton, whose "Morton's Fork" was a catch-22 method of ensuring that nobles paid increased taxes: those nobles who spent little must have saved much, and thus they could afford the increased taxes; on the other hand, those nobles who spent much obviously had the means to pay the increased taxes. Royal government was also reformed with the introduction of the King's Council that kept the nobility in check.

The capriciousness and lack of due process that indebted many would tarnish his legacy and were soon ended upon Henry VII's death, after a commission revealed widespread abuses. According to the contemporary historian Polydore Vergil, simple "greed" underscored the means by which royal control was over-asserted in Henry's final years. Henry VIII executed Richard Empson and Edmund Dudley, his two most hated tax collectors, on trumped-up charges of treason.

He established the pound avoirdupois as a standard of weight; it later became part of the Imperial and customary systems of units.

Henry VII's policy was both to maintain peace and to create economic prosperity. Up to a point, he succeeded. The Treaty of Redon was signed in February 1489 in between Henry and representatives of Brittany. Based on the terms of the accord, Henry sent 6000 troops to fight (at the expense of Brittany) under the command of Lord Daubeney. The purpose of the agreement was to prevent France from annexing Brittany. According to John M. Currin, the treaty redefined Anglo-Breton relations, Henry started a new policy to recover Guyenne and other lost Plantagenet claims in France. The treaty marks a shift from neutrality over the French invasion of Brittany to active intervention against it. 

Henry later conclude a treaty with France at Etaples that brought money into the coffers of England, and ensured the French would not support pretenders to the English throne, such as Perkin Warbeck. However, this treaty came at a slight price, as Henry mounted a minor invasion of Brittany in November 1492. Henry decided to keep Brittany out of French hands, signed an alliance with Spain to that end, and sent 6,000 troops to France. The confused, fractious nature of Breton politics undermined his efforts, which finally failed after three sizeable expeditions, at a cost of £24,000. However, as France was becoming more concerned with the Italian Wars, the French were happy to agree to the Treaty of Etaples. Henry had pressured the French by laying siege to Boulogne in October 1492. 
Henry had been under the financial and physical protection of the French throne or its vassals for most of his life, before he became king. To strengthen his position, however, he subsidised shipbuilding, so strengthening the navy (he commissioned Europe's first ever – and the world's oldest surviving – dry dock at Portsmouth in 1495) and improving trading opportunities.

Henry VII was one of the first European monarchs to recognise the importance of the newly united Spanish kingdom; he concluded the Treaty of Medina del Campo, by which his son, Arthur Tudor, was married to Catherine of Aragon. He also concluded the Treaty of Perpetual Peace with Scotland (the first treaty between England and Scotland for almost two centuries), which betrothed his daughter Margaret to King James IV of Scotland. By this marriage, Henry VII hoped to break the Auld Alliance between Scotland and France. Though this was not achieved during his reign, the marriage eventually led to the union of the English and Scottish crowns under Margaret's great-grandson, James VI and I, following the death of Henry's granddaughter Elizabeth I.

He also formed an alliance with Holy Roman Emperor Maximilian I (1493–1519) and persuaded Pope Innocent VIII to issue a papal bull of excommunication against all pretenders to Henry's throne.

Henry VII was much enriched by trading alum, which was used in the wool and cloth trades as a chemical fixative for dyeing fabrics. Since alum was mined in only one area in Europe (Tolfa, Italy), it was a scarce commodity and therefore especially valuable to its land holder, the pope. With the English economy heavily invested in wool production, Henry VII became involved in the alum trade in 1486. With the assistance of the Italian merchant banker Lodovico della Fava and the Italian banker Girolamo Frescobaldi, Henry VII became deeply involved in the trade by licensing ships, obtaining alum from the Ottoman Empire, and selling it to the Low Countries and in England. This trade made an expensive commodity cheaper, which raised opposition from Pope Julius II, since the Tolfa mine was a part of papal territory and had given the Pope monopoly control over alum.
Henry's most successful diplomatic achievement as regards the economy was the "Magnus Intercursus" ("great agreement") of 1496. In 1494, Henry embargoed trade (mainly in wool) with the Netherlands in retaliation for Margaret of Burgundy's support for Perkin Warbeck. The Merchant Adventurers, the company which enjoyed the monopoly of the Flemish wool trade, relocated from Antwerp to Calais. At the same time, Flemish merchants were ejected from England. The stand-off eventually paid off for Henry. Both parties realised they were mutually disadvantaged by the reduction in commerce. Its restoration by the "Magnus Intercursus" was very much to England's benefit in removing taxation for English merchants and significantly increasing England's wealth. In turn, Antwerp became an extremely important trade entrepôt (transshipment port), through which, for example, goods from the Baltic, spices from the east and Italian silks were exchanged for English cloth.

In 1506, Henry extorted the Treaty of Windsor from Philip the Handsome of Burgundy. Philip had been shipwrecked on the English coast, and while Henry's guest, was bullied into an agreement so favourable to England at the expense of the Netherlands that it was dubbed the "Malus Intercursus" ("evil agreement"). France, Burgundy, the Holy Roman Empire, Spain and the Hanseatic League all rejected the treaty, which was never in force. Philip died shortly after the negotiations.

Henry's principal problem was to restore royal authority in a realm recovering from the Wars of the Roses. There were too many powerful noblemen and, as a consequence of the system of so-called bastard feudalism, each had what amounted to private armies of indentured retainers (mercenaries masquerading as servants).
He was content to allow the nobles their regional influence if they were loyal to him. For instance, the Stanley family had control of Lancashire and Cheshire, upholding the peace on the condition that they stayed within the law. In other cases, he brought his over-powerful subjects to heel by decree. He passed laws against "livery" (the upper classes' flaunting of their adherents by giving them badges and emblems) and "maintenance" (the keeping of too many male "servants"). These laws were used shrewdly in levying fines upon those that he perceived as threats.

However, his principal weapon was the Court of Star Chamber. This revived an earlier practice of using a small (and trusted) group of the Privy Council as a personal or Prerogative Court, able to cut through the cumbersome legal system and act swiftly. Serious disputes involving the use of personal power, or threats to royal authority, were thus dealt with.

Henry VII used Justices of the Peace on a large, nationwide scale. They were appointed for every shire and served for a year at a time. Their chief task was to see that the laws of the country were obeyed in their area. Their powers and numbers steadily increased during the time of the Tudors, never more so than under Henry's reign. Despite this, Henry was keen to constrain their power and influence, applying the same principles to the Justices of the Peace as he did to the nobility: a similar system of bonds and recognisances to that which applied to both the gentry and the nobles who tried to exert their elevated influence over these local officials.

All Acts of Parliament were overseen by the Justices of the Peace. For example, Justices of the Peace could replace suspect jurors in accordance with the 1495 act preventing the corruption of juries. They were also in charge of various administrative duties, such as the checking of weights and measures.

By 1509, Justices of the Peace were key enforcers of law and order for Henry VII. They were unpaid, which, in comparison with modern standards, meant a lesser tax bill to pay for a police force. Local gentry saw the office as one of local influence and prestige and were therefore willing to serve. Overall, this was a successful area of policy for Henry, both in terms of efficiency and as a method of reducing the corruption endemic within the nobility of the Middle Ages.

In 1502, Henry VII's life took a difficult and personal turn in which many people he was close to died in quick succession. His first son and heir apparent, Arthur, Prince of Wales, died suddenly at Ludlow Castle, very likely from a viral respiratory illness known at the time as the "English sweating sickness". This made Henry, Duke of York, heir apparent to the throne. The King, normally a reserved man who rarely showed much emotion in public unless angry, surprised his courtiers by his intense grief and sobbing at his son's death, while his concern for the Queen is evidence that the marriage was a happy one, as is his reaction to the Queen's death the following year, when he shut himself away for several days, refusing to speak to anyone.

Henry VII wanted to maintain the Spanish alliance. He, therefore, arranged a papal dispensation from Pope Julius II for Prince Henry to marry his brother's widow Catherine, a relationship that would have otherwise precluded marriage in the Roman Catholic Church. In 1503, Queen Elizabeth died in childbirth, so King Henry had the dispensation also permit him to marry Catherine himself. After obtaining the dispensation, Henry had second thoughts about the marriage of his son and Catherine. Catherine's mother Isabella I of Castile had died and Catherine's sister Joanna had succeeded her; Catherine was, therefore, daughter of only one reigning monarch and so less desirable as a spouse for Henry VII's heir-apparent. The marriage did not take place during his lifetime. Otherwise, at the time of his father's arranging of the marriage to Catherine of Aragon, the future Henry VIII was too young to contract the marriage according to Canon Law and would be ineligible until age fourteen.

Henry made half-hearted plans to remarry and beget more heirs, but these never came to anything. He entertained thoughts of remarriage to renew the alliance with Spain — Joanna, Dowager Queen of Naples (daughter of Ferdinand I of Naples), Joanna, Queen of Castile (daughter of Ferdinand and Isabella), and Margaret, Dowager Duchess of Savoy (sister-in-law of Joanna of Castile), were all considered In 1505 he was sufficiently interested in a potential marriage to Joanna of Naples, that he sent ambassadors to Naples to report on the 27-year-old's physical suitability. The wedding never took place, and the physical description Henry sent with his ambassadors of what he desired in a new wife matched the description of Elizabeth. 

After 1503, records show the Tower of London was never again used as a royal residence by Henry Tudor, and all royal births under Henry VIII took place in palaces. Henry VII was shattered by the loss of Elizabeth, and her death broke his heart. Of all British kings, Henry VII is one of only a handful that never had any known mistress, and for the times, it is very unusual that he did not remarry: his son, Henry, was the only heir left and the death of Arthur put the position of the House of Tudor in a more precarious political position.

During his lifetime the nobility often jeered him for re-centralizing power in London, and later the 16th-century historian Francis Bacon was ruthlessly critical of the methods by which he enforced tax law, but it is equally true that Henry Tudor was hellbent on keeping detailed records of his personal finances, down to the last halfpenny; these and one account book detailing the expenses of his queen survive in the British National Archives, as do accounts of courtiers and many of the king's own letters. Until the death of his wife, the evidence is clear from these accounting books that Henry Tudor was a more doting father and husband than was widely known and there is evidence that his outwardly austere personality belied a devotion to his family. Letters to relatives have an affectionate tone not captured by official state business, as evidenced by many written to his mother Margaret. Many of the entries show a man who loosened his purse strings generously for his wife and children, and not just on necessities: in spring 1491 he spent a great amount of gold on a lute for his daughter Mary; the following year he spent money on a lion for Elizabeth's menagerie. With Elizabeth's death, the possibilities for such family indulgences greatly diminished. Immediately afterwards, Henry became very sick and nearly died himself, allowing only Margaret Beaufort, his mother, near him: "privily departed to a solitary place, and would that no man should resort unto him." Worse still, Henry's older daughter Margaret had previously been betrothed to the King of Scotland, James IV, and within months of her mother's death she had to be escorted to the border by her father: he would never see her again. Margaret Tudor wrote letters to her father declaring her homesickness, but Henry could do nothing but mourn the loss of his family and honor the terms of the peace treaty he had agreed to with the King of Scotland. 
Henry VII died of tuberculosis at Richmond Palace on 21 April 1509 and was buried in the chapel he commissioned in Westminster Abbey next to his wife, Elizabeth. He was succeeded by his second son, Henry VIII (reigned 1509–47). His mother survived him but died two months later on 29 June 1509.

Henry is the first English king of whose appearance good contemporary visual records in realistic portraits exist that are relatively free of idealization. At 27, he was tall and slender, with small blue eyes, which were said to have a noticeable animation of expression, and noticeably bad teeth in a long, sallow face beneath very fair hair. Amiable and high-spirited, Henry was friendly if dignified in manner, and it was clear to everyone that he was extremely intelligent. His biographer, Professor Chrimes, credits him – even before he had become king – with "a high degree of personal magnetism, ability to inspire confidence, and a growing reputation for shrewd decisiveness". On the debit side, he may have looked a little delicate as he suffered from poor health.

Historians have always compared Henry VII with his continental contemporaries, especially Louis XI of France and Ferdinand II of Aragon. By 1600 historians emphasised Henry's wisdom in drawing lessons in statecraft from other monarchs. In 1622 Francis Bacon published his "History of the Reign of King Henry VII". By 1900 the "New Monarchy" interpretation stressed the common factors that in each country led to the revival of monarchical power. This approach raised puzzling questions about similarities and differences in the development of national states. In the late 20th century a model of European state formation was prominent in which Henry less resembles Louis and Ferdinand.






</doc>
<doc id="14187" url="https://en.wikipedia.org/wiki?curid=14187" title="Henry VIII">
Henry VIII

Henry VIII (28 June 1491 – 28 January 1547) was King of England from 1509 until his death in 1547. Henry is best known for his six marriages, and, in particular, his efforts to have his first marriage (to Catherine of Aragon) annulled. His disagreement with Pope Clement VII on the question of such an annulment led Henry to initiate the English Reformation, separating the Church of England from papal authority. He appointed himself the Supreme Head of the Church of England and dissolved convents and monasteries, for which he was excommunicated. Henry is also known as "the father of the Royal Navy," as he invested heavily in the navy, increasing its size from a few to more than 50 ships, and established the Navy Board.

Domestically, Henry is known for his radical changes to the English Constitution, ushering in the theory of the divine right of kings. He also greatly expanded royal power during his reign. He frequently used charges of treason and heresy to quell dissent, and those accused were often executed without a formal trial by means of bills of attainder. He achieved many of his political aims through the work of his chief ministers, some of whom were banished or executed when they fell out of his favour. Thomas Wolsey, Thomas More, Thomas Cromwell, Richard Rich, and Thomas Cranmer all figured prominently in his administration.

Henry was an extravagant spender, using the proceeds from the dissolution of the monasteries and acts of the Reformation Parliament. He also converted the money that was formerly paid to Rome into royal revenue. Despite the money from these sources, he was continually on the verge of financial ruin due to his personal extravagance, as well as his numerous costly and largely unsuccessful wars, particularly with King Francis I of France, Holy Roman Emperor Charles V, James V of Scotland and the Scottish regency under the Earl of Arran and Mary of Guise. At home, he oversaw the legal union of England and Wales with the Laws in Wales Acts 1535 and 1542, and he was the first English monarch to rule as King of Ireland following the Crown of Ireland Act 1542.

Henry's contemporaries considered him an attractive, educated, and accomplished king. He has been described as "one of the most charismatic rulers to sit on the English throne" and his reign has been described as the "most important" in English history. He was an author and composer. As he aged, however, he became severely overweight and his health suffered, causing his death in 1547. He is frequently characterised in his later life as a lustful, egotistical, paranoid and tyrannical monarch. He was succeeded by his son Edward VI.

Born 28 June 1491 at the Palace of Placentia in Greenwich, Kent, Henry Tudor was the third child and second son of Henry VII and Elizabeth of York. Of the young Henry's six (or seven) siblings, only three – Arthur, Prince of Wales; Margaret; and Mary – survived infancy. He was baptised by Richard Fox, the Bishop of Exeter, at a church of the Observant Franciscans close to the palace. In 1493, at the age of two, Henry was appointed Constable of Dover Castle and Lord Warden of the Cinque Ports. He was subsequently appointed Earl Marshal of England and Lord Lieutenant of Ireland at age three, and was made a Knight of the Bath soon after. The day after the ceremony he was created Duke of York and a month or so later made Warden of the Scottish Marches. In May 1495, he was appointed to the Order of the Garter. The reason for all the appointments to a small child was so his father could keep personal control of lucrative positions and not share them with established families. Henry was given a first-rate education from leading tutors, becoming fluent in Latin and French, and learning at least some Italian. Not much is known about his early life – save for his appointments – because he was not expected to become king. In November 1501, Henry also played a considerable part in the ceremonies surrounding his brother's marriage to Catherine of Aragon, the youngest surviving child of King Ferdinand II of Aragon and Queen Isabella I of Castile. As Duke of York, Henry used the arms of his father as king, differenced by a "label of three points ermine". He was further honoured, on 9 February 1506, by Holy Roman Emperor Maximilian I who made him a Knight of the Golden Fleece.

In 1502, Arthur died at the age of 15, possibly of sweating sickness, just 20 weeks after his marriage to Catherine. Arthur's death thrust all his duties upon his younger brother, the 10-year-old Henry. After a little debate, Henry became the new Duke of Cornwall in October 1502, and the new Prince of Wales and Earl of Chester in February 1503. Henry VII gave the boy few tasks. Young Henry was strictly supervised and did not appear in public. As a result, he ascended the throne "untrained in the exacting art of kingship".

Henry VII renewed his efforts to seal a marital alliance between England and Spain, by offering his second son in marriage to Arthur's widow Catherine. Both Isabella and Henry VII were keen on the idea, which had arisen very shortly after Arthur's death. On 23 June 1503, a treaty was signed for their marriage, and they were betrothed two days later. A papal dispensation was only needed for the "impediment of public honesty" if the marriage had not been consummated as Catherine and her duenna claimed, but Henry VII and the Spanish ambassador set out instead to obtain a dispensation for "affinity", which took account of the possibility of consummation. Cohabitation was not possible because Henry was too young. Isabella's death in 1504, and the ensuing problems of succession in Castile, complicated matters. Her father preferred her to stay in England, but Henry VII's relations with Ferdinand had deteriorated. Catherine was therefore left in limbo for some time, culminating in Prince Henry's rejection of the marriage as soon he was able, at the age of 14. Ferdinand's solution was to make his daughter ambassador, allowing her to stay in England indefinitely. Devout, she began to believe that it was God's will that she marry the prince despite his opposition.

Henry VII died on 21 April 1509, and the 17-year-old Henry succeeded him as king. Soon after his father's burial on 10 May, Henry suddenly declared that he would indeed marry Catherine, leaving unresolved several issues concerning the papal dispensation and a missing part of the marriage portion. The new king maintained that it had been his father's dying wish that he marry Catherine. Whether or not this was true, it was certainly convenient. Emperor Maximilian I had been attempting to marry his granddaughter (and Catherine's niece) Eleanor to Henry; she had now been jilted. Henry's wedding to Catherine was kept low-key and was held at the friar's church in Greenwich on 11 June 1509. On 23 June 1509, Henry led the now 23-year-old Catherine from the Tower of London to Westminster Abbey for their coronation, which took place the following day. It was a grand affair: the king's passage was lined with tapestries and laid with fine cloth. Following the ceremony, there was a grand banquet in Westminster Hall. As Catherine wrote to her father, "our time is spent in continuous festival".

Two days after his coronation, Henry arrested his father's two most unpopular ministers, Sir Richard Empson and Edmund Dudley. They were charged with high treason and were executed in 1510. Politically-motivated executions would remain one of Henry's primary tactics for dealing with those who stood in his way. Henry also returned to the public some of the money supposedly extorted by the two ministers. By contrast, Henry's view of the House of York – potential rival claimants for the throne – was more moderate than his father's had been. Several who had been imprisoned by his father, including the Marquess of Dorset, were pardoned. Others (most notably Edmund de la Pole) went unreconciled; de la Pole was eventually beheaded in 1513, an execution prompted by his brother Richard siding against the king.

Soon after, Catherine conceived, but the child, a girl, was stillborn on 31 January 1510. About four months later, Catherine again became pregnant. On New Year's Day 1511, the child – Henry – was born. After the grief of losing their first child, the couple were pleased to have a boy and festivities were held, including a two-day joust known as the Westminster Tournament. However, the child died seven weeks later. Catherine had two stillborn sons in 1513 and 1515, but gave birth in February 1516 to a girl, Mary. Relations between Henry and Catherine had been strained, but they eased slightly after Mary's birth.

Although Henry's marriage to Catherine has since been described as "unusually good", it is known that Henry took mistresses. It was revealed in 1510 that Henry had been conducting an affair with one of the sisters of Edward Stafford, 3rd Duke of Buckingham, either Elizabeth or Anne Hastings, Countess of Huntingdon. The most significant mistress for about three years, starting in 1516, was Elizabeth Blount. Blount is one of only two completely undisputed mistresses, considered by some to be few for a virile young king. Exactly how many Henry had is disputed: David Loades believes Henry had mistresses "only to a very limited extent", whilst Alison Weir believes there were numerous other affairs. There is no evidence that Catherine protested, and in 1518 she fell pregnant again with another girl, who was also stillborn. Blount gave birth in June 1519 to Henry's illegitimate son, Henry FitzRoy. The young boy was made Duke of Richmond in June 1525 in what some thought was one step on the path to his eventual legitimisation. In 1533, FitzRoy married Mary Howard, but died childless three years later. At the time of Richmond's death in June 1536, Parliament was enacting the Second Succession Act, which could have allowed him to become king.

In 1510, France, with a fragile alliance with the Holy Roman Empire in the League of Cambrai, was winning a war against Venice. Henry renewed his father's friendship with Louis XII of France, an issue that divided his council. Certainly war with the combined might of the two powers would have been exceedingly difficult. Shortly thereafter, however, Henry also signed a pact with Ferdinand. After Pope Julius II created the anti-French Holy League in October 1511, Henry followed Ferdinand's lead and brought England into the new League. An initial joint Anglo-Spanish attack was planned for the spring to recover Aquitaine for England, the start of making Henry's dreams of ruling France a reality. The attack, however, following a formal declaration of war in April 1512, was not led by Henry personally and was a considerable failure; Ferdinand used it simply to further his own ends, and it strained the Anglo-Spanish alliance. Nevertheless, the French were pushed out of Italy soon after, and the alliance survived, with both parties keen to win further victories over the French. Henry then pulled off a diplomatic coup by convincing the Emperor to join the Holy League. Remarkably, Henry had also secured the promised title of "Most Christian King of France" from Julius and possibly coronation by the Pope himself in Paris, if only Louis could be defeated.

On 30 June 1513, Henry invaded France, and his troops defeated a French army at the Battle of the Spurs – a relatively minor result, but one which was seized on by the English for propaganda purposes. Soon after, the English took Thérouanne and handed it over to Maximillian; Tournai, a more significant settlement, followed. Henry had led the army personally, complete with large entourage. His absence from the country, however, had prompted his brother-in-law, James IV of Scotland, to invade England at the behest of Louis. Nevertheless, the English army, overseen by Queen Catherine, decisively defeated the Scots at the Battle of Flodden on 9 September 1513. Among the dead was the Scottish king, thus ending Scotland's brief involvement in the war. These campaigns had given Henry a taste of the military success he so desired. However, despite initial indications, he decided not to pursue a 1514 campaign. He had been supporting Ferdinand and Maximilian financially during the campaign but had received little in return; England's coffers were now empty. With the replacement of Julius by Pope Leo X, who was inclined to negotiate for peace with France, Henry signed his own treaty with Louis: his sister Mary would become Louis' wife, having previously been pledged to the younger Charles, and peace was secured for eight years, a remarkably long time.

Charles V ascended the thrones of both Spain and the Holy Roman Empire following the deaths of his grandfathers, Ferdinand in 1516 and Maximilian in 1519. Francis I likewise became king of France upon the death of Louis in 1515, leaving three relatively young rulers and an opportunity for a clean slate. The careful diplomacy of Cardinal Thomas Wolsey had resulted in the Treaty of London in 1518, aimed at uniting the kingdoms of western Europe in the wake of a new Ottoman threat, and it seemed that peace might be secured. Henry met Francis I on 7 June 1520 at the Field of the Cloth of Gold near Calais for a fortnight of lavish entertainment. Both hoped for friendly relations in place of the wars of the previous decade. The strong air of competition laid to rest any hopes of a renewal of the Treaty of London, however, and conflict was inevitable. Henry had more in common with Charles, whom he met once before and once after Francis. Charles brought the Empire into war with France in 1521; Henry offered to mediate, but little was achieved and by the end of the year Henry had aligned England with Charles. He still clung to his previous aim of restoring English lands in France, but also sought to secure an alliance with Burgundy, then part of Charles' realm, and the continued support of Charles. A small English attack in the north of France made up little ground. Charles defeated and captured Francis at Pavia and could dictate peace; but he believed he owed Henry nothing. Sensing this, Henry decided to take England out of the war before his ally, signing the Treaty of the More on 30 August 1525.

During his marriage to Catherine of Aragon, Henry conducted an affair with Mary Boleyn, Catherine's lady-in-waiting. There has been speculation that Mary's two children, Henry Carey and Catherine Carey, were fathered by Henry, but this has never been proved, and the King never acknowledged them as he did in the case of Henry FitzRoy. In 1525, as Henry grew more impatient with Catherine's inability to produce the male heir he desired, he became enamoured of Boleyn's sister, Anne Boleyn, then a charismatic young woman of 25 in the Queen's entourage. Anne, however, resisted his attempts to seduce her, and refused to become his mistress as her sister had. It was in this context that Henry considered his three options for finding a dynastic successor and hence resolving what came to be described at court as the King's "great matter". These options were legitimising Henry FitzRoy, which would take the intervention of the pope and would be open to challenge; marrying off Mary as soon as possible and hoping for a grandson to inherit directly, but Mary was considered unlikely to conceive before Henry's death; or somehow rejecting Catherine and marrying someone else of child-bearing age. Probably seeing the possibility of marrying Anne, the third was ultimately the most attractive possibility to the 34-year-old Henry, and it soon became the King's absorbing desire to annul his marriage to the now 40-year-old Catherine. It was a decision that would lead Henry to reject papal authority and initiate the English Reformation.

Henry's precise motivations and intentions over the coming years are not widely agreed on. Henry himself, at least in the early part of his reign, was a devout and well-informed Catholic to the extent that his 1521 publication "Assertio Septem Sacramentorum" ("Defence of the Seven Sacraments") earned him the title of "Fidei Defensor" (Defender of the Faith) from Pope Leo X. The work represented a staunch defence of papal supremacy, albeit one couched in somewhat contingent terms. It is not clear exactly when Henry changed his mind on the issue as he grew more intent on a second marriage. Certainly, by 1527 he had convinced himself that Catherine had produced no male heir because their union was "blighted in the eyes of God." Indeed, in marrying Catherine, his brother's wife, he had acted contrary to Leviticus 20:21, an impediment Henry now believed that the Pope never had the authority to dispense with. It was this argument Henry took to Pope Clement VII in 1527 in the hope of having his marriage to Catherine annulled, forgoing at least one less openly defiant line of attack. In going public, all hope of tempting Catherine to retire to a nunnery or otherwise stay quiet was lost. Henry sent his secretary, William Knight, to appeal directly to the Holy See by way of a deceptively worded draft papal bull. Knight was unsuccessful; the Pope could not be misled so easily.

Other missions concentrated on arranging an ecclesiastical court to meet in England, with a representative from Clement VII. Though Clement agreed to the creation of such a court, he never had any intention of empowering his legate, Lorenzo Campeggio, to decide in Henry's favour. This bias was perhaps the result of pressure from Emperor Charles V, Catherine's nephew, though it is not clear how far this influenced either Campeggio or the Pope. After less than two months of hearing evidence, Clement called the case back to Rome in July 1529, from which it was clear that it would never re-emerge. With the chance for an annulment lost and England's place in Europe forfeit, Cardinal Wolsey bore the blame. He was charged with "praemunire" in October 1529 and his fall from grace was "sudden and total". Briefly reconciled with Henry (and officially pardoned) in the first half of 1530, he was charged once more in November 1530, this time for treason, but died while awaiting trial. After a short period in which Henry took government upon his own shoulders, Sir Thomas More took on the role of Lord Chancellor and chief minister. Intelligent and able, but also a devout Catholic and opponent of the annulment, More initially cooperated with the king's new policy, denouncing Wolsey in Parliament.

A year later, Catherine was banished from court, and her rooms were given to Anne. Anne was an unusually educated and intellectual woman for her time, and was keenly absorbed and engaged with the ideas of the Protestant Reformers, though the extent to which she herself was a committed Protestant is much debated. When Archbishop of Canterbury William Warham died, Anne's influence and the need to find a trustworthy supporter of the annulment had Thomas Cranmer appointed to the vacant position. This was approved by the Pope, unaware of the King's nascent plans for the Church.

Henry was married to Catherine for 24 years. Their divorce has been described as a "deeply wounding and isolating" experience for Henry.

In the winter of 1532, Henry met with Francis I at Calais and enlisted the support of the French king for his new marriage. Immediately upon returning to Dover in England, Henry, now 41, and Anne went through a secret wedding service. She soon became pregnant, and there was a second wedding service in London on 25 January 1533. On 23 May 1533, Cranmer, sitting in judgment at a special court convened at Dunstable Priory to rule on the validity of the king's marriage to Catherine of Aragon, declared the marriage of Henry and Catherine null and void. Five days later, on 28 May 1533, Cranmer declared the marriage of Henry and Anne to be valid. Catherine was formally stripped of her title as queen, becoming instead "princess dowager" as the widow of Arthur. In her place, Anne was crowned queen consort on 1 June 1533. The queen gave birth to a daughter slightly prematurely on 7 September 1533. The child was christened Elizabeth, in honour of Henry's mother, Elizabeth of York.

Following the marriage, there was a period of consolidation, taking the form of a series of statutes of the Reformation Parliament aimed at finding solutions to any remaining issues, whilst protecting the new reforms from challenge, convincing the public of their legitimacy, and exposing and dealing with opponents. Although the canon law was dealt with at length by Cranmer and others, these acts were advanced by Thomas Cromwell, Thomas Audley and the Duke of Norfolk and indeed by Henry himself. With this process complete, in May 1532 More resigned as Lord Chancellor, leaving Cromwell as Henry's chief minister. With the Act of Succession 1533, Catherine's daughter, Mary, was declared illegitimate; Henry's marriage to Anne was declared legitimate; and Anne's issue was decided to be next in the line of succession. With the Acts of Supremacy in 1534, Parliament also recognised the King's status as head of the church in England and, with the Act in Restraint of Appeals in 1532, abolished the right of appeal to Rome. It was only then that Pope Clement took the step of excommunicating Henry and Thomas Cranmer, although the excommunication was not made official until some time later.

The king and queen were not pleased with married life. The royal couple enjoyed periods of calm and affection, but Anne refused to play the submissive role expected of her. The vivacity and opinionated intellect that had made her so attractive as an illicit lover made her too independent for the largely ceremonial role of a royal wife and it made her many enemies. For his part, Henry disliked Anne's constant irritability and violent temper. After a false pregnancy or miscarriage in 1534, he saw her failure to give him a son as a betrayal. As early as Christmas 1534, Henry was discussing with Cranmer and Cromwell the chances of leaving Anne without having to return to Catherine. Henry is traditionally believed to have had an affair with Margaret ("Madge") Shelton in 1535, although historian Antonia Fraser argues that Henry in fact had an affair with her sister Mary Shelton.

Opposition to Henry's religious policies was quickly suppressed in England. A number of dissenting monks, including the first Carthusian Martyrs, were executed and many more pilloried. The most prominent resisters included John Fisher, Bishop of Rochester, and Sir Thomas More, both of whom refused to take the oath to the King. Neither Henry nor Cromwell sought to have the men executed; rather, they hoped that the two might change their minds and save themselves. Fisher openly rejected Henry as the Supreme Head of the Church, but More was careful to avoid openly breaking the Treasons Act of 1534, which (unlike later acts) did not forbid mere silence. Both men were subsequently convicted of high treason, however – More on the evidence of a single conversation with Richard Rich, the Solicitor General. Both were duly executed in the summer of 1535.

These suppressions, as well as the Dissolution of the Lesser Monasteries Act of 1536, in turn contributed to more general resistance to Henry's reforms, most notably in the Pilgrimage of Grace, a large uprising in northern England in October 1536. Some 20,000 to 40,000 rebels were led by Robert Aske, together with parts of the northern nobility. Henry VIII promised the rebels he would pardon them and thanked them for raising the issues. Aske told the rebels they had been successful and they could disperse and go home. Henry saw the rebels as traitors and did not feel obliged to keep his promises with them, so when further violence occurred after Henry's offer of a pardon he was quick to break his promise of clemency. The leaders, including Aske, were arrested and executed for treason. In total, about 200 rebels were executed, and the disturbances ended.

On 8 January 1536, news reached the king and the queen that Catherine of Aragon had died. The following day, Henry dressed all in yellow, with a white feather in his bonnet. The queen was pregnant again, and she was aware of the consequences if she failed to give birth to a son. Later that month, the King was unhorsed in a tournament and was badly injured; it seemed for a time that his life was in danger. When news of this accident reached the queen, she was sent into shock and miscarried a male child that was about 15 weeks old, on the day of Catherine's funeral, 29 January 1536. For most observers, this personal loss was the beginning of the end of this royal marriage.

Although the Boleyn family still held important positions on the Privy Council, Anne had many enemies, including the Duke of Suffolk. Even her own uncle, the Duke of Norfolk, had come to resent her attitude to her power. The Boleyns preferred France over the Emperor as a potential ally, but the King's favour had swung towards the latter (partly because of Cromwell), damaging the family's influence. Also opposed to Anne were supporters of reconciliation with Princess Mary (among them the former supporters of Catherine), who had reached maturity. A second annulment was now a real possibility, although it is commonly believed that it was Cromwell's anti-Boleyn influence that led opponents to look for a way of having her executed.

Anne's downfall came shortly after she had recovered from her final miscarriage. Whether it was primarily the result of allegations of conspiracy, adultery, or witchcraft remains a matter of debate among historians. Early signs of a fall from grace included the King's new mistress, the 28-year-old Jane Seymour, being moved into new quarters, and Anne's brother, George Boleyn, being refused the Order of the Garter, which was instead given to Nicholas Carew. Between 30 April and 2 May, five men, including Anne's brother, were arrested on charges of treasonable adultery and accused of having sexual relationships with the queen. Anne was also arrested, accused of treasonous adultery and incest. Although the evidence against them was unconvincing, the accused were found guilty and condemned to death. George Boleyn and the other accused men were executed on 17 May 1536. At 8 am on 19 May 1536, Anne was executed on Tower Green.

The day after Anne's execution in 1536 the 45-year-old Henry became engaged to Seymour, who had been one of the Queen's ladies-in-waiting. They were married ten days later at the Palace of Whitehall, Whitehall, London, in the Queen's closet by Bishop Gardiner. On 12 October 1537, Jane gave birth to a son, Prince Edward, the future Edward VI. The birth was difficult, and the queen died on 24 October 1537 from an infection and was buried in Windsor. The euphoria that had accompanied Edward's birth became sorrow, but it was only over time that Henry came to long for his wife. At the time, Henry recovered quickly from the shock. Measures were immediately put in place to find another wife for Henry, which, at the insistence of Cromwell and the court, were focused on the European continent.

With Charles V distracted by the internal politics of his many kingdoms and external threats, and Henry and Francis on relatively good terms, domestic and not foreign policy issues had been Henry's priority in the first half of the 1530s. In 1536, for example, Henry granted his assent to the Laws in Wales Act 1535, which legally annexed Wales, uniting England and Wales into a single nation. This was followed by the Second Succession Act (the Act of Succession 1536), which declared Henry's children by Jane to be next in the line of succession and declared both Mary and Elizabeth illegitimate, thus excluding them from the throne. The king was also granted the power to further determine the line of succession in his will, should he have no further issue. However, when Charles and Francis made peace in January 1539, Henry became increasingly paranoid, perhaps as a result of receiving a constant list of threats to the kingdom (real or imaginary, minor or serious) supplied by Cromwell in his role as spymaster. Enriched by the dissolution of the monasteries, Henry used some of his financial reserves to build a series of coastal defences and set some aside for use in the event of a Franco-German invasion.

Having considered the matter, Cromwell suggested Anne, the 25-year-old sister of the Duke of Cleves, who was seen as an important ally in case of a Roman Catholic attack on England, for the duke fell between Lutheranism and Catholicism. Hans Holbein the Younger was dispatched to Cleves to paint a portrait of Anne for the king. Despite speculation that Holbein painted her in an overly flattering light, it is more likely that the portrait was accurate; Holbein remained in favour at court. After seeing Holbein's portrait, and urged on by the complimentary description of Anne given by his courtiers, the 49-year-old king agreed to wed Anne. However, it was not long before Henry wished to annul the marriage so he could marry another. Anne did not argue, and confirmed that the marriage had never been consummated. Anne's previous betrothal to the Duke of Lorraine's son Francis provided further grounds for the annulment. The marriage was subsequently dissolved, and Anne received the title of "The King's Sister", two houses and a generous allowance. It was soon clear that Henry had fallen for the 17-year-old Catherine Howard, the Duke of Norfolk's niece, the politics of which worried Cromwell, for Norfolk was a political opponent.

Shortly after, the religious reformers (and protégés of Cromwell) Robert Barnes, William Jerome and Thomas Garret were burned as heretics. Cromwell, meanwhile, fell out of favour although it is unclear exactly why, for there is little evidence of differences of domestic or foreign policy. Despite his role, he was never formally accused of being responsible for Henry's failed marriage. Cromwell was now surrounded by enemies at court, with Norfolk also able to draw on his niece's position. Cromwell was charged with treason, selling export licences, granting passports, and drawing up commissions without permission, and may also have been blamed for the failure of the foreign policy that accompanied the attempted marriage to Anne. He was subsequently attainted and beheaded.

On 28 July 1540 (the same day Cromwell was executed), Henry married the young Catherine Howard, a first cousin and lady-in-waiting of Anne Boleyn. He was absolutely delighted with his new queen, and awarded her the lands of Cromwell and a vast array of jewellery. Soon after the marriage, however, Queen Catherine had an affair with the courtier Thomas Culpeper. She also employed Francis Dereham, who had previously been informally engaged to her and had an affair with her prior to her marriage, as her secretary. The court was informed of her affair with Dereham whilst Henry was away; they dispatched Thomas Cranmer to investigate, who brought evidence of Queen Catherine's previous affair with Dereham to the king's notice. Though Henry originally refused to believe the allegations, Dereham confessed. It took another meeting of the council, however, before Henry believed the accusations against Dereham and went into a rage, blaming the council before consoling himself in hunting. When questioned, the queen could have admitted a prior contract to marry Dereham, which would have made her subsequent marriage to Henry invalid, but she instead claimed that Dereham had forced her to enter into an adulterous relationship. Dereham, meanwhile, exposed Queen Catherine's relationship with Culpeper. Culpeper and Dereham were both executed, and Catherine too was beheaded on 13 February 1542.

In 1538, the chief minister Thomas Cromwell pursued an extensive campaign against what his government termed "idolatry" practiced under the old religion, culminating in September with the dismantling of the shrine of St. Thomas Becket at Canterbury. As a consequence, the king was excommunicated by Pope Paul III on 17 December of the same year. In 1540, Henry sanctioned the complete destruction of shrines to saints. In 1542, England's remaining monasteries were all dissolved, and their property transferred to the Crown. Abbots and priors lost their seats in the House of Lords; only archbishops and bishops remained. Consequently, the Lords Spiritual—as members of the clergy with seats in the House of Lords were known—were for the first time outnumbered by the Lords Temporal.

The 1539 alliance between Francis and Charles had soured, eventually degenerating into renewed war. With Catherine of Aragon and Anne Boleyn dead, relations between Charles and Henry improved considerably, and Henry concluded a secret alliance with the Emperor and decided to enter the Italian War in favour of his new ally. An invasion of France was planned for 1543. In preparation for it, Henry moved to eliminate the potential threat of Scotland under the youthful James V. The Scots were defeated at Battle of Solway Moss on 24 November 1542, and James died on 15 December. Henry now hoped to unite the crowns of England and Scotland by marrying his son Edward to James' successor, Mary. The Scottish Regent Lord Arran agreed to the marriage in the Treaty of Greenwich on 1 July 1543, but it was rejected by the Parliament of Scotland on 11 December. The result was eight years of war between England and Scotland, a campaign later dubbed "the Rough Wooing". Despite several peace treaties, unrest continued in Scotland until Henry's death.

Despite the early success with Scotland, Henry hesitated to invade France, annoying Charles. Henry finally went to France in June 1544 with a two-pronged attack. One force under Norfolk ineffectively besieged Montreuil. The other, under Suffolk, laid siege to Boulogne. Henry later took personal command, and Boulogne fell on 18 September 1544. However, Henry had refused Charles' request to march against Paris. Charles' own campaign fizzled, and he made peace with France that same day. Henry was left alone against France, unable to make peace. Francis attempted to invade England in the summer of 1545, but reached only the Isle of Wight before being repulsed in the Battle of the Solent. Out of money, France and England signed the Treaty of Camp on 7 June 1546. Henry secured Boulogne for eight years. The city was then to be returned to France for 2 million crowns (£750,000). Henry needed the money; the 1544 campaign had cost £650,000, and England was once again bankrupt.

Henry married his last wife, the wealthy widow Catherine Parr, in July 1543. A reformer at heart, she argued with Henry over religion. Ultimately, Henry remained committed to an idiosyncratic mixture of Catholicism and Protestantism; the reactionary mood which had gained ground following the fall of Cromwell had neither eliminated his Protestant streak nor been overcome by it. Parr helped reconcile Henry with his daughters, Mary and Elizabeth. In 1543, the Third Succession Act put them back in the line of succession after Edward. The same act allowed Henry to determine further succession to the throne in his will.

Late in life, Henry became obese, with a waist measurement of , and had to be moved about with the help of mechanical inventions. He was covered with painful, pus-filled boils and possibly suffered from gout. His obesity and other medical problems can be traced to the jousting accident in 1536 in which he suffered a leg wound. The accident re-opened and aggravated a previous injury he had sustained years earlier, to the extent that his doctors found it difficult to treat. The chronic wound festered for the remainder of his life and became ulcerated, thus preventing him from maintaining the level of physical activity he had previously enjoyed. The jousting accident is also believed to have caused Henry's mood swings, which may have had a dramatic effect on his personality and temperament.

The theory that Henry suffered from syphilis has been dismissed by most historians. Historian Susan Maclean Kybett ascribes his demise to scurvy, which is caused by a lack of fresh fruits and vegetables. Alternatively, his wives' pattern of pregnancies and his mental deterioration have led some to suggest that the king may have been Kell positive and suffered from McLeod syndrome. According to another study, Henry VIII's history and body morphology may have been the result of traumatic brain injury after his 1536 jousting accident, which in turn led to a neuroendocrine cause of his obesity. This analysis identifies growth hormone deficiency (GHD) as the source for his increased adiposity but also significant behavioural changes noted in his later years, including his multiple marriages.

Henry's obesity hastened his death at the age of 55, which occurred on 28 January 1547 in the Palace of Whitehall, on what would have been his father's 90th birthday. The tomb he had planned (with components taken from the tomb intended for Cardinal Wolsey) was only partly constructed and would never be completed. (The sarcophagus and its base were later removed and used for Lord Nelson's tomb in the crypt of St. Paul's Cathedral.) Henry was interred in a vault at St George's Chapel, Windsor Castle, next to Jane Seymour. Over a hundred years later, King Charles I (1625–1649) was buried in the same vault.

English historian and House of Tudor expert David Starkey describes Henry VIII as a husband:What is extraordinary is that Henry was usually a very good husband. And he liked women -- that's why he married so many of them! He was very tender to them, we know that he addressed them as "sweetheart." He was a good lover, he was very generous: the wives were given huge settlements of land and jewels -- they were loaded with jewels. He was immensely considerate when they were pregnant. But, once he had fallen out of love... he just cut them off. He just withdrew. He abandoned them. They didn't even know he'd left them.

Upon Henry's death, he was succeeded by his son Edward VI. Since Edward was then only nine years old, he could not rule directly. Instead, Henry's will designated 16 executors to serve on a council of regency until Edward reached the age of 18. The executors chose Edward Seymour, 1st Earl of Hertford, Jane Seymour's elder brother, to be Lord Protector of the Realm. If Edward died childless, the throne was to pass to Mary, Henry VIII's daughter by Catherine of Aragon, and her heirs. If Mary's issue failed, the crown was to go to Elizabeth, Henry's daughter by Anne Boleyn, and her heirs. Finally, if Elizabeth's line became extinct, the crown was to be inherited by the descendants of Henry VIII's deceased younger sister, Mary, the Greys. The descendants of Henry's sister Margaret – the Stuarts, rulers of Scotland – were thereby excluded from the succession. This final provision failed when James VI of Scotland became King of England in 1603.

Henry cultivated the image of a Renaissance man, and his court was a centre of scholarly and artistic innovation and glamorous excess, epitomised by the Field of the Cloth of Gold. He scouted the country for choirboys, taking some directly from Wolsey's choir, and introduced Renaissance music into court. Musicians included Benedict de Opitiis, Richard Sampson, Ambrose Lupo, and Venetian organist Dionisio Memo, and Henry himself kept a considerable collection of instruments. He was skilled on the lute and could play the organ, and he was a talented player of the virginals. He could also sight read music and sing well. He was an accomplished musician, author, and poet; his best known piece of music is "Pastime with Good Company" ("The Kynges Ballade"), and he is reputed to have written "Greensleeves" but probably did not.

Henry was an avid gambler and dice player, and he excelled at sports, especially jousting, hunting, and real tennis. He was also known for his strong defence of conventional Christian piety. He was involved in the construction and improvement of several significant buildings, including Nonsuch Palace, King's College Chapel, Cambridge, and Westminster Abbey in London. Many of the existing buildings which he improved were properties confiscated from Wolsey, such as Christ Church, Oxford, Hampton Court Palace, the Palace of Whitehall, and Trinity College, Cambridge.

Henry was an intellectual, the first English king with a modern humanist education. He read and wrote English, French, and Latin, and owned a large library. He annotated many books and published one of his own, and he had numerous pamphlets and lectures prepared to support the reformation of the church. Richard Sampson's "Oratio" (1534), for example, was an argument for absolute obedience to the monarchy and claimed that the English church had always been independent from Rome. At the popular level, theatre and minstrel troupes funded by the crown travelled around the land to promote the new religious practices; the pope and Catholic priests and monks were mocked as foreign devils, while the glorious king was hailed as a brave and heroic defender of the true faith. Henry worked hard to present an image of unchallengeable authority and irresistible power.

Henry was a large, well-built athlete, over tall, strong, and broad in proportion, and he excelled at jousting and hunting. These were more than pastimes; they were political devices which served multiple goals, enhancing his athletic royal image, impressing foreign emissaries and rulers, and conveying his ability to suppress any rebellion. He arranged a jousting tournament at Greenwich in 1517 where he wore gilded armour and gilded horse trappings, and outfits of velvet, satin, and cloth of gold with pearls and jewels. It suitably impressed foreign ambassadors, one of whom wrote home that "the wealth and civilisation of the world are here, and those who call the English barbarians appear to me to render themselves such". Henry finally retired from jousting in 1536 after a heavy fall from his horse left him unconscious for two hours, but he continued to sponsor two lavish tournaments a year. He then started adding weight and lost the trim, athletic figure that had made him so handsome, and his courtiers began dressing in heavily padded clothes to emulate and flatter him. His health rapidly declined near the end of his reign.

The power of Tudor monarchs, including Henry, was 'whole' and 'entire', ruling, as they claimed, by the grace of God alone. The crown could also rely on the exclusive use of those functions that constituted the royal prerogative. These included acts of diplomacy (including royal marriages), declarations of war, management of the coinage, the issue of royal pardons and the power to summon and dissolve parliament as and when required. Nevertheless, as evident during Henry's break with Rome, the monarch worked within established limits, whether legal or financial, that forced him to work closely with both the nobility and parliament (representing the gentry).
In practice, Tudor monarchs used patronage to maintain a royal court that included formal institutions such as the Privy Council as well as more informal advisers and confidants. Both the rise and fall of court nobles could be swift: although the often-quoted figure of 72,000 executions of thieves during the last two years of his reign is inflated, Henry did undoubtedly execute at will, burning or beheading two of his wives, twenty peers, four leading public servants, six close attendants and friends, one cardinal (John Fisher) and numerous abbots. Among those who were in favour at any given point in Henry's reign, one could usually be identified as a chief minister, though one of the enduring debates in the historiography of the period has been the extent to which those chief ministers controlled Henry rather than vice versa. In particular, historian G. R. Elton has argued that one such minister, Thomas Cromwell, led a "Tudor revolution in government" quite independent of the king, whom Elton presented as an opportunistic, essentially lazy participant in the nitty-gritty of politics. Where Henry did intervene personally in the running of the country, Elton argued, he mostly did so to its detriment. The prominence and influence of faction in Henry's court is similarly discussed in the context of at least five episodes of Henry's reign, including the downfall of Anne Boleyn.

From 1514 to 1529, Thomas Wolsey (1473–1530), a cardinal of the established Church, oversaw domestic and foreign policy for the young king from his position as Lord Chancellor. Wolsey centralised the national government and extended the jurisdiction of the conciliar courts, particularly the Star Chamber. The Star Chamber's overall structure remained unchanged, but Wolsey used it to provide for much-needed reform of the criminal law. The power of the court itself did not outlive Wolsey, however, since no serious administrative reform was undertaken and its role was eventually devolved to the localities. Wolsey helped fill the gap left by Henry's declining participation in government (particularly in comparison to his father) but did so mostly by imposing himself in the King's place. His use of these courts to pursue personal grievances, and particularly to treat delinquents as if mere examples of a whole class worthy of punishment, angered the rich, who were annoyed as well by his enormous wealth and ostentatious living. Following Wolsey's downfall, Henry took full control of his government, although at court numerous complex factions continued to try to ruin and destroy each other.

Thomas Cromwell (c. 1485–1540) also came to define Henry's government. Returning to England from the continent in 1514 or 1515, Cromwell soon entered Wolsey's service. He turned to law, also picking up a good knowledge of the Bible, and was admitted to Gray's Inn in 1524. He became Wolsey's "man of all work". Cromwell, driven in part by his religious beliefs, attempted to reform the body politic of the English government through discussion and consent, and through the vehicle of continuity and not outward change. He was seen by many people as the man they wanted to bring about their shared aims, including Thomas Audley. By 1531, Cromwell and those associated with him were already responsible for the drafting of much legislation. Cromwell's first office was that of the master of the King's jewels in 1532, from which he began to invigorate the government finances. By this point, Cromwell's power as an efficient administrator, in a Council full of politicians, exceeded what Wolsey had achieved.

Cromwell did much work through his many offices to remove the tasks of government from the Royal Household (and ideologically from the personal body of the King) and into a public state. He did so, however, in a haphazard fashion that left several remnants, not least because he needed to retain Henry's support, his own power, and the possibility of actually achieving the plan he set out. Cromwell made the various income streams put in place by Henry VII more formal and assigned largely autonomous bodies for their administration. The role of the King's Council was transferred to a reformed Privy Council, much smaller and more efficient than its predecessor. A difference emerged between the financial health of the king, and that of the country, although Cromwell's fall undermined much of his bureaucracy, which required his hand to keep order among the many new bodies and prevent profligate spending that strained relations as well as finances. Cromwell's reforms ground to a halt in 1539, the initiative lost, and he failed to secure the passage of an enabling act, the Proclamation by the Crown Act 1539. He too was executed, on 28 July 1540.

Henry inherited a vast fortune and a prosperous economy from his father Henry VII, who had been frugal and careful with money. This fortune was estimated to be £1,250,000 (£375 million by today's standards). By comparison, however, the reign of Henry was a near-disaster in financial terms. Although he further augmented his royal treasury through the seizure of church lands, Henry's heavy spending and long periods of mismanagement damaged the economy.

Much of this wealth was spent by Henry on maintaining his court and household, including many of the building works he undertook on royal palaces. Henry hung 2,000 tapestries in his palaces; by comparison, James V of Scotland hung just 200. Henry took pride in showing off his collection of weapons, which included exotic archery equipment, 2,250 pieces of land ordnance and 6,500 handguns. Tudor monarchs had to fund all the expenses of government out of their own income. This income came from the Crown lands that Henry owned as well as from customs duties like tonnage and poundage, granted by parliament to the king for life. During Henry's reign the revenues of the Crown remained constant (around £100,000), but were eroded by inflation and rising prices brought about by war. Indeed, war and Henry's dynastic ambitions in Europe exhausted the surplus he had inherited from his father by the mid-1520s.

Whereas Henry VII had not involved Parliament in his affairs very much, Henry VIII had to turn to Parliament during his reign for money, in particular for grants of subsidies to fund his wars. The Dissolution of the Monasteries provided a means to replenish the treasury, and as a result the Crown took possession of monastic lands worth £120,000 (£36 million) a year. The Crown had profited a small amount in 1526 when Wolsey had put England onto a gold, rather than silver, standard, and had debased the currency slightly. Cromwell debased the currency more significantly, starting in Ireland in 1540. The English pound halved in value against the Flemish pound between 1540 and 1551 as a result. The nominal profit made was significant, helping to bring income and expenditure together, but it had a catastrophic effect on the overall economy of the country. In part, it helped to bring about a period of very high inflation from 1544 onwards.

Henry is generally credited with initiating the English Reformation – the process of transforming England from a Catholic country to a Protestant one – though his progress at the elite and mass levels is disputed, and the precise narrative not widely agreed. Certainly, in 1527, Henry, until then an observant and well-informed Catholic, appealed to the Pope for an annulment of his marriage to Catherine. No annulment was immediately forthcoming, since the papacy was now under the control of Charles V who was Catherine' s nephew. The traditional narrative gives this refusal as the trigger for Henry's rejection of papal supremacy (which he had previously defended). Yet as E.L.Woodward put it, Henry's determination to divorce Catherine was the occasion rather than the cause of the English Reformation so that "neither too much nor too little must be made of this divorce." Historian A. F. Pollard has also argued that even if Henry had not needed an annulment, he may have come to reject papal control over the governance of England purely for political reasons. Indeed, Henry needed a son to secure the Tudor Dynasty and avert the risk of civil war over disputed succession.

In any case, between 1532 and 1537, Henry instituted a number of statutes that dealt with the relationship between king and pope and hence the structure of the nascent Church of England. These included the Statute in Restraint of Appeals (passed 1533), which extended the charge of "praemunire" against all who introduced papal bulls into England, potentially exposing them to the death penalty if found guilty. Other acts included the Supplication against the Ordinaries and the Submission of the Clergy, which recognised Royal Supremacy over the church. The Ecclesiastical Appointments Act 1534 required the clergy to elect bishops nominated by the Sovereign. The Act of Supremacy in 1534 declared that the King was "the only Supreme Head on Earth of the Church of England" and the Treasons Act 1534 made it high treason, punishable by death, to refuse the Oath of Supremacy acknowledging the King as such. Similarly, following the passage of the Act of Succession 1533, all adults in the Kingdom were required to acknowledge the Act's provisions (declaring Henry's marriage to Anne legitimate and his marriage to Catherine illegitimate) by oath; those who refused were subject to imprisonment for life, and any publisher or printer of any literature alleging that the marriage to Anne was invalid subject to the death penalty. Finally, the Peter's Pence Act was passed, and it reiterated that England had "no superior under God, but only your Grace" and that Henry's "imperial crown" had been diminished by "the unreasonable and uncharitable usurpations and exactions" of the Pope. The King had much support from the Church under Cranmer.

Henry, to Thomas Cromwell's annoyance, insisted on parliamentary time to discuss questions of faith, which he achieved through the Duke of Norfolk. This led to the passing of the Act of Six Articles, whereby six major questions were all answered by asserting the religious orthodoxy, thus restraining the reform movement in England. It was followed by the beginnings of a reformed liturgy and of the Book of Common Prayer, which would take until 1549 to complete. The victory won by religious conservatives did not convert into much change in personnel, however, and Cranmer remained in his position. Overall, the rest of Henry's reign saw a subtle movement away from religious orthodoxy, helped in part by the deaths of prominent figures from before the break with Rome, especially the executions of Thomas More and John Fisher in 1535 for refusing to renounce papal authority. Henry established a new political theology of obedience to the crown that was continued for the next decade. It reflected Martin Luther's new interpretation of the fourth commandment ("Honour thy father and mother"), brought to England by William Tyndale. The founding of royal authority on the Ten Commandments was another important shift: reformers within the Church used the Commandments' emphasis on faith and the word of God, while conservatives emphasised the need for dedication to God and doing good. The reformers' efforts lay behind the publication of the "Great Bible" in 1539 in English. Protestant Reformers still faced persecution, particularly over objections to Henry's annulment. Many fled abroad, including the influential Tyndale, who was eventually executed and his body burned at Henry's behest.

When taxes once payable to Rome were transferred to the Crown, Cromwell saw the need to assess the taxable value of the Church's extensive holdings as they stood in 1535. The result was an extensive compendium, the "Valor Ecclesiasticus". In September of the same year, Cromwell commissioned a more general visitation of religious institutions, to be undertaken by four appointee visitors. The visitation focussed almost exclusively on the country's religious houses, with largely negative conclusions. In addition to reporting back to Cromwell, the visitors made the lives of the monks more difficult by enforcing strict behavioural standards. The result was to encourage self-dissolution. In any case, the evidence gathered by Cromwell led swiftly to the beginning of the state-enforced dissolution of the monasteries with all religious houses worth less than £200 vested by statute in the crown in January 1536. After a short pause, surviving religious houses were transferred one by one to the Crown and onto new owners, and the dissolution confirmed by a further statute in 1539. By January 1540 no such houses remained: some 800 had been dissolved. The process had been efficient, with minimal resistance, and brought the crown some £90,000 a year. The extent to which the dissolution of all houses was planned from the start is debated by historians; there is some evidence that major houses were originally intended only to be reformed. Cromwell's actions transferred a fifth of England's landed wealth to new hands. The programme was designed primarily to create a landed gentry beholden to the crown, which would use the lands much more efficiently. Although little opposition to the supremacy could be found in England's religious houses, they had links to the international church and were an obstacle to further religious reform.

Response to the reforms was mixed. The religious houses had been the only support of the impoverished, and the reforms alienated much of the population outside London, helping to provoke the great northern rising of 1536–1537, known as the Pilgrimage of Grace. Elsewhere the changes were accepted and welcomed, and those who clung to Catholic rites kept quiet or moved in secrecy. They would re-emerge during the reign of Henry's daughter Mary (1553–1558).

Apart from permanent garrisons at Berwick, Calais, and Carlisle, England's standing army numbered only a few hundred men. This was increased only slightly by Henry. Henry's invasion force of 1513, some 30,000 men, was composed of billmen and longbowmen, at a time when the other European nations were moving to hand guns and pikemen. The difference in capability was at this stage not significant, however, and Henry's forces had new armour and weaponry. They were also supported by battlefield artillery and the war wagon, relatively new innovations, and several large and expensive siege guns. The invasion force of 1544 was similarly well-equipped and organised, although command on the battlefield was laid with the dukes of Suffolk and Norfolk, which in the case of the latter produced disastrous results at Montreuil.

Henry's break with Rome incurred the threat of a large-scale French or Spanish invasion. To guard against this, in 1538, he began to build a chain of expensive, state-of-the-art defences, along Britain's southern and eastern coasts from Kent to Cornwall, largely built of material gained from the demolition of the monasteries. These were known as Henry VIII's Device Forts. He also strengthened existing coastal defence fortresses such as Dover Castle and, at Dover, Moat Bulwark and Archcliffe Fort, which he personally visited for a few months to supervise. Wolsey had many years before conducted the censuses required for an overhaul of the system of militia, but no reform resulted. In 1538–39, Cromwell overhauled the shire musters, but his work mainly served to demonstrate how inadequate they were in organisation. The building works, including that at Berwick, along with the reform of the militias and musters, were eventually finished under Queen Mary.
Henry is traditionally cited as one of the founders of the Royal Navy. Technologically, Henry invested in large cannon for his warships, an idea that had taken hold in other countries, to replace the smaller serpentines in use. He also flirted with designing ships personally – although his contribution to larger vessels, if any, is not known, it is believed that he influenced the design of rowbarges and similar galleys. Henry was also responsible for the creation of a permanent navy, with the supporting anchorages and dockyards. Tactically, Henry's reign saw the Navy move away from boarding tactics to employ gunnery instead. The Tudor navy was enlarged up to fifty ships (the "Mary Rose" was one of them), and Henry was responsible for the establishment of the "council for marine causes" to specifically oversee all the maintenance and operation of the Navy, becoming the basis for the later Admiralty.

At the beginning of Henry's reign, Ireland was effectively divided into three zones: the Pale, where English rule was unchallenged; Leinster and Munster, the so-called "obedient land" of Anglo-Irish peers; and the Gaelic Connaught and Ulster, with merely nominal English rule. Until 1513, Henry continued the policy of his father, to allow Irish lords to rule in the king's name and accept steep divisions between the communities. However, upon the death of the 8th Earl of Kildare, governor of Ireland, fractious Irish politics combined with a more ambitious Henry to cause trouble. When Thomas Butler, 7th Earl of Ormond died, Henry recognised one successor for Ormond's English, Welsh and Scottish lands, whilst in Ireland another took control. Kildare's successor, the 9th Earl, was replaced as Lord Lieutenant of Ireland by Thomas Howard, Earl of Surrey in 1520. Surrey's ambitious aims were costly, but ineffective; English rule became trapped between winning the Irish lords over with diplomacy, as favoured by Henry and Wolsey, and a sweeping military occupation as proposed by Surrey. Surrey was recalled in 1521, with Piers Butler – one of claimants to the Earldom of Ormond – appointed in his place. Butler proved unable to control opposition, including that of Kildare. Kildare was appointed chief governor in 1524, resuming his dispute with Butler, which had before been in a lull. Meanwhile, the Earl of Desmond, an Anglo-Irish peer, had turned his support to Richard de la Pole as pretender to the English throne; when in 1528 Kildare failed to take suitable actions against him, Kildare was once again removed from his post.

The Desmond situation was resolved on his death in 1529, which was followed by a period of uncertainty. This was effectively ended with the appointment of Henry FitzRoy, Duke of Richmond and the king's son, as lord lieutenant. Richmond had never before visited Ireland, his appointment a break with past policy. For a time it looked as if peace might be restored with the return of Kildare to Ireland to manage the tribes, but the effect was limited and the Irish parliament soon rendered ineffective. Ireland began to receive the attention of Cromwell, who had supporters of Ormond and Desmond promoted. Kildare, on the other hand, was summoned to London; after some hesitation, he departed for London in 1534, where he would face charges of treason. His son, Thomas, Lord Offaly was more forthright, denouncing the king and leading a "Catholic crusade" against the king, who was by this time mired in marital problems. Offaly had the Archbishop of Dublin murdered, and besieged Dublin. Offaly led a mixture of Pale gentry and Irish tribes, although he failed to secure the support of Lord Darcy, a sympathiser, or Charles V. What was effectively a civil war was ended with the intervention of 2,000 English troops – a large army by Irish standards – and the execution of Offaly (his father was already dead) and his uncles.

Although the Offaly revolt was followed by a determination to rule Ireland more closely, Henry was wary of drawn-out conflict with the tribes, and a royal commission recommended that the only relationship with the tribes was to be promises of peace, their land protected from English expansion. The man to lead this effort was Sir Antony St Leger, as Lord Deputy of Ireland, who would remain into the post past Henry's death. Until the break with Rome, it was widely believed that Ireland was a Papal possession granted as a mere fiefdom to the English king, so in 1541 Henry asserted England's claim to the Kingdom of Ireland free from the Papal overlordship. This change did, however, also allow a policy of peaceful reconciliation and expansion: the Lords of Ireland would grant their lands to the King, before being returned as fiefdoms. The incentive to comply with Henry's request was an accompanying barony, and thus a right to sit in the Irish House of Lords, which was to run in parallel with England's. The Irish law of the tribes did not suit such an arrangement, because the chieftain did not have the required rights; this made progress tortuous, and the plan was abandoned in 1543, not to be replaced.

The complexities and sheer scale of Henry's legacy ensured that, in the words of Betteridge and Freeman, "throughout the centuries, Henry has been praised and reviled, but he has never been ignored". Historian J.D. Mackie sums up Henry's personality and its impact on his achievements and popularity:

A particular focus of modern historiography has been the extent to which the events of Henry's life (including his marriages, foreign policy and religious changes) were the result of his own initiative and, if they were, whether they were the result of opportunism or of a principled undertaking by Henry. The traditional interpretation of those events was provided by historian A.F. Pollard, who in 1902 presented his own, largely positive, view of the king, lauding him, "as the king and statesman who, whatever his personal failings, led England down the road to parliamentary democracy and empire". Pollard's interpretation remained the dominant interpretation of Henry's life until the publication of the doctoral thesis of G. R. Elton in 1953.

Elton's book on "The Tudor Revolution in Government", maintained Pollard's positive interpretation of the Henrician period as a whole, but reinterpreted Henry himself as a follower rather than a leader. For Elton, it was Cromwell and not Henry who undertook the changes in government – Henry was shrewd, but lacked the vision to follow a complex plan through. Henry was little more, in other words, than an "ego-centric monstrosity" whose reign "owed its successes and virtues to better and greater men about him; most of its horrors and failures sprang more directly from [the king]".

Although the central tenets of Elton's thesis have since been questioned, it has consistently provided the starting point for much later work, including that of J. J. Scarisbrick, his student. Scarisbrick largely kept Elton's regard for Cromwell's abilities, but returned agency to Henry, who Scarisbrick considered to have ultimately directed and shaped policy. For Scarisbrick, Henry was a formidable, captivating man who "wore regality with a splendid conviction". The effect of endowing Henry with this ability, however, was largely negative in Scarisbrick's eyes: to Scarisbrick the Henrician period was one of upheaval and destruction and those in charge worthy of blame more than praise. Even among more recent biographers, including David Loades, David Starkey and John Guy, there has ultimately been little consensus on the extent to which Henry was responsible for the changes he oversaw or the correct assessment of those he did bring about.

This lack of clarity about Henry's control over events has contributed to the variation in the qualities ascribed to him: religious conservative or dangerous radical; lover of beauty or brutal destroyer of priceless artefacts; friend and patron or betrayer of those around him; chivalry incarnate or ruthless chauvinist. One traditional approach, favoured by Starkey and others, is to divide Henry's reign into two halves, the first Henry being dominated by positive qualities (politically inclusive, pious, athletic but also intellectual) who presided over a period of stability and calm, and the latter a "hulking tyrant" who presided over a period of dramatic, sometimes whimsical, change. Other writers have tried to merge Henry's disparate personality into a single whole; Lacey Baldwin Smith, for example, considered him an egotistical borderline neurotic given to great fits of temper and deep and dangerous suspicions, with a mechanical and conventional, but deeply held piety, and having at best a mediocre intellect.

Many changes were made to the royal style during his reign. Henry originally used the style "Henry the Eighth, by the Grace of God, King of England, France and Lord of Ireland". In 1521, pursuant to a grant from Pope Leo X rewarding Henry for his "Defence of the Seven Sacraments", the royal style became "Henry the Eighth, by the Grace of God, King of England and France, Defender of the Faith and Lord of Ireland". Following Henry's excommunication, Pope Paul III rescinded the grant of the title "Defender of the Faith", but an Act of Parliament (35 Hen 8 c 3) declared that it remained valid; and it continues in royal usage to the present day, as evidenced by the letters FID DEF or F.D. on all British coinage. Henry's motto was "Coeur Loyal" ("true heart"), and he had this embroidered on his clothes in the form of a heart symbol and with the word "loyal". His emblem was the Tudor rose and the Beaufort portcullis. As king, Henry's arms were the same as those used by his predecessors since Henry IV: "Quarterly, Azure three fleurs-de-lys Or (for France) and Gules three lions passant guardant in pale Or (for England)".

In 1535, Henry added the "supremacy phrase" to the royal style, which became "Henry the Eighth, by the Grace of God, King of England and France, Defender of the Faith, Lord of Ireland and of the Church of England in Earth Supreme Head". In 1536, the phrase "of the Church of England" changed to "of the Church of England and also of Ireland". In 1541, Henry had the Irish Parliament change the title "Lord of Ireland" to "King of Ireland" with the Crown of Ireland Act 1542, after being advised that many Irish people regarded the Pope as the true head of their country, with the Lord acting as a mere representative. The reason the Irish regarded the Pope as their overlord was that Ireland had originally been given to King Henry II of England by Pope Adrian IV in the 12th century as a feudal territory under papal overlordship. The meeting of Irish Parliament that proclaimed Henry VIII as King of Ireland was the first meeting attended by the Gaelic Irish chieftains as well as the Anglo-Irish aristocrats. The style "Henry the Eighth, by the Grace of God, King of England, France and Ireland, Defender of the Faith and of the Church of England and also of Ireland in Earth Supreme Head" remained in use until the end of Henry's reign.







</doc>
<doc id="14189" url="https://en.wikipedia.org/wiki?curid=14189" title="Haryana">
Haryana

Haryana () is one of the 28 states in India, located in the northern part of the country. It was carved out of the former state of East Punjab on 1 November 1966 on a linguistic basis. It is ranked 22nd in terms of area, with less than 1.4% () of India's land area. Chandigarh is the state capital, Faridabad in National Capital Region is the most populous city of the state, and Gurugram is a leading financial hub of the NCR, with major Fortune 500 companies located in it. Haryana has 6 administrative divisions, 22 districts, 72 sub-divisions, 93 revenue tehsils, 50 sub-tehsils, 140 community development blocks, 154 cities and towns, 6,848 villages, and 6222 villages panchayats.

As the largest recipient of investment per capita since 2000 in India, Haryana has the fifth highest per capita income among Indian states and territories, more than double the national average for year 2018–19. Haryana's state GSDP is 12th largest in India and grew at 12.96% between 2012 and 2017. There are by 30 special economic zones (SEZs), mainly located within the industrial corridor projects connecting the National Capital Region (NCR). Faridabad has been described as eighth fastest growing city in the world and third most in India. In services, Gurugram ranks number 1 in India in IT growth rate and existing technology infrastructure, and number 2 in startup ecosystem, innovation and livability. Haryana is the 7th highest among Indian states by human development index ranking.

Among the world's oldest and largest ancient civilisations, the Indus Valley Civilization sites at Rakhigarhi village in Hisar district and Bhirrana in Fatehabad district are 9,000 years old. Rich in history, monuments, heritage, flora and fauna, human resources and tourism with well developed economy, national highways and state roads, it is bordered by Himachal Pradesh to the north-east, by river Yamuna along its eastern border with Uttar Pradesh, by Rajasthan to the west and south, and Ghaggar-Hakra River flows along its northern border with Punjab. Since Haryana surrounds the country's capital Delhi on three sides (north, west and south), consequently a large area of Haryana is included in the economically-important National Capital Region for the purposes of planning and development.

The name Haryana is found in the works of the 12th-century AD Apabhramsha writer Vibudh Shridhar (VS 1189–1230). The name Haryana has been derived from the Sanskrit words "Hari" (the Hindu god Vishnu) and "ayana" (home), meaning "the Abode of God". However, scholars such as Muni Lal, Murli Chand Sharma, HA Phadke and Sukhdev Singh Chib believe that the name comes from a compound of the words "Hari" (Sanskrit "Harit", "green") and "Aranya" (forest).

The villages of Rakhigarhi in Hisar district and Bhirrana in Fatehabad district are home to the largest and one of the world's oldest ancient Indus Valley Civilization sites, dated at over 9,000 years old. Evidence of paved roads, a drainage system, a large-scale rainwater collection storage system, terracotta brick and statue production, and skilled metal working (in both bronze and precious metals) have been uncovered. According to archaeologists, Rakhigarhi may be the origin of Harappan civilisation, which arose in the Ghaggar basin in Haryana and gradually and slowly moved to the Indus valley.

During the Vedic era, Haryana was the site of the Kuru Kingdom, one of India's great Mahajanapadas.
The south of Haryana is the claimed location of the Vedic Brahmavarta region.

Ancient bronze and stone idols of Jain Tirthankara were found in archaeological expeditions in Badli, Bhiwani (Ranila, Charkhi Dadri and Badhra), Dadri, Gurgaon (Ferozepur Jhirka), Hansi, Hisar (Agroha), Kasan, Nahad, Narnaul, Pehowa, Rewari, Rohad, Rohtak (Asthal Bohar) and Sonepat in Haryana.

Pushyabhuti dynasty ruled parts of northern India in the 7th century with its capital at Thanesar. Harsha was a prominent king of the dynasty. Tomara dynasty ruled the south Haryana region in the 10th century. Anangpal Tomar was a prominent king among the Tomaras.

After the sack of Bhatner fort during the Timurid conquests of India in 1398, Timur attacked and sacked the cities of Sirsa, Fatehabad, Sunam, Kaithal and Panipat. When he reached the town of Sarsuti (Sirsa), the residents, who were mostly non-Muslims, fled and were chased by a detachment of Timur's troops, with thousands of them being killed and looted by the troops. From there he travelled to Fatehabad, whose residents fled and a large number of those remaining in the town were massacred. The Ahirs resisted him at Ahruni but were defeated, with thousands being killed and many being taken prisoners while the town was burnt to ashes. From there he travelled to Tohana, whose Jat inhabitants were stated to be robbers according to Sharaf ad-Din Ali Yazdi. They tried to resist but were defeated and fled. Timur's army pursued and killed 200 Jats, while taking many more as prisoners. He then sent a detachment to chase the fleeing Jats and killed 2,000 of them while their wives and children were enslaved and their property plundered. Timur proceeded to Kaithal whose residents were massacred and plundered, destroying all villages along the way. On the next day, he came to Assandh whose residents were "fire-worshippers" according to Yazdi, and had fled to Delhi. Next, he travelled to and subdued Tughlaqpur fort and Salwan before reaching Panipat whose residents had already fled. He then marched on to Loni fort.
Hemu claimed royal status after defeating Akbar's Mughal forces on 7 October 1556 in the Battle of Delhi and assumed the ancient title of Vikramaditya.
The area that is now Haryana has been ruled by some of the major empires of India. Panipat is known for three seminal battles in the history of India. In the First Battle of Panipat (1526), Babur defeated the Lodis. In the Second Battle of Panipat (1556), Akbar defeated the local Haryanvi Hindu Emperor of Delhi, who belonged to Rewari. Hem Chandra Vikramaditya had earlier won 22 battles across India from Punjab to Bengal, defeating Mughals and Afghans. Hemu had defeated Akbar's forces twice at Agra and the Battle of Delhi in 1556 to become the last Hindu Emperor of India with a formal coronation at Purana Quila in Delhi on 7 October 1556. In the Third Battle of Panipat (1761), the Afghan king Ahmad Shah Abdali defeated the Marathas.

Haryana as a state came into existence on 1 November 1966 the Punjab Reorganisation Act (1966). The Indian government set up the Shah Commission under the chairmanship of Justice JC Shah on 23 April 1966 to divide the existing state of Punjab and determine the boundaries of the new state of Haryana after consideration of the languages spoken by the people. The commission delivered its report on 31 May 1966 whereby the then-districts of Hisar, Mahendragarh, Gurgaon, Rohtak and Karnal were to be a part of the new state of Haryana. Further, the tehsils of Jind and Narwana in the Sangrur district – along with Naraingarh, Ambala and Jagadhri – were to be included.

The commission recommended that the tehsil of Kharar, which includes Chandigarh, the state capital of Punjab, should be a part of Haryana. However Kharar was given to Punjab. The city of Chandigarh was made a union territory, serving as the capital of both Punjab and Haryana.

Bhagwat Dayal Sharma became the first Chief Minister of Haryana.

According to the 2011 census, of total 25,350,000 population of Haryana, Hindus (87.46%) constitute the majority of the state's population with Muslims (7.03%) (mainly Meos) and Sikhs (4.91%) being the largest minorities.

Muslims are mainly found in the Nuh. Haryana has the second largest Sikh population in India after Punjab, and they mostly live in the districts adjoining Punjab, such as Sirsa, Jind, Fatehabad, Kaithal, Kurukshetra, Ambala and Panchkula.

The official language of Haryana is Hindi.
Several regional languages or dialects, often subsumed under Hindi, are spoken in the state. Predominant among them is Haryanvi (also known as Bangru), whose territory encompasses the central and eastern portions of Haryana. Hindustani is spoken in the northeast, Bagri in the west, and Ahirwati, Mewati and Braj Bhasha in the south.

There are also significant numbers of speakers of Urdu and Punjabi, the latter of which was recognised in 2010 as a second official language of Haryana for government and administrative purposes. After the state's formation, Telugu was made the state's "second language" – to be taught in schools – but it was not the "second official language" for official communication. Due to a lack of students, the language ultimately stopped being taught.

There are also some speakers of several major regional languages of neighbouring states or other parts of the subcontinent, like Bengali, Bhojpuri, Marwari, Mewari, Nepali and Saraiki, as well as smaller communities of speakers of languages that are dispersed across larger regions, like Bauria, Bazigar, Gujari, Gade Lohar, Oadki, and Sansi.

Haryana has its own unique traditional folk music, folk dances, saang (folk theatre), cinema, belief system such as Jathera (ancestral worship), and arts such as Phulkari and Shisha embroidery.

Folk music and dances of Haryana are based on satisfying cultural needs of primarily agrarian and martial natures of Haryanavi tribes.

Haryanvi musical folk theatre main types are Saang, Rasa lila and Ragini. The Saang and Ragini form of theatre was popularised by Lakhmi Chand.

Haryanvi folk dances and music have fast energetic movements. Three popular categories of dance are: festive-seasonal, devotional, and ceremonial-recreational. The festive-seasonal dances and songs are Gogaji/Gugga, Holi, Phaag, Sawan, Teej. The devotional dances and songs are Chaupaiya, Holi, Manjira, Ras Leela, Raginis). The ceremonial-recreational dances and songs are of following types: legendary bravery (Kissa and Ragini of male warriors and female Satis), love and romance (Been and its variant Nāginī dance, and Ragini), ceremonial (Dhamal Dance, Ghoomar, Jhoomar (male), Khoria, Loor, and Ragini).

Haryanvi folk music is based on day to day themes and injecting earthly humor enlivens the feel of the songs. Haryanvi music takes two main forms: "Classical folk music" and "Desi Folk music" (Country Music of Haryana), and sung in the form of ballads and love, valor and bravery, harvest, happiness and pangs of parting of lovers.

Classical Haryanvi folk music is based on Indian classical music. Hindustani classical ragas, learnt in gharana parampara of guru–shishya tradition, are used to sing songs of heroic bravery (such as Alha-Khand (1163-1202 CE) about the bravery of Alha and Udal, Jaimal and Patta of Maharana Udai Singh II), Brahmas worship and festive seasonal songs (such as Teej, Holi and Phaag songs of Phalgun month near Holi). Bravery songs are sung in high pitch.

Desi Haryanvi folk music, is a form of Haryanvi music, based on Raag Bhairvi, Raag Bhairav, Raag Kafi, Raag Jaijaivanti, Raag Jhinjhoti and Raag Pahadi and used for celebrating community bonhomie to sing seasonal songs, ballads, ceremonial songs (wedding, etc.) and related religious legendary tales such as Puran Bhagat. Relationship and songs celebrating love and life are sung in medium pitch. Ceremonial and religious songs are sung in low pitch. Young girls and women usually sing entertaining and fast seasonal, love, relationship and friendship related songs such as Phagan (song for eponymous season/month), Katak (songs for the eponymous season/month), Samman (songs for the eponymous season/month), bande-bandi (male-female duet songs), sathne (songs of sharing heartfelt feelings among female friends). Older women usually sing devotional Mangal Geet (auspicious songs) and ceremonial songs such as Bhajan, Bhat (wedding gift to the mother of bride or groom by her brother), Sagai, Ban (Hindu wedding ritual where pre-wedding festivities starts), Kuan-Poojan (a custom that is performed to welcome the birth of a child by worshiping the well or source of drinking water), Sanjhi and Holi festival.

Music and dance for Haryanvi people is a great way of demolishing societal differences as folk singers are highly esteemed and they are sought after and invited for the events, ceremonies and special occasions regardless of their caste or status. These inter-caste songs are fluid in nature, and never personalised for any specific caste, and they are sung collectively by women from different strata, castes, dialects. These songs do transform fluidly in dialect, style, words, etc. This adoptive style can be seen from the adoption of tunes of Bollywood movie songs into Haryanvi songs. Despite this continuous fluid transforming nature, Haryanvi songs have a distinct style of their own as explained above.

With the coming up of a strongly socio-economic metropolitan culture in the emergence of urban Gurgaon (Gurugram) Haryana is also witnessing community participation in public arts and city beautification. Several landmarks across Gurgaon are decorated with public murals and graffiti with cultural cohesive ideologies and stand the testimony of a lived sentiment in Haryana folk.

As per a survey, 13% of males and 7.8% of females of Haryana are non vegetarian. The regional cuisine features the staples of roti, saag, vegetarian sabzi and milk products such as ghee, milk, lassi and kheer.

Haryana has a concept of 36 Jātis or communities. Castes such as Jat, Rajput, Gujjar, Saini, Pasi, Ahir, Ror, Mev, Vishnoi and Harijan are some of the notable of these 36 Jātis.

Haryana is a landlocked state in northern India. It is between 27°39' to 30°35' N latitude and between 74°28' and 77°36' E longitude. The total geographical area of the state is 4.42 m ha, which is 1.4% of the geographical area of the country. The altitude of Haryana varies between 700 and 3600 ft (200 metres to 1200 metres) above sea level. Haryana has only 4% (compared to national 21.85%) area under forests. Karoh Peak, a tall mountain peak in the Sivalik Hills range of the greater Himalayas range located near Morni Hills area of Panchkula district, is highest point in Haryana.

Haryana has four main geographical features.

The Yamuna, tributary of Ganges, flows along the state's eastern boundary.

Northern Haryana has several north-east to south-west flowing rivers originating from the Sivalik Hills of Himalayas, such as Ghaggar-Hakra (palaeochannel of vedic Sarasvati river), Chautang (paleochannel of vedic Drishadvati river, tributary of Ghagghar), Tangri river (tributary of Ghagghar), Kaushalya river (tributary of Ghagghar), Markanda River (tributary of Ghagghar), Sarsuti, Dangri, Somb river. Haryana's main seasonal river, the Ghaggar-Hakra, known as Ghaggar before the Ottu barrage and as the Hakra downstream of the barrage, rises in the outer Himalayas, between the Yamuna and the Satluj and enters the state near Pinjore in the Panchkula district, passes through Ambala and Sirsa, it reaches Bikaner in Rajasthan and runs for before disappearing into the deserts of Rajasthan. The seasonal Markanda River, known as the "Aruna" in ancient times, originates from the lower Shivalik Hills and enters Haryana west of Ambala, and swells into a raging torrent during monsoon is notorious for its devastating power, carries its surplus water on to the Sanisa Lake where the Markanda joins the Sarasuti and later the Ghaggar.

Southern Haryana has several south-east to north-west flowing seasonal rivulets originating from the Aravalli Range in and around the hills in Mewat region, including Sahibi River (called Najafgarh drain in Delhi), Dohan river (tributary of Sahibi, originates at Mandoli village near Neem Ka Thana in Jhunjhunu district of Rajasthan and then disappears in Mahendragarh district), Krishnavati river (former tributary of Sahibi river, originates near Dariba and disappears in Mahendragarh district much before reaching Sahibi river) and Indori river (longest tributary of Sahibi River, originates in Sikar district of Rajasthan and flows to Rewari district of Haryana), these once were tributaries of the Drishadwati/Saraswati river.
Major canals are Western Yamuna Canal,

Major dams are Kaushalya Dam in Panchkula district, Hathnikund Barrage and Tajewala Barrage on Yamuna in Yamunanagar district, Pathrala barrage on Somb river in Yamunanagar district, ancient Anagpur Dam near Surajkund in Faridabad district, and Ottu barrage on Ghaggar-Hakra River in Sirsa district.

Major lakes are Dighal Wetland, Basai Wetland, Badkhal Lake in Faridabad, holy Brahma Sarovar and Sannihit Sarovar in Kurukshetra, Blue Bird Lake in Hisar, Damdama Lake at Sohna in Gurgram district, Hathni Kund in Yamunanagar district, Karna Lake at Karnal, ancient Surajkund in Faridabad, and Tilyar Lake in Rohtak.

The "Haryana State Waterbody Management Board" is responsible for rejuvenation of 14,000 Johads of Haryana and up to 60 lakes in National Capital Region falling within the Haryana state.

Only hot spring of Haryana is the Sohna Sulphur Hot Spring at Sohna in Gurugram district. Tosham Hill range has several sacred sulphur pond of religious significance that are revered for the healing impact of sulphur, such as "Pandu Teerth Kund", "Surya Kund", "Kukkar Kund", "Gyarasia Kund" or "Vyas Kund".

Seasonal waterfalls include Tikkar Taal twin lakes at Morni hiills, Dhosi Hill in Mahendragarh district and Pali village on outskirts of Faridabad.

Haryana is extremely hot in summer at around and mild in winter. The hottest months are May and June and the coldest December and January. The climate is arid to semi-arid with average rainfall of 354.5 mm. Around 29% of rainfall is received during the months from July to September, and the remaining rainfall is received during the period from December to February.

Forest cover in the state in 2013 was 3.59% (1586 km) and the Tree Cover in the state was 2.90% (1282 km), giving a total forest and tree cover of 6.49%. In 2016–17, 18,412 hectares were brought under tree cover by planting 14.1 million seedlings. Thorny, dry, deciduous forest and thorny shrubs can be found all over the state. During the monsoon, a carpet of grass covers the hills. Mulberry, eucalyptus, pine, kikar, shisham and babul are some of the trees found here. The species of fauna found in the state of Haryana include black buck, nilgai, panther, fox, mongoose, jackal and wild dog. More than 450 species of birds are found here.

Haryana has two national parks, eight wildlife sanctuaries, two wildlife conservation areas, four animal and bird breeding centers, one deer park and three zoos, all of which are managed by the Haryana Forest Department of the Government of Haryana.

Haryana Environment Protection Council is the advisory committee and Department of Environment, Haryana is the department responsible for the administration of environment. Areas of Haryana surrounding Delhi NCR are the most polluted. During smog of November 2017, Air quality index of Gurugram and Faridabad showed that the density of Fine particulates (2.5 PM diameter) was an average of 400 PM and monthly average of Haryana was 60 PM. Other sources of pollution are exhaust gases from old vehicles, stone crushers and brick kiln. Haryana has 7.5 million old vehicles, of which 40% are old more polluting vehicles, besides 500,000 new vehicles are added every year. Other majorly polluted cities are Bhiwani, Bahadurgarh, Dharuhera, Hisar and Yamunanagar.

The state is divided into 6 revenue divisions, 5 Police Ranges and 3 Police Commissionerates (c. January 2017). Six revenue divisions are: Ambala, Rohtak, Gurgaon, Hisar, Karnal and Faridabad. Haryana has 10 municipal corporations (Gurugram, Faridabad, Ambala, Panchkula, Yamunanagar, Rohtak, Hisar, Panipat, Karnal and Sonepat), 18 municipal councils and 52 municipalities (c. Jan 2018).

Within these there are 22 districts, 72 sub-divisions, 93 tehsils, 50 sub-tehsils, 140 blocks, 154 cities and towns, 6,841 villages, 6212 villages panchayats and numerous smaller dhanis.

Haryana Police force is the law enforcement agency of Haryana. Five Police Ranges are Ambala, Hissar, Karnal, Rewari and Rohtak. Three Police Commissionerates are Faridabad, Gurgaon and Panchkula. Cybercrime investigation cell is based in Gurgaon's Sector 51.

The highest judicial authority in the state is the Punjab and Haryana High Court, with next higher right of appeal to Supreme Court of India. Haryana uses an e-filing facility.

The Common Service Centres (CSCs) have been upgraded in all districts to offer hundreds of e-services to citizens, including applications of new water connection, sewer connection, electricity bill collection, ration card member registration, result of HBSE, admit cards for board examinations, online admission forms for government colleges, long route booking of buses, admission forms for Kurukshetra University and HUDA plots status inquiry. Haryana has become the first state to implement Aadhaar-enabled birth registration in all the districts. Thousands of all traditional offline state and central government services are also available 24/7 online through single unified UMANG app and portal as part of Digital India initiative.

Haryana's 14th placed 12.96% 2012-17 CAGR estimated 2017-18 GSDP of US$95 billion is split in to 52% services, 30% industries and 18% agriculture.

Services sector is split across 45% in real estate and financial and professional services, 26% trade and hospitality, 15% state and central govt employees, and 14% transport and logistics & warehousing. In IT services, Gurugram ranks number 1 in India in growth rate and existing technology infrastructure, and number 2 in startup ecosystem, innovation and livability (Nov 2016).

Industries sector is split across 69% manufacturing, 28% construction, 2% utilities and 1% mining. In industrial manufacturing, Haryana produces India's 67% of passenger cars, 60% of motorcycles, 50% of tractors and 50% of the refrigerators.

Services and industrial sectors are boosted by 7 operational SEZs and additional 23 formally approved SEZs (20 already notified and 3 in-principal approval) that are mostly spread along the Delhi–Mumbai Industrial Corridor, Amritsar Delhi Kolkata Industrial Corridor and Delhi Western Peripheral Expressway in NCR).

Agriculture sector is split across 93% crops and livestock, 4% commercial forestry and logging, and 2% fisheries. Agriculture sector of Haryana, with only less than 1.4% area of India, contributes 15% food grains to the central food security public distribution system, and 7% of total national agricultural exports including 60% of total national Basmati rice export.

Haryana is traditionally an agrarian society of zamindars (owner-cultivator farmers). The Green Revolution in Haryana of the 1960s combined with completion of Bhakra Dam in 1963 and Western Yamuna Command Network canal system in 1970s resulted in the significantly increased food grain production.

In 2015–2016, Haryana produced the following principal crops: 13,352,000 tonne wheat, 4,145,000 tonne rice, 7,169,000 tonne sugarcane, 993,000 tonne cotton and 855,000 tonne oilseeds (mustard seed, sunflower, etc.).

Vegetable production was: potato 853,806 tonnes, onion 705,795 tonnes, tomato 675,384 tonnes, cauliflower 578,953 tonnes, leafy vegetables 370,646 tonnes, brinjal 331,169 tonnes, guard 307,793 tonnes, peas 111,081 tonnes and others 269,993 tonnes.

Fruits production was: citrus 301,764 tonnes, guava 152,184 tonnes, mango 89,965 tonnes, chikoo 16,022 tonnes, aonla 12,056 tonnes and other fruits 25,848 tonnes.

Spices production was: garlic 40,497 tonnes, fenugreek 9,348 tonnes, ginger 4,304 tonnes and others 840 tonnes.

Cut flowers production was: marigold 61,830 tonnes, gladiolus 2,448,620 million, rose 1,861,160 million and other 691,300 million.

Medicinal plants production was: aloe vera 1403 tonnes and stevia 13 tonnes.

Haryana is well known for its high-yield Murrah buffalo. Other breeds of cattle native to Haryana are Haryanvi, Mewati, Sahiwal and Nili-Ravi.

To support its agrarian economy, both central government (Central Institute for Research on Buffaloes, Central Sheep Breeding Farm, National Research Centre on Equines, Central Institute of Fisheries, National Dairy Research Institute, Regional Centre for Biotechnology, Indian Institute of Wheat and Barley Research and National Bureau of Animal Genetic Resources) and state government (CCS HAU, LUVAS, Government Livestock Farm, Regional Fodder Station and Northern Region Farm Machinery Training and Testing Institute) have opened several institutes for research and education.


Haryana State has always given high priority to the expansion of electricity infrastructure, as it is one of the most important inputs for the development of the state. Haryana was the first state in the country to achieve 100% rural electrification in 1970 as well as the first in the country to link all villages with all-weather roads and provide safe drinking water facilities throughout the state.

Power in the state are:

Haryana has a total road length of , including 29 national highways, state highways, Major District Roads (MDR) and Other District Roads (ODR) (c. December 2017). A fleet of 3,864 Haryana Roadways buses covers a distance of 1.15 million km per day, and it was the first state in the country to introduce luxury video coaches.

Ancient Delhi Multan Road and Grand Trunk Road, South Asia's oldest and longest major roads, pass through Haryana. GT Road passes through the districts of Sonipat, Panipat, Karnal, Kurukshetra and Ambala in north Haryana where it enters Delhi and subsequently the industrial town of Faridabad on its way. The Kundli-Manesar-Palwal Expressway (KMP) will provide a high-speed link to northern Haryana with its southern districts such as Sonipat, Gurgaon, and Faridabad.

The Delhi-Agra Expressway (NH-2) that passes through Faridabad is being widened to six lanes from current four lanes. It will further boost Faridabad's connectivity with Delhi.

Rail network in Haryana is covered by five rail divisions under three rail zones. Diamond Quadrilateral High-speed rail network, Eastern Dedicated Freight Corridor (72 km) and Western Dedicated Freight Corridor (177 km) pass through Haryana.

Bikaner railway division of North Western Railway zone manages rail network in western and southern Haryana covering Bhatinda-Dabwali-Hanumangarh line, Rewari-Bhiwani-Hisar-Bathinda line, Hisar-Sadulpur line and Rewari-Loharu-Sadulpur line. Jaipur railway division of North Western Railway zone manages the rail network in south-west Haryana covering Rewari-Reengas-Jaipur line, Delhi-Alwar-Jaipur line and Loharu-Sikar line.

Delhi railway division of Northern Railway zone manages the rail network in north and east and central Haryana covering Delhi-Panipat-Ambala line, Delhi-Rohtak-Tohana line, Rewari–Rohtak line, Jind-Sonepat line and Delhi-Rewari line. Agra railway division of North Central Railway zone manages another very small part of the network in south-east Haryana covering Palwal-Mathura line only.

Ambala railway division of Northern Railway zone manages a small part of the rail network in north-east Haryana covering Ambala-Yamunanagar line, Ambala-Kurukshetra line and UNESCO World Heritage Kalka–Shimla Railway.

Delhi Metro connects the national capital Delhi with NCR cities of Faridabad, Gurugram and Bahadurgarh. Faridabad has the longest metro network in the NCR Region consisting of 11 stations and track length being 17 km.

The Haryana and Delhi governments have constructed the international standard Delhi Faridabad Skyway, the first of its kind in North India, to connect Delhi and Faridabad.

Haryana has a statewide network of telecommunication facilities. Haryana Government has its own statewide area network by which all government offices of 22 districts and 126 blocks across the state are connected with each other thus making it the first SWAN of the country. Bharat Sanchar Nigam Limited and most of the leading private sector players (such as Reliance Infocom, Tata Teleservices, Bharti Telecom, Idea Vodafone Essar, Aircel, Uninor and Videocon) have operations in the state. Two biggest cities of Haryana, Faridabad and Gurgaon which are part of National Capital Region come under the local Delhi Mobile Telecommunication System. The rest of the cities of Haryana comes under Haryana Telecommunication System.

Electronic media channels include, MTV, 9XM, Star Group, SET Max, News Time, NDTV 24x7 and Zee Group. The radio stations include All India Radio and other FM stations.

The major newspapers of Haryana include "Dainik Bhaskar", "Punjab Kesari", "Jag Bani", "Dainik Jagran", "The Tribune", "Amar Ujala", "Hindustan Times", "Dainik Tribune", "The Times of India" and "Hari-Bhumi".

The total fertility rate of Haryana is 2.3. The infant mortality rate is 41 (SRS 2013) and maternal mortality ratio is 146 (SRS 2010–2012). The state of Haryana has various Medical Colleges including Pandit Bhagwat Dayal Sharma Post Graduate Institute of Medical Sciences Rohtak, Bhagat Phool Singh Medical College in District Sonipat, ESIC Medical College, Faridabad along with notable private medical institutes like Medanta, Max Hospital, Gurgaon, Fortis Healthcare

Literacy rate in Haryana has seen an upward trend and is 76.64 percent as per 2011 population census. Male literacy stands at 85.38 percent, while female literacy is at 66.67 percent. In 2001, the literacy rate in Haryana stood at 67.91 percent of which male and female were 78.49 percent and 55.73 percent literate respectively. , Gurgaon city had the highest literacy rate in Haryana at 86.30% followed by Panchkula at 81.9 per cent and Ambala at 81.7 percent. In terms of districts, Rewari had the highest literacy rate in Haryana at 74%, higher than the national average of 59.5%: male literacy was 79%, and female 67%.

Haryana Board of School Education, established in September 1969 and shifted to Bhiwani in 1981, conducts public examinations at middle, matriculation, and senior secondary levels twice a year. Over 700,000 candidates attend annual examinations in February and March; 150,000 attend supplementary examinations each November. The Board also conducts examinations for Haryana Open School at senior and senior secondary levels twice a year. The Haryana government provides free education to women up to the bachelor's degree level.

In 2015–2016, there were nearly 20,000 schools, including 10,100 state government schools (36 Aarohi Schools, 11 Kasturba Gandhi Balika Vidyalayas, 21 Model Sanskriti Schools, 8,744 government primary school, 3386 government middle school, 1,284 government high school and 1,967 government senior secondary schools), 7,635 private schools (200 aided, 6,612 recognised unaided, and 821 unrecognied unaided private schools.)and several hundred other central government and private schools such as Kendriya Vidyalaya, Indian Army Public Schools, Jawahar Navodaya Vidyalaya and DAV schools affiliated to central government's CBSE and ICSE school boards.

Haryana has 48 universities and 1,038 colleges, including 115 government colleges, 88 govt-aided colleges and 96 self-finance colleges. Hisar has three universities: Chaudhary Charan Singh Haryana Agricultural University - Asia's largest agricultural university, Guru Jambheshwar University of Science and Technology, Lala Lajpat Rai University of Veterinary & Animal Sciences); several national agricultural and veterinary research centres (National Research Centre on Equines), Central Sheep Breeding Farm, National Institute on Pig Breeding and Research, Northern Region Farm Machinery Training and Testing Institute and Central Institute for Research on Buffaloes (CIRB); and more than 20 colleges including Maharaja Agrasen Medical College, Agroha.

Demographically, Haryana has 471,000 women and 457,000 men pursuing post-secondary school higher education. There are more 18,616 female teachers and 17,061 male teachers in higher education.

Union Minister Ravi Shankar Prasad announced on 27 February 2016 that National Institute of Electronics and Information Technology (NIELIT) would be set up in Kurukshetra to provide computer training to youth and a Software Technology Park of India (STPI) would be set up in Panchkula's existing HSIIDC IT Park in Sector 23. Hindi and English are compulsory languages in schools whereas Punjabi, Sanskrit and Urdu are chosen as optional languages.

In the 2010 Commonwealth Games at Delhi, 22 out of 38 gold medals that India won came from Haryana. During the 33rd National Games held in Assam in 2007, Haryana stood first in the nation with a medal tally of 80, including 30 gold, 22 silver and 28 bronze medals.

The 1983 World Cup winning captain Kapil Dev made his domestic-cricket debut playing for Haryana. Nahar Singh Stadium was built in Faridabad in the year 1981 for international cricket. This ground has the capacity to hold around 25,000 people as spectators. Tejli Sports Complex is an ultra-modern sports complex in Yamuna Nagar. Tau Devi Lal Stadium in Gurgaon is a multi-sport complex.

Chief Minister of Haryana Manohar Lal Khattar announced the "Haryana Sports and Physical Fitness Policy", a policy to support 26 Olympic sports, on 12 January 2015 with the words "We will develop Haryana as the sports hub of the country."

Haryana is home to Haryana Gold, one of India's eight professional basketball teams which compete in the country's UBA Pro Basketball League.

At the 2016 Summer Olympics, Sakshi Malik won the bronze medal in the 58 kg category, becoming the first Indian female wrestler to win a medal at the Olympics and the fourth female Olympic medalist from the country.

Notable Badminton Player, Saina Nehwal is from Hisar in Haryana.

Panipat, Hisar, Ambala and Rohtak are the cities in which the leading newspapers of Haryana are printed and circulated throughout Haryana, in which Dainik Bhaskar, Dainik Jagran, Punjab Kesari, Tribune, Aaj Samaj, Hari Bhoomi and Amar Ujala are prominent.




</doc>
<doc id="14190" url="https://en.wikipedia.org/wiki?curid=14190" title="Himachal Pradesh">
Himachal Pradesh

Himachal Pradesh (; "snow-laden province") is a state in the northern part of India. Situated in the Western Himalayas, it is one of the eleven mountain states and is characterized by an extreme landscape featuring several peaks and extensive river systems. Himachal Pradesh shares borders with the Union territories of Jammu and Kashmir and Ladakh to the north, and the states of Punjab to the west, Haryana to the southwest, and Uttarakhand and Uttar Pradesh to the south. The state also has a border with the occupied region of Tibet to the east.

The predominantly mountainous region comprising the present-day Himachal Pradesh has been inhabited since pre-historic times having witnessed multiple waves of human migration from other areas. Through its history, the region was mostly ruled by local kingdoms some of which accepted the suzerainty of larger empires. Prior to India's independence from the British, Himachal comprised the hilly regions of Punjab Province of British India. After independence, many of the hilly territories were organized as the Chief Commissioner's province of Himachal Pradesh which later became a union territory. In 1966, hilly areas of neighboring Punjab state were merged into Himachal and it was ultimately granted full statehood in 1971.

Himachal Pradesh is spread across valleys with many perennial rivers flowing through them. Almost 90% of the state's population lives in rural areas. Agriculture, horticulture, hydropower and tourism are important constituents of the state's economy. The hilly state is almost universally electrified with 99.5% of the households having electricity as of 2016. The state was declared India's second open-defecation-free state in 2016. According to a survey of CMS – India Corruption Study 2017, Himachal Pradesh is India's least corrupt state.

Tribes such as the Koli, Hali, Dagi, Dhaugri, Dasa, Khasa, Kanaura, and Kirat inhabited the region from the prehistoric era. The foothills of the modern state of Himachal Pradesh were inhabited by people from the Indus valley civilisation which flourished between 2250 and 1750 B.C. The Kols and Mundas are believed to be the original inhabitants to the hills of present-day Himachal Pradesh followed by the Bhotas and Kiratas.

During the Vedic period, several small republics known as "Janapada" existed which were later conquered by the Gupta Empire. After a brief period of supremacy by King Harshavardhana, the region was divided into several local powers headed by chieftains, including some Rajputs principalities. These kingdoms enjoyed a large degree of independence and were invaded by Delhi Sultanate a number of times. Mahmud Ghaznavi conquered Kangra at the beginning of the 11th century. Timur and Sikander Lodi also marched through the lower hills of the state and captured a number of forts and fought many battles. Several hill states acknowledged Mughal suzerainty and paid regular tribute to the Mughals.
The Kingdom of Gorkha conquered many kingdoms and came to power in Nepal in 1768. They consolidated their military power and began to expand their territory. Gradually, the Kingdom of Nepal annexed Sirmour and Shimla. Under the leadership of Amar Singh Thapa, the Nepali army laid siege to Kangra. They managed to defeat Sansar Chand Katoch, the ruler of Kangra, in 1806 with the help of many provincial chiefs. However, the Nepali army could not capture Kangra fort which came under Maharaja Ranjeet Singh in 1809. After the defeat, they expanded towards the south of the state. However, Raja Ram Singh, Raja of Siba State, captured the fort of Siba from the remnants of Lahore Darbar in Samvat 1846, during the First Anglo-Sikh War.

They came into direct conflict with the British along the "tarai" belt after which the British expelled them from the provinces of the Satluj. The British gradually emerged as the paramount power in the region. In the revolt of 1857, or first Indian war of independence, arising from a number of grievances against the British, the people of the hill states were not as politically active as were those in other parts of the country. They and their rulers, with the exception of Bushahr, remained more or less inactive. Some, including the rulers of Chamba, Bilaspur, Bhagal and Dhami, rendered help to the British government during the revolt.

The British territories came under the British Crown after Queen Victoria's proclamation of 1858. The states of Chamba, Mandi and Bilaspur made good progress in many fields during the British rule. During World War I, virtually all rulers of the hill states remained loyal and contributed to the British war effort, both in the form of men and materials. Among these were the states of Kangra, Jaswan, Datarpur, Guler, Rajgarh, Nurpur, Chamba, Suket, Mandi, and Bilaspur.

After independence, the Chief Commissioner's Province of Himachal Pradesh was organised on 15 April 1948 as a result of the integration of 28 petty princely states (including feudal princes and "zaildars") in the promontories of the western Himalayas. These were known as the Simla Hills States and four Punjab southern hill states under the Himachal Pradesh (Administration) Order, 1948 under Sections 3 and 4 of the Extra-Provincial Jurisdiction Act, 1947 (later renamed as the Foreign Jurisdiction Act, 1947 vide A.O. of 1950). The State of Bilaspur was merged into Himachal Pradesh on 1 July 1954 by the Himachal Pradesh and Bilaspur (New State) Act, 1954.

Himachal became a Part 'C' state on 26 January 1950 with the implementation of the Constitution of India and the Lieutenant Governor was appointed. The Legislative Assembly was elected in 1952. Himachal Pradesh became a union territory on 1 November 1956. Some areas of Punjab State— namely Simla, Kangra, Kullu and Lahul and Spiti Districts, Nalagarh Tehsil of Ambala District, Lohara, Amb and Una Janungo circles, some area of Santokhgarh Kanungo circle and some other specified area of Una Tehsil of Hoshiarpur District, besides some parts of Dhar Kalan Kanungo circle of Pathankot tehsil of Gurdaspur District—were merged with Himachal Pradesh on 1 November 1966 on enactment by Parliament of the Punjab Reorganisation Act, 1966. On 18 December 1970, the State of Himachal Pradesh Act was passed by Parliament, and the new state came into being on 25 January 1971. Himachal became the 18th state of the Indian Union with Dr. Yashwant Singh Parmar as its first chief minister.

Himachal is in the western Himalayas. Covering an area of , it is a mountainous state. Most of the state lies on the foothills of the Dhauladhar Range. At 6,816 m, Reo Purgyil is the highest mountain peak in the state of Himachal Pradesh.

The drainage system of Himachal is composed both of rivers and glaciers. Himalayan rivers criss-cross the entire mountain chain.
Himachal Pradesh provides water to both the Indus and Ganges basins. The drainage systems of the region are the Chandra Bhaga or the Chenab, the Ravi, the Beas, the Sutlej, and the Yamuna. These rivers are perennial and are fed by snow and rainfall. They are protected by an extensive cover of natural vegetation.

Due to extreme variation in elevation, great variation occurs in the climatic conditions of Himachal. The climate varies from hot and subhumid tropical in the southern tracts to, with more elevation, cold, alpine, and glacial in the northern and eastern mountain ranges. The state's winter capital, Dharamsala receives very heavy rainfall, while areas like Lahaul and Spiti are cold and almost rainless. Broadly, Himachal experiences three seasons: summer, winter, and rainy season. Summer lasts from mid-April till the end of June and most parts become very hot (except in the alpine zone which experiences a mild summer) with the average temperature ranging from . Winter lasts from late November till mid-March. Snowfall is common in alpine tracts .

Himachal Pradesh is one of the states that lies in the Indian Himalayan Region (IHR), one of the richest reservoirs of biological diversity in the world. As of 2002, the IHR is undergoing large scale irrational extraction of wild, medicinal herbs, thus endangering many of its high-value gene stock. To address this, a workshop on ‘Endangered Medicinal Plant Species in Himachal Pradesh’ was held in 2002 and the conference was attended by forty experts from diverse disciplines.

According to 2003 Forest Survey of India report, legally defined forest areas constitute 66.52% of the area of Himachal Pradesh. Vegetation in the state is dictated by elevation and precipitation. The state is endowed with a high diversity of medicinal and aromatic plants. Lahaul-Spiti region of the state, being a cold desert, supports unique plants of medicinal value including "Ferula jaeschkeana", "Hyoscyamus niger", "Lancea tibetica", and "Saussurea bracteata".

Himachal is also said to be the fruit bowl of the country, with orchards being widespread. Meadows and pastures are also seen clinging to steep slopes. After the winter season, the hillsides and orchards bloom with wild flowers, while gladiolas, carnations, marigolds, roses, chrysanthemums, tulips and lilies are carefully cultivated. Himachal Pradesh Horticultural Produce Marketing and Processing Corporation Ltd. (HPMC) is a state body that markets fresh and processed fruits.

Himachal Pradesh has around 463 bird 77 mammalian, 44 reptile and 80 fish species. Great Himalayan National Park, a UNESCO World Heritage Site and Pin Valley National Park are the national Parks located in the state. The state also has 30 wildlife sanctuaries and 3 conservation reserves.

The Legislative Assembly of Himachal Pradesh has no pre-constitution history. The State itself is a post-independence creation. It came into being as a centrally administered territory on 15 April 1948 from the integration of thirty erstwhile princely states.

Himachal Pradesh is governed through a parliamentary system of representative democracy, a feature the state shares with other Indian states. Universal suffrage is granted to residents. The legislature consists of elected members and special office bearers such as the Speaker and the Deputy Speaker who are elected by the members. Assembly meetings are presided over by the Speaker or the Deputy Speaker in the Speaker's absence. The judiciary is composed of the Himachal Pradesh High Court and a system of lower courts.

Executive authority is vested in the Council of Ministers headed by the , although the titular head of government is the Governor. The governor is the head of state appointed by the President of India. The leader of the party or coalition with a majority in the Legislative Assembly is appointed as the Chief Minister by the governor, and the Council of Ministers are appointed by the governor on the advice of the Chief Minister. The Council of Ministers reports to the Legislative Assembly. The Assembly is unicameral with 68 Members of the Legislative Assembly (MLA). Terms of office run for five years, unless the Assembly is dissolved prior to the completion of the term. Auxiliary authorities known as "panchayats", for which local body elections are regularly held, govern local affairs.

In the assembly elections held in November 2017, the BJP secured an absolute majority, winning 44 of the 68 seats while the Congress won only 21 of the 68 seats. Jai Ram Thakur was sworn in as Himachal Pradesh's Chief Minister for the first time in Shimla on 27 December 2017.

The state of Himachal Pradesh is divided into 12 districts which are grouped into three divisions, Shimla, Kangra and Mandi. The districts are further divided into 69 subdivisions, 78 blocks and 145 Tehsils.

The era of planning in Himachal Pradesh started in 1951 along with the rest of India with the implementation of the first five-year plan. The First Plan allocated 52.7 million to Himachal Pradesh. More than 50% of this expenditure was incurred on transport and communication; while the power sector got a share of just 4.6%, though it had steadily increased to 7% by the Third Plan. Expenditure on agriculture and allied activities increased from 14.4% in the First Plan to 32% in the Third Plan, showing a progressive decline afterwards from 24% in the Fourth Plan to less than 10% in the Tenth Plan. Expenditure on energy sector was 24.2% of the total in the Tenth Plan.

The total GDP for 2005-06 was estimated at 254 billion as against 230 billion in the year 2004–05, showing an increase of 10.5%. The GDP for fiscal 2015–16 was estimated at 1.110 trillion, which increased to 1.247 trillion in 2016–17, recording growth of 6.8%. The per capita income increased from 130,067 in 2015–16 to 147,277 in 2016–17. The state government's advance estimates for fiscal 2017–18 stated the total GDP and per capita income as 1.359 trillion and 158,462 respectively. As of 2018, Himachal is the 22nd-largest state economy in India with in gross domestic product and has the 13th-highest per capita income () among the states and union territories of India.

Himachal Pradesh also ranks as the second-best performing state in the country on human development indicators after Kerala. One of the Indian government's key initiatives to tackle unemployment is the National Rural Employment Guarantee Act (NREGA). The participation of women in the NREGA has been observed to vary across different regions of the nation. As of the year 2009–2010, Himachal Pradesh joined the category of high female participation, recording a 46% share of NREGS (National Rural Employment Guarantee Scheme) work days to women. This was a drastic increase from the 13% that was recorded in 2006–2007.

Agriculture accounts for 9.4% of the net state domestic product. It is the main source of income and employment in Himachal. About 90% of the population in Himachal depends directly upon agriculture, which provides direct employment to 62% of total workers of state. The main cereals grown include wheat, maize, rice and barley with major cropping systems being maize-wheat, rice-wheat and maize-potato-wheat. Pulses, fruits, vegetables and oilseeds are among the other crops grown in the state. Land husbandry initiatives such as the Mid-Himalayan Watershed Development Project, which includes the Himachal Pradesh Reforestation Project (HPRP), the world's largest clean development mechanism (CDM) undertaking, have improved agricultural yields and productivity, and raised rural household incomes.

Apple is the principal cash crop of the state grown principally in the districts of Shimla, Kinnaur, Kullu, Mandi, Chamba and some parts of Sirmaur and Lahaul-Spiti with an average annual production of five lakh tonnes and per hectare production of 8 to 10 tonnes. The apple cultivation constitute 49 per cent of the total area under fruit crops and 85% of total fruit production in the state with an estimated economy of 3500 crore. Apples from Himachal are exported to other Indian states and even other countries. In 2011–12, the total area under apple cultivation was 1.04 lakh hectares, increased from 90,347 hectares in 2000–01. According to the provisional estimates of Ministry of Agriculture & Farmers Welfare, the annual apple production in Himachal for fiscal 2015–16 stood at 7.53 lakh tonnes, making it India's second-largest apple-producing state after Jammu and Kashmir.

Hydropower is one of the major sources of income generation for the state. The state has an abundance of hydropower resources because of the presence of various perennial rivers. Many high-capacity hydropower plants have been constructed which produce surplus electricity that is sold to other states, such as Delhi, Punjab and West Bengal. The income generated from exporting the electricity to other states is being provided as subsidy to the consumers in the state. The rich hydropower resources of Himachal have resulted in the state becoming almost universally electrified with around 94.8% houses receiving electricity as of 2001, as compared to the national average of 55.9%. Himachal's hydro-electric power production is however yet to be fully utilised: The identified hydroelectric potential for the state is 27,436 MW in five river basins while the hydroelectric capacity in 2016 was 10,351 MW.

Tourism in Himachal Pradesh is a major contributor to the state's economy and growth. The Himalayas attracts tourists from all over the world. Hill stations like Shimla, Manali, Dharamshala, Dalhousie, Chamba, Khajjiar, Kullu and Kasauli are popular destinations for both domestic and foreign tourists. The state also has many important Hindu pilgrimage sites with prominent temples like Naina Devi Temple, Bajreshwari Mata Temple, Jwala Ji Temple, Chintpurni, Chamunda Devi Temple, Baijnath Temple, Bhimakali Temple, Bijli Mahadev and Jakhoo Temple. Manimahesh Lake situated in the Bharmour region of Chamba district is the venue of an annual Hindu pilgrimage trek held in the month of August which attracts lakhs of devotees. The state is also referred to as ""Dev Bhoomi"" (literally meaning "Abode of Gods") due to its mention as such in ancient Hindu texts and occurrence of a large number of historical temples in the state.

Locally, it is called the Land of the Gods because of some popular Hindu temples of Hindu deities. Although neighbouring state Uttarakhand is famous as the land of the gods because of the major Hindu shrines like Badrinath( One of the Char Dham), Kedarnath, Adi- Kailasha, Origin of holy rivers like Ganges, Yamuna, Holy city Haridwar, Rishikesh etc.

Himachal is also known for its adventure tourism activities like ice skating in Shimla, paragliding in Bir Billing and Solang valley, rafting in Kullu, skiing in Manali, boating in Bilaspur and trekking, horse riding and fishing in different parts in the state. Shimla, the state's capital, is home to Asia's only natural ice-skating rink. Spiti Valley in Lahaul and Spiti District situated at an altitude of over 3000 metres with its picturesque landscapes is an important destination for adventure seekers. The region also has some of the oldest Buddhist Monasteries in the world
Himachal hosted the first Paragliding World Cup in India from 24 to 31 October in 2015. The venue for the paragliding world cup was Bir Billing, which is 70 km from the tourist town Macleod Ganj, located in the heart of Himachal in Kangra District. Bir Billing is the centre for aero sports in Himachal and considered as best for paragliding. Buddhist monasteries, trekking to tribal villages and mountain biking are other local possibilities.

Himachal has three Domestic Airports in Kangra, Kullu and Shimla districts. The air routes connect the state with Delhi and Chandigarh.

The only broad gauge railway line in the whole state connects Una Himachal railway station to Nagal Dam in Punjab and runs all the way to Daulatpur, Himachal Pradesh. It is an electrified track since 1999.

Future constructions:

Himachal is known for its narrow-gauge railways. One is the Kalka-Shimla Railway, a UNESCO World Heritage Site, and another is the Pathankot-Jogindernagar line. The total length of these two tracks is . The Kalka-Shimla Railway passes through many tunnels and Bridgies, while the Pathankot–Jogindernagar meanders through a maze of hills and valleys. The total route length of the operational railway network in the state is .

Roads are the major mode of transport in the hilly terrains. The state has road network of , including eight National Highways (NH) that constitute and 19 State Highways with a total length of . Hamirpur district has the highest road density in the country. Some roads are closed during winter and monsoon seasons due to snow and landslides. The state-owned Himachal Road Transport Corporation with a fleet of over 3,100, operates bus services connecting important cities and towns with villages within the state and also on various interstate routes. In addition, around 5,000 private buses ply in the state.

Himachal Pradesh has a total population of 6,864,602 including 3,481,873 males and 3,382,729 females according to the Census of India 2011. It has only 0.57 per cent of India's total population, recording a growth of 12.81 per cent. The scheduled castes and scheduled tribes account for 25.19 per cent and 5.71 per cent of the population respectively. The sex ratio stood at 972 females per 1,000 males, recording a marginal increase from 968 in 2001. The child sex ratio increased from 896 in 2001 to 909 in 2011. The total fertility rate (TFR) per woman in 2015 stood at 1.7, one of the lowest in India.

In the census, the state is placed 21st on the population chart, followed by Tripura at 22nd place. Kangra district was top-ranked with a population strength of 1,507,223 (21.98%), Mandi district 999,518 (14.58%), Shimla district 813,384 (11.86%), Solan district 576,670 (8.41%), Sirmaur district 530,164 (7.73%), Una district 521,057 (7.60%), Chamba district 518,844 (7.57%), Hamirpur district 454,293 (6.63%), Kullu district 437,474 (6.38%), Bilaspur district 382,056 (5.57%), Kinnaur district 84,298 (1.23%) and Lahaul Spiti 31,528 (0.46%).

The life expectancy at birth in Himachal Pradesh increased significantly from 52.6 years in the period from 1970–75 (above the national average of 49.7 years) to 72.0 years for the period 2011–15 (above the national average of 68.3 years). The infant mortality rate stood at 40 in 2010, and the crude birth rate has declined from 37.3 in 1971 to 16.9 in 2010, below the national average of 26.5 in 1998. The crude death rate was 6.9 in 2010. Himachal Pradesh's literacy rate has almost doubled between 1981 and 2011 (see table to right). The state is one of the most literate states of India with a literacy rate of 83.78% as of 2011.

Hindi is the official language of Himachal Pradesh and is spoken by the majority of the population as a lingua franca. Sanskrit is the additional official language of the state. Most of the population, however, speak natively one or another of the Western Pahari languages (locally also known as "Himachali" or just "Pahari"), a subgroup of the Indo-Aryan languages that includes Mandeali, Chambeali, Kangri, Kullu, Bilaspuri, Mahasu Pahari and others. Additional Indo-Aryan languages spoken in the state include Punjabi (native to 4.4% of the population), Nepali (1.3%) and Kashmiri (0.8%). In parts of the state there are speakers of Tibeto-Burman languages like Kinnauri (1.2%), Tibetan (0.3%), Lahauli (0.16%), Pattani (0.12%), and others.

Hinduism is the major religion in Himachal Pradesh. More than 95% of the total population adheres to the Hindu faith and majorly follows Shaivism and Shaktism traditions 
, the distribution of which is evenly spread throughout the state. Himachal Pradesh has the highest proportion of Hindu population among all the states and union territories in India.

Other religions that form a small percentage are Islam, Sikhism and Buddhism. Muslims are mainly concentrated in Sirmaur, Chamba, Una and Solan districts where they form 2.53-6.27% of the population. Sikhs mostly live in towns and cities and constitute 1.16% of the state population. The Buddhists, who constitute 1.15%, are mainly natives and tribals from Lahaul and Spiti, where they form a majority of 62%, and Kinnaur, where they form 21.5%.

Himachal Pradesh was one of the few states that had remained largely untouched by external customs, largely due to its difficult terrain. With remarkable economic and social advancements, the state has changed rapidly. Himachal Pradesh is a multireligious, multicultural as well as a multilingual state like other Indian states. Western Pahari languages also known as Himachali languages are widely spoken in the state. Some of the most commonly spoken individual languages are Kangri, Mandeali, Kulvi, Chambeali, Bharmauri and Kinnauri.

The Hindu communities residing in Himachal include the "Brahmins", "Kayasthas", "Rajputs", "Sunars", "Kannets", "Rathis" and "Kolis". The tribal population of the state consists mainly of "Gaddis", "Gujjars", "Kanauras", "Pangwalas", "Bhots", "Swanglas" and "Lahaulas".

Himachal is well known for its handicrafts. The carpets, leather works, Kullu shawls, Kangra paintings, Chamba Rumals, stoles, embroidered grass footwear ("Pullan chappal"), silver jewellery, metal ware, knitted woolen socks, "Pattoo", basketry of cane and bamboo ("Wicker" and "Rattan") and woodwork are among the notable ones. Of late, the demand for these handicrafts has increased within and outside the country.

Himachali caps of various colour bands are also well-known local art work, and are often treated as a symbol of the Himachali identity. The colour of the Himachali caps has been an indicator of political loyalties in the hill state for a long period of time with Congress party leaders like Virbhadra Singh donning caps with green band and the rival BJP leader Prem Kumar Dhumal wearing a cap with maroon band. The former has served six terms as the Chief Minister of the state while the latter is a two-time Chief Minister. Local music and dance also reflects the cultural identity of the state. Through their dance and music, the Himachali people entreat their gods during local festivals and other special occasions.

Apart from national fairs and festivals, there are regional fairs and festivals, including the temple fairs in nearly every region that are of great significance to Himachal Pradesh. The Kullu Dussehra festival is nationally known. The day-to-day cuisine of "Himachalis" is similar to the rest of northern India with Punjabi and Tibetan influences. Lentils ("Dāl"), rice ("Chāwal" or "Bhāț"), vegetables ("Sabzī") and chapati (wheat flatbread) form the staple food of the local population. Non-vegetarian food is more preferred and accepted in Himachal Pradesh than elsewhere in India, partly due to the scarcity of fresh vegetables on the hilly terrain of the state. Himachali specialities include "Siddu", "Babru", "Khatta", "Mhanee", "Channa Madra", "Patrode", "Mah Ki Dal", "Chamba-Style Fried Fish", "Kullu Trout", "Chha Gosht", "Pahadi Chicken", "Sepu Badi", "Auriya Kaddu", "Aloo Palda", "Pateer", "Makki Ki Roti" and "Sarson Ka Saag", "Chouck", "Bhagjery" and "Chutney" of Til.

At the time of Independence, Himachal Pradesh had a literacy rate of 8% - one of the lowest in the country. By 2011, the literacy rate surged to over 82%, making Himachal one of the most-literate states in the country. There are over 10,000 primary schools, 1,000 secondary schools and more than 1,300 high schools in the state. In meeting the constitutional obligation to make primary education compulsory, Himachal became the first state in India to make elementary education accessible to every child. Himachal Pradesh is an exception to the nationwide gender bias in education levels. The state has a female literacy rate of around 76%. In addition, school enrolment and participation rates for girls are almost universal at the primary level. While higher levels of education do reflect a gender-based disparity, Himachal is still significantly ahead of other states at bridging the gap. The Hamirpur District in particular stands out for high literacy rates across all metrics of measurement.

The state government has played an instrumental role in the rise of literacy in the state by spending a significant proportion of the state's GDP on education. During the first six five-year plans, most of the development expenditure in education sector was utilised in quantitative expansion, but after the seventh five-year-plan the state government switched emphasis on qualitative improvement and modernisation of education. In an effort to raise the number of teaching staff at primary schools they appointed over 1000 teacher aids through the Vidya Upasak Yojna in 2001. The Sarva Shiksha Abhiyan is another HP government initiative that not only aims for universal elementary education but also encourages communities to engage in the management of schools. The Rashtriya Madhayamic Shiksha Abhiyan launched in 2009, is a similar scheme but focuses on improving access to quality secondary education.

The standard of education in the state has reached a considerably high level as compared to other states in India with several reputed educational institutes for higher studies. The Baddi University of Emerging Sciences and Technologies, Indian Institute of Technology Mandi, Indian Institute of Management Sirmaur, Himachal Pradesh University in Shimla, Central University of Himachal Pradesh, Dharamsala, National Institute of Technology, Hamirpur, Indian Institute of Information Technology Una, Alakh Prakash Goyal University, Maharaja Agrasen University, Himachal Pradesh National Law University are some of the notable universities in the state. Indira Gandhi Medical College and Hospital in Shimla, Dr. Rajendra Prasad Government Medical College in Kangra, Rajiv Gandhi Government Post Graduate Ayurvedic College in Paprola and Homoeopathic Medical College & Hospital in Kumarhatti are the prominent medical institutes in the state. Besides these, there is a Government Dental College in Shimla which is the state's first recognised dental institute.

The state government has also decided to start three major nursing colleges to develop the healthcare system of the state. CSK Himachal Pradesh Krishi Vishwavidyalya Palampur is one of the most renowned hill agriculture institutes in the world. Dr. Yashwant Singh Parmar University of Horticulture and Forestry has earned a unique distinction in India for imparting teaching, research and extension education in horticulture, forestry and allied disciplines. Further, state-run Jawaharlal Nehru Government Engineering College was inaugurated in 2006 at Sundernagar.
Himachal Pradesh also hosts a campus of the prestigious fashion college, National Institute of Fashion Technology (NIFT) in Kangra.

Source: "Department of Information and Public Relations."





</doc>
<doc id="14192" url="https://en.wikipedia.org/wiki?curid=14192" title="Helene">
Helene

Helene or Hélène may refer to:





</doc>
<doc id="14193" url="https://en.wikipedia.org/wiki?curid=14193" title="Hyperion">
Hyperion

Hyperion may refer to:












</doc>
<doc id="14194" url="https://en.wikipedia.org/wiki?curid=14194" title="History of medicine">
History of medicine

The history of medicine shows how societies have changed in their approach to illness and disease from ancient times to the present. Early medical traditions include those of Babylon, China, Egypt and India. Sushruta, from India, introduced the concepts of medical diagnosis and prognosis. The Hippocratic Oath was written in ancient Greece in the 5th century BCE, and is a direct inspiration for oaths of office that physicians swear upon entry into the profession today. In the Middle Ages, surgical practices inherited from the ancient masters were improved and then systematized in Rogerius's "The Practice of Surgery". Universities began systematic training of physicians around 1220 CE in Italy.

Invention of the microscope was a consequence of improved understanding, during the Renaissance. Prior to the 19th century, humorism (also known as humoralism) was thought to explain the cause of disease but it was gradually replaced by the germ theory of disease, leading to effective treatments and even cures for many infectious diseases. Military doctors advanced the methods of trauma treatment and surgery. Public health measures were developed especially in the 19th century as the rapid growth of cities required systematic sanitary measures. Advanced research centers opened in the early 20th century, often connected with major hospitals. The mid-20th century was characterized by new biological treatments, such as antibiotics. These advancements, along with developments in chemistry, genetics, and radiography led to modern medicine. Medicine was heavily professionalized in the 20th century, and new careers opened to women as nurses (from the 1870s) and as physicians (especially after 1970).

Although there is little record to establish when plants were first used for medicinal purposes (herbalism), the use of plants as healing agents, as well as clays and soils is ancient. Over time, through emulation of the behavior of fauna, a medicinal knowledge base developed and passed between generations. Even earlier, Neanderthals may have engaged in medical practices.
As tribal culture specialized specific castes, shamans and apothecaries fulfilled the role of healer.
The first known dentistry dates to c. 7000 BC in Baluchistan where Neolithic dentists used flint-tipped drills and bowstrings. The first known trepanning operation was carried out c. 5000 BC in Ensisheim, France. A possible amputation was carried out c. 4,900 BC in Buthiers-Bulancourt, France.

The ancient Mesopotamians had no distinction between "rational science" and magic. When a person became ill, doctors would prescribe both magical formulas to be recited as well as medicinal treatments. The earliest medical prescriptions appear in Sumerian during the Third Dynasty of Ur ( 2112 BC – 2004 BC). The oldest Babylonian texts on medicine date back to the Old Babylonian period in the first half of the 2nd millennium BCE. The most extensive Babylonian medical text, however, is the "Diagnostic Handbook" written by the "ummânū", or chief scholar, Esagil-kin-apli of Borsippa, during the reign of the Babylonian king Adad-apla-iddina (1069–1046 BCE). Along with the Egyptians, the Babylonians introduced the practice of diagnosis, prognosis, physical examination, and remedies. In addition, the "Diagnostic Handbook" introduced the methods of therapy and cause. The text contains a list of medical symptoms and often detailed empirical observations along with logical rules used in combining observed symptoms on the body of a patient with its diagnosis and prognosis. The "Diagnostic Handbook" was based on a logical set of axioms and assumptions, including the modern view that through the examination and inspection of the symptoms of a patient, it is possible to determine the patient's disease, its cause and future development, and the chances of the patient's recovery. The symptoms and diseases of a patient were treated through therapeutic means such as bandages, herbs and creams.

In East Semitic cultures, the main medicinal authority was a kind of exorcist-healer known as an "āšipu". The profession was generally passed down from father to son and was held in extremely high regard. Of less frequent recourse was another kind of healer known as an "asu", who corresponds more closely to a modern physician and treated physical symptoms using primarily folk remedies composed of various herbs, animal products, and minerals, as well as potions, enemas, and ointments or poultices. These physicians, who could be either male or female, also dressed wounds, set limbs, and performed simple surgeries. The ancient Mesopotamians also practiced prophylaxis and took measures to prevent the spread of disease.

Mental illnesses were well known in ancient Mesopotamia, where diseases and mental disorders were believed to be caused by specific deities. Because hands symbolized control over a person, mental illnesses were known as "hands" of certain deities. One psychological illness was known as "Qāt Ištar", meaning "Hand of Ishtar". Others were known as "Hand of Shamash", "Hand of the Ghost", and "Hand of the God". Descriptions of these illnesses, however, are so vague that it is usually impossible to determine which illnesses they correspond to in modern terminology. Mesopotamian doctors kept detailed record of their patients' hallucinations and assigned spiritual meanings to them. A patient who hallucinated that he was seeing a dog was predicted to die; whereas, if he saw a gazelle, he would recover. The royal family of Elam was notorious for its members frequently suffering from insanity. Erectile dysfunction was recognized as being rooted in psychological problems.

Ancient Egypt developed a large, varied and fruitful medical tradition. Herodotus described the Egyptians as "the healthiest of all men, next to the Libyans", because of the dry climate and the notable public health system that they possessed. According to him, "the practice of medicine is so specialized among them that each physician is a healer of one disease and no more." Although Egyptian medicine, to a considerable extent, dealt with the supernatural, it eventually developed a practical use in the fields of anatomy, public health, and clinical diagnostics.

Medical information in the Edwin Smith Papyrus may date to a time as early as 3000 BC. Imhotep in the 3rd dynasty is sometimes credited with being the founder of ancient Egyptian medicine and with being the original author of the "Edwin Smith Papyrus", detailing cures, ailments and anatomical observations. The "Edwin Smith Papyrus" is regarded as a copy of several earlier works and was written c. 1600 BC. It is an ancient textbook on surgery almost completely devoid of magical thinking and describes in exquisite detail the "examination, diagnosis, treatment," and "prognosis" of numerous ailments.

The Kahun Gynaecological Papyrus treats women's complaints, including problems with conception. Thirty four cases detailing diagnosis and treatment survive, some of them fragmentarily. Dating to 1800 BCE, it is the oldest surviving medical text of any kind.

Medical institutions, referred to as "Houses of Life" are known to have been established in ancient Egypt as early as 2200 BC.

The Ebers Papyrus is the oldest written text mentioning enemas. Many medications were administered by enemas and one of the many types of medical specialists was an Iri, the Shepherd of the Anus.

The earliest known physician is also credited to ancient Egypt: Hesy-Ra, "Chief of Dentists and Physicians" for King Djoser in the 27th century BCE. Also, the earliest known woman physician, Peseshet, practiced in Ancient Egypt at the time of the 4th dynasty. Her title was "Lady Overseer of the Lady Physicians." In addition to her supervisory role, Peseshet trained midwives at an ancient Egyptian medical school in Sais.

The Atharvaveda, a sacred text of Hinduism dating from the Early Iron Age, is one of the first Indian texts dealing with medicine. The Atharvaveda also contains prescriptions of herbs for various ailments. The use of herbs to treat ailments would later form a large part of Ayurveda.

Ayurveda, meaning the "complete knowledge for long life" is another medical system of India. Its two most famous texts belong to the schools of Charaka and Sushruta. The earliest foundations of Ayurveda were built on a synthesis of traditional herbal practices together with a massive addition of theoretical conceptualizations, new nosologies and new therapies dating from about 600 BCE onwards, and coming out of the communities of thinkers which included the Buddha and others.

According to the compendium of Charaka, the Charakasamhitā, health and disease are not predetermined and life may be prolonged by human effort. The compendium of Suśruta, the Suśrutasamhitā defines the purpose of medicine to cure the diseases of the sick, protect the healthy, and to prolong life. Both these ancient compendia include details of the examination, diagnosis, treatment, and prognosis of numerous ailments. The Suśrutasamhitā is notable for describing procedures on various forms of surgery, including rhinoplasty, the repair of torn ear lobes, perineal lithotomy, cataract surgery, and several other excisions and other surgical procedures. Most remarkable was Susruta's surgery specially the rhinoplasty for which he is called father of meodern plastic surgery. Susruta also described more than 125 surgical instruments in detail.Also remarkable is Sushruta's penchant for scientific classification:
His medical treatise consists of 184 chapters, 1,120 conditions are listed, including injuries and illnesses relating to aging and mental illness.

The Ayurvedic classics mention eight branches of medicine: kāyācikitsā (internal medicine), śalyacikitsā (surgery including anatomy), śālākyacikitsā (eye, ear, nose, and throat diseases), kaumārabhṛtya (pediatrics with obstetrics and gynaecology), bhūtavidyā (spirit and psychiatric medicine), agada tantra (toxicology with treatments of stings and bites), rasāyana (science of rejuvenation), and vājīkaraṇa (aphrodisiac and fertility). Apart from learning these, the student of Āyurveda was expected to know ten arts that were indispensable in the preparation and application of his medicines: distillation, operative skills, cooking, horticulture, metallurgy, sugar manufacture, pharmacy, analysis and separation of minerals, compounding of metals, and preparation of alkalis. The teaching of various subjects was done during the instruction of relevant clinical subjects. For example, the teaching of anatomy was a part of the teaching of surgery, embryology was a part of training in pediatrics and obstetrics, and the knowledge of physiology and pathology was interwoven in the teaching of all the clinical disciplines.
The normal length of the student's training appears to have been seven years. But the physician was to continue to learn.

As an alternative form of medicine in India, Unani medicine found deep roots and royal patronage during medieval times. It progressed during the Indian sultanate and mughal periods. Unani medicine is very close to Ayurveda. Both are based on the theory of the presence of the elements (in Unani, they are considered to be fire, water, earth, and air) in the human body. According to followers of Unani medicine, these elements are present in different fluids and their balance leads to health and their imbalance leads to illness.

By the 18th century CE, Sanskrit medical wisdom still dominated. Muslim rulers built large hospitals in 1595 in Hyderabad, and in Delhi in 1719, and numerous commentaries on ancient texts were written.

China also developed a large body of traditional medicine. Much of the philosophy of traditional Chinese medicine derived from empirical observations of disease and illness by Taoist physicians and reflects the classical Chinese belief that individual human experiences express causative principles effective in the environment at all scales. These causative principles, whether material, essential, or mystical, correlate as the expression of the natural order of the universe.

The foundational text of Chinese medicine is the Huangdi neijing, (or "Yellow Emperor's Inner Canon"), written 5th century to 3rd century BCE. Near the end of the 2nd century CE, during the Han dynasty, Zhang Zhongjing, wrote a "Treatise on Cold Damage", which contains the earliest known reference to the "Neijing Suwen". The Jin Dynasty practitioner and advocate of acupuncture and moxibustion, Huangfu Mi (215–282), also quotes the Yellow Emperor in his "Jiayi jing", c. 265. During the Tang Dynasty, the "Suwen" was expanded and revised and is now the best extant representation of the foundational roots of traditional Chinese medicine. Traditional Chinese Medicine that is based on the use of herbal medicine, acupuncture, massage and other forms of therapy has been practiced in China for thousands of years.

In the 18th century, during the Qing dynasty, there was a proliferation of popular books as well as more advanced encyclopedias on traditional medicine. Jesuit missionaries introduced Western science and medicine to the royal court, although the Chinese physicians ignored them.

Finally in the 19th century, Western medicine was introduced at the local level by Christian medical missionaries from the London Missionary Society (Britain), the Methodist Church (Britain) and the Presbyterian Church (US). Benjamin Hobson (1816–1873) in 1839, set up a highly successful Wai Ai Clinic in Guangzhou, China. The Hong Kong College of Medicine for Chinese was founded in 1887 by the London Missionary Society, with its first graduate (in 1892) being Sun Yat-sen, who later led the Chinese Revolution (1911). The Hong Kong College of Medicine for Chinese was the forerunner of the School of Medicine of the University of Hong Kong, which started in 1911.

Because of the social custom that men and women should not be near to one another, the women of China were reluctant to be treated by male doctors. The missionaries sent women doctors such as Dr. Mary Hannah Fulton (1854–1927). Supported by the Foreign Missions Board of the Presbyterian Church (US) she in 1902 founded the first medical college for women in China, the Hackett Medical College for Women, in Guangzhou.

When reading the Chinese classics, it is important for scholars to examine these works from the Chinese perspective. Historians have noted two key aspects of Chinese medical history: understanding conceptual differences when translating the term "shén", and observing the history from the perspective of cosmology rather than biology.

In Chinese classical texts, the term "shén" is the closest historical translation to the English word "body" because it sometimes refers to the physical human body in terms of being weighed or measured, but the term is to be understood as an “ensemble of functions” encompassing both the human psyche and emotions. This concept of the human body is opposed to the European duality of a separate mind and body. It is critical for scholars to understand the fundamental differences in concepts of the body in order to connect the medical theory of the classics to the “human organism” it is explaining.

Chinese scholars established a correlation between the cosmos and the “human organism.” The basic components of cosmology, qi, yin yang and the Five Phase theory, were used to explain health and disease in texts such as Huangdi neijing. Yin and yang are the changing factors in cosmology, with qi as the vital force or energy of life. The Five phase theory Wu Xing of the Han dynasty contains the elements wood, fire, earth, metal, and water. By understanding medicine from a cosmology perspective, historians better understand Chinese medical and social classifications such as gender, which was defined by a domination or remission of yang in terms of yin.

These two distinctions are imperative when analyzing the history of traditional Chinese medical science.

A majority of Chinese medical history written after the classical canons comes in the form of primary source case studies where academic physicians record the illness of a particular person and the healing techniques used, as well as their effectiveness. Historians have noted that Chinese scholars wrote these studies instead of “books of prescriptions or advice manuals;” in their historical and environmental understanding, no two illnesses were alike so the healing strategies of the practitioner was unique every time to the specific diagnosis of the patient. Medical case studies existed throughout Chinese history, but “individually authored and published case history” was a prominent creation of the Ming Dynasty. An example such case studies would be the literati physician, Cheng Congzhou, collection of 93 cases published in 1644.

Around 800 BCE Homer in "The Iliad" gives descriptions of wound treatment by the two sons of Asklepios, the admirable physicians Podaleirius and Machaon and one acting doctor, Patroclus. Because Machaon is wounded and Podaleirius is in combat Eurypylus asks Patroclus to "cut out this arrow from my thigh, wash off the blood with warm water and spread soothing ointment on the wound". Asklepios like Imhotep becomes god of healing over time.

Temples dedicated to the healer-god Asclepius, known as "Asclepieia" (, sing. , "'Asclepieion"), functioned as centers of medical advice, prognosis, and healing. At these shrines, patients would enter a dream-like state of induced sleep known as "enkoimesis" () not unlike anesthesia, in which they either received guidance from the deity in a dream or were cured by surgery. Asclepeia provided carefully controlled spaces conducive to healing and fulfilled several of the requirements of institutions created for healing. In the Asclepeion of Epidaurus, three large marble boards dated to 350 BCE preserve the names, case histories, complaints, and cures of about 70 patients who came to the temple with a problem and shed it there. Some of the surgical cures listed, such as the opening of an abdominal abscess or the removal of traumatic foreign material, are realistic enough to have taken place, but with the patient in a state of enkoimesis induced with the help of soporific substances such as opium. Alcmaeon of Croton wrote on medicine between 500 and 450 BCE. He argued that channels linked the sensory organs to the brain, and it is possible that he discovered one type of channel, the optic nerves, by dissection.

A towering figure in the history of medicine was the physician Hippocrates of Kos (c. 460c. 370 BCE), considered the "father of modern medicine." The Hippocratic Corpus is a collection of around seventy early medical works from ancient Greece strongly associated with Hippocrates and his students. Most famously, the Hippocratics invented the Hippocratic Oath for physicians. Contemporary physicians swear an oath of office which includes aspects found in early editions of the Hippocratic Oath.

Hippocrates and his followers were first to describe many diseases and medical conditions. Though humorism (humoralism) as a medical system predates 5th-century Greek medicine, Hippocrates and his students systemetized the thinking that illness can be explained by an imbalance of blood, phlegm, black bile, and yellow bile. Hippocrates is given credit for the first description of clubbing of the fingers, an important diagnostic sign in chronic suppurative lung disease, lung cancer and cyanotic heart disease. For this reason, clubbed fingers are sometimes referred to as "Hippocratic fingers". Hippocrates was also the first physician to describe the Hippocratic face in "Prognosis". Shakespeare famously alludes to this description when writing of Falstaff's death in Act II, Scene iii. of "Henry V".

Hippocrates began to categorize illnesses as acute, chronic, endemic and epidemic, and use terms such as, "exacerbation, relapse, resolution, crisis, paroxysm, peak, and convalescence."

Another of Hippocrates's major contributions may be found in his descriptions of the symptomatology, physical findings, surgical treatment and prognosis of thoracic empyema, i.e. suppuration of the lining of the chest cavity. His teachings remain relevant to present-day students of pulmonary medicine and surgery. Hippocrates was the first documented person to practise cardiothoracic surgery, and his findings are still valid.

Some of the techniques and theories developed by Hippocrates are now put into practice by the fields of Environmental and Integrative Medicine. These include recognizing the importance of taking a complete history which includes environmental exposures as well as foods eaten by the patient which might play a role in his or her illness.

Two great Alexandrians laid the foundations for the scientific study of anatomy and physiology, Herophilus of Chalcedon and Erasistratus of Ceos. Other Alexandrian surgeons gave us ligature (hemostasis), lithotomy, hernia operations, ophthalmic surgery, plastic surgery, methods of reduction of dislocations and fractures, tracheotomy, and mandrake as an anaesthetic. Some of what we know of them comes from Celsus and Galen of Pergamum.

Herophilus of Chalcedon, working at the medical school of Alexandria placed intelligence in the brain, and connected the nervous system to motion and sensation. Herophilus also distinguished between veins and arteries, noting that the latter pulse while the former do not. He and his contemporary, Erasistratus of Chios, researched the role of veins and nerves, mapping their courses across the body. Erasistratus connected the increased complexity of the surface of the human brain compared to other animals to its superior intelligence. He sometimes employed experiments to further his research, at one time repeatedly weighing a caged bird, and noting its weight loss between feeding times. In Erasistratus' physiology, air enters the body, is then drawn by the lungs into the heart, where it is transformed into vital spirit, and is then pumped by the arteries throughout the body. Some of this vital spirit reaches the brain, where it is transformed into animal spirit, which is then distributed by the nerves.

The Greek Galen (c. 129–216 AD) was one of the greatest physicians of the ancient world, studying and traveling widely in ancient Rome. He dissected animals to learn about the body, and performed many audacious operations—including brain and eye surgeries—that were not tried again for almost two millennia. In "Ars medica" ("Arts of Medicine"), he explained mental properties in terms of specific mixtures of the bodily parts.

Galen's medical works were regarded as authoritative until well into the Middle Ages. Galen left a physiological model of the human body that became the mainstay of the medieval physician's university anatomy curriculum, but it suffered greatly from stasis and intellectual stagnation because some of Galen's ideas were incorrect; he did not dissect a human body. Greek and Roman taboos had meant that dissection was usually banned in ancient times, but in the Middle Ages it changed.

In 1523 Galen's "On the Natural Faculties" was published in London. In the 1530s Belgian anatomist and physician Andreas Vesalius launched a project to translate many of Galen's Greek texts into Latin. Vesalius's most famous work, "De humani corporis fabrica" was greatly influenced by Galenic writing and form.

The Romans invented numerous surgical instruments, including the first instruments unique to women, as well as the surgical uses of forceps, scalpels, cautery, cross-bladed scissors, the surgical needle, the sound, and speculas. Romans also performed cataract surgery.

The Roman army physician Dioscorides (c. 40–90 CE), was a Greek botanist and pharmacologist. He wrote the encyclopedia "De Materia Medica" describing over 600 herbal cures, forming an influential pharmacopoeia which was used extensively for the following 1,500 years.

Early Christians in the Roman Empire incorporated medicine into their theology, ritual practices, and metaphors.

Byzantine medicine encompasses the common medical practices of the Byzantine Empire from about 400 AD to 1453 AD. Byzantine medicine was notable for building upon the knowledge base developed by its Greco-Roman predecessors. In preserving medical practices from antiquity, Byzantine medicine influenced Islamic medicine as well as fostering the Western rebirth of medicine during the Renaissance.

Byzantine physicians often compiled and standardized medical knowledge into textbooks. Their records tended to include both diagnostic explanations and technical drawings. The Medical Compendium in Seven Books, written by the leading physician Paul of Aegina, survived as a particularly thorough source of medical knowledge. This compendium, written in the late seventh century, remained in use as a standard textbook for the following 800 years.

Late antiquity ushered in a revolution in medical science, and historical records often mention civilian hospitals (although battlefield medicine and wartime triage were recorded well before Imperial Rome). Constantinople stood out as a center of medicine during the Middle Ages, which was aided by its crossroads location, wealth, and accumulated knowledge.

The first ever known example of separating conjoined twins occurred in the Byzantine Empire in the 10th century. The next example of separating conjoined twins will be first recorded many centuries later in Germany in 1689.

The Byzantine Empire's neighbors, the Persian Sassanid Empire, also made their noteworthy contributions mainly with the establishment of the Academy of Gondeshapur, which was "the most important medical center of the ancient world during the 6th and 7th centuries." In addition, Cyril Elgood, British physician and a historian of medicine in Persia, commented that thanks to medical centers like the Academy of Gondeshapur, "to a very large extent, the credit for the whole hospital system must be given to Persia."

The Islamic civilization rose to primacy in medical science as its physicians contributed significantly to the field of medicine, including anatomy, ophthalmology, pharmacology, pharmacy, physiology, and surgery. The Arabs were influenced by ancient Indian, Persian, Greek, Roman and Byzantine medical practices, and helped them develop further. Galen & Hippocrates were pre-eminent authorities. The translation of 129 of Galen's works into Arabic by the Nestorian Christian Hunayn ibn Ishaq and his assistants, and in particular Galen's insistence on a rational systematic approach to medicine, set the template for Islamic medicine, which rapidly spread throughout the Arab Empire. Its most famous physicians included the Persian polymaths Muhammad ibn Zakarīya al-Rāzi and Avicenna, who wrote more than 40 works on health, medicine, and well-being. Taking leads from Greece and Rome, Islamic scholars kept both the art and science of medicine alive and moving forward. Persian polymath Avicenna has also been called the "father of medicine". He wrote "The Canon of Medicine" which became a standard medical text at many medieval European universities, considered one of the most famous books in the history of medicine. "The Canon of Medicine" presents an overview of the contemporary medical knowledge of the medieval Islamic world, which had been influenced by earlier traditions including Greco-Roman medicine (particularly Galen), Persian medicine, Chinese medicine and Indian medicine. Persian physician al-Rāzi was one of the first to question the Greek theory of humorism, which nevertheless remained influential in both medieval Western and medieval Islamic medicine. Some volumes of al-Rāzi's work "Al-Mansuri", namely "On Surgery" and "A General Book on Therapy", became part of the medical curriculum in European universities. Additionally, he has been described as a doctor's doctor, the father of pediatrics, and a pioneer of ophthalmology. For example, he was the first to recognize the reaction of the eye's pupil to light.

After AD 400, the study and practice of medicine in the Western Roman Empire went into deep decline. Medical services were provided, especially for the poor, in the thousands of monastic hospitals that sprang up across Europe, but the care was rudimentary and mainly palliative. Most of the writings of Galen and Hippocrates were lost to the West, with the summaries and compendia of St. Isidore of Seville being the primary channel for transmitting Greek medical ideas. The Carolingian renaissance brought increased contact with Byzantium and a greater awareness of ancient medicine, but only with the twelfth-century renaissance and the new translations coming from Muslim and Jewish sources in Spain, and the fifteenth-century flood of resources after the fall of Constantinople did the West fully recover its acquaintance with classical antiquity.

Greek and Roman taboos had meant that dissection was usually banned in ancient times, but in the Middle Ages it changed: medical teachers and students at Bologna began to open human bodies, and Mondino de Luzzi (c. 1275–1326) produced the first known anatomy textbook based on human dissection.

Wallis identifies a prestige hierarchy with university educated physicians on top, followed by learned surgeons; craft-trained surgeons; barber surgeons; itinerant specialists such as dentist and oculists; empirics; and midwives.

The first medical schools were opened in the 9th century, most notably the Schola Medica Salernitana at Salerno in southern Italy. The cosmopolitan influences from Greek, Latin, Arabic, and Hebrew sources gave it an international reputation as the Hippocratic City. Students from wealthy families came for three years of preliminary studies and five of medical studies. The medicine, following the laws of Federico II, that he founded in 1224 the University ad improved the Schola Salernitana, in the period between 1200 and 1400, it had in Sicily (so-called Sicilian Middle Ages) a particular development so much to create a true school of Jewish medicine.

As a result of which, after a legal examination, was conferred to a Jewish Sicilian woman, Virdimura, wife of another physician Pasquale of Catania, the historical record of before woman officially trained to exercise of the medical profession.

By the thirteenth century, the medical school at Montpellier began to eclipse the Salernitan school. In the 12th century, universities were founded in Italy, France, and England, which soon developed schools of medicine. The University of Montpellier in France and Italy's University of Padua and University of Bologna were leading schools. Nearly all the learning was from lectures and readings in Hippocrates, Galen, Avicenna, and Aristotle. In later centuries, the importance of universities founded in the late Middle Ages gradually increased, e.g. Charles University in Prague (established in 1348), Jagiellonian University in Cracow (1364), University of Vienna (1365), Heidelberg University (1386) and University of Greifswald (1456).

The underlying principle of most medieval medicine was Galen's theory of humours. This was derived from the ancient medical works, and dominated all western medicine until the 19th century. The theory stated that within every individual there were four humours, or principal fluids—black bile, yellow bile, phlegm, and blood, these were produced by various organs in the body, and they had to be in balance for a person to remain healthy. Too much phlegm in the body, for example, caused lung problems; and the body tried to cough up the phlegm to restore a balance. The balance of humours in humans could be achieved by diet, medicines, and by blood-letting, using leeches. The four humours were also associated with the four seasons, black bile-autumn, yellow bile-summer, phlegm-winter and blood-spring.

Healing included both physical and spiritual therapeutics, such as the right herbs, a suitable diet, clean bedding, and the sense that care was always at hand. Other procedures used to help patients included the Mass, prayers, relics of saints, and music used to calm a troubled mind or quickened pulse.

In 1376, in Sicily, it was historically given, in relationship to the laws of Federico II that they foresaw an examination with a regal errand of physicists, the first qualification to the exercise of the medicine to a woman, Virdimura a Jewess of Catania, whose document is preserved in Palermo to the Italian national archives.

The Renaissance brought an intense focus on scholarship to Christian Europe. A major effort to translate the Arabic and Greek scientific works into Latin emerged. Europeans gradually became experts not only in the ancient writings of the Romans and Greeks, but in the contemporary writings of Islamic scientists. During the later centuries of the Renaissance came an increase in experimental investigation, particularly in the field of dissection and body examination, thus advancing our knowledge of human anatomy.

The development of modern neurology began in the 16th century in Italy and France with Niccolò Massa, Jean Fernel, Jacques Dubois and Andreas Vesalius. Vesalius described in detail the anatomy of the brain and other organs; he had little knowledge of the brain's function, thinking that it resided mainly in the ventricles. Over his lifetime he corrected over 200 of Galen's mistakes. Understanding of medical sciences and diagnosis improved, but with little direct benefit to health care. Few effective drugs existed, beyond opium and quinine. Folklore cures and potentially poisonous metal-based compounds were popular treatments.
Independently from Ibn al-Nafis, Michael Servetus rediscovered the pulmonary circulation, but this discovery did not reach the public because it was written down for the first time in the "Manuscript of Paris" in 1546, and later published in the theological work which he paid with his life in 1553. Later this was perfected by Renaldus Columbus and Andrea Cesalpino.

In 1628 the English physician William Harvey made a ground-breaking discovery when he correctly described the circulation of the blood in his "Exercitatio Anatomica de Motu Cordis et Sanguinis in Animalibus". Before this time the most useful manual in medicine used both by students and expert physicians was Dioscorides' "De Materia Medica", a pharmacopoeia.
Bacteria and protists were first observed with a microscope by Antonie van Leeuwenhoek in 1676, initiating the scientific field of microbiology.

Paracelsus (1493–1541), was an erratic and abusive innovator who rejected Galen and bookish knowledge, calling for experimental research, with heavy doses of mysticism, alchemy and magic mixed in. He rejected sacred magic (miracles) under Church auspisces and looked for cures in nature. He preached but he also pioneered the use of chemicals and minerals in medicine. His hermetical views were that sickness and health in the body relied on the harmony of man (microcosm) and Nature (macrocosm). He took an approach different from those before him, using this analogy not in the manner of soul-purification but in the manner that humans must have certain balances of minerals in their bodies, and that certain illnesses of the body had chemical remedies that could cure them. Most of his influence came after his death. Paracelsus is a highly controversial figure in the history of medicine, with most experts hailing him as a Father of Modern Medicine for shaking off religious orthodoxy and inspiring many researchers; others say he was a mystic more than a scientist and downplay his importance.

University training of physicians began in the 13th century.

The University of Padua was founded about 1220 by walkouts from the University of Bologna, and began teaching medicine in 1222. It played a leading role in the identification and treatment of diseases and ailments, specializing in autopsies and the inner workings of the body. Starting in 1595, Padua's famous anatomical theatre drew artists and scientists studying the human body during public dissections. The intensive study of Galen led to critiques of Galen modeled on his own writing, as in the first book of Vesalius's "De humani corporis fabrica." Andreas Vesalius held the chair of Surgery and Anatomy ("explicator chirurgiae") and in 1543 published his anatomical discoveries in "De Humani Corporis Fabrica". He portrayed the human body as an interdependent system of organ groupings. The book triggered great public interest in dissections and caused many other European cities to establish anatomical theatres.

At the University of Bologna the training of physicians began in 1219. The Italian city attracted students from across Europe. Taddeo Alderotti built a tradition of medical education that established the characteristic features of Italian learned medicine and was copied by medical schools elsewhere. Turisanus (d. 1320) was his student. The curriculum was revised and strengthened in 1560–1590. A representative professor was Julius Caesar Aranzi (Arantius) (1530–89). He became Professor of Anatomy and Surgery at the University of Bologna in 1556, where he established anatomy as a major branch of medicine for the first time. Aranzi combined anatomy with a description of pathological processes, based largely on his own research, Galen, and the work of his contemporary Italians. Aranzi discovered the 'Nodules of Aranzio' in the semilunar valves of the heart and wrote the first description of the superior levator palpebral and the coracobrachialis muscles. His books (in Latin) covered surgical techniques for many conditions, including hydrocephalus, nasal polyp, goitre and tumours to phimosis, ascites, haemorrhoids, anal abscess and fistulae.

Catholic women played large roles in health and healing in medieval and early modern Europe. A life as a nun was a prestigious role; wealthy families provided dowries for their daughters, and these funded the convents, while the nuns provided free nursing care for the poor.

The Catholic elites provided hospital services because of their theology of salvation that good works were the route to heaven. The Protestant reformers rejected the notion that rich men could gain God's grace through good works—and thereby escape purgatory—by providing cash endowments to charitable institutions. They also rejected the Catholic idea that the poor patients earned grace and salvation through their suffering. Protestants generally closed all the convents and most of the hospitals, sending women home to become housewives, often against their will. On the other hand, local officials recognized the public value of hospitals, and some were continued in Protestant lands, but without monks or nuns and in the control of local governments.

In London, the crown allowed two hospitals to continue their charitable work, under nonreligious control of city officials. The convents were all shut down but Harkness finds that women—some of them former nuns—were part of a new system that delivered essential medical services to people outside their family. They were employed by parishes and hospitals, as well as by private families, and provided nursing care as well as some medical, pharmaceutical, and surgical services.

Meanwhile, in Catholic lands such as France, rich families continued to fund convents and monasteries, and enrolled their daughters as nuns who provided free health services to the poor. Nursing was a religious role for the nurse, and there was little call for science.

During the Age of Enlightenment, the 18th century, science was held in high esteem and physicians upgraded their social status by becoming more scientific. The health field was crowded with self-trained barber-surgeons, apothecaries, midwives, drug peddlers, and charlatans.

Across Europe medical schools relied primarily on lectures and readings. The final year student would have limited clinical experience by trailing the professor through the wards. Laboratory work was uncommon, and dissections were rarely done because of legal restrictions on cadavers. Most schools were small, and only Edinburgh, Scotland, with 11,000 alumni, produced large numbers of graduates.

In Britain, there were but three small hospitals after 1550. Pelling and Webster estimate that in London in the 1580 to 1600 period, out of a population of nearly 200,000 people, there were about 500 medical practitioners. Nurses and midwives are not included. There were about 50 physicians, 100 licensed surgeons, 100 apothecaries, and 250 additional unlicensed practitioners. In the last category about 25% were women. All across Britain—and indeed all of the world—the vast majority of the people in city, town or countryside depended for medical care on local amateurs with no professional training but with a reputation as wise healers who could diagnose problems and advise sick people what to do—and perhaps set broken bones, pull a tooth, give some traditional herbs or brews or perform a little magic to cure what ailed them.

The London Dispensary opened in 1696, the first clinic in the British Empire to dispense medicines to poor sick people. The innovation was slow to catch on, but new dispensaries were open in the 1770s. In the colonies, small hospitals opened in Philadelphia in 1752, New York in 1771, and Boston (Massachusetts General Hospital) in 1811.

Guy's Hospital, the first great British hospital with a modern foundation opened in 1721 in London, with funding from businessman Thomas Guy. It had been preceded by St Bartholomew's Hospital and St Thomas's Hospital, both medieval foundations. In 1821 a bequest of £200,000 by William Hunt in 1829 funded expansion for an additional hundred beds at Guy's. Samuel Sharp (1709–78), a surgeon at Guy's Hospital from 1733 to 1757, was internationally famous; his "A Treatise on the Operations of Surgery" (1st ed., 1739), was the first British study focused exclusively on operative technique.

English physician Thomas Percival (1740–1804) wrote a comprehensive system of medical conduct, "Medical Ethics; or, a Code of Institutes and Precepts, Adapted to the Professional Conduct of Physicians and Surgeons" (1803) that set the standard for many textbooks.

In the Spanish Empire, the viceregal capital of Mexico City was a site of medical training for physicians and the creation of hospitals. Epidemic disease had decimated indigenous populations starting with the early sixteenth-century Spanish conquest of the Aztec empire, when a black auxiliary in the armed forces of conqueror Hernán Cortés, with an active case of smallpox, set off a virgin land epidemic among indigenous peoples, Spanish allies and enemies alike. Aztec emperor Cuitlahuac died of smallpox. Disease was a significant factor in the Spanish conquest elsewhere as well.

Medical education instituted at the Royal and Pontifical University of Mexico chiefly served the needs of urban elites. Male and female "curanderos" or lay practitioners, attended to the ills of the popular classes. The Spanish crown began regulating the medical profession just a few years after the conquest, setting up the Royal Tribunal of the Protomedicato, a board for licensing medical personnel in 1527. Licensing became more systematic after 1646 with physicians, druggists, surgeons, and bleeders requiring a license before they could publicly practice. Crown regulation of medical practice became more general in the Spanish empire.

Elites and the popular classes alike called on divine intervention in personal and society-wide health crises, such as the epidemic of 1737. The intervention of the Virgin of Guadalupe was depicted in a scene of dead and dying Indians, with elites on their knees praying for her aid. In the late eighteenth century, the crown began implementing secularizing policies on the Iberian peninsula and its overseas empire to control disease more systematically and scientifically.

Botanical medicines also became popular during the 16th, 17th, and 18th Centuries. Spanish pharmaceutical books during this time contain medicinal recipes consisting of spices, herbs, and other botanical products. For example, nutmeg oil was documented for curing stomach ailments and cardamom oil was believed to relieve intestinal ailments. During the rise of the global trade market, spices and herbs, along with many other goods, that were indigenous to different territories began to appear in different locations across the globe. Herbs and spices were especially popular for their utility in cooking and medicines. As a result of this popularity and increased demand for spices, some areas in Asia, like China and Indonesia, became hubs for spice cultivation and trade. The Spanish Empire also wanted to benefit from the international spice trade, so they looked towards their American colonies.

The Spanish American colonies became an area where the Spanish searched to discover new spices and indigenous American medicinal recipes. The Florentine Codex, a 16th-century ethnographic research study in Mesoamerica by the Spanish Franciscan friar Bernardino de Sahagún, is a major contribution to the history of Nahua medicine. The Spanish did discover many spices and herbs new to them, some of which were reportedly similar to Asian spices. A Spanish physician by the name of Nicolás Monardes studied many of the American spices coming into Spain. He documented many of the new American spices and their medicinal properties in his survey Historia medicinal de las cosas que se traen de nuestras Indias Occidentales. For example, Monardes describes the "Long Pepper" (Pimienta luenga), found along the coasts of the countries that are now known Panama and Colombia, as a pepper that was more flavorful, healthy, and spicy in comparison to the Eastern black pepper. The Spanish interest in American spices can first be seen in the commissioning of the Libellus de Medicinalibus Indorum Herbis, which was a Spanish-American codex describing indigenous American spices and herbs and describing the ways that these were used in natural Aztec medicines. The codex was commissioned in the year 1552 by Francisco de Mendoza, the son of Antonio de Mendoza, who was the first Viceroy of New Spain. Francisco de Mendoza was interested in studying the properties of these herbs and spices, so that he would be able to profit from the trade of these herbs and the medicines that could be produced by them.

Francisco de Mendoza recruited the help of Monardez in studying the traditional medicines of the indigenous people living in what was then the Spanish colonies. Monardez researched these medicines and performed experiments to discover the possibilities of spice cultivation and medicine creation in the Spanish colonies. The Spanish transplanted some herbs from Asia, but only a few foreign crops were successfully grown in the Spanish Colonies. One notable crop brought from Asia and successfully grown in the Spanish colonies was ginger, as it was considered Hispaniola's number 1 crop at the end of the 16th Century. The Spanish Empire did profit from cultivating herbs and spices, but they also introduced pre-Columbian American medicinal knowledge to Europe. Other Europeans were inspired by the actions of Spain and decided to try to establish a botanical transplant system in colonies that they controlled, however these subsequent attempts were not successful.

The practice of medicine changed in the face of rapid advances in science, as well as new approaches by physicians. Hospital doctors began much more systematic analysis of patients' symptoms in diagnosis. Among the more powerful new techniques were anaesthesia, and the development of both antiseptic and aseptic operating theatres. Effective cures were developed for certain endemic infectious diseases. However the decline in many of the most lethal diseases was due more to improvements in public health and nutrition than to advances in medicine.

Medicine was revolutionized in the 19th century and beyond by advances in chemistry, laboratory techniques, and equipment. Old ideas of infectious disease epidemiology were gradually replaced by advances in bacteriology and virology.

In the 1830s in Italy, Agostino Bassi traced the silkworm disease muscardine to microorganisms. Meanwhile, in Germany, Theodor Schwann led research on alcoholic fermentation by yeast, proposing that living microorganisms were responsible.
Leading chemists, such as Justus von Liebig, seeking solely physicochemical explanations, derided this claim and alleged that Schwann was regressing to vitalism.

In 1847 in Vienna, Ignaz Semmelweis (1818–1865), dramatically reduced the death rate of new mothers (due to childbed fever) by requiring physicians to clean their hands before attending childbirth, yet his principles were marginalized and attacked by professional peers. At that time most people still believed that infections were caused by foul odors called miasmas.

Eminent French scientist Louis Pasteur confirmed Schwann's fermentation experiments in 1857 and afterwards supported the hypothesis that yeast were microorganisms. Moreover, he suggested that such a process might also explain contagious disease. In 1860, Pasteur's report on bacterial fermentation of butyric acid motivated fellow Frenchman Casimir Davaine to identify a similar species (which he called "bacteridia") as the pathogen of the deadly disease anthrax. Others dismissed "bacteridia" as a mere byproduct of the disease. British surgeon Joseph Lister, however, took these findings seriously and subsequently introduced antisepsis to wound treatment in 1865.

German physician Robert Koch, noting fellow German Ferdinand Cohn's report of a spore stage of a certain bacterial species, traced the life cycle of Davaine's "bacteridia", identified spores, inoculated laboratory animals with them, and reproduced anthrax—a breakthrough for experimental pathology and germ theory of disease. Pasteur's group added ecological investigations confirming spores' role in the natural setting, while Koch published a landmark treatise in 1878 on the bacterial pathology of wounds. In 1881, Koch reported discovery of the "tubercle bacillus", cementing germ theory and Koch's acclaim.

Upon the outbreak of a cholera epidemic in Alexandria, Egypt, two medical missions went to investigate and attend the sick, one was sent out by Pasteur and the other led by Koch. Koch's group returned in 1883, having successfully discovered the cholera pathogen. In Germany, however, Koch's bacteriologists had to vie against Max von Pettenkofer, Germany's leading proponent of miasmatic theory. Pettenkofer conceded bacteria's casual involvement, but maintained that other, environmental factors were required to turn it pathogenic, and opposed water treatment as a misdirected effort amid more important ways to improve public health. The massive cholera epidemic in Hamburg in 1892 devastasted Pettenkoffer's position, and yielded German public health to "Koch's bacteriology".

On losing the 1883 rivalry in Alexandria, Pasteur switched research direction, and introduced his third vaccine—rabies vaccine—the first vaccine for humans since Jenner's for smallpox. From across the globe, donations poured in, funding the founding of Pasteur Institute, the globe's first biomedical institute, which opened in 1888. Along with Koch's bacteriologists, Pasteur's group—which preferred the term "microbiology"—led medicine into the new era of "scientific medicine" upon bacteriology and germ theory. Accepted from Jakob Henle, Koch's steps to confirm a species' pathogenicity became famed as "Koch's postulates". Although his proposed tuberculosis treatment, tuberculin, seemingly failed, it soon was used to test for infection with the involved species. In 1905, Koch was awarded the Nobel Prize in Physiology or Medicine, and remains renowned as the founder of medical microbiology.

Women had always served in ancillary roles, and as midwives and healers. The professionalization of medicine forced them increasingly to the sidelines. As hospitals multiplied they relied in Europe on orders of Roman Catholic nun-nurses, and German Protestant and Anglican deaconesses in the early 19th century. They were trained in traditional methods of physical care that involved little knowledge of medicine. The breakthrough to professionalization based on knowledge of advanced medicine was led by Florence Nightingale in England. She resolved to provide more advanced training than she saw on the Continent. At Kaiserswerth, where the first German nursing schools were founded in 1836 by Theodor Fliedner, she said, "The nursing was nil and the hygiene horrible.") Britain's male doctors preferred the old system, but Nightingale won out and her Nightingale Training School opened in 1860 and became a model. The Nightingale solution depended on the patronage of upper-class women, and they proved eager to serve. Royalty became involved. In 1902 the wife of the British king took control of the nursing unit of the British army, became its president, and renamed it after herself as the Queen Alexandra's Royal Army Nursing Corps; when she died the next queen became president. Today its Colonel In Chief is Sophie, Countess of Wessex, the daughter-in-law of Queen Elizabeth II. In the United States, upper-middle-class women who already supported hospitals promoted nursing. The new profession proved highly attractive to women of all backgrounds, and schools of nursing opened in the late 19th century. They soon a function of large hospitals, where they provided a steady stream of low-paid idealistic workers. The International Red Cross began operations in numerous countries in the late 19th century, promoting nursing as an ideal profession for middle-class women.

The Nightingale model was widely copied. Linda Richards (1841–1930) studied in London and became the first professionally trained American nurse. She established nursing training programs in the United States and Japan, and created the first system for keeping individual medical records for hospitalized patients. The Russian Orthodox Church sponsored seven orders of nursing sisters in the late 19th century. They ran hospitals, clinics, almshouses, pharmacies, and shelters as well as training schools for nurses. In the Soviet era (1917–1991), with the aristocratic sponsors gone, nursing became a low-prestige occupation based in poorly maintained hospitals.

It was very difficult for women to become doctors in any field before the 1970s. Elizabeth Blackwell (1821–1910) became the first woman to formally study and practice medicine in the United States. She was a leader in women's medical education. While Blackwell viewed medicine as a means for social and moral reform, her student Mary Putnam Jacobi (1842–1906) focused on curing disease. At a deeper level of disagreement, Blackwell felt that women would succeed in medicine because of their humane female values, but Jacobi believed that women should participate as the equals of men in all medical specialties using identical methods, values and insights. In the Soviet Union although the majority of medical doctors were women, they were paid less than the mostly male factory workers.

Paris (France) and Vienna were the two leading medical centers on the Continent in the era 1750–1914.

In the 1770s–1850s Paris became a world center of medical research and teaching. The "Paris School" emphasized that teaching and research should be based in large hospitals and promoted the professionalization of the medical profession and the emphasis on sanitation and public health. A major reformer was Jean-Antoine Chaptal (1756–1832), a physician who was Minister of Internal Affairs. He created the Paris Hospital, health councils, and other bodies.
Louis Pasteur (1822–1895) was one of the most important founders of medical microbiology. He is remembered for his remarkable breakthroughs in the causes and preventions of diseases. His discoveries reduced mortality from puerperal fever, and he created the first vaccines for rabies and anthrax. His experiments supported the germ theory of disease. He was best known to the general public for inventing a method to treat milk and wine in order to prevent it from causing sickness, a process that came to be called pasteurization. He is regarded as one of the three main founders of microbiology, together with Ferdinand Cohn and Robert Koch. He worked chiefly in Paris and in 1887 founded the Pasteur Institute there to perpetuate his commitment to basic research and its practical applications. As soon as his institute was created, Pasteur brought together scientists with various specialties. The first five departments were directed by Emile Duclaux (general microbiology research) and Charles Chamberland (microbe research applied to hygiene), as well as a biologist, Ilya Ilyich Mechnikov (morphological microbe research) and two physicians, Jacques-Joseph Grancher (rabies) and Emile Roux (technical microbe research). One year after the inauguration of the Institut Pasteur, Roux set up the first course of microbiology ever taught in the world, then entitled "Cours de Microbie Technique" (Course of microbe research techniques). It became the model for numerous research centers around the world named "Pasteur Institutes."

The First Viennese School of Medicine, 1750–1800, was led by the Dutchman Gerard van Swieten (1700–1772), who aimed to put medicine on new scientific foundations—promoting unprejudiced clinical observation, botanical and chemical research, and introducing simple but powerful remedies. When the Vienna General Hospital opened in 1784, it at once became the world's largest hospital and physicians acquired a facility that gradually developed into the most important research centre. Progress ended with the Napoleonic wars and the government shutdown in 1819 of all liberal journals and schools; this caused a general return to traditionalism and eclecticism in medicine.

Vienna was the capital of a diverse empire and attracted not just Germans but Czechs, Hungarians, Jews, Poles and others to its world-class medical facilities. After 1820 the Second Viennese School of Medicine emerged with the contributions of physicians such as Carl Freiherr von Rokitansky, Josef Škoda, Ferdinand Ritter von Hebra, and Ignaz Philipp Semmelweis. Basic medical science expanded and specialization advanced. Furthermore, the first dermatology, eye, as well as ear, nose, and throat clinics in the world were founded in Vienna. The textbook of ophthalmologist Georg Joseph Beer (1763–1821) "Lehre von den Augenkrankheiten" combined practical research and philosophical speculations, and became the standard reference work for decades.

After 1871 Berlin, the capital of the new German Empire, became a leading center for medical research. Robert Koch (1843–1910) was a representative leader. He became famous for isolating "Bacillus anthracis" (1877), the "Tuberculosis bacillus" (1882) and "Vibrio cholerae" (1883) and for his development of Koch's postulates. He was awarded the Nobel Prize in Physiology or Medicine in 1905 for his tuberculosis findings. Koch is one of the founders of microbiology, inspiring such major figures as Paul Ehrlich and Gerhard Domagk.

In the American Civil War (1861–65), as was typical of the 19th century, more soldiers died of disease than in battle, and even larger numbers were temporarily incapacitated by wounds, disease and accidents. Conditions were poor in the Confederacy, where doctors and medical supplies were in short supply. The war had a dramatic long-term impact on medicine in the U.S., from surgical technique to hospitals to nursing and to research facilities. Weapon development -particularly the appearance of Springfield Model 1861, mass-produced and much more accurate than muskets led to generals underestimating the risks of long range rifle fire; risks exemplified in the death of John Sedgwick and the disastrous Pickett's Charge. The rifles could shatter bone forcing amputation and longer ranges meant casualties were sometimes not quickly found. Evacuation of the wounded from Second Battle of Bull Run took a week. As in earlier wars, untreated casualties sometimes survived unexpectedly due to maggots debriding the wound -an observation which led to the surgical use of maggots -still a useful method in the absence of effective antibiotics.

The hygiene of the training and field camps was poor, especially at the beginning of the war when men who had seldom been far from home were brought together for training with thousands of strangers. First came epidemics of the childhood diseases of chicken pox, mumps, whooping cough, and, especially, measles. Operations in the South meant a dangerous and new disease environment, bringing diarrhea, dysentery, typhoid fever, and malaria. There were no antibiotics, so the surgeons prescribed coffee, whiskey, and quinine. Harsh weather, bad water, inadequate shelter in winter quarters, poor policing of camps, and dirty camp hospitals took their toll.

This was a common scenario in wars from time immemorial, and conditions faced by the Confederate army were even worse. The Union responded by building army hospitals in every state. What was different in the Union was the emergence of skilled, well-funded medical organizers who took proactive action, especially in the much enlarged United States Army Medical Department, and the United States Sanitary Commission, a new private agency. Numerous other new agencies also targeted the medical and morale needs of soldiers, including the United States Christian Commission as well as smaller private agencies.

The U.S. Army learned many lessons and in August 1886, it established the Hospital Corps.

A major breakthrough in epidemiology came with the introduction of statistical maps and graphs. They allowed careful analysis of seasonality issues in disease incidents, and the maps allowed public health officials to identify critical loci for the dissemination of disease. John Snow in London developed the methods. In 1849, he observed that the symptoms of cholera, which had already claimed around 500 lives within a month, were vomiting and diarrhoea. He concluded that the source of contamination must be through ingestion, rather than inhalation as was previously thought. It was this insight that resulted in the removal of The Pump On Broad Street, after which deaths from cholera plummeted afterwards. English nurse Florence Nightingale pioneered analysis of large amounts of statistical data, using graphs and tables, regarding the condition of thousands of patients in the Crimean War to evaluate the efficacy of hospital services. Her methods proved convincing and led to reforms in military and civilian hospitals, usually with the full support of the government.

By the late 19th and early 20th century English statisticians led by Francis Galton, Karl Pearson and Ronald Fisher developed the mathematical tools such as correlations and hypothesis tests that made possible much more sophisticated analysis of statistical data.

During the U.S. Civil War the Sanitary Commission collected enormous amounts of statistical data, and opened up the problems of storing information for fast access and mechanically searching for data patterns. The pioneer was John Shaw Billings (1838–1913). A senior surgeon in the war, Billings built the Library of the Surgeon General's Office (now the National Library of Medicine), the centerpiece of modern medical information systems. Billings figured out how to mechanically analyze medical and demographic data by turning facts into numbers and punching the numbers onto cardboard cards that could be sorted and counted by machine. The applications were developed by his assistant Herman Hollerith; Hollerith invented the punch card and counter-sorter system that dominated statistical data manipulation until the 1970s. Hollerith's company became International Business Machines (IBM) in 1911.

Johns Hopkins Hospital, founded in 1889, originated several modern medical practices, including residency and rounds.

European ideas of modern medicine were spread widely through the world by medical missionaries, and the dissemination of textbooks. Japanese elites enthusiastically embraced Western medicine after the Meiji Restoration of the 1860s. However they had been prepared by their knowledge of the Dutch and German medicine, for they had some contact with Europe through the Dutch. Highly influential was the 1765 edition of Hendrik van Deventer's pioneer work "Nieuw Ligt" ("A New Light") on Japanese obstetrics, especially on Katakura Kakuryo's publication in 1799 of "Sanka Hatsumo" ("Enlightenment of Obstetrics"). A cadre of Japanese physicians began to interact with Dutch doctors, who introduced smallpox vaccinations. By 1820 Japanese ranpô medical practitioners not only translated Dutch medical texts, they integrated their readings with clinical diagnoses. These men became leaders of the modernization of medicine in their country. They broke from Japanese traditions of closed medical fraternities and adopted the European approach of an open community of collaboration based on expertise in the latest scientific methods.

Kitasato Shibasaburō (1853–1931) studied bacteriology in Germany under Robert Koch. In 1891 he founded the Institute of Infectious Diseases in Tokyo, which introduced the study of bacteriology to Japan. He and French researcher Alexandre Yersin went to Hong Kong in 1894, where; Kitasato confirmed Yersin's discovery that the bacterium "Yersinia pestis" is the agent of the plague. In 1897 he isolates and described the organism that caused dysentery. He became the first dean of medicine at Keio University, and the first president of the Japan Medical Association.

Japanese physicians immediately recognized the values of X-Rays. They were able to purchase the equipment locally from the Shimadzu Company, which developed, manufactured, marketed, and distributed X-Ray machines after 1900. Japan not only adopted German methods of public health in the home islands, but implemented them in its colonies, especially Korea and Taiwan, and after 1931 in Manchuria. A heavy investment in sanitation resulted in a dramatic increase of life expectancy.

Until the nineteenth century, the care of the insane was largely a communal and family responsibility rather than a medical one. The vast majority of the mentally ill were treated in domestic contexts with only the most unmanageable or burdensome likely to be institutionally confined. This situation was transformed radically from the late eighteenth century as, amid changing cultural conceptions of madness, a new-found optimism in the curability of insanity within the asylum setting emerged. Increasingly, lunacy was perceived less as a physiological condition than as a mental and moral one to which the correct response was persuasion, aimed at inculcating internal restraint, rather than external coercion. This new therapeutic sensibility, referred to as moral treatment, was epitomised in French physician Philippe Pinel's quasi-mythological unchaining of the lunatics of the Bicêtre Hospital in Paris and realised in an institutional setting with the foundation in 1796 of the Quaker-run York Retreat in England.

From the early nineteenth century, as lay-led lunacy reform movements gained in influence, ever more state governments in the West extended their authority and responsibility over the mentally ill. Small-scale asylums, conceived as instruments to reshape both the mind and behaviour of the disturbed, proliferated across these regions. By the 1830s, moral treatment, together with the asylum itself, became increasingly medicalised and asylum doctors began to establish a distinct medical identity with the establishment in the 1840s of associations for their members in France, Germany, the United Kingdom and America, together with the founding of medico-psychological journals. Medical optimism in the capacity of the asylum to cure insanity soured by the close of the nineteenth century as the growth of the asylum population far outstripped that of the general population. Processes of long-term institutional segregation, allowing for the psychiatric conceptualisation of the natural course of mental illness, supported the perspective that the insane were a distinct population, subject to mental pathologies stemming from specific medical causes. As degeneration theory grew in influence from the mid-nineteenth century, heredity was seen as the central causal element in chronic mental illness, and, with national asylum systems overcrowded and insanity apparently undergoing an inexorable rise, the focus of psychiatric therapeutics shifted from a concern with treating the individual to maintaining the racial and biological health of national populations.

Emil Kraepelin (1856–1926) introduced new medical categories of mental illness, which eventually came into psychiatric usage despite their basis in behavior rather than pathology or underlying cause. Shell shock among frontline soldiers exposed to heavy artillery bombardment was first diagnosed by British Army doctors in 1915. By 1916, similar symptoms were also noted in soldiers not exposed to explosive shocks, leading to questions as to whether the disorder was physical or psychiatric. In the 1920s surrealist opposition to psychiatry was expressed in a number of surrealist publications. In the 1930s several controversial medical practices were introduced including inducing seizures (by electroshock, insulin or other drugs) or cutting parts of the brain apart (leucotomy or lobotomy). Both came into widespread use by psychiatry, but there were grave concerns and much opposition on grounds of basic morality, harmful effects, or misuse.

In the 1950s new psychiatric drugs, notably the antipsychotic chlorpromazine, were designed in laboratories and slowly came into preferred use. Although often accepted as an advance in some ways, there was some opposition, due to serious adverse effects such as tardive dyskinesia. Patients often opposed psychiatry and refused or stopped taking the drugs when not subject to psychiatric control. There was also increasing opposition to the use of psychiatric hospitals, and attempts to move people back into the community on a collaborative user-led group approach ("therapeutic communities") not controlled by psychiatry. Campaigns against masturbation were done in the Victorian era and elsewhere. Lobotomy was used until the 1970s to treat schizophrenia. This was denounced by the anti-psychiatric movement in the 1960s and later.

The ABO blood group system was discovered in 1901, and the Rhesus blood group system in 1937, facilitating blood transfusion.

During the 20th century, large-scale wars were attended with medics and mobile hospital units which developed advanced techniques for healing massive injuries and controlling infections rampant in battlefield conditions. During the Mexican Revolution (1910–1920), General Pancho Villa organized hospital trains for wounded soldiers. Boxcars marked "Servicio Sanitario" ("sanitary service") were re-purposed as surgical operating theaters and areas for recuperation, and staffed by up to 40 Mexican and U.S. physicians. Severely wounded soldiers were shuttled back to base hospitals. Canadian physician Norman Bethune, M.D. developed a mobile blood-transfusion service for frontline operations in the Spanish Civil War (1936–1939), but ironically, he himself died of blood poisoning.
Thousands of scarred troops provided the need for improved prosthetic limbs and expanded techniques in plastic surgery or reconstructive surgery. Those practices were combined to broaden cosmetic surgery and other forms of elective surgery.

During the second World War, Alexis Carrel and Henry Dakin developed the Carrel-Dakin method of treating wounds with an irrigation, Dakin's solution, a germicide which helped prevent gangrene.

The War spurred the usage of Roentgen's X-ray, and the electrocardiograph, for the monitoring of internal bodily functions. This was followed in the inter-war period by the development of the first anti-bacterial agents such as the sulpha antibiotics.

Public health measures became particularly important during the 1918 flu pandemic, which killed at least 50 million people around the world. It became an important case study in epidemiology. Bristow shows there was a gendered response of health caregivers to the pandemic in the United States. Male doctors were unable to cure the patients, and they felt like failures. Women nurses also saw their patients die, but they took pride in their success in fulfilling their professional role of caring for, ministering, comforting, and easing the last hours of their patients, and helping the families of the patients cope as well.

From 1917 to 1923, the American Red Cross moved into Europe with a battery of long-term child health projects. It built and operated hospitals and clinics, and organized antituberculosis and antityphus campaigns. A high priority involved child health programs such as clinics, better baby shows, playgrounds, fresh air camps, and courses for women on infant hygiene. Hundreds of U.S. doctors, nurses, and welfare professionals administered these programs, which aimed to reform the health of European youth and to reshape European public health and welfare along American lines.

The advances in medicine made a dramatic difference for Allied troops, while the Germans and especially the Japanese and Chinese suffered from a severe lack of newer medicines, techniques and facilities. Harrison finds that the chances of recovery for a badly wounded British infantryman were as much as 25 times better than in the First World War. The reason was that:

Unethical human subject research, and killing of patients with disabilities, peaked during the Nazi era, with Nazi human experimentation and Aktion T4 during the Holocaust as the most significant examples. Many of the details of these and related events were the focus of the Doctors' Trial. Subsequently, principles of medical ethics, such as the Nuremberg Code, were introduced to prevent a recurrence of such atrocities. After 1937, the Japanese Army established programs of biological warfare in China. In Unit 731, Japanese doctors and research scientists conducted large numbers of vivisections and experiments on human beings, mostly Chinese victims.

Starting in World War II, DDT was used as insecticide to combat insect vectors carrying malaria, which was endemic in most tropical regions of the world. The first goal was to protect soldiers, but it was widely adopted as a public health device. In Liberia, for example, the United States had large military operations during the war and the U.S. Public Health Service began the use of DDT for indoor residual spraying (IRS) and as a larvicide, with the goal of controlling malaria in Monrovia, the Liberian capital. In the early 1950s, the project was expanded to nearby villages. In 1953, the World Health Organization (WHO) launched an antimalaria program in parts of Liberia as a pilot project to determine the feasibility of malaria eradication in tropical Africa. However these projects encountered a spate of difficulties that foreshadowed the general retreat from malaria eradication efforts across tropical Africa by the mid-1960s.

The World Health Organization was founded in 1948 as a United Nations agency to improve global health. In most of the world, life expectancy has improved since then, and was about 67 years , and well above 80 years in some countries. Eradication of infectious diseases is an international effort, and several new vaccines have been developed during the post-war years, against infections such as measles, mumps, several strains of influenza and human papilloma virus. The long-known vaccine against Smallpox finally eradicated the disease in the 1970s, and Rinderpest was wiped out in 2011. Eradication of polio is underway. Tissue culture is important for development of vaccines. Though the early success of antiviral vaccines and antibacterial drugs, antiviral drugs were not introduced until the 1970s. Through the WHO, the international community has developed a response protocol against epidemics, displayed during the SARS epidemic in 2003, the Influenza A virus subtype H5N1 from 2004, the Ebola virus epidemic in West Africa and onwards.

As infectious diseases have become less lethal, and the most common causes of death in developed countries are now tumors and cardiovascular diseases, these conditions have received increased attention in medical research. Tobacco smoking as a cause of lung cancer was first researched in the 1920s, but was not widely supported by publications until the 1950s. Cancer treatment has been developed with radiotherapy, chemotherapy and surgical oncology.

Oral rehydration therapy has been extensively used since the 1970s to treat cholera and other diarrhea-inducing infections.

The sexual revolution included taboo-breaking research in human sexuality such as the 1948 and 1953 Kinsey reports, invention of hormonal contraception, and the normalization of abortion and homosexuality in many countries. Family planning has promoted a demographic transition in most of the world. With threatening sexually transmitted infections, not least HIV, use of barrier contraception has become imperative. The struggle against HIV has improved antiretroviral treatments.

X-ray imaging was the first kind of medical imaging, and later ultrasonic imaging, CT scanning, MR scanning and other imaging methods became available.

Genetics have advanced with the discovery of the DNA molecule, genetic mapping and gene therapy. Stem cell research took off in the 2000s (decade), with stem cell therapy as a promising method.

Evidence-based medicine is a modern concept, not introduced to literature until the 1990s.

Prosthetics have improved. In 1958, Arne Larsson in Sweden became the first patient to depend on an artificial cardiac pacemaker. He died in 2001 at age 86, having outlived its inventor, the surgeon, and 26 pacemakers. Lightweight materials as well as neural prosthetics emerged in the end of the 20th century.

Cardiac surgery was revolutionized in 1948 as open-heart surgery was introduced for the first time since 1925.

In 1954 Joseph Murray, J. Hartwell Harrison and others accomplished the first kidney transplantation. Transplantations of other organs, such as heart, liver and pancreas, were also introduced during the later 20th century. The first partial face transplant was performed in 2005, and the first full one in 2010. By the end of the 20th century, microtechnology had been used to create tiny robotic devices to assist microsurgery using micro-video and fiber-optic cameras to view internal tissues during surgery with minimally invasive practices.

Laparoscopic surgery was broadly introduced in the 1990s. Natural orifice surgery has followed. Remote surgery is another recent development, with the transatlantic Lindbergh operation in 2001 as a groundbreaking example.









</doc>
<doc id="14196" url="https://en.wikipedia.org/wiki?curid=14196" title="Hamoaze">
Hamoaze

The Hamoaze (; ) is an estuarine stretch of the tidal River Tamar, between its confluence with the River Lynher and Plymouth Sound, England.

The name first appears as "ryver of Hamose" in 1588 and it originally most likely applied just to a creek of the estuary that led up to the manor of Ham, north of the present-day Devonport Dockyard. The name evidently later came to be used for the estuary's main channel. The "ose" element possibly derives from Old English "wāse" meaning 'mud' (as in 'ooze') – the creek consisting of mud-banks at low tide.

The Hamoaze flows past Devonport Dockyard, which is one of three major bases of the Royal Navy today. The presence of large numbers of small watercraft are a challenge and hazard to the warships using the naval base and dockyard. Navigation on the waterway is controlled by the Queen's Harbour Master for Plymouth.

Settlements on the banks of the Hamoaze are Saltash, Wilcove, Torpoint and Cremyll in Cornwall, as well as Devonport and Plymouth in Devon.

Two regular ferry services crossing the Hamoaze exist: the Torpoint Ferry (a chain ferry that takes vehicles) and the Cremyll Ferry (passengers and cyclists only).

The Hamoaze has a street in Torpoint named after it.



</doc>
<doc id="14197" url="https://en.wikipedia.org/wiki?curid=14197" title="Hanover">
Hanover

Hanover (; ; ) is the capital and largest city of the German state of Lower Saxony. Its 535,061 (2017) inhabitants make it the 13th-largest city in Germany as well as the third-largest city in Northern Germany after Hamburg and Bremen. Hanover's urban area comprises the towns of Garbsen, Langenhagen and Laatzen and has a population of about 791,000 (2018). The Hanover Region has approximately 1.16 million inhabitants (2019).

The city lies at the confluence of the River Leine (progression: ) and its tributary Ihme, in the south of the North German Plain, and is the largest city in the Hannover–Braunschweig–Göttingen–Wolfsburg Metropolitan Region. It is the fifth-largest city in the Low German dialect area after Hamburg, Dortmund, Essen and Bremen.

Before it became the capital of Lower Saxony in 1946, Hanover was the capital of the Principality of Calenberg (1636–1692), the Electorate of Hanover (1692–1814), the Kingdom of Hanover (1814–1866), the Province of Hanover of the Kingdom of Prussia (1868–1918), the Province of Hanover of the Free State of Prussia (1918–1946) and of the State of Hanover (1946). From 1714 to 1837 Hanover was by personal union the family seat of the Hanoverian Kings of the United Kingdom of Great Britain and Ireland, under their title of the dukes of Brunswick-Lüneburg (later described as the Elector of Hanover).

The city is a major crossing point of railway lines and motorways (Autobahnen), connecting European main lines in both the east-west (Berlin–Ruhr area/Düsseldorf/Cologne) and north-south (Hamburg–Frankfurt/Stuttgart/Munich) directions. Hannover Airport lies north of the city, in Langenhagen, and is Germany's ninth-busiest airport. The city's most notable institutes of higher education are the Hannover Medical School with its university hospital (Klinikum der Medizinischen Hochschule Hannover) and the Leibniz University Hannover.

The Hanover fairground, owing to numerous extensions, especially for the Expo 2000, is the largest in the world. Hanover hosts annual commercial trade fairs such as the Hanover Fair and up to 2018 the CeBIT. The IAA Commercial Vehicles show takes place every two years. It is the world's leading trade show for transport, logistics and mobility. Every year Hanover hosts the Schützenfest Hannover, the world's largest marksmen's festival, and the Oktoberfest Hannover.

'Hanover' is the traditional English spelling. The German spelling (with a double n) is becoming more popular in English; recent editions of encyclopedias prefer the German spelling, and the local government uses the German spelling on English websites. The English pronunciation, with stress on the first syllable, is applied to both the German and English spellings, which is different from German pronunciation, with stress on the second syllable and a long second vowel. The traditional English spelling is still used in historical contexts, especially when referring to the British House of Hanover.

Hanover was founded in medieval times on the east bank of the River Leine. Its original name "Honovere" may mean 'high (river)bank', though this is debated (cf. "das Hohe Ufer"). Hanover was a small village of ferrymen and fishermen that became a comparatively large town in the 13th century, receiving town privileges in 1241, owing to its position at a natural crossroads. As overland travel was relatively difficult its position on the upper navigable reaches of the river helped it to grow by increasing trade. It was connected to the Hanseatic League city of Bremen by the Leine and was situated near the southern edge of the wide North German Plain and north-west of the Harz mountains, so that east-west traffic such as mule trains passed through it. Hanover was thus a gateway to the Rhine, Ruhr and Saar river valleys, their industrial areas which grew up to the southwest and the plains regions to the east and north, for overland traffic skirting the Harz between the Low Countries and Saxony or Thuringia.

In the 14th century the main churches of Hanover were built, as well as a city wall with three city gates. The beginning of industrialization in Germany led to trade in iron and silver from the northern Harz Mountains, which increased the city's importance.

In 1636 George, Duke of Brunswick-Lüneburg, ruler of the Brunswick-Lüneburg principality of Calenberg, moved his residence to Hanover. The Dukes of Brunswick-Lüneburg were elevated by the Holy Roman Emperor to the rank of Prince-Elector in 1692 and this elevation was confirmed by the Imperial Diet in 1708. Thus the principality was upgraded to the Electorate of Hanover, colloquially known as the Electorate of Hanover after Calenberg's capital (see also: House of Hanover). Its Electors later become monarchs of Great Britain (and from 1801 of the United Kingdom of Great Britain and Ireland). The first of these was George I Louis, who acceded to the British throne in 1714. The last British monarch who reigned in Hanover was William IV. Semi-Salic law, which required succession by the male line if possible, forbade the accession of Queen Victoria in Hanover. As a male-line descendant of George I, Queen Victoria was herself a member of the House of Hanover. Her descendants, however, bore her husband's titular name of Saxe-Coburg-Gotha. Three kings of Great Britain, or the United Kingdom, were concurrently also Electoral Princes of Hanover.

During the time of the personal union of the crowns of the United Kingdom and Hanover (1714–1837) the monarchs rarely visited the city. In fact during the reigns of the final three joint rulers (1760–1837) there was only one short visit, by George IV in 1821. From 1816 to 1837 Viceroy Adolphus represented the monarch in Hanover.

During the Seven Years' War the Battle of Hastenbeck was fought near the city on 26 July 1757. The French army defeated the Hanoverian Army of Observation, leading to the city's occupation as part of the Invasion of Hanover. It was recaptured by Anglo-German forces led by Ferdinand of Brunswick the following year.

After Napoleon imposed the Convention of Artlenburg (Convention of the Elbe) on July 5, 1803, about 35,000 French soldiers occupied Hanover. The Convention also required disbanding the army of Hanover. However, George III did not recognize the Convention of the Elbe. This resulted in a great number of soldiers from Hanover eventually emigrating to Great Britain, where the King's German Legion was formed. It was only troops from Hanover and Brunswick that consistently opposed France throughout the entire Napoleonic wars. The Legion later played an important role in the Peninsular War and the Battle of Waterloo in 1815. The Congress of Vienna in 1815 elevated the electorate to the Kingdom of Hanover. The capital town Hanover expanded to the western bank of the Leine and since then has grown considerably.

In 1837, the personal union of the United Kingdom and Hanover ended because William IV's heir in the United Kingdom was female (Queen Victoria). Hanover could be inherited only by male heirs. Thus, Hanover passed to William IV's brother, Ernest Augustus, and remained a kingdom until 1866, when it was annexed by Prussia during the Austro-Prussian war. Despite Hanover being expected to defeat Prussia at the Battle of Langensalza, Prussia employed Moltke the Elder's Kesselschlacht order of battle to instead destroy the Hanoverian army. The city of Hanover became the capital of the Prussian Province of Hanover. After the annexation, the people of Hanover generally opposed the Prussian government.

To Hanover's industry, however, the new connection with Prussia meant an improvement in business. The introduction of free trade promoted economic growth and led to the recovery of the Gründerzeit (the founders' era). Between 1879 and 1902 Hanover's population grew from 87,600 to 313,940. 

In 1842 the first horse railway was inaugurated, and from 1893 an electric tram was installed. In 1887 Hanover's Emile Berliner invented the record and the gramophone.

After 1937 the Lord Mayor and the state commissioners of Hanover were members of the NSDAP (Nazi party). A large Jewish population then existed in Hanover. In October 1938, 484 Hanoverian Jews of Polish origin were expelled to Poland, including the Grynszpan family. However, Poland refused to accept them, leaving them stranded at the border with thousands of other Polish-Jewish deportees, fed only intermittently by the Polish Red Cross and Jewish welfare organisations. The Grynszpans' son Herschel Grynszpan was in Paris at the time. When he learned of what was happening, he drove to the German embassy in Paris and shot the German diplomat Eduard Ernst vom Rath, who died shortly afterwards.

The Nazis took this act as a pretext to stage a nationwide pogrom known as Kristallnacht (9 November 1938). On that day, the synagogue of Hanover, designed in 1870 by Edwin Oppler in neo-romantic style, was burnt by the Nazis.

In September 1941, through the "Action Lauterbacher" plan, a ghettoisation of the remaining Hanoverian Jewish families began. Even before the Wannsee Conference, on 15 December 1941, the first Jews from Hanover were deported to Riga. A total of 2,400 people were deported, and very few survived. During the war seven concentration camps were constructed in Hanover, in which many Jews were confined. Of the approximately 4,800 Jews who had lived in Hannover in 1938, fewer than 100 were still in the city when troops of the United States Army arrived on 10 April 1945 to occupy Hanover at the end of the war. Today, a memorial at the Opera Square is a reminder of the persecution of the Jews in Hanover. 
After the war a large group of Orthodox Jewish survivors of the nearby Bergen-Belsen concentration camp settled in Hanover.
As an important railroad and road junction and production centre, Hanover was a major target for strategic bombing during World War II, including the Oil Campaign. Targets included the AFA (Stöcken), the Deurag-Nerag refinery (Misburg), the Continental plants (Vahrenwald and Limmer), the United light metal works (VLW) in Ricklingen and Laatzen (today Hanover fairground), the Hanover/Limmer rubber reclamation plant, the Hanomag factory (Linden) and the tank factory "M.N.H. Maschinenfabrik Niedersachsen" (Badenstedt). Residential areas were also targeted, and more than 6,000 civilians were killed by the Allied bombing raids. More than 90% of the city centre was destroyed in a total of 88 bombing raids. After the war, the Aegidienkirche was not rebuilt and its ruins were left as a war memorial.

The Allied ground advance into Germany reached Hanover in April 1945. The US 84th Infantry Division captured the city on 10 April 1945.

Hanover was in the British zone of occupation of Germany and became part of the new state (Land) of Lower Saxony in 1946.

Today Hanover is a Vice-President City of Mayors for Peace, an international mayoral organisation mobilising cities and citizens worldwide to abolish and eliminate nuclear weapons by the year 2020.

Hannover has an oceanic climate (Köppen: "Cfb") independent of the isotherm. Although the city is not on a coastal location, the predominant air masses are still from the ocean, unlike other places further east or south-central Germany.



One of Hanover's most famous sights is the "Royal Gardens of Herrenhausen". Its "Great Garden" is an important European baroque garden. The palace itself was largely destroyed by Allied bombing but has been reconstructed and reopened in 2013. Among the points of interest is the "Grotto". Its interior was designed by the French artist Niki de Saint Phalle). The Great Garden consists of several parts and contains Europe's highest garden fountain. The historic "Garden Theatre" hosted the musicals of the German rock musician Heinz Rudolf Kunze.

Also at Herrenhausen, the "Berggarten" is a botanical garden with the most varied collection of orchids in Europe. Some points of interest are the "Tropical House", the "Cactus House", the "Canary House" and the "Orchid House", and free-flying birds and butterflies. Near the entrance to the Berggarten is the historic "Library Pavillon". The "Mausoleum" of the Guelphs is also located in the Berggarten. Like the Great Garden, the Berggarten also consists of several parts, for example the "Paradies" and the "Prairie Garden". The "Georgengarten" is an English landscape garden. The "Leibniz Temple" and the "Georgen Palace" are two points of interest there.

The landmark of Hanover is the New Town Hall ("Neues Rathaus"). Inside the building are four scale models of the city. A worldwide unique diagonal/arch elevator goes up the large dome at a 17 degree angle to an observation deck.

The "Hanover Zoo" received the Park Scout Award for the fourth year running in 2009/10, placing it among the best zoos in Germany. The zoo consists of several theme areas: Sambesi, Meyers Farm, Gorilla-Mountain, Jungle-Palace, and Mullewapp. Some smaller areas are Australia, the wooded area for wolves, and the so-called swimming area with many seabirds. There is also a tropical house, a jungle house, and a show arena. The new Canadian-themed area, Yukon Bay, opened in 2010. In 2010 the Hanover Zoo had over 1.6 million visitors. There is also the "Sea Life Centre Hanover", which is the first tropical aquarium in Germany.

Another point of interest is the "Old Town". In the centre are the large Marktkirche (Church St. Georgii et Jacobi, preaching venue of the bishop of the Lutheran Landeskirche Hannovers) and the "Old Town Hall". Nearby are the "Leibniz House", the "Nolte House", and the "Beguine Tower". The "Kreuz-Church-Quarter" around the "Kreuz Church" contains many little lanes. Nearby is the old royal sports hall, now called the "Ballhof" theatre. On the edge of the Old Town are the "Market Hall", the "Leine Palace", and the ruin of the "Aegidien Church" which is now a monument to the victims of war and violence. Through the "Marstall Gate" the bank of the river "Leine" can be reached; the "Nanas" of Niki de Saint Phalle are located here. They are part of the "Mile of Sculptures", which starts from Trammplatz, leads along the river bank, crosses Königsworther Square, and ends at the entrance of the Georgengarten. Near the Old Town is the district of Calenberger Neustadt where the Catholic Basilica Minor of "St. Clemens", the "Reformed Church" and the Lutheran Neustädter Hof- und Stadtkirche St. Johannis are located.

Some other popular sights are the "Waterloo Column", the "Laves House", the "Wangenheim Palace", the "Lower Saxony State Archives", the "Hanover Playhouse", the "Kröpcke Clock", the "Anzeiger Tower Block", the "Administration Building of the NORD/LB", the "Cupola Hall" of the Congress Centre, the "Lower Saxony Stock", the "Ministry of Finance", the "Garten Church", the "Luther Church", the "Gehry Tower" (designed by the American architect Frank O. Gehry), the specially designed "Bus Stops", the "Opera House", "the Central Station", the "Maschsee" lake and the city forest "Eilenriede", which is one of the largest of its kind in Europe. With around 40 parks, forests and gardens, a couple of lakes, two rivers and one canal, Hanover offers a large variety of leisure activities.

Since 2007 the historic "Leibniz Letters", which can be viewed in the "Gottfried Wilhelm Leibniz Library", are on UNESCO's Memory of the World Register.

Outside the city centre is the "EXPO-Park", the former site of EXPO 2000. Some points of interest are the "Planet M.", the former "German Pavillon", some nations' vacant pavilions, the "Expowale", the "EXPO-Plaza" and the "EXPO-Gardens" (Parc Agricole, EXPO-Park South and the Gardens of change). The fairground can be reached by the "Exponale", one of the largest pedestrian bridges in Europe.

The "Hanover fairground" is the largest exhibition centre in the world.
It provides 496,000 square metres of covered indoor space, 58,000 square metres of open-air space, 27 halls and pavilions. Many of the Exhibition Centre's halls are architectural highlights. Furthermore, it offers the Convention Center with its 35 function rooms, glassed-in areas between halls, grassy park-like recreation zones and its own heliport. Two important sights on the fairground are the "Hermes Tower" (88.8 metres high) and the "EXPO Roof", the largest wooden roof in the world.

In the district of Anderten is the "European Cheese Centre", the only Cheese Experience Centre in Europe. Another tourist sight in Anderten is the "Hindenburg Lock", which was the biggest lock in Europe at the time of its construction in 1928. The "Tiergarten" (literally the "animals' garden") in the district of Kirchrode is a large forest originally used for deer and other game for the king's table.

In the district of Groß-Buchholz the 282-metre-high "Telemax" is located, which is the tallest building in Lower Saxony and the highest television tower in Northern Germany. Some other notable towers are the "VW-Tower" in the city centre and the old towers of the former middle-age defence belt: "Döhrener Tower", "Lister Tower" and the "Horse Tower".

The 36 most important sights of the city centre are connected with a -long red line, which is painted on the pavement. This so-called "Red Thread" marks out a walk that starts at the Tourist Information Office and ends on the Ernst-August-Square in front of the central station. There is also a guided sightseeing-bus tour through the city.

Hanover is headquarters for several Protestant organizations, including the World Communion of Reformed Churches, the Evangelical Church in Germany, the Reformed Alliance, the United Evangelical Lutheran Church of Germany, and the Independent Evangelical-Lutheran Church.

In 2015, 31.1% of the population were Protestant and 13.4% were Roman Catholic. The majority 55.5% were irreligious or other faith.

The Historisches Museum Hannover (Historic museum) describes the history of Hanover, from the medieval settlement "Honovere" to the world-famous Exhibition City of today. The museum focuses on the period from 1714 to 1834 when Hanover had a strong relationship with the British royal house.

With more than 4,000 members, the Kestnergesellschaft is the largest art society in Germany. The museum hosts exhibitions from classical modernist art to contemporary art. One big focus is put on film, video, contemporary music and architecture, room installments and big presentations of contemporary paintings, sculptures and video art.

The Kestner-Museum is located in the "House of 5.000 windows". The museum is named after August Kestner and exhibits 6,000 years of applied art in four areas: Ancient cultures, ancient Egypt, applied art and a valuable collection of historic coins.

The KUBUS is a forum for contemporary art. It features mostly exhibitions and projects of famous and important artists from Hanover.

The Kunstverein Hannover (Art Society Hanover) shows contemporary art and was established in 1832 as one of the first art societies in Germany. It is located in the "Künstlerhaus" (House of artists). There are around 7 international monografic and thematic Exhibitions in one year.

The Landesmuseum Hannover is the largest museum in Hanover. The art gallery shows European art from the 11th to the 20th century, the nature department shows the zoology, geology, botanic, geology and a "vivarium" with fish, insects, reptiles and amphibians. The primeval department shows the primeval history of Lower Saxony, and the folklore department shows cultures from all over the world.

The Sprengel Museum shows the art of the 20th century. It is one of the most notable art museums in Germany. The focus is put on the classical modernist art with the collection of "Kurt Schwitters", works of German expressionism, and French cubism, the cabinet of abstracts, the graphics and the department of photography and media. Furthermore, the museum shows the famous works of the French artist Niki de Saint-Phalle.

The Theatre Museum shows an exhibition of the history of the theatre in Hanover from the 17th century up to now: opera, concert, drama and ballet. The museum also hosts several touring exhibitions during the year.

The Wilhelm Busch Museum is the "German Museum of Caricature and Critical Graphic Arts". The collection of the works of Wilhelm Busch and the extensive collection of cartoons and critical graphics is unique in Germany. Furthermore, the museum hosts several exhibitions of national and international artists during the year.

A cabinet of coins is the Münzkabinett der TUI-AG. The "Polizeigeschichtliche Sammlung Niedersachsen" is the largest police museum in Germany. Textiles from all over the world can be visited in the "Museum for textile art". The "EXPOseeum" is the museum of the world-exhibition "EXPO 2000 Hannover". Carpets and objects from the orient can be visited in the "Oriental Carpet Museum". The "Museum for the visually impaired" is a rarity in Germany, there is only one other of its kind in Berlin. The "Museum of veterinary medicine" is unique in Germany. The "Museum for Energy History" describes the 150 years old history of the application of energy. The "Heimat-Museum Ahlem" shows the history of the district of Ahlem. The "Mahn- und Gedenkstätte Ahlem" describes the history of the Jewish people in Hanover and the "Stiftung Ahlers Pro Arte / Kestner Pro Arte" shows modern art. Modern art is also the main topic of the "Kunsthalle Faust", the "Nord/LB Art Gallery" and of the "Foro Artistico / Eisfabrik".

Some leading art events in Hanover are the "Long Night of the Museums" and the "Zinnober Kunstvolkslauf" which features all the galleries in Hanover.

People who are interested in astronomy should visit the "Observatory Geschwister Herschel" on the Lindener Mountain or the small planetarium inside of the Bismarck School.

Around 40 theatres are located in Hanover. The "Opera House", the "Schauspielhaus" (Play House), the "Ballhof eins", the "Ballhof zwei" and the "Cumberlandsche Galerie" belong to the "Lower Saxony State Theatre". The "Theater am Aegi" is Hanover's big theatre for musicals, shows and guest performances. The "Neues Theater" (New Theatre) is the boulevard theatre of Hanover. The "Theater für Niedersachsen" is another big theatre in Hanover, which also has an own musical company. Some of the most important musical productions are the rock musicals of the German rock musician Heinz Rudolph Kunze, which take place at the "Garden-Theatre" in the Great Garden.

Some important theatre-events are the "Tanztheater International", the "Long Night of the Theatres", the "Festival Theaterformen" and the "International Competition for Choreographers".

Hanover's leading cabaret-stage is the "GOP Variety theatre" which is located in the "Georgs Palace". Some other famous cabaret-stages are the "Variety Marlene", the "Uhu-Theatre". the theatre "Die Hinterbühne", the "Rampenlich Variety" and the revue-stage "TAK". The most important cabaret event is the "Kleines Fest im Großen Garten" (Little Festival in the Great Garden) which is the most successful cabaret festival in Germany. It features artists from around the world. Some other important events are the "Calenberger Cabaret Weeks", the "Hanover Cabaret Festival" and the "Wintervariety".

Hanover has two symphony orchestras: The Lower Saxon State Orchestra Hanover and the NDR Radiophilharmonie (North German Radio Philharmonic Orchestra). Two notable choirs have their homes in Hanover: the Girls Choir Hanover (Mädchenchor Hannover) and the Knabenchor Hannover (Boys Choir Hanover).

There are/were two big international competitions for classical music in Hanover:

The rock bands Scorpions and Fury in the Slaughterhouse are originally from Hanover. Acclaimed DJ Mousse T also has his main recording studio in the area. Rick J. Jordan, member of the band Scooter was born here in 1968. Eurovision Song Contest winner of 2010, Lena, is also from Hanover.

Hannover 96 (nickname "Die Roten" or 'The Reds') is the top local football team that currently plays in the 2. Bundesliga. Home games are played at the HDI-Arena, which hosted matches in the 1974 and 2006 World Cups and the Euro 1988. Their reserve team Hannover 96 II plays in the fourth league. Their home games were played in the traditional Eilenriedestadium till they moved to the HDI Arena due to DFL directives. Arminia Hannover is another traditional soccer team in Hanover that has played in the first league for years and plays now in the Niedersachsen-West Liga (Lower Saxony League West). Home matches are played in the Rudolf-Kalweit-Stadium.

The Hannover Indians are the local ice hockey team. They play in the third tier. Their home games are played at the traditional Eisstadion am Pferdeturm. The Hannover Scorpions played in Hanover in Germany's top league until 2013 when they sold their license and moved to Langenhagen.

Hanover was one of the rugby union capitals in Germany. The first German rugby team was founded in Hanover in 1878. Hanover-based teams dominated the German rugby scene for a long time. DRC Hannover plays in the first division, and "SV Odin von 1905" as well as SG 78/08 Hannover play in the second division.

The first German fencing club was founded in Hanover in 1862. Today there are three additional fencing clubs in Hanover.

The Hannover Korbjäger are the city's top basketball team. They play their home games at the IGS Linden.

Hanover is a centre for water sports. Thanks to the Maschsee lake, the rivers Ihme and Leine and to the Mittellandkanal channel, Hanover hosts sailing schools, yacht schools, waterski clubs, rowing clubs, canoe clubs and paddle clubs. The water polo team WASPO W98 plays in the first division.

The Hannover Regents play in the third Bundesliga (baseball) division. The Hannover Grizzlies, Armina Spartans and Hannover Stampeders are the local American football teams.

The Hannover Marathon is the biggest running event in Hanover with more than 11,000 participants and usually around 200.000 spectators. Some other important running events are the Gilde Stadtstaffel (relay), the Sport-Check Nachtlauf (night-running), the Herrenhäuser Team-Challenge, the Hannoversche Firmenlauf (company running) and the Silvesterlauf (sylvester running).

Hanover also hosts an important international cycle race: The "Nacht von Hannover" (night of Hanover). The race takes place around the Market Hall.

The lake Maschsee hosts the International Dragon Boat Races and the Canoe Polo-Tournament. Many regattas take place during the year. "Head of the river Leine" on the river Leine is one of the biggest rowing regattas in Hanover. One of Germany's most successful dragon boat teams, the All Sports Team Hannover, which has won since its foundation in year 2000 more than 100 medals on national and international competitions, is doing practising on the Maschsee in the heart of Hannover. The All Sports Team has received the award "Team of the Year 2013" in Lower Saxony.

Some other important sport events are the Lower Saxony Beach Volleyball Tournament, the international horse show "German Classics" and the international ice hockey tournament Nations Cup.

Hanover is one of the leading exhibition cities in the world. It hosts more than 60 international and national exhibitions every year. The most popular ones are the "CeBIT", the "Hanover Fair", the "Domotex", the "Ligna", the "IAA Nutzfahrzeuge" and the "Agritechnica". Hanover also hosts a huge number of congresses and symposiums like the "International Symposium on Society and Resource Management."

Hanover is also host to the "Schützenfest Hannover," the largest marksmen's fun fair in the world which takes place once a year (late June to early July) (2014 − July 4th to the 13th). Founded in 1529, it consists of more than 260 rides and inns, five large beer tents and a big entertainment programme. The highlight of this fun fair is the long "Parade of the Marksmen" with more than 12,000 participants from all over the world, including around 5,000 marksmen, 128 bands, and more than 70 wagons, carriages, and other festival vehicles. This makes it the longest procession in Europe. Around 2 million people visit this fun fair every year. The landmark of this fun fair is the biggest transportable Ferris wheel in the world, about ( high).

Hanover also hosts one of the two largest spring festivals in Europe, with around 180 rides and inns, 2 large beer tents, and around 1.5 million visitors each year. The Oktoberfest Hannover is the second largest Oktoberfest in the world with around 160 rides and inns, two large beer tents and around 1 million visitors each year.

The "Maschsee Festival" takes place around the Maschsee Lake. Each year around 2 million visitors come to enjoy live music, comedy, cabaret, and much more. It is the largest Volksfest of its kind in Northern Germany. The Great Garden hosts every year the "International Fireworks Competition", and the "International Festival Weeks Herrenhausen," with music and cabaret performances. The "Carnival Procession" is around long and consists of 3.000 participants, around 30 festival vehicles and around 20 bands and takes place every year.

Other festivals include the Festival "Feuer und Flamme" (Fire and Flames), the "Gartenfestival" (Garden Festival), the "Herbstfestival" (Autumn Festival), the "Harley Days", the "Steintor Festival" (Steintor is a party area in the city centre) and the "Lister-Meile-Festival" (Lister Meile is a large pedestrian area).

Hanover also hosts food-oriented festivals including the "Wine Festival" and the "Gourmet Festival". It also hosts some special markets. The "Old Town Flea Market" is the oldest flea market in Germany and the "Market for Art and Trade" has a high reputation. Some other major markets include the "Christmas Markets of the City of Hanover" in the Old Town and city centre, and the Lister Meile.

The city's central station, Hannover Hauptbahnhof, is a hub of the German high-speed ICE network. It is the starting point of the Hanover-Würzburg high-speed rail line and also the central hub for the Hanover S-Bahn. It offers many international and national connections.

Hanover and its area is served by Hanover/Langenhagen International Airport (IATA code: HAJ; ICAO code: EDDV)

Hanover is also an important hub of Germany's Autobahn network; the junction of two major autobahns, the A2 and A7 is at "Kreuz Hannover-Ost", at the northeastern edge of the city.

Local autobahns are A 352 (a short cut between A7 [north] and A2 [west], also known as the "airport autobahn" because it passes "Hanover Airport") and the A 37.

The Schnellweg "(en: expressway)" system, a number of Bundesstraße roads, forms a structure loosely resembling a large ring road together with A2 and A7. The roads are B 3, B 6 and Bundesstraße 65|B 65, called Westschnellweg (B6 on the northern part, B3 on the southern part), Messeschnellweg (B3, becomes A37 near Burgdorf, crosses A2, becomes B3 again, changes to B6 at "Seelhorster Kreuz", then passes the Hanover fairground as B6 and becomes A37 again before merging into A7) and Südschnellweg (starts out as B65, becomes B3/B6/B65 upon crossing "Westschnellweg", then becomes B65 again at "Seelhorster Kreuz").

Hanover has an extensive Stadtbahn and bus system, operated by üstra. The city is famous for its designer buses and tramways, the TW 6000 and TW 2000 trams being the most well-known examples.

Cycle paths are very common in the city centre. At off-peak hours you are allowed to take your bike on a tram or bus.

Various industrial businesses are located in Hannover. The Volkswagen Commercial Vehicles Transporter (VWN) factory at Hannover-Stöcken is the biggest employer in the region and operates a large plant at the northern edge of town adjoining the Mittellandkanal and Motorway A2. Volkswagen shares a coal-burning power plant with a factory of German tire and automobile parts manufacturer Continental AG. Continental AG, founded in Hanover in 1871, is one of the city's major companies. Since 2008 a take-over has been in progress: the Schaeffler Group from Herzogenaurach (Bavaria) holds the majority of Continental's stock but were required due to the financial crisis to deposit the options as securities at banks.

The audio equipment company Sennheiser and the travel group TUI AG are both based in Hanover. Hanover is home to many insurance companies including Talanx, VHV Group, and Concordia Insurance. The major global reinsurance company Hannover Re also has its headquarters east of the city centre.

In 2012, the city generated a GDP of €29.5 billion, which is equivalent to €74,822 per employee. The gross value of production in 2012 was €26.4 billion, which is equivalent to €66,822 per employee.
Around 300,000 employees were counted in 2014. Of these, 189,000 had their primary residence in Hanover, while 164,892 commute into the city every day.

In 2014 the city was home to 34,198 businesses, of which 9,342 were registered in the German Trade Register and 24,856 counted as small businesses. Hence, more than half of the metropolitan area's businesses in the German Trade Register are located in Hanover (17,485 total).
Hannoverimpuls GMBH is a joint business development company from the city and region of Hannover. The company was founded in 2003 and supports the start-up, growth and relocation of businesses in the Hannover Region. The focus is on seven sectors, which stand for sustainable economic growth: Automotive, Energy Solutions, Information and Communications Technology, Life Sciences, Optical Technologies, Creative Industries and Production Engineering.

A range of programmes supports companies from the key industries in their expansion plans in Hannover or abroad. Three regional centres specifically promote international economic relations with Russia, India and Turkey.

The Leibniz University Hannover is the largest funded institution in Hanover for providing higher education to the students from around the world. Below are the names of the universities and some of the important schools, including newly opened Hannover Medical Research School in 2003 for attracting the students from biology background from around the world.

There are several universities in Hanover:
There is one University of Applied Science and Arts in Hanover:

The "Schulbiologiezentrum Hannover" maintains practical biology schools in four locations (Botanischer Schulgarten Burg, Freiluftschule Burg, Zooschule Hannover, and Botanischer Schulgarten Linden). The University of Veterinary Medicine Hanover also maintains its own botanical garden specializing in medicinal and poisonous plants, the Heil- und Giftpflanzengarten der Tierärztlichen Hochschule Hannover.

The following is a selection of famous Hanover-natives, personalities connected with the city and honorary citizens:

Hanover is twinned with:




</doc>
<doc id="14199" url="https://en.wikipedia.org/wiki?curid=14199" title="Handheld game console">
Handheld game console

A handheld game console, or simply handheld console, is a small, portable self-contained video game console with a built-in screen, game controls and speakers. Handheld game consoles are smaller than home video game consoles and contain the console, screen, speakers, and controls in one unit, allowing people to carry them and play them at any time or place.

In 1976, Mattel introduced the first handheld electronic game with the release of "Auto Race". Later, several companies—including Coleco and Milton Bradley—made their own single-game, lightweight table-top or handheld electronic game devices. The first handheld game console with interchangeable cartridges is the Milton Bradley Microvision in 1979. The first commercial successful handheld console was Merlin from 1978 which sold more than 5 million units. The first internet-enabled handheld console and the first with a touchscreen was the Game.com released by Tiger Electronics in 1997.

Nintendo is credited with popularizing the handheld console concept with the release of the Game Boy in 1989 and continues to dominate the handheld console market.

The origins of handheld game consoles are found in handheld and tabletop electronic game devices of the 1970s and early 1980s. These electronic devices are capable of playing only a single game, they fit in the palm of the hand or on a tabletop, and they may make use of a variety of video displays such as LED, VFD, or LCD. In 1978, handheld electronic games were described by "Popular Electronics" magazine as "nonvideo electronic games" and "non-TV games" as distinct from devices that required use of a television screen. Handheld electronic games, in turn, find their origins in the synthesis of previous handheld and tabletop electro-mechanical devices such as Waco's "Electronic Tic-Tac-Toe" (1972) Cragstan's "Periscope-Firing Range" (1951), and the emerging optoelectronic-display-driven calculator market of the early 1970s. This synthesis happened in 1976, when "Mattel began work on a line of calculator-sized sports games that became the world's first handheld electronic games. The project began when Michael Katz, Mattel's new product category marketing director, told the engineers in the electronics group to design a game the size of a calculator, using LED (light-emitting diode) technology."

The result was the 1976 release of "Auto Race". Followed by "Football" later in 1977, the two games were so successful that according to Katz, "these simple electronic handheld games turned into a '$400 million category.'" Mattel would later win the honor of being recognized by the industry for innovation in handheld game device displays. Soon, other manufacturers including Coleco, Parker Brothers, Milton Bradley, Entex, and Bandai began following up with their own tabletop and handheld electronic games.

In 1979 the LCD-based Microvision, designed by Smith Engineering and distributed by Milton-Bradley, became the first handheld game console and the first to use interchangeable game cartridges. The Microvision game "Cosmic Hunter" (1981) also introduced the concept of a directional pad on handheld gaming devices, and is operated by using the thumb to manipulate the on-screen character in any of four directions.

In 1979, Gunpei Yokoi, traveling on a bullet train, saw a bored businessman playing with an LCD calculator by pressing the buttons. Yokoi then thought of an idea for a watch that doubled as a miniature game machine for killing time. Starting in 1980, Nintendo began to release a series of electronic games designed by Yokoi called the Game & Watch games. Taking advantage of the technology used in the credit-card-sized calculators that had appeared on the market, Yokoi designed the series of LCD-based games to include a digital time display in the corner of the screen. For later, more complicated Game & Watch games, Yokoi invented a cross shaped directional pad or "D-pad" for control of on-screen characters. Yokoi also included his directional pad on the NES controllers, and the cross-shaped thumb controller soon became standard on game console controllers and ubiquitous across the video game industry since. When Yokoi began designing Nintendo's first handheld game console, he came up with a device that married the elements of his Game & Watch devices and the Famicom console, including both items' D-pad controller. The result was the Nintendo Game Boy.

In 1982, the Bandai LCD Solarpower was the first solar-powered gaming device. Some of its games, such as the horror-themed game "Terror House", features two LCD panels, one stacked on the other, for an early 3D effect. In 1983, Takara Tomy's Tomytronic 3D simulates 3D by having two LCD panels that were lit by external light through a window on top of the device, making it the first dedicated home video 3D hardware.

The late 1980s and early 1990s saw the beginnings of the modern day handheld game console industry, after the demise of the Microvision. As backlit LCD game consoles with color graphics consume a lot of power, they were not battery-friendly like the non-backlit original Game Boy whose monochrome graphics allowed longer battery life. By this point, rechargeable battery technology had not yet matured and so the more advanced game consoles of the time such as the Sega Game Gear and Atari Lynx did not have nearly as much success as the Game Boy.

Even though third-party rechargeable batteries were available for the battery-hungry alternatives to the Game Boy, these batteries employed a nickel-cadmium process and had to be completely discharged before being recharged to ensure maximum efficiency; lead-acid batteries could be used with automobile circuit limiters (cigarette lighter plug devices); but the batteries had mediocre portability. The later NiMH batteries, which do not share this requirement for maximum efficiency, were not released until the late 1990s, years after the Game Gear, Atari Lynx, and original Game Boy had been discontinued. During the time when technologically superior handhelds had strict technical limitations, batteries had a very low mAh rating since batteries with heavy power density were not yet available.

Modern game systems such as the Nintendo DS and PlayStation Portable have rechargeable Lithium-Ion batteries with proprietary shapes. Other seventh-generation consoles such as the GP2X use standard alkaline batteries. Because the mAh rating of alkaline batteries has increased since the 1990s, the power needed for handhelds like the GP2X may be supplied by relatively few batteries.

Nintendo released the Game Boy on April 21, 1989 (September 1990 for the UK). The design team headed by Gunpei Yokoi had also been responsible for the Game & Watch system, as well as the Nintendo Entertainment System games "Metroid" and "Kid Icarus". The Game Boy came under scrutiny by some industry critics, saying that the monochrome screen was too small, and the processing power was inadequate. The design team had felt that low initial cost and battery economy were more important concerns, and when compared to the Microvision, the Game Boy was a huge leap forward.

Yokoi recognized that the Game Boy needed a killer app—at least one game that would define the console, and persuade customers to buy it. In June 1988, Minoru Arakawa, then-CEO of Nintendo of America saw a demonstration of the game "Tetris" at a trade show. Nintendo purchased the rights for the game, and packaged it with the Game Boy system as a launch title. It was almost an immediate hit. By the end of the year more than a million units were sold in the US. As of March 31, 2005, the Game Boy and Game Boy Color combined to sell over 118 million units worldwide.

In 1987, Epyx created the Handy Game; a device that would turn into the Atari Lynx in 1989. It is the first color handheld console ever made, as well as the first with a backlit screen. It also features networking support with up to 17 other players, and advanced hardware that allows the zooming and scaling of sprites. The Lynx can also be turned upside down to accommodate left-handed players. However, all these features came at a very high price point, which drove consumers to seek cheaper alternatives. The Lynx is also very unwieldy, consumes batteries very quickly, and lacked the third-party support enjoyed by its competitors. Due to its high price, short battery life, production shortages, a dearth of compelling games, and Nintendo's aggressive marketing campaign, and despite a redesign in 1991, the Lynx became a commercial failure. Despite this, companies like Telegames helped to keep the system alive long past its commercial relevance, and when new owner Hasbro released the rights to develop for the public domain, independent developers like Songbird have managed to release new commercial games for the system every year until 2004's "Winter Games".

The TurboExpress is a portable version of the TurboGrafx, released in 1990 for $249.99 (the price was briefly raised to $299.99, soon dropped back to $249.99, and by 1992 it was $199.99). Its Japanese equivalent is the PC Engine GT.

It is the most advanced handheld of its time and can play all the TurboGrafx-16's games (which are on a small, credit-card sized media called HuCards). It has a 66 mm (2.6 in.) screen, the same as the original Game Boy, but in a much higher resolution, and can display 64 sprites at once, 16 per scanline, in 512 colors. Although the hardware can only handle 481 simultaneous colors. It has 8 kilobytes of RAM. The Turbo runs the HuC6820 CPU at 1.79 or 7.16 MHz.

The optional "TurboVision" TV tuner includes RCA audio/video input, allowing users to use TurboExpress as a video monitor. The "TurboLink" allowed two-player play. "Falcon", a flight simulator, included a "head-to-head" dogfight mode that can only be accessed via TurboLink. However, very few TG-16 games offered co-op play modes especially designed with the TurboExpress in mind.

The Bitcorp Gamate is the one of the first handheld game systems created in response to the Nintendo Game Boy. It was released in Asia in 1990 and distributed worldwide by 1991.

Like the Sega Game Gear, it was horizontal in orientation and like the Game Boy, required 4 AA batteries. Unlike many later Game Boy clones, its internal components were professionally assembled (no "glop-top" chips). Unfortunately the system's fatal flaw is its screen. Even by the standards of the day, its screen is rather difficult to use, suffering from similar motion blur problems that were common complaints with the first generation Game Boys. Likely because of this fact sales were quite poor, and Bitcorp closed by 1992. However, new games continued to be published for the Asian market, possibly as late as 1994. The total number of games released for the system remains unknown.

Gamate games were designed for stereo sound, but the console is only equipped with a mono speaker. 

The Game Gear is the third color handheld console, after the Lynx and the TurboExpress; produced by Sega. Released in Japan in 1990 and in North America and Europe in 1991, it is based on the Master System, which gave Sega the ability to quickly create Game Gear games from its large library of games for the Master System. While never reaching the level of success enjoyed by Nintendo, the Game Gear proved to be a fairly durable competitor, lasting longer than any other Game Boy rivals.

While the Game Gear is most frequently seen in black or navy blue, it was also released in a variety of additional colors: red, light blue, yellow, clear, and violet. All of these variations were released in small quantities and frequently only in the Asian market.

Following Sega's success with the Game Gear, they began development on a successor during the early 1990s, which was intended to feature a touchscreen interface, many years before the Nintendo DS. However, such a technology was very expensive at the time, and the handheld itself was estimated to have cost around $289 were it to be released. Sega eventually chose to shelve the idea and instead release the Genesis Nomad, a handheld version of the Genesis, as the successor.

The Watara Supervision was released in 1992 in an attempt to compete with the Nintendo Game Boy. The first model was designed very much like a Game Boy, but it is grey in color and has a slightly larger screen. The second model was made with a hinge across the center and can be bent slightly to provide greater comfort for the user. While the system did enjoy a modest degree of success, it never impacted the sales of Nintendo or Sega. The Supervision was redesigned a final time as "The Magnum". Released in limited quantities it was roughly equivalent to the Game Boy Pocket. It was available in three colors: yellow, green and grey. Watara designed many of the games themselves, but did receive some third party support, most notably from Sachen.

A TV adapter was available in both PAL and NTSC formats that could transfer the Supervision's black-and-white palette to 4 colors, similar in some regards to the Super Game Boy from Nintendo.

The Hartung Game Master is an obscure handheld released at an unknown point in the early 1990s. Its graphics were much lower than most of its contemporaries, similar in complexity to the Atari 2600. It was available in black, white, and purple, and was frequently rebranded by its distributors, such as Delplay, Videojet and Systema.
The exact number of games released is not known, but is likely around 20. The system most frequently turns up in Europe and Australia.

By this time, the lack of significant development in Nintendo's product line began allowing more advanced systems such as the Neo Geo Pocket Color and the WonderSwan Color to be developed.

The Nomad was released in October 1995 in North America only. The release was five years into the market span of the Genesis, with an existing library of more than 500 Genesis games. According to former Sega of America research and development head Joe Miller, the Nomad was not intended to be the Game Gear's replacement and believes that there was little planning from Sega of Japan for the new handheld. Sega was supporting five different consoles: Saturn, Genesis, Game Gear, Pico, and the Master System, as well as the Sega CD and 32X add-ons. In Japan, the Mega Drive had never been successful and the Saturn was more successful than Sony's PlayStation, so Sega Enterprises CEO Hayao Nakayama decided to focus on the Saturn. By 1999, the Nomad was being sold at less than a third of its original price.

The Game Boy Pocket is a redesigned version of the original Game Boy having the same features. It was released in 1996. Notably, this variation is smaller and lighter. It comes in seven different colors; red, yellow, green, black, clear, silver, blue, and pink. It has space for two AAA batteries, which provide approximately 10 hours of game play. The screen was changed to a true black-and-white display, rather than the "pea soup" monochromatic display of the original Game Boy. Although, like its predecessor, the Game Boy Pocket has no backlight to allow play in a darkened area, it did notably improve visibility and pixel response-time (mostly eliminating ghosting).

Another notable improvement over the original Game Boy is a black-and-white display screen, rather than the green-tinted display of the original Game Boy, that also featured improved response time for less blurring during motion. The first model of the Game Boy Pocket did not have an LED to show battery levels, but the feature was added due to public demand. The Game Boy Pocket was not a new software platform and played the same software as the original Game Boy model.

The Game.com (pronounced in TV commercials as "game com", not "game dot com", and not capitalized in marketing material) is a handheld game console released by Tiger Electronics in September 1997. It featured many new ideas for handheld consoles and was aimed at an older target audience, sporting PDA-style features and functions such as a touch screen and stylus. However, Tiger hoped it would also challenge Nintendo's Game Boy and gain a following among younger gamers too. Unlike other handheld game consoles, the first game.com consoles included two slots for game cartridges, which would not happen again until the Tapwave Zodiac, the DS and DS Lite, and could be connected to a 14.4 kbit/s modem. Later models had only a single cartridge slot.

The Game Boy Color (also referred to as GBC or CGB) is Nintendo's successor to the Game Boy and was released on October 21, 1998, in Japan and in November of the same year in the United States. It features a color screen, and is slightly bigger than the Game Boy Pocket. The processor is twice as fast as a Game Boy's and has twice as much memory. It also had an infrared communications port for wireless linking which did not appear in later versions of the Game Boy, such as the Game Boy Advance.

The Game Boy Color was a response to pressure from game developers for a new system, as they felt that the Game Boy, even in its latest incarnation, the Game Boy Pocket, was insufficient. The resulting product was backward compatible, a first for a handheld console system, and leveraged the large library of games and great installed base of the predecessor system. This became a major feature of the Game Boy line, since it allowed each new launch to begin with a significantly larger library than any of its competitors. As of March 31, 2005, the Game Boy and Game Boy Color combined to sell 118.69 million units worldwide.

The console is capable of displaying up to 56 different colors simultaneously on screen from its palette of 32,768, and can add basic four-color shading to games that had been developed for the original Game Boy. It can also give the sprites and backgrounds separate colors, for a total of more than four colors.

The Neo Geo Pocket Color (or NGPC) was released in 1999 in Japan, and later that year in the United States and Europe. It is a 16-bit color handheld game console designed by SNK, the maker of the Neo Geo home console and arcade machine. It came after SNK's original Neo Geo Pocket monochrome handheld, which debuted in 1998 in Japan.

In 2000 following SNK's purchase by Japanese Pachinko manufacturer Aruze, the Neo Geo Pocket Color was dropped from both the US and European markets, purportedly due to commercial failure.

The system seemed well on its way to being a success in the U.S. It was more successful than any Game Boy competitor since Sega's Game Gear, but was hurt by several factors, such as SNK's infamous lack of communication with third-party developers, and anticipation of the Game Boy Advance. The decision to ship U.S. games in cardboard boxes in a cost-cutting move rather than hard plastic cases that Japanese and European releases were shipped in may have also hurt US sales.

The WonderSwan Color is a handheld game console designed by Bandai. It was released on December 9, 2000, in Japan, Although the WonderSwan Color was slightly larger and heavier (7 mm and 2 g) compared to the original WonderSwan, the color version featured 512 kB of RAM and a larger color LCD screen. In addition, the WonderSwan Color is compatible with the original WonderSwan library of games.

Prior to WonderSwan's release, Nintendo had virtually a monopoly in the Japanese video game handheld market. After the release of the WonderSwan Color, Bandai took approximately 8% of the market share in Japan partly due to its low price of 6800 yen (approximately US$65). Another reason for the WonderSwan's success in Japan was the fact that Bandai managed to get a deal with Square to port over the original Famicom "Final Fantasy" games with improved graphics and controls. However, with the popularity of the Game Boy Advance and the reconciliation between Square and Nintendo, the WonderSwan Color and its successor, the SwanCrystal quickly lost its competitive advantage.

The 2000s saw a major leap in innovation, particularly in the second half with the release of the DS and PSP.

In 2001, Nintendo released the Game Boy Advance (GBA or AGB), which added two shoulder buttons, a larger screen, and more computing power than the Game Boy Color.

The design was revised two years later when the Game Boy Advance SP (GBA SP), a more compact version, was released. The SP features a "clamshell" design (folding open and closed, like a laptop computer), as well as a frontlit color display and rechargeable battery. Despite the smaller form factor, the screen remained the same size as that of the original. In 2005, the Game Boy Micro was released. This revision sacrifices screen size and backwards compatibility with previous Game Boys for a dramatic reduction in total size and a brighter backlit screen. A new SP model with a backlit screen was released in some regions around the same time.

Along with the Nintendo GameCube, the GBA also introduced the concept of "connectivity": using a handheld system as a console controller. A handful of games use this feature, most notably "Animal Crossing", "Pac-Man Vs.", "Final Fantasy Crystal Chronicles", "", "", "Metroid Prime", and "".

As of December 31, 2007, the GBA, GBA SP, and the Game Boy Micro combined have sold 80.72 million units worldwide.

The original GP32 was released in 2001 by the South Korean company Game Park a few months after the launch of the Game Boy Advance. It featured a 32-bit CPU, 133 MHz processor, MP3 and Divx player, and e-book reader. SmartMedia cards were used for storage, and could hold up to 128mb of anything downloaded through a USB cable from a PC. The GP32 was redesigned in 2003. A front-lit screen was added and the new version was called GP32 FLU (Front Light Unit). In summer 2004, another redesign, the GP32 BLU, was made, and added a backlit screen. This version of the handheld was planned for release outside South Korea; in Europe, and it was released for example in Spain (VirginPlay was the distributor). While not a commercial success on a level with mainstream handhelds (only 30,000 units were sold), it ended up being used mainly as a platform for user-made applications and emulators of other systems, being popular with developers and more technically adept users.

Nokia released the N-Gage in 2003. It was designed as a combination MP3 player, cellphone, PDA, radio, and gaming device. The system received much criticism alleging defects in its physical design and layout, including its vertically oriented screen and requirement of removing the battery to change game cartridges. The most well known of these was "sidetalking", or the act of placing the phone speaker and receiver on an edge of the device instead of one of the flat sides, causing the user to appear as if they are speaking into a taco.

The N-Gage QD was later released to address the design flaws of the original. However, certain features available in the original N-Gage, including MP3 playback, FM radio reception, and USB connectivity were removed.

Second generation of N-Gage launched on April 3, 2008 in the form of a service for selected Nokia Smartphones.

The Cybiko is a Russian hand-held computer introduced in May 2000 by David Yang's company and designed for teenage audiences, featuring its own two-way radio text messaging system. It has over 430 "official" freeware games and applications. Because of the text messaging system, it features a QWERTY keyboard that was used with a stylus. An MP3 player add-on was made for the unit as well as a SmartMedia card reader. The company stopped manufacturing the units after two product versions and only a few years on the market. Cybikos can communicate with each other up to a maximum range of 300 metres (0.19 miles). Several Cybikos can chat with each other in a wireless chatroom.

Cybiko Classic:

There were two models of the Classic Cybiko. Visually, the only difference was that the original version had a power switch on the side, whilst the updated version used the "escape" key for power management. Internally, the differences between the two models were in the internal memory, and the location of the firmware.

Cybiko Xtreme:

The Cybiko Xtreme was the second-generation Cybiko handheld. It featured various improvements over the original Cybiko, such as a faster processor, more RAM, more ROM, a new operating system, a new keyboard layout and case design, greater wireless range, a microphone, improved audio output, and smaller size.

In 2003, Tapwave released the Zodiac. It was designed to be a PDA-handheld game console hybrid. It supported photos, movies, music, Internet, and documents. The Zodiac used a special version Palm OS 5, 5.2T, that supported the special gaming buttons and graphics chip. Two versions were available, Zodiac 1 and 2, differing in memory and looks. The Zodiac line ended in July 2005 when Tapwave declared bankruptcy.

The Nintendo DS was released in November 2004. Among its new features were the incorporation of two screens, a touchscreen, wireless connectivity, and a microphone port. As with the Game Boy Advance SP, the DS features a clamshell design, with the two screens aligned vertically on either side of the hinge.

The DS's lower screen is touch sensitive, designed to be pressed with a stylus, a user's finger or a special "thumb pad" (a small plastic pad attached to the console's wrist strap, which can be affixed to the thumb to simulate an analog stick). More traditional controls include four face buttons, two shoulder buttons, a D-pad, and "Start" and "Select" buttons. The console also features online capabilities via the Nintendo Wi-Fi Connection and ad-hoc wireless networking for multiplayer games with up to sixteen players. It is backwards-compatible with all Game Boy Advance games, but not games designed for the Game Boy or Game Boy Color.

On January 2006, Nintendo revealed an updated version of the DS: the Nintendo DS Lite (released on March 2, 2006, in Japan) with an updated, smaller form factor (42% smaller and 21% lighter than the original Nintendo DS), a cleaner design, longer battery life, and brighter, higher-quality displays, with adjustable brightness. It is also able to connect wirelessly with Nintendo's Wii console.

On October 2, 2008, Nintendo announced the Nintendo DSi, with larger, 3.25-inch screens and two integrated cameras. It has an SD card storage slot in place of the Game Boy Advance slot, plus internal flash memory for storing downloaded games. It was released on November 1, 2008, in Japan, and was released in Australia on April 2, 2009, April 3, 2009 in Europe, and April 5, 2009 in North America. On October 29, 2009, Nintendo announced a larger version of the DSi, called the DSi XL, which was released on November 21, 2009.

As of December 31, 2009, the Nintendo DS, Nintendo DS Lite, and Nintendo DSi combined have sold 125.13 million units worldwide.

The GameKing is a handheld game console released by the Chinese company TimeTop in 2004. The first model while original in design owes a large debt to Nintendo's Game Boy Advance. The second model, the GameKing 2, is believed to be inspired by Sony's PSP. This model also was upgraded with a backlit screen, with a distracting background transparency (which can be removed by opening up the console). A color model, the GameKing 3 apparently exists, but was only made for a brief time and was difficult to purchase outside of Asia. Whether intentionally or not, the GameKing has the most primitive graphics of any handheld released since the Game Boy of 1989. 

As many of the games have an "old school" simplicity, the device has developed a small cult following. The Gameking's speaker is quite loud and the cartridges' sophisticated looping soundtracks (sampled from other sources) are seemingly at odds with its primitive graphics.

TimeTop made at least one additional device sometimes labeled as "GameKing", but while it seems to possess more advanced graphics, is essentially an emulator that plays a handful of multi-carts (like the GB Station Light II). Outside of Asia (especially China) however the Gameking remains relatively unheard of due to the enduring popularity of Japanese handhelds such as those manufactured by Nintendo and Sony.

The PlayStation Portable (officially abbreviated PSP) is a handheld game console manufactured and marketed by Sony Computer Entertainment. Development of the console was first announced during E3 2003, and it was unveiled on May 11, 2004, at a Sony press conference before E3 2004. The system was released in Japan on December 12, 2004, in North America on March 24, 2005, and in the PAL region on September 1, 2005.

The PlayStation Portable is the first handheld video game console to use an optical disc format, Universal Media Disc (UMD), for distribution of its games. UMD Video discs with movies and television shows were also released. The PSP utilized the Sony/SanDisk Memory Stick Pro Duo format as its primary storage medium. Other distinguishing features of the console include its large viewing screen, multi-media capabilities, and connectivity with the PlayStation 3, other PSPs, and the Internet.

Tiger's Gizmondo came out in the UK during March 2005 and it was released in the U.S. during October 2005. It is designed to play music, movies, and games, have a camera for taking and storing photos, and have GPS functions. It also has Internet capabilities. It has a phone for sending text and multimedia messages. Email was promised at launch, but was never released before Gizmondo, and ultimately Tiger Telematics', downfall in early 2006. Users obtained a second service pack, unreleased, hoping to find such functionality. However, Service Pack B did not activate the e-mail functionality.

The GP2X is an open-source, Linux-based handheld video game console and media player created by GamePark Holdings of South Korea, designed for homebrew developers as well as commercial developers. It is commonly used to run emulators for game consoles such as Neo-Geo, Genesis, Master System, Game Gear, Amstrad CPC, Commodore 64, Nintendo Entertainment System, TurboGrafx-16, MAME and others.

A new version called the "F200" was released October 30, 2007, and features a touchscreen, among other changes. Followed by GP2X Wiz (2009) and GP2X Caanoo (2010).

The Dingoo A-320 is a micro-sized gaming handheld that resembles the Game Boy Micro and is open to game development. It also supports music, radio, emulators (8 bit and 16 bit) and video playing capabilities with its own interface much like the PSP. There is also an onboard radio and recording program. It is currently available in two colors — white and black. Other similar products from the same manufacturer are the Dingoo A-330 (also known as Geimi), Dingoo A-360, Dingoo A-380 (available in pink, white and black) and the recently released Dingoo A-320E.

The PSP Go is a version of the PlayStation Portable handheld game console manufactured by Sony. It was released on October 1, 2009, in American and European territories, and on November 1 in Japan. It was revealed prior to E3 2009 through Sony's Qore VOD service. Although its design is significantly different from other PSPs, it is not intended to replace the PSP 3000, which Sony continued to manufacture, sell, and support. On April 20, 2011, the manufacturer announced that the PSP Go would be discontinued so that they may concentrate on the PlayStation Vita. Sony later said that only the European and Japanese versions were being cut, and that the console would still be available in the US.
Unlike previous PSP models, the PSP Go does not feature a UMD drive, but instead has 16 GB of internal flash memory to store games, video, pictures, and other media. This can be extended by up to 32 GB with the use of a Memory Stick Micro (M2) flash card. Also unlike previous PSP models, the PSP Go's rechargeable battery is not removable or replaceable by the user. The unit is 43% lighter and 56% smaller than the original PSP-1000, and 16% lighter and 35% smaller than the PSP-3000. It has a 3.8" 480 × 272 LCD (compared to the larger 4.3" 480 × 272 pixel LCD on previous PSP models). The screen slides up to reveal the main controls. The overall shape and sliding mechanism are similar to that of Sony's mylo COM-2 internet device.

The Pandora is a handheld game console/UMPC/PDA hybrid designed to take advantage of existing open source software and to be a target for home-brew development. It runs a full distribution of Linux, and in functionality is like a small PC with gaming controls. It is developed by OpenPandora, which is made up of former distributors and community members of the GP32 and GP2X handhelds.

OpenPandora began taking pre-orders for one batch of 4000 devices in November 2008 and after manufacturing delays, began shipping to customers on May 21, 2010.
The FC-16 Go is a portable Super NES hardware clone manufactured by Yobo Gameware in 2009. It features a 3.5-inch display, two wireless controllers, and CRT cables that allow cartridges to be played on a television screen. Unlike other Super NES clone consoles, it has region tabs that only allow NTSC North American cartridges to be played. Later revisions feature stereo sound output, larger shoulder buttons, and a slightly re-arranged button, power, and A/V output layout.

The Nintendo 3DS is the successor to Nintendo's DS handheld. The autostereoscopic device is able to project stereoscopic three-dimensional effects without requirement of active shutter or passive polarized glasses, which are required by most current 3D televisions to display the 3D effect. The 3DS was released in Japan on February 26, 2011; in Europe on March 25, 2011; in North America on March 27, 2011, and in Australia on March 31, 2011. The system features backward compatibility with Nintendo DS series software, including Nintendo DSi software. It also features an online service called the Nintendo eShop, launched on June 6, 2011, in North America and June 7, 2011, in Europe and Japan, which allows owners to download games, demos, applications and information on upcoming film and game releases. On November 24, 2011, a limited edition "Legend of Zelda 25th Anniversary 3DS" was released that contained a unique Cosmo Black unit decorated with gold Legend of Zelda related imagery, along with a copy of "The Legend of Zelda: Ocarina of Time 3D".

There are also other models including the Nintendo 2DS and the New Nintendo 3DS, the latter with a larger (XL/LL) variant, like the original Nintendo 3DS. The 2DS also has a successor, called the New Nintendo 2DS XL.
The Sony Ericsson Xperia PLAY is a handheld game console smartphone produced by Sony Ericsson under the Xperia smartphone brand. The device runs Android 2.3 Gingerbread, and is the first to be part of the PlayStation Certified program which means that it can play PlayStation Suite games. The device is a horizontally sliding phone with its original form resembling the Xperia X10 while the slider below resembles the slider of the PSP Go. The slider features a D-pad on the left side, a set of standard PlayStation buttons (, , and ) on the right, a long rectangular touchpad in the middle, start and select buttons on the bottom right corner, a menu button on the bottom left corner, and two shoulder buttons (L and R) on the back of the device. It is powered by a 1 GHz Qualcomm Snapdragon processor, a Qualcomm Adreno 205 GPU, and features a display measuring 4.0 inches (100 mm) (854 × 480), an 8-megapixel camera, 512 MB RAM, 8 GB internal storage, and a micro-USB connector. It supports microSD cards, versus the Memory Stick variants used in PSP consoles. The device was revealed officially for the first time in a Super Bowl ad on Sunday, February 6, 2011. On February 13, 2011, at Mobile World Congress (MWC) 2011, it was announced that the device would be shipping globally in March 2011, with a launch lineup of around 50 software titles.
The PlayStation Vita is the successor to Sony's PlayStation Portable (PSP) Handheld series. It was released in Japan on December 17, 2011 and in Europe, Australia, North and South America on February 22, 2012.

The handheld includes two analog sticks, a 5-inch (130 mm) OLED/LCD multi-touch capacitive touchscreen, and supports Bluetooth, Wi-Fi and optional 3G. Internally, the PS Vita features a 4 core ARM Cortex-A9 MPCore processor and a 4 core SGX543MP4+ graphics processing unit, as well as LiveArea software as its main user interface, which succeeds the XrossMediaBar.

The device is fully backwards-compatible with PlayStation Portable games digitally released on the PlayStation Network via the PlayStation Store. However, PSone Classics and PS2 titles were not compatible at the time of the primary public release in Japan. The Vita's dual analog sticks will be supported on selected PSP games. The graphics for PSP releases will be up-scaled, with a smoothing filter to reduce pixelation.

On September 20, 2018, Sony announced at Tokyo Game Show 2018 that the Vita would be discontinued in 2019, ending its hardware production. Production of Vita hardware officially ended on March 1, 2019.
The Razer Switchblade was a prototype pocket-sized like a Nintendo DSi XL designed to run Windows 7, featured a multi-touch LCD screen and an adaptive keyboard that changed keys depending on the game the user would play. It also was to feature a full mouse.

It was first unveiled on January 5, 2011, on the Consumer Electronics Show (CES). The Switchblade won The Best of CES 2011 People's Voice award. It has since been in development and the release date is still unknown. The device has likely been suspended indefinitely.
Project Shield is a handheld system developed by Nvidia announced at CES 2013. It runs on Android 4.2 and uses Nvidia Tegra 4 SoC. The hardware includes a 5-inches multitouch screen with support for HD graphics (720p). The console allows for the streaming of games running on a compatible desktop PC, or laptop.

Nvidia Shield Portable has received mixed reception from critics. Generally, reviewers praised the performance of the device, but criticized the cost and lack of worthwhile games. Engadget's review noted the system's "extremely impressive PC gaming", but also that due to its high price, the device was "a hard sell as a portable game console", especially when compared to similar handhelds on the market. CNET's Eric Franklin states in his review of the device that "The Nvidia Shield is an extremely well made device, with performance that pretty much obliterates any mobile product before it; but like most new console launches, there is currently a lack of available games worth your time." Eurogamer's comprehensive review of the device provides a detailed account of the device and its features; concluded by saying: "In the here and now, the first-gen Shield Portable is a gloriously niche, luxury product - the most powerful Android system on the market by a clear stretch and possessing a unique link to PC gaming that's seriously impressive in beta form, and can only get better." 

The Nintendo Switch is a hybrid console that can either be used in a handheld form, or inserted into a docking station attached to a television to play on a bigger screen. The Switch features two detachable wireless controllers, called Joy-Con, which can be used individually or attached to a grip to provide a traditional gamepad form. A handheld-only revision named Nintendo Switch Lite was released on September 20, 2019.

The Switch Lite had sold about 1.95 million units worldwide by September 30, 2019, only 10 days after its launch. Nintendo also announced a brand new color, coral, which came to the market on March 20, 2020.











</doc>
<doc id="14200" url="https://en.wikipedia.org/wiki?curid=14200" title="Heinrich Abeken">
Heinrich Abeken

Heinrich Abeken (August 19, 1809August 8, 1872) was a German theologian and Prussian Privy Legation Councillor in the Ministry of Foreign Affairs in Berlin.

Abeken was born and raised in the city of Osnabrück as a son of a merchant, he was incited to a higher education by the example of his uncle Bernhard Rudolf Abeken. After finishing the college in Osnabrück, he moved in 1827 to visit the University of Berlin to study theology. He soon combined philosophical and philological studies and was interested in art and modern literature.

In 1831, Abeken acquired a licenciate of theology. At the end of the year he visited Rome, and was welcomed in the house of Christian Karl Josias, Freiherr von Bunsen. Abeken participated in Bunsen's works, namely an evangelical prayer and hymn-book. In 1834 he became chaplain to the Prussian embassy in Rome. He married his first wife, who died soon thereafter. Bunsen left Rome in 1838 and Abeken followed soon thereafter to Germany. In 1841, he was sent to England to help in founding a German-English missionary bishopric in Jerusalem. In the same year, he was sent by Frederick William IV of Prussia to Egypt and Ethiopia, where he joined an expedition led by professor Karl Richard Lepsius. In 1845 and 1846 he returned via Jerusalem and Rome to Germany. He became Legation Councillor in Berlin, later Council Referee at the Ministry of Foreign Affairs.

In 1848 he received an appointment in the Prussian ministry for foreign affairs, and in 1853 was promoted to be privy councillor of legation ("Geheimer Legationsrath"). Abeken remained in charge for more than twenty years of Prussian politics, assisting Otto Theodor Freiherr von Manteuffel and Chancellor Otto von Bismarck. The latter was so much pleased with Abeken's work that officials started to call Abeken "the quill [i.e., the scribe] of Bismarck." Abeken married again in 1866; his second wife was Hedwig von Olfers, daughter of the general director of the royal museums, Privy Councilor von Olfers.

He was much employed by Bismarck in the writing of official despatches, and stood high in the favour of King William, whom he often accompanied on his journeys as representative of the foreign office. He was present with the king during the campaigns of 1866 and 1870–71. In 1851 he published anonymously "Babylon und Jerusalem," a scathing criticism of the views of the Countess von Hahn-Hahn.

During the war against Austria in 1866 as well as in the wars against France in 1870 and 1871, Abeken stayed in the Prussian headquarters. A major part of the dispatches of the time have been written by him. Unfortunately his health was damaged by the endeavours of these travels, and he died after an illness of several months. Emperor Wilhelm I described Abeken in a condolence letter to his widow: "One of my most reliable advisors, standing on my side in the most decisive moments; His loss is irreplaceable to me; In him his fatherland has lost one of the most noble and most loyal men and officials."

Despite his engagement in politics, Abeken never lost his interest in theology and continued to publish and speak in this sector during all of his life. He was interested in art and archeology, and was sponsor of the Archeological Institute of Rome and member of the Archeological Society of Rome. He founded a Circle of Friends of the Greek Literature in Berlin and was member of the prize commission for the royal Schiller-Prize.




</doc>
<doc id="14201" url="https://en.wikipedia.org/wiki?curid=14201" title="Henry Bruce, 1st Baron Aberdare">
Henry Bruce, 1st Baron Aberdare

Henry Austin Bruce, 1st Baron Aberdare (16 April 181525 February 1895) was a British Liberal Party politician, who served in government most notably as Home Secretary (1868–1873) and as Lord President of the Council.

Henry Bruce was born at Duffryn, Aberdare, Glamorganshire, the son of John Bruce, a Glamorganshire landowner, and his first wife Sarah, daughter of Reverend Hugh Williams Austin. John Bruce's original family name was Knight, but on coming of age in 1805 he assumed the name of Bruce: his mother, through whom he inherited the Duffryn estate, was the daughter of William Bruce, high sheriff of Glamorganshire.

Henry was educated from the age of twelve at the Bishop Gore School, Swansea (Swansea Grammar School). In 1837 he was called to the bar from Lincoln's Inn. Shortly after he had begun to practice, the discovery of coal beneath the Duffryn and other Aberdare Valley estates brought his family great wealth. From 1847 to 1854 Bruce was stipendiary magistrate for Merthyr Tydfil and Aberdare, resigning the position in the latter year, after entering parliament as Liberal member for Merthyr Tydfil.

Bruce was returned unopposed as MP for Merthyr Tydfil in December 1852, following the death of Sir John Guest. He did so with the enthusiastic support of the late member's political allies, notably the iron masters of Dowlais, and he was thereafter regarded by his political opponents, most notably in the Aberdare Valley, as their nominee. Even so, Bruce's parliamentary record demonstrated support for liberal policies, with the exception of the ballot. The electorate in the constituency at this time remained relatively small, excluding the vast majority of the working classes.

Significantly, however, Bruce's relationship with the miners of the Aberdare Valley, in particular, deteriorated as a result of the Aberdare Strike of 1857–8. In a speech to a large audience of miners at the Aberdare Market Hall, Bruce sought to strike a conciliatory tone in persuading the miners to return to work. In a second speech, however, he delivered a broadside against the trade union movement generally, referring to the violence engendered elsewhere as a result of strikes and to alleged examples of intimidation and violence in the immediate locality. The strike damaged his reputation and may well have contributed to his eventual election defeat ten years later. In 1855, Bruce was appointed a trustee of the Dowlais Iron Company and played a role in the further development of the iron industry.

In November 1862, after nearly ten years in Parliament, he became Under-Secretary of State for the Home Department, and held that office until April 1864. He became a Privy Councillor and a Charity Commissioner for England and Wales in 1864, when he was moved to be Vice-President of the Council of Education.

At the 1868 General Election, Merthyr Tydfil became a two-member constituency with a much-increased electorate as a result of the Second Reform Act of 1867. Since the formation of the constituency, Merthyr Tydfil had dominated representation as the vast majority of the electorate lived in the town and its vicinity, whereas there was a much lower number of electors in the neighbouring Aberdare Valley. During the 1850s and 1860s, however, the population of Aberdare grew rapidly, and the franchise changes in 1867 gave the vote to large numbers of miners in that valley. Amongst these new electors, Bruce remained unpopular as a result of his actions during the 1857-8 dispute. Initially, it appeared that the Aberdare iron master, Richard Fothergill, would be elected to the second seat alongside Bruce. However, the appearance of a third Liberal candidate, Henry Richard, a nonconformist radical popular in both Merthyr and Aberdare, left Bruce on the defensive and he was ultimately defeated, finishing in third place behind both Richard and Fothergill.

After losing his seat, Bruce was elected for Renfrewshire on 25 January 1869, he was made Home Secretary by William Ewart Gladstone. His tenure of this office was conspicuous for a reform of the licensing laws, and he was responsible for the Licensing Act 1872, which made the magistrates the licensing authority, increased the penalties for misconduct in public-houses and shortened the number of hours for the sale of drink. In 1873 Bruce relinquished the home secretaryship, at Gladstone's request, to become Lord President of the Council, and was elevated to the peerage as Baron Aberdare, of Duffryn in the County of Glamorgan, on 23 August that year. Being a Gladstonian Liberal, Aberdare had hoped for a much more radical proposal to keep existing licensee holders for a further ten years, and to prevent any new applicants. Its unpopularity pricked his nonconformist's conscience, when like Gladstone himself he had a strong leaning towards Temperance. He had already pursued 'moral improvement' on miners in the regulations attempting to further ban boys from the pits. The Trades Union Act 1871 was another more liberal regime giving further rights to unions, and protection from malicious prosecutions.

The defeat of the Liberal government in the following year terminated Lord Aberdare's official political life, and he subsequently devoted himself to social, educational and economic questions. Education became one of Lord Aberdare's main interests in later life. His interest had been shown by the speech on Welsh education which he had made on 5 May 1862. In 1880, he was appointed to chair the Departmental Committee on Intermediate and Higher Education in Wales and Monmouthshire, whose report ultimately led to the Welsh Intermediate Education Act of 1889. The report also stimulated the campaign for the provision of university education in Wales. In 1883, Lord Aberdare was elected the first president of the University College of South Wales and Monmouthshire. In his inaugural address he declared that the framework of Welsh education would not be complete until there was a University of Wales. The University was eventually founded in 1893 and Aberdare became its first chancellor.

In 1876 he was elected a Fellow of the Royal Society; from 1878 to 1891 he was president of the Royal Historical Society. and in 1881 he became president of both the Royal Geographical Society and the Girls' Day School Trust. In 1888 he headed the commission that established the Official Table of Drops, listing how far a person of a particular weight should be dropped when hanged for a capital offence (the only method of 'judicial execution' in the United Kingdom at that time), to ensure an instant and painless death, by cleanly breaking the neck between the 2nd and 3rd vertebrae, an 'exacting science', eventually brought to perfection by Chief Executioner Albert Pierrepoint. Prisoners health, clothing and discipline was a particular concern even at the end of his career. In the Lords he spoke at some length to the Home Affairs Committee chaired by Arthur Balfour about the prison rules system. Aberdare had always maintained a healthy skepticism about intemperate working-classes; in 1878 urging greater vigilance against the vice of excessive drinking, he took evidence on miners and railway colliers habitual imbibing. The committee tried to establish special legislation based on a link between Sunday Opening and absenteeism established in 1868. Aberdare had been interested in the plight of working class drinkers since Gladstone had appointed him Home Secretary. The defeat of the Licensing Bill by the Tory 'beerage' and publicans was drafted to limit hours and protect the public, but it persuaded a convinced Anglican forever more of the iniquities.

In 1882 he began a connection with West Africa which lasted the rest of his life, by accepting the chairmanship of the National African Company, formed by Sir George Goldie, which in 1886 received a charter under the title of the Royal Niger Company and in 1899 was taken over by the British government, its territories being constituted the protectorate of Nigeria. West African affairs, however, by no means exhausted Lord Aberdare's energies, and it was principally through his efforts that a charter was in 1894 obtained for the University College of South Wales and Monmouthshire,a constituent institution of the University of Wales. This is now Cardiff University. Lord Aberdare, who in 1885 was made a Knight Grand Cross of the Order of the Bath, presided over several Royal Commissions at different times.

Henry Bruce married firstly Annabella, daughter of Richard Beadon, of Clifton by Annabella A'Court, sister of 1st Baron Heytesbury, on 6 January 1846. They had one son and three daughters. 

After her death on 28 July 1852 he married secondly on 17 August 1854 Norah Creina Blanche, youngest daughter of Lt-Gen Sir William Napier, KCB the historian of the Peninsular War, whose biography he edited, by Caroline Amelia, second daughter of Gen. Hon Henry Edward Fox, son of the Earl of Ilchester. They had seven daughters and two sons, of whom:

Lord Aberdare died at his London home, 39 Princes Gardens, South Kensington, on 25 February 1895, aged 79, and was succeeded in the barony by his only son by his first marriage, Henry. He was survived by his wife, Lady Aberdare, born 1827, who died on 27 April 1897. She was a proponent of women's education and active in the establishment of Aberdare Hall in Cardiff.

Henry Austin Bruce is buried at Aberffrwd Cemetery in Mountain Ash, Wales. His large family plot is surrounded by a chain, and his gravestone is a simple Celtic cross with double plinth and kerb. In place is written "To God the Judge of all and to the spirits of just men more perfect."




</doc>
<doc id="14203" url="https://en.wikipedia.org/wiki?curid=14203" title="Harpers Ferry (disambiguation)">
Harpers Ferry (disambiguation)

Harpers Ferry is the name of several places in the United States of America:

Harpers Ferry may also refer to:



</doc>
<doc id="14204" url="https://en.wikipedia.org/wiki?curid=14204" title="Halophile">
Halophile

The halophiles, named after the Greek word for "salt-loving", are extremophiles that thrive in high salt concentrations. While most halophiles are classified into the domain Archaea, there are also bacterial halophiles and some eukaryotic species, such as the alga "Dunaliella salina" and fungus "Wallemia ichthyophaga". Some well-known species give off a red color from carotenoid compounds, notably bacteriorhodopsin. Halophiles can be found in water bodies with salt concentration five times greater than that of the ocean, such as the Great Salt Lake in Utah, Owens Lake in California, the Dead Sea, and in evaporation ponds. They are theorized to be a possible candidate for extremophiles living in the salty subsurface water ocean of Jupiter's Europa and other similar moons.

Halophiles are categorized by the extent of their halotolerance: slight, moderate, or extreme. Slight halophiles prefer 0.3 to 0.8 M (1.7 to 4.8%—seawater is 0.6 M or 3.5%), moderate halophiles 0.8 to 3.4 M (4.7 to 20%), and extreme halophiles 3.4 to 5.1 M (20 to 30%) salt content. Halophiles require sodium chloride (salt) for growth, in contrast to halotolerant organisms, which do not require salt but can grow under saline conditions.

High salinity represents an extreme environment in which relatively few organisms have been able to adapt and survive. Most halophilic and all halotolerant organisms expend energy to exclude salt from their cytoplasm to avoid protein aggregation ('salting out'). To survive the high salinities, halophiles employ two differing strategies to prevent desiccation through osmotic movement of water out of their cytoplasm. Both strategies work by increasing the internal osmolarity of the cell. The first strategy is employed by the majority of halophilic bacteria, some archaea, yeasts, algae, and fungi; the organism accumulates organic compounds in the cytoplasm—osmoprotectants which are known as compatible solutes. These can be either synthesised or accumulated from the environment. The most common compatible solutes are neutral or zwitterionic, and include amino acids, sugars, polyols, betaines, and ectoines, as well as derivatives of some of these compounds.

The second, more radical adaptation involves selectively absorbing potassium (K) ions into the cytoplasm. This adaptation is restricted to the moderately halophilic bacterial order "Halanaerobiales", the extremely halophilic archaeal family "Halobacteriaceae", and the extremely halophilic bacterium "Salinibacter ruber". The presence of this adaptation in three distinct evolutionary lineages suggests convergent evolution of this strategy, it being unlikely to be an ancient characteristic retained in only scattered groups or passed on through massive lateral gene transfer. The primary reason for this is the entire intracellular machinery (enzymes, structural proteins, etc.) must be adapted to high salt levels, whereas in the compatible solute adaptation, little or no adjustment is required to intracellular macromolecules; in fact, the compatible solutes often act as more general stress protectants, as well as just osmoprotectants.

Of particular note are the extreme halophiles or haloarchaea (often known as halobacteria), a group of archaea, which require at least a 2 M salt concentration and are usually found in saturated solutions (about 36% w/v salts). These are the primary inhabitants of salt lakes, inland seas, and evaporating ponds of seawater, such as the deep salterns, where they tint the water column and sediments bright colors. These species most likely perish if they are exposed to anything other than a very high-concentration, salt-conditioned environment. These prokaryotes require salt for growth. The high concentration of sodium chloride in their environment limits the availability of oxygen for respiration. Their cellular machinery is adapted to high salt concentrations by having charged amino acids on their surfaces, allowing the retention of water molecules around these components. They are heterotrophs that normally respire by aerobic means. Most halophiles are unable to survive outside their high-salt native environments. Many halophiles are so fragile that when they are placed in distilled water, they immediately lyse from the change in osmotic conditions.

Halophiles use a variety of energy sources and can be aerobic or anaerobic; anaerobic halophiles include phototrophic, fermentative, sulfate-reducing, homoacetogenic, and methanogenic species.

The Haloarchaea, and particularly the family Halobacteriaceae, are members of the domain "Archaea", and comprise the majority of the prokaryotic population in hypersaline environments. Currently, 15 recognised genera are in the family. The domain Bacteria (mainly "Salinibacter ruber") can comprise up to 25% of the prokaryotic community, but is more commonly a much lower percentage of the overall population. At times, the alga "Dunaliella salina" can also proliferate in this environment.

A comparatively wide range of taxa has been isolated from saltern crystalliser ponds, including members of these genera: "Haloferax, Halogeometricum, Halococcus, Haloterrigena, Halorubrum, Haloarcula", and "Halobacterium". However, the viable counts in these cultivation studies have been small when compared to total counts, and the numerical significance of these isolates has been unclear. Only recently has it become possible to determine the identities and relative abundances of organisms in natural populations, typically using PCR-based strategies that target 16S small subunit ribosomal ribonucleic acid (16S rRNA) genes. While comparatively few studies of this type have been performed, results from these suggest that some of the most readily isolated and studied genera may not in fact be significant in the "in situ" community. This is seen in cases such as the genus "Haloarcula", which is estimated to make up less than 0.1% of the" in situ" community, but commonly appears in isolation studies.

The comparative genomic and proteomic analysis showed distinct molecular signatures exist for the environmental adaptation of halophiles. At the protein level, the halophilic species are characterized by low hydrophobicity, an overrepresentation of acidic residues, underrepresentation of Cys, lower propensities for helix formation, and higher propensities for coil structure. The core of these proteins is less hydrophobic, such as DHFR, that was found to have narrower β-strands.
At the DNA level, the halophiles exhibit distinct dinucleotide and codon usage.

"Halobacteriaceae" is a family that includes a large part of halophilic archaea. The genus "Halobacterium" under it has a high tolerance for elevated levels of salinity. Some species of halobacteria have acidic proteins that resist the denaturing effects of salts. "Halococcus" is another genus of the family Halobacteriaceae.

Some hypersaline lakes are habitat to numerous families of halophiles. For example, the Makgadikgadi Pans in Botswana form a vast, seasonal, high-salinity water body that manifests halophilic species within the diatom genus "Nitzschia" in the family Bacillariaceae, as well as species within the genus "Lovenula" in the family Diaptomidae. Owens Lake in California also contains a large population of the halophilic bacterium "Halobacterium halobium".

"Wallemia ichthyophaga" is a basidiomycetous fungus, which requires at least 1.5 M sodium chloride for "in vitro" growth, and it thrives even in media saturated with salt. Obligate requirement for salt is an exception in fungi. Even species that can tolerate salt concentrations close to saturation (for example "Hortaea werneckii") in almost all cases grow well in standard microbiological media without the addition of salt.

The fermentation of salty foods (such as soy sauce, Chinese fermented beans, salted cod, salted anchovies, sauerkraut, etc.) often involves halophiles as either essential ingredients or accidental contaminants. One example is "Chromohalobacter beijerinckii", found in salted beans preserved in brine and in salted herring. "Tetragenococcus halophilus" is found in salted anchovies and soy sauce.

"Artemia" is a ubiquitous genus of small halophilic crustaceans living in salt lakes (such as Great Salt Lake) and solar salterns that can exist in water approaching the precipitation point of NaCl (340 g/L) and can withstand strong osmotic shocks due to its mitigating strategies for fluctuating salinity levels, such as its unique larval salt gland and osmoregulatory capacity.

North Ronaldsay sheep are a breed of sheep originating from Orkney, Scotland. They have limited access to freshwater sources on the island and their only food source is seaweed. They have adapted to handle salt concentrations that would kill other breeds of sheep.




</doc>
<doc id="14205" url="https://en.wikipedia.org/wiki?curid=14205" title="Herbert A. Simon">
Herbert A. Simon

Herbert Alexander Simon (June 15, 1916 – February 9, 2001) was an American economist, political scientist and cognitive psychologist, whose primary research interest was decision-making within organizations and is best known for the theories of "bounded rationality" and "satisficing". He received the Nobel Prize in Economics in 1978 and the Turing Award in 1975. His research was noted for its interdisciplinary nature and spanned across the fields of cognitive science, computer science, public administration, management, and political science. He was at Carnegie Mellon University for most of his career, from 1949 to 2001.

Notably, Simon was among the pioneers of several modern-day scientific domains such as artificial intelligence, information processing, decision-making, problem-solving, organization theory, and complex systems. He was among the earliest to analyze the architecture of complexity and to propose a preferential attachment mechanism to explain power law distributions.

Herbert Alexander Simon was born in Milwaukee, Wisconsin on June 15, 1916. Simon's father, Arthur Simon (1881–1948), was a Jewish electrical engineer who came to the United States from Germany in 1903 after earning his engineering degree at Technische Hochschule Darmstadt. An inventor, Arthur also was an independent patent attorney. Simon's mother, Edna Marguerite Merkel (1888-1969), was an accomplished pianist whose ancestors came from Prague and Cologne. Simon's European ancestors were piano makers, goldsmiths, and vintners. Like his father, Simon's mother also came from a family with Jewish, Lutheran, and Catholic backgrounds. 

Simon attended Milwaukee Public Schools, where he developed an interest in science and established himself as an atheist. While attending middle school, Simon wrote a letter to "the editor of the "Milwaukee Journal" defending the civil liberties of atheists". Unlike most children, Simon's family introduced him to the idea that human behavior could be studied scientifically; his mother's younger brother, Harold Merkel (1892-1922), who studied economics at the University of Wisconsin–Madison under John R. Commons, became one of his earliest influences. Through Harold's books on economics and psychology, Simon discovered social science. Among his earliest influences, Simon cited Norman Angell for his book "The Great Illusion" and Henry George for his book "Progress and Poverty". While attending high school, Simon joined the debate team, where he argued "from conviction, rather than cussedness" in favor of George's single tax.

In 1933, Simon entered the University of Chicago, and, following his early influences, decided to study social science and mathematics. Simon was interested in studying biology but chose not to pursue the field because of his "color-blindness and awkwardness in the laboratory". At an early age, Simon learned he was color blind and discovered the external world is not the same as the perceived world. While in college, Simon focused on political science and economics. Simon's most important mentor was Henry Schultz, an econometrician and mathematical economist. Simon received both his B.A. (1936) and his Ph.D. (1943) in political science from the University of Chicago, where he studied under Harold Lasswell, Nicolas Rashevsky, Rudolf Carnap, Henry Schultz, and Charles Edward Merriam. After enrolling in a course on "Measuring Municipal Governments," Simon became a research assistant for Clarence Ridley, and the two co-authored "Measuring Municipal Activities: A Survey of Suggested Criteria for Appraising Administration" in 1938. Simon's studies led him to the field of organizational decision-making, which became the subject of his doctoral dissertation.

After graduating with his undergraduate degree, Simon obtained a research assistantship in municipal administration which turned into a directorship at the University of California, Berkeley.

From 1942 to 1949, Simon was a professor of political science and also served as department chairman at Illinois Institute of Technology in Chicago. There, he began participating in the seminars held by the staff of the Cowles Commission who at that time included Trygve Haavelmo, Jacob Marschak, and Tjalling Koopmans. He thus began an in-depth study of economics in the area of institutionalism. Marschak brought Simon in to assist in the study he was currently undertaking with Sam Schurr of the "prospective economic effects of atomic energy".

From 1949 to 2001, Simon was a faculty at Carnegie Mellon. In 1949, Simon became a professor of administration and chairman of the Department of Industrial Management at Carnegie Tech (later to become Carnegie Mellon University). Simon later also taught psychology and computer science in the same university, (occasionally visiting other universities).
Seeking to replace the highly simplified classical approach to economic modeling, Simon became best known for his theory of corporate decision in his book "Administrative Behavior". In this book he based his concepts with an approach that recognized multiple factors that contribute to decision making. His organization and administration interest allowed him to not only serve three times as a university department chairman, but he also played a big part in the creation of the Economic Cooperation Administration in 1948; administrative team that administered aid to the Marshall Plan for the U.S. government, serving on President Lyndon Johnson's Science Advisory Committee, and also the National Academy of Science. Simon has made a great number of contributions to both economic analysis and applications. Because of this, his work can be found in a number of economic literary works, making contributions to areas such as mathematical economics including theorem, human rationality, behavioral study of firms, theory of casual ordering, and the analysis of the parameter identification problem in econometrics.

"Administrative Behavior", first published in 1947, and updated across the years was based on Simon's doctoral dissertation. It served as the foundation for his life's work. The centerpiece of this book is the behavioral and cognitive processes of humans making rational choices, that is, decisions. By his definition, an operational administrative decision should be correct and efficient, and it must be practical to implement with a set of coordinated means.
Simon recognized that a theory of administration is largely a theory of human decision making, and as such must be based on both economics and on psychology. He states:

Contrary to the "homo economicus" stereotype, Simon argued that alternatives and consequences may be partly known, and means and ends imperfectly differentiated, incompletely related, or poorly detailed.

Simon defined the task of rational decision making is to select the alternative that results in the more preferred set of all the possible consequences. Correctness of administrative decisions was thus measured by:

The task of choice was divided into three required steps:


Any given individual or organization attempting to implement this model in a real situation would be unable to comply with the three requirements. Simon argued that knowledge of all alternatives, or all consequences that follow from each alternative is impossible in many realistic cases.

Simon attempted to determine the techniques and/or behavioral processes that a person or organization could bring to bear to achieve approximately the best result given limits on rational decision making. Simon writes:

Simon therefore, describes work in terms of an economic framework, conditioned on human cognitive limitations: Economic man and Administrative man.

"Administrative Behavior" addresses a wide range of human behaviors, cognitive abilities, management techniques, personnel policies, training goals and procedures, specialized roles, criteria for evaluation of accuracy and efficiency, and all of the ramifications of communication processes. Simon is particularly interested in how these factors influence the making of decisions, both directly and indirectly.

Simon argued that the two outcomes of a choice require monitoring and that many members of the organization would be expected to focus on adequacy, but that administrative management must pay particular attention to the efficiency with which the desired result was obtained.

Simon followed Chester Barnard who pointed out that "the decisions that an individual makes as a member of an organization are quite distinct from his personal decisions". Personal choices may be determined whether an individual joins a particular organization, and continue to be made in his or her extra–organizational private life. As a member of an organization, however, that individual makes decisions not in relationship to personal needs and results, but in an impersonal sense as part of the organizational intent, purpose, and effect. Organizational inducements, rewards, and sanctions are all designed to form, strengthen, and maintain this identification.

Simon saw two universal elements of human social behavior as key to creating the possibility of organizational behavior in human individuals: Authority (addressed in Chapter VII—The Role of Authority) and in Loyalties and Identification (Addressed in Chapter X: Loyalties, and Organizational Identification).

Authority is a well-studied, primary mark of organizational behavior, straightforwardly defined in the organizational context as the ability and right of an individual of higher rank to guide the decisions of an individual of lower rank. The actions, attitudes, and relationships of the dominant and subordinate individuals constitute components of role behavior that may vary widely in form, style, and content, but do not vary in the expectation of obedience by the one of superior status, and willingness to obey from the subordinate.

Loyalty was defined by Simon as the "process whereby the individual substitutes organizational objectives (service objectives or conservation objectives) for his own aims as the value-indices which determine his organizational decisions". This entailed evaluating alternative choices in terms of their consequences for the group rather than only for onself or ones family.

Decisions can be complex admixtures of facts and values. Information about facts, especially empirically-proven facts or facts derived from specialized experience, are more easily transmitted in the exercise of authority than are the expressions of values. Simon is primarily interested in seeking identification of the individual employee with the organizational goals and values. Following Lasswell, he states that "a person identifies himself with a group when, in making a decision, he evaluates the several alternatives of choice in terms of their consequences for the specified group". A person may identify himself with any number of social, geographic, economic, racial, religious, familial, educational, gender, political, and sports groups. Indeed, the number and variety are unlimited. The fundamental problem for organizations is to recognize that personal and group identifications may either facilitate or obstruct correct decision making for the organization. A specific organization has to determine deliberately, and specify in appropriate detail and clear language, its own goals, objectives, means, ends, and values.

Simon has been critical of traditional economics' elementary understanding of decision-making, and argues it "is too quick to build an idealistic, unrealistic picture of the decision-making process and then prescribe on the basis of such unrealistic picture". His contributions to research in the area of administrative decision-making have become increasingly mainstream in the business community.

Herbert Simon rediscovered path diagrams which were invented by Sewall Wright around 1920. Source: The Book of Why Judea Pearl, Dana Mackenzie p.79.

Simon was a pioneer in the field of artificial intelligence, creating with Allen Newell the Logic Theory Machine (1956) and the General Problem Solver (GPS) (1957) programs. GPS may possibly be the first method developed for separating problem solving strategy from information about particular problems. Both programs were developed using the Information Processing Language (IPL) (1956) developed by Newell, Cliff Shaw, and Simon. Donald Knuth mentions the development of list processing in IPL, with the linked list originally called "NSS memory" for its inventors. In 1957, Simon predicted that computer chess would surpass human chess abilities within "ten years" when, in reality, that transition took about forty years.

In the early 1960s psychologist Ulric Neisser asserted that while machines are capable of replicating "cold cognition" behaviors such as reasoning, planning, perceiving, and deciding, they would never be able to replicate "hot cognition" behaviors such as pain, pleasure, desire, and other emotions. Simon responded to Neisser's views in 1963 by writing a paper on emotional cognition, which he updated in 1967 and published in "Psychological Review". Simon's work on emotional cognition was largely ignored by the artificial intelligence research community for several years, but subsequent work on emotions by Sloman and Picard helped refocus attention on Simon's paper and eventually, made it highly influential on the topic.

Simon also collaborated with James G. March on several works in organization theory.

With Allen Newell, Simon developed a theory for the simulation of human problem solving behavior using production rules. The study of human problem solving required new kinds of human measurements and, with Anders Ericsson, Simon developed the experimental technique of verbal protocol analysis. Simon was interested in the role of knowledge in expertise. He said that to become an expert on a topic required about ten years of experience and he and colleagues estimated that expertise was the result of learning roughly 50,000 chunks of information. A chess expert was said to have learned about 50,000 chunks or chess position patterns.

He was awarded the ACM Turing Award, along with Allen Newell, in 1975. "In joint scientific efforts extending over twenty years, initially in collaboration with J. C. (Cliff) Shaw at the RAND Corporation, and with numerous faculty and student colleagues at Carnegie Mellon University, they have made basic contributions to artificial intelligence, the psychology of human cognition, and list processing."

Simon was interested in how humans learn and, with Edward Feigenbaum, he developed the EPAM (Elementary Perceiver and Memorizer) theory, one of the first theories of learning to be implemented as a computer program. EPAM was able to explain a large number of phenomena in the field of verbal learning. Later versions of the model were applied to concept formation and the acquisition of expertise. With Fernand Gobet, he has expanded the EPAM theory into the CHREST computational model. The theory explains how simple chunks of information form the building blocks of schemata, which are more complex structures. CHREST has been used predominantly, to simulate aspects of chess expertise.

Simon has been credited for revolutionary changes in microeconomics. He is responsible for the concept of organizational decision-making as it is known today. He also was the first to discuss this concept in terms of uncertainty; i.e., it is impossible to have perfect and complete information at any given time to make a decision. While this notion was not entirely new, Simon is best known for its origination. It was in this area that he was awarded the Nobel Prize in 1978.

At the Cowles Commission, Simon's main goal was to link economic theory to mathematics and statistics. His main contributions were to the fields of general equilibrium and econometrics. He was greatly influenced by the marginalist debate that began in the 1930s. The popular work of the time argued that it was not apparent empirically that entrepreneurs needed to follow the marginalist principles of profit-maximization/cost-minimization in running organizations. The argument went on to note that profit maximization was not accomplished, in part, because of the lack of complete information. In decision-making, Simon believed that agents face uncertainty about the future and costs in acquiring information in the present. These factors limit the extent to which agents may make a fully rational decision, thus they possess only "bounded rationality" and must make decisions by "satisficing", or choosing that which might not be optimal, but which will make them happy enough. Bounded rationality is a central theme in behavioral economics. It is concerned with the ways in which the actual decision making process influences decision. Theories of bounded rationality relax one or more assumptions of standard expected utility theory.

Further, Simon emphasized that psychologists invoke a "procedural" definition of rationality, whereas economists employ a "substantive" definition. Gustavos Barros argued that the procedural rationality concept does not have a significant presence in the economics field and has never had nearly as much weight as the concept of bounded rationality. However, in an earlier article, Bhargava (1997) noted the importance of Simon's arguments and emphasized that there are several applications of the "procedural" definition of rationality in econometric analyses of data on health. In particular, economists should employ "auxiliary assumptions" that reflect the knowledge in the relevant biomedical fields, and guide the specification of econometric models for health outcomes.

Simon was also known for his research on industrial organization. He determined that the internal organization of firms and the external business decisions thereof, did not conform to the neoclassical theories of "rational" decision-making. Simon wrote many articles on the topic over the course of his life, mainly focusing on the issue of decision-making within the behavior of what he termed "bounded rationality". "Rational behavior, in economics, means that individuals maximize their utility function under the constraints they face (e.g., their budget constraint, limited choices, ...) in pursuit of their self-interest. This is reflected in the theory of subjective expected utility. The term, bounded rationality, is used to designate rational choice that takes into account the cognitive limitations of both knowledge and cognitive capacity. Bounded rationality is a central theme in behavioral economics. It is concerned with the ways in which the actual decision-making process influences decisions. Theories of bounded rationality relax one or more assumptions of standard expected utility theory".

Simon determined that the best way to study these areas was through computer simulations. As such, he developed an interest in computer science. Simon's main interests in computer science were in artificial intelligence, human–computer interaction, principles of the organization of humans and machines as information processing systems, the use of computers to study (by modeling) philosophical problems of the nature of intelligence and of epistemology, and the social implications of computer technology.

In his youth, Simon took an interest in land economics and Georgism, an idea known at the time as "single tax". The system is meant to redistribute unearned economic rent to the public and improve land use. In 1979, Simon still maintained these ideas and argued that land value tax should replace taxes on wages.

Some of Simon's economic research was directed toward understanding technological change in general and the information processing revolution in particular.

Simon's work has strongly influenced John Mighton, developer of a program that has achieved significant success in improving mathematics performance among elementary and high school students. Mighton cites a 2000 paper by Simon and two coauthors that counters arguments by French mathematics educator, Guy Brousseau, and others suggesting that excessive practice hampers children's understanding:

He received many top-level honors in life, including becoming a fellow of the American Academy of Arts and Sciences in 1959; election as a Member of the National Academy of Sciences in 1967; APA Award for Distinguished Scientific Contributions to Psychology (1969); the ACM's Turing Award for making "basic contributions to artificial intelligence, the psychology of human cognition, and list processing" (1975); the Nobel Memorial Prize in Economics "for his pioneering research into the decision-making process within economic organizations" (1978); the National Medal of Science (1986); the APA's Award for Outstanding Lifetime Contributions to Psychology (1993); ACM fellow (1994); and IJCAI Award for Research Excellence (1995).


Simon was a prolific writer and authored 27 books and almost a thousand papers. As of 2016, Simon was the most cited person in artificial intelligence and cognitive psychology on Google Scholar. With almost a thousand highly cited publications, he was one of the most influential social scientists of the twentieth century.



Simon married Dorothea Pye in 1938. Their marriage lasted 63 years until his death. In January 2001, Simon underwent surgery at UPMC Presbyterian to remove a cancerous tumor in his abdomen. Although the surgery was successful, Simon later succumbed to the complications that followed. They had three children, Katherine, Peter, and Barbara. His wife died in 2002.

From 1950 to 1955, Simon studied mathematical economics and during this time, together with David Hawkins, discovered and proved the Hawkins–Simon theorem on the "conditions for the existence of positive solution vectors for input-output matrices". He also developed theorems on near-decomposability and aggregation. Having begun to apply these theorems to organizations, by 1954 Simon determined that the best way to study problem-solving was to simulate it with computer programs, which led to his interest in computer simulation of human cognition. Founded during the 1950s, he was among the first members of the Society for General Systems Research.

Simon had a keen interest in the arts, as he was a pianist. He was a friend of Robert Lepper and Richard Rappaport. Rappaport also painted Simon's commissioned portrait at Carnegie Mellon University. He was also a keen mountain climber. As a testament to his wide interests, he at one point taught an undergraduate course on the French Revolution.



 


</doc>
<doc id="14207" url="https://en.wikipedia.org/wiki?curid=14207" title="Hematite">
Hematite

Hematite, also spelled as haematite, is a common iron oxide with a formula of FeO and is widespread in rocks and soils. Hematite forms in the shape of crystals through the rhombohedral lattice system, and it has the same crystal structure as ilmenite and corundum. Hematite and ilmenite form a complete solid solution at temperatures above .

Hematite is colored black to steel or silver-gray, brown to reddish-brown, or red. It is mined as the main ore of iron. Varieties include "kidney ore", "martite" (pseudomorphs after magnetite), "iron rose" and "specularite" (specular hematite). While these forms vary, they all have a rust-red streak. Hematite is harder than pure iron, but much more brittle. Maghemite is a hematite- and magnetite-related oxide mineral.

Large deposits of hematite are found in banded iron formations. Gray hematite is typically found in places that can have still, standing water or mineral hot springs, such as those in Yellowstone National Park in North America. The mineral can precipitate out of water and collect in layers at the bottom of a lake, spring, or other standing water. Hematite can also occur without water, usually as the result of volcanic activity.

Clay-sized hematite crystals can also occur as a secondary mineral formed by weathering processes in soil, and along with other iron oxides or oxyhydroxides such as goethite, is responsible for the red color of many tropical, ancient, or otherwise highly weathered soils.

The name hematite is derived from the Greek word for blood "(haima)", due to the red coloration found in some varieties of hematite. The color of hematite lends itself to use as a pigment. The English name of the stone is derived from Middle French "hématite pierre", which was imported from Latin "lapis haematites" the 15th century, which originated from Ancient Greek ("haimatitēs lithos", "blood-red stone").

Ochre is a clay that is colored by varying amounts of hematite, varying between 20% and 70%. Red ochre contains unhydrated hematite, whereas yellow ochre contains hydrated hematite (FeO · HO). The principal use of ochre is for tinting with a permanent color.

The red chalk writing of this mineral was one of the earliest in the history of humans. The powdery mineral was first used 164,000 years ago by the Pinnacle-Point man, possibly for social purposes. Hematite residues are also found in graves from 80,000 years ago. Near Rydno in Poland and Lovas in Hungary red chalk mines have been found that are from 5000 BC, belonging to the Linear Pottery culture at the Upper Rhine.

Rich deposits of hematite have been found on the island of Elba that have been mined since the time of the Etruscans.

Hematite is an antiferromagnetic material below the Morin transition at , and a canted antiferromagnet or weakly ferromagnetic above the Morin transition and below its Néel temperature at , above which it is paramagnetic.

The magnetic structure of α-hematite was the subject of considerable discussion and debate during the 1950s, as it appeared to be ferromagnetic with a Curie temperature of approximately , but with an extremely small magnetic moment (0.002 Bohr magnetons). Adding to the surprise was a transition with a decrease in temperature at around to a phase with no net magnetic moment. It was shown that the system is essentially antiferromagnetic, but that the low symmetry of the cation sites allows spin–orbit coupling to cause canting of the moments when they are in the plane perpendicular to the "c" axis. The disappearance of the moment with a decrease in temperature at is caused by a change in the anisotropy which causes the moments to align along the "c" axis. In this configuration, spin canting does not reduce the energy. The magnetic properties of bulk hematite differ from their nanoscale counterparts. For example, the Morin transition temperature of hematite decreases with a decrease in the particle size. The suppression of this transition has been observed in hematite nanoparticles and is attributed to the presence of impurities, water molecules and defects in the crystals lattice. Hematite is part of a complex solid solution oxyhydroxide system having various contents of water, hydroxyl groups and vacancy substitutions that affect the mineral's magnetic and crystal chemical properties. Two other end-members are referred to as protohematite and hydrohematite.

Enhanced magnetic coercivities for hematite have been achieved by dry-heating a two-line ferrihydrite precursor prepared from solution. Hematite exhibited temperature-dependent magnetic coercivity values ranging from . The origin of these high coercivity values has been interpreted as a consequence of the subparticle structure induced by the different particle and crystallite size growth rates at increasing annealing temperature. These differences in the growth rates are translated into a progressive development of a subparticle structure at the nanoscale. At lower temperatures (350–600 °C), single particles crystallize however; at higher temperatures (600–1000 °C), the growth of crystalline aggregates with a subparticle structure is favored.

Hematite is present in the waste tailings of iron mines. A recently developed process, magnetation, uses magnets to glean waste hematite from old mine tailings in Minnesota's vast Mesabi Range iron district. Falu red is a pigment used in traditional Swedish house paints. Originally, it was made from tailings of the Falu mine.

The spectral signature of hematite was seen on the planet Mars by the infrared spectrometer on the NASA "Mars Global Surveyor" and "2001 Mars Odyssey" spacecraft in orbit around Mars. The mineral was seen in abundance at two sites on the planet, the Terra Meridiani site, near the Martian equator at 0° longitude, and the Aram Chaos site near the Valles Marineris. Several other sites also showed hematite, such as Aureum Chaos. Because terrestrial hematite is typically a mineral formed in aqueous environments or by aqueous alteration, this detection was scientifically interesting enough that the second of the two Mars Exploration Rovers was sent to a site in the Terra Meridiani region designated Meridiani Planum. In-situ investigations by the "Opportunity" rover showed a significant amount of hematite, much of it in the form of small spherules that were informally named "blueberries" by the science team. Analysis indicates that these spherules are apparently concretions formed from a water solution.
"Knowing just how the hematite on Mars was formed will help us characterize the past environment and determine whether that environment was favorable for life".

Hematite's popularity in jewelry rose in England during the Victorian era, due to its use in mourning jewelry. Certain types of hematite- or iron-oxide-rich clay, especially Armenian bole, have been used in gilding. Hematite is also used in art such as in the creation of intaglio engraved gems. Hematine is a synthetic material sold as "magnetic hematite".




</doc>
<doc id="14208" url="https://en.wikipedia.org/wiki?curid=14208" title="Holocene extinction">
Holocene extinction

The Holocene extinction, otherwise referred to as the sixth mass extinction or Anthropocene extinction, is an ongoing extinction event of species during the present Holocene epoch (with the more recent time sometimes called Anthropocene) as a result of human activity. The included extinctions span numerous families of plants and animals, including mammals, birds, reptiles, amphibians, fishes and invertebrates. With widespread degradation of highly biodiverse habitats such as coral reefs and rainforests, as well as other areas, the vast majority of these extinctions are thought to be "undocumented", as the species are undiscovered at the time of their extinction, or no one has yet discovered their extinction. The current rate of extinction of species is estimated at 100 to 1,000 times higher than natural background rates.

The Holocene extinction includes the disappearance of large land animals known as megafauna, starting at the end of the last glacial period. Megafauna outside of the African mainland (thus excluding Madagascar), which did not evolve alongside humans, proved highly sensitive to the introduction of new predation, and many died out shortly after early humans began spreading and hunting across the Earth (many African species have also gone extinct in the Holocene, but – with few exceptions – megafauna of the mainland was largely unaffected until a few hundred years ago). These extinctions, occurring near the Pleistocene–Holocene boundary, are sometimes referred to as the Quaternary extinction event.

The most popular theory is that human overhunting of species added to existing stress conditions as the extinction coincides with human emergence. Although there is debate regarding how much human predation affected their decline, certain population declines have been directly correlated with human activity, such as the extinction events of New Zealand and Hawaii. Aside from humans, climate change may have been a driving factor in the megafaunal extinctions, especially at the end of the Pleistocene.

Ecologically, humanity has been noted as an unprecedented "global superpredator" that consistently preys on the adults of other apex predators, and has worldwide effects on food webs. There have been extinctions of species on every land mass and in every ocean: there are many famous examples within Africa, Asia, Europe, Australia, North and South America, and on smaller islands. Overall, the Holocene extinction can be linked to the human impact on the environment. The Holocene extinction continues into the 21st century, with meat consumption, overfishing, and ocean acidification and the decline in amphibian populations being a few broader examples of a cosmopolitan decline in biodiversity. Human population growth and increasing per capita consumption are considered to be the primary drivers of this decline.

The 2019 "Global Assessment Report on Biodiversity and Ecosystem Services", published by the United Nations' Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services, posits that roughly one million species of plants and animals face extinction within decades as the result of human actions.

The Holocene extinction is also known as the "sixth extinction", as it is possibly the sixth mass extinction event, after the Ordovician–Silurian extinction events, the Late Devonian extinction, the Permian–Triassic extinction event, the Triassic–Jurassic extinction event, and the Cretaceous–Paleogene extinction event. Mass extinctions are characterized by the loss of at least 75% of species within a geologically short period of time. There is no general agreement on where the Holocene, or anthropogenic, extinction begins, and the Quaternary extinction event, which includes climate change resulting in the end of the last ice age, ends, or if they should be considered separate events at all. Some have suggested that anthropogenic extinctions may have begun as early as when the first modern humans spread out of Africa between 200,000 and 100,000 years ago; this is supported by rapid megafaunal extinction following recent human colonisation in Australia, New Zealand and Madagascar, as might be expected when any large, adaptable predator (invasive species) moves into a new ecosystem. In many cases, it is suggested that even minimal hunting pressure was enough to wipe out large fauna, particularly on geographically isolated islands. Only during the most recent parts of the extinction have plants also suffered large losses.

In "The Future of Life" (2002), Edward Osborne Wilson of Harvard calculated that, if the current rate of human disruption of the biosphere continues, one-half of Earth's higher lifeforms will be extinct by 2100. A 1998 poll conducted by the American Museum of Natural History found that 70% of biologists acknowledge an ongoing anthropogenic extinction event. At present, the rate of extinction of species is estimated at 100 to 1,000 times higher than the background extinction rate, the historically typical rate of extinction (in terms of the natural evolution of the planet); also, the current rate of extinction is 10 to 100 times higher than in any of the previous mass extinctions in the history of Earth. One scientist estimates the current extinction rate may be 10,000 times the background extinction rate, although most scientists predict a much lower extinction rate than this outlying estimate. Theoretical ecologist Stuart Pimm stated that the extinction rate for plants is 100 times higher than normal.

In a pair of studies published in 2015, extrapolation from observed extinction of Hawaiian snails led to the conclusion that 7% of all species on Earth may have been lost already.

There is widespread consensus among scientists that human activity is accelerating the extinction of many animal species through the destruction of habitats, the consumption of animals as resources, and the elimination of species that humans view as threats or competitors. But some contend that this biotic destruction has yet to reach the level of the previous five mass extinctions. Stuart Pimm, for example, asserts that the sixth mass extinction "is something that hasn't happened yet – we are on the edge of it." In November 2017, a statement, titled "World Scientists’ Warning to Humanity: A Second Notice", led by eight authors and signed by 15,364 scientists from 184 countries asserted that, among other things, "we have unleashed a mass extinction event, the sixth in roughly 540 million years, wherein many current life forms could be annihilated or at least committed to extinction by the end of this century."

The abundance of species extinctions considered anthropogenic, or due to human activity, has sometimes (especially when referring to hypothesized future events) been collectively called the "Anthropocene extinction". "Anthropocene" is a term introduced in 2000. Some now postulate that a new geological epoch has begun, with the most abrupt and widespread extinction of species since the Cretaceous–Paleogene extinction event 66 million years ago.

The term "anthropocene" is being used more frequently by scientists, and some commentators may refer to the current and projected future extinctions as part of a longer Holocene extinction. The Holocene–Anthropocene boundary is contested, with some commentators asserting significant human influence on climate for much of what is normally regarded as the Holocene Epoch. Other commentators place the Holocene–Anthropocene boundary at the industrial revolution and also say that "[f]ormal adoption of this term in the near future will largely depend on its utility, particularly to earth scientists working on late Holocene successions."

It has been suggested that human activity has made the period starting from the mid-20th century different enough from the rest of the Holocene to consider it a new geological epoch, known as the Anthropocene, a term which was considered for inclusion in the timeline of Earth's history by the International Commission on Stratigraphy in 2016. In order to constitute the Holocene as an extinction event, scientists must determine exactly when anthropogenic greenhouse gas emissions began to measurably alter natural atmospheric levels on a global scale, and when these alterations caused changes to global climate. Using chemical proxies from Antarctic ice cores, researchers have estimated the fluctuations of carbon dioxide (CO) and methane (CH) gases in the Earth's atmosphere during the late Pleistocene and Holocene epochs. Estimates of the fluctuations of these two gases in the atmosphere, using chemical proxies from Antarctic ice cores, generally indicate that the peak of the Anthropocene occurred within the previous two centuries: typically beginning with the Industrial Revolution, when the highest greenhouse gas levels were recorded.

The Holocene extinction is mainly caused by human activities. Extinction of animals, plants, and other organisms caused by human actions may go as far back as the late Pleistocene, over 12,000 years ago. There is a correlation between megafaunal extinction and the arrival of humans, and human population growth and per-capita consumption growth, prominently in the past two centuries, are regarded as the underlying causes of extinction.

Human civilization was founded on and grew from agriculture. The more land used for farming, the greater the population a civilization could sustain, and subsequent popularization of farming led to habitat conversion.

Habitat destruction by humans, including oceanic devastation, such as through overfishing and contamination; and the modification and destruction of vast tracts of land and river systems around the world to meet solely human-centered ends (with 13 percent of Earth's ice-free land surface now used as row-crop agricultural sites, 26 percent used as pastures, and 4 percent urban-industrial areas), thus replacing the original local ecosystems. The sustained conversion of biodiversity rich forests and wetlands into poorer fields and pastures (of lesser carrying capacity for wild species), over the last 10,000 years, has considerably reduced the Earth's carrying capacity for wild birds, among other organisms, in both population size and species count.

Other, related human causes of the extinction event include deforestation, hunting, pollution, the introduction in various regions of non-native species, and the widespread transmission of infectious diseases spread through livestock and crops. Humans both create and destroy crop cultivar and domesticated animal varieties. Advances in transportation and industrial farming has led to monoculture and the extinction of many cultivars. The use of certain plants and animals for food has also resulted in their extinction, including silphium and the passenger pigeon.

Some scholars assert that the emergence of capitalism as the dominant economic system has accelerated ecological exploitation and destruction, and has also exacerbated mass species extinction. CUNY professor David Harvey, for example, posits that the neoliberal era "happens to be the era of the fastest mass extinction of species in the Earth's recent history".

Megafauna were once found on every continent of the world and large islands such as New Zealand and Madagascar, but are now almost exclusively found on the continent of Africa, with notable comparisons on Australia and the islands previously mentioned experiencing population crashes and trophic cascades shortly after the earliest human settlers. It has been suggested that the African megafauna survived because they evolved alongside humans. The timing of South American megafaunal extinction appears to precede human arrival, although the possibility that human activity at the time impacted the global climate enough to cause such an extinction has been suggested.

It has been noted, in the face of such evidence, that humans are unique in ecology as an unprecedented "global superpredator", regularly preying on large numbers of fully grown terrestrial and marine apex predators, and with a great deal of influence over food webs and climatic systems worldwide. Although significant debate exists as to how much human predation and indirect effects contributed to prehistoric extinctions, certain population crashes have been directly correlated with human arrival. A 2018 study published in "PNAS" found that since the dawn of human civilization, 83% of wild mammals, 80% of marine mammals, 50% of plants and 15% of fish have vanished. Currently, livestock make up 60% of the biomass of all mammals on earth, followed by humans (36%) and wild mammals (4%). As for birds, 70% are domesticated, such as poultry, whereas only 30% are wild.

Recent investigations about hunter-gatherer landscape burning has a major implication for the current debate about the timing of the Anthropocene and the role that humans may have played in the production of greenhouse gases prior to the Industrial Revolution. Studies on early hunter-gatherers raises questions about the current use of population size or density as a proxy for the amount of land clearance and anthropogenic burning that took place in pre-industrial times. Scientists have questioned the correlation between population size and early territorial alterations. Ruddiman and Ellis' research paper in 2009 makes the case that early farmers involved in systems of agriculture used more land per capita than growers later in the Holocene, who intensified their labor to produce more food per unit of area (thus, per laborer); arguing that agricultural involvement in rice production implemented thousands of years ago by relatively small populations have created significant environmental impacts through large-scale means of deforestation.

While a number of human-derived factors are recognized as contributing to rising atmospheric concentrations of CH (methane) and CO (carbon dioxide), deforestation and territorial clearance practices associated with agricultural development may be contributing most to these concentrations globally. Scientists that are employing a variance of archaeological and paleoecological data argue that the processes contributing to substantial human modification of the environment spanned many thousands of years ago on a global scale and thus, not originating as early as the Industrial Revolution. Gaining popularity on his uncommon hypothesis, palaeoclimatologist William Ruddiman in 2003, stipulated that in the early Holocene 11,000 years ago, atmospheric carbon dioxide and methane levels fluctuated in a pattern which was different from the Pleistocene epoch before it. He argued that the patterns of the significant decline of CO levels during the last ice age of the Pleistocene inversely correlates to the Holocene where there have been dramatic increases of CO around 8000 years ago and CH levels 3000 years after that. The correlation between the decrease of CO in the Pleistocene and the increase of it during the Holocene implies that the causation of this spark of greenhouse gases into the atmosphere was the growth of human agriculture during the Holocene such as the anthropogenic expansion of (human) land use and irrigation.

Human arrival in the Caribbean around 6,000 years ago is correlated with the extinction of many species. Examples include many different genera of ground and arboreal sloths across all islands. These sloths were generally smaller than those found on the South American continent. "Megalocnus" were the largest genus at up to , "Acratocnus" were medium-sized relatives of modern two-toed sloths endemic to Cuba, "Imagocnus" also of Cuba, "Neocnus" and many others.

Recent research, based on archaeological and paleontological digs on 70 different Pacific islands has shown that numerous species became extinct as people moved across the Pacific, starting 30,000 years ago in the Bismarck Archipelago and Solomon Islands. It is currently estimated that among the bird species of the Pacific, some 2000 species have gone extinct since the arrival of humans, representing a 20% drop in the biodiversity of birds worldwide.

The first settlers are thought to have arrived in the islands between 300 and 800 CE, with European arrival in the 16th century. Hawaii is notable for its endemism of plants, birds, insects, mollusks and fish; 30% of its organisms are endemic. Many of its species are endangered or have gone extinct, primarily due to accidentally introduced species and livestock grazing. Over 40% of its bird species have gone extinct, and it is the location of 75% of extinctions in the United States. Extinction has increased in Hawaii over the last 200 years and is relatively well documented, with extinctions among native snails used as estimates for global extinction rates.

Australia was once home to a large assemblage of megafauna, with many parallels to those found on the African continent today. Australia's fauna is characterised by primarily marsupial mammals, and many reptiles and birds, all existing as giant forms until recently. Humans arrived on the continent very early, about 50,000 years ago. The extent human arrival contributed is controversial; climatic drying of Australia 40,000–60,000 years ago was an unlikely cause, as it was less severe in speed or magnitude than previous regional climate change which failed to kill off megafauna. Extinctions in Australia continued from original settlement until today in both plants and animals, whilst many more animals and plants have declined or are endangered.

Due to the older timeframe and the soil chemistry on the continent, very little subfossil preservation evidence exists relative to elsewhere. However, continent-wide extinction of all genera weighing over 100 kilograms, and six of seven genera weighing between 45 and 100 kilograms occurred around 46,400 years ago (4,000 years after human arrival) and the fact that megafauna survived until a later date on the island of Tasmania following the establishment of a land bridge suggest direct hunting or anthropogenic ecosystem disruption such as fire-stick farming as likely causes. The first evidence of direct human predation leading to extinction in Australia was published in 2016.
Within 500 years of the arrival of humans between 2,500–2,000 years ago, nearly all of Madagascar's distinct, endemic and geographically isolated megafauna became extinct. The largest animals, of more than , were extinct very shortly after the first human arrival, with large and medium-sized species dying out after prolonged hunting pressure from an expanding human population moving into more remote regions of the island around 1000 years ago. Smaller fauna experienced initial increases due to decreased competition, and then subsequent declines over the last 500 years. All fauna weighing over died out. The primary reasons for this are human hunting and habitat loss from early aridification, both of which persist and threaten Madagascar's remaining taxa today.

The eight or more species of elephant birds, giant flightless ratites in the genera "Aepyornis", "Vorombe", and "Mullerornis", are extinct from over-hunting, as well as 17 species of lemur, known as giant, subfossil lemurs. Some of these lemurs typically weighed over , and fossils have provided evidence of human butchery on many species.

New Zealand is characterised by its geographic isolation and island biogeography, and had been isolated from mainland Australia for 80 million years. It was the last large land mass to be colonised by humans. The arrival of Polynesian settlers circa 12th century resulted in the extinction of all of the islands' megafaunal birds within several hundred years. The last moa, large flightless ratites, became extinct within 200 years of the arrival of human settlers. The Polynesians also introduced the Polynesian rat. This may have put some pressure on other birds but at the time of early European contact (18th century) and colonisation (19th century) the bird life was prolific. With them, the Europeans brought ship rats, possums, cats and mustelids which decimated native bird life, some of which had adapted flightlessness and ground nesting habits and others had no defensive behavior as a result of having no extant endemic mammalian predators. The kakapo, the world's biggest parrot, which is flightless, now only exists in managed breeding sanctuaries. New Zealand's national emblem, the kiwi, is on the endangered bird list.

There has been a debate as to the extent to which the disappearance of megafauna at the end of the last glacial period can be attributed to human activities by hunting, or even by slaughter of prey populations. Discoveries at Monte Verde in South America and at Meadowcroft Rock Shelter in Pennsylvania have caused a controversy regarding the Clovis culture. There likely would have been human settlements prior to the Clovis Culture, and the history of humans in the Americas may extend back many thousands of years before the Clovis culture. The amount of correlation between human arrival and megafauna extinction is still being debated: for example, in Wrangel Island in Siberia the extinction of dwarf woolly mammoths (approximately 2000 BCE) did not coincide with the arrival of humans, nor did megafaunal mass extinction on the South American continent, although it has been suggested climate changes induced by anthropogenic effects elsewhere in the world may have contributed.

Comparisons are sometimes made between recent extinctions (approximately since the industrial revolution) and the Pleistocene extinction near the end of the last glacial period. The latter is exemplified by the extinction of large herbivores such as the woolly mammoth and the carnivores that preyed on them. Humans of this era actively hunted the mammoth and the mastodon, but it is not known if this hunting was the cause of the subsequent massive ecological changes, widespread extinctions and climate changes.

The ecosystems encountered by the first Americans had not been exposed to human interaction, and may have been far less resilient to human made changes than the ecosystems encountered by industrial era humans. Therefore, the actions of the Clovis people, despite seeming insignificant by today's standards could indeed have had a profound effect on the ecosystems and wild life which was entirely unused to human influence.

Africa experienced the smallest decline in megafauna compared to the other continents. This is presumably due to the idea that Afroeurasian megafauna evolved alongside humans, and thus developed a healthy fear of them, unlike the comparatively tame animals of other continents. Unlike other continents, the megafauna of Eurasia went extinct over a relatively long period of time, possibly due to climate fluctuations fragmenting and decreasing populations, leaving them vulnerable to over-exploitation, as with the steppe bison ("Bison priscus"). The warming of the arctic region caused the rapid decline of grasslands, which had a negative effect on the grazing megafauna of Eurasia. Most of what once was mammoth steppe has been converted to mire, rendering the environment incapable of supporting them, notably the woolly mammoth.

One of the main theories to the extinction is climate change. The climate change theory has suggested that a change in climate near the end of the late Pleistocene stressed the megafauna to the point of extinction. Some scientists favor abrupt climate change as the catalyst for the extinction of the mega-fauna at the end of the Pleistocene, but there are many who believe increased hunting from early modern humans also played a part, with others even suggesting that the two interacted. However, the annual mean temperature of the current interglacial period for the last 10,000 years is no higher than that of previous interglacial periods, yet some of the same megafauna survived similar temperature increases. In the Americas, a controversial explanation for the shift in climate is presented under the Younger Dryas impact hypothesis, which states that the impact of comets cooled global temperatures.

Megafauna play a significant role in the lateral transport of mineral nutrients in an ecosystem, tending to translocate them from areas of high to those of lower abundance. They do so by their movement between the time they consume the nutrient and the time they release it through elimination (or, to a much lesser extent, through decomposition after death). In South America's Amazon Basin, it is estimated that such lateral diffusion was reduced over 98% following the megafaunal extinctions that occurred roughly 12,500 years ago. Given that phosphorus availability is thought to limit productivity in much of the region, the decrease in its transport from the western part of the basin and from floodplains (both of which derive their supply from the uplift of the Andes) to other areas is thought to have significantly impacted the region's ecology, and the effects may not yet have reached their limits. The extinction of the mammoths allowed grasslands they had maintained through grazing habits to become birch forests. The new forest and the resulting forest fires may have induced climate change. Such disappearances might be the result of the proliferation of modern humans; some recent studies favor this theory.

Large populations of megaherbivores have the potential to contribute greatly to the atmospheric concentration of methane, which is an important greenhouse gas. Modern ruminant herbivores produce methane as a byproduct of foregut fermentation in digestion, and release it through belching or flatulence. Today, around 20% of annual methane emissions come from livestock methane release. In the Mesozoic, it has been estimated that sauropods could have emitted 520 million tons of methane to the atmosphere annually, contributing to the warmer climate of the time (up to 10 °C warmer than at present). This large emission follows from the enormous estimated biomass of sauropods, and because methane production of individual herbivores is believed to be almost proportional to their mass.

Recent studies have indicated that the extinction of megafaunal herbivores may have caused a reduction in atmospheric methane. This hypothesis is relatively new. One study examined the methane emissions from the bison that occupied the Great Plains of North America before contact with European settlers. The study estimated that the removal of the bison caused a decrease of as much as 2.2 million tons per year. Another study examined the change in the methane concentration in the atmosphere at the end of the Pleistocene epoch after the extinction of megafauna in the Americas. After early humans migrated to the Americas about 13,000 BP, their hunting and other associated ecological impacts led to the extinction of many megafaunal species there. Calculations suggest that this extinction decreased methane production by about 9.6 million tons per year. This suggests that the absence of megafaunal methane emissions may have contributed to the abrupt climatic cooling at the onset of the Younger Dryas. The decrease in atmospheric methane that occurred at that time, as recorded in ice cores, was 2–4 times more rapid than any other decrease in the last half million years, suggesting that an unusual mechanism was at work.

The hyperdisease hypothesis, proposed by Ross MacPhee in 1997, states that the megafaunal die-off was due to an indirect transmission of diseases by newly arriving aboriginal humans. According to MacPhee, aboriginals or animals travelling with them, such as domestic dogs or livestock, introduced one or more highly virulent diseases into new environments whose native population had no immunity to them, eventually leading to their extinction. K-selection animals, such as the now-extinct megafauna, are especially vulnerable to diseases, as opposed to r-selection animals who have a shorter gestation period and a higher population size. Humans are thought to be the sole cause as other earlier migrations of animals into North America from Eurasia did not cause extinctions.

There are many problems with this theory, as this disease would have to meet several criteria: it has to be able to sustain itself in an environment with no hosts; it has to have a high infection rate; and be extremely lethal, with a mortality rate of 50–75%. Disease has to be very virulent to kill off all the individuals in a genus or species, and even such a virulent disease as West Nile fever is unlikely to have caused extinction.

However, diseases have been the cause for some extinctions. The introduction of avian malaria and avipoxvirus, for example, have had a negative impact on the endemic birds of Hawaii.

The loss of species from ecological communities, defaunation, is primarily driven by human activity. This has resulted in empty forests, ecological communities depleted of large vertebrates. This is not to be confused with extinction, as it includes both the disappearance of species and declines in abundance. Defaunation effects were first implied at the Symposium of Plant-Animal Interactions at the University of Campinas, Brazil in 1988 in the context of Neotropical forests. Since then, the term has gained broader usage in conservation biology as a global phenomenon.

Big cat populations have severely declined over the last half-century and could face extinction in the following decades. According to IUCN estimates: lions are down to 25,000, from 450,000; leopards are down to 50,000, from 750,000; cheetahs are down to 12,000, from 45,000; tigers are down to 3,000 in the wild, from 50,000. A December 2016 study by the Zoological Society of London, Panthera Corporation and Wildlife Conservation Society showed that cheetahs are far closer to extinction than previously thought, with only 7,100 remaining in the wild, and crammed within only 9% of their historic range. Human pressures are to blame for the cheetah population crash, including prey loss due to overhunting by people, retaliatory killing from farmers, habitat loss and the illegal wildlife trade.

The term pollinator decline refers to the reduction in abundance of insect and other animal pollinators in many ecosystems worldwide beginning at the end of the twentieth century, and continuing into the present day. Pollinators, which are necessary for 75% of food crops, are declining globally in both abundance and diversity. A 2017 study led by Radboud University's Hans de Kroon indicated that the biomass of insect life in Germany had declined by three-quarters in the previous 25 years. Participating researcher Dave Goulson of Sussex University stated that their study suggested that humans are making large parts of the planet uninhabitable for wildlife. Goulson characterized the situation as an approaching "ecological Armageddon", adding that "if we lose the insects then everything is going to collapse." As of 2019, 40% of insect species are in decline, and a third are endangered. The most significant drivers in the decline of insect populations are associated with intensive farming practices, along with pesticide use and climate change.

Various species are predicted to become extinct in the near future, among them the rhinoceros, nonhuman primates. pangolins, and giraffes. Hunting alone threatens bird and mammalian populations around the world. The direct killing of megafauna for meat and body parts is the primary driver of their destruction, with 70% of the 362 megafauna species in decline as of 2019. Mammals in particular have suffered such severe losses as the result of human activity that it could take several million years for them to recover. According to the WWF's 2016 "Living Planet Report", global wildlife populations have declined 58% since 1970, primarily due to habitat destruction, over-hunting and pollution. They project that if current trends continue, 67% of wildlife could disappear by 2020. In their 2018 report, the WWF found that overconsumption of resources by the global population has destroyed 60% of animal populations since 1970, and this continued destruction of wildlife is an emergency which threatens the survival of human civilization. 189 countries, which are signatory to the Convention on Biological Diversity (Rio Accord), have committed to preparing a Biodiversity Action Plan, a first step at identifying specific endangered species and habitats, country by country.
A June 2020 study published in "PNAS" posits that the contemporary extinction crisis "may be the most serious environmental threat to the persistence of civilization, because it is irreversible" and that its acceleration "is certain because of the still fast growth in human numbers and consumption rates." The study found that more than 500 vertebrate species are poised to be lost in the next two decades.

Recent extinctions are more directly attributable to human influences, whereas prehistoric extinctions can be attributed to other factors, such as global climate change. The International Union for Conservation of Nature (IUCN) characterises 'recent' extinction as those that have occurred past the cut-off point of 1500, and at least 875 species have gone extinct since that time and 2012. Some species, such as the Père David's deer and the Hawaiian crow, are extinct in the wild, and survive solely in captive populations. Other species, such as the Florida panther, are ecologically extinct, surviving in such low numbers that they essentially have no impact on the ecosystem. Other populations are only locally extinct (extirpated), still existence elsewhere, but reduced in distribution, as with the extinction of gray whales in the Atlantic, and of the leatherback sea turtle in Malaysia.

Most recently, insect populations have experienced rapid surprising declines. Insects have declined at an annual rate of 2.5% over the last 25–30 years. The most severe effects may include Puerto Rico, where insect ground fall has declined by 98% in the previous 35 years. Butterflies and moths are experiencing some of the most severe effect. Butterfly species have declined by 58% on farmland in England. In the last ten years, 40% of insect species and 22% of mammal species have disappeared. Germany is experiencing a 75% decline. Climate change and agriculture are believed to be the most significant contributors to the change.

A 2019 study published in "Nature Communications" found that rapid biodiversity loss is impacting larger mammals and birds to a much greater extent than smaller ones, with the body mass of such animals expected to shrink by 25% over the next century. Over the past 125,000 years, the average body size of wildlife has fallen by 14% as human actions eradicated megafauna on all continents with the exception of Africa. Another 2019 study published in "Biology Letters" found that extinction rates are perhaps much higher than previously estimated, in particular for bird species.

Global warming is widely accepted as being a contributor to extinction worldwide, in a similar way that previous extinction events have generally included a rapid change in global climate and meteorology. It is also expected to disrupt sex ratios in many reptiles which have temperature-dependent sex determination.

The removal of land to clear way for palm oil plantations releases carbon emissions held in the peatlands of Indonesia. Palm oil mainly serves as a cheap cooking oil, and also as a (controversial) biofuel. However, damage to peatland contributes to 4% of global greenhouse gas emissions, and 8% of those caused by burning fossil fuels. Palm oil cultivation has also been criticized for other impacts to the environment, including deforestation, which has threatened critically endangered species such as the orangutan and the tree-kangaroo. The IUCN stated in 2016 that the species could go extinct within a decade if measures are not taken to preserve the rainforests in which they live.

Some scientists and academics assert that industrial agriculture and the growing demand for meat is contributing to significant global biodiversity loss as this is a significant driver of deforestation and habitat destruction; species-rich habitats, such as significant portions of the Amazon region, are being converted to agriculture for meat production. A 2017 study by the World Wildlife Fund (WWF) found that 60% of biodiversity loss can be attributed to the vast scale of feed crop cultivation required to rear tens of billions of farm animals. Moreover, a 2006 report by the Food and Agriculture Organization (FAO) of the United Nations, "Livestock's Long Shadow", also found that the livestock sector is a "leading player" in biodiversity loss. More recently, in 2019, the IPBES "Global Assessment Report on Biodiversity and Ecosystem Services" attributed much of this ecological destruction to agriculture and fishing, with the meat and dairy industries having a very significant impact. Since the 1970s food production has soared in order to feed a growing human population and bolster economic growth, but at a huge price to the environment and other species. The report says some 25% of the earth's ice-free land is used for cattle grazing. A 2020 study published in "Nature Communications" warned that human impacts from housing, industrial agriculture and in particular meat consumption are wiping out 50 billion years of earth's evolutionary history and driving to extinction some of the "most unique animals on the planet," among them the Aye-aye lemur, the Chinese crocodile lizard and the pangolin. Said lead author Rikki Gumbs:

Rising levels of carbon dioxide are resulting in influx of this gas into the ocean, increasing its acidity. Marine organisms which possess calcium carbonate shells or exoskeletons experience physiological pressure as the carbonate reacts with acid. For example, this is already resulting in coral bleaching on various coral reefs worldwide, which provide valuable habitat and maintain a high biodiversity. Marine gastropods, bivalves and other invertebrates are also affected, as are the organisms that feed on them. According to a 2018 study published in "Science", global Orca populations are poised to collapse due to toxic chemical and PCB pollution. PCBs are still leaking into the sea in spite of being banned for decades.

Some researchers suggest that by 2050 there could be more plastic than fish in the oceans by weight, with about of plastic being discharged into the oceans annually. Single-use plastics, such as plastic shopping bags, make up the bulk of this, and can often be ingested by marine life, such as with sea turtles. These plastics can degrade into microplastics, smaller particles that can affect a larger array of species. Microplastics make up the bulk of the Great Pacific Garbage Patch, and their smaller size is detrimental to cleanup efforts.

In March 2019, "Nature Climate Change" published a study by ecologists from Yale University, who found that over the next half century, human land use will reduce the habitats of 1,700 species by up to 50%, pushing them closer to extinction. That same month "PLOS Biology" published a similar study drawing on work at the University of Queensland, which found that "more than 1,200 species globally face threats to their survival in more than 90% of their habitat and will almost certainly face extinction without conservation intervention".

Since 1970, the populations of migratory freshwater fish have declined by 76%, according to research published by the Zoological Society of London in July 2020. Overall, around one in three freshwater fish species are threatened with extinction due to human-driven habitat degradation and overfishing.

Overhunting can reduce the local population of game animals by more than half, as well as reducing population density, and may lead to extinction for some species. Populations located nearer to villages are significantly more at risk of depletion. Several conservationist organizations, among them IFAW and HSUS, assert that trophy hunters, particularly from the United States, are playing a significant role in the decline of giraffes, which they refer to as a "silent extinction".

The surge in the mass killings by poachers involved in the illegal ivory trade along with habitat loss is threatening African elephant populations. In 1979, their populations stood at 1.7 million; at present there are fewer than 400,000 remaining. Prior to European colonization, scientists believe Africa was home to roughly 20 million elephants. According to the Great Elephant Census, 30% of African elephants (or 144,000 individuals) disappeared over a seven-year period, 2007 to 2014. African elephants could become extinct by 2035 if poaching rates continue.

Fishing has had a devastating effect on marine organism populations for several centuries even before the explosion of destructive and highly effective fishing practices like trawling. Humans are unique among predators in that they regularly prey on other adult apex predators, particularly in marine environments; bluefin tuna, blue whales, North Atlantic right whales and over fifty species of sharks and rays are vulnerable to predation pressure from human fishing, in particular commercial fishing. A 2016 study published in "Science" concludes that humans tend to hunt larger species, and this could disrupt ocean ecosystems for millions of years. A 2020 study published in "Science Advances" found that around 18% of marine megafauna, including iconic species such as the Great white shark, are at risk of extinction from human pressures over the next century. In a worst-case scenario, 40% could go extinct over the same time period.

The decline of amphibian populations has also been identified as an indicator of environmental degradation. As well as habitat loss, introduced predators and pollution, Chytridiomycosis, a fungal infection accidentally spread by human travel, globalization and the wildlife trade, has caused severe population drops of over 500 amphibian species, and perhaps 90 extinctions, including (among many others) the extinction of the golden toad in Costa Rica and the Gastric-brooding frog in Australia. Many other amphibian species now face extinction, including the reduction of Rabb's fringe-limbed treefrog to an endling, and the extinction of the Panamanian golden frog in the wild. Chytrid fungus has spread across Australia, New Zealand, Central America and Africa, including countries with high amphibian diversity such as cloud forests in Honduras and Madagascar. "Batrachochytrium salamandrivorans" is a similar infection currently threatening salamanders. Amphibians are now the most endangered vertebrate group, having existed for more than 300 million years through three other mass extinctions.

Millions of bats in the US have been dying off since 2012 due to a fungal infection spread from European bats, which appear to be immune. Population drops have been as great as 90% within five years, and extinction of at least one bat species is predicted. There is currently no form of treatment, and such declines have been described as "unprecedented" in bat evolutionary history by Alan Hicks of the New York State Department of Environmental Conservation.

Between 2007 and 2013, over ten million beehives were abandoned due to colony collapse disorder, which causes worker bees to abandon the queen. Though no single cause has gained widespread acceptance by the scientific community, proposals include infections with "Varroa" and "Acarapis" mites; malnutrition; various pathogens; genetic factors; immunodeficiencies; loss of habitat; changing beekeeping practices; or a combination of factors.

Some leading scientists have advocated for the global community to designate as protected areas 30 percent of the planet by 2030, and 50 percent by 2050, in order to mitigate the contemporary extinction crisis as the human population is projected to grow to 10 billion by the middle of the century. Human consumption of food and water resources is also projected to double by this time.

In November 2018, the UN's biodiversity chief Cristiana Pașca Palmer urged people around the world to put pressure on governments to implement significant protections for wildlife by 2020, as rampant biodiversity loss is a "silent killer" as dangerous as global warming, but has received little attention by comparison. She says that "It’s different from climate change, where people feel the impact in everyday life. With biodiversity, it is not so clear but by the time you feel what is happening, it may be too late." In January 2020, the UN Convention on Biological Diversity drafted a Paris-style plan to stop biodiversity and ecosystem collapse by setting a deadline of 2030 to protect 30% of the earth's land and oceans and reduce pollution by 50%, with the goal of allowing for the restoration of ecosystems by 2050. The world failed to meet similar targets for 2020 set by the convention during a summit in Japan in 2010.

Some scientists have proposed keeping extinctions below 20 per year for the next century as a global target to reduce species loss, which is the biodiversity equivalent of the 2 °C climate target, although it is still much higher than the normal background rate of two per year prior to anthropogenic impacts on the natural world.



</doc>
<doc id="14210" url="https://en.wikipedia.org/wiki?curid=14210" title="Harrison Narcotics Tax Act">
Harrison Narcotics Tax Act

The Harrison Narcotics Tax Act (Ch. 1, ) was a United States federal law that regulated and taxed the production, importation, and distribution of opiates and coca products. The act was proposed by Representative Francis Burton Harrison of New York and was approved on December 17, 1914.

"An Act To provide for the registration of, with collectors of internal revenue, and to impose a special tax on all persons who produce, import, manufacture, compound, deal in, dispense, sell, distribute, or give away opium or coca leaves, their salts, derivatives, or preparations, and for other purposes." The courts interpreted this to mean that physicians could prescribe narcotics to patients in the course of normal treatment, but not for the treatment of addiction.

The Harrison Anti-Narcotic legislation consisted of three U.S. House bills imposing restrictions on the availability and consumption of the psychoactive drug opium. U.S. House bills and passed conjointly with House bill or the Opium and Coca Leaves Trade Restrictions Act.

Although technically illegal for purposes of distribution and use, the distribution, sale and use of cocaine was still legal for registered companies and individuals.

Following the Spanish–American War the U.S. acquired the Philippines from Spain. At that time, opium addiction constituted a significant problem in the civilian population of the Philippines.

Charles Henry Brent was an American Episcopal bishop who served as Missionary Bishop of the Philippines beginning in 1901. He convened a Commission of Inquiry, known as the Brent Commission, for the purpose of examining alternatives to a licensing system for opium addicts. The Commission recommended that narcotics should be subject to international control. The recommendations of the Brent Commission were endorsed by the United States Department of State and in 1906 President Theodore Roosevelt called for an international conference, the International Opium Commission, which was held in Shanghai in February 1909. A second conference was held at The Hague in May 1911, and out of it came the first international drug control treaty, the International Opium Convention of 1912.

In the 1800s, opiates and cocaine were mostly unregulated drugs. In the 1890s, the Sears & Roebuck catalogue, which was distributed to millions of Americans homes, offered a syringe and a small amount of cocaine for $1.50. On the other hand, as early as 1880, some states and localities had already passed laws against smoking opium, at least in public, in the Los Angeles Herald, mentioning the city law against opium smoking.

At the beginning of the 20th century, cocaine began to be linked to crime. In 1900, the "Journal of the American Medical Association" published an editorial stating, "Negroes in the South are reported as being addicted to a new form of vice – that of 'cocaine sniffing' or the 'coke habit.'" Some newspapers later claimed cocaine use caused blacks to rape white women and was improving their pistol marksmanship. Chinese immigrants were blamed for importing the opium-smoking habit to the U.S. The 1903 blue-ribbon citizens' panel, the Committee on the Acquirement of the Drug Habit, concluded, "If the Chinaman cannot get along without his dope we can get along without him."

Theodore Roosevelt appointed Dr. Hamilton Wright as the first Opium Commissioner of the United States in 1908. In 1909, Wright attended the International Opium Commission in Shanghai as the American delegate. He was accompanied by Charles Henry Brent, the Episcopal Bishop. On March 12, 1911, Wright was quoted in an article in "The New York Times": "Of all the nations of the world, the United States consumes most habit-forming drugs per capita. Opium, the most pernicious drug known to humanity, is surrounded, in this country, with far fewer safeguards than any other nation in Europe fences it with." He further claimed that "it has been authoritatively stated that cocaine is often the direct incentive to the crime of rape by the negroes of the South and other sections of the country". He also stated that "one of the most unfortunate phases of smoking opium in this country is the large number of women who have become involved and were living as common-law wives or cohabitating with Chinese in the Chinatowns of our various cities".

Opium usage had begun to decline by 1914 after rising dramatically in the post Civil War Era, peaking at around one-half million pounds per year in 1896. Demand gradually declined thereafter in response to mounting public concern, local and state regulations, and the Pure Food and Drugs Act of 1906, which required labeling of patent medicines that contained opiates, cocaine, alcohol, cannabis and other intoxicants. As of 1911, an estimated one U.S. citizen in 400 (0.25%) was addicted to some form of opium. The opium addicts were mostly women who were prescribed and dispensed legal opiates by physicians and pharmacist for "female problems" (probably pain at menstruation) or white men and Chinese at the Opium dens. Between two-thirds and three-quarters of these addicts were women. By 1914, forty-six states had regulations on cocaine and twenty-nine states had laws against opium, morphine, and heroin.

Several authors have argued that the debate was merely to regulate trade and collect a tax. However, the committee report prior to the debate on the house floor and the debate itself, discussed the rise of opiate use in the United States. Harrison stated that "The purpose of this Bill can hardly be said to raise revenue, because it prohibits the importation of something upon which we have hitherto collected revenue." Later Harrison stated, "We are not attempting to collect revenue, but regulate commerce." House representative Thomas Sisson stated, "The purpose of this bill—and we are all in sympathy with it—is to prevent the use of opium in the United States, destructive as it is to human happiness and human life."

The drafters played on fears of "drug-crazed, sex-mad negroes" and made references to Negroes under the influence of drugs murdering whites, degenerate Mexicans smoking marijuana, and "Chinamen" seducing white women with drugs. Dr. Hamilton Wright, testified at a hearing for the Harrison Act. Wright alleged that drugs made blacks uncontrollable, gave them superhuman powers and caused them to rebel against white authority. Dr. Christopher Koch of the State Pharmacy Board of Pennsylvania testified that "Most of the attacks upon the white women of the South are the direct result of a cocaine-crazed Negro brain".

Before the Act was passed, on February 8, 1914, "The New York Times" published an article entitled "Negro Cocaine 'Fiends' Are New Southern Menace: Murder and Insanity Increasing Among Lower-Class Blacks" by Edward Huntington Williams, which reported that Southern sheriffs had increased the caliber of their weapons from .32 to .38 to bring down Negroes under the effect of cocaine.

Despite the extreme racialization of the issue that took place in the buildup to the Act's passage, the contemporary research on the subject indicated that black Americans were using cocaine and opium at much lower rates than white Americans.

Enforcement began in 1915.

The act appears to be concerned about the marketing of opiates. However, a clause applying to doctors allowed distribution "in the course of his professional practice only." This clause was interpreted after 1917 to mean that a doctor could not prescribe opiates to an addict, since addiction was not considered a disease. A number of doctors were arrested and some were imprisoned. The medical profession quickly learned not to supply opiates to addicts. In "United States v. Doremus", 249 U.S. 86 (1919), the Supreme Court ruled that the Harrison Act was constitutional, and in "Webb v. United States", 249 U.S. 96, 99 (1919) that physicians could not prescribe narcotics solely for maintenance.

The impact of diminished supply was obvious by mid-1915. A 1918 commission called for sterner law enforcement, while newspapers published sensational articles about addiction-related crime waves. Congress responded by tightening up the Harrison Act—the importation of heroin for any purpose was banned in 1924.

After other complementary laws (for example implementing the Uniform State Narcotic Drug Act in 1934) and other actions by the government, the number of addicts of opium started to decrease fast from 1925 to a level that in 1945 that was about one tenth of the level in 1914.

The use of the term 'narcotics' in the title of the act to describe not just opiates but also cocaine—which is a central nervous system stimulant, not a narcotic—initiated a precedent of frequent legislative and judicial misclassification of various substances as 'narcotics'. Today, law enforcement agencies, popular media, the United Nations, other nations and even some medical practitioners can be observed applying the term very broadly and often pejoratively in reference to a wide range of illicit substances, regardless of the more precise definition existing in medical contexts. For this reason, however, 'narcotic' has come to mean any illegally used drug, but it is useful as a shorthand for referring to a controlled drug in a context where its legal status is more important than its physiological effects.

The remaining effect of this act, which has largely been superseded by the Controlled Substances Act of 1970, is the warning "*Warning: May be habit forming" on labels, package inserts, and other places where ingredients are listed in the case of many opioids, barbiturates, medicinal formulations of cocaine, and chloral hydrate.

The act also marks the beginning of the creation of the modern, criminal drug addict and the American black market for drugs. Within five years the Rainey Committee, a Special Committee on Investigation appointed by Secretary of the Treasury William Gibbs McAdoo and led by Congressman T. Rainey, reported in June, 1919 that drugs were being smuggled into the country by sea, and across the Mexican and Canadian borders by nationally established organisations and that the United States consumed 470,000 pounds of opium annually, compared to 17,000 pounds in both France and Germany. The Monthly Summary of Foreign Commerce of the United States recorded that in the 7 months to January 1920, 528,635 pounds of opium was imported, compared to 74,650 pounds in the same period in 1919.

The Act's applicability in prosecuting doctors who prescribe narcotics to addicts was successfully challenged in "Linder v. United States" in 1925, as Justice McReynolds ruled that the federal government has no power to regulate medical practice.



</doc>
<doc id="14215" url="https://en.wikipedia.org/wiki?curid=14215" title="Horse tack">
Horse tack

Tack is equipment or accessories equipped on horses and other equines in the course of their use as domesticated animals. Saddles, stirrups, bridles, halters, reins, bits, harnesses, martingales, and breastplates are all forms of horse tack. Equipping a horse is often referred to as tacking up. A room to store such equipment, usually near or in a stable, is a tack room.

Saddles are seats for the rider, fastened to the horse's back by means of a "girth" (English-style riding), known as a "cinch" in the Western US, a wide strap that goes around the horse at a point about four inches behind the forelegs. Some western saddles will also have a second strap known as a "flank" or "back cinch" that fastens at the rear of the saddle and goes around the widest part of the horse's belly.

It is important that the saddle be comfortable for both the rider and the horse as an improperly fitting saddle may create pressure points on the horse's back muscle (Latissimus dorsi) and cause the horse pain and can lead to the horse, rider, or both getting injured.

There are many types of saddle, each specially designed for its given task.
Saddles are usually divided into two major categories: "English saddles" and "Western saddles" according to the riding discipline they are used in. Other types of saddles, such as racing saddles, Australian saddles, sidesaddles and endurance saddles do not necessarily fit neatly in either category.


Stirrups are supports for the rider's feet that hang down on either side of the saddle. They provide greater stability for the rider but can have safety concerns due to the potential for a rider's feet to get stuck in them. If a rider is thrown from a horse but has a foot caught in the stirrup, they could be dragged if the horse runs away. To minimize this risk, a number of safety precautions are taken. First, most riders wear riding boots with a heel and a smooth sole. Next, some saddles, particularly English saddles, have safety bars that allow a stirrup leather to fall off the saddle if pulled backwards by a falling rider. Other precautions are done with stirrup design itself. Western saddles have wide stirrup treads that make it more difficult for the foot to become trapped. A number of saddle styles incorporate a tapedero, which is covering over the front of the stirrup that keeps the foot from sliding all the way through the stirrup. The English stirrup (or "iron") has several design variations which are either shaped to allow the rider's foot to slip out easily or are closed with a very heavy rubber band. The invention of stirrups was of great historic significance in mounted combat, giving the rider secure foot support while on horseback.

"Bridles", hackamores, "halters" or "headcollars", and similar equipment consist of various arrangements of straps around the horse's head, and are used for control and communication with the animal.

A "halter" (US) or "headcollar" (UK) (occasionally "headstall") consists of a noseband and headstall that buckles around the horse's head and allows the horse to be led or tied. The lead rope is separate, and it may be short (from six to ten feet, two to three meters) for everyday leading and tying, or much longer (up to , eight meters) for tasks such as for leading packhorses or for picketing a horse out to graze.

Some horses, particularly stallions, may have a chain attached to the lead rope and placed over the nose or under the jaw to increase the control provided by a halter while being led. Most of the time, horses are not ridden with a halter, as it offers insufficient precision and control. Halters have no bit.

In Australian and British English, a "halter" is a rope with a spliced running loop around the nose and another over the poll, used mainly for unbroken horses or for cattle. The lead rope cannot be removed from the halter. A show halter is made from rolled leather and the lead attaches to form the chinpiece of the noseband. These halters are not suitable for paddock usage or in loose stalls. An "underhalter" is a lightweight halter or headcollar which is made with only one small buckle, and can be worn under a bridle for tethering a horse without untacking.

Bridles usually have a "bit" attached to "reins" and are used for riding and driving horses.

"English Bridles" have a "cavesson" style noseband and are seen in English riding. Their reins are buckled to one another, and they have little adornment or flashy hardware.

"Western Bridles" used in Western riding usually have no noseband, are made of thin bridle leather. They may have long, separated "Split" reins or shorter closed reins, which sometimes include an attached "Romal". Western bridles are often adorned with silver or other decorative features.

"Double bridles" are a type of English bridle that use two bits in the mouth at once, a snaffle and a curb. The two bits allow the rider to have very precise control of the horse. As a rule, only very advanced horses and riders use double bridles. Double bridles are usually seen in the top levels of dressage, but also are seen in certain types of show hack and Saddle seat competition.

A "hackamore" is a headgear that utilizes a heavy noseband of some sort, rather than a bit, most often used to train young horses or to go easy on an older horse's mouth. Hackamores are more often seen in western riding. Some related styles of headgear that control a horse with a noseband rather than a bit are known as bitless bridles.

The word "hackamore" is derived from the Spanish word "jáquima." Hackamores are seen in western riding disciplines, as well as in endurance riding and English riding disciplines such as show jumping and the stadium phase of eventing. While the classic bosal-style hackamore is usually used to start young horses, other designs, such as various bitless bridles and the mechanical hackamore are often seen on mature horses with dental issues that make bit use painful, horses with certain training problems, and on horses with mouth or tongue injuries. Some riders also like to use them in the winter to avoid putting a frozen metal bit into a horse's mouth.

Like bitted bridles, noseband-based designs can be gentle or harsh, depending on the hands of the rider. It is a myth that a bit is cruel and a hackamore is gentler. The horse's face is very soft and sensitive with many nerve endings. Misuse of a hackamore can cause swelling on the nose, scraping on the nose and jawbone, and extreme misuse may cause damage to the bones and cartilage of the horse's head.

A "longeing cavesson" (UK: "lungeing") is a special type of halter or noseband used for longeing a horse. Longeing is the activity of having a horse walk, trot and/or canter in a large circle around the handler at the end of a rope that is 25 to long. It is used for training and exercise.

Reins consist of leather straps or rope attached to the outer ends of a "bit" and extend to the rider's or driver's hands. Reins are the means by which a horse rider or driver communicates directional commands to the horse's head. Pulling on the reins can be used to steer or stop the horse. The sides of a horse's mouth are sensitive, so pulling on the reins pulls the bit, which then pulls the horse's head from side to side, which is how the horse is controlled.

On some types of harnesses there might be supporting rings to carry the reins over the horse's back. When pairs of horses are used in drawing a wagon or coach it is usual for the outer side of each pair to be connected to reins and the inside of the bits connected by a short bridging strap or rope. The driver carries "four-in-hand" or "six-in-hand" being the number of reins connecting to the pairs of horses.

A rein may be attached to a halter to lead or guide the horse in a circle for training purposes or to lead a packhorse, but a simple lead rope is more often used for these purposes. A longe line is sometimes called a "longe rein," but it is actually a flat line about long, usually made of nylon or cotton web, about one inch wide, thus longer and wider than even a driving rein.

Horses should never be tied by the reins. Not only do they break easily, but, being attached to a bit in the horse's sensitive mouth, a great deal of pain can be inflicted if a bridled horse sets back against being tied.

A bit is a device placed in a horse's mouth, kept on a horse's head by means of a headstall. There are many types, each useful for specific types of riding and training.

The mouthpiece of the bit does not rest on the teeth of the horse, but rather rests on the gums or "bars" of the horse's mouth in an interdental space behind the front incisors and in front of the back molars. It is important that the style of bit is appropriate to the horse's needs and is fitted properly for it to function properly and be as comfortable as possible for the horse.

The basic "classic" styles of bits are:

While there are literally hundreds of types of bit mouthpieces, bit rings and bit shanks, essentially there are really only two broad categories: direct pressure bits, broadly termed snaffle bits; and leverage bits, usually termed curbs.

Bits that act with direct pressure on the tongue and lips of the bit are in the general category of "snaffle" bits. Snaffle bits commonly have a single jointed mouthpiece and act with a nutcracker effect on the bars, tongue and occasionally roof of the mouth. However, regardless of mouthpiece, any bit that operates only on direct pressure is a "snaffle" bit.

Leverage bits have shanks coming off the mouthpiece to create leverage that applies pressure to the poll, chin groove and mouth of the horse are in the category of "curb" bits. Any bit with shanks that works off of leverage is a "curb" bit, regardless of whether the mouthpiece is solid or jointed.

Some combination or hybrid bits combine direct pressure and leverage, such as the Kimblewick or Kimberwicke, which adds slight leverage to a two-rein design that resembles a snaffle; and the four rein designs such as the single mouthpiece Pelham bit and the double bridle, which places a curb and a snaffle bit simultaneously in the horse's mouth.

In the wrong hands even the mildest bit can hurt the horse. Conversely, a very severe bit, in the right hands, can transmit subtle commands that cause no pain to the horse. Bit commands should be given with only the quietest movements of the hands, and much steering and stopping should be done with the legs and seat.

A horse harness is a set of devices and straps that attaches a horse to a cart, carriage, sledge or any other load. There are two main styles of harnesses - breaststrap and collar and hames style. These differ in how the weight of the load is attached. Most Harnesses are made from leather, which is the traditional material for harnesses, though some designs are now made of nylon webbing or synthetic biothane.

A breaststrap harness has a wide leather strap going horizontally across the horses' breast, attached to the traces and then to the load. This is used only for lighter loads. A collar and hames harness has a collar around the horses' neck with wood or metal hames in the collar. The traces attach from the hames to the load. This type of harness is needed for heavy draft work.

Both types will also have a bridle and reins. A harness that is used to support shafts, such as on a cart pulled by a single horse, will also have a "saddle" attached to the harness to help the horse support the shafts and "breeching" to brake the forward motion of the vehicle, especially when stopping or moving downhill. Horses guiding vehicles by means of a pole, such as two-horse teams pulling a wagon, a hay-mower, or a dray, will have "pole-straps" attached to the lower part of the horse collar.

Breastplates, breastcollars or breastgirths attach to the front of the saddle, cross the horse's chest, and usually have a strap that runs between the horse's front legs and attaches to the girth. They keep the saddle from sliding back or sideways. They are usually seen in demanding, fast-paced sports. They are crucial pieces of safety equipment for English riding activities requiring jumping, such as eventing, show jumping, polo, and fox hunting. They are also seen in Western riding events, particularly in rodeo, reining and cutting, where it is particularly important to prevent a saddle from shifting. They may also be worn in other horse show classes for decorative purposes.

A martingale is a piece of equipment that keeps a horse from raising its head too high. Various styles can be used as a control measure, to prevent the horse from avoiding rider commands by raising its head out of position; or as a safety measure to keep the horse from tossing its head high or hard enough to smack its rider in the face.

They are allowed in many types of competition, especially those where speed or jumping may be required, but are not allowed in most "flat" classes at horse shows, though an exception is made in a few classes limited exclusively to young or "green" horses who may not yet be fully trained.

Martingales are usually attached to the horse one of two ways. They are either attached to the center chest ring of a breastplate or, if no breastplate is worn, they are attached by two straps, one that goes around the horse's neck, and the other that attaches to the girth, with the martingale itself beginning at the point in the center of the chest where the neck and girth straps intersect.

Martingale types include:



There are other training devices that fall loosely in the martingale category, in that they use straps attached to the reins or bit which limit the movement of the horse's head or add leverage to the rider's hands in order to control the horse's head. Common devices of this nature include the overcheck, the chambon, de Gogue, grazing reins, draw reins and the "bitting harness" or "bitting rig". However, most of this equipment is used for training purposes and is not legal in any competition. In some disciplines, use of leverage devices, even in training, is controversial.




</doc>
<doc id="14216" url="https://en.wikipedia.org/wiki?curid=14216" title="Hausa language">
Hausa language

Hausa (; /) is a Chadic language spoken by the Hausa people, the largest ethnic group in Sub-Saharan Africa, mainly within the territories of Niger and the northern half of Nigeria, and with significant minorities in Ghana, Sudan, and Cameroon.

Hausa is a member of the Afroasiatic language family and is the most widely spoken language within the Chadic branch of that family. Ethnologue estimated that it was spoken as a first language by some 47 million people and as a second language by another 25 million, bringing the total number of Hausa speakers to an estimated 72 million. By more recent estimations, Hausa is estimated to be spoken by 100–150 million people, making it the most spoken indigenous African language.

Hausa belongs to the West Chadic languages subgroup of the Chadic languages group, which in turn is part of the Afroasiatic language family. Other Afroasiatic languages are Semitic languages including Arabic, Aramaic languages, Hebrew, extinct Phoenician, the extinct Akkadian, as well as the Ethio-Semitic Amharic, Tigre, Tigrinya, and Gurage; Berber languages; Cushitic languages including Somali and Oromo; other Chadic languages including Glavda, Babur, Mwaghavul, Tera, Tangale, Karekare, Bole, Sayawa, Bwatiye, Ngas, Bade, Gwandara, Galambu, Pali, Higi, Ron, Duhwa, Margi, Kilba, Duwai and many others.

Roger Blench (2011) has found that Hausa in fact has many words that are not found in other Chadic languages. This is because Hausa had borrowed from Adamawa, Plateau, Kainji, Nupoid, and other Benue-Congo languages during its expansion across the Nigerian Middle Belt. While those languages became assimilated, many of their words had changed the lexicon of Hausa. Some likely influence from vanished Nilo-Saharan languages on Hausa was also proposed by Blench. Blench (2011) lists these Hausa words of likely Benue-Congo origin:

Native speakers of Hausa, the Hausa people, are mostly found in Niger, in Northern Nigeria, and in Chad. Furthermore, the language is used as a "lingua franca" by non-native speakers in most of Northern Nigeria and Southern Niger, and as a trade language across a much larger swathe of West Africa (Benin, Ghana, Cameroon, Togo, Ivory Coast) and parts of Sudan.

Eastern Hausa dialects include "Dauranci" in Daura, "Kananci" in Kano, "Bausanci" in Bauchi, "Gudduranci" in Katagum Misau and part of Borno, and "Hadejanci" in Hadejiya.

Western Hausa dialects include "Sakkwatanci" in Sokoto, "Katsinanci" in Katsina, "Arewanci" in Gobir, Adar, Kebbi, and Zamfara, and "Kurhwayanci" in Kurfey in Niger. Katsina is transitional between Eastern and Western dialects. Sokoto is used in a variety of classical Hausa literature, and is often known as "Classical Hausa".

Northern Hausa dialects include "Arewa" (meaning 'North') and "Arewaci".

"Zazzaganci" in Zazzau is the major Southern dialect.

The Daura ("Dauranchi") and Kano ("Kananci") dialect are the standard. The BBC, Deutsche Welle, Radio France Internationale and Voice of America offer Hausa services on their international news web sites using Dauranci and Kananci. In recent language development Zazzaganci took over the innovation of writing and speaking the current Hausa language use.

The western to eastern Hausa dialects of "Kurhwayanci", Dam"agaram" and "Aderawa", represent the traditional northernmost limit of native Hausa communities. These are spoken in the northernmost sahel and mid-Saharan regions in west and central Niger in the Tillaberi, Tahoua, Dosso, Maradi, Agadez and Zinder regions. While mutually comprehensible with other dialects (especially "Sakkwatanci", and to a lesser extent "Gaananci"), the northernmost dialects have slight grammatical and lexical differences owing to frequent contact with the Zarma, Fula, and Tuareg groups and cultural changes owing to the geographical differences between the grassland and desert zones. These dialects also have the quality of bordering on non-tonal pitch accent dialects.

This link between non-tonality and geographic location is not limited to Hausa alone, but is exhibited in other northern dialects of neighbouring languages; such as differences within the Songhay language (between the non-tonal northernmost dialects of Koyra Chiini in Timbuktu and Koyraboro Senni in Gao; and the tonal southern Zarma dialect, spoken from western Niger to northern Ghana), and within the Soninke language (between the non-tonal northernmost dialects of Imraguen and Nemadi spoken in east-central Mauritania; and the tonal southern dialects of Senegal, Mali and the Sahel).

The Ghanaian Hausa dialect ("Gaananci"), spoken in Ghana, Togo, and western Ivory Coast, is a distinct western native Hausa dialect-bloc with adequate linguistic and media resources available. Separate smaller Hausa dialects are spoken by an unknown number of Hausa further west in parts of Burkina Faso, and in the Haoussa Foulane, Badji Haoussa, Guezou Haoussa, and Ansongo districts of northeastern Mali (where it is designated as a minority language by the Malian government), but there are very little linguistic resources and research done on these particular dialects at this time.

Gaananci forms a separate group from other Western Hausa dialects, as it now falls outside the contiguous Hausa-dominant area, and is usually identified by the use of "c" for "ky", and "j" for "gy". This is attributed to the fact that Ghana's Hausa population descend from Hausa-Fulani traders settled in the zongo districts of major trade-towns up and down the previous Asante, Gonja and Dagomba kingdoms stretching from the sahel to coastal regions, in particular the cities of Tamale, Salaga, Bawku, Bolgatanga, Achimota, Nima and Kumasi.

Gaananci exhibits noted inflected influences from Zarma, Gur, Jula-Bambara, Akan, and Soninke, as Ghana is the westernmost area in which the Hausa language is a major lingua-franca among sahelian/Muslim West Africans, including both Ghanaian and non-Ghanaian zango migrants primarily from the northern regions, or Mali and Burkina Faso. Ghana also marks the westernmost boundary in which the Hausa people inhabit in any considerable number. Immediately west and north of Ghana (in Cote d'Ivoire, and Burkina Faso), Hausa is abruptly replaced with Dioula–Bambara as the main sahelian/Muslim lingua-franca of what become predominantly Manding areas, and native Hausa-speakers plummet to a very small urban minority.

Because of this, and the presence of surrounding Akan, Gbe, Gur and Mande languages, Gaananci was historically isolated from the other Hausa dialects. Despite this difference, grammatical similarities between "Sakkwatanci" and Ghanaian Hausa determine that the dialect, and the origin of the Ghanaian Hausa people themselves, are derived from the northwestern Hausa area surrounding Sokoto.

Hausa is also widely spoken by non-native Gur, and Mandé Ghanaian Muslims, but differs from Gaananci, and rather has features consistent with non-native Hausa dialects.

Hausa is also spoken in various parts of Cameroon and Chad, which combined the mixed dialects of Northern Nigeria and Niger. In addition, Arabic has had a great influence in the way Hausa is spoken by the native Hausa speakers in these areas.

In West Africa, Hausa's use as a lingua franca has given rise to a non-native pronunciation that differs vastly from native pronunciation by way of key omissions of implosive and ejective consonants present in native Hausa dialects, such as "ɗ", "ɓ" and "kʼ/ƙ", which are pronounced by non-native speakers as "d", "b" and "k" respectively. This creates confusion among non-native and native Hausa speakers, as non-native pronunciation does not distinguish words like ' ("correct") and ' ("one-by-one"). Another difference between native and non-native Hausa is the omission of vowel length in words and change in the standard tone of native Hausa dialects (ranging from native Fulani and Tuareg Hausa-speakers omitting tone altogether, to Hausa speakers with Gur or Yoruba mother tongues using additional tonal structures similar to those used in their native languages). Use of masculine and feminine gender nouns and sentence structure are usually omitted or interchanged, and many native Hausa nouns and verbs are substituted with non-native terms from local languages.

Non-native speakers of Hausa numbered more than 25 million and, in some areas, live close to native Hausa. It has replaced many other languages especially in the north-central and north-eastern part of Nigeria and continues to gain popularity in other parts of Africa as a result of Hausa movies and music which spread out throughout the region.

There are several pidgin forms of Hausa. Barikanchi was formerly used in the colonial army of Nigeria. Gibanawa is currently in widespread use in Jega in northwestern Nigeria, south of the native Hausa area.

Hausa has between 23 and 25 consonant phonemes depending on the speaker.

The three-way contrast between palatalized velars , plain velars , and labialized velars is found only before long and short , e.g. ('grass'), ('to increase'), ('shea-nuts'). Before front vowels, only palatalized and labialized velars occur, e.g. ('jealousy') vs. ('side of body'). Before rounded vowels, only labialized velars occur, e.g. ('ringworm').
Other analysis suggest the presence of the three additional phonemes .

Hausa has glottalic consonants (implosives and ejectives) at four or five places of articulation (depending on the dialect). They require movement of the glottis during pronunciation and have a staccato sound.

They are written with modified versions of Latin letters. They can also be denoted with an apostrophe, either before or after depending on the letter, as shown below.


Hausa has five phonetic vowel sounds, which can be either short or long, giving a total of 10 monophthongs. In addition, there are four joint vowels (diphthongs), giving a total number of 14 vowel phonemes.


In comparison with the long vowels, the short can be similar in quality to the long vowels, mid-centralized to or centralized to .

Medial can be neutralized to , with the rounding depending on the environment.

Medial are neutralized with .

The short can be either similar in quality to the long , or it can be as high as , with possible intermediate pronunciations ().


Hausa is a tonal language. Each of its five vowels may have low tone, high tone or falling tone. In standard written Hausa, tone is not marked. In recent linguistic and pedagogical materials, tone is marked by means of diacritics.

An acute accent () may be used for high tone, but the usual practice is to leave high tone unmarked.

Except for the Zaria and Bauchi dialects spoken south of Kano, Hausa distinguishes between masculine and feminine genders.

Hausa, like the rest of the Chadic languages, is known for its complex, irregular pluralization of nouns. Noun plurals in Hausa are derived using a variety of morphological processes, such as suffixation, infixation, reduplication, or a combination of any of these processes. There are 20 plural classes proposed by Newman (2000).

Hausa's modern official orthography is a Latin-based alphabet called "boko", which was introduced in the 1930s by the British colonial administration.
The letter "ƴ" (y with a right hook) is used only in Niger; in Nigeria it is written "ʼy".

Tone and vowel length are not marked in writing. So, for example, "from" and "battle" are both written "daga". The distinction between and (which does not exist for all speakers) is not always marked.

Hausa has also been written in "ajami", an Arabic alphabet, since the early 17th century. The first known work to be written in Hausa is Riwayar Nabi Musa by Abdullahi Suka in the 17th century. There is no standard system of using "ajami", and different writers may use letters with different values. Short vowels are written regularly with the help of vowel marks, which are seldom used in Arabic texts other than the Quran. Many medieval Hausa manuscripts in "ajami", similar to the Timbuktu Manuscripts, have been discovered recently; some of them even describe constellations and calendars.

In the following table, short and long "e" are shown along with the Arabic letter for "t" ().

Hausa is one of three indigenous languages of Nigeria which has been rendered in braille.

At least three other writing systems for Hausa have been proposed or "discovered". None of these are in active use beyond perhaps some individuals.





</doc>
<doc id="14220" url="https://en.wikipedia.org/wiki?curid=14220" title="History of mathematics">
History of mathematics

The area of study known as the history of mathematics is primarily an investigation into the origin of discoveries in mathematics and, to a lesser extent, an investigation into the mathematical methods and notation of the past. Before the modern age and the worldwide spread of knowledge, written examples of new mathematical developments have come to light only in a few locales. From 3000 BC the Mesopotamian states of Sumer, Akkad and Assyria, together with Ancient Egypt and Ebla began using arithmetic, algebra and geometry for purposes of taxation, commerce, trade and also in the patterns in nature, the field of astronomy and to record time/formulate calendars.

The most ancient mathematical texts available are from Mesopotamia and Egypt – "Plimpton 322" (Babylonian c. 1900 BC), the "Rhind Mathematical Papyrus" (Egyptian c. 2000–1800 BC) and the "Moscow Mathematical Papyrus" (Egyptian c. 1890 BC). All of these texts mention the so-called Pythagorean triples and so, by inference, the Pythagorean theorem, seems to be the most ancient and widespread mathematical development after basic arithmetic and geometry.

The study of mathematics as a "demonstrative discipline" begins in the 6th century BC with the Pythagoreans, who coined the term "mathematics" from the ancient Greek "μάθημα" ("mathema"), meaning "subject of instruction". Greek mathematics greatly refined the methods (especially through the introduction of deductive reasoning and mathematical rigor in proofs) and expanded the subject matter of mathematics. Although they made virtually no contributions to theoretical mathematics, the ancient Romans used applied mathematics in surveying, structural engineering, mechanical engineering, bookkeeping, creation of lunar and solar calendars, and even arts and crafts. Chinese mathematics made early contributions, including a place value system and the first use of negative numbers. The Hindu–Arabic numeral system and the rules for the use of its operations, in use throughout the world today evolved over the course of the first millennium AD in India and were transmitted to the Western world via Islamic mathematics through the work of Muḥammad ibn Mūsā al-Khwārizmī. Islamic mathematics, in turn, developed and expanded the mathematics known to these civilizations. Contemporaneous with but independent of these traditions were the mathematics developed by the Maya civilization of Mexico and Central America, where the concept of zero was given a standard symbol in Maya numerals.

Many Greek and Arabic texts on mathematics were translated into Latin from the 12th century onward, leading to further development of mathematics in Medieval Europe. From ancient times through the Middle Ages, periods of mathematical discovery were often followed by centuries of stagnation. Beginning in Renaissance Italy in the 15th century, new mathematical developments, interacting with new scientific discoveries, were made at an increasing pace that continues through the present day. This includes the groundbreaking work of both Isaac Newton and Gottfried Wilhelm Leibniz in the development of infinitesimal calculus during the course of the 17th century. At the end of the 19th century the International Congress of Mathematicians was founded and continues to spearhead advances in the field.

The origins of mathematical thought lie in the concepts of number, patterns in nature, magnitude, and form. Modern studies of animal cognition have shown that these concepts are not unique to humans. Such concepts would have been part of everyday life in hunter-gatherer societies. The idea of the "number" concept evolving gradually over time is supported by the existence of languages which preserve the distinction between "one", "two", and "many", but not of numbers larger than two.

Prehistoric artifacts discovered in Africa, dated 20,000 years old or more suggest early attempts to quantify time. The Ishango bone, found near the headwaters of the Nile river (northeastern Congo), may be more than 20,000 years old and consists of a series of marks carved in three columns running the length of the bone. Common interpretations are that the Ishango bone shows either a "tally" of the earliest known demonstration of sequences of prime numbers or a six-month lunar calendar. Peter Rudman argues that the development of the concept of prime numbers could only have come about after the concept of division, which he dates to after 10,000 BC, with prime numbers probably not being understood until about 500 BC. He also writes that "no attempt has been made to explain why a tally of something should exhibit multiples of two, prime numbers between 10 and 20, and some numbers that are almost multiples of 10." The Ishango bone, according to scholar Alexander Marshack, may have influenced the later development of mathematics in Egypt as, like some entries on the Ishango bone, Egyptian arithmetic also made use of multiplication by 2; this however, is disputed.

Predynastic Egyptians of the 5th millennium BC pictorially represented geometric designs. It has been claimed that megalithic monuments in England and Scotland, dating from the 3rd millennium BC, incorporate geometric ideas such as circles, ellipses, and Pythagorean triples in their design. All of the above are disputed however, and the currently oldest undisputed mathematical documents are from Babylonian and dynastic Egyptian sources.

Babylonian mathematics refers to any mathematics of the peoples of Mesopotamia (modern Iraq) from the days of the early Sumerians through the Hellenistic period almost to the dawn of Christianity. The majority of Babylonian mathematical work comes from two widely separated periods: The first few hundred years of the second millennium BC (Old Babylonian period), and the last few centuries of the first millennium BC (Seleucid period). It is named Babylonian mathematics due to the central role of Babylon as a place of study. Later under the Arab Empire, Mesopotamia, especially Baghdad, once again became an important center of study for Islamic mathematics.
In contrast to the sparsity of sources in Egyptian mathematics, our knowledge of Babylonian mathematics is derived from more than 400 clay tablets unearthed since the 1850s. Written in Cuneiform script, tablets were inscribed whilst the clay was moist, and baked hard in an oven or by the heat of the sun. Some of these appear to be graded homework.

The earliest evidence of written mathematics dates back to the ancient Sumerians, who built the earliest civilization in Mesopotamia. They developed a complex system of metrology from 3000 BC. From around 2500 BC onwards, the Sumerians wrote multiplication tables on clay tablets and dealt with geometrical exercises and division problems. The earliest traces of the Babylonian numerals also date back to this period.
Babylonian mathematics were written using a sexagesimal (base-60) numeral system. From this derives the modern-day usage of 60 seconds in a minute, 60 minutes in an hour, and 360 (60 × 6) degrees in a circle, as well as the use of seconds and minutes of arc to denote fractions of a degree. It is likely the sexagesimal system was chosen because 60 can be evenly divided by 2, 3, 4, 5, 6, 10, 12, 15, 20 and 30. Also, unlike the Egyptians, Greeks, and Romans, the Babylonians had a true place-value system, where digits written in the left column represented larger values, much as in the decimal system. The power of the Babylonian notational system lay in that it could be used to represent fractions as easily as whole numbers; thus multiplying two numbers that contained fractions was no different than multiplying integers, similar to our modern notation. The notational system of the Babylonians was the best of any civilization until the Renaissance, and its power allowed it to achieve remarkable computational accuracy; for example, the Babylonian tablet YBC 7289 gives an approximation of accurate to five decimal places. The Babylonians lacked, however, an equivalent of the decimal point, and so the place value of a symbol often had to be inferred from the context. By the Seleucid period, the Babylonians had developed a zero symbol as a placeholder for empty positions; however it was only used for intermediate positions. This zero sign does not appear in terminal positions, thus the Babylonians came close but did not develop a true place value system.

Other topics covered by Babylonian mathematics include fractions, algebra, quadratic and cubic equations, and the calculation of regular reciprocal pairs. The tablets also include multiplication tables and methods for solving linear, quadratic equations and cubic equations, a remarkable achievement for the time. Tablets from the Old Babylonian period also contain the earliest known statement of the Pythagorean theorem. However, as with Egyptian mathematics, Babylonian mathematics shows no awareness of the difference between exact and approximate solutions, or the solvability of a problem, and most importantly, no explicit statement of the need for proofs or logical principles.

Egyptian mathematics refers to mathematics written in the Egyptian language. From the Hellenistic period, Greek replaced Egyptian as the written language of Egyptian scholars. Mathematical study in Egypt later continued under the Arab Empire as part of Islamic mathematics, when Arabic became the written language of Egyptian scholars.

The most extensive Egyptian mathematical text is the Rhind papyrus (sometimes also called the Ahmes Papyrus after its author), dated to c. 1650 BC but likely a copy of an older document from the Middle Kingdom of about 2000–1800 BC. It is an instruction manual for students in arithmetic and geometry. In addition to giving area formulas and methods for multiplication, division and working with unit fractions, it also contains evidence of other mathematical knowledge, including composite and prime numbers; arithmetic, geometric and harmonic means; and simplistic understandings of both the Sieve of Eratosthenes and perfect number theory (namely, that of the number 6). It also shows how to solve first order linear equations as well as arithmetic and geometric series.

Another significant Egyptian mathematical text is the Moscow papyrus, also from the Middle Kingdom period, dated to c. 1890 BC. It consists of what are today called "word problems" or "story problems", which were apparently intended as entertainment. One problem is considered to be of particular importance because it gives a method for finding the volume of a frustum (truncated pyramid).

Finally, the Berlin Papyrus 6619 (c. 1800 BC) shows that ancient Egyptians could solve a second-order algebraic equation.

Greek mathematics refers to the mathematics written in the Greek language from the time of Thales of Miletus (~600 BC) to the closure of the Academy of Athens in 529 AD. Greek mathematicians lived in cities spread over the entire Eastern Mediterranean, from Italy to North Africa, but were united by culture and language. Greek mathematics of the period following Alexander the Great is sometimes called Hellenistic mathematics.

Greek mathematics was much more sophisticated than the mathematics that had been developed by earlier cultures. All surviving records of pre-Greek mathematics show the use of inductive reasoning, that is, repeated observations used to establish rules of thumb. Greek mathematicians, by contrast, used deductive reasoning. The Greeks used logic to derive conclusions from definitions and axioms, and used mathematical rigor to prove them.

Greek mathematics is thought to have begun with Thales of Miletus (c. 624–c.546 BC) and Pythagoras of Samos (c. 582–c. 507 BC). Although the extent of the influence is disputed, they were probably inspired by Egyptian and Babylonian mathematics. According to legend, Pythagoras traveled to Egypt to learn mathematics, geometry, and astronomy from Egyptian priests.

Thales used geometry to solve problems such as calculating the height of pyramids and the distance of ships from the shore. He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries to Thales' Theorem. As a result, he has been hailed as the first true mathematician and the first known individual to whom a mathematical discovery has been attributed. Pythagoras established the Pythagorean School, whose doctrine it was that mathematics ruled the universe and whose motto was "All is number". It was the Pythagoreans who coined the term "mathematics", and with whom the study of mathematics for its own sake begins. The Pythagoreans are credited with the first proof of the Pythagorean theorem, though the statement of the theorem has a long history, and with the proof of the existence of irrational numbers. Although he was preceded by the Babylonians and the Chinese, the Neopythagorean mathematician Nicomachus (60–120 AD) provided one of the earliest Greco-Roman multiplication tables, whereas the oldest extant Greek multiplication table is found on a wax tablet dated to the 1st century AD (now found in the British Museum). The association of the Neopythagoreans with the Western invention of the multiplication table is evident in its later Medieval name: the "mensa Pythagorica".

Plato (428/427 BC – 348/347 BC) is important in the history of mathematics for inspiring and guiding others. His Platonic Academy, in Athens, became the mathematical center of the world in the 4th century BC, and it was from this school that the leading mathematicians of the day, such as Eudoxus of Cnidus, came. Plato also discussed the foundations of mathematics, clarified some of the definitions (e.g. that of a line as "breadthless length"), and reorganized the assumptions. The analytic method is ascribed to Plato, while a formula for obtaining Pythagorean triples bears his name.

Eudoxus (408–c. 355 BC) developed the method of exhaustion, a precursor of modern integration and a theory of ratios that avoided the problem of incommensurable magnitudes. The former allowed the calculations of areas and volumes of curvilinear figures, while the latter enabled subsequent geometers to make significant advances in geometry. Though he made no specific technical mathematical discoveries, Aristotle (384–c. 322 BC) contributed significantly to the development of mathematics by laying the foundations of logic.
In the 3rd century BC, the premier center of mathematical education and research was the Musaeum of Alexandria. It was there that Euclid (c. 300 BC) taught, and wrote the "Elements", widely considered the most successful and influential textbook of all time. The "Elements" introduced mathematical rigor through the axiomatic method and is the earliest example of the format still used in mathematics today, that of definition, axiom, theorem, and proof. Although most of the contents of the "Elements" were already known, Euclid arranged them into a single, coherent logical framework. The "Elements" was known to all educated people in the West up through the middle of the 20th century and its contents are still taught in geometry classes today. In addition to the familiar theorems of Euclidean geometry, the "Elements" was meant as an introductory textbook to all mathematical subjects of the time, such as number theory, algebra and solid geometry, including proofs that the square root of two is irrational and that there are infinitely many prime numbers. Euclid also wrote extensively on other subjects, such as conic sections, optics, spherical geometry, and mechanics, but only half of his writings survive.
Archimedes (c. 287–212 BC) of Syracuse, widely considered the greatest mathematician of antiquity, used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, in a manner not too dissimilar from modern calculus. He also showed one could use the method of exhaustion to calculate the value of π with as much precision as desired, and obtained the most accurate value of π then known, . He also studied the spiral bearing his name, obtained formulas for the volumes of surfaces of revolution (paraboloid, ellipsoid, hyperboloid), and an ingenious method of exponentiation for expressing very large numbers. While he is also known for his contributions to physics and several advanced mechanical devices, Archimedes himself placed far greater value on the products of his thought and general mathematical principles. He regarded as his greatest achievement his finding of the surface area and volume of a sphere, which he obtained by proving these are 2/3 the surface area and volume of a cylinder circumscribing the sphere.
Apollonius of Perga (c. 262–190 BC) made significant advances to the study of conic sections, showing that one can obtain all three varieties of conic section by varying the angle of the plane that cuts a double-napped cone. He also coined the terminology in use today for conic sections, namely parabola ("place beside" or "comparison"), "ellipse" ("deficiency"), and "hyperbola" ("a throw beyond"). His work "Conics" is one of the best known and preserved mathematical works from antiquity, and in it he derives many theorems concerning conic sections that would prove invaluable to later mathematicians and astronomers studying planetary motion, such as Isaac Newton. While neither Apollonius nor any other Greek mathematicians made the leap to coordinate geometry, Apollonius' treatment of curves is in some ways similar to the modern treatment, and some of his work seems to anticipate the development of analytical geometry by Descartes some 1800 years later.

Around the same time, Eratosthenes of Cyrene (c. 276–194 BC) devised the Sieve of Eratosthenes for finding prime numbers. The 3rd century BC is generally regarded as the "Golden Age" of Greek mathematics, with advances in pure mathematics henceforth in relative decline. Nevertheless, in the centuries that followed significant advances were made in applied mathematics, most notably trigonometry, largely to address the needs of astronomers. Hipparchus of Nicaea (c. 190–120 BC) is considered the founder of trigonometry for compiling the first known trigonometric table, and to him is also due the systematic use of the 360 degree circle. Heron of Alexandria (c. 10–70 AD) is credited with Heron's formula for finding the area of a scalene triangle and with being the first to recognize the possibility of negative numbers possessing square roots. Menelaus of Alexandria (c. 100 AD) pioneered spherical trigonometry through Menelaus' theorem. The most complete and influential trigonometric work of antiquity is the "Almagest" of Ptolemy (c. AD 90–168), a landmark astronomical treatise whose trigonometric tables would be used by astronomers for the next thousand years. Ptolemy is also credited with Ptolemy's theorem for deriving trigonometric quantities, and the most accurate value of π outside of China until the medieval period, 3.1416.
Following a period of stagnation after Ptolemy, the period between 250 and 350 AD is sometimes referred to as the "Silver Age" of Greek mathematics. During this period, Diophantus made significant advances in algebra, particularly indeterminate analysis, which is also known as "Diophantine analysis". The study of Diophantine equations and Diophantine approximations is a significant area of research to this day. His main work was the "Arithmetica", a collection of 150 algebraic problems dealing with exact solutions to determinate and indeterminate equations. The "Arithmetica" had a significant influence on later mathematicians, such as Pierre de Fermat, who arrived at his famous Last Theorem after trying to generalize a problem he had read in the "Arithmetica" (that of dividing a square into two squares). Diophantus also made significant advances in notation, the "Arithmetica" being the first instance of algebraic symbolism and syncopation.
Among the last great Greek mathematicians is Pappus of Alexandria (4th century AD). He is known for his hexagon theorem and centroid theorem, as well as the Pappus configuration and Pappus graph. His "Collection" is a major source of knowledge on Greek mathematics as most of it has survived. Pappus is considered the last major innovator in Greek mathematics, with subsequent work consisting mostly of commentaries on earlier work.

The first woman mathematician recorded by history was Hypatia of Alexandria (AD 350–415). She succeeded her father (Theon of Alexandria) as Librarian at the Great Library and wrote many works on applied mathematics. Because of a political dispute, the Christian community in Alexandria had her stripped publicly and executed. Her death is sometimes taken as the end of the era of the Alexandrian Greek mathematics, although work did continue in Athens for another century with figures such as Proclus, Simplicius and Eutocius. Although Proclus and Simplicius were more philosophers than mathematicians, their commentaries on earlier works are valuable sources on Greek mathematics. The closure of the neo-Platonic Academy of Athens by the emperor Justinian in 529 AD is traditionally held as marking the end of the era of Greek mathematics, although the Greek tradition continued unbroken in the Byzantine empire with mathematicians such as Anthemius of Tralles and Isidore of Miletus, the architects of the Hagia Sophia. Nevertheless, Byzantine mathematics consisted mostly of commentaries, with little in the way of innovation, and the centers of mathematical innovation were to be found elsewhere by this time.

Although ethnic Greek mathematicians continued under the rule of the late Roman Republic and subsequent Roman Empire, there were no noteworthy native Latin mathematicians in comparison. Ancient Romans such as Cicero (106–43 BC), an influential Roman statesman who studied mathematics in Greece, believed that Roman surveyors and calculators were far more interested in applied mathematics than the theoretical mathematics and geometry that were prized by the Greeks. It is unclear if the Romans first derived their numerical system directly from the Greek precedent or from Etruscan numerals used by the Etruscan civilization centered in what is now Tuscany, central Italy.

Using calculation, Romans were adept at both instigating and detecting financial fraud, as well as managing taxes for the treasury. Siculus Flaccus, one of the Roman "gromatici" (i.e. land surveyor), wrote the "Categories of Fields", which aided Roman surveyors in measuring the surface areas of allotted lands and territories. Aside from managing trade and taxes, the Romans also regularly applied mathematics to solve problems in engineering, including the erection of architecture such as bridges, road-building, and preparation for military campaigns. Arts and crafts such as Roman mosaics, inspired by previous Greek designs, created illusionist geometric patterns and rich, detailed scenes that required precise measurements for each tessera tile, the opus tessellatum pieces on average measuring eight millimeters square and the finer opus vermiculatum pieces having an average surface of four millimeters square.

The creation of the Roman calendar also necessitated basic mathematics. The first calendar allegedly dates back to 8th century BC during the Roman Kingdom and included 356 days plus a leap year every other year. In contrast, the lunar calendar of the Republican era contained 355 days, roughly ten-and-one-fourth days shorter than the solar year, a discrepancy that was solved by adding an extra month into the calendar after the 23rd of February. This calendar was supplanted by the Julian calendar, a solar calendar organized by Julius Caesar (100–44 BC) and devised by Sosigenes of Alexandria to include a leap day every four years in a 365-day cycle. This calendar, which contained an error of 11 minutes and 14 seconds, was later corrected by the Gregorian calendar organized by Pope Gregory XIII (), virtually the same solar calendar used in modern times as the international standard calendar.

At roughly the same time, the Han Chinese and the Romans both invented the wheeled odometer device for measuring distances traveled, the Roman model first described by the Roman civil engineer and architect Vitruvius (c. 80 BC – c. 15 BC). The device was used at least until the reign of emperor Commodus (), but its design seems to have been lost until experiments were made during the 15th century in Western Europe. Perhaps relying on similar gear-work and technology found in the Antikythera mechanism, the odometer of Vitruvius featured chariot wheels measuring 4 feet (1.2 m) in diameter turning four-hundred times in one Roman mile (roughly 4590 ft/1400 m). With each revolution, a pin-and-axle device engaged a 400-tooth cogwheel that turned a second gear responsible for dropping pebbles into a box, each pebble representing one mile traversed.

An analysis of early Chinese mathematics has demonstrated its unique development compared to other parts of the world, leading scholars to assume an entirely independent development. The oldest extant mathematical text from China is the "Zhoubi Suanjing", variously dated to between 1200 BC and 100 BC, though a date of about 300 BC during the Warring States Period appears reasonable. However, the Tsinghua Bamboo Slips, containing the earliest known decimal multiplication table (although ancient Babylonians had ones with a base of 60), is dated around 305 BC and is perhaps the oldest surviving mathematical text of China.
Of particular note is the use in Chinese mathematics of a decimal positional notation system, the so-called "rod numerals" in which distinct ciphers were used for numbers between 1 and 10, and additional ciphers for powers of ten. Thus, the number 123 would be written using the symbol for "1", followed by the symbol for "100", then the symbol for "2" followed by the symbol for "10", followed by the symbol for "3". This was the most advanced number system in the world at the time, apparently in use several centuries before the common era and well before the development of the Indian numeral system. Rod numerals allowed the representation of numbers as large as desired and allowed calculations to be carried out on the "suan pan", or Chinese abacus. The date of the invention of the "suan pan" is not certain, but the earliest written mention dates from AD 190, in Xu Yue's "Supplementary Notes on the Art of Figures".

The oldest existent work on geometry in China comes from the philosophical Mohist canon c. 330 BC, compiled by the followers of Mozi (470–390 BC). The "Mo Jing" described various aspects of many fields associated with physical science, and provided a small number of geometrical theorems as well. It also defined the concepts of circumference, diameter, radius, and volume.
In 212 BC, the Emperor Qin Shi Huang commanded all books in the Qin Empire other than officially sanctioned ones be burned. This decree was not universally obeyed, but as a consequence of this order little is known about ancient Chinese mathematics before this date. After the book burning of 212 BC, the Han dynasty (202 BC–220 AD) produced works of mathematics which presumably expanded on works that are now lost. The most important of these is "The Nine Chapters on the Mathematical Art", the full title of which appeared by AD 179, but existed in part under other titles beforehand. It consists of 246 word problems involving agriculture, business, employment of geometry to figure height spans and dimension ratios for Chinese pagoda towers, engineering, surveying, and includes material on right triangles. It created mathematical proof for the Pythagorean theorem, and a mathematical formula for Gaussian elimination. The treatise also provides values of π, which Chinese mathematicians originally approximated as 3 until Liu Xin (d. 23 AD) provided a figure of 3.1457 and subsequently Zhang Heng (78–139) approximated pi as 3.1724, as well as 3.162 by taking the square root of 10. Liu Hui commented on the "Nine Chapters" in the 3rd century AD and gave a value of π accurate to 5 decimal places (i.e. 3.14159). Though more of a matter of computational stamina than theoretical insight, in the 5th century AD Zu Chongzhi computed the value of π to seven decimal places (i.e. 3.141592), which remained the most accurate value of π for almost the next 1000 years. He also established a method which would later be called Cavalieri's principle to find the volume of a sphere.

The high-water mark of Chinese mathematics occurred in the 13th century during the latter half of the Song dynasty (960–1279), with the development of Chinese algebra. The most important text from that period is the "Precious Mirror of the Four Elements" by Zhu Shijie (1249–1314), dealing with the solution of simultaneous higher order algebraic equations using a method similar to Horner's method. The "Precious Mirror" also contains a diagram of Pascal's triangle with coefficients of binomial expansions through the eighth power, though both appear in Chinese works as early as 1100. The Chinese also made use of the complex combinatorial diagram known as the magic square and magic circles, described in ancient times and perfected by Yang Hui (AD 1238–1298).

Even after European mathematics began to flourish during the Renaissance, European and Chinese mathematics were separate traditions, with significant Chinese mathematical output in decline from the 13th century onwards. Jesuit missionaries such as Matteo Ricci carried mathematical ideas back and forth between the two cultures from the 16th to 18th centuries, though at this point far more mathematical ideas were entering China than leaving.

Japanese mathematics, Korean mathematics, and Vietnamese mathematics are traditionally viewed as stemming from Chinese mathematics and belonging to the Confucian-based East Asian cultural sphere. Korean and Japanese mathematics were heavily influenced by the algebraic works produced during China's Song dynasty, whereas Vietnamese mathematics was heavily indebted to popular works of China's Ming dynasty (1368–1644). For instance, although Vietnamese mathematical treatises were written in either Chinese or the native Vietnamese Chữ Nôm script, all of them followed the Chinese format of presenting a collection of problems with algorithms for solving them, followed by numerical answers. Mathematics in Vietnam and Korea were mostly associated with the professional court bureaucracy of mathematicians and astronomers, whereas in Japan it was more prevalent in the realm of private schools.

The earliest civilization on the Indian subcontinent is the Indus Valley Civilization (mature phase: 2600 to 1900 BC) that flourished in the Indus river basin. Their cities were laid out with geometric regularity, but no known mathematical documents survive from this civilization.

The oldest extant mathematical records from India are the Sulba Sutras (dated variously between the 8th century BC and the 2nd century AD), appendices to religious texts which give simple rules for constructing altars of various shapes, such as squares, rectangles, parallelograms, and others. As with Egypt, the preoccupation with temple functions points to an origin of mathematics in religious ritual. The Sulba Sutras give methods for constructing a circle with approximately the same area as a given square, which imply several different approximations of the value of π. In addition, they compute the square root of 2 to several decimal places, list Pythagorean triples, and give a statement of the Pythagorean theorem. All of these results are present in Babylonian mathematics, indicating Mesopotamian influence. It is not known to what extent the Sulba Sutras influenced later Indian mathematicians. As in China, there is a lack of continuity in Indian mathematics; significant advances are separated by long periods of inactivity.

Pāṇini (c. 5th century BC) formulated the rules for Sanskrit grammar. His notation was similar to modern mathematical notation, and used metarules, transformations, and recursion. Pingala (roughly 3rd–1st centuries BC) in his treatise of prosody uses a device corresponding to a binary numeral system. His discussion of the combinatorics of meters corresponds to an elementary version of the binomial theorem. Pingala's work also contains the basic ideas of Fibonacci numbers (called "mātrāmeru").

The next significant mathematical documents from India after the "Sulba Sutras" are the "Siddhantas", astronomical treatises from the 4th and 5th centuries AD (Gupta period) showing strong Hellenistic influence. They are significant in that they contain the first instance of trigonometric relations based on the half-chord, as is the case in modern trigonometry, rather than the full chord, as was the case in Ptolemaic trigonometry. Through a series of translation errors, the words "sine" and "cosine" derive from the Sanskrit "jiya" and "kojiya".
Around 500 AD, Aryabhata wrote the "Aryabhatiya", a slim volume, written in verse, intended to supplement the rules of calculation used in astronomy and mathematical mensuration, though with no feeling for logic or deductive methodology. Though about half of the entries are wrong, it is in the "Aryabhatiya" that the decimal place-value system first appears. Several centuries later, the Muslim mathematician Abu Rayhan Biruni described the "Aryabhatiya" as a "mix of common pebbles and costly crystals".

In the 7th century, Brahmagupta identified the Brahmagupta theorem, Brahmagupta's identity and Brahmagupta's formula, and for the first time, in "Brahma-sphuta-siddhanta", he lucidly explained the use of zero as both a placeholder and decimal digit, and explained the Hindu–Arabic numeral system. It was from a translation of this Indian text on mathematics (c. 770) that Islamic mathematicians were introduced to this numeral system, which they adapted as Arabic numerals. Islamic scholars carried knowledge of this number system to Europe by the 12th century, and it has now displaced all older number systems throughout the world. Various symbol sets are used to represent numbers in the Hindu–Arabic numeral system, all of which evolved from the Brahmi numerals. Each of the roughly dozen major scripts of India has its own numeral glyphs. In the 10th century, Halayudha's commentary on Pingala's work contains a study of the Fibonacci sequence and Pascal's triangle, and describes the formation of a matrix.

In the 12th century, Bhāskara II lived in southern India and wrote extensively on all then known branches of mathematics. His work contains mathematical objects equivalent or approximately equivalent to infinitesimals, derivatives, the mean value theorem and the derivative of the sine function. To what extent he anticipated the invention of calculus is a controversial subject among historians of mathematics.

In the 14th century, Madhava of Sangamagrama, the founder of the so-called Kerala School of Mathematics, found the Madhava–Leibniz series and obtained from it a transformed series, whose first 21 terms he used to compute the value of π as 3.14159265359. Madhava also found the Madhava-Gregory series to determine the arctangent, the Madhava-Newton power series to determine sine and cosine and the Taylor approximation for sine and cosine functions. In the 16th century, Jyesthadeva consolidated many of the Kerala School's developments and theorems in the "Yukti-bhāṣā".

The Islamic Empire established across Persia, the Middle East, Central Asia, North Africa, Iberia, and in parts of India in the 8th century made significant contributions towards mathematics. Although most Islamic texts on mathematics were written in Arabic, most of them were not written by Arabs, since much like the status of Greek in the Hellenistic world, Arabic was used as the written language of non-Arab scholars throughout the Islamic world at the time. Persians contributed to the world of Mathematics alongside Arabs.

In the 9th century, the Persian mathematician Muḥammad ibn Mūsā al-Khwārizmī wrote several important books on the Hindu–Arabic numerals and on methods for solving equations. His book "On the Calculation with Hindu Numerals", written about 825, along with the work of Al-Kindi, were instrumental in spreading Indian mathematics and Indian numerals to the West. The word "algorithm" is derived from the Latinization of his name, Algoritmi, and the word "algebra" from the title of one of his works, "Al-Kitāb al-mukhtaṣar fī hīsāb al-ğabr wa’l-muqābala" ("The Compendious Book on Calculation by Completion and Balancing"). He gave an exhaustive explanation for the algebraic solution of quadratic equations with positive roots, and he was the first to teach algebra in an elementary form and for its own sake. He also discussed the fundamental method of "reduction" and "balancing", referring to the transposition of subtracted terms to the other side of an equation, that is, the cancellation of like terms on opposite sides of the equation. This is the operation which al-Khwārizmī originally described as "al-jabr". His algebra was also no longer concerned "with a series of problems to be resolved, but an exposition which starts with primitive terms in which the combinations must give all possible prototypes for equations, which henceforward explicitly constitute the true object of study." He also studied an equation for its own sake and "in a generic manner, insofar as it does not simply emerge in the course of solving a problem, but is specifically called on to define an infinite class of problems."

In Egypt, Abu Kamil extended algebra to the set of irrational numbers, accepting square roots and fourth roots as solutions and coefficients to quadratic equations. He also developed techniques used to solve three non-linear simultaneous equations with three unknown variables. One unique feature of his works was trying to find all the possible solutions to some of his problems, including one where he found 2676 solutions. His works formed an important foundation for the development of algebra and influenced later mathematicians, such as al-Karaji and Fibonacci.

Further developments in algebra were made by Al-Karaji in his treatise "al-Fakhri", where he extends the methodology to incorporate integer powers and integer roots of unknown quantities. Something close to a proof by mathematical induction appears in a book written by Al-Karaji around 1000 AD, who used it to prove the binomial theorem, Pascal's triangle, and the sum of integral cubes. The historian of mathematics, F. Woepcke, praised Al-Karaji for being "the first who introduced the theory of algebraic calculus." Also in the 10th century, Abul Wafa translated the works of Diophantus into Arabic. Ibn al-Haytham was the first mathematician to derive the formula for the sum of the fourth powers, using a method that is readily generalizable for determining the general formula for the sum of any integral powers. He performed an integration in order to find the volume of a paraboloid, and was able to generalize his result for the integrals of polynomials up to the fourth degree. He thus came close to finding a general formula for the integrals of polynomials, but he was not concerned with any polynomials higher than the fourth degree.

In the late 11th century, Omar Khayyam wrote "Discussions of the Difficulties in Euclid", a book about what he perceived as flaws in Euclid's "Elements", especially the parallel postulate. He was also the first to find the general geometric solution to cubic equations. He was also very influential in calendar reform.

In the 13th century, Nasir al-Din Tusi (Nasireddin) made advances in spherical trigonometry. He also wrote influential work on Euclid's parallel postulate. In the 15th century, Ghiyath al-Kashi computed the value of π to the 16th decimal place. Kashi also had an algorithm for calculating "n"th roots, which was a special case of the methods given many centuries later by Ruffini and Horner.

Other achievements of Muslim mathematicians during this period include the addition of the decimal point notation to the Arabic numerals, the discovery of all the modern trigonometric functions besides the sine, al-Kindi's introduction of cryptanalysis and frequency analysis, the development of analytic geometry by Ibn al-Haytham, the beginning of algebraic geometry by Omar Khayyam and the development of an algebraic notation by al-Qalasādī.

During the time of the Ottoman Empire and Safavid Empire from the 15th century, the development of Islamic mathematics became stagnant.

In the Pre-Columbian Americas, the Maya civilization that flourished in Mexico and Central America during the 1st millennium AD developed a unique tradition of mathematics that, due to its geographic isolation, was entirely independent of existing European, Egyptian, and Asian mathematics. Maya numerals utilized a base of 20, the vigesimal system, instead of a base of ten that forms the basis of the decimal system used by most modern cultures. The Mayas used mathematics to create the Maya calendar as well as to predict astronomical phenomena in their native Maya astronomy. While the concept of zero had to be inferred in the mathematics of many contemporary cultures, the Mayas developed a standard symbol for it.

Medieval European interest in mathematics was driven by concerns quite different from those of modern mathematicians. One driving element was the belief that mathematics provided the key to understanding the created order of nature, frequently justified by Plato's "Timaeus" and the biblical passage (in the "Book of Wisdom") that God had "ordered all things in measure, and number, and weight".

Boethius provided a place for mathematics in the curriculum in the 6th century when he coined the term "quadrivium" to describe the study of arithmetic, geometry, astronomy, and music. He wrote "De institutione arithmetica", a free translation from the Greek of Nicomachus's "Introduction to Arithmetic"; "De institutione musica", also derived from Greek sources; and a series of excerpts from Euclid's "Elements". His works were theoretical, rather than practical, and were the basis of mathematical study until the recovery of Greek and Arabic mathematical works.

In the 12th century, European scholars traveled to Spain and Sicily seeking scientific Arabic texts, including al-Khwārizmī's "The Compendious Book on Calculation by Completion and Balancing", translated into Latin by Robert of Chester, and the complete text of Euclid's "Elements", translated in various versions by Adelard of Bath, Herman of Carinthia, and Gerard of Cremona. These and other new sources sparked a renewal of mathematics.

Leonardo of Pisa, now known as Fibonacci, serendipitously learned about the Hindu–Arabic numerals on a trip to what is now Béjaïa, Algeria with his merchant father. (Europe was still using Roman numerals.) There, he observed a system of arithmetic (specifically algorism) which due to the positional notation of Hindu–Arabic numerals was much more efficient and greatly facilitated commerce. Leonardo wrote "Liber Abaci" in 1202 (updated in 1254) introducing the technique to Europe and beginning a long period of popularizing it. The book also brought to Europe what is now known as the Fibonacci sequence (known to Indian mathematicians for hundreds of years before that) which was used as an unremarkable example within the text.

The 14th century saw the development of new mathematical concepts to investigate a wide range of problems. One important contribution was development of mathematics of local motion.

Thomas Bradwardine proposed that speed (V) increases in arithmetic proportion as the ratio of force (F) to resistance (R) increases in geometric proportion. Bradwardine expressed this by a series of specific examples, but although the logarithm had not yet been conceived, we can express his conclusion anachronistically by writing:
V = log (F/R). Bradwardine's analysis is an example of transferring a mathematical technique used by al-Kindi and Arnald of Villanova to quantify the nature of compound medicines to a different physical problem.
One of the 14th-century Oxford Calculators, William Heytesbury, lacking differential calculus and the concept of limits, proposed to measure instantaneous speed "by the path that would be described by [a body] if... it were moved uniformly at the same degree of speed with which it is moved in that given instant".

Heytesbury and others mathematically determined the distance covered by a body undergoing uniformly accelerated motion (today solved by integration), stating that "a moving body uniformly acquiring or losing that increment [of speed] will traverse in some given time a [distance] completely equal to that which it would traverse if it were moving continuously through the same time with the mean degree [of speed]".

Nicole Oresme at the University of Paris and the Italian Giovanni di Casali independently provided graphical demonstrations of this relationship, asserting that the area under the line depicting the constant acceleration, represented the total distance traveled. In a later mathematical commentary on Euclid's "Elements", Oresme made a more detailed general analysis in which he demonstrated that a body will acquire in each successive increment of time an increment of any quality that increases as the odd numbers. Since Euclid had demonstrated the sum of the odd numbers are the square numbers, the total quality acquired by the body increases as the square of the time.

During the Renaissance, the development of mathematics and of accounting were intertwined. While there is no direct relationship between algebra and accounting, the teaching of the subjects and the books published often intended for the children of merchants who were sent to reckoning schools (in Flanders and Germany) or abacus schools (known as "abbaco" in Italy), where they learned the skills useful for trade and commerce. There is probably no need for algebra in performing bookkeeping operations, but for complex bartering operations or the calculation of compound interest, a basic knowledge of arithmetic was mandatory and knowledge of algebra was very useful.

Piero della Francesca (c. 1415–1492) wrote books on solid geometry and linear perspective, including "De Prospectiva Pingendi (On Perspective for Painting)", "Trattato d’Abaco (Abacus Treatise)", and "De corporibus regularibus (Regular Solids)".
Luca Pacioli's "Summa de Arithmetica, Geometria, Proportioni et Proportionalità" (Italian: "Review of Arithmetic, Geometry, Ratio and Proportion") was first printed and published in Venice in 1494. It included a 27-page treatise on bookkeeping, ""Particularis de Computis et Scripturis"" (Italian: "Details of Calculation and Recording"). It was written primarily for, and sold mainly to, merchants who used the book as a reference text, as a source of pleasure from the mathematical puzzles it contained, and to aid the education of their sons. In "Summa Arithmetica", Pacioli introduced symbols for plus and minus for the first time in a printed book, symbols that became standard notation in Italian Renaissance mathematics. "Summa Arithmetica" was also the first known book printed in Italy to contain algebra. Pacioli obtained many of his ideas from Piero Della Francesca whom he plagiarized.

In Italy, during the first half of the 16th century, Scipione del Ferro and Niccolò Fontana Tartaglia discovered solutions for cubic equations. Gerolamo Cardano published them in his 1545 book "Ars Magna", together with a solution for the quartic equations, discovered by his student Lodovico Ferrari. In 1572 Rafael Bombelli published his "L'Algebra" in which he showed how to deal with the imaginary quantities that could appear in Cardano's formula for solving cubic equations.

Simon Stevin's book "De Thiende" ('the art of tenths'), first published in Dutch in 1585, contained the first systematic treatment of decimal notation, which influenced all later work on the real number system.

Driven by the demands of navigation and the growing need for accurate maps of large areas, trigonometry grew to be a major branch of mathematics. Bartholomaeus Pitiscus was the first to use the word, publishing his "Trigonometria" in 1595. Regiomontanus's table of sines and cosines was published in 1533.

During the Renaissance the desire of artists to represent the natural world realistically, together with the rediscovered philosophy of the Greeks, led artists to study mathematics. They were also the engineers and architects of that time, and so had need of mathematics in any case. The art of painting in perspective, and the developments in geometry that involved, were studied intensely.

The 17th century saw an unprecedented increase of mathematical and scientific ideas across Europe. Galileo observed the moons of Jupiter in orbit about that planet, using a telescope based on a toy imported from Holland. Tycho Brahe had gathered an enormous quantity of mathematical data describing the positions of the planets in the sky. By his position as Brahe's assistant, Johannes Kepler was first exposed to and seriously interacted with the topic of planetary motion. Kepler's calculations were made simpler by the contemporaneous invention of logarithms by John Napier and Jost Bürgi. Kepler succeeded in formulating mathematical laws of planetary motion.
The analytic geometry developed by René Descartes (1596–1650) allowed those orbits to be plotted on a graph, in Cartesian coordinates.

Building on earlier work by many predecessors, Isaac Newton discovered the laws of physics explaining Kepler's Laws, and brought together the concepts now known as calculus. Independently, Gottfried Wilhelm Leibniz, who is arguably one of the most important mathematicians of the 17th century, developed calculus and much of the calculus notation still in use today. Science and mathematics had become an international endeavor, which would soon spread over the entire world.

In addition to the application of mathematics to the studies of the heavens, applied mathematics began to expand into new areas, with the correspondence of Pierre de Fermat and Blaise Pascal. Pascal and Fermat set the groundwork for the investigations of probability theory and the corresponding rules of combinatorics in their discussions over a game of gambling. Pascal, with his wager, attempted to use the newly developing probability theory to argue for a life devoted to religion, on the grounds that even if the probability of success was small, the rewards were infinite. In some sense, this foreshadowed the development of utility theory in the 18th–19th century.

The most influential mathematician of the 18th century was arguably Leonhard Euler. His contributions range from founding the study of graph theory with the Seven Bridges of Königsberg problem to standardizing many modern mathematical terms and notations. For example, he named the square root of minus 1 with the symbol "i", and he popularized the use of the Greek letter formula_1 to stand for the ratio of a circle's circumference to its diameter. He made numerous contributions to the study of topology, graph theory, calculus, combinatorics, and complex analysis, as evidenced by the multitude of theorems and notations named for him.

Other important European mathematicians of the 18th century included Joseph Louis Lagrange, who did pioneering work in number theory, algebra, differential calculus, and the calculus of variations, and Laplace who, in the age of Napoleon, did important work on the foundations of celestial mechanics and on statistics.

Throughout the 19th century mathematics became increasingly abstract. Carl Friedrich Gauss (1777–1855) epitomizes this trend. He did revolutionary work on functions of complex variables, in geometry, and on the convergence of series, leaving aside his many contributions to science. He also gave the first satisfactory proofs of the fundamental theorem of algebra and of the quadratic reciprocity law.
This century saw the development of the two forms of non-Euclidean geometry, where the parallel postulate of Euclidean geometry no longer holds.
The Russian mathematician Nikolai Ivanovich Lobachevsky and his rival, the Hungarian mathematician János Bolyai, independently defined and studied hyperbolic geometry, where uniqueness of parallels no longer holds. In this geometry the sum of angles in a triangle add up to less than 180°. Elliptic geometry was developed later in the 19th century by the German mathematician Bernhard Riemann; here no parallel can be found and the angles in a triangle add up to more than 180°. Riemann also developed Riemannian geometry, which unifies and vastly generalizes the three types of geometry, and he defined the concept of a manifold, which generalizes the ideas of curves and surfaces.

The 19th century saw the beginning of a great deal of abstract algebra. Hermann Grassmann in Germany gave a first version of vector spaces, William Rowan Hamilton in Ireland developed noncommutative algebra. The British mathematician George Boole devised an algebra that soon evolved into what is now called Boolean algebra, in which the only numbers were 0 and 1. Boolean algebra is the starting point of mathematical logic and has important applications in electrical engineering and computer science.
Augustin-Louis Cauchy, Bernhard Riemann, and Karl Weierstrass reformulated the calculus in a more rigorous fashion.

Also, for the first time, the limits of mathematics were explored. Niels Henrik Abel, a Norwegian, and Évariste Galois, a Frenchman, proved that there is no general algebraic method for solving polynomial equations of degree greater than four (Abel–Ruffini theorem). Other 19th-century mathematicians utilized this in their proofs that straightedge and compass alone are not sufficient to trisect an arbitrary angle, to construct the side of a cube twice the volume of a given cube, nor to construct a square equal in area to a given circle. Mathematicians had vainly attempted to solve all of these problems since the time of the ancient Greeks. On the other hand, the limitation of three dimensions in geometry was surpassed in the 19th century through considerations of parameter space and hypercomplex numbers.

Abel and Galois's investigations into the solutions of various polynomial equations laid the groundwork for further developments of group theory, and the associated fields of abstract algebra. In the 20th century physicists and other scientists have seen group theory as the ideal way to study symmetry.

In the later 19th century, Georg Cantor established the first foundations of set theory, which enabled the rigorous treatment of the notion of infinity and has become the common language of nearly all mathematics. Cantor's set theory, and the rise of mathematical logic in the hands of Peano, L.E.J. Brouwer, David Hilbert, Bertrand Russell, and A.N. Whitehead, initiated a long running debate on the foundations of mathematics.

The 19th century saw the founding of a number of national mathematical societies: the London Mathematical Society in 1865, the Société Mathématique de France in 1872, the Circolo Matematico di Palermo in 1884, the Edinburgh Mathematical Society in 1883, and the American Mathematical Society in 1888. The first international, special-interest society, the Quaternion Society, was formed in 1899, in the context of a vector controversy.

In 1897, Hensel introduced p-adic numbers.

The 20th century saw mathematics become a major profession. Every year, thousands of new Ph.D.s in mathematics were awarded, and jobs were available in both teaching and industry. An effort to catalogue the areas and applications of mathematics was undertaken in Klein's encyclopedia.

In a 1900 speech to the International Congress of Mathematicians, David Hilbert set out a list of 23 unsolved problems in mathematics. These problems, spanning many areas of mathematics, formed a central focus for much of 20th-century mathematics. Today, 10 have been solved, 7 are partially solved, and 2 are still open. The remaining 4 are too loosely formulated to be stated as solved or not.
Notable historical conjectures were finally proven. In 1976, Wolfgang Haken and Kenneth Appel proved the four color theorem, controversial at the time for the use of a computer to do so. Andrew Wiles, building on the work of others, proved Fermat's Last Theorem in 1995. Paul Cohen and Kurt Gödel proved that the continuum hypothesis is independent of (could neither be proved nor disproved from) the standard axioms of set theory. In 1998 Thomas Callister Hales proved the Kepler conjecture.

Mathematical collaborations of unprecedented size and scope took place. An example is the classification of finite simple groups (also called the "enormous theorem"), whose proof between 1955 and 2004 required 500-odd journal articles by about 100 authors, and filling tens of thousands of pages. A group of French mathematicians, including Jean Dieudonné and André Weil, publishing under the pseudonym "Nicolas Bourbaki", attempted to exposit all of known mathematics as a coherent rigorous whole. The resulting several dozen volumes has had a controversial influence on mathematical education.
Differential geometry came into its own when Albert Einstein used it in general relativity. Entirely new areas of mathematics such as mathematical logic, topology, and John von Neumann's game theory changed the kinds of questions that could be answered by mathematical methods. All kinds of structures were abstracted using axioms and given names like metric spaces, topological spaces etc. As mathematicians do, the concept of an abstract structure was itself abstracted and led to category theory. Grothendieck and Serre recast algebraic geometry using sheaf theory. Large advances were made in the qualitative study of dynamical systems that Poincaré had begun in the 1890s.
Measure theory was developed in the late 19th and early 20th centuries. Applications of measures include the Lebesgue integral, Kolmogorov's axiomatisation of probability theory, and ergodic theory. Knot theory greatly expanded. Quantum mechanics led to the development of functional analysis. Other new areas include Laurent Schwartz's distribution theory, fixed point theory, singularity theory and René Thom's catastrophe theory, model theory, and Mandelbrot's fractals. Lie theory with its Lie groups and Lie algebras became one of the major areas of study.

Non-standard analysis, introduced by Abraham Robinson, rehabilitated the infinitesimal approach to calculus, which had fallen into disrepute in favour of the theory of limits, by extending the field of real numbers to the Hyperreal numbers which include infinitesimal and infinite quantities. An even larger number system, the surreal numbers were discovered by John Horton Conway in connection with combinatorial games.

The development and continual improvement of computers, at first mechanical analog machines and then digital electronic machines, allowed industry to deal with larger and larger amounts of data to facilitate mass production and distribution and communication, and new areas of mathematics were developed to deal with this: Alan Turing's computability theory; complexity theory; Derrick Henry Lehmer's use of ENIAC to further number theory and the Lucas-Lehmer test; Rózsa Péter's recursive function theory; Claude Shannon's information theory; signal processing; data analysis; optimization and other areas of operations research. In the preceding centuries much mathematical focus was on calculus and continuous functions, but the rise of computing and communication networks led to an increasing importance of discrete concepts and the expansion of combinatorics including graph theory. The speed and data processing abilities of computers also enabled the handling of mathematical problems that were too time-consuming to deal with by pencil and paper calculations, leading to areas such as numerical analysis and symbolic computation. Some of the most important methods and algorithms of the 20th century are: the simplex algorithm, the fast Fourier transform, error-correcting codes, the Kalman filter from control theory and the RSA algorithm of public-key cryptography.

At the same time, deep insights were made about the limitations to mathematics. In 1929 and 1930, it was proved the truth or falsity of all statements formulated about the natural numbers plus one of addition and multiplication, was decidable, i.e. could be determined by some algorithm. In 1931, Kurt Gödel found that this was not the case for the natural numbers plus both addition and multiplication; this system, known as Peano arithmetic, was in fact incompletable. (Peano arithmetic is adequate for a good deal of number theory, including the notion of prime number.) A consequence of Gödel's two incompleteness theorems is that in any mathematical system that includes Peano arithmetic (including all of analysis and geometry), truth necessarily outruns proof, i.e. there are true statements that cannot be proved within the system. Hence mathematics cannot be reduced to mathematical logic, and David Hilbert's dream of making all of mathematics complete and consistent needed to be reformulated.
One of the more colorful figures in 20th-century mathematics was Srinivasa Aiyangar Ramanujan (1887–1920), an Indian autodidact who conjectured or proved over 3000 theorems, including properties of highly composite numbers, the partition function and its asymptotics, and mock theta functions. He also made major investigations in the areas of gamma functions, modular forms, divergent series, hypergeometric series and prime number theory.

Paul Erdős published more papers than any other mathematician in history, working with hundreds of collaborators. Mathematicians have a game equivalent to the Kevin Bacon Game, which leads to the Erdős number of a mathematician. This describes the "collaborative distance" between a person and Paul Erdős, as measured by joint authorship of mathematical papers.

Emmy Noether has been described by many as the most important woman in the history of mathematics. She studied the theories of rings, fields, and algebras.

As in most areas of study, the explosion of knowledge in the scientific age has led to specialization: by the end of the century there were hundreds of specialized areas in mathematics and the Mathematics Subject Classification was dozens of pages long. More and more mathematical journals were published and, by the end of the century, the development of the World Wide Web led to online publishing.

In 2000, the Clay Mathematics Institute announced the seven Millennium Prize Problems, and in 2003 the Poincaré conjecture was solved by Grigori Perelman (who declined to accept an award, as he was critical of the mathematics establishment).

Most mathematical journals now have online versions as well as print versions, and many online-only journals are launched. There is an increasing drive towards open access publishing, first popularized by the arXiv.

There are many observable trends in mathematics, the most notable being that the subject is growing ever larger, computers are ever more important and powerful, the application of mathematics to bioinformatics is rapidly expanding, and the volume of data being produced by science and industry, facilitated by computers, is explosively expanding.













</doc>
<doc id="14223" url="https://en.wikipedia.org/wiki?curid=14223" title="HSK">
HSK

HSK may refer to:



</doc>
<doc id="14225" url="https://en.wikipedia.org/wiki?curid=14225" title="Hydrogen atom">
Hydrogen atom

A hydrogen atom is an atom of the chemical element hydrogen. The electrically neutral atom contains a single positively charged proton and a single negatively charged electron bound to the nucleus by the Coulomb force. Atomic hydrogen constitutes about 75% of the baryonic mass of the universe.

In everyday life on Earth, isolated hydrogen atoms (called "atomic hydrogen") are extremely rare. Instead, a hydrogen atom tends to combine with other atoms in compounds, or with another hydrogen atom to form ordinary (diatomic) hydrogen gas, H. "Atomic hydrogen" and "hydrogen atom" in ordinary English use have overlapping, yet distinct, meanings. For example, a water molecule contains two hydrogen atoms, but does not contain atomic hydrogen (which would refer to isolated hydrogen atoms).

Atomic spectroscopy shows that there is a discrete infinite set of states in which a hydrogen (or any) atom can exist, contrary to the predictions of classical physics. Attempts to develop a theoretical understanding of the states of the hydrogen atom have been important to the history of quantum mechanics, since all other atoms can be roughly understood by knowing in detail about this simplest atomic structure.

The most abundant isotope, hydrogen-1, protium, or light hydrogen, contains no neutrons and is simply a proton and an electron. Protium is stable and makes up 99.985% of naturally occurring hydrogen atoms.

Deuterium contains one neutron and one proton. Deuterium is stable and makes up 0.0156% of naturally occurring hydrogen and is used in industrial processes like nuclear reactors and Nuclear Magnetic Resonance.

Tritium contains two neutrons and one proton and is not stable, decaying with a half-life of 12.32 years. Because of its short half-life, tritium does not exist in nature except in trace amounts.

Heavier isotopes of hydrogen are only created artificially in particle accelerators and have half-lives on the order of 10 seconds. They are unbound resonances located beyond the neutron drip line; this results in prompt emission of a neutron.

The formulas below are valid for all three isotopes of hydrogen, but slightly different values of the Rydberg constant (correction formula given below) must be used for each hydrogen isotope.

Lone neutral hydrogen atoms are rare under normal conditions. However, neutral hydrogen is common when it is covalently bound to another atom, and hydrogen atoms can also exist in cationic and anionic forms.

If a neutral hydrogen atom loses its electron, it becomes a cation. The resulting ion, which consists solely of a proton for the usual isotope, is written as "H" and sometimes called "hydron". Free protons are common in the interstellar medium, and solar wind. In the context of aqueous solutions of classical Brønsted–Lowry acids, such as hydrochloric acid, it is actually hydronium, HO, that is meant. Instead of a literal ionized single hydrogen atom being formed, the acid transfers the hydrogen to HO, forming HO. 

If instead a hydrogen atom gains a second electron, it becomes an anion. The hydrogen anion is written as "H" and called "hydride".

The hydrogen atom has special significance in quantum mechanics and quantum field theory as a simple two-body problem physical system which has yielded many simple analytical solutions in closed-form.

Experiments by Ernest Rutherford in 1909 showed the structure of the atom to be a dense, positive nucleus with a tenuous negative charge cloud around it. This immediately raised questions about how such a system could be stable. Classical electromagnetism had shown that any accelerating charge radiates energy, as shown by the Larmor formula. If the electron is assumed to orbit in a perfect circle and radiates energy continuously, the electron would rapidly spiral into the nucleus with a fall time of:

where formula_2 is the Bohr radius and formula_3 is the classical electron radius. If this were true, all atoms would instantly collapse, however atoms seem to be stable. Furthermore, the spiral inward would release a smear of electromagnetic frequencies as the orbit got smaller. Instead, atoms were observed to only emit discrete frequencies of radiation. The resolution would lie in the development of quantum mechanics.

In 1913, Niels Bohr obtained the energy levels and spectral frequencies of the hydrogen atom after making a number of simple assumptions in order to correct the failed classical model. The assumptions included:


Bohr supposed that the electron's angular momentum is quantized with possible values:

and formula_6 is Planck constant over formula_7. He also supposed that the centripetal force which keeps the electron in its orbit is provided by the Coulomb force, and that energy is conserved. Bohr derived the energy of each orbit of the hydrogen atom to be:

where formula_9 is the electron mass, formula_10 is the electron charge, formula_11 is the vacuum permittivity, and formula_12 is the quantum number (now known as the principal quantum number). Bohr's predictions matched experiments measuring the hydrogen spectral series to the first order, giving more confidence to a theory that used quantized values.

For formula_13, the value

is called the Rydberg unit of energy. It is related to the Rydberg constant formula_15 of atomic physics by formula_16

The exact value of the Rydberg constant assumes that the nucleus is infinitely massive with respect to the electron. For hydrogen-1, hydrogen-2 (deuterium), and hydrogen-3 (tritium) which have finite mass, the constant must be slightly modified to use the reduced mass of the system, rather than simply the mass of the electron. This includes the kinetic energy of the nucleus in the problem, because the total (electron plus nuclear) kinetic energy is equivalent to the kinetic energy of the reduced mass moving with a velocity equal to the electron velocity relative to the nucleus. However, since the nucleus is much heavier than the electron, the electron mass and reduced mass are nearly the same. The Rydberg constant "R" for a hydrogen atom (one electron), "R" is given by

where formula_18 is the mass of the atomic nucleus. For hydrogen-1, the quantity formula_19 is about 1/1836 (i.e. the electron-to-proton mass ratio). For deuterium and tritium, the ratios are about 1/3670 and 1/5497 respectively. These figures, when added to 1 in the denominator, represent very small corrections in the value of "R", and thus only small corrections to all energy levels in corresponding hydrogen isotopes.

There were still problems with Bohr's model: 


Most of these shortcomings were resolved by Arnold Sommerfeld's modification of the Bohr model. Sommerfeld introduced two additional degrees of freedom, allowing an electron to move on an elliptical orbit characterized by its eccentricity and declination with respect to a chosen axis. This introduced two additional quantum numbers, which correspond to the orbital angular momentum and its projection on the chosen axis. Thus the correct multiplicity of states (except for the factor 2 accounting for the yet unknown electron spin) was found. Further, by applying special relativity to the elliptic orbits, Sommerfeld succeeded in deriving the correct expression for the fine structure of hydrogen spectra (which happens to be exactly the same as in the most elaborate Dirac theory). However, some observed phenomena, such as the anomalous Zeeman effect, remained unexplained. These issues were resolved with the full development of quantum mechanics and the Dirac equation. It is often alleged that the Schrödinger equation is superior to the Bohr–Sommerfeld theory in describing hydrogen atom. This is not the case, as most of the results of both approaches coincide or are very close (a remarkable exception is the problem of hydrogen atom in crossed electric and magnetic fields, which cannot be self-consistently solved in the framework of the Bohr–Sommerfeld theory), and in both theories the main shortcomings result from the absence of the electron spin. It was the complete failure of the Bohr–Sommerfeld theory to explain many-electron systems (such as helium atom or hydrogen molecule) which demonstrated its inadequacy in describing quantum phenomena.

The Schrödinger equation allows one to calculate the stationary states and also the time evolution of quantum systems. Exact analytical answers are available for the nonrelativistic hydrogen atom. Before we go to present a formal account, here we give an elementary overview. 

Given that the hydrogen atom contains a nucleus and an electron, quantum mechanics allows one to predict the probability of finding the electron at any given radial distance formula_22. It is given by the square of a mathematical function known as the "wavefunction," which is a solution of the Schrödinger equation. The lowest energy equilibrium state of the hydrogen atom is known as the ground state. The ground state wave function is known as the formula_23 wavefunction. It is written as:

Here, formula_2 is the numerical value of the Bohr radius. The probability of finding the electron at a distance formula_22 in any radial direction is the squared value of the wavefunction:

The formula_28 wavefunction is spherically symmetric, and the surface area of a shell at distance formula_22 is formula_30, so the total probability formula_31 of the electron being in a shell at a distance formula_22 and thickness formula_33 is

It turns out that this is a maximum at formula_35. That is, the Bohr picture of an electron orbiting the nucleus at radius formula_2 is recovered as a statistically valid result. However, although the electron is most likely to be on a Bohr orbit, there is a finite probability that the electron may be at any other place formula_22, with the
probability indicated by the square of the wavefunction. Since the probability of finding the electron "somewhere" in the whole volume is unity, the integral of formula_31 is unity. Then we say that the wavefunction is properly normalized.

As discussed below, the ground state formula_28 is also indicated by the quantum numbers formula_40. The second lowest energy states, just above the ground state, are given by the quantum numbers formula_41, formula_42, and formula_43. These formula_44 states all have the same energy and are known as the formula_45 and formula_46 states. There is one formula_45 state:

and there are three formula_46 states:

An electron in the formula_45 or formula_46 state is most likely to be found in the second Bohr orbit with energy given by the Bohr formula. 
The Hamiltonian of the hydrogen atom is the radial kinetic energy operator and Coulomb attraction force between the positive proton and negative electron. Using the time-independent Schrödinger equation, ignoring all spin-coupling interactions and using the reduced mass formula_54, the equation is written as:

Expanding the Laplacian in spherical coordinates:

This is a separable, partial differential equation which can be solved in terms of special functions. The normalized position wavefunctions, given in spherical coordinates are:

where:

The quantum numbers can take the following values: 

Additionally, these wavefunctions are "normalized" (i.e., the integral of their modulus square equals 1) and orthogonal:

where formula_72 is the state represented by the wavefunction formula_73 in Dirac notation, and formula_74 is the Kronecker delta function.

The wavefunctions in momentum space are related to the wavefunctions in position space through a Fourier transform

which, for the bound states, results in 

where formula_77 denotes a Gegenbauer polynomial and formula_78 is in units of formula_79.

The solutions to the Schrödinger equation for hydrogen are analytical, giving a simple expression for the hydrogen energy levels and thus the frequencies of the hydrogen spectral lines and fully reproduced the Bohr model and went beyond it. It also yields two other quantum numbers and the shape of the electron's wave function ("orbital") for the various possible quantum-mechanical states, thus explaining the anisotropic character of atomic bonds.

The Schrödinger equation also applies to more complicated atoms and molecules. When there is more than one electron or nucleus the solution is not analytical and either computer calculations are necessary or simplifying assumptions must be made.

Since the Schrödinger equation is only valid for non-relativistic quantum mechanics, the solutions it yields for the hydrogen atom are not entirely correct. The Dirac equation of relativistic quantum theory improves these solutions (see below).

The solution of the Schrödinger equation (wave equation) for the hydrogen atom uses the fact that the Coulomb potential produced by the nucleus is isotropic (it is radially symmetric in space and only depends on the distance to the nucleus). Although the resulting energy eigenfunctions (the "orbitals") are not necessarily isotropic themselves, their dependence on the angular coordinates follows completely generally from this isotropy of the underlying potential: the eigenstates of the Hamiltonian (that is, the energy eigenstates) can be chosen as simultaneous eigenstates of the angular momentum operator. This corresponds to the fact that angular momentum is conserved in the orbital motion of the electron around the nucleus. Therefore, the energy eigenstates may be classified by two angular momentum quantum numbers, formula_64 and formula_65 (both are integers). The angular momentum quantum number formula_82 determines the magnitude of the angular momentum. The magnetic quantum number formula_83 determines the projection of the angular momentum on the (arbitrarily chosen) formula_84-axis.

In addition to mathematical expressions for total angular momentum and angular momentum projection of wavefunctions, an expression for the radial dependence of the wave functions must be found. It is only here that the details of the formula_85 Coulomb potential enter (leading to Laguerre polynomials in formula_22). This leads to a third quantum number, the principal quantum number formula_68. The principal quantum number in hydrogen is related to the atom's total energy.

Note that the maximum value of the angular momentum quantum number is limited by the principal quantum number: it can run only up to formula_88, i.e., formula_89.

Due to angular momentum conservation, states of the same formula_64 but different formula_65 have the same energy (this holds for all problems with rotational symmetry). In addition, for the hydrogen atom, states of the same formula_92 but different formula_64 are also degenerate (i.e., they have the same energy). However, this is a specific property of hydrogen and is no longer true for more complicated atoms which have an (effective) potential differing from the form formula_85 (due to the presence of the inner electrons shielding the nucleus potential).

Taking into account the spin of the electron adds a last quantum number, the projection of the electron's spin angular momentum along the formula_84-axis, which can take on two values. Therefore, any eigenstate of the electron in the hydrogen atom is described fully by four quantum numbers. According to the usual rules of quantum mechanics, the actual state of the electron may be any superposition of these states. This explains also why the choice of formula_84-axis for the directional quantization of the angular momentum vector is immaterial: an orbital of given formula_64 and formula_98 obtained for another preferred axis formula_99 can always be represented as a suitable superposition of the various states of different formula_65 (but same formula_64) that have been obtained for formula_84.

In 1928, Paul Dirac found an equation that was fully compatible with special relativity, and (as a consequence) made the wave function a 4-component "Dirac spinor" including "up" and "down" spin components, with both positive and "negative" energy (or matter and antimatter). The solution to this equation gave the following results, more accurate than the Schrödinger solution.

The energy levels of hydrogen, including fine structure (excluding Lamb shift and hyperfine structure), are given by the Sommerfeld fine structure expression:

where formula_21 is the fine-structure constant and formula_105 is the total angular momentum quantum number, which is equal to formula_106, depending on the orientation of the electron spin relative to the orbital angular momentum. This formula represents a small correction to the energy obtained by Bohr and Schrödinger as given above. The factor in square brackets in the last expression is nearly one; the extra term arises from relativistic effects (for details, see #Features going beyond the Schrödinger solution). It is worth noting that this expression was first obtained by A. Sommerfeld in 1916 based on the relativistic version of the old Bohr theory. Sommerfeld has however used different notation for the quantum numbers.

The coherent states have been proposed as

which satisfies formula_108 and takes the form

The image to the right shows the first few hydrogen atom orbitals (energy eigenfunctions). These are cross-sections of the probability density that are color-coded (black represents zero density and white represents the highest density). The angular momentum (orbital) quantum number "ℓ" is denoted in each column, using the usual spectroscopic letter code ("s" means "ℓ" = 0, "p" means "ℓ" = 1, "d" means "ℓ" = 2). The main (principal) quantum number "n" (= 1, 2, 3, ...) is marked to the right of each row. For all pictures the magnetic quantum number "m" has been set to 0, and the cross-sectional plane is the "xz"-plane ("z" is the vertical axis). The probability density in three-dimensional space is obtained by rotating the one shown here around the "z"-axis.

The "ground state", i.e. the state of lowest energy, in which the electron is usually found, is the first one, the 1"s" state (principal quantum level "n" = 1, "ℓ" = 0).

Black lines occur in each but the first orbital: these are the nodes of the wavefunction, i.e. where the probability density is zero. (More precisely, the nodes are spherical harmonics that appear as a result of solving Schrödinger equation in spherical coordinates.)

The quantum numbers determine the layout of these nodes. There are:


There are several important effects that are neglected by the Schrödinger equation and which are responsible for certain small but measurable deviations of the real spectral lines from the predicted ones:


Both of these features (and more) are incorporated in the relativistic Dirac equation, with predictions that come still closer to experiment. Again the Dirac equation may be solved analytically in the special case of a two-body system, such as the hydrogen atom. The resulting solution quantum states now must be classified by the total angular momentum number "j" (arising through the coupling between electron spin and orbital angular momentum). States of the same "j" and the same "n" are still degenerate. Thus, direct analytical solution of Dirac equation predicts 2S() and 2P() levels of Hydrogen to have exactly the same energy, which is in a contradiction with observations (Lamb-Retherford experiment).


For these developments, it was essential that the solution of the Dirac equation for the hydrogen atom could be worked out exactly, such that any experimentally observed deviation had to be taken seriously as a signal of failure of the theory.

In the language of Heisenberg's matrix mechanics, the hydrogen atom was first solved by Wolfgang Pauli using a rotational symmetry in four dimensions [O(4)-symmetry] generated by the angular momentum 
and the Laplace–Runge–Lenz vector. By extending the symmetry group O(4) to the dynamical group O(4,2),
the entire spectrum and all transitions were embedded in a single irreducible group representation.

In 1979 the (non relativistic) hydrogen atom was solved for the first time within Feynman's path integral formulation
of quantum mechanics by Duru and Kleinert. This work greatly extended the range of applicability of Feynman's method.





</doc>
<doc id="14227" url="https://en.wikipedia.org/wiki?curid=14227" title="Elagabalus">
Elagabalus

Elagabalus or Heliogabalus ( 204 – 11 March 222), officially known as Antoninus, was Roman emperor from 218 to 222. His short reign was conspicuous for sex scandals and religious controversy. He was cousin to the emperor Caracalla, and came from a prominent Arab family in Emesa (Homs), Syria, where in his early youth he served as head priest of the namesake sun god Elagabalus. After the death of Caracalla, Elagabalus was raised to the principate at 14 years of age in an army revolt instigated by his grandmother, Julia Maesa, against Caracalla's short-lived successor, Macrinus. As a private citizen, he was probably named "Varius Avitus Bassianus". Upon becoming emperor he took the name "Marcus Aurelius Antoninus", and became known after his native god only after his death.

Later historians suggest Elagabalus showed a disregard for Roman religious traditions and sexual taboos. He replaced the traditional head of the Roman pantheon, Jupiter, with the deity Elagabalus, of whom he had been high priest. He forced leading members of Rome's government to participate in religious rites celebrating this deity, over which he personally presided. Elagabalus married four women, including a Vestal Virgin, and lavished favours on male courtiers popularly thought to have been his lovers. He was also reported to have prostituted himself. His behavior estranged the Praetorian Guard, the Senate, and the common people alike. Amidst growing opposition, Elagabalus, just 18 years old, was assassinated and replaced by his cousin Severus Alexander in March 222. The assassination plot against Elagabalus was devised by his grandmother, Julia Maesa, and carried out by disaffected members of the Praetorian Guard.

Elagabalus developed a reputation among his contemporaries for extreme eccentricity, decadence, and zealotry. This tradition has persisted, and with writers of the early modern age he suffers one of the worst reputations among Roman emperors. Edward Gibbon, for example, wrote that Elagabalus "abandoned himself to the grossest pleasures with ungoverned fury". According to Barthold Georg Niebuhr, "The name Elagabalus is branded in history above all others" because of his "unspeakably disgusting life". An example of a modern historian's assessment would be Adrian Goldsworthy's: "Elagabalus was not a tyrant, but he was an incompetent, probably the least able emperor Rome had ever had." Despite universal condemnation, some scholars do write warmly about him, including 6th century Roman chronicler John Malalas, and Warwick Ball, a modern historian who described him as innovative and "a tragic enigma lost behind centuries of prejudice". 

Elagabalus is considered by some to be an early transgender figure and one of the first on record as seeking sex reassignment surgery.

Elagabalus was born no later than 204 CE, though perhaps as early as 203, to Sextus Varius Marcellus and Julia Soaemias Bassiana, who had probably married around the year 200 (and no later than 204). Elagabalus's full birth name was probably (Sextus) Varius Avitus Bassianus, the last name being apparently the surname of the Emesene family. Marcellus was an equestrian, later elevated to a senatorial position. Julia Soaemias was a cousin of the emperor Caracalla, and there were rumors (which Soaemias later publicly supported) that Elagabalus was Caracalla's child. Marcellus's tombstone attests that Elagabalus had at least one brother, about whom nothing is known. Elagabalus's grandmother, Julia Maesa, was the widow of the consul Julius Avitus, the sister of Julia Domna, and the sister-in-law of the emperor Septimius Severus. Other relatives included Elagabalus's aunt Julia Avita Mamaea and uncle Marcus Julius Gessius Marcianus and their son Severus Alexander.

Elagabalus's family held hereditary rights to the priesthood of the sun god Elagabal, of whom Elagabalus was the high priest at Emesa (modern Homs) in Roman Syria as part of the Arab Emesene dynasty. The deity's Latin name, "Elagabalus", is a Latinized version of the Arabic "Ilāh ha-Gabal", from "ilāh" ("god") and "gabal" ("mountain"), meaning "God of the Mountain", the Emesene manifestation of Ba'al. Initially venerated at Emesa, the deity's cult spread to other parts of the Roman Empire in the 2nd century; a dedication has been found as far away as Woerden (in the Netherlands), near the Roman "limes". The god was later imported to Rome and assimilated with the sun god known as Sol Indiges in the era of the Roman Republic times and as Sol Invictus during the late third century. In Greek, the sun god is Helios, hence Elagabal was known as "Heliogabalus", a hybrid of "Helios" and "Elagabalus".

Herodian writes that when the emperor Macrinus came to power, he suppressed the threat to his reign from the family of his assassinated predecessor, Caracalla, by exiling them—Julia Maesa, her two daughters, and her eldest grandson Elagabalus—to their estate at Emesa in Syria. Almost upon arrival in Syria, Maesa began a plot with her advisor and Elagabalus' tutor, Gannys, to overthrow Macrinus and elevate the fourteen-year-old Elagabalus to the imperial throne.

Maesa spread a rumor, which Soaemias publicly supported, that Elagabalus was the illegitimate child of Caracalla and so deserved the loyalty of Roman soldiers and senators who had sworn allegiance to Caracalla. The soldiers of the Third Legion at Raphana, who had enjoyed greater privileges under Caracalla and resented Macrinus (and may have been impressed or bribed by Maea's wealth), supported this claim. At sunrise on 16 May 218, Elagabalus was declared emperor by Publius Valerius Comazon, commander of the legion. To strengthen his legitimacy, Elagabalus adopted the same name Caracalla bore as emperor, Marcus Aurelius Antoninus. Cassius Dio states that some officers tried to keep the soldiers loyal to Macrinus, but they were unsuccessful. 

Praetorian prefect Ulpius Julianus responded by attacking the Third Legion, most likely on Macrinus's orders (though one account says he acted on his own before Macrinus knew of the rebellion). Herodian suggests Macrinus underestimated the threat, considering the rebellion inconsequential. During the fighting, Julianus's soldiers killed their officers and joined Elagabalus's forces. 

Macrinus asked the Roman Senate to denounce Elagabalus as "the False Antoninus", and they complied, declaring war on Elagabalus and his family. Macrinus made his son Diadumenian co-emperor, and attempted to secure the loyalty of the Second Legion with large cash payments. During a banquet to celebrate this at Apamea, however, a messenger presented Macrinus with the severed head of his defeated prefect Julianus. Macrinus therefore retreated to Antioch, after which the Second Legion shifted its loyalties to Elagabalus.

Elagabalus's legionnaires, commanded by Gannys, defeated Macrinus and Diadumenian and their Praetorian Guard at the Battle of Antioch on 8 June 218, prevailing after Macrinus's troops broke ranks after he fled the battlefield. Macrinus fled for Italy, but was intercepted near Chalcedon and executed in Cappadocia, while Diadumenian was captured at Zeugma and executed.

That month, Elagabalus wrote to the senate, assuming the imperial titles without waiting for senatorial approval, which violated tradition but was a common practice among 3rd-century emperors. Letters of reconciliation were dispatched to Rome extending amnesty to the Senate and recognizing its laws, while also condemning the administration of Macrinus and his son.

The senators responded by acknowledging Elagabalus as emperor and accepting his claim to be the son of Caracalla. Caracalla and Julia Domna were both deified by the Senate, both Julia Maesa and Julia Soaemias were elevated to the rank of Augustae, and the memory of Macrinus was expunged by the Senate.(Elagabalus's imperial artifacts assert that he succeeded Caracalla directly.) Comazon was appointed commander of the Praetorian Guard.

Elagabalus stayed for a time at Antioch, apparently to quell various mutinies. Dio outlines several, which historian Fergus Millar places prior to the winter of 218–219. These included one by Gellius Maximus, who commanded the Fourth Legion and was executed, and one by Verus, who commanded the Third Gallic Legion, which was disbanded once he was put down.

Next, according to Herodian, Elagabalus and his entourage spent the winter of 218–219 in Bithynia at Nicomedia, and then traveled through Thrace and Moesia to Italy in the first half of 219. Herodian says that Elagabalus had a painting of himself sent ahead to Rome to be hung over a statue of the goddess Victoria in the Senate House so people would not be surprised by his Eastern garb, but it is unclear if such a painting actually existed, and Dio does not mention it. If the painting was indeed hung over Victoria, it put senators in the position of seeming to make offerings to Elagabalus when they made offerings to Victoria.

On his way to Rome, Elagabalus and his allies executed several prominent supporters of Macrinus, such as Syrian governor Fabius Agrippinus and former Thracian governor C. Claudius Attalus Paterculianus. In Rome, however, his offer of amnesty for the Roman upper class was largely honoured, though the jurist Ulpian was exiled. Elagabalus made Comazon praetorian prefect, and later consul (220) and prefect of the city (three times, 220-222), which Dio regarded as a violation of Roman norms. Herodian and the "Augustan History" say that Elagabalus alienated many by giving powerful positions to other allies.

Dio states that Elagabalus wanted to marry a charioteer named Hierocles and to declare him Caesar, like (Dio says) he had previously wanted to marry Gannys and name him Caesar. The athlete Aurelius Zoticus is said by Dio to have been Elagabalus's lover and (non-administrative) "Cubicularius", while the "Augustan History" says Zoticus was a husband to Elagabalus and held greater political influence.

Elagabalus's relationships to his mother Julia Soaemias and grandmother Julia Maesa were strong at first; they were influential supporters from the beginning, and Macrinus declared war on them as well as Elagabalus. Accordingly, they became the first women allowed into the Senate, and both received senatorial titles: Soaemias the established title of "Clarissima," and Maesa the more unorthodox "Mater Castrorum et Senatus" ("Mother of the army camp and of the Senate"). They exercised influence over the young emperor throughout his reign, and are found on many coins and inscriptions, a rare honor for Roman women.

Under Elagabalus, the gradual devaluation of Roman aurei and denarii continued (with the silver purity of the denarius dropping from 58% to 46.5%), though antoniniani had a higher metal content than under Caracalla.

Since the reign of Septimius Severus, sun worship had increased throughout the Empire. In a shocking move, Elagabalus instated Elagabal as the chief deity of the Roman pantheon, honored above Jupiter, with Elagabalus himself its chief priest.

As a token of respect for Roman religion, however, Elagabalus joined either Astarte, Minerva, Urania, or some combination of the three to Elagabal as consort. A union between Elagabal and a traditional goddess would have served to strengthen ties between the new religion and the imperial cult. In fact, there may have been an effort to introduce Elagabal, Urania, and Athena as the new Capitoline triad of Rome—replacing Jupiter, Juno, and Minerva.

He aroused further discontent when he married the vestal virgin Aquilia Severa, Vesta's high priestess, claiming the marriage would produce "godlike children". This was a flagrant breach of Roman law and tradition, which held that any Vestal found to have engaged in sexual intercourse was to be buried alive.

A lavish temple called the Elagabalium was built on the east face of the Palatine Hill to house Elagabal,who was represented by a black conical meteorite from Emesa. Herodian wrote "this stone is worshipped as though it were sent from heaven; on it there are some small projecting pieces and markings that are pointed out, which the people would like to believe are a rough picture of the sun, because this is how they see them".

Dio writes that in order to increase his piety as high priest of Elagabal atop a new Roman pantheon, Elagabalus had himself circumcised and swore to abstain from swine. He forced senators to watch while he danced circling the altar of Elagabal to the accompaniment of drums and cymbals. Each summer solstice he held a festival dedicated to the god, which became popular with the masses because of the free food distributed on these occasions. During this festival, Elagabalus placed the black stone on a chariot adorned with gold and jewels, which he paraded through the city:

The most sacred relics from the Roman religion were transferred from their respective shrines to the Elagabalium, including the emblem of the Great Mother, the fire of Vesta, the Shields of the Salii, and the Palladium, so that no other god could be worshipped except in association with Elagabal. Although his native cult was widely ridiculed by contemporaries, sun-worship was popular among the soldiers and would be promoted by several later emperors.

The question of Elagabalus' sexual orientation is confused, owing to salacious and unreliable sources. Cassius Dio states that Elagabalus was married five times (twice to the same woman). His first wife was Julia Cornelia Paula, whom he married prior to August 29, 219; between then and August 28, 220, he divorced Paula, took the Vestal Virgin Julia Aquilia Severa as his second wife, divorced her, and took a third wife, whom Herodian says was Annia Aurelia Faustina, a descendant of Marcus Aurelius and the widow of a man Elagabalus had recently had executed, Pomponius Bassus. In the last year of his reign, Elagabalus divorced Annia Faustina and remarried Aquilia Severa.

Dio states that another "husband of this woman [Elagabalus] was Hierocles", an ex-slave and chariot driver from Caria. The "Augustan History" claims that Elagabalus also married a man named Zoticus, an athlete from Smyrna, while Dio says only that Zoticus was his cubicularius. Dio says that Elagabalus prostituted himself in taverns and brothels.

Dio (who referred to Elagabalus with feminine pronouns) says Elagabalus delighted in being called Hierocles' mistress, wife, and queen. The emperor reportedly wore makeup and wigs, preferred to be called a lady and not a lord, and offered vast sums to any physician who could provide him with a vagina; for this reason the emperor is seen by some writers as an early transgender figure and one of the first on record as seeking sex reassignment surgery.

By 221 Elagabalus' eccentricities, particularly his relationship with Hierocles, increasingly provoked the soldiers of the Praetorian Guard. When Elagabalus' grandmother Julia Maesa perceived that popular support for the emperor was waning, she decided that he and his mother, who had encouraged his religious practices, had to be replaced. As alternatives, she turned to her other daughter, Julia Avita Mamaea, and her daughter's son, the fifteen-year-old Severus Alexander.

Prevailing on Elagabalus, she arranged that he appoint his cousin Alexander as his heir and that the boy be given the title of "Caesar". Alexander shared the consulship with the emperor that year. However, Elagabalus reconsidered this arrangement when he began to suspect that the Praetorian Guard preferred his cousin to himself.

Elagabalus ordered various attempts on Alexander's life, due to failing in getting approval from the senate in stripping Alexander of his shared title. According to Dio, Elagabalus invented the rumor that Alexander was near death, in order to see how the Praetorians would react. A riot ensued, and the Guard demanded to see Elagabalus and Alexander in the Praetorian camp.

The Emperor complied and on 11 March 222 he publicly presented his cousin along with his own mother, Julia Soaemias. On their arrival the soldiers started cheering Alexander while ignoring Elagabalus, who ordered the summary arrest and execution of anyone who had taken part in this display of insubordination. In response, members of the Praetorian Guard attacked Elagabalus and his mother:

Following his assassination, many associates of Elagabalus were killed or deposed, including his lover Hierocles. His religious edicts were reversed and the stone of Elagabal was sent back to Emesa. Women were again barred from attending meetings of the Senate. The practice of "damnatio memoriae"—erasing from the public record a disgraced personage formerly of note—was systematically applied in his case. Several images, including an over-life-size statue of him as Hercules that is now in Naples, were re-carved with the face of Alexander Severus.

The source of many of these stories of Elagabalus's depravity is the "Augustan History" ("Historia Augusta"), which includes controversial claims. It is most likely that the "Historia Augusta" was written towards the end of the 4th century, during the reign of Emperor Theodosius I. The life of Elagabalus as described in the "Augustan History" is of uncertain historical merit. Sections 13 to 17, relating to the fall of Elagabalus, are less controversial among historians. The author of the most scandalous stories in the "Augustan History" concedes: "However, both these matters and some others which pass belief were, I think, invented by people who wanted to depreciate Heliogabalus to win favour with Alexander."

The historian Cassius Dio, who lived from the second half of the 2nd century until sometime after 229, wrote a contemporary account of Elagabalus. Born into a patrician family, he spent the greater part of his life in public service. He was a senator under emperor Commodus and governor of Smyrna after the death of Septimius Severus. Afterwards, he served as suffect consul around 205, and as proconsul in Africa and Pannonia. 

Dio's "Roman History" spans nearly a millennium, from the arrival of Aeneas in Italy until the year 229. His contemporaneous account of Elagabalus's reign is generally considered more reliable than the "Augustan History", though by his own admission Dio spent the greater part of the relevant period outside of Rome and had to rely on second-hand information.

Furthermore, the political climate in the aftermath of Elagabalus' reign, as well as Dio's own position within the government of Severus Alexander, who held him in high esteem and made him consul again, likely influenced the truth of this part of his history for the worse. Dio regularly refers to Elagabalus as Sardanapalus, partly to distinguish him from his divine namesake, but chiefly to do his part in maintaining the "damnatio memoriae" and to associate him with another autocrat notorious for a dissolute life.

In some instances, Dio's account is demonstrably inaccurate, as when he says Elagabalus appointed unqualified officials and that Comazon had no military experience before being named to head the Praetorian Guard, when in fact Comazon had commanded the Third Legion. Dio also gives different accounts in different places of when and by whom Diadumenian was given imperial names and titles.

Another contemporary of Elagabalus' was Herodian, a minor Roman civil servant who lived from c. 170 until 240. His work, "History of the Roman Empire since Marcus Aurelius", commonly abbreviated as "Roman History", is an eyewitness account of the reign of Commodus until the beginning of the reign of Gordian III. His work largely overlaps with Dio's own "Roman History", but the texts, written independently of each other, agree more often than not about the emperor and his short but eventful reign.

Although Herodian is not deemed as reliable as Dio, his lack of literary and scholarly pretensions make him less biased than senatorial historians. Herodian is considered the most important source for the religious reforms which took place during the reign of Elagabalus, which have been confirmed by numismatic and archaeological evidence.

For readers of the modern age, "The History of the Decline and Fall of the Roman Empire" by Edward Gibbon (1737–1794) further cemented the scandalous reputation of Elagabalus. Gibbon not only accepted and expressed outrage at the allegations of the ancient historians, but he might have added some details of his own; he is the first historian known to claim that Gannys was a eunuch, for example. Gibbon wrote:

The 20th-century anthropologist James George Frazer (author of "The Golden Bough") took seriously the monotheistic aspirations of the emperor, but also ridiculed him: "The dainty priest of the Sun [was] the most abandoned reprobate who ever sat upon a throne ... It was the intention of this eminently religious but crack-brained despot to supersede the worship of all the gods, not only at Rome but throughout the world, by the single worship of Elagabalus or the Sun."

The first book-length biography was "The Amazing Emperor Heliogabalus" (1911) by J. Stuart Hay, "a serious and systematic study" more sympathetic than that of previous historians, which nonetheless stressed the exoticism of Elagabalus, calling his reign one of "enormous wealth and excessive prodigality, luxury and aestheticism, carried to their ultimate extreme, and sensuality in all the refinements of its Eastern habit".

Some recent historians paint a more favourable picture of the emperor's rule. Martijn Icks, in "Images of Elagabalus" (2008; republished as "The Crimes of Elagabalus" in 2012), doubts the reliability of the ancient sources and argues that it was the emperor's unorthodox religious policies that alienated the power elite of Rome, to the point that his grandmother saw fit to eliminate him and replace him with his cousin. He described ancient stories pertaining to the emperor as “part of a long tradition of ‘character assassination’ in ancient historiography and biography.”

Leonardo de Arrizabalaga y Prado, in "The Emperor Elagabalus: Fact or Fiction?" (2008), is also critical of the ancient historians and speculates that neither religion nor sexuality played a role in the fall of the young emperor. He was simply the loser in a power struggle within the imperial family; the loyalty of the Praetorian Guards was up for sale, and Julia Maesa had the resources to outmaneuver and outbribe her grandson. In this version of events, once Elagabalus, his mother, and his immediate circle had been murdered, a campaign of character assassination began, resulting in a grotesque caricature that has persisted to the present day.

Warwick Ball, in his book "Rome in the East", writes a very apologetic account of the emperor. He argues that the wild descriptions of his religious rites were exaggerated and should be dismissed as propaganda, similar to how pagan descriptions of Christian rites (involving cannibalism and unspeakable orgies) have since been dismissed. Ball continues to describe the emperor’s ritual processions (marriage of the gods) as sound political and religious policy; that syncretism of eastern and western deities deserves praise rather than the ridicule he received. Ultimately, he paints Elagabalus as a child forced to become emperor by his scheming grandmother, and who rightfully, as high-priest of a cult, continued his rituals even after becoming emperor, which he viewed as a secondary occupation. Finally, he notes the eventual victory of Elagabalus, as his deity would be welcomed by Rome in its Sol Invictus form, brought back from Emesa by Aurelian 50 years later. Sol Invictus influenced the Christian beliefs of Constantine, being grafted into Christianity till this day.

Due to the ancient stories about him, Elagabalus became something of an (anti-)hero in the Decadent movement of the late 19th century. He often appears in literature and other creative media as the epitome of a young, amoral aesthete. His life and character have informed or at least inspired many famous works of art, especially by Decadents, and by contemporary artists. The most notable of these works include:










 


</doc>
<doc id="14229" url="https://en.wikipedia.org/wiki?curid=14229" title="Homeopathy">
Homeopathy

Homeopathy or homoeopathy is a pseudoscientific system of alternative medicine. It was created in 1796 by Samuel Hahnemann. Its practitioners, called homeopaths, believe that a substance that causes symptoms of a disease in healthy people would cure similar symptoms in sick people; this doctrine is called "similia similibus curentur", or "like cures like". Homeopathic preparations are termed "remedies" and are made using homeopathic dilution. In this process, a chosen substance is repeatedly and thoroughly diluted. The final product is chemically indistinguishable from the diluent, which is usually either distilled water, ethanol or sugar; often, not even a single molecule of the original substance can be expected to remain in the product. Between the dilution iterations homeopaths practice hitting and/or violently shaking the product, and claim that it makes the diluent remember the original substance after its removal. Practitioners claim that such preparations, upon oral intake, can treat or cure disease.

All relevant scientific knowledge about physics, chemistry, biochemistry and biology gained since at least the mid-19th century confirms that homeopathic remedies have no active content. They are biochemically inert, and have no effect on any known disease. Hahnemann's theory of disease, centered around principles he termed miasms, is inconsistent with subsequent identification of viruses and bacteria as causes of disease. Clinical trials have been conducted, and generally demonstrated no objective effect from homeopathic preparations. The fundamental implausibility of homeopathy as well as a lack of demonstrable effectiveness has led to it being characterized within the scientific and medical communities as quackery and nonsense.

After assessments of homeopathy, national and international bodies have recommended the withdrawal of government funding. Australia, the United Kingdom, Switzerland and France, as well as the European Academies' Science Advisory Council, and the Commission on Pseudoscience and Research Fraud of Russian Academy of Sciences each concluded that homeopathy is ineffective, and recommended against the practice receiving any further funding. Notably, France and England, where homeopathy was formerly prevalent, are in the process of removing all public funding. The National Health Service in England ceased funding homeopathic remedies in November 2017 and asked the Department of Health in the UK to add homeopathic remedies to the blacklist of forbidden prescription items, and France will remove funding by 2021. In November 2018, Spain also announced moves to ban homeopathy and other pseudotherapies.

Homeopathy, the longest established alternative medicine to come out of Europe, was created in 1796 by Samuel Hahnemann. Hahnemann rejected the mainstream medicine of the late 18th century as irrational and inadvisable because it was largely ineffective and often harmful. He advocated the use of single drugs at lower doses and promoted an immaterial, vitalistic view of how living organisms function. The term "homeopathy" was coined by Hahnemann and first appeared in print in 1807. He also coined the expression "allopathic medicine", which was used to pejoratively refer to traditional Western medicine.

Hahnemann conceived of homeopathy while translating a medical treatise by the Scottish physician and chemist William Cullen into German. Being sceptical of Cullen's theory that cinchona cured malaria because it was bitter, Hahnemann ingested some bark specifically to investigate what would happen. He experienced fever, shivering and joint pain: symptoms similar to those of malaria itself. From this, Hahnemann came to believe that all effective drugs produce symptoms in healthy individuals similar to those of the diseases that they treat, in accord with the "law of similars" that had been proposed by ancient physicians. This led to the name ""homeopathy"", which comes from the "hómoios", "-like" and "páthos", "suffering".

Hahnemann's law of similars is an "ipse dixit" that does not derive from the scientific method. An account of the effects of eating cinchona bark noted by Oliver Wendell Holmes, and published in 1861, failed to reproduce the symptoms Hahnemann reported. Subsequent scientific work showed that cinchona cures malaria because it contains quinine, which kills the "Plasmodium falciparum" parasite that causes the disease; the mechanism of action is unrelated to Hahnemann's ideas.

Hahnemann began to test what effects substances may have produced in humans, a procedure later called "homeopathic proving". These tests required subjects to test the effects of ingesting substances by clearly recording all of their symptoms as well as the ancillary conditions under which they appeared. He published a collection of provings in 1805, and a second collection of 65 preparations appeared in his book, "Materia Medica Pura" (1810).

Because Hahnemann believed that large doses of drugs that caused similar symptoms would only aggravate illness, he advocated extreme dilutions of the substances; he devised a technique for making dilutions that he believed would preserve a substance's therapeutic properties while removing its harmful effects. Hahnemann believed that this process aroused and enhanced "the spirit-like medicinal powers of the crude substances". He gathered and published a complete overview of his new medical system in his book, "The Organon of the Healing Art" (1810), whose 6th edition, published in 1921, is still used by homeopaths today.

In the "Organon", Hahnemann introduced the concept of "miasms" as "infectious principles" underlying chronic disease and as "peculiar morbid derangement[s] of vital force". Hahnemann associated each miasm with specific diseases, and thought that initial exposure to miasms causes local symptoms, such as skin or venereal diseases. His assertion was that if these symptoms were suppressed by medication, the cause went deeper and began to manifest itself as diseases of the internal organs. Homeopathy maintains that treating diseases by directly alleviating their symptoms, as is sometimes done in conventional medicine, is ineffective because all "disease can generally be traced to some latent, deep-seated, underlying chronic, or inherited tendency". The underlying imputed miasm still remains, and deep-seated ailments can be corrected only by removing the deeper disturbance of the vital force.

Hahnemann's hypotheses for miasms originally presented only three local symptoms: psora (the itch), syphilis (venereal disease) or sycosis (fig-wart disease). Of these the most important was "psora", described as being related to any itching diseases of the skin and was claimed to be the foundation of many further disease conditions. Hahnemann believed it to be the cause of such diseases as epilepsy, cancer, jaundice, deafness, and cataracts. Since Hahnemann's time, other miasms have been proposed, some replacing one or more of psora's proposed functions, including tuberculosis and cancer miasms.

The law of susceptibility implies that a negative state of mind can attract miasms to invade the body and produce symptoms of diseases. Hahnemann rejected the notion of a disease as a separate thing or invading entity, and insisted it was always part of the "living whole".

Hahnemann's miasm theory remains disputed and controversial within homeopathy even in modern times. The theory of miasms has been criticized as an explanation developed to preserve the system of homeopathy in the face of treatment failures, and for being inadequate to cover the many hundreds of sorts of diseases, as well as for failing to explain disease predispositions, as well as genetics, environmental factors, and the unique disease history of each patient.

Homeopathy achieved its greatest popularity in the 19th century. It was introduced to the United States in 1825 by Hans Birch Gram, a student of Hahnemann. The first homeopathic school in the US opened in 1835, and in 1844, the first US national medical association, the American Institute of Homeopathy, was established. Throughout the 19th century, dozens of homeopathic institutions appeared in Europe and the United States, and by 1900, there were 22 homeopathic colleges and 15,000 practitioners in the United States. 

Because medical practice of the time relied on ineffective and often dangerous treatments, patients of homeopaths often had better outcomes than those of the doctors. Homeopathic preparations, even if ineffective, would almost surely cause no harm, making the users of homeopathic preparations less likely to be killed by the treatment that was supposed to be helping them. The relative success of homeopathy in the 19th century may have led to the abandonment of the ineffective and harmful treatments of bloodletting and purging and begun the move towards more effective, science-based medicine. One reason for the growing popularity of homeopathy was its apparent success in treating people suffering from infectious disease epidemics. During 19th-century epidemics of diseases such as cholera, death rates in homeopathic hospitals were often lower than in conventional hospitals, where the treatments used at the time were often harmful and did little or nothing to combat the diseases.

From its inception, however, homeopathy was criticized by mainstream science. Sir John Forbes, physician to Queen Victoria, said in 1843 that the extremely small doses of homeopathy were regularly derided as useless, "an outrage to human reason". James Young Simpson said in 1853 of the highly diluted drugs: "No poison, however strong or powerful, the billionth or decillionth of which would in the least degree affect a man or harm a fly." 19th-century American physician and author Oliver Wendell Holmes was also a vocal critic of homeopathy and published an essay entitled "Homœopathy and Its Kindred Delusions" (1842). The members of the French Homeopathic Society observed in 1867 that some leading homeopathists of Europe not only were abandoning the practice of administering infinitesimal doses but were also no longer defending it. The last school in the US exclusively teaching homeopathy closed in 1920.

According to , the Nazi regime in Germany was fascinated by homeopathy, and spent large sums of money on researching its mechanisms, but without gaining a positive result. Unschuld further argues that homeopathy never subsequently took root in the United States, but remained more deeply established in European thinking. In the United States, the "Food, Drug, and Cosmetic Act" of 1938 (sponsored by Royal Copeland, a Senator from New York and homeopathic physician) recognized homeopathic preparations as drugs. In the 1950s, there were only 75 pure homeopaths practising in the U.S. By the mid to late 1970s, homeopathy made a significant comeback and sales of some homeopathic companies increased tenfold. 

Some homeopaths give credit for the revival to Greek homeopath George Vithoulkas, who performed a "great deal of research to update the scenarios and refine the theories and practice of homeopathy", beginning in the 1970s, but Ernst and Singh consider it to be linked to the rise of the New Age movement. Bruce Hood has argued that the increased popularity of homeopathy in recent times may be due to the comparatively long consultations practitioners are willing to give their patients, and to an irrational preference for "natural" products, which people think are the basis of homeopathic preparations.

Since the beginning of the 21st century, a series of meta-analyses have further shown that the therapeutic claims of homeopathy lack scientific justification. This had lead to a decrease or suspension of funding by many governments. In a 2010 report, the Science and Technology Committee of the United Kingdom House of Commons recommended that homeopathy should no longer be a beneficiary of NHS funding due its lack of scientific credibility; funding ceased in 2017. They also asked the Department of Health in the UK to add homeopathic remedies to the blacklist of forbidden prescription items,

In 2015, the National Health and Medical Research Council of Australia found there were 'there are no health conditions for which there is reliable evidence that homeopathy is effective". The federal government only ended up accepting three of the 45 recommendations made by the 2018 review of Pharmacy Remuneration and Regulation in 2018. The Food and Drug Administration (FDA) held a hearing in 2015, requesting public comment on the regulation of homeopathic drugs. In 2017 the FDA announced it would strengthen regulation of homeopathic products.

In July 2019, the French healthcare minister announced that social security reimbursements for homeopathic drugs will be phased out before 2021. France has long had a stronger belief in the virtues of homeopathic drugs than many other countries and the world's biggest manufacturer of alternative medicine drugs, Boiron, is located in that country. Spain has also announced moves to ban homeopathy and other pseudotherapies.

Homeopathic preparations are referred to as "homeopathic remedies". Practitioners rely on two types of reference when prescribing: "Materia medica" and repertories. A homeopathic "materia medica" is a collection of "drug pictures", organized alphabetically. A homeopathic repertory is a quick reference version of the "materia medica" that indexes the symptoms and then the associated remedies for each. In both cases different compilers may dispute particular inclusions. The first symptomatic homeopathic "materia medica" was arranged by Hahnemann. The first homeopathic repertory was Georg Jahr's "Symptomenkodex", published in German in 1835, and translated into English as the "Repertory to the more Characteristic Symptoms of Materia Medica" in 1838.This version was less focused on disease categories and was the forerunner to later works by James Tyler Kent. There are over 118 repertories published in English with Kents being one of the most used.

Homeopaths generally begin with a consultation, which can be a 10-15 minute appointment or last for over an hour, where the patient describes their medical history and symptoms, with an emphasis on "modalities" (if they change depending on the weather and other external factors). They also take information on mood, likes and dislikes, their physical, mental and emotional states, life circumstances, any physical or emotional illnesses and their personalty. This "symptom picture" is matched to the "drug picture" in the "materia medica" or repertory and is used to specify certain homeopathic remedies. In classical homeopathy, the practitioner attempts to match a single preparation to the totality of symptoms (the "simlilum"), while "clinical homeopathy" involves combinations of preparations based on the various symptoms of an illness.

Homeopathy uses animal, plant, mineral, and synthetic substances in its preparations, generally referring to them using Latin or faux-Latin names. Examples include "arsenicum album" (arsenic oxide), "natrum muriaticum" (sodium chloride or table salt), "Lachesis muta" (the venom of the bushmaster snake), "opium", and "thyroidinum" (thyroid hormone). Homeopaths say this is to ensure accuracy. In the USA the common name must be displayed, although the Latin one can also be present.

Isopathy is a therapy derived from homeopathy where the preparations come from diseased or pathological products such as fecal, urinary, respiratory discharges, blood, and tissue. They are called nosodes (from the Greek "nosos", disease) wiuth preparations made from "healthy" specimens being termed "sarcodes". Many so-called "homeopathic vaccines" are a form of isopathy. Tautopathy is a form of isopathy where the preparations are composed of drugs or vaccines that a person has consumed in the past, in the belief that this can reverse lingering damage caused by the initial use. There is no convincing scientific evidence for isopathy as an effective method of treatment.

Some modern homeopaths use preparations they call "imponderables" because they do not originate from a substance but some other phenomenon presumed to have been "captured" by alcohol or lactose. Examples include X-rays and sunlight. Another derivitive is electrohomoeopathy, where an electric bio-energy of therapeutic value is supposedly extracted from plants. Popular in the late nineteenth century, electrohomeopathy is considered pseudo scientific and has been described as "utter idiocy". In 2012, the Allahabad High Court in Uttar Pradesh, India, handed down a decree which stated that electrohomeopathy was an unrecognized system of medicine which was quackery. 

Other minority practices include paper preparations, where the substance and dilution are written on pieces of paper and either pinned to the patients' clothing, put in their pockets, or placed under glasses of water that are then given to the patients. Radionics, the use of electromagnetic radiation such as radio waves, can also be used to manufacture preparations. Such practices have been strongly criticized by classical homeopaths as unfounded, speculative, and verging upon magic and superstition. Flower preparations are produced by placing flowers in water and exposing them to sunlight. The most famous of these are the Bach flower remedies, which were developed by Edward Bach.

Hahnemann found that undiluted doses caused reactions, sometimes dangerous ones, so specified that preparations be given at the lowest possible dose. He found that this reduced potency as well as side-effects, but formed the view that vigorous shaking by striking on an elastic surface – a process termed "succussion" by homeopaths – nullified this. It has been said that he came to this conclusion after deciding preparations subjected to agitation in transit, such as in saddle bags or in a carriage, were more "potent". Hahnemann had a saddle-maker construct a special wooden striking board covered in leather on one side and stuffed with horsehair. The process of dilution and succussion is termed "dynamization" or "potentization" by homeopaths. In industrial manufacture this may be done by machine. There are differences of opinion on the number and force of strikes, and some practitioners dispute the need for succussion. There are no laboratory assays and the importance and techniques for succussion cannot be determined with any certainty from the literature.

Serial dilution is achieved by taking an amount of the mixture and adding solvent, but the "Korsakovian" method may also be used. In the Korsakovian method the vessel in which the preparations are manufactured is emptied, refilled with solvent, with the volume of fluid adhering to the walls of the vessel deemed sufficient for the new batch. Insoluble solids, such as granite, diamond, and platinum, are diluted by grinding them with lactose ("trituration"). Fluxion, which dilutes the substance by continuously passing water through the vial, and radionic preparation methods of preparation do not require succussion. 

Three main logarithmic dilution scales are in regular use in homeopathy. Hahnemann created the "centesimal" or "C scale", diluting a substance by a factor of 100 at each stage. There is also a decimal dilution scale (notated as "X" or "D") in which the preparation is diluted by a factor of 10 at each stage. The centesimal scale was favoured by Hahnemann for most of his life. In his last ten years of his life, Hahnemann also developed a quintamillesimal (Q) or LM scale diluting the drug 1 part in 50,000 parts of diluent. 

A 2C dilution requires a substance to be diluted to one part in 100, and then some of that diluted solution diluted by a further factor of 100. This works out to one part of the original substance in 10,000 parts of the solution. A 6C dilution repeats this process six times, ending up with the original substance diluted by a factor of 100 (one part in one trillion). Higher dilutions follow the same pattern. In homeopathy, a solution that is more dilute is described as having a higher "potency", and more dilute substances are considered by homeopaths to be stronger and deeper-acting. The Korsakovian method is sometimes referred to as K on the label of a homeopathic preparation, e.g. 200CK is a 200C preparation made using the Korsakovian method.The end product is usually so diluted as to be indistinguishable from the diluent (pure water, sugar or alcohol). Hahnemann advocated dilutions of 1 part to 10, that is 30C, for most purposes. Hahnemann regularly used dilutions up to 300C but opined that "there must be a limit to the matter, it cannot go on indefinitely". The greatest dilution reasonably likely to contain at least one molecule of the original substance is around 12C. Homeopathic pills are made from an inert substance (often sugars, typically lactose), upon which a drop of liquid homeopathic preparation is placed and allowed to evaporate. The process of homeopathic dilution results in no objectively detectable active ingredient in most cases, but some preparations (e.g. calendula and arnica creams) do contain pharmacologically active doses.
Critics of homeopathy commonly attempt to illustrate the dilutions involved in homeopathy with analogies. An example given states that a 12C solution is equivalent to a "pinch of salt in both the North and South Atlantic Oceans", which is approximately correct. One-third of a drop of some original substance diluted into all the water on earth would produce a preparation with a concentration of about 13C. A 200C dilution of duck liver, marketed under the name Oscillococcinum, would require 10 universes worth of molecules to simply have just one original molecule in the final substance. The high dilutions characteristically used are often considered to be the most controversial and implausible aspect of homeopathy.

Not all homeopaths advocate high dilutions. Preparations at concentrations below 4X are considered an important part of homeopathic heritage. Many of the early homeopaths were originally doctors and generally used lower dilutions such as "3X" or "6X", rarely going beyond "12X". The split between lower and higher dilutions followed ideological lines. Those favouring low dilutions stressed pathology and a stronger link to conventional medicine, while those favouring high dilutions emphasized vital force, miasms and a spiritual interpretation of disease. Some products with such relatively lower dilutions continue to be sold, but like their counterparts, they have not been conclusively demonstrated to have any effect beyond that of a placebo.

Homeopaths say that they can determine the properties of their preparations by following a method which they call "proving". As performed by Hahnemann, provings involved administering various preparations to healthy volunteers. The volunteers were then observed, often for months at a time. They were made to keep extensive journals detailing all of their symptoms at specific times throughout the day. They were forbidden from consuming coffee, tea, spices, or wine for the duration of the experiment; playing chess was also prohibited because Hahnemann considered it to be "too exciting", though they were allowed to drink beer and encouraged to exercise in moderation. At first Hahnemann used undiluted doses for provings, but he later advocated provings with preparations at a 30C dilution, and most modern provings are carried out using ultra-dilute preparations.

Provings are claimed to have been important in the development of the clinical trial, due to their early use of simple control groups, systematic and quantitative procedures, and some of the first application of statistics in medicine. The lengthy records of self-experimentation by homeopaths have occasionally proven useful in the development of modern drugs: For example, evidence that nitroglycerin might be useful as a treatment for angina was discovered by looking through homeopathic provings, though homeopaths themselves never used it for that purpose at that time. The first recorded provings were published by Hahnemann in his 1796 "Essay on a New Principle". His "Fragmenta de Viribus" (1805) contained the results of 27 provings, and his 1810 "Materia Medica Pura" contained 65. For James Tyler Kent's 1905 "Lectures on Homoeopathic Materia Medica", 217 preparations underwent provings and newer substances are continually added to contemporary versions.

Though the proving process has superficial similarities with clinical trials, it is fundamentally different in that the process is subjective, not blinded, and modern provings are unlikely to use pharmacologically active levels of the substance under proving. As early as 1842, Oliver Holmes had noted that provings were impossibly vague, and the purported effect was not repeatable among different subjects.

Outside of the alternative medicine community, scientists have long considered homeopathy a sham or a pseudoscience, and the mainstream medical community regards it as quackery. There is an overall absence of sound statistical evidence of therapeutic efficacy, which is consistent with the lack of any biologically plausible pharmacological agent or mechanism. Proponents argue that homeopathic medicines must work by some, as yet undefined, biophysical mechanism.

The lack of convincing scientific evidence supporting its efficacy and its use of preparations without active ingredients have led to characterizations of homeopathy as pseudoscience and quackery, or, in the words of a 1998 medical review, "placebo therapy at best and quackery at worst". The Russian Academy of Sciences considers homeopathy a "dangerous 'pseudoscience' that does not work", and "urges people to treat homeopathy 'on a par with magic. The Chief Medical Officer for England, Dame Sally Davies, has stated that homeopathic preparations are "rubbish" and do not serve as anything more than placebos. In 2013, Mark Walport, the UK Government Chief Scientific Adviser and head of the Government Office for Science said "homoeopathy is nonsense, it is non-science." His predecessor, John Beddington, referring to his views that homeopathy "has no underpinning of scientific basis" being "fundamentally ignored" by the Government.

Jack Killen, acting deputy director of the National Center for Complementary and Alternative Medicine, says homeopathy "goes beyond current understanding of chemistry and physics". He adds: "There is, to my knowledge, no condition for which homeopathy has been proven to be an effective treatment." Ben Goldacre says that homeopaths who misrepresent scientific evidence to a scientifically illiterate public, have "... walled themselves off from academic medicine, and critique has been all too often met with avoidance rather than argument". Homeopaths often prefer to ignore meta-analyses in favour of cherry picked positive results, such as by promoting a particular observational study (one which Goldacre describes as "little more than a customer-satisfaction survey") as if it were more informative than a series of randomized controlled trials.

In an article entitled "Should We Maintain an Open Mind about Homeopathy?" published in the "American Journal of Medicine", Michael Baum and Edzard Ernstwriting to other physicianswrote that "Homeopathy is among the worst examples of faith-based medicine... These axioms [of homeopathy] are not only out of line with scientific facts but also directly opposed to them. If homeopathy is correct, much of physics, chemistry, and pharmacology must be incorrect...".

The "very" low concentration of homeopathic preparations, which often lack even a single molecule of the diluted substance, has been the basis of questions about the effects of the preparations since the 19th century. The extreme dilutions used in homeopathic preparations usually leave not one molecule of the original substance in the final product. James Randi and the groups have highlighted the lack of active ingredients by taking large 'overdoses'. None of the hundreds of demonstrators in the UK, Australia, New Zealand, Canada and the US were injured and "no one was cured of anything, either". The laws of chemistry state that there is a limit to the dilution that can be made without losing the original substance altogether. This limit, which is related to Avogadro's number, is roughly equal to homeopathic dilutions of 12C or 24X (1 part in 10).

Modern advocates of homeopathy have proposed a concept of "water memory", according to which water "remembers" the substances mixed in it, and transmits the effect of those substances when consumed. This concept is inconsistent with the current understanding of matter, and water memory has never been demonstrated to have any detectable effect, biological or otherwise. Existence of a pharmacological effect in the absence of any true active ingredient is inconsistent with the law of mass action and the observed dose-response relationships characteristic of therapeutic drugs 

Homeopaths contend that their methods produce a therapeutically active preparation, selectively including only the intended substance, though critics note that any water will have been in contact with millions of different substances throughout its history, and homeopaths have not been able to account for a reason why only the selected homeopathic substance would be a special case in their process.

Practitioners also hold that higher dilutions produce stronger medicinal effects. This idea is also inconsistent with observed dose-response relationships, where effects are dependent on the concentration of the active ingredient in the body. Some contend that the phenomenon of hormesis may support the idea of dilution increasing potency, but the dose-response relationship outside the zone of hormesis declines with dilution as normal, and nonlinear pharmacological effects do not provide any credible support for homeopathy.

No individual homeopathic preparation has been unambiguously shown by research to be different from placebo. The methodological quality of the primary research was generally low, with such problems as weaknesses in study design and reporting, small sample size, and selection bias. Since better quality trials have become available, the evidence for efficacy of homeopathy preparations has diminished; the highest-quality trials indicate that the preparations themselves exert no intrinsic effect. A review conducted in 2010 of all the pertinent studies of "best evidence" produced by the Cochrane Collaboration concluded that "the most reliable evidence – that produced by Cochrane reviews – fails to demonstrate that homeopathic medicines have effects beyond placebo."

Government-level reviews have been conducted in recent years. In 2009 the United Kingdom's House of Commons Science and Technology Committee concluded that there was no compelling evidence of effect other than placebo. The Australian National Health and Medical Research Council completed a comprehensive review of the effectiveness of homeopathic preparations in 2015, in which it concluded that "there were no health conditions for which there was reliable evidence that homeopathy was effective." The European Academies' Science Advisory Council (EASAC) published its official analysis in 2017 finding a lack of evidence that homeopathic products are effective, and raising concerns about quality control.

In contrast a 2011 book was published purportedly financed by the Swiss government that concluded that homeopathy was effective and cost efficient. It was hailed by proponents of homeopathy as proof that homeopathy works. It was found to be scientifically, logically and ethically flawed, with most authors having a conflict of interest. The Swiss Federal Office of Public Health released a statement saying the book was published without the consent of the Swiss government or administration.

A 2001 systematic review found that the methodological quality in the majority of randomized trials in homeopathy had shortcomings. A separate 2001 systematic review that assessed the quality of clinical trials of homeopathy found that such trials were generally of lower quality than trials of conventional medicine. Publication bias, where positive results are more likely to be published in journals, has been particularly marked in alternative medicine journals, where few of the published articles (just 5% during the year 2000) tend to report null results. Regarding the way in which homeopathy is represented in the medical literature, a systematic review found signs of bias in the publications of clinical trials (towards negative representation in mainstream medical journals, and "vice versa" in alternative medicine journals), but not in reviews.

Meta-analyses are essential tools to summarize evidence of therapeutic efficacy. Early systematic reviews and meta-analyses of trials evaluating the efficacy of homeopathic preparations in comparison with placebo often tended to generate positive results. Reports of three large meta-analyses warned that firm conclusions could not be reached, largely due to methodological flaws in the primary studies and the difficulty in controlling for publication bias. The positive finding of one of the early meta-analyses, published in "The Lancet" in 1997, was later reframed by the same research team as an "overestimated the effects of homeopathic treatments." A systematic review of the available systematic reviews confirmed in 2002 that higher-quality trials tended to have less positive results, and found no convincing evidence that any homeopathic preparation exerts clinical effects different from placebo. The same conclusion was also reached in 2005 in a meta-analysis published in "The Lancet". A 2017 systematic review and meta-analysis found that the most reliable evidence did not support the effectiveness of non-individualized homeopathy.

Some clinical trials have tested individualized homeopathy. A 1998 review of 19 placebo-controlled trials showed a pooled odds ratio of 1.17 to 2.23 in favour of individualized homeopathy over the placebo, but no difference was seen when the analysis was restricted to the methodologically best trials. The authors concluded that "the results of the available randomized trials suggest that individualized homeopathy has an effect over placebo. The evidence, however, is not convincing because of methodological shortcomings and inconsistencies." A 2014 systematic review and meta-analysis found that individualized homeopathic remedies may be slightly more effective than placebos, though the authors noted that their findings were based on low- or unclear-quality evidence. The same research team later reported that taking into account model validity did not significantly affect this conclusion.

Health organization, including the UK's National Health Service, the American Medical Association, the FASEB, and the National Health and Medical Research Council of Australia, have issued statements saying that there is no good-quality evidence that homeopathy is effective as a treatment for any health condition. In 2009, World Health Organization official Mario Raviglione criticized the use of homeopathy to treat tuberculosis; similarly, another WHO spokesperson argued there was no evidence homeopathy would be an effective treatment for diarrhoea. They warned against the use of homeopathy for serious conditions such as depression, HIV and malaria. The American College of Medical Toxicology and the American Academy of Clinical Toxicology recommend that no one use homeopathic treatment for disease or as a preventive health measure. These organizations report that no evidence exists that homeopathic treatment is effective, but that there is evidence that using these treatments produces harm and can bring indirect health risks by delaying conventional treatment.

Science offers a variety of explanations for how homeopathy may appear to cure diseases or alleviate symptoms even though the preparations themselves are inert:

While some articles have suggested that homeopathic solutions of high dilution can have statistically significant effects on organic processes including the growth of grain and enzyme reactions, such evidence is disputed since attempts to replicate them have failed. In 2001 and 2004, Madeleine Ennis published a number of studies that reported that homeopathic dilutions of histamine exerted an effect on the activity of basophils. In response to the first of these studies, "Horizon" aired a programme in which British scientists attempted to replicate Ennis' results; they were unable to do so.A 2007 systematic review of high-dilution experiments found that none of the experiments with positive results could be reproduced by all investigators.

In 1988, French immunologist Jacques Benveniste published a paper in the journal "Nature" while working at INSERM. The paper purported to have discovered that basophils, released histamine when exposed to a homeopathic dilution of anti-immunoglobulin E antibody. Sceptical of the findings, "Nature" assembled an independent investigative team to determine the accuracy of the research After investigation the team found that the experiments were "statistically ill-controlled", "interpretation has been clouded by the exclusion of measurements in conflict with the claim", and concluded, "We believe that experimental data have been uncritically assessed and their imperfections inadequately reported."

The provision of homeopathic preparations has been described as unethical. Michael Baum, Professor Emeritus of Surgery and visiting Professor of Medical Humanities at University College London (UCL), has described homoeopathy as a "cruel deception". Edzard Ernst, the first Professor of Complementary Medicine in the United Kingdom and a former homeopathic practitioner, has expressed his concerns about pharmacists who violate their ethical code by failing to provide customers with "necessary and relevant information" about the true nature of the homeopathic products they advertise and sell. In 2013 the UK Advertising Standards Authority concluded that the Society of Homeopaths were targeting vulnerable ill people and discouraging the use of essential medical treatment while making misleading claims of efficacy for homeopathic products. In 2015 the Federal Court of Australia imposed penalties on a homeopathic company for making false or misleading statements about the efficacy of the whooping cough vaccine and recommending homeopathic remedies as an alternative.

A 2000 review by homeopaths reported that homeopathic preparations are "unlikely to provoke severe adverse reactions". In 2012, a systematic review evaluating evidence of homeopathy's possible adverse effects concluded that "homeopathy has the potential to harm patients and consumers in both direct and indirect ways". A 2016 systematic review and meta-analysis found that, in homeopathic clinical trials, adverse effects were reported among the patients who received homeopathy about as often as they were reported among patients who received placebo or conventional medicine.

Some homeopathic preparations involve poisons such as Belladonna, arsenic, and poison ivy. In rare cases, the original ingredients are present at detectable levels. This may be due to improper preparation or intentional low dilution. Serious adverse effects such as seizures and death have been reported or associated with some homeopathic preparations. Instances of arsenic poisoning have occurred. In 2009, the FDA advised consumers to stop using three discontinued cold remedy Zicam products because it could cause permanent damage to users' sense of smell. In 2016 the FDA issued a safety alert to consumers warning against the use of homeopathic teething gels and tablets following reports of adverse events after their use. A previous FDA investigation had found that these products were improperly diluted and contained "unsafe levels of belladonna" and that the reports of serious adverse events in children using this product were "consistent with belladonna toxicity".

Patients who choose to use homeopathy rather than evidence-based medicine risk missing timely diagnosis and effective treatment, thereby worsening the outcomes of serious conditions such as cancer. Critics have cited cases of patients failing to receive proper treatment for diseases that could have been easily managed with conventional medicine and who have died as a result. They have also condemned the "marketing practice" of criticizing and downplaying the effectiveness of mainstream medicine. Homeopaths claim that use of conventional medicines will "push the disease deeper" and cause more serious conditions, a process referred to as "suppression". In 1978, Anthony Campbell, a consultant physician at the Royal London Homeopathic Hospital, criticized statements by George Vithoulkas claiming that syphilis, when treated with antibiotics, would develop into secondary and tertiary syphilis with involvement of the central nervous system. Vithoulkas' claims echo the idea that treating a disease with external medication used to treat the symptoms would only drive it deeper into the body and conflict with scientific studies, which indicate that penicillin treatment produces a complete cure of syphilis in more than 90% of cases.

The use of homeopathy as a preventive for serious infectious diseases, called homeoprophylaxis, is especially controversial. Some homeopaths (particularly those who are non-physicians) advise their patients against immunization. Others have suggested that vaccines be replaced with homeopathic "nosodes". While Hahnemann was opposed to such preparations, modern homeopaths often use them although there is no evidence to indicate they have any beneficial effects. Promotion of homeopathic alternatives to vaccines has been characterized as dangerous, inappropriate and irresponsible. In December 2014, the Australian homeopathy supplier Homeopathy Plus! was found to have acted deceptively in promoting homeopathic alternatives to vaccines. In 2019, an investigative journalism piece by the Telegraph revealed that homeopathy practitioners were actively discouraging patients from vaccinating their children. Cases of homeopaths advising against the use of anti-malarial drugs have also been identified.putting visitors to the tropics in severe danger. 

A 2006 review recommends that pharmacy colleges include a required course where ethical dilemmas inherent in recommending products lacking proven safety and efficacy data be discussed and that students should be taught where unproven systems such as homeopathy depart from evidence-based medicine.

Homeopathy is fairly common in some countries while being uncommon in others; is highly regulated in some countries and mostly unregulated in others. It is practised worldwide and professional qualifications and licences are needed in most countries. In some countries, there are no specific legal regulations concerning the use of homeopathy, while in others, licences or degrees in conventional medicine from accredited universities are required.

Some homeopathic treatment is covered by the public health service of several European countries, including France (being phased out coverage in 2021), Scotland, and Luxembourg. In England, the NHS recommends against prescribing homeopathic preparations, although in 2018 prescriptions worth £55,000 were written in defiance of the guidelines. This represented less than 0.001% of the total NHS prescribing budget. In 2016 the UK's Committee of Advertising Practice (CAP) Compliance team wrote to homeopaths in the UK to "remind them of the rules that govern what they can and can't say in their marketing materials". The letter told homeopaths to "ensure that they do not make any direct or implied claims that homeopathy can treat medical conditions" and asks them to review their marketing communications "including websites and social media pages" to ensure compliance.

In other countries, such as Belgium, homeopathy is not covered. In Austria, the public health service requires scientific proof of effectiveness in order to reimburse medical treatments and homeopathy is listed as not reimbursable, but exceptions can be made; private health insurance policies sometimes include homeopathic treatment. The Swiss government, after a 5-year trial, withdrew coverage of homeopathy and four other complementary treatments in 2005, stating that they did not meet efficacy and cost-effectiveness criteria, but following a referendum in 2009 the five therapies have been reinstated for a further 6-year trial period from 2012.

In Germany, to become a homeopathic physician, one must attend a three-year training programme, while France, Austria and Denmark mandate licences to diagnose any illness or dispense of any product whose purpose is to treat any illness.
The Indian government recognizes homeopathy as one of its national systems of medicine; it has established AYUSH or the Department of Ayurveda, Yoga and Naturopathy, Unani, Siddha and Homoeopathy under the Ministry of Health & Family Welfare. The south Indian state of Kerala also has a cabinet-level AYUSH department. The Central Council of Homoeopathy was established in 1973 to monitor higher education in homeopathy, and the National Institute of Homoeopathy in 1975. A minimum of a recognized diploma in homeopathy and registration on a state register or the Central Register of Homoeopathy is required to practise homeopathy in India.

In 2015, the FDA held a hearing on homeopathic product regulation. Invitees representing the scientific and medical community, and various pro-homeopathy stakeholders, gave testimonials on homeopathic products and the regulatory role played by the FDA. Representatives from the Center for Inquiry (CFI) and the Committee for Skeptical Inquiry (CSI) gave a testimonial which summarized the basis of the organization's objection to homeopathic products, the harm that is done to the general public and proposed regulatory actions: They noted a 2012 report by the American Association of Poison Control Centers which listed 10,311 reported cases of poison exposure related to homeopathic agents, among which 8,788 cases were attributed to children five years of age or younger, as well as examples of harm – including deaths – caused to patients who relied on homeopathics instead of proven medical treatment. 

The United States Federal Trade Commission (FTC) issued an "Enforcement Policy Statement Regarding Marketing Claims for Over-the-Counter Homeopathic Drugs" which specified that the FTC will hold efficacy and safety claims for OTC homeopathic drugs to the same standard as other products making similar claims. In conjunction with the 2016 FTC Enforcement Policy Statement, the FTC also released its "Homeopathic Medicine & Advertising Workshop Report", which summarizes the panel presentations and related public comments in addition to describing consumer research commissioned by the FTC. The report concluded that claims of homeopathy effectiveness "are not accepted by most modern medical experts and do not constitute competent and reliable scientific evidence that these products have the claimed treatment effects." In 2017, the FDA announced it would strengthen regulation of homeopathic products focusing on "situations where homeopathic treatments are being marketed for serious diseases or conditions but have not been shown to offer clinical benefits" and where "products labeled as homeopathic contain potentially harmful ingredients or do not meet current good manufacturing practices."

In Australia the sale of homeopathic products is regulated by the Therapeutic Goods Administration. In 2015, the National Health and Medical Research Council of Australia concluded that there is no reliable evidence that homeopathy is effective and should not be used to treat health conditions that are "chronic, serious, or could become serious". They recommended anyone considering using homeopathy should first get advice from a registered health practitioner. A 2017 review into Pharmacy Remuneration and Regulation recommended that products be banned from pharmacies, while noted the concerns the government did not adopt the recommendation. In New Zealand there are no regulations specific to homeopathy and the New Zealand Medical Association does not oppose the use of homeopathy, a stance that has been called unethical by some doctors.

In the UK about 1000 UK doctors practice homoeopathy, most being general practitioners who prescribe a limited number of remedies. A further 1500 homoeopaths with no medical training are also thought to practice. Over ten thousand German and French doctors use homoeopathy.

The idea of using homeopathy as a treatment for other animals termed "veterinary homeopathy", dates back to the inception of homeopathy; Hahnemann himself wrote and spoke of the use of homeopathy in animals other than humans. The FDA has not approved homeopathic products as veterinary medicine in the U.S. In the UK, veterinary surgeons who use homeopathy may belong to the Faculty of Homeopathy and/or to the British Association of Homeopathic Veterinary Surgeons. Animals may be treated only by qualified veterinary surgeons in the UK and some other countries. Internationally, the body that supports and represents homeopathic veterinarians is the International Association for Veterinary Homeopathy.

Little existing research on the subject is not of a high enough scientific standard to provide reliable data on efficacy. Given that homeopathy's effects in humans appear to be mainly due to the placebo effect and the counseling aspects of the consultation, it is likely that homeopathic treatments would be even less effective in animals. 

Other studies have also found that giving animals placebos can play active roles in influencing pet owners to believe in the effectiveness of the treatment when none exists. The British Veterinary Association's position statement on alternative medicines says that it "cannot endorse" homeopathy, and the Australian Veterinary Association includes it on its list of "ineffective therapies". A 2016 review of peer-reviewed articles from 1981 to 2014 by scientists from the University of Kassel, Germany, concluded that there was insufficient evidence to support the use of homeopathy in livestock as a way to prevent or treat infectious diseases.

The UK's Department for Environment, Food and Rural Affairs (Defra) has adopted a robust position against use of "alternative" pet preparations including homeopathy.

In the April 1997 edition of "FDA Consumer", William T. Jarvis, the President of the National Council Against Health Fraud, said "Homeopathy is a fraud perpetrated on the public with the government's blessing, thanks to the abuse of political power of Sen. Royal S. Copeland [chief sponsor of the 1938 Food, Drug, and Cosmetic Act]."

Mock "overdosing" on homeopathic preparations by individuals or groups in "mass suicides" have become more popular since James Randi began taking entire bottles of homeopathic sleeping pills before giving lectures. In 2010 The Merseyside Skeptics Society from the United Kingdom launched the , encouraging groups to publicly overdose as groups. In 2011 the 10:23 campaign expanded and saw sixty-nine groups participate; fifty-four submitted videos. In April 2012, at the Berkeley SkeptiCal conference, over 100 people participated in a mass overdose, taking "coffea cruda", which is supposed to treat sleeplessness.

In 2011, the non-profit educational organizations Center for Inquiry (CFI) and the associated Committee for Skeptical Inquiry (CSI) have petitioned the U.S. Food and Drug Administration (FDA) to initiate "rulemaking that would require all over-the-counter homeopathic drugs to meet the same standards of effectiveness as non-homeopathic drugs" and "to place warning labels on homeopathic drugs until such time as they are shown to be effective". In a separate petition, CFI and CSI request FDA to issue warning letters to Boiron, maker of Oscillococcinum, regarding their marketing tactic and criticize Boiron for misleading labelling and advertising of Oscillococcinum. In 2015, CFI filed comments urging the Federal Trade Commission to end the false advertising practice of homeopathy. On November 15, 2016, FTC declared that homeopathic products cannot include claims of effectiveness without "competent and reliable scientific evidence". If no such evidence exists, they must state this fact clearly on their labeling, and state that the product's claims are based only on 18th-century theories that have been discarded by modern science. Failure to do so will be considered a violation of the FTC Act.
CFI in Canada is calling for persons that feel they were harmed by homeopathic products to contact them.

In July 2018 the Center for Inquiry filed a lawsuit against CVS for consumer fraud over its sale of homeopathic medicines. According to Nicholas Little, CFI's Vice President and General Counsel:

The filing in part contends that apart from being a waste of money, choosing homeopathic treatments to the exclusion of evidence-based medicines can result in worsened or prolonged symptoms, and in some cases, even death. It also claimed that CVS was selling homeopathic products on an easier-to-obtain basis than standard medication; for example on the CVS website Oscillococcinum could be purchased as a "Flu remedy", whereas the Tylenol brand could only be purchased by visiting a physical pharmacy. In May, 2019, CFI brought a similar lawsuit against Walmart for "committing wide-scale consumer fraud and endangering the health of its customers through its sale and marketing of homeopathic medicines".

In 2019, CFI also conducted a survey to assess the effects on consumer attitudes of having been informed about the lack of scientific evidence for the efficacy of the homeopathic remedies sold by Walmart and CVS. The survey found that "Exposure to the truth about the pseudoscience of homeopathy leads a large percentage of consumers to feel ripped off and deceived by the two largest drug retailers".

In August 2011, a class action lawsuit was filed against Boiron on behalf of "all California residents who purchased Oscillo at any time within the past four years". The lawsuit charged that it "is nothing more than a sugar pill", "despite falsely advertising that it contains an active ingredient known to treat flu symptoms". In March 2012, Boiron agreed to spend up to $12 million to settle the claims of falsely advertising the benefits of its homeopathic preparations.

In July 2012, CBC News reporter Erica Johnson for "Marketplace" conducted an investigation on the homeopathy industry in Canada; her findings were that it is "based on flawed science and some loopy thinking". Center for Inquiry (CFI) Vancouver skeptics participated in a mass overdose outside an emergency room in Vancouver, B.C., taking entire bottles of "medications" that should have made them sleepy, nauseous or dead; after 45 minutes of observation no ill effects were felt. Johnson asked homeopaths and company representatives about cures for cancer and vaccine claims. All reported positive results but none could offer any science backing up their statements, only that "it works". Johnson was unable to find any evidence that homeopathic preparations contain any active ingredient. Analysis performed at the University of Toronto's chemistry department found that the active ingredient is so small "it is equivalent to 5 billion times less than the amount of aspirin ... in a single pellet". Belladonna and ipecac "would be indistinguishable from each other in a blind test".

Homeopathic services offered at Bristol Homeopathic Hospital in the UK ceased in October 2015, partly in response to increased public awareness as a result of the and a campaign led by the Good Thinking Society. University Hospitals Bristol confirmed that it would cease to offer homeopathic therapies from October 2015, at which point homeopathic therapies would no longer be included in the contract. Homeopathic services in the Bristol area were relocated to "a new independent social enterprise" at which Bristol Clinical Commissioning Group revealed "there are currently no (NHS) contracts for homeopathy in place." Following a threat of legal action by the Good Thinking Society campaign group, the British government has stated that the Department of Health will hold a consultation in 2016 regarding whether homeopathic treatments should be added to the NHS treatments blacklist (officially, Schedule 1 of the National Health Service (General Medical Services Contracts) (Prescription of Drugs etc.) Regulations 2004), that specifies a blacklist of medicines not to be prescribed under the NHS.

In March 2016, the University of Barcelona cancelled its master's degree in Homeopathy citing "lack of scientific basis", after advice from the Spanish Ministry of Health stated that "Homeopathy has not definitely proved its efficacy under any indication or concrete clinical situation". Shortly afterwards, in April 2016, the University of Valencia announced the elimination of its Masters in Homeopathy for 2017.

In June 2016, blogger and sceptic Jithin Mohandas launched a petition through Change.org asking the government of Kerala, India, to stop admitting students to homeopathy medical colleges. Mohandas said that government approval of these colleges makes them appear legitimate, leading thousands of talented students to join them and end up with invalid degrees. The petition asks that homeopathy colleges be converted to regular medical colleges and that people with homeopathy degrees be provided with training in scientific medicine.

In Germany, physician and critic of alternative medicine Irmgard Oepen was a relentless critic of homeopathy.




</doc>
<doc id="14231" url="https://en.wikipedia.org/wiki?curid=14231" title="Hairpin">
Hairpin

A hairpin or hair pin is a long device used to hold a person's hair in place. It may be used simply to secure long hair out of the way for convenience or as part of an elaborate hairstyle or coiffure. The earliest evidence for dressing the hair may be seen in carved "venus figurines" such as the Venus of Brassempouy and the Venus of Willendorf. The creation of different hairstyles, especially among women, seems to be common to all cultures and all periods and many past, and current, societies use hairpins.

Hairpins made of metal, ivory, bronze, carved wood, etc. were used in ancient Assyria and Egypt for securing decorated hairstyles. Such hairpins suggest, as graves show, that many were luxury objects among the Egyptians and later the Greeks, Etruscans, and Romans. Major success came in 1901 with the invention of the spiral hairpin by New Zealand inventor Ernest Godward. This was a predecessor of the hair clip.

The hairpin may be decorative and encrusted with jewels and ornaments, or it may be utilitarian, and designed to be almost invisible while holding a hairstyle in place.

Some hairpins are a single straight pin, but modern versions are more likely to be constructed from different lengths of wire that are bent in half with a u-shaped end and a few kinks along the two opposite portions. The finished pin may vary from two to six inches in last length. The length of the wires enables placement in several designs of hairstyles to hold the nature in place. The kinks enable retaining the pin during normal movements.

A hairpin patent was issued to Kelly Chamandy in 1925.

Hairpins (generally known as ; ) are an important symbol in Chinese culture. In ancient China, hairpins were worn by all genders, and they were essential items for everyday hairstyling, mainly for securing and decorating a hair bun. Furthermore, hairpins worn by women could also represent their social status.

In Han Chinese culture, when young girls reached the age of fifteen, they were allowed to take part in a rite of passage known as "" (), or “hairpin initiation” . This ceremony marks the coming of age of young women. Particularly, before the age of fifteen, girls did not use hairpins as they wore their hair in braids, and they were considered as children. When they turned fifteen, they could be considered as young women after the ceremony, and they started to style their hair as buns secured and embellished by hairpins. This practice indicated these young women may now enter into marriage. However, if a young woman hadn't been consented to marriage before age twenty, or she hadn't yet participated in a coming of age ceremony, she must attend a ceremony when she turned twenty.

In comparison with “”, the male equivalent known as “guan li” () or “hat initiation”, usually took place five years later, at the age of twenty. In the 21st century hanfu movement, an attempt to revive the traditional Han Chinese coming-of-age ceremonies has been made, and the ideal age to attend the ceremony is twenty years old for all genders.

While hairpins can symbolize the transition from childhood to adulthood, they were closely connected to the concept of marriage as well. At the time of an engagement, the fiancée may take a hairpin from her hair and give it to her fiancé as a pledge: this can be seen as a reversal of the Western tradition, in which the future groom presents an engagement ring to his betrothed. After the wedding ceremony, the husband should put the hairpin back into his spouse's hair.

Hair has always carried many psychological, philosophical, romantic, and cultural meanings in Chinese culture. In Han culture, people call the union between two people “” (), literally “tying hair”. During the wedding ceremony, some Chinese couples exchange a lock of hair as a pledge, while others break a hairpin into two parts, and then, each of the betrothed take one part with them for keeping. If this couple were ever to get separated in the future, when they reunite, they can piece the two halves together, and the completed hairpin would serve as a proof of their identities as well as a symbol of their reunion. In addition, a married heterosexual couple is sometimes referred to as “” (), an idiom which implies the relationship between the pair is very intimate and happy, just like how their hair has been tied together.



</doc>
<doc id="14233" url="https://en.wikipedia.org/wiki?curid=14233" title="Hate speech">
Hate speech

Hate speech is defined by the Cambridge Dictionary as "public speech that expresses hate or encourages violence towards a person or group based on something such as race, religion, sex, or sexual orientation". Hate speech is "usually thought to include communications of animosity or disparagement of an individual or a group on account of a group characteristic such as race, colour, national origin, sex, disability, religion, or sexual orientation".

There has been much debate over freedom of speech, hate speech and hate speech legislation. The laws of some countries describe hate speech as speech, gestures, conduct, writing, or displays that incite violence or prejudicial actions against a group or individuals on the basis of their membership in the group, or which disparage or intimidate a group or individuals on the basis of their membership in the group. The law may identify a group based on certain characteristics. In some countries, hate speech is not a legal term. Additionally, in some countries, including the United States, much of what falls under the category of "hate speech" is constitutionally protected. In other countries, a victim of hate speech may seek redress under civil law, criminal law, or both.

Laws against hate speech can be divided into two types: those intended to preserve public order and those intended to protect human dignity. 

The laws designed to protect public order require that a higher threshold be violated, so they are not often enforced. For example, in Northern Ireland, as of 1992, only one person has been prosecuted for violating the regulation in 21 years. 

The laws meant to protect human dignity have a much lower threshold for violation, so those in Canada, Denmark, France, Germany and the Netherlands tend to be more frequently enforced.

The global nature of the internet makes it extremely difficult to set limits or boundaries to cyberspace.

The International Covenant on Civil and Political Rights (ICCPR) states that "any advocacy of national, racial or religious hatred that constitutes incitement to discrimination, hostility, or violence shall be prohibited by law". The Convention on the Elimination of All Forms of Racial Discrimination (ICERD) prohibits all incitement to racism. Concerning the debate over how freedom of speech applies to the Internet, conferences concerning such sites have been sponsored by the United Nations High Commissioner for Refugees. "Direct and public incitement to commit genocide" is prohibited by the 1948 Genocide Convention.

On May 31, 2016, Facebook, Google, Microsoft, and Twitter, jointly agreed to a European Union code of conduct obligating them to review "[the] majority of valid notifications for removal of illegal hate speech" posted on their services within 24 hours.

Prior to this in 2013, Facebook, with pressure from over 100 advocacy groups including the Everyday Sexism Project, agreed to change their hate speech policies after data released regarding content that promoted domestic and sexual violence against women led to the withdrawal of advertising by 15 large companies.

Civil liberties activist Nadine Strossen says that, while efforts to censor hate speech have the goal of protecting the most vulnerable, they are ineffective and may have the opposite effect: disadvantaged and ethnic minorities being charged with violating laws against hate speech.

Kim Holmes, Vice President of the conservative Heritage Foundation and a critic of hate speech theory, has argued that it "assumes bad faith on the part of people regardless of their stated intentions" and that it "obliterates the ethical responsibility of the individual".

Rebecca Ruth Gould, a professor of Eurasian and Russian Studies at Harvard University, argues that laws against hate speech constitutes viewpoint discrimination (prohibited by First Amendment jurisprudence in the United States) as the legal system punishes some viewpoints but not others.

Research indicates that when people support censoring hate speech, they are motivated by concerns about the effects the speech has on others than they are about its effects on themselves.

There are also positive benefits to hate speech that are often overlooked. Allowing hate speech provides a more accurate view of the human condition, provides opportunities to change people's minds, and identifies certain people that may need to be avoided in certain circumstances.



</doc>
<doc id="14236" url="https://en.wikipedia.org/wiki?curid=14236" title="Henrik Ibsen">
Henrik Ibsen

Henrik Johan Ibsen (; ; 20 March 1828 – 23 May 1906) was a Norwegian playwright and theatre director. As one of the founders of modernism in theatre, Ibsen is often referred to as "the father of realism" and one of the most influential playwrights of his time. His major works include "Brand", "Peer Gynt", "An Enemy of the People", "Emperor and Galilean", "A Doll's House", "Hedda Gabler", "Ghosts", "The Wild Duck", "When We Dead Awaken", "Rosmersholm", and "The Master Builder". He is the most frequently performed dramatist in the world after Shakespeare, and "A Doll's House" was the world's most performed play in 2006.

Ibsen's early poetic and cinematic play "Peer Gynt" has strong surreal elements. After "Peer Gynt" Ibsen abandoned verse and wrote in realistic prose. Several of his later dramas were considered scandalous to many of his era, when European theatre was expected to model strict morals of family life and propriety. Ibsen's later work examined the realities that lay behind the facades, revealing much that was disquieting to a number of his contemporaries. He had a critical eye and conducted a free inquiry into the conditions of life and issues of morality. In many critics' estimates "The Wild Duck" and "Rosmersholm" are "vying with each other as rivals for the top place among Ibsen's works;" Ibsen himself regarded "Emperor and Galilean" as his masterpiece.

Ibsen is often ranked as one of the most distinguished playwrights in the European tradition. He is widely regarded as the foremost playwright of the nineteenth century. He influenced other playwrights and novelists such as George Bernard Shaw, Oscar Wilde, Arthur Miller, James Joyce, Eugene O'Neill, and Miroslav Krleža. Ibsen was nominated for the Nobel Prize in Literature in 1902, 1903, and 1904.

Ibsen wrote his plays in Danish (the common written language of Denmark and Norway during his lifetime) and they were published by the Danish publisher Gyldendal. Although most of his plays are set in Norway—often in places reminiscent of Skien, the port town where he grew up—Ibsen lived for 27 years in Italy and Germany, and rarely visited Norway during his most productive years. Born into a patrician merchant family, the intertwined Ibsen and Paus family, Ibsen shaped his dramas according to his family background and often modelled characters after family members. He was the father of Prime Minister Sigurd Ibsen. Ibsen's dramas had a strong influence upon contemporary culture.

Ibsen was born into an affluent merchant family in the wealthy port town of Skien in Bratsberg (Telemark). His parents were Knud Ibsen (1797–1877) and Marichen Altenburg (1799–1869). Henrik Ibsen wrote that “my parents were members on both sides of the most respected families in Skien,” explaining that he was closely related with “just about all the patrician families who then dominated the place and its surroundings.”

His parents, though not related by blood, had been raised as something that resembled social siblings. Knud Ibsen's biological father, ship's captain Henrich Ibsen, died at sea when he was newborn in 1797 and his mother married captain Ole Paus the following year; Ole Paus was the brother of Marichen's mother Hedevig Paus, and their families were very close; for example Ole's oldest biological son and Knud's half-brother Henrik Johan Paus was raised in Hedevig's home together with his cousin Marichen, and the biological and social children of the Paus siblings, including Knud and Marichen, spent much of their childhood together. Older Ibsen scholars have claimed that Henrik Ibsen was fascinated by his parents’ “strange, almost incestuous marriage,” and he would treat the subject of incestuous relationships in several plays, notably his masterpiece "Rosmersholm"; on the other hand Jørgen Haave points out that his parents' close relation wasn't that unusual in the Skien elite.

When Henrik Ibsen was around seven years old, his father's fortunes took a significant turn for the worse, and the family was eventually forced to sell the major Altenburg building in central Skien and move permanently to their large summer house, "Venstøp", outside of the city. They were still relatively affluent, had servants and socialised with other members of the Skien elite; their closest neighbours on Southern Venstøp were former ship-owner and mayor Ulrich Frederik Cudrio and his family, who also had been forced to sell their townhouse. Henrik's sister Hedvig would write about their mother: "She was a quiet, lovable woman, the soul of the house, everything to her husband and children. She sacrificed herself time and time again. There was no bitterness or reproach in her." In 1843, after Henrik left home, the Ibsen family moved to a townhouse at Snipetorp, owned by Knud Ibsen's half-brother, wealthy banker and ship-owner Christopher Blom Paus.

Older Ibsen historiography has claimed that his father experienced financial ruin and that this had a strong influence on Ibsen's later work. Haave points out that his father's economic problems in the 1830s were mainly the result of the difficult times and something the Ibsen family had in common with many others of the bourgeoisie; Haave further argues that Henrik Ibsen had a relatively happy and comfortable childhood. Many Ibsen scholars have compared characters and themes in his plays to his family and upbringing; his themes often deal with issues of financial difficulty as well as moral conflicts stemming from dark secrets hidden from society. Ibsen himself confirmed that he both modelled and named characters in his plays after his own family. A central theme in Ibsen's plays is the portrayal of suffering women, echoing his mother Marichen Altenburg according to Templeton, who argues that Ibsen's sympathy with women would eventually find significant expression with their portrayal in dramas such as "A Doll's House" and "Rosmersholm".

At fifteen, Ibsen was forced to leave school. He moved to the small town of Grimstad to become an apprentice pharmacist and began writing plays. In 1846, when Ibsen was 18, he had a liaison with Else Sophie Jensdatter Birkedalen which produced a son, Hans Jacob Hendrichsen Birkdalen, whose upbringing Ibsen paid for until the boy was fourteen, though Ibsen never saw Hans Jacob. Ibsen went to Christiania (later renamed Kristiania and then Oslo) intending to matriculate at the university. He soon rejected the idea (his earlier attempts at entering university were blocked as he did not pass all his entrance exams), preferring to commit himself to writing. His first play, the tragedy "Catilina" (1850), was published under the pseudonym "Brynjolf Bjarme", when he was only 22, but it was not performed. His first play to be staged, "The Burial Mound" (1850), received little attention. Still, Ibsen was determined to be a playwright, although the numerous plays he wrote in the following years remained unsuccessful. Ibsen's main inspiration in the early period, right up to "Peer Gynt", was apparently the Norwegian author Henrik Wergeland and the Norwegian folk tales as collected by Peter Christen Asbjørnsen and Jørgen Moe. In Ibsen's youth, Wergeland was the most acclaimed, and by far the most read, Norwegian poet and playwright.

He spent the next several years employed at Det norske Theater (Bergen), where he was involved in the production of more than 145 plays as a writer, director, and producer. During this period, he published five new, though largely unremarkable, plays. Despite Ibsen's failure to achieve success as a playwright, he gained a great deal of practical experience at the Norwegian Theater, experience that was to prove valuable when he continued writing.

Ibsen returned to Christiania in 1858 to become the creative director of the Christiania Theatre. He married Suzannah Thoresen on 18 June 1858 and she gave birth to their only child Sigurd on 23 December 1859. The couple lived in very poor financial circumstances and Ibsen became very disenchanted with life in Norway. In 1864, he left Christiania and went to Sorrento in Italy in self-imposed exile. He didn't return to his native land for the next 27 years, and when he returned to it he was a noted, but controversial, playwright.

His next play, "Brand" (1865), brought him the critical acclaim he sought, along with a measure of financial success, as did the following play, "Peer Gynt" (1867), to which Edvard Grieg famously composed incidental music and songs. Although Ibsen read excerpts of the Danish philosopher Søren Kierkegaard and traces of the latter's influence are evident in "Brand", it was not until after "Brand" that Ibsen came to take Kierkegaard seriously. Initially annoyed with his friend Georg Brandes for comparing Brand to Kierkegaard, Ibsen nevertheless read "Either/Or" and "Fear and Trembling". Ibsen's next play "Peer Gynt" was consciously informed by Kierkegaard.

With success, Ibsen became more confident and began to introduce more and more of his own beliefs and judgements into the drama, exploring what he termed the "drama of ideas". His next series of plays are often considered his Golden Age, when he entered the height of his power and influence, becoming the center of dramatic controversy across Europe.

Ibsen moved from Italy to Dresden, Germany, in 1868, where he spent years writing the play he regarded as his main work, "Emperor and Galilean" (1873), dramatizing the life and times of the Roman emperor Julian the Apostate. Although Ibsen himself always looked back on this play as the cornerstone of his entire works, very few shared his opinion, and his next works would be much more acclaimed. Ibsen moved to Munich in 1875 and began work on his first contemporary realist drama "The Pillars of Society", first published and performed in 1877. "A Doll's House" followed in 1879. This play is a scathing criticism of the marital roles accepted by men and women which characterized Ibsen's society.

Ibsen was already in his fifties when "A Doll’s House" was published. He himself saw his latter plays as a series. At the end of his career, he described them as “that series of dramas which began with "A Doll’s House" and which is now completed with "When We Dead Awaken"”. Furthermore, it was the reception of "A Doll’s House" which brought Ibsen international acclaim.

"Ghosts" followed in 1881, another scathing commentary on the morality of Ibsen's society, in which a widow reveals to her pastor that she had hidden the evils of her marriage for its duration. The pastor had advised her to marry her fiancé despite his philandering, and she did so in the belief that her love would reform him. But his philandering continued right up until his death, and his vices are passed on to their son in the form of syphilis. The mention of venereal disease alone was scandalous, but to show how it could poison a respectable family was considered intolerable.

In "An Enemy of the People" (1882), Ibsen went even further. In earlier plays, controversial elements were important and even pivotal components of the action, but they were on the small scale of individual households. In "An Enemy", controversy became the primary focus, and the antagonist was the entire community. One primary message of the play is that the individual, who stands alone, is more often "right" than the mass of people, who are portrayed as ignorant and sheeplike. Contemporary society's belief was that the community was a noble institution that could be trusted, a notion Ibsen challenged. In "An Enemy of the People", Ibsen chastised not only the conservatism of society, but also the liberalism of the time. He illustrated how people on both sides of the social spectrum could be equally self-serving. "An Enemy of the People" was written as a response to the people who had rejected his previous work, "Ghosts". The plot of the play is a veiled look at the way people reacted to the plot of "Ghosts". The protagonist is a physician in a vacation spot whose primary draw is a public bath. The doctor discovers that the water is contaminated by the local tannery. He expects to be acclaimed for saving the town from the nightmare of infecting visitors with disease, but instead he is declared an 'enemy of the people' by the locals, who band against him and even throw stones through his windows. The play ends with his complete ostracism. It is obvious to the reader that disaster is in store for the town as well as for the doctor.

As audiences by now expected, Ibsen's next play again attacked entrenched beliefs and assumptions; but this time, his attack was not against society's mores, but against overeager reformers and their idealism. Always an iconoclast, Ibsen saw himself as an objective observer of society, “like a lone franc tireur in the outposts”, playing a lone hand, as he put it. Ibsen, perhaps more than any of his contemporaries, relied upon immediate sources such as newspapers and second-hand report for his contact with intellectual thought. He claimed to be ignorant of books, leaving them to his wife and son, but, as Georg Brandes described, “he seemed to stand in some mysterious correspondence with the fermenting, germinating ideas of the day.

"The Wild Duck" (1884) is by many considered Ibsen's finest work, and it is certainly the most complex. It tells the story of Gregers Werle, a young man who returns to his hometown after an extended exile and is reunited with his boyhood friend Hjalmar Ekdal. Over the course of the play, the many secrets that lie behind the Ekdals' apparently happy home are revealed to Gregers, who insists on pursuing the absolute truth, or the "Summons of the Ideal". Among these truths: Gregers' father impregnated his servant Gina, then married her off to Hjalmar to legitimize the child. Another man has been disgraced and imprisoned for a crime the elder Werle committed. Furthermore, while Hjalmar spends his days working on a wholly imaginary "invention", his wife is earning the household income.

Ibsen displays masterful use of irony: despite his dogmatic insistence on truth, Gregers never says what he thinks but only insinuates, and is never understood until the play reaches its climax. Gregers hammers away at Hjalmar through innuendo and coded phrases until he realizes the truth; Gina's daughter, Hedvig, is not his child. Blinded by Gregers' insistence on absolute truth, he disavows the child. Seeing the damage he has wrought, Gregers determines to repair things, and suggests to Hedvig that she sacrifice the wild duck, her wounded pet, to prove her love for Hjalmar. Hedvig, alone among the characters, recognizes that Gregers always speaks in code, and looking for the deeper meaning in the first important statement Gregers makes which does not contain one, kills herself rather than the duck in order to prove her love for him in the ultimate act of self-sacrifice. Only too late do Hjalmar and Gregers realize that the absolute truth of the "ideal" is sometimes too much for the human heart to bear.

Late in his career, Ibsen turned to a more introspective drama that had much less to do with denunciations of society's moral values and more to do with the problems of individuals. In such later plays as "Hedda Gabler" (1890) and "The Master Builder" (1892), Ibsen explored psychological conflicts that transcended a simple rejection of current conventions. Many modern readers, who might regard anti-Victorian didacticism as dated, simplistic or hackneyed, have found these later works to be of absorbing interest for their hard-edged, objective consideration of interpersonal confrontation. "Hedda Gabler" and "A Doll’s House" are regularly cited as Ibsen's most popular and influential plays, with the title role of Hedda regarded as one of the most challenging and rewarding for an actress even in the present day.

Ibsen had completely rewritten the rules of drama with a realism which was to be adopted by Chekhov and others and which we see in the theatre to this day. From Ibsen forward, challenging assumptions and directly speaking about issues has been considered one of the factors that makes a play art rather than entertainment. His works were brought to an English-speaking audience, largely thanks to the efforts of William Archer and Edmund Gosse. These in turn had a profound influence on the young James Joyce who venerates him in his early autobiographical novel "Stephen Hero". Ibsen returned to Norway in 1891, but it was in many ways not the Norway he had left. Indeed, he had played a major role in the changes that had happened across society. Modernism was on the rise, not only in the theatre, but across public life.. Michael Meyer's translations in the 1950s were welcomed by actors and directors as playable, rather than academic. As The Times newspaper put it, "‘This, one may think, is how Ibsen might have expressed himself in English'."

Ibsen intentionally obscured his influences. However, asked later what he had read when he wrote Catiline, Ibsen replied that he had read only the Danish Norse saga-inspired Romantic tragedian Adam Oehlenschläger and Ludvig Holberg, "the Scandinavian Molière".

At the time when Ibsen was writing, literature was emerging as a formidable force in 19th century society. It was still a relatively new form of popular discussion and entertainment . With the vast increase in literacy towards the end of the century, the possibilities of literature being used for subversion struck horror into the heart of the Establishment. Ibsen's plays, from "A Doll’s House" onwards, caused an uproar: not just in Norway, but throughout Europe, and even across the Atlantic in America. No other artist, apart from Richard Wagner, had such an effect internationally, inspiring almost blasphemous adoration and hysterical abuse.

After the publication of "Ghosts", he wrote: “while the storm lasted, I have made many studies and observations and I shall not hesitate to exploit them in my future writings.” Indeed, his next play "An Enemy of the People" was initially regard by the critics to be simply his response to the violent criticism which had greeted "Ghosts". Ibsen expected criticism: as he wrote to his publisher: “"Ghosts" will probably cause alarm in some circles, but it can’t be helped. If it did not, there would have been no necessity for me to have written it.”

Ibsen didn't just read the critical reaction to his plays, he actively corresponded with critics, publishers, theatre directors and newspaper editors on the subject. The interpretation of his work, both by critics and directors, concerned him greatly. He often advised directors on which actor or actress would be suitable for a particular role. [An example of this is a letter he wrote to Hans Schroder in November 1884, with detailed instructions for the production of "The Wild Duck".]

Ibsen's plays initially reached a far wider audience as read plays rather than in performance. It was 20 years, for instance, before the authorities would allow "Ghosts" to be performed in Norway. Each new play that Ibsen wrote, from 1879 onwards, had an explosive effect on intellectual circles. This was greatest for "A Doll’s House" and "Ghosts", and it did lessen with the later plays, but the translation of Ibsen's works into German, French and English during the decade following the initial publication of each play and frequent new productions as and when permission was granted, meant that Ibsen remained a topic of lively conversation throughout the latter decades of the 19th century. When "A Doll’s House" was published, it had an explosive effect: it was the centre of every conversation at every social gathering in Christiana. One hostess even wrote on the invitations to her soirée, “You are politely requested not to mention Mr Ibsen’s new play”.

On 23 May 1906, Ibsen died in his home at Arbins gade 1 in Kristiania (now Oslo) after a series of strokes in March 1900. When, on 22 May, his nurse assured a visitor that he was a little better, Ibsen spluttered his last words "On the contrary" ("Tvertimod!"). He died the following day at 2:30 pm.

Ibsen was buried in Vår Frelsers gravlund ("The Graveyard of Our Savior") in central Oslo.

The 100th anniversary of Ibsen's death in 2006 was commemorated with an "Ibsen year" in Norway and other countries. In 2006, the homebuilding company Selvaag also opened "Peer Gynt" Sculpture Park in Oslo, Norway, in Henrik Ibsen's honour, making it possible to follow the dramatic play "Peer Gynt" scene by scene. Will Eno's adaptation of Ibsen's "Peer Gynt", titled "Gnit", had its world premiere at the 37th Humana Festival of New American Plays in March 2013.

On 23 May 2006, The Ibsen Museum in Oslo re-opened, to the public, the house where Ibsen had spent his last eleven years, completely restored with the original interior, colours, and decor.

The social questions which concerned Ibsen belonged unequivocally to the 19th century. From a modern perspective, the aspects of his writing that appeal most are the psychological issues which he explored. The social issues, taken up so prominently in his own day, have become dated, as has the late-Victorian middle-class setting of his plays. The fact that, whether read and staged, they still possess a compelling power is testament to his enduring quality as a thinker and a dramatist.

On the occasion of the 100th anniversary of Ibsen's death in 2006, the Norwegian government organised the Ibsen Year, which included celebrations around the world. The NRK produced a miniseries on Ibsen's childhood and youth in 2006, "An Immortal Man". Several prizes are awarded in the name of Henrik Ibsen, among them the International Ibsen Award, the Norwegian Ibsen Award and the Ibsen Centennial Commemoration Award.

Every year, since 2008, the annual "Delhi Ibsen Festival", is held in Delhi, India, organized by the Dramatic Art and Design Academy (DADA) in collaboration with The Royal Norwegian Embassy in India. It features plays by Ibsen, performed by artists from various parts of the world in varied languages and styles.

The Ibsen Society of America (ISA) was founded in 1978 at the close of the Ibsen Sesquicentennial Symposium held in New York City to mark the 150th anniversary of Henrik Ibsen's birth. Distinguished Ibsen translator and critic Rolf Fjelde, Professor of Literature at Pratt Institute and the chief organizer of the Symposium, was elected Founding President. In December 1979, the ISA was certified as a non-profit corporation under the laws of the State of New York. Its purpose is to foster through lectures, readings, performances, conferences, and publications an understanding of Ibsen's works as they are interpreted as texts and produced on stage and in film and other media. An annual newsletter "Ibsen News and Comment" is distributed to all members.

Ibsen's ancestry has been a much studied subject, due to his perceived foreignness and due to the influence of his biography and family on his plays. Ibsen often made references to his family in his plays, sometimes by name, or by modelling characters after them.

The oldest documented member of the Ibsen family was ship's captain Rasmus Ibsen (1632–1703) from Stege, Denmark. His son, ship's captain Peder Ibsen became a burgher of Bergen in Norway in 1726. Henrik Ibsen had Danish, German, Norwegian and some distant Scottish ancestry. Most of his ancestors belonged to the merchant class of original Danish and German extraction, and many of his ancestors were ship's captains.

Ibsen's biographer Henrik Jæger famously wrote in 1888 that Ibsen did not have a drop of Norwegian blood in his veins, stating that "the ancestral Ibsen was a Dane". This, however, is not completely accurate; notably through his grandmother Hedevig Paus, Ibsen was descended from one of the very few families of the patrician class of original Norwegian extraction, known since the 15th century. Ibsen's ancestors had mostly lived in Norway for several generations, even though many had foreign ancestry.

The name Ibsen is originally a patronymic, meaning "son of Ib" (Ib is a Danish variant of Jacob). The patronymic became "frozen", i.e. it became a permanent family name, in the 17th century. The phenomenon of patronymics becoming frozen started in the 17th century in bourgeois families in Denmark, and the practice was only widely adopted in Norway from around 1900.

From his marriage with Suzannah Thoresen, Ibsen had one son, lawyer and government minister Sigurd Ibsen. Sigurd Ibsen married Bergljot Bjørnson, the daughter of Bjørnstjerne Bjørnson. Their son was Tancred Ibsen, who became a film director and was married to Lillebil Ibsen; their only child was diplomat Tancred Ibsen, Jr. Sigurd Ibsen's daughter, Irene Ibsen, married Josias Bille, a member of the Danish ancient noble Bille family; their son was Danish actor Joen Bille.

Ibsen was decorated Knight in 1873, Commander in 1892, and with the Grand Cross of the Order of St. Olav in 1893. He received the Grand Cross of the Danish Order of the Dannebrog, and the Grand Cross of the Swedish Order of the Polar Star, and was Knight, First Class of the Order of Vasa.

Well known stage directors in Austria and Germany as Theodor Lobe (1833–1905), Paul Barnay (1884–1960), Max Burckhard (1854–1912), Otto Brahm (1956–1912), Carl Heine (1861–1927), Paul Albert Glaeser-Wilken (1874–1942), Victor Barnowsky (1875–1952), Eugen Robert (1877–1944), Leopold Jessner (1878–1945), Ludwig Barnay (1884–1960), Alfred Rotter (1886–1933), Fritz Rotter (1888–1939), (1900–1973) and Peter Zadek (1926–2009) performed the work of Ibsen.

In 1995, the asteroid 5696 Ibsen was named in his memory.

Plays entirely or partly in verse are marked .


The authoritative translation in the English language for Ibsen remains the 1928 ten-volume version of the "Complete Works of Henrik Ibsen" from Oxford University Press. Many other translations of individual plays by Ibsen have appeared since 1928 though none have purported to be a new version of the complete works of Ibsen.






</doc>
<doc id="14240" url="https://en.wikipedia.org/wiki?curid=14240" title="Hawaiian language">
Hawaiian language

Hawaiian (Hawaiian: ", ) is a Polynesian language that takes its name from Hawaii, the largest island in the tropical North Pacific archipelago where it developed. Hawaiian, along with English, is an official language of the State of Hawaii. King Kamehameha III established the first Hawaiian-language constitution in 1839 and 1840.

For various reasons, including territorial legislation establishing English as the official language in schools, the number of native speakers of Hawaiian gradually decreased during the period from the 1830s to the 1950s. Hawaiian was essentially displaced by English on six of seven inhabited islands. In 2001, native speakers of Hawaiian amounted to less than 0.1% of the statewide population. Linguists were unsure if Hawaiian and other endangered languages would survive.

Nevertheless, from around 1949 to the present day, there has been a gradual increase in attention to and promotion of the language. Public Hawaiian-language immersion preschools called Pūnana Leo were established in 1984; other immersion schools followed soon after that. The first students to start in immersion preschool have now graduated from college and many are fluent Hawaiian speakers. The federal government has acknowledged this development. For example, the Hawaiian National Park Language Correction Act of 2000 changed the names of several national parks in Hawaii, observing the Hawaiian spelling. However, the language is still classified as critically endangered by UNESCO.

A creole language, Hawaiian Pidgin (or Hawaii Creole English, HCE), is more commonly spoken in Hawaii than Hawaiian. Some linguists, as well as many locals, argue that Hawaiian Pidgin is a dialect of American English.

The Hawaiian alphabet has 13 letters: five vowels: a e i o u (each with a long pronunciation and a short one) and eight consonants: he ke la mu nu pi we, including a glottal stop called okina.

The Hawaiian language takes its name from the largest island in the Hawaiian state, Hawaii (" in the Hawaiian language). The island name was first written in English in 1778 by British explorer James Cook and his crew members. They wrote it as "Owhyhee" or "Owhyee". Explorers Mortimer (1791) and Otto von Kotzebue (1821) used that spelling.

The initial "O" in the name is a reflection of the fact that Hawaiian predicates unique identity by using a copula form, "o", immediately before a proper noun. Thus, in Hawaiian, the name of the island is expressed by saying "", which means "[This] is Hawaii." The Cook expedition also wrote "Otaheite" rather than "Tahiti."

The spelling "why" in the name reflects the pronunciation of "wh" in 18th-century English (still used in parts of the English-speaking world). "Why" was pronounced . The spelling "hee" or "ee" in the name represents the sounds , or .

Putting the parts together, "O-why-(h)ee" reflects , a reasonable approximation of the native pronunciation, .

American missionaries bound for Hawaii used the phrases "Owhihe Language" and "Owhyhee language" in Boston prior to their departure in October 1819 and during their five-month voyage to Hawaii. They still used such phrases as late as March 1822. However, by July 1823, they had begun using the phrase "Hawaiian Language."

In Hawaiian, "" means "language: Hawaiian", since adjectives follow nouns.

Hawaiian is a Polynesian member of the Austronesian language family. It is closely related to other Polynesian languages, such as Samoan, Marquesan, Tahitian, Māori, Rapa Nui (the language of Easter Island) and Tongan.

According to Schütz (1994), the Marquesans colonized the archipelago in roughly 300 CE followed by later waves of immigration from the Society Islands and Samoa-Tonga. Their languages, over time, became the Hawaiian language within the Hawaiian Islands. Kimura and Wilson (1983) also state: Linguists agree that Hawaiian is closely related to Eastern Polynesian, with a particularly strong link in the Southern Marquesas, and a secondary link in Tahiti, which may be explained by voyaging between the Hawaiian and Society Islands.

The genetic history of the Hawaiian language is demonstrated primarily through the application of lexicostatistics, which involves quantitative comparison of lexical cognates, and the comparative method. Both the number of cognates and the phonological similarity of cognates are measures of language relationship.

The following table provides a limited lexicostatistical data set for ten numbers. The asterisk (*) is used to show that these are hypothetical, reconstructed forms. In the table, the year date of the modern forms is rounded off to 2000 CE to emphasize the 6000-year time lapse since the PAN era.

Note: For the number "10", the Tongan form in the table is part of the word ('ten'). The Hawaiian cognate is part of the word ('ten days'); however, the more common word for "10" used in counting and quantifying is , a different root.

Application of the lexicostatistical method to the data in the table will show the four languages to be related to one another, with Tagalog having 100% cognacy with PAN, while Hawaiian and Tongan have 100% cognacy with each other, but 90% with Tagalog and PAN. This is because the forms for each number are cognates, except the Hawaiian and Tongan words for the number "1", which are cognate with each other, but not with Tagalog and PAN. When the full set of 200 meanings is used, the percentages will be much lower. For example, Elbert found Hawaiian and Tongan to have 49% (98 ÷ 200) shared cognacy. This points out the importance of data-set size for this method, where less data leads to cruder results, while more data leads to better results.

Application of the comparative method will show partly different genetic relationships. It will point out sound changes, such as:
This method will recognize sound change #1 as a shared innovation of Hawaiian and Tongan. It will also take the Hawaiian and Tongan cognates for "1" as another shared innovation. Due to these exclusively shared features, Hawaiian and Tongan are found to be more closely related to one another than either is to Tagalog or Amis.

The forms in the table show that the Austronesian vowels tend to be relatively stable, while the consonants are relatively volatile. It is also apparent that the Hawaiian words for "3", "5", and "8" have remained essentially unchanged for 6000 years.

In 1778, British explorer James Cook made Europe's initial, recorded first contact with Hawaiʻi, beginning a new phase in the development of Hawaiian. During the next forty years, the sounds of Spanish (1789), Russian (1804), French (1816), and German (1816) arrived in Hawaii via other explorers and businessmen. Hawaiian began to be written for the first time, largely restricted to isolated names and words, and word lists collected by explorers and travelers.

The early explorers and merchants who first brought European languages to the Hawaiian islands also took on a few native crew members who brought the Hawaiian language into new territory. Hawaiians took these nautical jobs because their traditional way of life changed due to plantations, and although there were not enough of these Hawaiian-speaking explorers to establish any viable speech communities abroad, they still had a noticeable presence. One of them, a boy in his teens known as Obookiah (), had a major impact on the future of the language. He sailed to New England, where he eventually became a student at the Foreign Mission School in Cornwall, Connecticut. He inspired New Englanders to support a Christian mission to Hawaii, and provided information on the Hawaiian language to the American missionaries there prior to their departure for Hawaii in 1819.

Like all natural spoken languages, the Hawaiian language was originally just an oral language. The native people of the Hawaiian language relayed religion, traditions, history, and views of their world through stories that were handed down from generation to generation. One form of storytelling most commonly associated with the Hawaiian islands is hula. Nathaniel B. Emerson notes that "It kept the communal imagination in living touch with the nation's legendary past".

The islanders' connection with their stories is argued to be one reason why Captain James Cook received a pleasant welcome. Marshall Sahlins has observed that Hawaiian folktales began bearing similar content to those of the Western world in the eighteenth century. He argues this was caused by the timing of Captain Cook's arrival, which was coincidentally when the indigenous Hawaiians were celebrating the Makahiki festival. The islanders' story foretold of the god Lono's return at the time of the Makahiki festival.

In 1820, Protestant missionaries from New England arrived in Hawaii.

Adelbert von Chamisso might have consulted with a native speaker of Hawaiian in Berlin, Germany, before publishing his grammar of Hawaiian ("") in 1837. When Hawaiian King David Kalākaua took a trip around the world, he brought his native language with him. When his wife, Queen Kapiolani, and his sister, Princess (later Queen) Liliuokalani, took a trip across North America and on to the British Islands, in 1887, Liliuokalani's composition "Aloha Oe" was already a famous song in the U.S.
In 1834, the first Hawaiian-language newspapers were published by missionaries working with locals. The missionaries also played a significant role in publishing a vocabulary (1836) grammar (1854) and dictionary (1865) of Hawaiian. Literacy in Hawaiian was widespread among the local population, especially ethnic Hawaiians. Use of the language among the general population might have peaked around 1881. Even so, some people worried, as early as 1854, that the language was "soon destined to extinction."

The decline of the Hawaiian language was accelerated by the coup that overthrew the Hawaiian monarchy and dethroned the existing Hawaiian queen. Thereafter, a law was instituted that required English as the main language of school instruction. The law cited is identified as Act 57, sec. 30 of the 1896 Laws of the Republic of Hawaii:

This law established English as the medium of instruction for the government-recognized schools both "public and private". While it did not ban or make illegal the Hawaiian language in other contexts, its implementation in the schools had far-reaching effects. Those who had been pushing for English-only schools took this law as licence to extinguish the native language at the early education level. While the law didn't make Hawaiian illegal (it was still commonly spoken at the time), many children who spoke Hawaiian at school, including on the playground, were disciplined. This included corporal punishment and going to the home of the offending child to advise them strongly to stop speaking it in their home. Moreover, the law specifically provided for teaching languages "in addition to the English language," reducing Hawaiian to the status of an extra language, subject to approval by the Department. Hawaiian was not taught initially in any school, including the all-Hawaiian Kamehameha Schools. This is largely because when these schools were founded, like Kamehameha Schools founded in 1887 (nine years before this law), Hawaiian was being spoken in the home. Once this law was enacted, individuals at these institutions took it upon themselves to enforce a ban on Hawaiian. Beginning in 1900, Mary Kawena Pukui, who was later the co-author of the Hawaiian–English Dictionary, was punished for speaking Hawaiian by being rapped on the forehead, allowed to eat only bread and water for lunch, and denied home visits on holidays. Winona Beamer was expelled from Kamehameha Schools in 1937 for chanting Hawaiian.

In 1949, the legislature of the Territory of Hawaii commissioned Mary Pukui and Samuel Elbert to write a new dictionary of Hawaiian, either revising the Andrews-Parker work or starting from scratch. Pukui and Elbert took a middle course, using what they could from the Andrews dictionary, but making certain improvements and additions that were more significant than a minor revision. The dictionary they produced, in 1957, introduced an era of gradual increase in attention to the language and culture.

Efforts to promote the language have increased in recent decades. Hawaiian-language "immersion" schools are now open to children whose families want to reintroduce Hawaiian language for future generations. The Aha Pūnana Leo’s Hawaiian language preschools in Hilo, Hawaii, have received international recognition. The local National Public Radio station features a short segment titled "Hawaiian word of the day" and a Hawaiian language news broadcast. Honolulu television station KGMB ran a weekly Hawaiian language program, "Āhai Ōlelo Ola", as recently as 2010. Additionally, the Sunday editions of the "Honolulu Star-Advertiser", the largest newspaper in Hawaii, feature a brief article called "Kauakukalahale" written entirely in Hawaiian by teachers, students, and community members.

Today, the number of native speakers of Hawaiian, which was under 0.1% of the statewide population in 1997, has risen to 2,000, out of 24,000 total who are fluent in the language, according to the US 2011 census. On six of the seven permanently inhabited islands, Hawaiian has been largely displaced by English, but on Niihau, native speakers of Hawaiian have remained fairly isolated and have continued to use Hawaiian almost exclusively.

The isolated island of Niʻihau, located off the southwest coast of Kauai, is the one island where Hawaiian (more specifically a local dialect of Hawaiian known as Niihau dialect) is still spoken as the language of daily life. 

Hawaiians had no written language prior to Western contact, except for petroglyph symbols.
The modern Hawaiian alphabet, "ka pīāpā Hawaii", is based on the Latin script. Hawaiian words end "only" in vowels, and every consonant must be followed by a vowel. The Hawaiian alphabetical order has all of the vowels before the consonants, as in the following chart.
This writing system was developed by American Protestant missionaries during 1820–1826. It was the first thing they ever printed in Hawaii, on January 7, 1822, and it originally included the consonants "B, D, R, T," and "V," in addition to the current ones ("H, K, L, M, N, P, W"), and it had "F, G, S, Y" and "Z" for "spelling foreign words". The initial printing also showed the five vowel letters ("A, E, I, O, U") and seven of the short diphthongs ("AE, AI, AO, AU, EI, EU, OU").

In 1826, the developers voted to eliminate some of the letters which represented functionally redundant allophones (called "interchangeable letters"), enabling the Hawaiian alphabet to approach the ideal state of one-symbol-one-phoneme, and thereby optimizing the ease with which people could teach and learn the reading and writing of Hawaiian. For example, instead of spelling one and the same word as "pule, bule, pure," and "bure" (because of interchangeable "p/b" and "l/r"), the word is spelled only as "pule".

However, hundreds of words were very rapidly borrowed into Hawaiian from English, Greek, Hebrew, Latin, and Syriac. Although these loan words were necessarily Hawaiianized, they often retained some of their "non-Hawaiian letters" in their published forms. For example, "Brazil" fully Hawaiianized is "Palakila", but retaining "foreign letters" it is "Barazila". Another example is "Gibraltar", written as "Kipalaleka" or "Gibaraleta". While and are not regarded as Hawaiian sounds, , , and were represented in the original alphabet, so the letters ("b", "r", and "t") for the latter are not truly "non-Hawaiian" or "foreign", even though their post-1826 use in published matter generally marked words of foreign origin.

"ʻOkina" ("oki" 'cut' + "-na" '-ing') is the modern Hawaiian name for the symbol (a letter) that represents the glottal stop. It was formerly known as "uina" ("snap").

For examples of the okina, consider the Hawaiian words "Hawaii" and "Oahu" (often simply "Hawaii" and "Oahu" in English orthography). In Hawaiian, these words are pronounced and , and are written with an okina where the glottal stop is pronounced.

Elbert & Pukui's "Hawaiian Grammar" says "The glottal stop, ‘, is made by closing the glottis or space between the vocal cords, the result being something like the hiatus in English "oh-oh"."

As early as 1823, the missionaries made some limited use of the apostrophe to represent the glottal stop, but they did not make it a letter of the alphabet. In publishing the Hawaiian Bible, they used it to distinguish "kou" ('my') from "kou" ('your'). In 1864, William DeWitt Alexander published a grammar of Hawaiian in which he made it clear that the glottal stop (calling it "guttural break") is definitely a true consonant of the Hawaiian language. He wrote it using an apostrophe. In 1922, the Andrews-Parker dictionary of Hawaiian made limited use of the opening single quote symbol, then called "reversed apostrophe" or "inverse comma", to represent the glottal stop. Subsequent dictionaries and written material associated with the Hawaiian language revitalization have preferred to use this symbol, the "okina", to better represent spoken Hawaiian. Nonetheless, excluding the "okina" may facilitate interface with English-oriented media, or even be preferred stylistically by some Hawaiian speakers, in homage to 19th century written texts. So there is variation today in the use of this symbol.

The okina is written in various ways for electronic uses:

Because many people who want to write the ʻokina are not familiar with these specific characters and/or do not have access to the appropriate fonts and input and display systems, it is sometimes written with more familiar and readily available characters:

A modern Hawaiian name for the macron symbol is "kahakō" ("kaha" 'mark' + "kō" 'long'). It was formerly known as "mekona" (Hawaiianization of "macron"). It can be written as a diacritical mark which looks like a hyphen or dash written above a vowel, i.e., "ā ē ī ō ū" and "Ā Ē Ī Ō Ū". It is used to show that the marked vowel is a "double", or "geminate", or "long" vowel, in phonological terms. (See: Vowel length)

As early as 1821, at least one of the missionaries, Hiram Bingham, was using macrons (and breves) in making handwritten transcriptions of Hawaiian vowels. The missionaries specifically requested their sponsor in Boston to send them some type (fonts) with accented vowel characters, including vowels with macrons, but the sponsor made only one response and sent the wrong font size (pica instead of small pica). Thus, they could not print ā, ē, ī, ō, nor ū (at the right size), even though they wanted to.

Due to extensive allophony, Hawaiian has more than 13 phones. Although vowel length is phonemic, long vowels are not always pronounced as such, even though under the rules for assigning stress in Hawaiian, a long vowel will always receive stress.

Hawaiian is known for having very few consonant phonemes – eight: . It is notable that Hawaiian has allophonic variation of with , with , and (in some dialects) with . The – variation is quite unusual among the world's languages, and is likely a product both of the small number of consonants in Hawaiian, and the recent shift of historical *t to modern –, after historical *k had shifted to . In some dialects, remains as in some words. These variations are largely free, though there are conditioning factors. tends to especially in words with both and , such as in the island name "Lānai" (–), though this is not always the case: "eleele" or "eneene" "black". The allophone is almost universal at the beginnings of words, whereas is most common before the vowel . is also the norm after and , whereas is usual after and . After and initially, however, and are in free variation.

Hawaiian has five short and five long vowels, plus diphthongs.

Hawaiian has five pure vowels. The short vowels are , and the long vowels, if they are considered separate phonemes rather than simply sequences of like vowels, are . When stressed, short and have been described as becoming and , while when unstressed they are and . Parker Jones (2017), however, did not find a reduction of /a/ to in the phonetic analysis of a young speaker from Hilo, Hawaiʻi; so there is at least some variation in how /a/ is realised. also tends to become next to , , and another , as in "Pele" . Some grammatical particles vary between short and long vowels. These include "a" and "o" "of", "ma" "at", "na" and "no" "for". Between a back vowel or and a following non-back vowel (), there is an epenthetic , which is generally not written. Between a front vowel or and a following non-front vowel (), there is an epenthetic (a "y" sound), which is never written.

The short-vowel diphthongs are . In all except perhaps , these are falling diphthongs. However, they are not as tightly bound as the diphthongs of English, and may be considered vowel sequences. (The second vowel in such sequences may receive the stress, but in such cases it is not counted as a diphthong.) In fast speech, tends to and tends to , conflating these diphthongs with and .

There are only a limited number of vowels which may follow long vowels, and some authors treat these sequences as diphthongs as well: .

Hawaiian syllable structure is (C)V. All CV syllables occur except for "wū"; "wu" occurs only in two words borrowed from English. As shown by Schütz, Hawaiian word-stress is predictable in words of one to four syllables, but not in words of five or more syllables. Hawaiian phonological processes include palatalization and deletion of consonants, as well as raising, diphthongization, deletion, and compensatory lengthening of vowels.

Historically, glottal stop developed from *k. Loss of intervocalic consonant phonemes has resulted in Hawaiian long vowels and diphthongs.

Hawaiian is an analytic language with verb–subject–object word order. While there is no use of inflection for verbs, in Hawaiian, like other Austronesian personal pronouns, declension is found in the differentiation between a- and o-class genitive case personal pronouns in order to indicate inalienable possession in a binary possessive class system. Also like many Austronesian languages, Hawaiian pronouns employ separate words for inclusive and exclusive we (clusivity), and distinguish singular, dual, and plural. The grammatical function of verbs is marked by adjacent particles (short words) and by their relative positions, that indicate tense–aspect–mood.

Some examples of verb phrase patterns:

Nouns can be marked with articles:

"ka" and "ke" are singular definite articles. "ke" is used before words beginning with a-, e-, o- and k-, and with some words beginning - and p-. "ka" is used in all other cases. "nā" is the plural definite article.

To show part of a group, the word "kekahi" is used. To show a bigger part, "mau" is inserted to pluralize the subject.

Examples:





</doc>
<doc id="14245" url="https://en.wikipedia.org/wiki?curid=14245" title="Second Polish Republic">
Second Polish Republic

The Second Polish Republic, commonly known as interwar Poland, refers to the country of Poland in the period between the two World Wars (1918–1939). Officially known as the Republic of Poland (), the Polish state was re-established in 1918, in the aftermath of World War I. The Second Republic ceased to exist in 1939, when Poland was invaded by Nazi Germany, the Soviet Union and the Slovak Republic, marking the beginning of the European theatre of World War II.

In 1938, the Second Republic was the sixth largest country in Europe. According to the 1921 census, the number of inhabitants was 27.2 million. By 1939, just before the outbreak of World War II, this had grown to an estimated 35.1 million. Almost a third of the population came from minority groups: 13.9% Ruthenians; 10% Ashkenazi Jews; 3.1% Belarusians; 2.3% Germans and 3.4% Czechs and Lithuanians. At the same time, a significant number of ethnic Poles lived outside the country's borders.

When, after several regional conflicts, the borders of the state were finalized in 1922, Poland's neighbors were Czechoslovakia, Germany, the Free City of Danzig, Lithuania, Latvia, Romania and the Soviet Union. It had access to the Baltic Sea via a short strip of coastline either side of the city of Gdynia, known as the Polish Corridor. Between March and August 1939, Poland also shared a border with the then-Hungarian governorate of Subcarpathia. The political conditions of the Second Republic were heavily influenced by the aftermath of World War I and conflicts with neighbouring states as well as the emergence of Nazism in Germany.

The Second Republic maintained moderate economic development. The cultural hubs of interwar PolandWarsaw, Kraków, Poznań, Wilno and Lwówbecame major European cities and the sites of internationally acclaimed universities and other institutions of higher education.

The political borders of interwar Poland were distinctly different from modern Poland, as it contained less territory in the west and more territory in the east.

After more than a century of Partitions between the Austrian, the Prussian, and the Russian imperial powers, Poland re-emerged as a sovereign state at the end of the First World War in Europe in 1917–1918. The victorious Allies of World War I confirmed the rebirth of Poland in the Treaty of Versailles of June 1919. It was one of the great stories of the 1919 Paris Peace Conference. Poland solidified its independence in a series of border wars fought by the newly formed Polish Army from 1918 to 1921. The extent of the eastern half of the interwar territory of Poland was settled diplomatically in 1922 and internationally recognized by the League of Nations.

In the course of World War I (1914-1918), Germany gradually gained overall dominance on the Eastern Front as the Imperial Russian Army fell back. German and Austro-Hungarian armies seized the Russian-ruled part of what became Poland. In a failed attempt to resolve the Polish question as quickly as possible, Berlin set up a German puppet state on 5 November 1916, with a governing Provisional Council of State and (from 15 October 1917) a Regency Council ("Rada Regencyjna Królestwa Polskiego"). The Council administered the country under German auspices (see also Mitteleuropa), pending the election of a king. A month before Germany surrendered on 11 November 1918 and the war ended, the Regency Council had dissolved the Council of State, and announced its intention to restore Polish independence (7 October 1918). With the notable exception of the Marxist-oriented Social Democratic Party of the Kingdom of Poland and Lithuania ("SDKPiL"), most Polish political parties supported this move. On 23 October the Regency Council appointed a new government under Józef Świeżyński and began conscription into the Polish Army.

In 1918–1919, over 100 workers' councils sprang up on Polish territories; on 5 November 1918, in Lublin, the first Soviet of Delegates was established. On 6 November socialists proclaimed the Republic of Tarnobrzeg at Tarnobrzeg in Austrian Galicia. The same day the Socialist, Ignacy Daszyński, set up a Provisional People's Government of the Republic of Poland ("Tymczasowy Rząd Ludowy Republiki Polskiej") in Lublin. On Sunday, 10 November at 7 a.m., Józef Piłsudski, newly freed from 16 months in a German prison in Magdeburg, returned by train to Warsaw. Piłsudski, together with Colonel Kazimierz Sosnkowski, was greeted at Warsaw's railway station by Regent Zdzisław Lubomirski and by Colonel Adam Koc. Next day, due to his popularity and support from most political parties, the Regency Council appointed Piłsudski as Commander in Chief of the Polish Armed Forces. On 14 November, the Council dissolved itself and transferred all its authority to Piłsudski as Chief of State ("Naczelnik Państwa"). After consultation with Piłsudski, Daszyński's government dissolved itself and a new government formed under Jędrzej Moraczewski. In 1918 Italy became the first country in Europe to recognize Poland's renewed sovereignty.
Centers of government that formed at that time in Galicia (formerly Austrian-ruled southern Poland) included the National Council of the Principality of Cieszyn (established in November 1918), the Republic of Zakopane and the Polish Liquidation Committee (28 October). Soon afterward, the Polish–Ukrainian War broke out in Lwów (1 November 1918) between forces of the Military Committee of Ukrainians and the Polish irregular units made up of students known as the Lwów Eaglets, who were later supported by the Polish Army (see Battle of Lwów (1918), Battle of Przemyśl (1918)). Meanwhile, in western Poland, another war of national liberation began under the banner of the Greater Poland uprising (1918–19). In January 1919 Czechoslovakian forces attacked Polish units in the area of Zaolzie (see Polish–Czechoslovak War). Soon afterwards the Polish–Lithuanian War (ca 1919-1920) began, and in August 1919 Polish-speaking residents of Upper Silesia initiated a series of three Silesian Uprisings. The most critical military conflict of that period, however, the Polish–Soviet War (1919-1921), ended in a decisive Polish victory. In 1919 the Warsaw government suppressed the Republic of Tarnobrzeg and the workers' councils.

The Second Polish Republic was a parliamentary democracy from 1919 (see Small Constitution of 1919) to 1926, with the President having limited powers. The Parliament elected him, and he could appoint the Prime Minister as well as the government with the Sejm's (lower house's) approval, but he could only dissolve the Sejm with the Senate's consent. Moreover, his power to pass decrees was limited by the requirement that the Prime Minister and the appropriate other Minister had to verify his decrees with their signatures. Poland was one of the first countries in the world to recognize women's suffrage. Women in Poland were granted the right to vote on 28 November 1918 by a decree of Józef Piłsudski.

The major political parties at this time were the Polish Socialist Party, National Democrats, various Peasant Parties, Christian Democrats, and political groups of ethnic minorities (German: German Social Democratic Party of Poland, Jewish: General Jewish Labour Bund in Poland, United Jewish Socialist Workers Party, and Ukrainian: Ukrainian National Democratic Alliance). Frequently changing governments (see 1919 Polish legislative election, 1922 Polish legislative election) and other negative publicity the politicians received (such as accusations of corruption or 1919 Polish coup attempt), made them increasingly unpopular. Major politicians at this time, in addition to Piłsudski, included peasant activist Wincenty Witos (Prime Minister three times) and right-wing leader Roman Dmowski. Ethnic minorities were represented in the Sejm; e.g. in 1928 – 1930 there was the Ukrainian-Belarusian Club, with 26 Ukrainian and 4 Belarusian members.
After the Polish – Soviet war, Marshal Piłsudski led an intentionally modest life, writing historical books for a living. After he took power by a military coup in May 1926, he emphasized that he wanted to heal the Polish society and politics of excessive partisan politics. His regime, accordingly, was called Sanacja in Polish. The 1928 parliamentary elections were still considered free and fair, although the pro-Piłsudski Nonpartisan Bloc for Cooperation with the Government won them. The following three parliamentary elections (in 1930, 1935 and 1938) were manipulated, with opposition activists sent to Bereza Kartuska prison (see also Brest trials). As a result, pro-government party Camp of National Unity won huge majorities in them. Piłsudski died just after an authoritarian constitution was approved in the spring of 1935. During the last four years of the Second Polish Republic, the major politicians included President Ignacy Mościcki, Foreign Minister Józef Beck and the Commander-in-Chief of the Polish Army, Edward Rydz-Śmigły. The country was divided into 104 electoral districts, and those politicians who were forced to leave Poland, founded Front Morges in 1936. The government that ruled Second Polish Republic in its final years is frequently referred to as Piłsudski's colonels.
Interwar Poland had a considerably large army of 950,000 soldiers on active duty: in 37 infantry divisions, 11 cavalry brigades, and two armored brigades, plus artillery units. Another 700,000 men served in the reserves. At the outbreak of the war, the Polish army was able to put in the field almost one million soldiers, 4,300 guns, 1,280 tanks and 745 aircraft.

The training of the Polish Army was thorough. The non-commissioned officers were a competent body of men with expert knowledge and high ideals. The officers, both senior and junior, constantly refreshed their training in the field and in the lecture-hall, where modern technical achievement and the lessons of contemporary wars were demonstrated and discussed. The equipment of the Polish army was less developed technically than that of Nazi Germany and its rearmament was slowed down by confidence in Western European military support and by budget difficulties.

After regaining its independence, Poland was faced with major economic difficulties. In addition to the devastation wrought by World War I, the exploitation of the Polish economy by the German and Russian occupying powers, and the sabotage performed by retreating armies, the new republic was faced with the task of economically unifying disparate economic regions, which had previously been part of different countries. Within the borders of the Republic were the remnants of three different economic systems, with five different currencies (the German mark, the Russian ruble, the Austrian crown, the Polish marka and the Ostrubel) and with little or no direct infrastructural links. The situation was so bad that neighboring industrial centers as well as major cities lacked direct railroad links, because they had been parts of different nations. For example, there was no direct railroad connection between Warsaw and Kraków until 1934. This situation was described by Melchior Wańkowicz in his book Sztafeta.

On top of this was the massive destruction left after both World War I and the Polish–Soviet War. There was also a great economic disparity between the eastern (commonly called "Poland B") and western (called "Poland A") parts of the country, with the western half, especially areas that had belonged to the German Empire being much more developed and prosperous. Frequent border closures and a customs war with Germany also had negative economic impacts on Poland. In 1924 Prime Minister and Economic Minister Władysław Grabski introduced the złoty as a single common currency for Poland (it replaced the Polish marka), which remained a stable currency. The currency helped Poland to control the massive hyperinflation. It was the only country in Europe able to do this without foreign loans or aid. The average annual growth rate (GDP per capita) was 5.24% in 1920–29 and 0.34% in 1929–38.

Hostile relations with neighbors were a major problem for the economy of interbellum Poland. In the year 1937, foreign trade with all neighbors amounted to only 21% of Poland's total. Trade with Germany, Poland's most important neighbour, accounted for 14.3% of Polish exchange. Foreign trade with the Soviet Union (0.8%) was virtually nonexistent. Czechoslovakia accounted for 3.9%, Latvia for 0.3%, and Romania for 0.8%. By mid-1938, after the Anschluss of Austria, Greater Germany was responsible for as much as 23% of Polish foreign trade.
The basis of Poland's gradual recovery after the Great Depression was its mass economic development plans (see Four Year Plan), which oversaw the building of three key infrastructural elements. The first was the establishment of the Gdynia seaport, which allowed Poland to completely bypass Gdańsk (which was under heavy German pressure to boycott Polish coal exports). The second was construction of the 500-kilometer rail connection between Upper Silesia and Gdynia, called Polish Coal Trunk-Line, which served freight trains with coal. The third was the creation of a central industrial district, named "COP – Central Industrial Region" (Centralny Okręg Przemysłowy). Unfortunately, these developments were interrupted and largely destroyed by the German and Soviet invasion and the start of World War II. Other achievements of interbellum Poland included Stalowa Wola (a brand new city, built in a forest around a steel mill), Mościce (now a district of Tarnów, with a large nitrate factory), and the creation of a central bank. There were several trade fairs, with the most popular being Poznań International Fair, Lwów's Targi Wschodnie, and Wilno's Targi Północne. Polish Radio had ten stations (see Radio stations in interwar Poland), with the eleventh one planned to be opened in the autumn of 1939. Furthermore, in 1935 Polish engineers began working on the TV services. By early 1939, experts of the Polish Radio built four TV sets. The first movie broadcast by experimental Polish TV was Barbara Radziwiłłówna, and by 1940, regular TV service was scheduled to begin operation.

Interbellum Poland was also a country with numerous social problems. Unemployment was high, and poverty in the countryside was widespread, which resulted in several cases of social unrest, such as the 1923 Kraków riot, and 1937 peasant strike in Poland. There were conflicts with national minorities, such as Pacification of Ukrainians in Eastern Galicia (1930), relations with Polish neighbors were sometimes complicated (see Soviet raid on Stołpce, Polish–Czechoslovak border conflicts, 1938 Polish ultimatum to Lithuania). On top of this, there were natural disasters, such as the 1934 flood in Poland.

Interbellum Poland was unofficially divided into two parts – better developed "Poland A" in the west, and underdeveloped "Poland B" in the east. Polish industry was concentrated in the west, mostly in Polish Upper Silesia, and the adjacent Lesser Poland's province of Zagłębie Dąbrowskie, where the bulk of coal mines and steel plants was located. Furthermore, heavy industry plants were located in Częstochowa ("Huta Częstochowa", founded in 1896), Ostrowiec Świętokrzyski ("Huta Ostrowiec", founded in 1837–1839), Stalowa Wola (brand new industrial city, which was built from scratch in 1937 – 1938), Chrzanów ("Fablok", founded in 1919), Jaworzno, Trzebinia (oil refinery, opened in 1895), Łódź (the seat of Polish textile industry), Poznań (H. Cegielski – Poznań), Kraków and Warsaw (Ursus Factory). Further east, in Kresy, industrial centers included two major cities of the region – Lwów and Wilno (Elektrit).

Besides coal mining, Poland also had deposits of oil in Borysław, Drohobycz, Jasło and Gorlice (see Polmin), potassium salt (TESP), and basalt (Janowa Dolina). Apart from already-existing industrial areas, in the mid-1930s, an ambitious, state-sponsored project of Central Industrial Region was started under Minister Eugeniusz Kwiatkowski. One of characteristic features of Polish economy in the interbellum was gradual nationalization of major plants. This was the case of Ursus Factory (see Państwowe Zakłady Inżynieryjne), and several steelworks, such as "Huta Pokój" in Ruda Śląska – Nowy Bytom, "Huta Królewska" in Chorzów – Królewska Huta, "Huta Laura" in Siemianowice Śląskie, as well as "Scheibler and Grohman Works" in Łódź.

According to the 1939 Statistical Yearbook of Poland, total length of railways of Poland (as for 31 December 1937) was . Rail density was per . Railways were very dense in western part of the country, while in the east, especially Polesie, rail was non-existent in some counties. During the interbellum period, the Polish government constructed several new lines, mainly in the central part of the country (see also Polish State Railroads Summer 1939). Construction of extensive Warszawa Główna railway station was never finished due to the war, and Polish railroads were famous for their punctuality (see Luxtorpeda, Strzała Bałtyku, Latający Wilnianin).

In the interbellum, road network of Poland was dense, but the quality of the roads was very poor – only 7% of all roads was paved and ready for automobile use, and none of the major cities were connected with each other by a good-quality highway. Poles built in 1939 only one highway, 28 km of straight concrete road connecting villages Warlubie and Osiek (mid-northern Poland). It was designed by Italian engineer Piero Puricelli.
In the mid-1930s, Poland had of roads, but only 58,000 had hard surface (gravel, cobblestone or sett), and 2,500 were modern, with asphalt or concrete surface. In different parts of the country, there were sections of paved roads, which suddenly ended, and were followed by dirt roads. The poor condition of the roads was the result of both long-lasting foreign dominance and inadequate funding. On 29 January 1931, the Polish Parliament created the State Road Fund, the purpose of which was to collect money for the construction and conservation of roads. The government drafted a 10-year plan, with road priorities: a highway from Wilno, through Warsaw and Kraków, to Zakopane (called Marshal Piłsudski Highway), asphalt highways from Warsaw to Poznań and Łódź, as well as a Warsaw ring road. However, the plan turned out to be too ambitious, with insufficient money in the national budget to pay for it. In January 1938, the Polish Road Congress estimated that Poland would need to spend three times as much money on roads to keep up with Western Europe.

In 1939, before the outbreak of the war, LOT Polish Airlines, which was established in 1929, had its hub at Warsaw Okęcie Airport. At that time, LOT maintained several services, both domestic and international. Warsaw had regular domestic connections with Gdynia-Rumia, Danzig-Langfuhr, Katowice-Muchowiec, Kraków-Rakowice-Czyżyny, Lwów-Skniłów, Poznań-Ławica, and Wilno-Porubanek. Furthermore, in cooperation with Air France, LARES, Lufthansa, and Malert, international connections were maintained with Athens, Beirut, Berlin, Bucharest, Budapest, Helsinki, Kaunas, London, Paris, Prague, Riga, Rome, Tallinn, and Zagreb.

Statistically, the majority of citizens lived in the countryside (75% in 1921). Farmers made up 65% of the population. In 1929, agricultural production made up 65% of Poland's GNP. After 123 years of partitions, regions of the country were very unevenly developed. Lands of former German Empire were most advanced; in Greater Poland and Pomerelia, crops were on Western European level. The situation was much worse in parts of Congress Poland, Eastern Borderlands, and former Galicia, where agriculture was most backward and primitive, with a large number of small farms, unable to succeed in either the domestic and international market. Another problem was the overpopulation of the countryside, which resulted in chronic unemployment. Living conditions were so bad in several eastern regions, such as counties inhabited by the Hutsul minority, that there was permanent starvation. Farmers rebelled against the government (see: 1937 peasant strike in Poland), and the situation began to change in the late 1930s, due to construction of several factories for the Central Industrial Region, which gave employment to thousands of countryside residents.

Beginning in June 1925 there was a customs' war with the revanchist Weimar Republic imposing trade embargo against Poland for nearly a decade; involving tariffs, and broad economic restrictions. After 1933 the trade war ended. The new agreements regulated and promoted trade. Germany became Poland's largest trading partner, followed by Britain. In October 1938 Germany granted a credit of Rm 60,000,000 to Poland (120,000,000 zloty, or £4,800,000) which was never realized, due to the outbreak of war. Germany would deliver factory equipment and machinery in return for Polish timber and agricultural produce. This new trade was to be "in addition" to the existing German-Polish trade agreements.

In 1919, the Polish government introduced compulsory education for all children aged 7 to 14, in an effort to limit illiteracy, which was widespread especially in the former Russian Partition and the Austrian Partition of eastern Poland. In 1921, one-third of citizens of Poland remained illiterate (38% in the countryside). The process was slow, but by 1931, the illiteracy level had dropped to 23% overall (27% in the countryside) and further down to 18% in 1937. By 1939, over 90% of children attended school. In 1932, Minister of Religion and Education Janusz Jędrzejewicz carried out a major reform which introduced two main levels of education: "common school" ("szkoła powszechna"), with three levels – 4 grades + 2 grades + 1 grade; and "middle school" ("szkoła średnia"), with two levels – 4 grades of comprehensive middle school and 2 grades of specified high school (classical, humanistic, natural and mathematical). A graduate of middle school received a "small matura", while a graduate of high school received a "big matura", which enabled them to seek university-level education.

Before 1918, Poland had three universities: Jagiellonian University, University of Warsaw and Lwów University. Catholic University of Lublin was established in 1918; Adam Mickiewicz University, Poznań, in 1919; and finally, in 1922, after the annexation of Republic of Central Lithuania, Wilno University became the Republic's sixth university. There were also three technical colleges: the Warsaw University of Technology, Lwów Polytechnic and the AGH University of Science and Technology in Kraków, established in 1919. Warsaw University of Life Sciences was an agricultural institute. By 1939, there were around 50,000 students enrolled in further education. Women made up 28% of university students, the second highest proportion in Europe.

Polish science in the interbellum was renowned for its mathematicians gathered around the Lwów School of Mathematics, the Kraków School of Mathematics, as well as Warsaw School of Mathematics. There were world-class philosophers in the Lwów–Warsaw school of logic and philosophy. Florian Znaniecki founded Polish sociological studies. Rudolf Weigl invented a vaccine against typhus. Bronisław Malinowski counted among the most important anthropologists of the 20th century. 

In Polish literature, the 1920s were marked by the domination of poetry. Polish poets were divided into two groups – the Skamanderites (Jan Lechoń, Julian Tuwim, Antoni Słonimski and Jarosław Iwaszkiewicz) and the Futurists (Anatol Stern, Bruno Jasieński, Aleksander Wat, Julian Przyboś). Apart from well-established novelists (Stefan Żeromski, Władysław Reymont), new names appeared in the interbellum – Zofia Nałkowska, Maria Dąbrowska, Jarosław Iwaszkiewicz, Jan Parandowski, Bruno Schultz, Stanisław Ignacy Witkiewicz, Witold Gombrowicz. Among other notable artists there were sculptor Xawery Dunikowski, painters Julian Fałat, Wojciech Kossak and Jacek Malczewski, composers Karol Szymanowski, Feliks Nowowiejski, and Artur Rubinstein, singer Jan Kiepura. 

Theatre was immensely popular in the interbellum, with three main centers in the cities of Warsaw, Wilno and Lwów. Altogether, there were 103 theaters in Poland and a number of other theatrical institutions (including 100 folk theaters). In 1936, different shows were seen by 5 million people, and main figures of Polish theatre of the time were Juliusz Osterwa, Stefan Jaracz, and Leon Schiller. Also, before the outbreak of the war, there were approximately one million radios (see Radio stations in interwar Poland).

The administrative division of the Republic was based on a three-tier system. On the lowest rung were the "gminy", local town and village governments akin to districts or parishes. These were then grouped together into "powiaty" (akin to counties), which, in turn, were grouped as "województwa" (voivodeships, akin to provinces).

Historically, Poland was a nation of many nationalities. This was especially true after independence was regained in the wake of World War I and the subsequent Polish–Soviet War ending at Peace of Riga. The census of 1921 shows 30.8% of the population consisted of ethnic minorities, compared with a share of 1.6% (solely identifying with a non-Polish ethnic group) or 3.8% (including those identifying with both the Polish ethnicity and with another ethnic group) in 2011. The first spontaneous flight of about 500,000 Poles from the Soviet Union occurred during the reconstitution of sovereign Poland. In the second wave, between November 1919 and June 1924 some 1,200,000 people left the territory of the USSR for Poland. It is estimated that some 460,000 of them spoke Polish as the first language. According to the 1931 Polish Census: 68.9% of the population was Polish, 13.9% were Ukrainian, around 10% Jewish, 3.1% Belarusian, 2.3% German and 2.8% other, including Lithuanian, Czech, Armenian, Russian, and Romani. The situation of minorities was a complex subject and changed during the period.

Poland was also a nation of many religions. In 1921, 16,057,229 Poles (approx. 62.5%) were Roman (Latin) Catholics, 3,031,057 citizens of Poland (approx. 11.8%) were Eastern Rite Catholics (mostly Ukrainian Greek Catholics and Armenian Rite Catholics), 2,815,817 (approx. 10.95%) were Greek Orthodox, 2,771,949 (approx. 10.8%) were Jewish, and 940,232 (approx. 3.7%) were Protestants (mostly Lutheran).

By 1931, Poland had the second largest Jewish population in the world, with one-fifth of all the world's Jews residing within its borders (approx. 3,136,000). The urban population of interbellum Poland was rising steadily; in 1921, only 24% of Poles lived in the cities, in the late 1930s, that proportion grew to 30%. In more than a decade, the population of Warsaw grew by 200,000, Łódź by 150,000, and Poznań – by 100,000. This was due not only to internal migration, but also to an extremely high birth rate..

From the 1920s the Polish government excluded Jews from receiving government bank loans, public sector employment, and obtaining business licenses. From the 1930s measures were taken against Jewish shops, Jewish export firms, Shechita as well as limitations were placed on Jewish admission to the medical and legal professions, Jews in business associations and the enrollment of Jews into universities. The far-right National Democracy (Endecja from the abbreviation "ND") often organized anti-Jewish boycotts. Following the death of Józef Piłsudski in 1935, the Endecja intensified their efforts which triggered violence in extreme cases in smaller towns across the country. In 1937, the National Democracy movement passed resolutions that "its main aim and duty must be to remove the Jews from all spheres of social, economic, and cultural life in Poland". The government in response organized the Camp of National Unity (OZON), which in 1938 took control of the Polish Sejm and subsequently drafted anti-Semitic legislation similar to the Anti-Jewish laws in Germany, Hungary, and Romania. OZON advocated mass emigration of Jews from Poland, numerus clausus (see also Ghetto benches), and other limitation on Jewish rights. According to William W. Hagen by 1939, prior to the war, Polish Jews were threatened with conditions similar to those in Nazi Germany.

The pre-war government also restricted rights of people who declared Ukrainian nationality, belonged to the Eastern Orthodox Church and inhabited the Eastern Borderlands of the Second Polish Republic. The Ukrainian language was restricted in every field possible, especially in governmental institutions, and the term "Ruthenian" was enforced in an attempt to ban the use of the term "Ukrainian". Ukrainians were categorized as uneducated second-class peasants or third world people and rarely settled outside the Eastern Borderland region due to the prevailing Ukrainophobia and restrictions imposed. Numerous attempts at restoring the Ukrainian state were suppressed and any existent violence or terrorism initiated by the Organization of Ukrainian Nationalists was emphasized to create an image of a "brutal Eastern savage".

The Second Polish Republic was mainly flat with average elevation of above sea level, except for the southernmost Carpathian Mountains (after World War II and its border changes, the average elevation of Poland decreased to ). Only 13% of territory, along the southern border, was higher than . The highest elevation in the country was Mount Rysy, which rises in the Tatra Range of the Carpathians, approximately south of Kraków. Between October 1938 and September 1939, the highest elevation was Lodowy Szczyt (known in the Slovak language as "Ľadový štít"), which rises above sea level. The largest lake was Lake Narach.
The country's total area, after the annexation of Zaolzie, was . It extended from north to south and from east to west. On 1 January 1938, total length of boundaries was , including: of coastline (out of which were made by the Hel Peninsula), the with Soviet Union, 948 kilometers with Czechoslovakia (until 1938), with Germany (together with East Prussia), and with other countries (Lithuania, Romania, Latvia, Danzig). The warmest yearly average temperature was in Kraków among major cities of the Second Polish Republic, at in 1938; and the coldest in Wilno ( in 1938). Extreme geographical points of Poland included Przeświata River in Somino to the north (located in the Braslaw county of the Wilno Voivodeship); Manczin River to the south (located in the Kosów county of the Stanisławów Voivodeship); Spasibiorki near railway to Połock to the east (located in the Dzisna county of the Wilno Voivodeship); and Mukocinek near Warta River and Meszyn Lake to the west (located in the Międzychód county of the Poznań Voivodeship).

Almost 75% of the territory of interbellum Poland was drained northward into the Baltic Sea by the Vistula (total area of drainage basin of the Vistula within boundaries of the Second Polish Republic was , the Niemen (), the Odra () and the Daugava (). The remaining part of the country was drained southward, into the Black Sea, by the rivers that drain into the Dnieper (Pripyat, Horyn and Styr, all together ) as well as Dniester ()

The Second World War in 1939 ended the sovereign Second Polish Republic. The German invasion of Poland began on 1 September 1939, one week after Nazi Germany and the Soviet Union signed the secret Molotov–Ribbentrop Pact. On that day, Germany and Slovakia attacked Poland, and on 17 September the Soviets attacked eastern Poland. Warsaw fell to the Nazis on 28 September after a twenty-day siege. Open organized Polish resistance ended on 6 October 1939 after the Battle of Kock, with Germany and the Soviet Union occupying most of the country. Lithuania annexed the area of Wilno, and Slovakia seized areas along Poland's southern border - including Górna Orawa and Tatranská Javorina - which Poland had annexed from Czechoslovakia in October 1938. Poland did not surrender to the invaders, but continued fighting under the auspices of the Polish government-in-exile and of the Polish Underground State. After the signing of the German–Soviet Treaty of Friendship, Cooperation and Demarcation on 28 September 1939, Polish areas occupied by Nazi Germany either became directly annexed to the Third Reich, or became part of the General Government. The Soviet Union, following Elections to the People's Assemblies of Western Ukraine and Western Belarus (22 October 1939), annexed eastern Poland partly to the Byelorussian Soviet Socialist Republic, and partly to the Ukrainian Soviet Socialist Republic (November 1939).
Polish war plans (Plan West and Plan East) failed as soon as Germany invaded in 1939. The Polish losses in combat against Germans (killed and missing in action) amounted to ca. 70,000 men. Some 420,000 of them were taken prisoners. Losses against the Red Army (which invaded Poland on 17 September) added up to 6,000 to 7,000 of casualties and MIA, 250,000 were taken prisoners. Although the Polish army – considering the inactivity of the Allies – was in an unfavorable position – it managed to inflict serious losses to the enemies: 20,000 German soldiers were killed or MIA, 674 tanks and 319 armored vehicles destroyed or badly damaged, 230 aircraft shot down; the Red Army lost (killed and MIA) about 2,500 soldiers, 150 combat vehicles and 20 aircraft. The Soviet invasion of Poland, and lack of promised aid from the Western Allies, contributed to the Polish forces defeat by 6 October 1939.
A popular myth is that Polish cavalry armed with lances charged German tanks during the September 1939 campaign. This often repeated account, first reported by Italian journalists as German propaganda, concerned an action by the Polish 18th Lancer Regiment near Chojnice. This arose from misreporting of a single clash on 1 September 1939 near Krojanty, when two squadrons of the Polish 18th Lancers armed with sabers surprised and wiped out a German infantry formation with a mounted saber charge. Shortly after midnight the 2nd (Motorized) Division was compelled to withdraw by Polish cavalry, before the Poles were caught in the open by German armored cars. The story arose because some German armored cars appeared and gunned down 20 troopers as the cavalry escaped. Even this failed to persuade everyone to reexamine their beliefs—there were some who thought Polish cavalry had been improperly employed in 1939.

Between 1939 and 1990, the Polish government-in-exile operated in Paris and later in London, presenting itself as the only legal and legitimate representative of the Polish nation. In 1990 the last president in exile, Ryszard Kaczorowski handed the presidential insignia to the newly elected President, Lech Wałęsa, signifying continuity between the Second and Third republics.









</doc>
<doc id="14246" url="https://en.wikipedia.org/wiki?curid=14246" title="Hedwig">
Hedwig

Hedwig may refer to:




</doc>
<doc id="14251" url="https://en.wikipedia.org/wiki?curid=14251" title="HMS Resolution">
HMS Resolution

Several ships of the Royal Navy have borne the name HMS "Resolution". However, the first English warship to bear the name "Resolution" was actually the first rate "Prince Royal" (built in 1610 and rebuilt in 1641), which was renamed "Resolution" in 1650 following the inauguration of the Commonwealth, and continued to bear that name until 1660, when the name "Prince Royal" was restored. The name "Resolution" was bestowed on the first of the vessels listed below:


Also

Ships named "Resolution" have earned the following battle honours:


Citations

References


</doc>
<doc id="14254" url="https://en.wikipedia.org/wiki?curid=14254" title="Helen Keller">
Helen Keller

Helen Adams Keller (June 27, 1880 – June 1, 1968) was an American author, political activist, and lecturer. She was the first deaf-blind person to earn a Bachelor of Arts degree. The story of Keller and her teacher, Anne Sullivan, was made famous by Keller's autobiography, "The Story of My Life", and its adaptations for film and stage, "The Miracle Worker". Her birthplace in West Tuscumbia, Alabama, is now a museum and sponsors an annual "Helen Keller Day". Her June 27 birthday is commemorated as Helen Keller Day in Pennsylvania and, in the centenary year of her birth, was recognized by a presidential proclamation from US President Jimmy Carter.

A prolific author, Keller was well-traveled and outspoken in her convictions. A member of the Socialist Party of America and the Industrial Workers of the World, she campaigned for women's suffrage, labor rights, socialism, antimilitarism, and other similar causes. She was inducted into the Alabama Women's Hall of Fame in 1971 and was one of twelve inaugural inductees to the Alabama Writers Hall of Fame on June 8, 2015.

Helen Adams Keller was born on June 27, 1880 in Tuscumbia, Alabama. Her family lived on a homestead, Ivy Green, that Helen's grandfather had built decades earlier. She had four siblings: two full siblings, Mildred Campbell (Keller) Tyson and Phillip Brooks Keller, and two older half-brothers from her father's prior marriage, James McDonald Keller and William Simpson Keller.

Her father, Arthur Henley Keller (1836–1896), spent many years as an editor of the Tuscumbia "North Alabamian" and had served as a captain in the Confederate Army. 
The family were part of the slaveholding elite before the war, but lost status later. Her mother, Catherine Everett (Adams) Keller (1856–1921), known as "Kate", was the daughter of Charles W. Adams, a Confederate general. Her paternal lineage was traced to Casper Keller, a native of Switzerland. One of Helen's Swiss ancestors was the first teacher for the deaf in Zurich. Keller reflected on this irony in her first autobiography, stating "that there is no king who has not had a slave among his ancestors, and no slave who has not had a king among his."

At 19 months old, Keller contracted an unknown illness described by doctors as "an acute congestion of the stomach and the brain", which might have been scarlet fever or meningitis. The illness left her both deaf and blind. She lived, as she recalled in her autobiography, "at sea in a dense fog".

At that time, Keller was able to communicate somewhat with Martha Washington, the two-years older daughter of the family cook, who understood her signs; by the age of seven, Keller had more than 60 home signs to communicate with her family, and could distinguish people by the vibration of their footsteps.

In 1886, Keller's mother, inspired by an account in Charles Dickens' "American Notes" of the successful education of another deaf and blind woman, Laura Bridgman, dispatched the young Keller, accompanied by her father, to seek out physician J. Julian Chisolm, an eye, ear, nose, and throat specialist in Baltimore, for advice.

Chisholm referred the Kellers to Alexander Graham Bell, who was working with deaf children at the time. Bell advised them to contact the Perkins Institute for the Blind, the school where Bridgman had been educated, which was then located in South Boston. Michael Anagnos, the school's director, asked a 20-year-old alumna of the school, Anne Sullivan, herself visually impaired, to become Keller's instructor. It was the beginning of a nearly 50-year-long relationship during which Sullivan evolved into Keller's governess and eventually her companion.

Sullivan arrived at Keller's house on March 5, 1887, a day Keller would forever remember as "my soul's birthday." Sullivan immediately began to teach Helen to communicate by spelling words into her hand, beginning with "d-o-l-l" for the doll that she had brought Keller as a present. Keller was frustrated, at first, because she did not understand that every object had a word uniquely identifying it. When Sullivan was trying to teach Keller the word for "mug", Keller became so frustrated she broke the mug. But soon she began imitating Sullivan's hand gestures. "I did not know that I was spelling a word or even that words existed," Keller remembered. "I was simply making my fingers go in monkey-like imitation."

Keller's breakthrough in communication came the next month when she realized that the motions her teacher was making on the palm of her hand, while running cool water over her other hand, symbolized the idea of "water". Writing in her autobiography, "The Story of My Life," Keller recalled the moment: "I stood still, my whole attention fixed upon the motions of her fingers. Suddenly I felt a misty consciousness as of something forgotten — a thrill of returning thought; and somehow the mystery of language was revealed to me. I knew then that w-a-t-e-r meant the wonderful cool something that was flowing over my hand. The living word awakened my soul, gave it light, hope, set it free!" Keller then nearly exhausted Sullivan, demanding the names of all the other familiar objects in her world.

Helen Keller was viewed as isolated but was very in touch with the outside world. She was able to enjoy music by feeling the beat and she was able to have a strong connection with animals through touch. She was delayed at picking up language, but that did not stop her from having a voice.

In May 1888, Keller started attending the Perkins Institute for the Blind. In 1894, Keller and Sullivan moved to New York to attend the Wright-Humason School for the Deaf, and to learn from Sarah Fuller at the Horace Mann School for the Deaf. In 1896, they returned to Massachusetts, and Keller entered The Cambridge School for Young Ladies before gaining admittance, in 1900, to Radcliffe College of Harvard University, where she lived in Briggs Hall, South House. Her admirer, Mark Twain, had introduced her to Standard Oil magnate Henry Huttleston Rogers, who, with his wife Abbie, paid for her education. In 1904, at the age of 24, Keller graduated as a member of Phi Beta Kappa from Radcliffe, becoming the first deaf-blind person to earn a Bachelor of Arts degree. She maintained a correspondence with the Austrian philosopher and pedagogue Wilhelm Jerusalem, who was one of the first to discover her literary talent.

Determined to communicate with others as conventionally as possible, Keller learned to speak and spent much of her life giving speeches and lectures on aspects of her life. She learned to "hear" people's speech using the Tadoma method, which means using her fingers to feel the lips and throat of the speaker—her sense of touch had heightened. She became proficient at using braille and reading sign language with her hands as well. Shortly before World War I, with the assistance of the Zoellner Quartet, she determined that by placing her fingertips on a resonant tabletop she could experience music played close by.

On January 22, 1916, Keller and Sullivan traveled to the small town of Menomonie in western Wisconsin to deliver a lecture at the Mabel Tainter Memorial Building. Details of her talk were provided in the weekly "Dunn County News" on January 22, 1916:
A message of optimism, of hope, of good cheer, and of loving service was brought to Menomonie Saturday—a message that will linger long with those fortunate enough to have received it. This message came with the visit of Helen Keller and her teacher, Mrs. John Macy, and both had a hand in imparting it Saturday evening to a splendid audience that filled The Memorial. The wonderful girl who has so brilliantly triumphed over the triple afflictions of blindness, dumbness and deafness, gave a talk with her own lips on "Happiness", and it will be remembered always as a piece of inspired teaching by those who heard it.

Anne Sullivan stayed as a companion to Helen Keller long after she taught her. Sullivan married John Macy in 1905, and her health started failing around 1914. Polly Thomson (February 20, 1885 – March 21, 1960) was hired to keep house. She was a young woman from Scotland who had no experience with deaf or blind people. She progressed to working as a secretary as well, and eventually became a constant companion to Keller.

Keller moved to Forest Hills, Queens, together with Sullivan and Macy, and used the house as a base for her efforts on behalf of the American Foundation for the Blind. "While in her thirties Helen had a love affair, became secretly engaged, and defied her teacher and family by attempting an elopement with the man she loved." He was the finger-spelling socialist "Peter Fagan, a young "Boston Herald" reporter who was sent to Helen's home to act as her private secretary when lifelong companion, Anne, fell ill." 
At the time, her father had died and Sullivan was recovering in Lake Placid and Puerto Rico.
Keller had moved with her mother in Montgomery, Alabama.

Anne Sullivan died in 1936, with Keller holding her hand, after falling into a coma as a result of coronary thrombosis. Keller and Thomson moved to Connecticut. They traveled worldwide and raised funds for the blind. Thomson had a stroke in 1957 from which she never fully recovered, and died in 1960. Winnie Corbally, a nurse originally hired to care for Thomson in 1957, stayed on after Thomson's death and was Keller's companion for the rest of her life.

Keller went on to become a world-famous speaker and author. She is remembered as an advocate for people with disabilities, amid numerous other causes. The deaf community was widely impacted by her. She traveled to twenty-five different countries giving motivational speeches about Deaf people's conditions. She was a suffragist, pacifist, radical socialist, birth control supporter, and opponent of Woodrow Wilson. In 1915, she and George A. Kessler founded the Helen Keller International (HKI) organization. This organization is devoted to research in vision, health, and nutrition. 
In 1916 she sent money to the NAACP ashamed of the Southern un-Christian treatment of "colored people".
In 1920, she helped to found the American Civil Liberties Union (ACLU). Keller traveled to over 40 countries with Sullivan, making several trips to Japan and becoming a favorite of the Japanese people. Keller met every U.S. president from Grover Cleveland to Lyndon B. Johnson and was friends with many famous figures, including Alexander Graham Bell, Charlie Chaplin and Mark Twain. Keller and Twain were both considered political radicals allied with leftist politics.

Keller was a member of the Socialist Party and actively campaigned and wrote in support of the working class from 1909 to 1921. Many of her speeches and writings were about women's right to vote and the impacts of war; in addition, she supported causes that opposed military intervention. She had speech therapy in order to have her voice heard better by the public. When the Rockefeller-owned press refused to print her articles, she protested until her work was finally published. She supported Socialist Party candidate Eugene V. Debs in each of his campaigns for the presidency. Before reading "Progress and Poverty", Helen Keller was already a socialist who believed that Georgism was a good step in the right direction. She later wrote of finding "in Henry George's philosophy a rare beauty and power of inspiration, and a splendid faith in the essential nobility of human nature".

Keller claimed that newspaper columnists who had praised her courage and intelligence before she expressed her socialist views now called attention to her disabilities. The editor of the "Brooklyn Eagle" wrote that her "mistakes sprung out of the manifest limitations of her development". Keller responded to that editor, referring to having met him before he knew of her political views:

Keller joined the Industrial Workers of the World (the IWW, known as the Wobblies) in 1912, saying that parliamentary socialism was "sinking in the political bog". She wrote for the IWW between 1916 and 1918. In "Why I Became an IWW", Keller explained that her motivation for activism came in part from her concern about blindness and other disabilities:

The last sentence refers to prostitution and syphilis, the former a frequent cause of the latter, and the latter a leading cause of blindness. In the same interview, Keller also cited the 1912 strike of textile workers in Lawrence, Massachusetts for instigating her support of socialism.

Keller supported eugenics. In 1915, she wrote in favor of refusing life-saving medical procedures to infants with severe mental impairments or physical deformities, stating that their lives were not worthwhile and they would likely become criminals. Keller also expressed concerns about human overpopulation.

Keller wrote a total of 12 published books and several articles.

One of her earliest pieces of writing, at age 11, was "The Frost King" (1891). There were allegations that this story had been plagiarized from "The Frost Fairies" by Margaret Canby. An investigation into the matter revealed that Keller may have experienced a case of cryptomnesia, which was that she had Canby's story read to her but forgot about it, while the memory remained in her subconscious.

At age 22, Keller published her autobiography, "The Story of My Life" (1903), with help from Sullivan and Sullivan's husband, John Macy. It recounts the story of her life up to age 21 and was written during her time in college.

Keller wrote "The World I Live In" in 1908, giving readers an insight into how she felt about the world. "Out of the Dark", a series of essays on socialism, was published in 1913.

When Keller was young, Anne Sullivan introduced her to Phillips Brooks, who introduced her to Christianity, Keller famously saying: "I always knew He was there, but I didn't know His name!"

Her spiritual autobiography, "My Religion", was published in 1927 and then in 1994 extensively revised and re-issued under the title "Light in My Darkness". It advocates the teachings of Emanuel Swedenborg, the Christian theologian and mystic who gave a spiritual interpretation of the teachings of the Bible and who claimed that the Second Coming of Jesus Christ had already taken place.

Keller described the core of her belief in these words:

But in Swedenborg's teaching it [Divine Providence] is shown to be the government of God's Love and Wisdom and the creation of uses. Since His Life cannot be less in one being than another, or His Love manifested less fully in one thing than another, His Providence must needs be universal ... He has provided religion of some kind everywhere, and it does not matter to what race or creed anyone belongs if he is faithful to his ideals of right living.

Keller visited 35 countries from 1946 to 1957.

In 1948 she went to New Zealand and visited deaf schools in Christchurch and Auckland. She met Deaf Society of Canterbury Life Member Patty Still in Christchurch.

Keller suffered a series of strokes in 1961 and spent the last years of her life at her home.

On September 14, 1964, President Lyndon B. Johnson awarded her the Presidential Medal of Freedom, one of the United States' two highest civilian honors. In 1965 she was elected to the National Women's Hall of Fame at the New York World's Fair.

Keller devoted much of her later life to raising funds for the American Foundation for the Blind. She died in her sleep on June 1, 1968, at her home, Arcan Ridge, located in Easton, Connecticut, a few weeks short of her eighty-eighth birthday. A service was held in her honor at the National Cathedral in Washington, D.C., her body was cremated and her ashes were placed there next to her constant companions, Anne Sullivan and Polly Thomson. She was buried at the Washington National Cathedral in Washington, D.C.

Keller's life has been interpreted many times. She appeared in a silent film, "Deliverance" (1919), which told her story in a melodramatic, allegorical style.

She was also the subject of the Academy Award-winning documentary "Helen Keller in Her Story", narrated by her friend and noted theatrical actress Katharine Cornell. She was also profiled in "The Story of Helen Keller", part of the Famous Americans series produced by Hearst Entertainment.

"The Miracle Worker" is a cycle of dramatic works ultimately derived from her autobiography, "The Story of My Life". The various dramas each describe the relationship between Keller and Sullivan, depicting how the teacher led her from a state of almost feral wildness into education, activism, and intellectual celebrity. The common title of the cycle echoes Mark Twain's description of Sullivan as a "miracle worker". Its first realization was the 1957 "Playhouse 90" teleplay of that title by William Gibson. He adapted it for a Broadway production in 1959 and an Oscar-winning feature film in 1962, starring Anne Bancroft and Patty Duke. It was remade for television in 1979 and 2000.

In 1984, Keller's life story was made into a TV movie called "The Miracle Continues". This film, a semi-sequel to "The Miracle Worker", recounts her college years and her early adult life. None of the early movies hint at the social activism that would become the hallmark of Keller's later life, although a Disney version produced in 2000 states in the credits that she became an activist for social equality.

The Bollywood movie "Black" (2005) was largely based on Keller's story, from her childhood to her graduation.

A documentary called "Shining Soul: Helen Keller's Spiritual Life and Legacy" was produced by the Swedenborg Foundation in the same year. The film focuses on the role played by Emanuel Swedenborg's spiritual theology in her life and how it inspired Keller's triumph over her triple disabilities of blindness, deafness and a severe speech impediment.

On March 6, 2008, the New England Historic Genealogical Society announced that a staff member had discovered a rare 1888 photograph showing Helen and Anne, which, although previously published, had escaped widespread attention. Depicting Helen holding one of her many dolls, it is believed to be the earliest surviving photograph of Anne Sullivan Macy.

Video footage showing Helen Keller learning to mimic speech sounds also exists.

A biography of Helen Keller was written by the German Jewish author Hildegard Johanna Kaeser.

A painting titled "The Advocate: Tribute to Helen Keller" was created by three artists from Kerala, India as a tribute to Helen Keller. The Painting was created in association with a non-profit organization Art d'Hope Foundation, artists groups Palette People and XakBoX Design & Art Studio. This painting was created for a fundraising event to help blind students in India and was inaugurated by M. G. Rajamanikyam, IAS (District Collector Ernakulam) on Helen Keller day (June 27, 2016). The painting depicts the major events of Helen Keller's life and is one of the biggest paintings done based on Helen Keller's life.

A preschool for the deaf and hard of hearing in Mysore, India, was originally named after Helen Keller by its founder, K. K. Srinivasan.
In 1999, Keller was listed in Gallup's Most Widely Admired People of the 20th century.

In 2003, Alabama honored its native daughter on its state quarter. The Alabama state quarter is the only circulating U.S. coin to feature braille.

The Helen Keller Hospital in Sheffield, Alabama, is dedicated to her.

Streets are named after Helen Keller in Zürich, Switzerland, in the US, in Getafe, Spain, in Lod, Israel, in Lisbon, Portugal, and in Caen, France.

In 1973, Helen Keller was inducted into the National Women's Hall of Fame.

A stamp was issued in 1980 by the United States Postal Service depicting Keller and Sullivan, to mark the centennial of Keller's birth.

On October 7, 2009, a bronze statue of Keller was added to the National Statuary Hall Collection, as a replacement for the State of Alabama's former 1908 statue of the education reformer Jabez Lamar Monroe Curry.

Archival material of Helen Keller stored in New York was lost when the Twin Towers were destroyed in the September 11 attacks.

The Helen Keller Archives are owned by the American Foundation for the Blind.






</doc>
<doc id="14257" url="https://en.wikipedia.org/wiki?curid=14257" title="Haddocks' Eyes">
Haddocks' Eyes

"Haddocks' Eyes" is a song sung by The White Knight from Lewis Carroll's 1871 novel "Through the Looking-Glass", . 

"Haddocks' Eyes" is an example used to elaborate on the symbolic status of the concept of "name": a name as identification marker may be assigned to anything, including another name, thus introducing different levels of symbolization. It was discussed in several works on logic and philosophy.

The White Knight explains to Alice a confusing nomenclature for the song.
‘You are sad,’ the Knight said in an anxious tone: ‘let me sing you a
song to comfort you.’

‘Is it very long?’ Alice asked, for she had heard a good deal of poetry
that day.

‘It’s long,’ said the Knight, ‘but very, VERY beautiful. Everybody that
hears me sing it--either it brings the TEARS into their eyes, or else--’

‘Or else what?’ said Alice, for the Knight had made a sudden pause.

‘Or else it doesn’t, you know. The name of the song is called “HADDOCKS’
EYES.”’

‘Oh, that’s the name of the song, is it?’ Alice said, trying to feel
interested.

‘No, you don’t understand,’ the Knight said, looking a little vexed.
‘That’s what the name is CALLED. The name really IS “THE AGED AGED
MAN.”’

‘Then I ought to have said “That’s what the SONG is called”?’ Alice
corrected herself.

‘No, you oughtn’t: that’s quite another thing! The SONG is called “WAYS
AND MEANS”: but that’s only what it’s CALLED, you know!’

‘Well, what IS the song, then?’ said Alice, who was by this time
completely bewildered.

‘I was coming to that,’ the Knight said. ‘The song really IS “A-SITTING
ON A GATE”: and the tune’s my own invention.’

To summarize:


The complicated terminology distinguishing between 'the song, what the song is called, the name of the song, and what the name of the song is called' both uses and mentions the use–mention distinction.

The White Knight sings the song to a tune he claims as his own invention, but which Alice recognises as "I give thee all, I can no more". By the time Alice heard it, she was already tired of poetry.

The song parodies the plot, but not the style or metre, of "Resolution and Independence" by William Wordsworth.

Like "Jabberwocky," another poem published in "Through the Looking Glass," "Haddocks’ Eyes" appears to have been revised over the course of many years. In 1856, Carroll published the following poem anonymously under the name "Upon the Lonely Moor". It bears an obvious resemblance to "Haddocks' Eyes."



</doc>
<doc id="14260" url="https://en.wikipedia.org/wiki?curid=14260" title="Hoosier">
Hoosier

Hoosier is the official demonym for a resident of the U.S. state of Indiana. The origin of the term remains a matter of debate within the state, but "Hoosier" was in general use by the 1840s, having been popularized by Richmond resident John Finley's 1833 poem "The Hoosier's Nest". Anyone born in Indiana or a resident at the time is considered to be a Hoosier. Indiana adopted the nickname "The Hoosier State" more than 150 years ago.

"Hoosier" is used in the names of numerous Indiana-based businesses and organizations. "Hoosiers" is also the name of the Indiana University athletic teams and seven active and one disbanded athletic conferences in the Indiana High School Athletic Association have the word "Hoosier" in their name. As there is no accepted embodiment of a Hoosier, the IU schools are represented through their letters and colors alone. In addition to universal acceptance by residents of Indiana, the term is also the official demonym according to the U.S. Government Publishing Office.

In addition to "The Hoosier's Nest", the term also appeared in the "Indianapolis Journal"<nowiki>'</nowiki>s "Carrier's Address" on January 1, 1833. There are many suggestions for the derivation of the word but none is universally accepted. In 1833 the Pittsburgh Statesman said the term had been in use for "some time past" and suggested it originated from census workers calling "Who's here?". Also in 1833, former Indiana Governor J.B. Ray began publishing a newspaper titled "The Hoosier". 

In 1900, Meredith Nicholson wrote "The Hoosiers", an early attempt to study the etymology of the word as applied to Indiana residents. Jacob Piatt Dunn, longtime secretary of the Indiana Historical Society, published "The Word Hoosier", a similar attempt, in 1907. Both chronicled some of the popular and satirical etymologies circulating at the time and focused much of their attention on the use of the word in the Upland South to refer to woodsmen, yokels, and rough people. Dunn traced the word back to the Cumbrian "hoozer", meaning anything unusually large, derived from the Old English "hoo" (as at Sutton Hoo), meaning "high" and "hill". The importance of immigrants from northern England and southern Scotland was reflected in numerous placenames including the Cumberland Mountains, the Cumberland River, and the Cumberland Gap. Nicholson defended the people of Indiana against such an association, while Dunn concluded that the early settlers had adopted the nickname self-mockingly and that it had lost its negative associations by the time of Finley's poem.

Johnathan Clark Smith subsequently showed that Nicholson and Dunn's earliest sources within Indiana were mistaken. A letter by James Curtis cited by Dunn and others as the earliest known use of the term was actually written in 1846, not 1826. Similarly, the use of the term in an 1859 newspaper item quoting an 1827 diary entry by Sandford Cox was more likely an editorial comment and not from the original diary. Smith's earliest sources led him to argue that the word originated as a term along the Ohio River for flatboatmen from Indiana and did not acquire its pejorative meanings until 1836, "after" Finley's poem.

William Piersen, a history professor at Fisk University, argued for a connection to the Methodist minister Rev. Harry Hosier (–May 1806), who evangelized the American frontier at the beginning of the 19th century as part of the Second Great Awakening. "Black Harry" had been born a slave in North Carolina and sold north to Baltimore, Maryland, before gaining his freedom and beginning his ministry around the end of the American Revolution. He was a close associate and personal friend of Bishop Francis Asbury, the "Father of the American Methodist Church". Benjamin Rush said of him that "making allowances for his illiteracy, he was the greatest orator in America". His sermons called on Methodists to reject slavery and to champion the common working man. Piersen proposed that Methodist communities inspired by his example took or were given a variant spelling of his name (possibly influenced by the "yokel" slang) during the decades after his ministry.

According to Washington County newspaper reports of the time, Abraham Stover was Colonel of the Indiana Militia. He was a colorful figure in early Washington County history. Along with his son-in-law, John B. Brough, he was considered one of the two strongest men in Washington County. He was always being challenged to prove his might, and seems to have won several fights over men half his age. After whipping six or eight men in a fist fight in Louisville, Kentucky, he cracked his fists and said, "Ain't I a husher", which was changed in the news to "Hoosier", and thus originated the name of Hoosier in connection with Indiana men.

Jorge Santander Serrano, a PhD student from Indiana University, has also suggested that "Hoosier" might come from the French words for 'redness', , or 'red-faced', . According to this hypothesis, the early pejorative use of the word "Hoosier" may have a link to the color red ("rouge" in French) which is associated with indigenous peoples, pejoratively called "red men" or "red-skins", and also with poor white people by calling them "red-necks".

Humorous folk etymologies for the term "hoosier" have a long history, as recounted by Dunn in "The Word Hoosier".

One account traces the word to the necessary caution of approaching houses on the frontier. In order to avoid being shot, a traveler would call out from afar to let themselves be known. The inhabitants of the cabin would then reply "Who's here?" which in the Appalachian English of the early settlers slurred into "Who'sh 'ere?" and thence into "Hoosier?" A variant of this account had the Indiana pioneers calling out "Who'sh 'ere?" as a general greeting and warning when hearing someone in the bushes and tall grass, to avoid shooting a relative or friend in error.

The poet James Whitcomb Riley facetiously suggested that the fierce brawling that took place in Indiana involved enough biting that the expression "Whose ear?" became notable. This arose from or inspired the story of two 19th-century French immigrants brawling in a tavern in the foothills of southern Indiana. One was cut and a third Frenchman walked in to see an ear on the dirt floor of the tavern, prompting him to slur out "Whosh ear?"

Two related stories trace the origin of the term to gangs of workers from Indiana under the direction of a Mr. Hoosier.

The account related by Dunn is that a Louisville contractor named Samuel Hoosier preferred to hire workers from communities on the Indiana side of the Ohio River like New Albany rather than Kentuckians. During the excavation of the first canal around the Falls of the Ohio from 1826 to 1833, his employees became known as "Hoosier's men" and then simply "Hoosiers". The usage spread from these hard-working laborers to all of the Indiana boatmen in the area and then spread north with the settlement of the state. The story was told to Dunn in 1901 by a man who had heard it from a Hoosier relative while traveling in southern Tennessee. Dunn could not find any family of the given name in any directory in the region or anyone else in southern Tennessee who had heard the story and accounted himself dubious. This version was subsequently retold by Gov. Evan Bayh and Sen. Vance Hartke, who introduced the story into the "Congressional Record" in 1975, and matches the timing and location of Smith's subsequent research. However, the U.S. Army Corps of Engineers has been unable to find any record of a Hoosier or Hosier in surviving canal company records.

The word "hoosier" is still used in Greater St. Louis to denote a "yokel" or "white trash". The word is also encountered in sea shanties. In the book "Shanties from the Seven Seas" by Stan Hugill, in reference to its former use to denote cotton-stowers, who would move bales of cotton to and from the holds of ships and force them in tightly by means of jackscrews.

A Hoosier cabinet, often shortened to "hoosier", is a type of free-standing kitchen cabinet popular in the early decades of the twentieth century. Almost all of these cabinets were produced by companies located in Indiana and the name derives from the largest of them, the Hoosier Manufacturing Co. of New Castle, Indiana. Other Indiana businesses include Hoosier Racing Tire and the Hoosier Bat Company, manufacturer of wooden baseball bats.

The word may have originated from the term for shooing a pig away from its mate at feeding time.

The RCA Dome, former home of the Indianapolis Colts, was known as the "Hoosier Dome" before RCA purchased the naming rights in 1994. The RCA Dome was replaced by Lucas Oil Stadium in 2008.




</doc>
<doc id="14263" url="https://en.wikipedia.org/wiki?curid=14263" title="Horner's method">
Horner's method

In mathematics, Horner's rule (or Horner's method, Horner's scheme etc.) is a method for approximating the roots of polynomials that was described by William George Horner in 1819. It gave a convenient way for implementing the Newton–Raphson method for polynomials that was suitable for efficient hand calculation and was widely used until computers came into general use in about 1970. 

One aspect of this method was the use of synthetic division (aka Ruffini's rule) for implementing polynomial long division, and some elementary level texts use Horner's method as yet another name for this ancient trick which Horner ascribed to Joseph-Louis Lagrange but which can be traced back many hundreds of years to Chinese and Persian mathematicians.

After the introduction of computers Horner's root-finding method went out of use and as a result the term Horner's method (rule etc.) has often been used to mean just the algorithm for polynomial evaluation that underlies synthetic division.

The key idea behind this is the identity:
formula_1

This allows evaluation of a polynomial of degree with only formula_2 multiplications and formula_2 additions. This is optimal, since there are polynomials of degree that cannot be evaluated with fewer arithmetic operations 

Given the polynomial

where formula_5 are constant coefficients, the problem is to evaluate the polynomial at a specific value formula_6 of formula_7

For this, a new sequence of constants is defined recursively as follows:

Then formula_9 is the value of formula_10.

To see why this works, the polynomial can be written in the form

Thus, by iteratively substituting the formula_12 into the expression,

Now, it can be proven that;
This expression constitutes Horner's practical application, as it offers a very quick way of determining the outcome of;
with b (which is equal to p(x)) being the division's remainder, as is demonstrated by the examples below. if x is a root of p(x), then b = 0 (meaning the remainder is 0), which means you can factor p(x) with (x-x).<br>
As to finding the consecutive b-values, you start with determining b, which is simply equal to a. You then work your way down to the other b's, using the formula;
till you arrive at b.

Evaluate formula_17 for formula_18

We use synthetic division as follows:

The entries in the third row are the sum of those in the first two. Each entry in the second row is the product of the "x"-value (3 in this example) with the third-row entry immediately to the left. The entries in the first row are the coefficients of the polynomial to be evaluated. Then the remainder of formula_19 on division by formula_20 is 5.

But by the polynomial remainder theorem, we know that the remainder is formula_21. Thus formula_22

In this example, if formula_23 we can see that formula_24, the entries in the third row. So, synthetic division is based on Horner's method.

As a consequence of the polynomial remainder theorem, the entries in the third row are the coefficients of the second-degree polynomial, the quotient of formula_19 on division by formula_26. 
The remainder is 5. This makes Horner's method useful for polynomial long division.

Divide formula_27 by formula_28:

The quotient is formula_29.

Let formula_30 and formula_31. Divide formula_32 by formula_33 using Horner's method.
The third row is the sum of the first two rows, divided by 2. Each entry in the second row is the product of 1 with the third-row entry to the left. The answer is

Evaluation using the monomial form of a degree-"n" polynomial requires at most "n" additions and ("n" + "n")/2 multiplications, if powers are calculated by repeated multiplication and each monomial is evaluated individually. (This can be reduced to "n" additions and 2"n" − 1 multiplications by evaluating the powers of "x" iteratively.) If numerical data are represented in terms of digits (or bits), then the naive algorithm also entails storing approximately 2"n" times the number of bits of "x" (the evaluated polynomial has approximate magnitude "x", and one must also store "x" itself). By contrast, Horner's method requires only "n" additions and "n" multiplications, and its storage requirements are only "n" times the number of bits of "x". Alternatively, Horner's method can be computed with "n" fused multiply–adds. Horner's method can also be extended to evaluate the first "k" derivatives of the polynomial with "kn" additions and multiplications.

Horner's method is optimal, in the sense that any algorithm to evaluate an arbitrary polynomial must use at least as many operations. Alexander Ostrowski proved in 1954 that the number of additions required is minimal. Victor Pan proved in 1966 that the number of multiplications is minimal. However, when "x" is a matrix, Horner's method is not optimal.

This assumes that the polynomial is evaluated in monomial form and no preconditioning of the representation is allowed, which makes sense if the polynomial is evaluated only once. However, if preconditioning is allowed and the polynomial is to be evaluated many times, then faster algorithms are possible. They involve a transformation of the representation of the polynomial. In general, a degree-"n" polynomial can be evaluated using only +2 multiplications and "n" additions.

A disadvantage of Horner's rule is that all of the operations are sequentially dependent, so it is not possible to take advantage of instruction level parallelism on modern computers. In most applications where the efficiency of polynomial evaluation matters, many low-order polynomials are evaluated simultaneously (for each pixel or polygon in computer graphics, or for each grid square in a numerical simulation), so it is not necessary to find parallelism within a single polynomial evaluation.

If, however, one is evaluating a single polynomial of very high order, it may be useful to break it up as follows:

More generally, the summation can be broken into "k" parts:
where the inner summations may be evaluated using separate parallel instances of Horner's method. This requires slightly more operations than the basic Horner's method, but allows "k"-way SIMD execution of most of them.

Horner's method is a fast, code-efficient method for multiplication and division of binary numbers on a microcontroller with no hardware multiplier. One of the binary numbers to be multiplied is represented as a trivial polynomial, where (using the above notation) formula_37, and formula_38. Then, "x" (or "x" to some power) is repeatedly factored out. In this binary numeral system (base 2), formula_38, so powers of 2 are repeatedly factored out.

For example, to find the product of two numbers (0.15625) and "m":

To find the product of two binary numbers "d" and "m":

In general, for a binary number with bit values (formula_41) the product is
At this stage in the algorithm, it is required that terms with zero-valued coefficients are dropped, so that only binary coefficients equal to one are counted, thus the problem of multiplication or division by zero is not an issue, despite this implication in the factored equation:

The denominators all equal one (or the term is absent), so this reduces to
or equivalently (as consistent with the "method" described above)

In binary (base-2) math, multiplication by a power of 2 is merely a register shift operation. Thus, multiplying by 2 is calculated in base-2 by an arithmetic shift. The factor (2) is a right arithmetic shift, a (0) results in no operation (since 2 = 1 is the multiplicative identity element), and a (2) results in a left arithmetic shift.
The multiplication product can now be quickly calculated using only arithmetic shift operations, addition and subtraction.

The method is particularly fast on processors supporting a single-instruction shift-and-addition-accumulate. Compared to a C floating-point library, Horner's method sacrifices some accuracy, however it is nominally 13 times faster (16 times faster when the "canonical signed digit" (CSD) form is used) and uses only 20% of the code space.

Horner's method can be used to convert between different positional numeral systems – in which case "x" is the base of the number system, and the "a" coefficients are the digits of the base-"x" representation of a given number – and can also be used if "x" is a matrix, in which case the gain in computational efficiency is even greater. In fact, when "x" is a matrix, further acceleration is possible which exploits the structure of matrix multiplication, and only formula_46 instead of "n" multiplies are needed (at the expense of requiring more storage) using the 1973 method of Paterson and Stockmeyer.

Using the long division algorithm in combination with Newton's method, it is possible to approximate the real roots of a polynomial. The algorithm works as follows. Given a polynomial formula_47 of degree formula_2 with zeros formula_49 make some initial guess formula_50 such that formula_51. Now iterate the following two steps:


These two steps are repeated until all real zeros are found for the polynomial. If the approximated zeros are not precise enough, the obtained values can be used as initial guesses for Newton's method but using the full polynomial rather than the reduced polynomials.

Consider the polynomial

which can be expanded to

From the above we know that the largest root of this polynomial is 7 so we are able to make an initial guess of 8. Using Newton's method the first zero of 7 is found as shown in black in the figure to the right. Next formula_61 is divided by formula_62 to obtain

which is drawn in red in the figure to the right. Newton's method is used to find the largest zero of this polynomial with an initial guess of 7. The largest zero of this polynomial which corresponds to the second largest zero of the original polynomial is found at 3 and is circled in red. The degree 5 polynomial is now divided by formula_64 to obtain

which is shown in yellow. The zero for this polynomial is found at 2 again using Newton's method and is circled in yellow. Horner's method is now used to obtain

which is shown in green and found to have a zero at −3. This polynomial is further reduced to

which is shown in blue and yields a zero of −5. The final root of the original polynomial may be found by either using the final zero as an initial guess for Newton's method, or by reducing formula_68 and solving the linear equation. As can be seen, the expected roots of −8, −5, −3, 2, 3, and 7 were found.

Horner's method can be modified to compute the divided difference formula_69 Given the polynomial (as before)

proceed as follows

At completion, we have
This computation of the divided difference is subject to less
round-off error than evaluating formula_61 and formula_74 separately, particularly when
formula_75. Substituting
formula_76 in this method gives formula_77, the derivative of formula_61.

Horner's paper, titled "A new method of solving numerical equations of all orders, by continuous approximation", was read before the Royal Society of London, at its meeting on July 1, 1819, with Davies Gilbert, Vice-President and Treasurer, in the chair; this was the final meeting of the session before the Society adjourned for its Summer recess. When a sequel was read before the Society in 1823, it was again at the final meeting of the session. On both occasions, papers by James Ivory, FRS, were also read. In 1819, it was Horner's paper that got through to publication in the "Philosophical Transactions". later in the year, Ivory's paper falling by the way, despite Ivory being a Fellow; in 1823, when a total of ten papers were read, fortunes as regards publication, were reversed. Gilbert, who had strong connections with the West of England and may have had social contact with Horner, resident as Horner was in Bristol and Bath, published his own survey of Horner-type methods earlier in 1823.

Horner's paper in Part II of "Philosophical Transactions of the Royal Society of London" for 1819 was warmly and expansively welcomed by a reviewer in the issue of "The Monthly Review: or, Literary Journal" for April, 1820; in comparison, a technical paper by Charles Babbage is dismissed curtly in this review. However, the reviewer noted that another, similar method had also recently been published by the architect and mathematical expositor, Peter Nicholson. This theme is developed in a further review of some of Nicholson's books in the issue of "The Monthly Review" for December, 1820, which in turn ends with notice of the appearance of a booklet by Theophilus Holdred, from whom Nicholson acknowledges he obtained the gist of his approach in the first place, although claiming to have improved upon it. The sequence of reviews is concluded in the issue of "The Monthly Review" for September, 1821, with the reviewer concluding that whereas Holdred was the first person to discover a direct and general practical solution of numerical equations, he had not reduced it to its simplest form by the time of Horner's publication, and saying that had Holdred published forty years earlier when he first discovered his method, his contribution could be more easily recognized. The reviewer is exceptionally well-informed, even having cited Horner's preparatory correspondence with Peter Barlow in 1818, seeking work of Budan. The Bodlean Library, Oxford has the Editor's annotated copy of "The Monthly Review" from which it is clear that the most active reviewer in mathematics in 1814 and 1815 (the last years for which this information has been published) was none other than Peter Barlow, one of the foremost specialists on approximation theory of the period, suggesting that it was Barlow, who wrote this sequence of reviews. As it also happened, Henry Atkinson, of Newcastle, devised a similar approximation scheme in 1809; he had consulted his fellow Geordie, Charles Hutton, another specialist and a senior colleague of Barlow at the Royal Military Academy, Woolwich, only to be advised that, while his work was publishable, it was unlikely to have much impact. J. R. Young, writing in the mid-1830s, concluded that Holdred's first method replicated Atkinson's while his improved method was only added to Holdred's booklet some months after its first appearance in 1820, when Horner's paper was already in circulation.

The feature of Horner's writing that most distinguishes it from his English contemporaries is the way he draws on the Continental literature, notably the work of Arbogast. The advocacy, as well as the detraction, of Horner's Method has this as an unspoken subtext. Quite how he gained that familiarity has not been determined. Horner is known to have made a close reading of John Bonneycastle's book on algebra. Bonneycastle recognizes that Arbogast has the general, combinatorial expression for the reversion of series, a project going back at least to Newton. But Bonneycastle's main purpose in mentioning Arbogast is not to praise him, but to observe that Arbogast's notation is incompatible with the approach he adopts. The gap in Horner's reading was the work of Paolo Ruffini, except that, as far as awareness of Ruffini goes, citations of Ruffini's work by authors, including medical authors, in "Philosophical Transactions" speak volumes, as there are none — Ruffini's name only appears in 1814, recording a work he donated to the Royal Society. Ruffini might have done better if his work had appeared in French, as had Malfatti's Problem in the reformulation of Joseph Diez Gergonne, or had he written in French, as had , a source quoted by Bonneycastle on series reversion. (Today, Cagnoli is in the Italian Wikipedia, as shown, but has yet to make it into either French or English.)

Fuller showed that the method in Horner's 1819 paper differs from what afterwards became known as "Horner's method" and that in consequence the priority for this method should go to Holdred (1920). This view may be compared with the remarks concerning the works of Horner and Holdred in the previous paragraph. Fuller also takes aim at Augustus De Morgan. Precocious though Augustus de Morgan was, he was not the reviewer for "The Monthly Review", while several others — Thomas Stephens Davies, J. R. Young, Stephen Fenwick, T. T. Wilkinson — wrote Horner firmly into their records, not least Horner, as he published extensively up until the year of his death in 1837. His paper in 1819 was one that would have been difficult to miss. In contrast, the only other mathematical sighting of Holdred is a single named contribution to "The Gentleman's Mathematical Companion", an answer to a problem.

It is questionable to what extent it was De Morgan's advocacy of Horner's priority in discovery that led to "Horner's method" being so called in textbooks, but it is true that those suggesting this tend to know of Horner largely through intermediaries, of whom De Morgan made himself a prime example. However, this method "qua" method was known long before Horner. In reverse chronological order, Horner's method was already known to:


However, this observation on its own masks significant differences in conception and also, as noted with Ruffini's work, issues of accessibility.

Qin Jiushao, in his "Shu Shu Jiu Zhang" ("Mathematical Treatise in Nine Sections"; 1247), presents a portfolio of methods of Horner-type for solving polynomial equations, which was based on earlier works of the 11th century Song dynasty mathematician Jia Xian; for example, one method is specifically suited to bi-quintics, of which Qin gives an instance, in keeping with the then Chinese custom of case studies. The first person writing in English to note the connection with Horner's method was Alexander Wylie, writing in "The North China Herald" in 1852; perhaps conflating and misconstruing different Chinese phrases, Wylie calls the method "Harmoniously Alternating Evolution" (which does not agree with his Chinese, "linglong kaifang", not that at that date he uses pinyin), working the case of one of Qin's quartics and giving, for comparison, the working with Horner's method. Yoshio Mikami in "Development of Mathematics in China and Japan" published in Leipzig in 1913, gave a detailed description of Qin's method, using the quartic illustrated to the above right in a worked example; he wrote:

However, as Mikami is also aware, it was "not altogether impossible" that a related work, "Si Yuan Yu Jian" ("Jade Mirror of the Four Unknowns; 1303)" by Zhu Shijie might make the shorter journey across to Japan, but seemingly it never did, although another work of Zhu, "Suan Xue Qi Meng", had a seminal influence on the development of traditional mathematics in the Edo period, starting in the mid-1600s. Ulrich Libbrecht (at the time teaching in school, but subsequently a professor of comparative philosophy) gave a detailed description in his doctoral thesis of Qin's method, he concluded: "It is obvious that this procedure is a Chinese invention ... the method was not known in India". He said, Fibonacci probably learned of it from Arabs, who perhaps borrowed from the Chinese. Here, the problems is that there is no more evidence for this speculation than there is of the method being known in India. Of course, the extraction of square and cube roots along similar lines is already discussed by Liu Hui in connection with Problems IV.16 and 22 in "Jiu Zhang Suan Shu", while Wang Xiaotong in the 7th century supposes his readers can solve cubics by an approximation method described in his book Jigu Suanjing.





</doc>
<doc id="14268" url="https://en.wikipedia.org/wiki?curid=14268" title="Hapworth 16, 1924">
Hapworth 16, 1924

"Hapworth 16, 1924" was the last original work J. D. Salinger published in his lifetime. It appeared in the June 19, 1965, edition of "The New Yorker", infamously taking up almost the entire magazine. It is the "youngest" of Salinger's Glass family stories, in the sense that the narrated events happen chronologically before those in the rest of the series. 

Both contemporary and later literary critics harshly panned "Hapworth 16, 1924"; writing in "The New York Times", Michiko Kakutani called it "a sour, implausible and, sad to say, completely charmless story ... filled with digressions, narcissistic asides and ridiculous shaggy-dog circumlocutions." Even kind critics have regarded the work as "a long-winded sob story" that many have found "simply unreadable", and it has been speculated that this response was the reason Salinger decided to quit publishing. But Salinger is also said to have considered the story a "high point of his writing" and made tentative steps to have it reprinted, though those came to nothing.

The story is presented in the form of a letter from camp written by a seven-year-old Seymour Glass (the main character of "A Perfect Day for Bananafish"). In this respect, the plot is identical to Salinger's previous unpublished story "The Ocean Full of Bowling Balls", written 18 years earlier in 1947. In the course of requesting an inordinate quantity of reading matter from home, Seymour predicts his brother's success as a writer as well as his own death and offers critical assessments of a number of major writers.

After the story's appearance in "The New Yorker", Salinger—who had already withdrawn to his home in New Hampshire—stopped publishing altogether. Since the story never appeared in book form, readers had to seek out that issue or find it on microfilm. Finally, with the release of "The Complete New Yorker" on DVD in 2005, the story was once again widely available.

In 1996, Orchises Press, a small Virginia publishing house, started a process of publishing "Hapworth" in book form. Orchises Press owner Roger Lathbury has described the effort in "The Washington Post" and, three months after Salinger's death, in "New York" magazine"." According to Lathbury, Salinger was deeply concerned with the proposed book's appearance, even visiting Washington to examine the cloth for the binding. Salinger also sent Lathbury numerous "infectious and delightful and loving" letters.

Following publishing norms, Lathbury applied for Library of Congress Cataloging in Publication data, unaware of how publicly available the information would be. A writer in Seattle, researching an article on Jeff Bezos, came across the "Hapworth" publication date, and told his sister, a journalist for the "Washington Business Journal", who wrote an article about the upcoming book. This led to substantial coverage in the press. Shortly before the books were to be shipped, Salinger changed his mind, and Orchises withdrew the book. New publication dates were repeatedly announced, but it never appeared. Lathbury said, "I never reached back out. I thought about writing some letters, but it wouldn't have done any good."




</doc>
<doc id="14269" url="https://en.wikipedia.org/wiki?curid=14269" title="Hypnotic">
Hypnotic

Hypnotic (from Greek "Hypnos", sleep), or soporific drugs, commonly known as sleeping pills, are a class of psychoactive drugs whose primary function is to induce sleep and for the treatment of insomnia (sleeplessness), or for surgical anesthesia.

This group is related to sedatives. Whereas the term "sedative" describes drugs that serve to calm or relieve anxiety, the term "hypnotic" generally describes drugs whose main purpose is to initiate, sustain, or lengthen sleep. Because these two functions frequently overlap, and because drugs in this class generally produce dose-dependent effects (ranging from anxiolysis to loss of consciousness) they are often referred to collectively as sedative-hypnotic drugs.

Hypnotic drugs are regularly prescribed for insomnia and other sleep disorders, with over 95% of insomnia patients being prescribed hypnotics in some countries. Many hypnotic drugs are habit-forming and, due to many factors known to disturb the human sleep pattern, a physician may instead recommend changes in the environment before and during sleep, better sleep hygiene, the avoidance of caffeine or other stimulating substances, or behavioral interventions such as cognitive behavioral therapy for insomnia (CBT-I) before prescribing medication for sleep. When prescribed, hypnotic medication should be used for the shortest period of time necessary.

Among individuals with sleep disorders, 13.7% are taking or prescribed nonbenzodiazepines, while 10.8% are taking benzodiazepines, as of 2010, in the USA. Early classes of drugs, such as barbiturates, have fallen out of use in most practices but are still prescribed for some patients. In children, prescribing hypnotics is not yet acceptable unless used to treat night terrors or sleepwalking. Elderly people are more sensitive to potential side effects of daytime fatigue and cognitive impairments, and a meta-analysis found that the risks generally outweigh any marginal benefits of hypnotics in the elderly. A review of the literature regarding benzodiazepine hypnotics and Z-drugs concluded that these drugs can have adverse effects, such as dependence and accidents, and that optimal treatment uses the lowest effective dose for the shortest therapeutic time period, with gradual discontinuation in order to improve health without worsening of sleep.

Falling outside the above-mentioned categories, the neuro-hormone melatonin has a hypnotic function.

Hypnotica was a class of somniferous drugs and substances tested in medicine of the 1890s and later, including: Urethan, Acetal, Methylal, Sulfonal, paraldehyde, Amylenhydrate, Hypnon, Chloralurethan and Ohloralamid or Chloralimid.

Research about using medications to treat insomnia evolved throughout the last half of the 20th century. Treatment for insomnia in psychiatry dates back to 1869 when chloral hydrate was first used as a soporific. Barbiturates emerged as the first class of drugs that emerged in the early 1900s, after which chemical substitution allowed derivative compounds. Although the best drug family at the time (less toxic and with fewer side effects) they were dangerous in overdose and tended to cause physical and psychological dependence.

During the 1970s, quinazolinones and benzodiazepines were introduced as safer alternatives to replace barbiturates; by the late 1970s benzodiazepines emerged as the safer drug.

Benzodiazepines are not without their drawbacks; substance dependence is possible, and deaths from overdoses sometimes occur, especially in combination with alcohol and/or other depressants. Questions have been raised as to whether they disturb sleep architecture.

Nonbenzodiazepines are the most recent development (1990s–present). Although it's clear that they are less toxic than their predecessors, barbiturates, comparative efficacy over benzodiazepines have not been established. Without longitudinal studies, it is hard to determine; however some psychiatrists recommend these drugs, citing research suggesting they are equally potent with less potential for abuse.

Other sleep remedies that may be considered "sedative-hypnotics" exist; psychiatrists will sometimes prescribe medicines off-label if they have sedating effects. Examples of these include mirtazapine (an antidepressant), clonidine (generally prescribed to regulate blood pressure), quetiapine (an antipsychotic), and the over-the-counter sleep aid diphenhydramine (Benadryl – an antihistamine). Off-label sleep remedies are particularly useful when first-line treatment is unsuccessful or deemed unsafe (for example, in patients with a history of substance abuse).

Barbiturates are drugs that act as central nervous system depressants, and can therefore produce a wide spectrum of effects, from mild sedation to total anesthesia. They are also effective as anxiolytics, hypnotics, and anticonvulsalgesic effects; however, these effects are somewhat weak, preventing barbiturates from being used in surgery in the absence of other analgesics. They have dependence liability, both physical and psychological. Barbiturates have now largely been replaced by benzodiazepines in routine medical practice – for example, in the treatment of anxiety and insomnia – mainly because benzodiazepines are significantly less dangerous in overdose. However, barbiturates are still used in general anesthesia, for epilepsy, and assisted suicide. Barbiturates are derivatives of barbituric acid.

The principal mechanism of action of barbiturates is believed to be positive allosteric modulation of GABA receptors.

Examples include amobarbital, pentobarbital, phenobarbital, secobarbital, and sodium thiopental.

Quinazolinones are also a class of drugs which function as hypnotic/sedatives that contain a 4-quinazolinone core. Their use has also been proposed in the treatment of cancer.

Examples of quinazolinones include cloroqualone, diproqualone, etaqualone (Aolan, Athinazone, Ethinazone), mebroqualone, mecloqualone (Nubarene, Casfen), and methaqualone (Quaalude).

Benzodiazepines can be useful for short-term treatment of insomnia. Their use beyond 2 to 4 weeks is not recommended due to the risk of dependence. It is preferred that benzodiazepines be taken intermittently and at the lowest effective dose. They improve sleep-related problems by shortening the time spent in bed before falling asleep, prolonging the sleep time, and, in general, reducing wakefulness. Like alcohol, benzodiazepines are commonly used to treat insomnia in the short-term (both prescribed and self-medicated), but worsen sleep in the long-term. While benzodiazepines can put people to sleep (i.e., inhibit NREM stage 1 and 2 sleep), while asleep, the drugs disrupt sleep architecture: decreasing sleep time, delaying time to REM sleep, and decreasing deep slow-wave sleep (the most restorative part of sleep for both energy and mood).

Other drawbacks of hypnotics, including benzodiazepines, are possible tolerance to their effects, rebound insomnia, and reduced slow-wave sleep and a withdrawal period typified by rebound insomnia and a prolonged period of anxiety and agitation. The list of benzodiazepines approved for the treatment of insomnia is fairly similar among most countries, but which benzodiazepines are officially designated as first-line hypnotics prescribed for the treatment of insomnia can vary distinctly between countries. Longer-acting benzodiazepines such as nitrazepam and diazepam have residual effects that may persist into the next day and are, in general, not recommended.

It is not clear as to whether the new nonbenzodiazepine hypnotics (Z-drugs) are better than the short-acting benzodiazepines. The efficacy of these two groups of medications is similar. According to the US Agency for Healthcare Research and Quality, indirect comparison indicates that side-effects from benzodiazepines may be about twice as frequent as from nonbenzodiazepines. Some experts suggest using nonbenzodiazepines preferentially as a first-line long-term treatment of insomnia. However, the UK National Institute for Health and Clinical Excellence (NICE) did not find any convincing evidence in favor of Z-drugs. A NICE review pointed out that short-acting Z-drugs were inappropriately compared in clinical trials with long-acting benzodiazepines. There have been no trials comparing short-acting Z-drugs with appropriate doses of short-acting benzodiazepines. Based on this, NICE recommended choosing the hypnotic based on cost and the patient's preference.

Older adults should not use benzodiazepines to treat insomnia unless other treatments have failed to be effective. When benzodiazepines are used, patients, their caretakers, and their physician should discuss the increased risk of harms, including evidence which shows twice the incidence of traffic collisions among driving patients as well as falls and hip fracture for all older patients.

Their mechanism of action is primarily at GABA receptors.

Nonbenzodiazepines are a class of psychoactive drugs that are very "benzodiazepine-like" in nature. Nonbenzodiazepine pharmacodynamics are almost entirely the same as benzodiazepine drugs and therefore entail similar benefits, side-effects, and risks. Nonbenzodiazepines, however, have dissimilar or entirely different chemical structures, and therefore are unrelated to benzodiazepines on a molecular level.

Examples include zopiclone (Imovane, Zimovane), eszopiclone (Lunesta), zaleplon (Sonata), and zolpidem (Ambien, Stilnox, Stilnoct).

Research on nonbenzodiazepines is new and conflicting. A review by a team of researchers suggests the use of these drugs for people that have trouble falling asleep but not staying asleep, as next-day impairments were minimal. The team noted that the safety of these drugs had been established, but called for more research into their long-term effectiveness in treating insomnia. Other evidence suggests that tolerance to nonbenzodiazepines may be slower to develop than with benzodiazepines. A different team was more skeptical, finding little benefit over benzodiazepines.

Melatonin, the hormone produced in the pineal gland in the brain and secreted in dim light and darkness, among its other functions, promotes sleep in diurnal mammals.

In common use, the term "antihistamine" refers only to compounds that inhibit action at the H receptor (and not H, etc.).

Clinically, H antagonists are used to treat certain allergies. Sedation is a common side-effect, and some H antagonists, such as diphenhydramine (Benadryl) and doxylamine, are also used to treat insomnia.

Second-generation antihistamines cross the blood–brain barrier to a much lower degree than the first ones. This results in their primarily affecting peripheral histamine receptors, and therefore having a much lower sedative effect. High doses can still induce the central nervous system effect of drowsiness.

Some antidepressants have sedating effects.

Examples include:

Examples of antipsychotics with sedation as a side effect:






The use of sedative medications in older people generally should be avoided. These medications are associated with poorer health outcomes, including cognitive decline.

Therefore, sedatives and hypnotics should be avoided in people with dementia according to the clinical guidelines known as the Medication Appropriateness Tool for Comorbid Health Conditions in Dementia (MATCH-D). The use of these medications can further impede cognitive function for people with dementia, who are also more sensitive to side effects of medications. 





</doc>
<doc id="14273" url="https://en.wikipedia.org/wiki?curid=14273" title="HMS Dunraven">
HMS Dunraven

HMS "Dunraven" was a Q-Ship of the Royal Navy during World War I.

On 8 August 1917, 130 miles southwest of Ushant in the Bay of Biscay, disguised as the collier "Boverton" and commanded by Gordon Campbell, VC, "Dunraven" spotted , commanded by "Oberleutnant zur See" Reinhold Saltzwedel. Saltzwedel believed the disguised ship was a merchant vessel. The U-boat submerged and closed with "Dunraven" before surfacing astern at 11:43 am and opening fire at long range. "Dunraven" made smoke and sent off a panic party (a small number of men who "abandon ship" during an attack to continue the impersonation of a merchant).

Shells began hitting "Dunraven", detonating her depth charges and setting her stern afire. Her crew remained hidden letting the fires burn. Then a 4-inch (102 mm) gun and crew were blown away revealing "Dunraven"s identity as a warship, and "UC-71" submerged. A second "panic party" abandoned ship. "Dunraven" was hit by a torpedo. A third "panic party" went over the side, leaving only two guns manned. "UC-71" surfaced, shelled "Dunraven" and again submerged. Campbell replied with two torpedoes that missed, and around 3 pm, the undamaged U-boat left that area. Only one of "Dunraven"s crew was killed, but the Q-Ship was sinking.

The British destroyer picked up "Dunraven"s survivors and took her in tow for Plymouth, but "Dunraven" sank at 1:30 am early on 10 August 1917 to the north of Ushant.

In recognition, two Victoria Crosses were awarded, one to the ship's First Lieutenant, Lt. Charles George Bonner RNR, and the other, by ballot, to a gunlayer, Petty Officer Ernest Herbert Pitcher.

Captain Campbell later wrote:

Captain Campbell had been previously awarded the Victoria Cross, in February 1917, for the sinking of .


</doc>
<doc id="14275" url="https://en.wikipedia.org/wiki?curid=14275" title="Hacker ethic">
Hacker ethic

The hacker ethic is a philosophy and set of moral values that is common within hacker culture. Practitioners of the hacker ethic believe that sharing information and data with others is an ethical imperative. The hacker ethic is related to the concept of freedom of information, as well as the political theories of liberalism, anarchism, and libertarianism.

While some tenets of the hacker ethic were described in other texts like "Computer Lib/Dream Machines" (1974) by Ted Nelson, the term "hacker ethic" is generally attributed to journalist Steven Levy, who appears to have been the first to document both the philosophy and the founders of the philosophy in his 1984 book titled "."

The hacker ethic originated at the Massachusetts Institute of Technology in the 1950s–1960s. The term "hacker" had long been used there to describe college pranks that MIT students would regularly devise, and was used more generally to describe a project undertaken or a product built to fulfill some constructive goal but also out of pleasure for mere involvement.

MIT housed an early IBM 704 computer inside the Electronic Accounting Machinery (EAM) room in 1959. This room became the staging grounds for early hackers, as MIT students from the Tech Model Railroad Club sneaked inside the EAM room after hours to attempt programming the 30-ton, computer.

The hacker ethic was described as a "new way of life, with a philosophy, an ethic and a dream". However, the elements of the hacker ethic were not openly debated and discussed; rather they were implicitly accepted and silently agreed upon.

The free software movement was born in the early 1980s from followers of the hacker ethic. Its founder, Richard Stallman, is referred to by Steven Levy as "the last true hacker".

Richard Stallman describes:

The hacker ethic refers to the feelings of right and wrong, to the ethical ideas this community of people had—that knowledge should be shared with other people who can benefit from it, and that important resources should be utilized rather than wasted.

and states more precisely that hacking (which Stallman defines as playful cleverness) and ethics are two separate issues:

Just because someone enjoys hacking does not mean he has an ethical commitment to treating other people properly. Some hackers care about ethics—I do, for instance—but that is not part of being a hacker, it is a separate trait. [...] Hacking is not primarily about an ethical issue. [...] hacking tends to lead a significant number of hackers to think about ethical questions in a certain way. I would not want to completely deny all connection between hacking and views on ethics.

As Levy summarized in the preface of "Hackers", the general tenets or principles of hacker ethic include:


In addition to those principles, Levy also described more specific hacker ethics and beliefs in chapter 2, "The Hacker Ethic": The ethics he described in chapter 2 are:


From the early days of modern computing through to the 1970s, it was far more common for computer users to have the freedoms that are provided by an ethic of open sharing and collaboration. Software, including source code, was commonly shared by individuals who used computers. Most companies had a business model based on hardware sales, and provided or bundled the associated software free of charge. According to Levy's account, sharing was the norm and expected within the non-corporate hacker culture. The principle of sharing stemmed from the open atmosphere and informal access to resources at MIT. During the early days of computers and programming, the hackers at MIT would develop a program and share it with other computer users.

If the hack was deemed particularly good, then the program might be posted on a board somewhere near one of the computers. Other programs that could be built upon it and improved it were saved to tapes and added to a drawer of programs, readily accessible to all the other hackers. At any time, a fellow hacker might reach into the drawer, pick out the program, and begin adding to it or "bumming" it to make it better. Bumming referred to the process of making the code more concise so that more can be done in fewer instructions, saving precious memory for further enhancements.

In the second generation of hackers, sharing was about sharing with the general public in addition to sharing with other hackers. A particular organization of hackers that was concerned with sharing computers with the general public was a group called Community Memory. This group of hackers and idealists put computers in public places for anyone to use. The first community computer was placed outside of Leopold's Records in Berkeley, California.

Another sharing of resources occurred when Bob Albrecht provided considerable resources for a non-profit organization called the People's Computer Company (PCC). PCC opened a computer center where anyone could use the computers there for fifty cents per hour.

This second generation practice of sharing contributed to the battles of free and open software. In fact, when Bill Gates' version of BASIC for the Altair was shared among the hacker community, Gates claimed to have lost a considerable sum of money because few users paid for the software. As a result, Gates wrote an Open Letter to Hobbyists. This letter was published by several computer magazines and newsletters, most notably that of the Homebrew Computer Club where much of the sharing occurred.

Many of the principles and tenets of hacker ethic contribute to a common goal: the Hands-On Imperative. As Levy described in Chapter 2, "Hackers believe that essential lessons can be learned about the systems—about the world—from taking things apart, seeing how they work, and using this knowledge to create new and more interesting things."

Employing the Hands-On Imperative requires free access, open information, and the sharing of knowledge. To a true hacker, if the Hands-On Imperative is restricted, then the ends justify the means to make it unrestricted "so that improvements can be made". When these principles are not present, hackers tend to work around them. For example, when the computers at MIT were protected either by physical locks or login programs, the hackers there systematically worked around them in order to have access to the machines. Hackers assumed a "willful blindness" in the pursuit of perfection.

This behavior was not malicious in nature: the MIT hackers did not seek to harm the systems or their users. This deeply contrasts with the modern, media-encouraged image of hackers who crack secure systems in order to steal information or complete an act of cyber-vandalism.

Throughout writings about hackers and their work processes, a common value of community and collaboration is present. For example, in Levy's "Hackers", each generation of hackers had geographically based communities where collaboration and sharing occurred. For the hackers at MIT, it was the labs where the computers were running. For the hardware hackers (second generation) and the game hackers (third generation) the geographic area was centered in Silicon Valley where the Homebrew Computer Club and the People's Computer Company helped hackers network, collaborate, and share their work.

The concept of community and collaboration is still relevant today, although hackers are no longer limited to collaboration in geographic regions. Now collaboration takes place via the Internet. Eric S. Raymond identifies and explains this conceptual shift in "The Cathedral and the Bazaar":

Before cheap Internet, there were some geographically compact communities where the culture encouraged Weinberg's egoless programming, and a developer could easily attract a lot of skilled kibitzers and co-developers. Bell Labs, the MIT AI and LCS labs, UC Berkeley: these became the home of innovations that are legendary and still potent.

Raymond also notes that the success of Linux coincided with the wide availability of the World Wide Web. The value of community is still in high practice and use today.

Levy identifies several "true hackers" who significantly influenced the hacker ethic. Some well-known "true hackers" include:


Levy also identified the "hardware hackers" (the "second generation", mostly centered in Silicon Valley) and the "game hackers" (or the "third generation"). All three generations of hackers, according to Levy, embodied the principles of the hacker ethic. Some of Levy's "second-generation" hackers include:


Levy's "third generation" practitioners of hacker ethic include:


In 2001, Finnish philosopher Pekka Himanen promoted the hacker ethic in opposition to the Protestant work ethic. In Himanen's opinion, the hacker ethic is more closely related to the virtue ethics found in the writings of Plato and of Aristotle. Himanen explained these ideas in a book, "The Hacker Ethic and the Spirit of the Information Age", with a prologue contributed by Linus Torvalds and an epilogue by Manuel Castells.

In this manifesto, the authors wrote about a hacker ethic centering on passion, hard work, creativity and joy in creating software. Both Himanen and Torvalds were inspired by the Sampo in Finnish mythology. The Sampo, described in the Kalevala saga, was a magical artifact constructed by Ilmarinen, the blacksmith god, that brought good fortune to its holder; nobody knows exactly what it was supposed to be. The Sampo has been interpreted in many ways: a world pillar or world tree, a compass or astrolabe, a chest containing a treasure, a Byzantine coin die, a decorated Vendel period shield, a Christian relic, etc. Kalevala saga compiler Lönnrot interpreted it to be a "quern" or mill of some sort that made flour, salt, and gold out of thin air.





</doc>
<doc id="14276" url="https://en.wikipedia.org/wiki?curid=14276" title="Hotel">
Hotel

A hotel is an establishment that provides paid lodging on a short-term basis. Facilities provided inside a hotel room may range from a modest-quality mattress in a small room to large suites with bigger, higher-quality beds, a dresser, a refrigerator and other kitchen facilities, upholstered chairs, a flat screen television, and en-suite bathrooms. Small, lower-priced hotels may offer only the most basic guest services and facilities. Larger, higher-priced hotels may provide additional guest facilities such as a swimming pool, business centre (with computers, printers, and other office equipment), childcare, conference and event facilities, tennis or basketball courts, gymnasium, restaurants, day spa, and social function services. Hotel rooms are usually numbered (or named in some smaller hotels and B&Bs) to allow guests to identify their room. Some boutique, high-end hotels have custom decorated rooms. Some hotels offer meals as part of a room and board arrangement. In the United Kingdom, a hotel is required by law to serve food and drinks to all guests within certain stated hours. In Japan, capsule hotels provide a tiny room suitable only for sleeping and shared bathroom facilities.

The precursor to the modern hotel was the inn of medieval Europe. For a period of about 200 years from the mid-17th century, coaching inns served as a place for lodging for coach travelers. Inns began to cater to richer clients in the mid-18th century. One of the first hotels in a modern sense was opened in Exeter in 1768. Hotels proliferated throughout Western Europe and North America in the early 19th century, and luxury hotels began to spring up in the later part of the 19th century.

Hotel operations vary in size, function, complexity, and cost. Most hotels and major hospitality companies have set industry standards to classify hotel types. An upscale full-service hotel facility offers luxury amenities, full-service accommodations, an on-site restaurant, and the highest level of personalized service, such as a concierge, room service, and clothes pressing staff. Full-service hotels often contain upscale full-service facilities with many full-service accommodations, an on-site full-service restaurant, and a variety of on-site amenities. Boutique hotels are smaller independent, non-branded hotels that often contain upscale facilities. Small to medium-sized hotel establishments offer a limited amount of on-site amenities. Economy hotels are small to medium-sized hotel establishments that offer basic accommodations with little to no services. Extended stay hotels are small to medium-sized hotels that offer longer-term full-service accommodations compared to a traditional hotel.

Timeshare and destination clubs are a form of property ownership involving ownership of an individual unit of accommodation for seasonal usage. A motel is a small-sized low-rise lodging with direct access to individual rooms from the car park. Boutique hotels are typically hotels with a unique environment or intimate setting. A number of hotels have entered the public consciousness through popular culture, such as the Ritz Hotel in London. Some hotels are built specifically as destinations in themselves, for example casinos and holiday resorts.

Most hotel establishments are run by a General Manager who serves as the head executive (often referred to as the "Hotel Manager"), department heads who oversee various departments within a hotel (e.g., food service), middle managers, administrative staff, and line-level supervisors. The organizational chart and volume of job positions and hierarchy varies by hotel size, function and class, and is often determined by hotel ownership and managing companies.

The word "hotel" is derived from the French "hôtel" (coming from the same origin as "hospital"), which referred to a French version of a building seeing frequent visitors, and providing care, rather than a place offering accommodation. In contemporary French usage, "hôtel" now has the same meaning as the English term, and "hôtel particulier" is used for the old meaning, as well as "hôtel" in some place names such as Hôtel-Dieu (in Paris), which has been a hospital since the Middle Ages. The French spelling, with the circumflex, was also used in English, but is now rare. The circumflex replaces the 's' found in the earlier "hostel" spelling, which over time took on a new, but closely related meaning. Grammatically, hotels usually take the definite article – hence "The Astoria Hotel" or simply "The Astoria."

Facilities offering hospitality to travellers have been a feature of the earliest civilizations. In Greco-Roman culture and ancient Persia, hospitals for recuperation and rest were built at thermal baths. Japan's Nishiyama Onsen Keiunkan, founded in 705, was officially recognised by the Guinness World Records as the oldest hotel in the world. During the Middle Ages, various religious orders at monasteries and abbeys would offer accommodation for travellers on the road.

The precursor to the modern hotel was the inn of medieval Europe, possibly dating back to the rule of Ancient Rome. These would provide for the needs of travellers, including food and lodging, stabling and fodder for the traveller's horse(s) and fresh horses for the mail coach. Famous London examples of inns include the George and the Tabard. A typical layout of an inn had an inner court with bedrooms on the two sides, with the kitchen and parlour at the front and the stables at the back.

For a period of about 200 years from the mid-17th century, coaching inns served as a place for lodging for coach travellers (in other words, a roadhouse). Coaching inns stabled teams of horses for stagecoaches and mail coaches and replaced tired teams with fresh teams. Traditionally they were seven miles apart, but this depended very much on the terrain.
Some English towns had as many as ten such inns and rivalry between them was intense, not only for the income from the stagecoach operators but for the revenue for food and drink supplied to the wealthy passengers. By the end of the century, coaching inns were being run more professionally, with a regular timetable being followed and fixed menus for food.

Inns began to cater to richer clients in the mid-18th century, and consequently grew in grandeur and the level of service provided. One of the first hotels in a modern sense was opened in Exeter in 1768, although the idea only really caught on in the early 19th century. In 1812, Mivart's Hotel opened its doors in London, later changing its name to Claridge's.

Hotels proliferated throughout Western Europe and North America in the 19th century. Luxury hotels, including the 1829 Tremont House in Boston, the 1836 Astor House in New York City, the 1889 Savoy Hotel in London, and the Ritz chain of hotels in London and Paris in the late 1890s, catered to an ever more wealthy clientele.

Hotels cater to travelers from many countries and languages, since no one country dominates the travel industry.

Hotel operations vary in size, function, and cost. Most hotels and major hospitality companies that operate hotels have set widely accepted industry standards to classify hotel types. General categories include the following:

International luxury hotels offer high-quality amenities, full-service accommodations, on-site full-service restaurants, and the highest level of personalized and professional service in major or capital cities. International luxury hotels are classified with at least a Five Diamond rating or Five Star hotel rating depending on the country and local classification standards. "Examples include: Grand Hyatt, Conrad, InterContinental, Sofitel, Mandarin Oriental, Four Seasons, The Peninsula, Rosewood, JW Marriott and The Ritz-Carlton."

Lifestyle luxury resorts are branded hotels that appeal to a guest with lifestyle or personal image in specific locations. They are typically full-service and classified as luxury. A key characteristic of lifestyle resorts is focus on providing a unique guest experience as opposed to simply providing lodging. Normally, lifestyle resorts are classified with a Five Star hotel rating depending on the country and local classification standards. Examples include: Waldorf Astoria, St. Regis, Shangri-La, Oberoi, Belmond, Jumeirah, Aman, Taj Hotels, Hoshino, Raffles, Fairmont, Banyan Tree, Regent and Park Hyatt.

Upscale full-service hotels often provide a wide array of guest services and on-site facilities. Commonly found amenities may include: on-site food and beverage (room service and restaurants), meeting and conference services and facilities, fitness center, and business center. Upscale full-service hotels range in quality from upscale to luxury. This classification is based upon the quality of facilities and amenities offered by the hotel. Examples include: W Hotels, Sheraton, Langham, Kempinski, 
Kimpton Hotels, Hilton, Lotte, Renaissance, Marriott and Hyatt Regency brands.

Boutique hotels are smaller independent non-branded hotels that often contain mid-scale to upscale facilities of varying size in unique or intimate settings with full-service accommodations. These hotels are generally 100 rooms or fewer.

Small to medium-sized hotel establishments that offer a limited number of on-site amenities that only cater and market to a specific demographic of travelers, such as the single business traveler. Most focused or select service hotels may still offer full-service accommodations but may lack leisure amenities such as an on-site restaurant or a swimming pool. Examples include Hyatt Place, Holiday Inn, Courtyard by Marriott and Hilton Garden Inn.

Small to medium-sized hotel establishments that offer a very limited number of on-site amenities and often only offer basic accommodations with little to no services, these facilities normally only cater and market to a specific demographic of travelers, such as the budget-minded traveler seeking a "no frills" accommodation. Limited service hotels often lack an on-site restaurant but in return may offer a limited complimentary food and beverage amenity such as on-site continental breakfast service. Examples include Ibis Budget, Hampton Inn, Aloft, Holiday Inn Express, Fairfield Inn, Four Points by Sheraton.

Extended stay hotels are small to medium-sized hotels that offer longer-term full-service accommodations compared to a traditional hotel. Extended stay hotels may offer non-traditional pricing methods such as a weekly rate that caters towards travelers in need of short-term accommodations for an extended period of time. Similar to limited and select service hotels, on-site amenities are normally limited and most extended stay hotels lack an on-site restaurant. Examples include Staybridge Suites, Candlewood Suites, Homewood Suites by Hilton, Home2 Suites by Hilton, Residence Inn by Marriott, Element, and Extended Stay America.

Timeshare and Destination clubs are a form of property ownership also referred to as a vacation ownership involving the purchase and ownership of an individual unit of accommodation for seasonal usage during a specified period of time. Timeshare resorts often offer amenities similar that of a full-service hotel with on-site restaurant(s), swimming pools, recreation grounds, and other leisure-oriented amenities. Destination clubs on the other hand may offer more exclusive private accommodations such as private houses in a neighborhood-style setting. Examples of timeshare brands include Hilton Grand Vacations, Marriott Vacation Club International, Westgate Resorts, Disney Vacation Club, and Holiday Inn Club Vacations.

A motel, an abbreviation for "motor hotel", is a small-sized low-rise lodging establishment similar to a limited service, lower-cost hotel, but typically with direct access to individual rooms from the car park. Motels were built to serve road travellers, including travellers on road trip vacations and workers who drive for their job (travelling salespeople, truck drivers, etc.). Common during the 1950s and 1960s, motels were often located adjacent to a major highway, where they were built on inexpensive land at the edge of towns or along stretches of freeway.

New motel construction is rare in the 2000s as hotel chains have been building economy-priced, limited-service franchised properties at freeway exits which compete for largely the same clientele, largely saturating the market by the 1990s. Motels are still useful in less populated areas for driving travelers, but the more populated an area becomes, the more hotels move in to meet the demand for accommodation. While many motels are unbranded and independent, many of the other motels which remain in operation joined national franchise chains, often rebranding themselves as hotels, inns or lodges. Some examples of chains with motels include EconoLodge, Motel 6, Super 8, and Travelodge.

Motels in some parts of the world are more often regarded as places for romantic assignations where rooms are often rented by the hour. This is fairly common in parts of Latin America.

Hotels may offer rooms for microstays, a type of booking for less than 24 hours where the customer chooses the check in time and the length of the stay. This allows the hotel increased revenue by reselling the same room several times a day.

Hotel management is a globally accepted professional career field and academic field of study. Degree programs such as hospitality management studies, a business degree, and/or certification programs formally prepare hotel managers for industry practice.

Most hotel establishments consist of a General Manager who serves as the head executive (often referred to as the "Hotel Manager"), department heads who oversee various departments within a hotel, middle managers, administrative staff, and line-level supervisors. The organizational chart and volume of job positions and hierarchy varies by hotel size, function, and is often determined by hotel ownership and managing companies.

Boutique hotels are typically hotels with a unique environment or intimate setting.
Some hotels have gained their renown through tradition, by hosting significant events or persons, such as Schloss Cecilienhof in Potsdam, Germany, which derives its fame from the Potsdam Conference of the World War II allies Winston Churchill, Harry Truman and Joseph Stalin in 1945. The Taj Mahal Palace & Tower in Mumbai is one of India's most famous and historic hotels because of its association with the Indian independence movement. Some establishments have given name to a particular meal or beverage, as is the case with the Waldorf Astoria in New York City, United States where the Waldorf Salad was first created or the Hotel Sacher in Vienna, Austria, home of the Sachertorte. Others have achieved fame by association with dishes or cocktails created on their premises, such as the Hotel de Paris where the crêpe Suzette was invented or the Raffles Hotel in Singapore, where the Singapore Sling cocktail was devised.

A number of hotels have entered the public consciousness through popular culture, such as the Ritz Hotel in London, through its association with Irving Berlin's song, 'Puttin' on the Ritz'. The Algonquin Hotel in New York City is famed as the meeting place of the literary group, the Algonquin Round Table, and Hotel Chelsea, also in New York City, has been the subject of a number of songs and the scene of the stabbing of Nancy Spungen (allegedly by her boyfriend Sid Vicious).

Some hotels are built specifically as a destination in itself to create a captive trade, example at casinos, amusement parks and holiday resorts. Though hotels have always been built in popular destinations, the defining characteristic of a resort hotel is that it exists purely to serve another attraction, the two having the same owners.

On the Las Vegas Strip there is a tradition of one-upmanship with luxurious and extravagant hotels in a concentrated area. This trend now has extended to other resorts worldwide, but the concentration in Las Vegas is still the world's highest: nineteen of the world's twenty-five largest hotels by room count are on the Strip, with a total of over 67,000 rooms.


The Null Stern Hotel in Teufen, Appenzellerland, Switzerland and the Concrete Mushrooms in Albania are former nuclear bunkers transformed into hotels.

The Cuevas Pedro Antonio de Alarcón (named after the author) in Guadix, Spain, as well as several hotels in Cappadocia, Turkey, are notable for being built into natural cave formations, some with rooms underground. The Desert Cave Hotel in Coober Pedy, South Australia is built into the remains of an opal mine.

Located on the coast but high above sea level, these hotels offer unobstructed panoramic views and a great sense of privacy without the feeling of total isolation. Some examples from around the globe are the Riosol Hotel in Gran Canaria, Caruso Belvedere Hotel in Amalfi Coast (Italy), Aman Resorts Amankila in Bali, Birkenhead House in Hermanus (South Africa), The Caves in Jamaica and Caesar Augustus in Capri.

Capsule hotels are a type of economical hotel first introduced in Japan, where people sleep in stacks of rectangular containers.

Some hotels fill daytime occupancy with day rooms, for example, Rodeway Inn and Suites near Port Everglades in Fort Lauderdale, Florida. Day rooms are booked in a block of hours typically between 8 am and 5 pm, before the typical night shift. These are similar to transit hotels in that they appeal to travelers, however, unlike transit hotels, they do not eliminate the need to go through Customs.

Garden hotels, famous for their gardens before they became hotels, include Gravetye Manor, the home of garden designer William Robinson, and Cliveden, designed by Charles Barry with a rose garden by Geoffrey Jellicoe.

The Ice Hotel in Jukkasjärvi, Sweden, was the first ice hotel in the world; first built in 1990, it is built each winter and melts every spring. The Hotel de Glace in Duschenay, Canada, opened in 2001 and it's North America's only ice hotel. It is redesigned and rebuilt in its entirety every year. 
Ice hotels can also be included within larger ice complexes; for example, the Mammut Snow Hotel in Finland is located within the walls of the Kemi snow castle; and the Lainio Snow Hotel is part of a snow village near Ylläs, Finland. There is an arctic snowhotel in Rovaniemi in Lapland, Finland, along with glass igloos. The first glass igloos were built in 1999 in Finland,they became the Kakslauttanen Arctic Resort with 65 buildings, 53 small ones for two people and 12 large ones for four people. Glass igloos, with their roof made of thermal glass, allow guests to admire auroras comfortably from their beds. 

A love hotel (also 'love motel', especially in Taiwan) is a type of short-stay hotel found around the world, operated primarily for the purpose of allowing guests privacy for sexual activities, typically for one to three hours, but with overnight as an option. Styles of premises vary from extremely low-end to extravagantly appointed. In Japan, love hotels have a history of over 400 years.

A referral hotel is a hotel chain that offers branding to independently operated hotels; the chain itself is founded by or owned by the member hotels as a group. Many former referral chains have been converted to franchises; the largest surviving member-owned chain is Best Western.

The first recorded purpose-built railway hotel was the Great Western Hotel, which opened adjacent to Reading railway station in 1844, shortly after the Great Western Railway opened its line from London. The building still exists, and although it has been used for other purposes over the years, it is now again a hotel and a member of the Malmaison hotel chain.

Frequently, expanding railway companies built grand hotels at their termini, such as the Midland Hotel, Manchester next to the former Manchester Central Station, and in London the ones above St Pancras railway station and Charing Cross railway station. London also has the Chiltern Court Hotel above Baker Street tube station, there are also Canada's grand railway hotels. They are or were mostly, but not exclusively, used by those traveling by rail.

The Maya Guesthouse in Nax Mont-Noble in the Swiss Alps, is the first hotel in Europe built entirely with straw bales. Due to the insulation values of the walls it needs no conventional heating or air conditioning system, although the Maya Guesthouse is built at an altitude of in the Alps.

Transit hotels are short stay hotels typically used at international airports where passengers can stay while waiting to change airplanes. The hotels are typically on the airside and do not require a visa for a stay or re-admission through security checkpoints.

Some hotels are built with living trees as structural elements, for example the Treehotel near Piteå, Sweden, the Costa Rica Tree House in the Gandoca-Manzanillo Wildlife Refuge, Costa Rica; the Treetops Hotel in Aberdare National Park, Kenya; the Ariau Towers near Manaus, Brazil, on the Rio Negro in the Amazon; and Bayram's Tree Houses in Olympos, Turkey.

Some hotels have accommodation underwater, such as Utter Inn in Lake Mälaren, Sweden. Hydropolis, project in Dubai, would have had suites on the bottom of the Persian Gulf, and Jules' Undersea Lodge in Key Largo, Florida requires scuba diving to access its rooms.

A resort island is an island or an archipelago that contains resorts, hotels, overwater bungalows, restaurants, tourist attractions and its amenities. Maldives has the most overwater bungalows resorts.

In 2006, "Guinness World Records" listed the First World Hotel in Genting Highlands, Malaysia, as the world's largest hotel with a total of 6,118 rooms (and which has now expanded to 7,351 rooms). The Izmailovo Hotel in Moscow has the most beds, with 7,500, followed by The Venetian and The Palazzo complex in Las Vegas (7,117 rooms) and MGM Grand Las Vegas complex (6,852 rooms).

According to the Guinness Book of World Records, the oldest hotel in operation is the Nisiyama Onsen Keiunkan in Yamanashi, Japan. The hotel, first opened in AD 707, has been operated by the same family for forty-six generations. The title was held until 2011 by the Hoshi Ryokan, in the Awazu Onsen area of Komatsu, Japan, which opened in the year 718, as the history of the Nisiyama Onsen Keiunkan was virtually unknown.

The Rosewood Guangzhou located on the top floors of the 108-story Guangzhou CTF Finance Centre in Tianhe District, Guangzhou, China. Soaring to 530-meters at its highest point, earns the singular status as the world's highest hotel.

In October 2014, the Anbang Insurance Group, based in China, purchased the Waldorf Astoria New York in Manhattan for US$1.95 billion, making it the world's most expensive hotel ever sold.

A number of public figures have notably chosen to take up semi-permanent or permanent residence in hotels.



</doc>
<doc id="14277" url="https://en.wikipedia.org/wiki?curid=14277" title="Hebrew mythology">
Hebrew mythology

Hebrew mythology may refer to:



</doc>
