<doc id="18376" url="https://en.wikipedia.org/wiki?curid=18376" title="Loran-C">
Loran-C

Loran-C was a "hyperbolic" radio navigation system that allowed a receiver to determine its position by listening to low frequency radio signals transmitted by fixed land-based radio beacons. Loran-C combined two different techniques to provide a signal that was both long-range and highly accurate, features that had formerly been incompatible. The disadvantage was the expense of the equipment needed to interpret the signals, which meant that Loran-C was used primarily by militaries after it was first introduced in 1957.

By the 1970s the cost, weight and size of electronics needed to implement Loran-C had been dramatically reduced due to the introduction of solid-state electronics and, from the mid-1970s, early microcontrollers to process the signal. Low-cost and easy-to-use Loran-C units became common from the late 1970s, especially in the early 1980s, and the earlier LORAN system was discontinued in favor of installing more Loran-C stations around the world. Loran-C became one of the most common and widely used navigation systems for large areas of North America, Europe, Japan and the entire Atlantic and Pacific areas. The Soviet Union operated a nearly identical system, CHAYKA.

The introduction of civilian satellite navigation in the 1990s led to a very rapid drop-off in Loran-C use. Discussions about the future of Loran-C began in the 1990s; several turn-off dates were announced and then cancelled. In 2010 the Canadian systems were shut down, along with Loran-C/CHAYKA stations shared with Russia. Several other chains remained active, and some had been upgraded for continued use. At the end of 2015, navigation chains in most of Europe were turned off. In December 2015 in the United States there was also renewed discussion of funding an eLoran system, and NIST was offering to fund development of a microchip-sized eLoran receiver for distribution of timing signals.

United States legislation introduced later, such as the National Timing Resilience and Security Act of 2017 and other bills, may resurrect Loran.

The original LORAN was proposed by Alfred Lee Loomis at a meeting of the Microwave Committee. The United States Army Air Corps were interested in the concept for aircraft navigation, and after some discussion they returned a requirement for a system offering accuracy of about at a range of , and a maximum range as great as for high-flying aircraft. The Microwave Committee, by this time organized into what would become the MIT Radiation Laboratory, took up development as Project 3. During the initial meetings, a member of the UK liaison team, Taffy Bowen, mentioned that he was aware the British were also working on a similar concept, but had no information on its performance.

The development team, led by Loomis, made rapid progress on the transmitter design and tested several systems during 1940 before settling on a 3 MHz design. Extensive signal-strength measurements were made by mounting a conventional radio receiver in a station wagon and driving around the eastern states. However, the custom receiver design and its associated cathode-ray tube displays proved to be a bigger problem. In spite of several efforts to design around the problem, instability in the display prevented accurate measurements.

By this time the team had become much more familiar with the British Gee system, and were aware of their related work on "strobes", a time base generator that produced well-positioned "pips" on the display that could be used for accurate measurement. They met with the Gee team in 1941, and immediately adopted this solution. This meeting also revealed that Project 3 and Gee called for almost identical systems, with similar performance, range and accuracy, but Gee had already completed basic development and was entering into initial production, making Project 3 superfluous.

In response, the Project 3 team told the Army Air Force to adopt Gee, and realigned their own efforts to provide long-range navigation on the oceans. This led to United States Navy interest, and a series of experiments quickly demonstrated that systems using the basic Gee concept, but operating at a lower frequency around 2 MHz would offer reasonable accuracy on the order of a few miles over distances on the order of , at least at night when signals of this frequency range were able to skip off the ionosphere. Rapid development followed, and a system covering the western Atlantic was operational in 1943. Additional stations followed, first covering the European side of the Atlantic, and then a large expansion in the Pacific. By the end of the war, there were 72 operational LORAN stations and as many as 75,000 receivers.

In 1958 the operation of the LORAN system was handed over to the United States Coast Guard, which renamed the system "Loran-A", the lower-case name being introduced at that time.

There are two ways to implement the timing measurements needed for a hyperbolic navigation system, pulse timing systems like Gee and LORAN, and phase-timing systems like the Decca Navigator System.

The former requires sharp pulses of signal, and their accuracy is generally limited to how rapidly the pulses can be turned on and off, which is a function of the carrier frequency. There is an ambiguity in the signal; the same measurements can be valid at two locations relative to the broadcasters, but in normal operation, they are hundreds of kilometres apart, so one possibility can be eliminated.

The second system uses constant signals ("continuous wave") and takes measurements by comparing the phase of two signals. This system is easy to use even at very low frequencies. However, its signal is ambiguous over the distance of a wavelength, meaning there are hundreds of locations that will return the same signal. Decca referred to these ambiguous locations as "cells". This demands some other navigation method to be used in conjunction to pick which cell the receiver is within, and then using the phase measurements to place the receiver accurately within the cell.

Numerous efforts were made to provide some sort of secondary low-accuracy system that could be used with a phase-comparison system like Decca in order to resolve the ambiguity. Among the many methods was a directional broadcast system known as POPI, and a variety of systems combining pulse-timing for low-accuracy navigation and then using phase-comparison for fine adjustment. Decca themselves had set aside one frequency, "9f", for testing this combined-signal concept, but did not have the chance to do so until much later. Similar concepts were also used in the experimental Navarho system in the United States.

It was known from the start of the LORAN project that the same CRT displays that showed the LORAN pulses could, when suitably magnified, also show the individual waves of the intermediate frequency. This meant that pulse-matching could be used to get a rough fix, and then the operator could gain additional timing accuracy by lining up the individual waves within the pulse, like Decca. This could either be used to greatly increase the accuracy of LORAN, or alternately, offer similar accuracy using much lower carrier frequencies, and thus greatly extend the effective range. This would require the transmitter stations to be synchronized both in time and phase, but much of this problem had already been solved by Decca engineers.

The long-range option was of considerable interest to the Coast Guard, who set up an experimental system known as LF LORAN in 1945. This operated at much lower frequencies than the original LORAN, at 180 kHz, and required very long balloon-borne antennas. Testing was carried out throughout the year, including several long-distance flights as far as Brazil. The experimental system was then sent to Canada where it was used during Operation Muskox in the Arctic. Accuracy was found to be at , a significant advance over LORAN. With the ending of Muskox, it was decided to keep the system running under what became known as "Operation Musk Calf", run by a group consisting of the United States Air Force, Royal Canadian Air Force, Royal Canadian Navy and the UK Royal Corps of Signals. The system ran until September 1947.

This led to another major test series, this time by the newly-formed United States Air Force, known as Operation Beetle. Beetle was located in the far north, on the Canada-Alaska border, and used new guy-stayed steel towers, replacing the earlier system's balloon-lofted cable antennas. The system became operational in 1948 and ran for two years until February 1950. Unfortunately, the stations proved poorly sited, as the radio transmission over the permafrost was much shorter than expected and synchronization of the signals between the stations using ground waves proved impossible. The tests also showed that the system was extremely difficult to use in practice; it was easy for the operator to select the wrong sections of the waveforms on their display, leading to significant real-world inaccuracy.

In 1946 the Rome Air Development Center sent out contracts for longer-ranged and more-accurate navigation systems that would be used for long-range bombing navigation. As the United States Army Air Forces were moving towards smaller crews, only three in the Boeing B-47 Stratojet for instance, a high degree of automation was desired. Two contracts were accepted; Sperry Gyroscope proposed the CYCLAN system (CYCLe matching LorAN) which was broadly similar to LF LORAN but with additional automation, and Sylvania proposed Whyn using continuous wave navigation like Decca, but with additional coding using frequency modulation. In spite of great efforts, Whyn could never be made to work, and was abandoned.

CYCLAN operated by sending the same LF LORAN-like signals on two frequencies, LF LORAN's 180 kHz and again on 200 kHz. The associated equipment would look for a rising amplitude that indicated the start of the signal pulse, and then use sampling gates to extract the carrier phase. Using two receivers solved the problem of mis-aligning the pulses, because the phases would only align properly between the two copies of the signal when the same pulses were being compared. None of this was trivial; using the era's tube-based electronics, the experimental CYCLAN system filled much of a semi-trailer.

CYCLAN proved highly successful, so much so that it became increasingly clear that the problems that led the engineers to use two frequencies were simply not as bad as expected. It appeared that a system using a single frequency would work just as well, given the right electronics. This was especially good news, as the 200 kHz frequency was interfering with existing broadcasts, and had to be moved to 160 kHz during testing.

Through this period the issue of radio spectrum use was becoming a major concern, and had led to international efforts to decide on a frequency band suitable for long-range navigation. This process eventually settled on the band from 90 to 100 kHz. CYCLAN appeared to suggest that accuracy at even lower frequencies was not a problem, and the only real concern was the expense of the equipment involved.

The success of the CYCLAN system led to a further contract with Sperry in 1952 for a new system with the twin goals of working in the 100 kHz range while being equally accurate, less complex and less expensive. These goals would normally be contradictory, but the CYCLAN system gave all involved the confidence that these could be met. The resulting system was known as Cytac.

To solve the complexity problem, a new circuit was developed to properly time the sampling of the signal. This consisted of a circuit to extract the envelope of the pulse, another to extract the derivative of the envelope, and finally another that subtracted the derivative from the envelope. The result of this final operation would become negative during a very specific and stable part of the rising edge of the pulse, and this zero-crossing was used to trigger a very short-time sampling gate. This system replaced the complex system of clocks used in CYCLAN. By simply measuring the time between the zero-crossings of the master and secondary, pulse-timing was extracted.

The output of the envelope sampler was also sent to a phase-shifter that adjusted the output of a local clock that locked to the master carrier using a phase-locked loop. This retained the phase of the master signal long enough for the secondary signal to arrive. Gating on the secondary signal was then compared to this master signal in a phase detector, and a varying voltage was produced depending on the difference in phase. This voltage represented the fine-positioning measurement.

The system was generally successful during testing through 1953, but there were concerns raised about the signal power at long range and the possibility of jamming. This led to further modifications of the basic signal. The first was to broadcast a series of pulses instead of just one, broadcasting more energy during a given time and improving the ability of the receivers to tune in a useful signal. They also added a fixed 45° phase shift to every pulse, so simple continuous-wave jamming signals could be identified and rejected.

The Cytac system underwent an enormous series of tests across the United States and offshore. Given the potential accuracy of the system, even minor changes to the groundwave synchronization were found to cause errors that could be eliminated — issues such as the number of rivers the signal crossed caused predictable delays that could be measured and then factored into navigation solutions. This led to a series of "correction contours" that could be added to the received signal to adjust for these concerns, and these were printed on the Cytac charts. Using prominent features on dams as target points, a series of tests demonstrated that the uncorrected signals provided accuracy on the order of 100 yards, while adding the correction contour adjustments reduced this to the order of ten yards.

It was at this moment that the United States Air Force, having taken over these efforts while moving from the United States Army Air Forces, dropped their interest in the project. Although the reasons are not well recorded, it appears the idea of a fully automated bombing system using radio aids was no longer considered possible. The AAF had been involved in missions covering about 1000 km (the distance from London to Berlin) and the Cytac system would work well at these ranges, but as the mission changed to trans-polar missions of 5,000 km or more, even Cytac did not offer the range and accuracy needed. They turned their attention to the use of inertial platforms and Doppler radar systems, cancelling work on Cytac as, well as a competing system known as Navarho.

Around this period the United States Navy began work on a similar system using combined pulse and phase comparison, but based on the existing LORAN frequency of 200 kHz. By this time the United States Navy had handed operational control of the LORAN system to the Coast Guard, and it was assumed the same arrangement would be true for any new system as well. Thus, the United States Coast Guard was given the choice of naming the systems, and decided to rename the existing system Loran-A, and the new system Loran-B.

With Cytac fully developed and its test system on the east coast of the United States mothballed, the United States Navy also decided to re-commission Cytac for tests in the long-range role. An extensive series of tests across the Atlantic were carried out by the USCGC "Androscoggin" starting in April 1956. Meanwhile, Loran-B proved to have serious problems keeping their transmitters in phase, and that work was abandoned. Minor changes were made to the Cytac systems to further simplify it, including a reduction in the pulse-chain spacing from 1200 to 1000 µs, the pulse rate changed to 20 pps to match the existing Loran-A system, and the phase-shifting between pulses to an alternating 0, 180-degree shift instead of 45 degrees at every pulse within the chain.

The result was Loran-C. Testing of the new system was intensive, and over-water flights around Bermuda demonstrated that 50% of fixes lay within a circle, a dramatic improvement over the original Loran-A, meeting the accuracy of the Gee system, but at much greater range. The first chain was set up using the original experimental Cytac system, and a second one in the Mediterranean in 1957. Further chains covering the North Atlantic and large areas of the Pacific followed. At the time global charts were printed with shaded sections representing the area where a accurate fix could be obtained under most operational conditions. Loran-C operated in the 90 to 110 kHz frequency range.

Loran-C had originally been designed to be highly automated, allowing the system to be operated more rapidly than the original LORAN's multi-minute measurement. It was also operated in "chains" of linked stations, allowing a fix to be made by simultaneously comparing two slaves to a single master. The downside of this approach was that the required electronic equipment, built using 1950s-era tube technology, was very large. Looking for companies with knowledge of seaborne, multi-channel phase-comparison electronics led, ironically, to Decca, who built the AN/SPN-31, the first widely used Loran-C receiver. The AN/SPN-31 weighed over and had 52 controls.

Airborne units followed, and an adapted AN/SPN-31 was tested in an Avro Vulcan in 1963. By the mid-1960s, units with some transistorization were becoming more common, and a chain was set up in Vietnam to support the United States' war efforts there. A number of commercial airline operators experimented with the system as well, using it for navigation on the great circle route between North America and Europe. However, inertial platforms ultimately became more common in this role.

In 1969, Decca sued the United States Navy for patent infringement, producing ample documentation of their work on the basic concept as early as 1944, along with the "missing" 9f frequency at 98 kHz that had been set aside for experiments using this system. Decca won the initial suit, but the judgement was overturned on appeal when the Navy claimed "wartime expediency".

When Loran-C became widespread, the United States Air Force once again became interested in using it as a guidance system. They proposed a new system layered on top of Loran-C, using it as the coarse guidance signal in much the same way that pulses were the coarse guidance and phase-comparison used for fine. To provide an extra-fine guidance signal, Loran-D interleaved another train of eight pulses immediately after the signals from one of the existing Loran-C stations, folding the two signals together. This technique became known as "Supernumary Interpulse Modulation" (SIM). These were broadcast from low-power portable transmitters, offering relatively short-range service of high accuracy.

Loran-D was used only experimentally during war-games in the 1960s from a transmitter set in the UK. The system was also used in a limited fashion during the Vietnam War, combined with the Pave Spot laser designator system, a combination known as Pave Nail. Using mobile transmitters, the AN/ARN-92 LORAN navigation receiver could achieve accuracy on the order of , which the Spot system improved to about . The SIM concept became a system for sending additional data.

At about the same time, Motorola proposed a new system using pseudo-random pulse-chains. This mechanism ensures that no two chains within a given period (on the order of many seconds) will have the same pattern, making it easy to determine if the signal is a groundwave from a recent transmission or a multi-hop signal from a previous one. The system, Multi-User Tactical Navigation Systems (MUTNS) was used briefly but it was found that Loran-D met the same requirements but had the added advantage of being a standard Loran-C signal as well. Although MUTNS was unrelated to the Loran systems, it was sometimes referred to as Loran-F.

In spite of its many advantages, the high cost of implementing a Loran-C receiver made it uneconomical for many users. Additionally, as military users upgraded from Loran-A to Loran-C, large numbers of surplus Loran-A receivers were dumped on the market. This made Loran-A popular in spite of being less accurate and fairly difficult to operate. By the early 1970s the introduction of integrated circuits combining a complete radio receiver began to greatly reduce the complexity of Loran-A measurements, and fully automated units the size of a stereo receiver became common. For those users requiring higher accuracy, Decca had considerable success with their Decca Navigator system, and produced units that combined both receivers, using Loran to eliminate the ambiguities in Decca.

The same rapid development of microelectronics that made Loran-A so easy to operate worked equally well on the Loran-C signals, and the obvious desire to have a long-range system that could also provide enough accuracy for lake and harbour navigation led to the "opening" of the Loran-C system to public use in 1974. Civilian receivers quickly followed, and dual-system A/C receivers were also common for a time. The switch from A to C was extremely rapid, due largely to rapidly falling prices which led to many users' first receiver being Loran-C. By the late 1970s the Coast Guard decided to turn off Loran-A, in favour of adding additional Loran-C stations to cover gaps is its coverage. The original Loran-A network was shut down in 1979 and 1980, with a few units used in the Pacific for some time. Given the widespread availability of Loran-A charts, many Loran-C receivers included a system for converting coordinates between A and C units.

One of the reasons for Loran-C's opening to the public was the move from Loran to new forms of navigation, including inertial navigation systems, Transit and OMEGA, meant that the security of Loran was no longer as stringent as it was as a primary form of navigation. As these newer systems gave way to GPS through the 1980s and 90s, this process repeated itself, but this time the military was able to separate GPS's signals in such a way that it could provide both secure military and insecure civilian signals at the same time. GPS was more difficult to receive and decode, but by the 1990s the required electronics were already as small and inexpensive as Loran-C, leading to rapid adoption that has become largely universal.

Although Loran-C was largely redundant by 2000, it has not universally disappeared due to a number of concerns. One is that the GPS system can be jammed through a variety of means; although the same is true of Loran-C, the transmitters are close-at-hand and can be adjusted if need be. More importantly, there are effects that might cause the GPS system to become unusable over wide areas, notably space weather events and potential EMP events. Loran, located entirely under the atmosphere, offers more resilience to these sorts of issues. There has been considerable debate about the relative merits of keeping the Loran-C system operational as a result of considerations like these.

In November 2009, the United States Coast Guard announced that Loran-C was not needed by the U.S. for maritime navigation. This decision left the fate of LORAN and eLORAN in the United States to the Secretary of the Department of Homeland Security. Per a subsequent announcement, the US Coast Guard, in accordance with the DHS Appropriations Act, terminated the transmission of all U.S. Loran-C signals on 8 February 2010. On 1 August 2010 the U.S. transmission of the Russian American signal was terminated, and on 3 August 2010 all Canadian signals were shut down by the USCG and the CCG.

The European Union had decided that the potential security advantages of Loran are worthy not only of keeping the system operational, but upgrading it and adding new stations. This is part of the wider Eurofix system which combines GPS, Galileo and nine Loran stations into a single integrated system.

However, in 2014, Norway and France both announced that all of their remaining transmitters, which make up a significant part of the Eurofix system, would be shut down on 31 December 2015. The two remaining transmitters in Europe (Anthorn, UK and Sylt, Germany) would no longer be able to sustain a positioning and navigation Loran service, with the result that the UK announced its trial eLoran service would be discontinued from the same date.

In conventional navigation, measuring one's location, or "taking a fix", is accomplished by taking two measurements against well known locations. In optical systems this is typically accomplished by measuring the angle to two landmarks, and then drawing lines on a nautical chart at those angles, producing an intersection that reveals the ship's location. Radio methods can also use the same concept with the aid of a radio direction finder, but due to the nature of radio propagation, such instruments are subject to significant errors, especially at night. More accurate radio navigation can be made using pulse timing or phase comparison techniques, which rely on the time-of-flight of the signals. In comparison to angle measurements, these remain fairly steady over time, and most of the effects that change these values are fixed objects like rivers and lakes that can be accounted for on charts.

Timing systems can reveal the absolute distance to an object, as is the case in radar. The problem in the navigational case is that the receiver has to know when the original signal was sent. In theory, one could synchronize an accurate clock to the signal before leaving port, and then use that to compare the timing of the signal during the voyage. However, in the 1940s no suitable system was available that could hold an accurate signal over the time span of an operational mission.

Instead, radio navigation systems adopted the "multilateration" concept. which is based on the difference in times (or phase) instead of the absolute time. The basic idea is that it is relatively easy to synchronize two ground stations, using a signal shared over a phone line for instance, so one can be sure that the signals received were sent at exactly the same time. They will not be received at exactly the same time, however, as the receiver will receive the signal from the closer station first. Timing the difference between two signals can be easily accomplished, first by physically measuring them on a cathode-ray tube, or simple electronics in the case of phase comparison.

The difference in signal timing does not reveal the location by itself. Instead, it determines a series of locations where that timing is possible. For instance, if the two stations are 300 km apart and the receiver measures no difference in the two signals, that implies that the receiver is somewhere along a line equidistant between the two. If the signal from one is received exactly 100 µs, then the receiver is closer to one station than the other. Plotting all the locations where one station is 30 km closer than the other produces a curved line. Taking a fix is accomplished by making two such measurements with different pairs of stations, and then looking up both curves on a navigational chart. The curves are known as "lines of position" or LOP.

In practice, radio navigation systems normally use a "chain" of three or four stations, all synchronized to a "master" signal that is broadcast from one of the stations. The others, the "secondaries", are positioned so their LOPs cross at acute angles, which increases the accuracy of the fix. So for instance, a given chain might have four stations with the master in the center, allowing a receiver to pick the signals from two secondaries that are currently as close to right angles as possible given their current location. Modern systems, which know the locations of all the broadcasters, can automate which stations to pick.

In the case of LORAN, one station remains constant in each application of the principle, the "primary", being paired up separately with two other "secondary" stations. Given two secondary stations, the time difference (TD) between the primary and first secondary identifies one curve, and the time difference between the primary and second secondary identifies another curve, the intersections of which will determine a geographic point in relation to the position of the three stations. These curves are referred to as "TD lines".

In practice, LORAN is implemented in integrated regional arrays, or "chains", consisting of one "primary" station and at least two (but often more) "secondary" stations, with a uniform "group repetition interval" (GRI) defined in microseconds. The amount of time before transmitting the next set of pulses is defined by the distance between the start of transmission of primary to the next start of transmission of primary signal.

The secondary stations receive this pulse signal from the primary, then wait a preset number of milliseconds, known as the "secondary coding delay", to transmit a response signal. In a given chain, each secondary's coding delay is different, allowing for separate identification of each secondary's signal. (In practice, however, modern LORAN receivers do not rely on this for secondary identification.)

Every LORAN chain in the world uses a unique Group Repetition Interval, the number of which, when multiplied by ten, gives how many microseconds pass between pulses from a given station in the chain. In practice, the delays in many, but not all, chains are multiples of 100 microseconds. LORAN chains are often referred to by this designation, "e.g.", GRI 9960, the designation for the LORAN chain serving the Northeastern United States.

Due to the nature of hyperbolic curves, a particular combination of a primary and two secondary stations can possibly result in a "grid" where the grid lines intersect at shallow angles. For ideal positional accuracy, it is desirable to operate on a navigational grid where the grid lines are closer to right angles (orthogonal) to each other. As the receiver travels through a chain, a certain selection of secondaries whose TD lines initially formed a near-orthogonal grid can become a grid that is significantly skewed. As a result, the selection of one or both secondaries should be changed so that the TD lines of the new combination are closer to right angles. To allow this, nearly all chains provide at least three, and as many as five, secondaries.

Where available, common marine nautical charts include visible representations of TD lines at regular intervals over water areas. The TD lines representing a given primary-secondary pairing are printed with distinct colors, and note the specific time difference indicated by each line. On a nautical chart, the denotation for each Line of Position from a receiver, relative to axis and color, can be found at the bottom of the chart. The color on official charts for stations and the timed-lines of position follow no specific conformance for the purpose of the International Hydrographic Organization (IHO). However, local chart producers may color these in a specific conformance to their standard. Always consult the chart notes, administrations Chart1 reference, and information given on the chart for the most accurate information regarding surveys, datum, and reliability.

There are three major factors when considering signal delay and propagation in relation to LORAN-C:

The chart notes should indicate whether ASF corrections have been made (Canadian Hydrographic Service (CHS) charts, for example, include them). Otherwise, the appropriate correction factors must be obtained before use.

Due to interference and propagation issues suffered from land features and artificial structures such as tall buildings, the accuracy of the LORAN signal can be degraded considerably in inland areas (see Limitations). As a result, nautical charts will not show TD lines in those areas, to prevent reliance on LORAN-C for navigation.
Traditional LORAN receivers display the time difference between each pairing of the primary and one of the two selected secondary stations, which is then used to find the appropriate TD line on the chart. Modern LORAN receivers display latitude and longitude coordinates instead of time differences, and, with the advent of time difference comparison and electronics, provide improved accuracy and better position fixing, allowing the observer to plot their position on a nautical chart more easily. When using such coordinates, the datum used by the receiver (usually WGS84) must match that of the chart, or manual conversion calculations must be performed before the coordinates can be used.

Each LORAN station is equipped with a suite of specialized equipment to generate the precisely timed signals used to modulate / drive the transmitting equipment. Up to three commercial cesium atomic clocks are used to generate 5 MHz and pulse per second (or 1 Hz) signals that are used by timing equipment to generate the various GRI-dependent drive signals for the transmitting equipment.

While each U.S.-operated LORAN station is supposed to be synchronized to within 100 ns of Coordinated Universal Time (UTC), the actual accuracy achieved as of 1994 was within 500 ns.

LORAN-C transmitters operate at peak powers of 100–4,000 kilowatts, comparable to longwave broadcasting stations. Most use 190–220 metre tall mast radiators, insulated from ground. The masts are inductively lengthened and fed by a loading coil (see: electrical length). A well known-example of a station using such an antenna is Rantum. Free-standing tower radiators in this height range are also used. Carolina Beach uses a free-standing antenna tower. Some LORAN-C transmitters with output powers of 1,000 kW and higher used extremely tall mast radiators (see below). Other high power LORAN-C stations, like George, used four T-antennas mounted on four guyed masts arranged in a square.

All LORAN-C antennas are designed to radiate an omnidirectional pattern. Unlike longwave broadcasting stations, LORAN-C stations cannot use backup antennas because the exact position of the antenna is a part of the navigation calculation. The slightly different physical location of a backup antenna would produce Lines of Position different from those of the primary antenna.

LORAN suffers from electronic effects of weather and the ionospheric effects of sunrise and sunset. The most accurate signal is the groundwave that follows the Earth's surface, ideally over seawater. At night the indirect skywave, bent back to the surface by the ionosphere, is a problem as multiple signals may arrive via different paths (multipath interference). The ionosphere's reaction to sunrise and sunset accounts for the particular disturbance during those periods. Geomagnetic storms have serious effects, as with any radio based system.

LORAN uses ground-based transmitters that only cover certain regions. Coverage is quite good in North America, Europe, and the Pacific Rim.

The absolute accuracy of LORAN-C varies from . Repeatable accuracy is much greater, typically from .

LORAN Data Channel (LDC) is a project underway between the FAA and United States Coast Guard to send low bit rate data using the LORAN system. Messages to be sent include station identification, absolute time, and position correction messages. In 2001, data similar to Wide Area Augmentation System (WAAS) GPS correction messages were sent as part of a test of the Alaskan LORAN chain. As of November 2005, test messages using LDC were being broadcast from several U.S. LORAN stations.

In recent years, LORAN-C has been used in Europe to send differential GPS and other messages, employing a similar method of transmission known as EUROFIX.

A system called SPS (Saudi Positioning System), similar to EUROFIX, is in use in Saudi Arabia. GPS differential corrections and GPS integrity information are added to the LORAN signal. A combined GPS/LORAN receiver is used, and if a GPS fix is not available it automatically switches over to LORAN.

As LORAN systems are maintained and operated by governments, their continued existence is subject to public policy. With the evolution of other electronic navigation systems, such as satellite navigation systems, funding for existing systems is not always assured.

Critics, who have called for the elimination of the system, state that the LORAN system has too few users, lacks cost-effectiveness, and that GNSS signals are superior to LORAN. Supporters of continued and improved LORAN operation note that LORAN uses a strong signal, which is difficult to jam, and that LORAN is an independent, dissimilar, and complementary system to other forms of electronic navigation, which helps ensure availability of navigation signals.

On 26 February 2009, the U.S. Office of Management and Budget released the first blueprint for the Fiscal Year 2010 budget. This document identified the LORAN-C system as "outdated" and supported its termination at an estimated savings of $36 million in 2010 and $190 million over five years.

On 21 April 2009 the U.S. Senate Committee on Commerce, Science and Transportation and the Committee on Homeland Security and Governmental Affairs released inputs to the FY 2010 Concurrent Budget Resolution with backing for the continued support for the LORAN system, acknowledging the investment already made in infrastructure upgrades and recognizing the studies performed and multi-departmental conclusion that eLORAN is the best backup to GPS.

Senator Jay Rockefeller, Chairman of the Committee on Commerce, Science and Transportation, wrote that the committee recognized the priority in "Maintaining LORAN-C while transitioning to eLORAN" as means of enhancing the national security, marine safety and environmental protection missions of the Coast Guard.

Senator Collins, the ranking member on the Committee on Homeland Security and Governmental Affairs wrote that the President's budget overview proposal to terminate the LORAN-C system is inconsistent with the recent investments, recognized studies and the mission of the U.S. Coast Guard. The committee also recognizes the $160 million investment already made toward upgrading the LORAN-C system to support the full deployment of eLORAN.

Further, the Committees also recognize the many studies which evaluated GPS backup systems and concluded both the need to back up GPS and identified eLORAN as the best and most viable backup. "This proposal is inconsistent with the recently released (January 2009) Federal Radionavigation Plan (FRP), which was jointly prepared by DHS and the Departments of Defense (DOD) and Transportation (DOT). The FRP proposed the eLORAN program to serve as a Position, Navigation and Timing (PNT) backup to GPS (Global Positioning System)."

On 7 May 2009, President Barack Obama proposed cutting funding (approx. $35 million/year) for LORAN, citing its redundancy alongside GPS. In regard to the pending Congressional bill, H.R. 2892, it was subsequently announced that "[t]he Administration supports the Committee's aim to achieve an orderly termination through a phased decommissioning beginning in January 2010, and the requirement that certifications be provided to document that the LORAN-C termination will not impair maritime safety or the development of possible GPS backup capabilities or needs."

Also on 7 May 2009, the U.S. General Accounting Office (GAO), the investigative arm of Congress, released a report citing the very real potential for the GPS system to degrade or fail in light of program delays which have resulted in scheduled GPS satellite launches slipping by up to three years.

On 12 May 2009 the March 2007 Independent Assessment Team (IAT) report on LORAN was released to the public. In its report the ITA stated that it "unanimously recommends that the U.S. government complete the eLORAN upgrade and commit to eLORAN as the national backup to GPS for 20 years." The release of the report followed an extensive Freedom of Information Act (FOIA) battle waged by industry representatives against the federal government. Originally completed 20 March 2007 and presented to the co-sponsoring Department of Transportation and Department of Homeland Security (DHS) Executive Committees, the report carefully considered existing navigation systems, including GPS. The unanimous recommendation for keeping the LORAN system and upgrading to eLORAN was based on the team's conclusion that LORAN is operational, deployed and sufficiently accurate to supplement GPS. The team also concluded that the cost to decommission the LORAN system would exceed the cost of deploying eLORAN, thus negating any stated savings as offered by the Obama administration and revealing the vulnerability of the U.S. to GPS disruption.

In November 2009, the U.S. Coast Guard announced that the LORAN-C stations under its control would be closed down for budgetary reasons after 4 January 2010 provided the Secretary of the Department of Homeland Security certified that LORAN is not needed as a backup for GPS.

On 7 January 2010, Homeland Security published a notice of the permanent discontinuation of LORAN-C operation. Effective 2000 UTC 8 February 2010, the United States Coast Guard terminated all operation and broadcast of LORAN-C signals in the United States. The United States Coast Guard transmission of the Russian American CHAYKA signal was terminated on 1 August 2010. The transmission of Canadian LORAN-C signals was terminated on 3 August 2010.

With the potential vulnerability of GNSS systems, and their own propagation and reception limitations, renewed interest in LORAN applications and development has appeared. Enhanced LORAN, also known as eLORAN or E-LORAN, comprises an advancement in receiver design and transmission characteristics which increase the accuracy and usefulness of traditional LORAN. With reported accuracy as good as ± 8 meters, the system becomes competitive with unenhanced GPS. eLORAN also includes additional pulses which can transmit auxiliary data such as Differential GPS (DGPS) corrections, as well ensure data integrity against spoofing.

eLORAN receivers now use "all in view" reception, incorporating signals from all stations in range, not solely those from a single GRI, incorporating time signals and other data from up to forty stations. These enhancements in LORAN make it adequate as a substitute for scenarios where GPS is unavailable or degraded. 
In recent years the United States Coast Guard has reported several episodes of GPS interference in the Black Sea. South Korea has claimed that North Korea has jammed GPS near the border, interfering with airplanes and ships. By 2018, the United States will build a new eLoran system as a complement to and backup for the GPS system. And the South Korean government has already pushed plans to have three eLoran beacons active by 2019, which is enough to provide accurate corrections for all shipments in the region if North Korea (or anyone else) tries to block GPS again.

On 31 May 2007, the UK Department for Transport (DfT), via the General Lighthouse Authorities (GLA), awarded a 15-year contract to provide a state-of-the-art enhanced LORAN (eLORAN) service to improve the safety of mariners in the UK and Western Europe. The service contract was to operate in two phases, with development work and further focus for European agreement on eLORAN service provision from 2007 through 2010, and full operation of the eLORAN service from 2010 through 2022. The first eLORAN transmitter was situated at Anthorn Radio Station Cumbria, UK, and was operated by Babcock International (previously Babcock Communications).

eLORAN: The UK government granted approval for seven differential eLoran ship-positioning technology stations to be built along the south and east coasts of the UK to help counter the threat of jamming of global positioning systems. They were set to reach initial operational capability by summer 2014. The General Lighthouse Authorities (GLAs) of the UK and Ireland announced 31 October 2014 the initial operational capability of UK maritime eLoran. Seven differential reference stations provided additional position, navigation, and timing (PNT) information via low-frequency pulses to ships fitted with eLoran receivers. The service was to help ensure they could navigate safely in the event of GPS failure in one of the busiest shipping regions in the world, with expected annual traffic of 200,000 vessels by 2020.

Despite these plans, in light of the decision by France and Norway to cease Loran transmissions on 31 December 2015, the UK announced at the start of that month that its eLoran service would be discontinued on the same day.

A list of LORAN-C transmitters. Stations with an antenna tower taller than 300 metres (984 feet) are shown in bold.





</doc>
<doc id="18377" url="https://en.wikipedia.org/wiki?curid=18377" title="Lunatic">
Lunatic

Lunatic is an antiquated term referring to a person who is seen as mentally ill, dangerous, foolish, or crazy—conditions once attributed to "lunacy." The word derives from "lunaticus" meaning "of the moon" or "moonstruck". The term was once commonly used in law.

The term "lunatic" derives from the Latin word "lunaticus", which originally referred mainly to epilepsy and madness, as diseases thought to be caused by the moon. The King James Version of the Bible records "lunatick" in the Gospel of Matthew which has been interpreted as a reference to epilepsy. By the fourth and fifth centuries, astrologers were commonly using the term to refer to neurological and psychiatric diseases.Philosophers such as Aristotle and Pliny the Elder argued that the full moon induced insane individuals with bipolar disorder by providing light during nights which would otherwise have been dark, and affecting susceptible individuals through the well-known route of sleep deprivation. Until at least 1700, it was also a common belief that the moon influenced fevers, rheumatism, episodes of epilepsy and other diseases.

In the jurisdiction of England and Wales the Lunacy Acts 1890–1922 referred to "lunatics", but the Mental Treatment Act 1930 changed the legal term to "person of unsound mind", an expression which was replaced under the Mental Health Act 1959 by "mental illness". "Person of unsound mind" was the term used in 1950 in the English version of the European Convention on Human Rights as one of the types of person who could be deprived of liberty by a judicial process. The 1930 Act also replaced the term "asylum" with "mental hospital". Criminal lunatics became Broadmoor patients in 1948 under the National Health Service Act 1946.

On December 5, 2012, the US House of Representatives passed legislation approved earlier by the US Senate removing the word "lunatic" from all federal laws in the United States. President Barack Obama signed the 21st Century Language Act of 2012 into law on December 28, 2012.

"Of unsound mind" or "non compos mentis" are alternatives to "lunatic", the most conspicuous term used for insanity in the law in the late 19th century.

The term "lunatic" was sometimes used to describe those who sought to discover a reliable method of determining longitude (before John Harrison developed the marine chronometer method of determining longitude, the main theory was the Method of Lunar Distances, advanced by Astronomer Royal Nevil Maskelyne). The artist William Hogarth portrayed a "longitude lunatic" in the eight scene of his 1733 work "A Rake's Progress". Twenty years later, though, Hogarth described John Harrison's H-1 chronometer as "one of the most exquisite movements ever made."

Later, members of the Lunar Society of Birmingham called themselves "lunaticks". In an age with little street lighting, the society met on or near the night of the full moon.




</doc>
<doc id="18379" url="https://en.wikipedia.org/wiki?curid=18379" title="Linear timecode">
Linear timecode

Linear (or Longitudinal) Timecode (LTC) is an encoding of SMPTE timecode data in an audio signal, as defined in SMPTE 12M specification. The audio signal is commonly recorded on a VTR track or other storage media. The bits are encoded using the biphase mark code (also known as "FM"): a 0 bit has a single transition at the start of the bit period. A 1 bit has two transitions, at the beginning and middle of the period. This encoding is self-clocking. Each frame is terminated by a 'sync word' which has a special predefined sync relationship with any video or film content.

A special bit in the linear timecode frame, the "biphase mark correction" bit, ensures that there are an even number of AC transitions in each timecode frame.

The sound of linear timecode is a jarring and distinctive noise and has been used as a sound-effects shorthand to imply "telemetry" or "computers".

In broadcast video situations, the LTC generator should be tied into house black burst, as should all devices using timecode, to ensure correct color framing and correct synchronization of all digital clocks. When synchronizing multiple clock-dependent digital devices together with video, such as digital audio recorders, the devices must be connected to a common word clock signal that is derived from the house black burst signal. This can be accomplished by using a generator that generates both black burst and video-resolved word clock, or by synchronizing the master digital device to video, and synchronizing all subsequent devices to the word clock output of the master digital device (and to LTC).

Made up of 80 bits per frame, where there may be 24, 25 or 30 frames per second, LTC timecode varies from 960 Hz (binary zeros at 24 frames/s) to 2400 Hz (binary ones at 30 frames/s), and thus is comfortably in the audio frequency range. LTC can exist as either a balanced or unbalanced signal, and can be treated as an audio signal in regards to distribution. Like audio, LTC can be distributed by standard audio wiring, connectors, distribution amplifiers, and patchbays, and can be ground-isolated with audio transformers. It can also be distributed via 75 ohm video cable and video distribution amplifiers, although the voltage attenuation caused by using a 75 ohm system may cause the signal to drop to a level that can not be read by some equipment.

Care has to be taken with analog audio to avoid audible 'breakthrough' (aka "crosstalk") from the LTC track to the audio tracks.

LTC care:


Longitudinal SMPTE timecode should be played back at a middle-level when recorded on an audio track, as both low and high levels will introduce distortion.

The basic format is an 80-bit code that gives the time of day to the second, and the frame number within the second. Values are stored in binary-coded decimal, least significant bit first.
There are thirty-two bits of user data, usually used for a reel number and date.




</doc>
<doc id="18381" url="https://en.wikipedia.org/wiki?curid=18381" title="John William Strutt, 3rd Baron Rayleigh">
John William Strutt, 3rd Baron Rayleigh

John William Strutt, 3rd Baron Rayleigh, (; 12 November 1842 – 30 June 1919), was a British scientist who made extensive contributions to both theoretical and experimental physics. He spent all of his academic career at the University of Cambridge. Among many honors, he received the 1904 Nobel Prize in Physics "for his investigations of the densities of the most important gases and for his discovery of argon in connection with these studies." He served as President of the Royal Society from 1905 to 1908 and as Chancellor of the University of Cambridge from 1908 to 1919.

Rayleigh provided the first theoretical treatment of the elastic scattering of light by particles much smaller than the light's wavelength, a phenomenon now known as "Rayleigh scattering", which notably explains why the sky is blue. He studied and described transverse surface waves in solids, now known as "Rayleigh waves". He contributed extensively to fluid dynamics, with concepts such as the Rayleigh number (a dimensionless number associated with natural convection), Rayleigh flow, the Rayleigh–Taylor instability, and Rayleigh's criterion for the stability of Taylor–Couette flow. He also formulated the circulation theory of aerodynamic lift. In optics, Rayleigh proposed a well known criterion for angular resolution. His derivation of the Rayleigh–Jeans law for classical black-body radiation later played an important role in birth of quantum mechanics (see Ultraviolet catastrophe). Rayleigh's textbook "The Theory of Sound" (1877) is still used today by acousticians and engineers.

Strutt was born on 12 November 1842 at Langford Grove in Maldon, Essex. In his early years he suffered from frailty and poor health. He attended Eton College and Harrow School (each for only a short period), before going on to the University of Cambridge in 1861 where he studied mathematics at Trinity College, Cambridge. He obtained a Bachelor of Arts degree (Senior Wrangler and 1st Smith's Prize) in 1865, and a Master of Arts in 1868. He was subsequently elected to a Fellowship of Trinity. He held the post until his marriage to Evelyn Balfour, daughter of James Maitland Balfour, in 1871. He had three sons with her. In 1873, on the death of his father, John Strutt, 2nd Baron Rayleigh, he inherited the Barony of Rayleigh.

He was the second Cavendish Professor of Physics at the University of Cambridge (following James Clerk Maxwell), from 1879 to 1884. He first described dynamic soaring by seabirds in 1883, in the British journal "Nature". From 1887 to 1905 he was Professor of Natural Philosophy at the Royal Institution.

Around the year 1900 Rayleigh developed the "duplex" (combination of two) theory of human sound localisation using two binaural cues, interaural phase difference (IPD) and interaural level difference (ILD) (based on analysis of a spherical head with no external pinnae). The theory posits that we use two primary cues for sound lateralisation, using the difference in the phases of sinusoidal components of the sound and the difference in amplitude (level) between the two ears.

During the First World War, he was president of the government's "Advisory Committee for Aeronautics", which was located at the National Physical Laboratory, and chaired by Richard Glazebrook. 

In 1919, Rayleigh served as President of the Society for Psychical Research. As an advocate that simplicity and theory be part of the scientific method, Rayleigh argued for the principle of similitude.

Rayleigh was elected Fellow of the Royal Society on 12 June 1873, and served as president of the Royal Society from 1905 to 1908. From time to time Rayleigh participated in the House of Lords; however, he spoke up only if politics attempted to become involved in science.

He died on 30 June 1919, at his home in Witham, Essex. He was succeeded, as the 4th Lord Rayleigh, by his son Robert John Strutt, another well-known physicist. Lord Rayleigh was buried in the graveyard of All Saints' Church in Terling in Essex.

Rayleigh was an Anglican. Though he did not write about the relationship of science and religion, he retained a personal interest in spiritual matters. When his scientific papers were to be published in a collection by the Cambridge University Press, Strutt wanted to include a religious quotation from the Bible, but he was discouraged from doing so, as he later reported:

Still, he had his wish and the quotation was printed in the five-volume collection of scientific papers. In a letter to a family member, he wrote about his rejection of materialism and spoke of Jesus Christ as a moral teacher:

He held an interest in parapsychology and was an early member of the Society for Psychical Research (SPR). He was not convinced of spiritualism but remained open to the possibility of supernatural phenomena. Rayleigh was the president of the SPR in 1919. He gave a presidential address in the year of his death but did not come to any definite conclusions.

The lunar crater "Rayleigh" as well as the Martian crater "Rayleigh" were named in his honour. The asteroid 22740 Rayleigh was named after him on 1 June 2007. A type of surface waves are known as Rayleigh waves. The rayl, a unit of specific acoustic impedance, is also named for him. Rayleigh was also awarded with (in chronological order):

Lord Rayleigh was among the original recipients of the Order of Merit (OM) in the 1902 Coronation Honours list published on 26 June 1902, and received the order from King Edward VII at Buckingham Palace on 8 August 1902.

He received the degree of "Doctor mathematicae (honoris causa)" from the Royal Frederick University on 6 September 1902, when they celebrated the centennial of the birth of mathematician Niels Henrik Abel.

Sir William Ramsay, his co-worker in the investigation to discover Argon described Rayleigh as "the greatest man alive" while speaking to Lady Ramsay during his last illness.

H. M. Hyndman said of Rayleigh that "no man ever showed less consciousness of great genius".




</doc>
<doc id="18382" url="https://en.wikipedia.org/wiki?curid=18382" title="Lunisolar calendar">
Lunisolar calendar

A lunisolar calendar is a calendar in many cultures whose date indicates both the Moon phase and the time of the solar year. If the solar year is defined as a tropical year, then a lunisolar calendar will give an indication of the season; if it is taken as a sidereal year, then the calendar will predict the constellation near which the full moon may occur. As with all calendars which divide the year into months there is an additional requirement that the year have a whole number of months. In this case ordinary years consist of twelve months but every second or third year is an embolismic year, which adds a thirteenth intercalary, embolismic, or leap month.

Their months are based on the regular cycle of the Moon's phases. So lunisolar calendars are lunar calendars with – in contrast to them – additional intercalation rules being used to bring them into a rough agreement with the solar year and thus with the seasons.

The main other type of calendar is a solar calendar.

The Hebrew, Jain, Buddhist, Hindu and Kurdish as well as the traditional Burmese, Chinese, Japanese, Tibetan, Vietnamese, Mongolian and Korean calendars (in the east Asian cultural sphere), plus the ancient Hellenic, Coligny, and Babylonian calendars are all lunisolar. Also, some of the ancient pre-Islamic calendars in south Arabia followed a lunisolar system. The Chinese, Coligny and
Hebrew lunisolar calendars track more or less the tropical year whereas the Buddhist and Hindu lunisolar calendars track the sidereal year. Therefore, the first three give an idea of the seasons whereas the last two give an idea of the position among the constellations of the full moon. The Tibetan calendar was influenced by both the Chinese and Buddhist calendars. The Germanic peoples also used a lunisolar calendar before their conversion to Christianity.

The Islamic calendar is lunar, but not a lunisolar calendar because its date is not related to the Sun. The civil versions of the Julian and Gregorian calendars are solar, because their dates do not indicate the Moon phase – however, both the Gregorian and Julian calendars include undated lunar calendars that allow them to calculate the Christian celebration of Easter, so both are lunisolar calendars in that respect.

A rough idea of the frequency of the intercalary or leap month in all lunisolar calendars can be obtained by the following calculation, using approximate lengths of months and years in days:

Intercalation of leap months is frequently controlled by the "epact", which is the difference between the lunar and solar years (approximately 11 days). The Metonic cycle, used in the Hebrew calendar and the Julian and Gregorian ecclesiastical calendars, adds seven months during every nineteen-year period. The classic Metonic cycle can be reproduced by assigning an initial epact value of 1 to the last year of the cycle and incrementing by 11 each year. Between the last year of one cycle and the first year of the next the increment is 12. This adjustment, the "saltus lunae", causes the epacts to repeat every 19 years. When the epact goes above 29 an intercalary month is added and 30 is subtracted. The intercalary years are numbers 3, 6, 8, 11, 14, 17 and 19. Both the Hebrew calendar and the Julian calendar use this sequence.

The Buddhist and Hebrew calendars restrict the leap month to a single month of the year; the number of common months between leap months is, therefore, usually 36, but occasionally only 24 months. Because the Chinese and Hindu lunisolar calendars allow the leap month to occur after or before (respectively) any month but use the true motion of the Sun, their leap months do not usually occur within a couple of months of perihelion, when the apparent speed of the Sun along the ecliptic is fastest (now about 3 January). This increases the usual number of common months between leap months to roughly 34 months when a doublet of common years occurs, while reducing the number to about 29 months when only a common singleton occurs.

An alternative way of dealing with the fact that a solar year does not contain an integer number of months is by including uncounted time in the year that does not belong to any month. Some Coast Salish peoples used a calendar of this kind. For instance, the Chehalis began their count of lunar months from the arrival of spawning chinook salmon (in Gregorian calendar October), and counted 10 months, leaving an uncounted period until the next chinook salmon run.

The Gregorian calendar has a lunisolar calendar, which is used to determine the date of Easter. The rules are in the Computus.

The following is a list of lunisolar calendars:






</doc>
<doc id="18383" url="https://en.wikipedia.org/wiki?curid=18383" title="Leonids">
Leonids

The Leonids ( ) are a prolific meteor shower associated with the comet Tempel–Tuttle, which are also known for their spectacular meteor storms that occur about every 33 years. The Leonids get their name from the location of their radiant in the constellation Leo: the meteors appear to radiate from that point in the sky. Their proper Greek name should be Leon"t"ids (Λεοντίδαι, "Leontídai"), but the word was initially constructed as a Greek/Latin hybrid and it has been used since. They peak in the month of November.

Earth moves through the meteoroid stream of particles left from the passages of a comet. The stream comprises solid particles, known as meteoroids, ejected by the comet as its frozen gases evaporate under the heat of the Sun when it is close enough – typically closer than Jupiter's orbit. The Leonids are a fast moving stream which encounter the path of Earth and impact at 72 km/s. Larger Leonids which are about 10 mm across have a mass of half a gram and are known for generating bright (apparent magnitude −1.5) meteors. An annual Leonid shower may deposit 12 or 13 tons of particles across the entire planet.

The meteoroids left by the comet are organized in trails in orbits similar to—though different from—that of the comet. They are differentially disturbed by the planets, in particular Jupiter and to a lesser extent by radiation pressure from the sun, the Poynting–Robertson effect, and the Yarkovsky effect. These trails of meteoroids cause meteor showers when Earth encounters them. Old trails are spatially not dense and compose the meteor shower with a few meteors per minute. In the case of the Leonids, that tends to peak around November 18, but some are spread through several days on either side and the specific peak changes every year. Conversely, young trails are spatially very dense and the cause of meteor outbursts when the Earth enters one. 

The Leonids also produce meteor storms (very large outbursts) about every 33 years, during which activity exceeds 1,000 meteors per hour, with some events exceeding 100,000 meteors per hour, in contrast to the sporadic background (5 to 8 meteors per hour) and the shower background (several meteors per hour).

The Leonids are famous because their meteor showers, or storms, can be among the most spectacular. Because of the storm of 1833 and the recent developments in scientific thought of the time (see for example the identification of Halley's Comet), the Leonids have had a major effect on the development of the scientific study of meteors, which had previously been thought to be atmospheric phenomena. Although it has been suggested the Leonid meteor shower and storms have been noted in ancient times, it was the meteor storm of 1833 that broke into people's modern day awareness – it was of truly superlative strength. One estimate of the peak rate is over one hundred thousand meteors an hour, but another, done as the storm abated, estimated in excess of 240,000 meteors during the nine hours of the storm, over the entire region of North America east of the Rocky Mountains.

It was marked by several nations of Native Americans: the Cheyenne established a peace treaty and the Lakota calendar was reset. Abolitionists including Harriet Tubman and Frederick Douglass as well as slave-owners took note and others. The "New York Evening Post" carried a series of articles on the event including reports from Canada to Jamaica, it made news in several states beyond New York and though it appeared in North America was talked about in Europe. The journalism of the event tended to rise above the partisan debates of the time and reviewed facts as they could be sought out. Abraham Lincoln commented on it years later. Near Independence, Missouri, in Clay County, a refugee Mormon community watched the meteor shower on the banks of the Missouri River after having been driven from their homes by local settlers. The founder and first leader of Mormonism, Joseph Smith, afterwards noted in his journal his belief that this event was a literal fulfillment of the word of God and a sure sign that the coming of Christ was close at hand. Though it was noted in the midwest and eastern areas it was also noted in the far west.

Denison Olmsted explained the event most accurately. After spending the last weeks of 1833 collecting information, he presented his findings in January 1834 to the "American Journal of Science and Arts", published in January–April 1834, and January 1836. He noted the shower was of short duration and was not seen in Europe, and that the meteors radiated from a point in the constellation of Leo and he speculated the meteors had originated from a cloud of particles in space. Accounts of the 1866 repeat of the Leonids counted hundreds per minute/a few thousand per hr in Europe. The Leonids were again seen in 1867, when moonlight reduced the rates to 1,000 meteors per hour. Another strong appearance of the Leonids in 1868 reached an intensity of 1,000 meteors per hour in dark skies. It was in 1866–67 that information on Comet Tempel-Tuttle was gathered, pointing it out as the source of the meteor shower and meteor storms. When the storms failed to return in 1899, it was generally thought that the dust had moved on and the storms were a thing of the past.

Then, in 1966, a spectacular meteor storm was seen over the Americas. Historical notes were gathered thus noting the Leonids back to 900AD. Radar studies showed the 1966 storm included a relatively high percentage of smaller particles while 1965's lower activity had a much higher proportion of larger particles. In 1981 Donald K. Yeomans of the Jet Propulsion Laboratory reviewed the history of meteor showers for the Leonids and the history of the dynamic orbit of Comet Tempel-Tuttle. A graph from it was adapted and re-published in "Sky and Telescope". It showed relative positions of the Earth and Tempel-Tuttle and marks where Earth encountered dense dust. This showed that the meteoroids are mostly behind and outside the path of the comet, but paths of the Earth through the cloud of particles resulting in powerful storms were very near paths of nearly no activity. But overall the 1998 Leonids were in a favorable position so interest was rising.

Leading up to the 1998 return, an airborne observing campaign was organized to mobilize modern observing techniques by Peter Jenniskens at NASA Ames Research Center. There were also efforts to observe impacts of meteoroids, as an example of transient lunar phenomenon, on the Moon in 1999. A particular reason to observe the Moon is that our vantage from a location on Earth sees only meteors coming into the atmosphere relatively close to us while impacts on the Moon would be visible from across the Moon in a single view. The sodium tail of the Moon tripled just after the 1998 Leonid shower which was composed of larger meteoroids (which in the case of the Earth was witnessed as fireballs.) However, in 1999 the sodium tail of the Moon did not change from the Leonid impacts.

Research by Kondrat'eva, Reznikov and colleagues at Kazan University had shown how meteor storms could be accurately predicted, but for some years the worldwide meteor community remained largely unaware of these results. The work of David J. Asher, Armagh Observatory and Robert H. McNaught, Siding Spring Observatory and independently by Esko Lyytinen in 1999, following on from the Kazan research, is considered by most meteor experts as the breakthrough in modern analysis of meteor storms. Whereas previously it was hazardous to guess if there would be a storm or little activity, the predictions of Asher and McNaught timed bursts in activity down to ten minutes by narrowing down the clouds of particles to individual streams from each passage of the comet, and their trajectories amended by subsequent passage near planets. However, whether a specific meteoroid trail will be primarily composed of small or large particles, and thus the relative brightness of the meteors, was not understood. But McNaught did extend the work to examine the placement of the Moon with trails and saw a large chance of a storm impacting in 1999 from a trail while there were less direct impacts from trails in 2000 and 2001 (successive contact with trails through 2006 showed no hits.)

Viewing campaigns resulted in spectacular footage from the 1999, 2001, and 2002 storms which produced up to 3,000 Leonid meteors per hour. Predictions for the Moon's Leonid impacts also noted that in 2000 the side of the Moon facing the stream was away from the Earth, but that impacts should be in number enough to raise a cloud of particles kicked off the Moon which could cause a detectable increase in the sodium tail of the Moon. Research using the explanation of meteor trails/streams have explained the storms of the past. The 1833 storm was not due to the recent passage of the comet, but from a direct impact with the previous 1800 dust trail. The meteoroids from the 1733 passage of Comet Tempel-Tuttle resulted in the 1866 storm and the 1966 storm was from the 1899 passage of the comet. The double spikes in Leonid activity in 2001 and in 2002 were due to the passage of the comet's dust ejected in 1767 and 1866. This ground breaking work was soon applied to other meteor showers – for example the 2004 June Bootids. Peter Jenniskens has published predictions for the next 50 years. However, a close encounter with Jupiter is expected to perturb the comet's path, and many streams, making storms of historic magnitude unlikely for many decades. Recent work tries to take into account the roles of differences in parent bodies and the specifics of their orbits, ejection velocities off the solid mass of the core of a comet, radiation pressure from the Sun, the Poynting–Robertson effect, and the Yarkovsky effect on the particles of different sizes and rates of rotation to explain differences between meteor showers in terms of being predominantly fireballs or small meteors.
Predictions until the end of the 21st century have been published by Mikhail Maslov.

Two appearances of the Leonids frame the story of the novel "Blood Meridian" by Cormac McCarthy.

The 1833 shower is referenced in the fourth section of William Faulkner's short story "The Bear," as published in his novel "Go Down, Moses". As Ike reads the entries chronicling the slaves owned by his family, the recording for Tomy lists her death as June 1833, "Yr stars fell"."

The Leonids feature in the second poem of T.S. Eliot’s ‘Four Quartets’, ‘East Coker’: ”Scorpion fights against the sun / Until the sun and moon go down / Comets weep and Leonids fly.”

In "Pokémon Omega Ruby and Alpha Sapphire", the Leonid is referenced as the Litleonids meteor shower (named after the Pokémon species Litleo). This has a big impact in an event in its post game named "The Delta Episode" in which one of the meteors contain Deoxys.

The shower is part of the plot in the first episode of "The Brokenwood Mysteries", a New Zealand crime series.

In season 3, Episode 8 of "The Big Bang Theory", Leonard, Howard and Raj are off camping to observe the Leonid meteor shower, eat cannabis laced cookies, and end up missing the shower altogether.

In season 1, Episode 15 of "Thunderbirds Are Go", "Relic", Tracy family members, Alan and Scott, travel to the far side of the Moon to rescue one of their father's old friends from an almost-decomissioned moonbase at risk of being destroyed by the Leonid meteor shower. The series is set in the year 2060.





</doc>
<doc id="18384" url="https://en.wikipedia.org/wiki?curid=18384" title="Labarum">
Labarum

The labarum () was a "vexillum" (military standard) that displayed the "Chi-Rho" symbol ☧, a christogram formed from the first two Greek letters of the word "Christ" (, or Χριστός) — "Chi" (χ) and "Rho" (ρ). It was first used by the Roman emperor Constantine the Great. 

Ancient sources draw an unambiguous distinction between the two terms "labarum" and "Chi-Rho", even though later usage sometimes regards the two as synonyms. The name labarum was applied both to the original standard used by Constantine the Great and to the many standards produced in imitation of it in the Late Antique world, and subsequently.

Beyond its derivation from Latin "labarum", the etymology of the word is unclear. The Oxford English Dictionary offers no further derivation from within Latin. Some derive it from Latin /labāre/ 'to totter, to waver' (in the sense of the "waving" of a flag in the breeze) or "laureum [vexillum]" ("laurel standard"). An origin as a loan into Latin from a Celtic language or Basque has also been postulated. There is a traditional Basque symbol called the lauburu; though the name is only attested from the 19th century onwards the motif occurs in engravings dating as early as the 2nd century AD.

On the evening of October 27, 312 AD, with his army preparing for the Battle of the Milvian Bridge, the emperor Constantine I claimed to have had a vision which led him to believe he was fighting under the protection of the Christian God.

Lactantius states that, in the night before the battle, Constantine was commanded in a dream to "delineate the heavenly sign on the shields of his soldiers". Obeying this command, "he marked on their shields the letter X, with a perpendicular line drawn through it and turned round thus at the top, being the cipher of Christ". Having had their shields marked in this fashion, Constantine's troops readied themselves for battle.

From Eusebius, two accounts of a battle survive. The first, shorter one in the "Ecclesiastical History" leaves no doubt that God helped Constantine but does not mention any vision. In his later "Life of Constantine", Eusebius gives a detailed account of a vision and stresses that he had heard the story from the emperor himself. According to this version, Constantine with his army was marching somewhere (Eusebius does not specify the actual location of the event, but it clearly is not in the camp at Rome) when he looked up to the sun and saw a cross of light above it, and with it the Greek words "Ἐν Τούτῳ Νίκα". The traditionally employed Latin translation of the Greek is "in hoc signo vinces"— literally "In this sign, you will conquer." However, a direct translation from the original Greek text of Eusebius into English gives the phrase "By this, conquer!"

At first he was unsure of the meaning of the apparition, but the following night he had a dream in which Christ explained to him that he should use the sign against his enemies. Eusebius then continues to describe the labarum, the military standard used by Constantine in his later wars against Licinius, showing the Chi-Rho sign.

Those two accounts have been merged in popular notion into Constantine seeing the Chi-Rho sign on the evening before the battle. Both authors agree that the sign was not readily understandable as denoting Christ, which corresponds with the fact that there is no certain evidence of the use of the letters chi and rho as a Christian sign before Constantine. Its first appearance is on a Constantinian silver coin from c. 317, which proves that Constantine did use the sign at that time. He made extensive use of the Chi-Rho and the labarum later in the conflict with Licinius.

The vision has been interpreted in a solar context (e.g. as a solar halo phenomenon), which would have been reshaped to fit with the Christian beliefs of the later Constantine.

An alternate explanation of the intersecting celestial symbol has been advanced by George Latura, which claims that Plato's visible god in "Timaeus" is in fact the intersection of the Milky Way and the Zodiacal Light, a rare apparition important to pagan beliefs that Christian bishops reinvented as a Christian symbol.

"A Description of the Standard of the Cross, which the Romans now call the Labarum."
"Now it was made in the following manner. A long spear, overlaid with gold, formed the figure of the cross by means of a transverse bar laid over it. On the top of the whole was fixed a wreath of gold and precious stones; and within this, the symbol of the Saviour’s name, two letters indicating the name of Christ by means of its initial characters, the letter P being intersected by X in its centre: and these letters the emperor was in the habit of wearing on his helmet at a later period. From the cross-bar of the spear was suspended a cloth, a royal piece, covered with a profuse embroidery of most brilliant precious stones; and which, being also richly interlaced with gold, presented an indescribable degree of beauty to the beholder. This banner was of a square form, and the upright staff, whose lower section was of great length, of the pious emperor and his children on its upper part, beneath the trophy of the cross, and immediately above the embroidered banner."

"The emperor constantly made use of this sign of salvation as a safeguard against every adverse and hostile power, and commanded that others similar to it should be carried at the head of all his armies."

The labarum does not appear on any of several standards depicted on the Arch of Constantine, which was erected just three years after the battle. If Eusebius' oath-confirmed account of Constantine's vision and the role it played in his victory and conversion can be trusted, then a grand opportunity for the kind of political propaganda that the Arch was built to present was missed. Many historians have argued that in the early years after the battle, the Emperor had not yet decided to give clear public support to Christianity, whether from a lack of personal faith or because of fear of religious friction. The arch's inscription does say that the Emperor had saved the "res publica" ("by greatness of mind and by instinct [or impulse] of divinity"). Continuing the iconography of his predecessors, Constantine's coinage at the time was inscribed with solar symbolism, interpreted as representing "Sol Invictus" (the Unconquered Sun), Helios, Apollo, or Mithras, but in 325 and thereafter the coinage ceases to be explicitly pagan, and Sol Invictus disappears. And although Eusebius' "Historia Ecclesiae" further reports that Constantine had a statue of himself "holding the sign of the Savior [the cross] in his right hand" erected after his victorious entry into Rome, there are no other reports to confirm such a monument.

Historians still dispute whether Constantine was the first Christian Emperor to support a peaceful transition to Christianity during his rule, or an undecided pagan believer until middle age, and also how strongly influenced he was in his political-religious decisions by his Christian mother St. Helena.

As for the labarum itself, there is little evidence for its use before 317. In the course of Constantine's second war against Licinius in 324, the latter developed a superstitious dread of Constantine's standard. During the attack of Constantine's troops at the Battle of Adrianople the guard of the labarum standard were directed to move it to any part of the field where his soldiers seemed to be faltering. The appearance of this talismanic object appeared to embolden Constantine's troops and dismay those of Licinius. At the final battle of the war, the Battle of Chrysopolis, Licinius, though prominently displaying the images of Rome's pagan pantheon on his own battle line, forbade his troops from actively attacking the labarum, or even looking at it directly.

Constantine felt that both Licinius and Arius were agents of Satan, and associated them with the serpent described in the Book of Revelation (). Constantine represented Licinius as a snake on his coins.

Eusebius stated that in addition to the singular labarum of Constantine, other similar standards (labara) were issued to the Roman army. This is confirmed by the two labara depicted being held by a soldier on a coin of Vetranio (illustrated) dating from 350.

A later Byzantine manuscript indicates that a jewelled labarum standard believed to have been that of Constantine was preserved for centuries, as an object of great veneration, in the imperial treasury at Constantinople. The labarum, with minor variations in its form, was widely used by the Christian Roman emperors who followed Constantine.

A miniature version of the labarum became part of the imperial regalia of Byzantine rulers, who were often depicted carrying it in their right hands.

The term "labarum" can be generally applied to any ecclesiastical banner, such as those carried in religious processions.

"The Holy Lavaro" were a set of early national Greek flags, blessed by the Greek Orthodox Church. Under these banners the Greeks united throughout the Greek Revolution (1821), a war of liberation waged against the Ottoman Empire.

Labarum also gives its name (Labaro) to a suburb of Rome adjacent to Prima Porta, one of the sites where the 'Vision of Constantine' is placed by tradition.




</doc>
<doc id="18385" url="https://en.wikipedia.org/wiki?curid=18385" title="Lactantius">
Lactantius

Lucius Caecilius Firmianus Lactantius (c. 250 – c. 325) was an early Christian author who became an advisor to the first Christian Roman emperor, Constantine I, guiding his religious policy as it developed, and a tutor to his son Crispus. His most important work is the "Institutiones Divinae" ("The Divine Institutes"), an apologetic treatise intended to establish the reasonableness and truth of Christianity to pagan critics.

He is best known for his apologetic works, widely read during the Renaissance by humanists who called Lactantius the "Christian cicero". Also widely attributed to Lactantius is the poem "The Phoenix", which is based on the myth of the phoenix from Oriental mythology. Though the poem is not clearly Christian in its motifs, modern scholars have found some literary evidence in the text to suggest the author had a Christian interpretation of the eastern myth as a symbol of resurrection.

Lactantius, a Latin-speaking North African of Berber origin, was not born into a Christian family. He was a pupil of Arnobius who taught at Sicca Veneria, an important city in Numidia. In his early life, he taught rhetoric in his native town, which may have been Cirta in Numidia, where an inscription mentions a certain "L. Caecilius Firmianus".

Lactantius had a successful public career at first. At the request of the Roman Emperor Diocletian, he became an official professor of rhetoric in Nicomedia; the voyage from Africa is described in his poem "Hodoeporicum" (now lost). There, he associated in the imperial circle with the administrator and polemicist Sossianus Hierocles and the pagan philosopher Porphyry; he first met Constantine, and Galerius, whom he cast as villain in the persecutions. Having converted to Christianity, he resigned his post before Diocletian's purging of Christians from his immediate staff and before the publication of Diocletian's first "Edict against the Christians" (February 24, 303).

As a Latin "rhetor" in a Greek city, he subsequently lived in poverty according to Saint Jerome and eked out a living by writing until Constantine I became his patron. The persecution forced him to leave Nicomedia, perhaps re-locating to North Africa. The Emperor Constantine appointed the elderly Lactantius Latin tutor to his son Crispus in 309-310 who was probably 10-15 years old at the time. Lactantius followed Crispus to Trier in 317, when Crispus was made Caesar (lesser co-emperor) and sent to the city. Crispus was put to death by order of his father Constantine I in 326, but when Lactantius died and under what circumstances are unknown.

Like so many of the early Christian authors, Lactantius depended on classical models. The early humanists called him the "Christian Cicero" ("Cicero Christianus"). A translator of the "Divine Institutes" wrote: "Lactantius has always held a very high place among the Christian Fathers, not only on account of the subject-matter of his writings, but also on account of the varied erudition, the sweetness of expression, and the grace and elegance of style, by which they are characterized."

He wrote apologetic works explaining Christianity in terms that would be palatable to educated people who still practiced the traditional religions of the Empire. He defended Christian beliefs against the criticisms of Hellenistic philosophers. His "Divinae Institutiones" ("Divine Institutes") were an early example of a systematic presentation of Christian thought.

He was considered somewhat heretical after his death, but Renaissance humanists took a renewed interest in him, more for his elaborately rhetorical Latin style than for his theology. His works were copied in manuscript several times in the 15th century and were first printed in 1465 by the Germans Arnold Pannartz and Konrad Sweynheim at the Abbey of Subiaco. This edition was the first book printed in Italy to have a date of printing, as well as the first use of a Greek alphabet font anywhere, which was apparently produced in the course of printing, as the early pages leave Greek text blank. It was probably the fourth book ever printed in Italy. A copy of this edition was sold at auction in 2000 for more than $1 million.

Like many writers in the first few centuries of the early church, Lactantius took a premillennialist view, holding that the second coming of Christ will precede a millennium or a thousand-year reign of Christ on earth. According to Charles E. Hill, "With Lactantius in the early fourth century we see a determined attempt to revive a more “genuine” form of chiliasm." Lactantius quoted the Sibyls extensively (although the Sibylline Oracles are now considered to be pseudepigrapha). Book VII of "The Divine Institutes" indicates a familiarity with Jewish, Christian, Egyptian and Iranian apocalyptic material.

None of the fathers thus far had been more verbose on the subject of the millennial kingdom than Lactantius or more particular in describing the times and events preceding and following. He held to the literalist interpretation of the millennium, that the millennium originates with the second advent of Christ and marks the destruction of the wicked, the binding of the devil and the raising of the righteous dead.

He depicted Jesus reigning with the resurrected righteous on this earth during the seventh thousand years prior to the general judgment. In the end, the devil, having been bound during the thousand years, is loosed; the enslaved nations rebel against the righteous, who hide underground until the hosts, attacking the Holy City, are overwhelmed by fire and brimstone and mutual slaughter and buried altogether by an earthquake: rather unnecessarily, it would seem, since the wicked are thereupon raised again to be sent into eternal punishment. Next, God renews the earth, after the punishment of the wicked, and the Lord alone is thenceforth worshiped in the renovated earth.

Lactantius confidently stated that the beginning of the end would be the fall, or breakup, of the Roman Empire. However, this view fell out of favor with the conversion of Constantine and the improved lot of Christians: "Many Christians felt that any expectation of the downfall of the empire was as disloyal to God as it was to Rome."

Attempts to determine the time of the End were viewed as in contradiction to Acts 1:7: "It is not for you to know the times or seasons that the Father has established by his own authority," and Mark 13:32: "But of that day or hour, no one knows, neither the angels in heaven, nor the Son, but only the Father."






</doc>
<doc id="18386" url="https://en.wikipedia.org/wiki?curid=18386" title="Laconia">
Laconia

Laconia or Lakonia (, , ) is a historical and administrative region of Greece located on the southeastern part of the Peloponnese peninsula. Its administrative capital is Sparta. The word "laconic"—to speak in a blunt, concise way—is derived from the name of this region, a reference to the ancient Spartans who were renowned for their verbal austerity and blunt, often pithy remarks.

Laconia is bordered by Messenia to the west and Arcadia to the north and is surrounded by the Myrtoan Sea to the east and by the Laconian Gulf and the Mediterranean Sea to the south. It encompasses Cape Malea and Cape Tainaron and a large part of the Mani Peninsula. The Mani Peninsula is in the west region of Laconia. The islands of Kythira and Antikythera lie to the south, but they administratively belong to the Attica regional unit of islands. The island, Elafonisos, situated between the Laconian mainland and Kythira, is part of Laconia.

The Eurotas is the longest river in the prefecture. The valley of the Eurotas is predominantly an agricultural region that contains many citrus groves, olive groves, and pasture lands. It is the location of the largest orange production in the Peloponnese and probably in all of Greece. "Lakonia", a brand of orange juice, is based in Amykles.

The main mountain ranges are the Taygetus in the west and the Parnon in the northeast. Taygetus, known as Pentadaktylos ("five-fingers") throughout the Middle Ages, is west of Sparta and the Eurotas valley. It is the highest mountain in Laconia and the Peloponnese and is mostly covered with pine trees. Two roads join the Messenia and Laconia prefectures: one is a tortuous mountain pass through Taygetus and the other bypasses the mountain via the Mani district to the south.

The stalactite cave, Dirou, a major tourist attraction, is located south of Areopolis in the southwest of Laconia.

Laconia has a Mediterranean climate with warm winters and hot summers. Snow is rare on the coast throughout the winter but is very common in the mountains.

Evidence of Neolithic settlement in southern Laconia has been found during excavations of the Alepotrypa cave site. In ancient Greece, this was the principal region of the Spartan state. For much of classical antiquity the Spartan sphere of influence expanded to Messenia, whose inhabitants (the Helots) were enslaved. Significant archaeological recovery exists at the Vaphio-tomb site in Laconia. Found here is advanced Bronze Age art as well as evidence of cultural associations with the contemporaneous Minoan culture on Crete. Laconia was at war with the Kingdom of Macedonia and saw several battles; at the end of the Mycenaean period, the population of Laconia sharply declined. From the early-2nd century BC until 395 AD, it was a part of the Roman Empire.

In the medieval period, Laconia formed part of the Byzantine Empire. Following the Fourth Crusade, it was gradually conquered by the Frankish Principality of Achaea. In the 1260s, however, the Byzantines recovered Mystras and other fortresses in the region and managed to evict the Franks from Laconia, which became the nucleus of a new Byzantine province. By the mid-14th century, this evolved into the Despotate of Morea, held by the last Greek ruling dynasty, the Palaiologoi. With the fall of the Despotate to the Ottomans in 1460, Laconia was conquered as well.

With the exception of a 30-year interval of Venetian rule, Laconia remained under Ottoman control until the outbreak of the Greek War of Independence of 1821. Following independence, Sparta was selected as the capital of the modern prefecture, and its economy and agriculture expanded. With the incorporation of the British-ruled Ionian Islands into Greece in 1864, Elafonissos became part of the prefecture. After World War II and the Greek Civil War, its population began to somewhat decline, as people moved from the villages toward the larger cities of Greece and abroad.

In 1992, a devastating fire ruined the finest olive crops in the northern part of the prefecture, and affected the area of Sellasia along with Oinountas and its surrounding areas. Firefighters, helicopters and planes battled for days to put out the horrific fire.

The Mani portion along with Gytheio became famous in Greece for filming episodes of "Vendetta", broadcast on Mega Channel throughout Greece and abroad on Mega Cosmos.

In early 2006, flooding ruined olive and citrus crops as well as properties and villages along the Eurotas river. In the summer 2006, a terrible fire devastated a part of the Mani Peninsula, ruining forests, crops, and numerous villages.

The regional unit, Laconia, is subdivided into five municipalities. These are (number as in the map in the infobox):

As a part of the 2011 Kallikratis government reform, regional unit Laconia was created out of the former prefecture Laconia (). The prefecture had the same territory as the present regional unit. At the same time, the municipalities were reorganised, according to the table below.

"Note:" Provinces no longer hold any legal status in Greece.


The main cities and towns of Laconia are (ranked by 2011 census population):







</doc>
<doc id="18387" url="https://en.wikipedia.org/wiki?curid=18387" title="Lanista">
Lanista

Lanista is a genus of African bush-crickets (Orthoptera: Tettigoniidae) in the subfamily Conocephalinae.



</doc>
<doc id="18388" url="https://en.wikipedia.org/wiki?curid=18388" title="Laocoön">
Laocoön

Laocoön (; , ), the son of Acoetes, is a figure in Greek and Roman mythology and the Epic Cycle. He was a Trojan priest who was attacked, with his two sons, by giant serpents sent by the gods. The story of Laocoön has been the subject of numerous artists, both in ancient and in more contemporary times.

The most detailed description of Laocoön's grisly fate was provided by Quintus Smyrnaeus in "Posthomerica", a later, literary version of events following the "Iliad". According to Quintus, Laocoön begged the Trojans to set fire to the horse to ensure it was not a trick. Athena, angry with him and the Trojans, shook the ground around Laocoön's feet and painfully blinded him. The Trojans, watching this unfold, assumed Laocoön was punished for the Trojans' mutilating and doubting Sinon, the undercover Greek soldier sent to convince the Trojans to let him and the horse inside their city walls. Thus, the Trojans wheeled the great wooden Horse in. Laocoön did not give up trying to convince the Trojans to burn the horse, and Athena made him pay even further. She sent two giant sea-serpents to strangle and kill him and his two sons. In another version of the story, it was said that Poseidon sent the sea-serpents to strangle and kill Laocoön and his two sons. 

According to Apollodorus, it was Apollo who sent the two sea-serpents. Laocoön had insulted Apollo by sleeping with his wife in front of the "divine image".

Virgil used the story in the "Aeneid." According to Virgil, Laocoön advised the Trojans to not receive the horse from the Greeks. They disregarded Laocoön's advice and were taken in by the deceitful testimony of Sinon. The enraged Laocoön threw his spear at the Horse in response. Minerva then sent sea-serpents to strangle Laocoön and his two sons, Antiphantes and Thymbraeus, for his actions. "Laocoön, ostensibly sacrificing a bull to Neptune on behalf of the city (lines 201ff.), becomes himself the tragic victim, as the simile (lines 223–24) makes clear. In some sense, his death must be symbolic of the city as a whole," S. V. Tracy notes. According to the Hellenistic poet Euphorion of Chalcis, Laocoön is in fact punished for procreating upon holy ground sacred to Poseidon; only unlucky timing caused the Trojans to misinterpret his death as punishment for striking the Horse, which they bring into the city with disastrous consequences. The episode furnished the subject of Sophocles' lost tragedy, "Laocoön".

In "Aeneid", Virgil describes the circumstances of Laocoön's death:

The story of Laocoön is not mentioned by Homer, but it had been the subject of a tragedy, now lost, by Sophocles and was mentioned by other Greek writers, though the events around the attack by the serpents vary considerably. The most famous account of these is now in Virgil's "Aeneid" where Laocoön was a priest of Neptune (Poseidon), who was killed with both his sons after attempting to expose the ruse of the Trojan Horse by striking it with a spear.
Virgil gives Laocoön the famous line "Equō nē crēdite, Teucrī / Quidquid id est, timeō Danaōs et dōna ferentēs", or "Do not trust the Horse, Trojans / Whatever it is, I fear the Greeks even bearing gifts." This line is the source of the saying: "Beware of Greeks bearing gifts."

In Sophocles, however, he was a priest of Apollo who should have been celibate but had married. The serpents killed only the two sons, leaving Laocoön himself alive to suffer. In other versions, he was killed for having committed an impiety by making love with his wife in the presence of a cult image in a sanctuary, or simply making a sacrifice in the temple with his wife present. In this second group of versions, the snakes were sent by Poseidon and in the first by Poseidon and Athena, or Apollo, and the deaths were interpreted by the Trojans as proof that the horse was a sacred object. The two versions have rather different morals: Laocoön was either punished for doing wrong, or for being right.

The death of Laocoön was famously depicted in a much-admired marble "Laocoön and His Sons", attributed by Pliny the Elder to the Rhodian sculptors Agesander, Athenodoros, and Polydorus, which stands in the Vatican Museums, Rome. Copies have been executed by various artists, notably Baccio Bandinelli. These show the complete sculpture (with conjectural reconstructions of the missing pieces) and can be seen in Rhodes, at the Palace of the Grand Master of the Knights of Rhodes, Rome, the Uffizi Gallery in Florence and in front of the Archaeological Museum, Odessa, Ukraine, amongst others. Alexander Calder also designed a stabile which he called Laocoön in 1947; it's part of the Eli and Edyth Broad collection in Los Angeles.

The marble Laocoön provided the central image for Lessing's "Laocoön", 1766, an aesthetic polemic directed against Winckelmann and the comte de Caylus. Daniel Albright reengages the role of the figure of Laocoön in aesthetic thought in his book "Untwisting the Serpent: Modernism in Literature, Music, and Other Arts". [cite El Greco painting]

In addition to other literary references, John Barth employs a bust of Laocoön in his novella, "The End of the Road". The R.E.M. song "Laughing" references Laocoön, rendering him female ("Laocoön and her two sons"), they also reference Laocoön in the song "Harborcoat". The marble's pose is parodied in the comic book "Asterix and the Laurel Wreath". American author Joyce Carol Oates also references Laocoön in her 1989 novel "American Appetites". In Stave V of "A Christmas Carol", by Charles Dickens (1843), Scrooge awakes on Christmas morning, "making a perfect Laocoon of himself with his stockings". Barbara Tuchman's "The March of Folly" begins with an extensive analysis of the Laocoön story. The American feminist poet and author Marge Piercy includes a poem titled, "Laocoön is the name of the figure", in her collection "Stone, Paper, Knife" (1983), relating love lost and beginning. John Steinbeck references Laocoön in his American literary classic "East of Eden", referring to a picture of “Laocoön completely wrapped in snakes” when describing artwork hanging in classrooms at the Salinas schoolhouse. 

In Hector Berlioz's opera "Les Troyens", the death of Laocoön is a pivotal moment of the first act after Aeneas' entrance, sung by eight singers and a double choir ("ottetto et double chœur"). It begins with the verse "Châtiment effroyable" ("frightful punishment").



Compiled by Tracy, 1987:452 note 3, which also mentions a fragmentary line possibly by Nicander.



</doc>
<doc id="18389" url="https://en.wikipedia.org/wiki?curid=18389" title="Limburg an der Lahn">
Limburg an der Lahn

Limburg an der Lahn (officially abbreviated "Limburg a. d. Lahn") is the district seat of Limburg-Weilburg in Hesse, Germany.

Limburg lies in western Hessen between the Taunus and the Westerwald on the river Lahn.

The town lies roughly centrally in a basin within the Rhenish Slate Mountains which is surrounded by the low ranges of the Taunus and Westerwald and called the Limburg Basin ("Limburger Becken"). Owing to the favourable soil and climate, the Limburg Basin stands as one of Hesse's richest agricultural regions and moreover, with its convenient Lahn crossing, it has been of great importance to transport since the Middle Ages. Within the basin, the Lahn's otherwise rather narrow lower valley broadens out noticeably, making Limburg's mean elevation only 117 m above sea level.

Limburg forms, together with the town of Diez, a middle centre (in terms of Central place theory) but partially functions as an upper centre to western Middle Hesse.

Limburg's residential neighbourhoods reach beyond the town limits; the neighbouring centres of Elz and Diez run seamlessly together.

Surrounding towns and communities are the community of Elz and the town of Hadamar in the north, the community of Beselich in the northeast, the town of Runkel in the east, the communities of Villmar and Brechen in the southeast, the community of Hünfelden in the south (all in Limburg-Weilburg), the community of Holzheim in the southwest, and the town of Diez and the communities of Aull and Gückingen in the west (all in the Rhein-Lahn-Kreis in Rhineland-Palatinate).

The nearest major cities are Wetzlar and Gießen to the north east, Wiesbaden and Frankfurt to the south and Koblenz to the west.

The town consists of eight formerly autonomous "Ortsteile" or villages, listed here by population.


Blumenrod is also often called a constituent community, although this is actually only a big residential neighbourhood in the main town's south end. Its landmark is the "Domäne Blumenrod", a former manor house that has been restored and remodelled by the Limburg Free Evangelical community.

Limburg's biggest outlying centre is Lindenholzhausen (3,329 residents as of June 2006); the second biggest is Linter.

The derivation of the name “Limburg” is not quite clear and may well hearken back to a castle built here ("Burg" means "castle" in German). In 910 the town was first mentioned as "Lintpurc". Two of the popular theories are:

About 800 A.D., the first castle buildings arose on the Limburg crags. This was probably designed for the protection of a ford over the river Lahn. In the decades that followed, the town developed under the castle's protection. Limburg is first mentioned in documents in 910 under the name of "Lintpurc" when Louis the Child granted Konrad Kurzbold an estate in the community on which he was to build a church. Konrad Kurzbold laid the foundation stone for Saint George's Monastery Church, where he was also buried. The community soon increased in importance with the monastery's founding and profited from the lively goods trade on the "Via Publica".

In 1150, a wooden bridge was built across the Lahn. The long-distance road from Cologne to Frankfurt am Main subsequently ran through Limburg. In the early 13th century, Limburg Castle was built in its current form. Shortly afterwards, the town passed into the ownership of the Lords of Ysenburg. In 1214, the community was granted town rights. Remains of the fortification wall from the years 1130, 1230 and 1340 with a maximum length of roughly one thousand metres indicate to this day the blossoming town's quick development in the Middle Ages. There is proof of a mint in Limburg in 1180.

One line of the Lords of Ysenburg resided from 1258 to 1406 at Limburg Castle and took their name from their seat, Limburg. From this line came the House of Limburg-Stirum and also Imagina of Isenburg-Limburg, German King Adolf's wife.

The ruling class among the mediaeval townsfolk were rich merchant families whose houses stood right near the castle tower and were surrounded by the first town wall once it was built. The area of today's Rossmarkt ("Horse Market"), in which many simple craftsmen lived, was only brought within the fortifications once the second town wall was built. The inhabitants there, however, unlike the merchant élite, were accorded no entitlement to a voice in town affairs and were not allowed to send representatives to the town council. Nevertheless, they had to bear the main financial burden of running the town. Only in 1458 were they allowed to send two representatives to town council.

Saint George's Cathedral ("Sankt-Georgs-Dom") built on the old monastery church's site, and also called "Georgsdom", was consecrated in 1235. On 14 May 1289, a devastating fire wiped out great parts of the inner town, although these were subsequently rebuilt. One of the houses built at that time was the Römer 2-4-6, which is today one of Germany's oldest half-timbered houses. In 1337, Limburg's Jews were expelled from the town. Only in 1341 were they once again able to settle in the town, by royal decree. In 1344 a half share of the town was pledged to the Electorate of Trier, and in 1420, the town passed wholly into the ownership of Trier. This event, along with another town fire in 1342, the Black Death in 1349, 1356 and 1365, but above all the rise of the Territorial Princes, led to a gradual decline. In 1315 and 1346, the old stone Lahn Bridge was built (presumably in two sections).

Against the background of the German Peasants' War, unrest also arose among the townsfolk in 1525. After the Elector of Trier had demanded that the townsmen turn a Lutheran preacher out of the town, a board made up of townsmen who were ineligible for council functions handed the council a 30-point comprehensive list of demands on 24 May. It dealt mainly with financial participation and equality in taxation, trade and building issues with the merchant class. In the days that followed, these demands were reduced in negotiations between the council and the board to 16 points, which were likely also taken up with the Elector afterwards. On 5 August, however, Archbishop Richard ordered the council to overturn all concessions to the townsmen. Furthermore, a ban on assembly was decreed, and the ineligible townsmen were stripped of their right to send two representatives to council.

In 1806, Limburg came into the possession of the newly founded Duchy of Nassau. In 1818 the town wall was torn down. In 1827 the town was raised to a Catholic episcopal seat. In 1866 the Duchy and with it Limburg passed to Prussia in the wake of the Austro-Prussian War. As of 1862, Limburg became a railway hub and from 1886 a district seat. In 1892, the Pallottines settled in town, but only the men; the women came in 1895.

During World War I there was a major prisoner of war camp at Limburg an der Lahn. Many Irish members of the British Army were interned there until the end of the war and at one stage they were visited by the Irish republican leader Roger Casement in an attempt to win recruits for the forthcoming Irish rebellion.

From 1919 to 1923, Limburg was the "capital" of a short-lived state called Free State Bottleneck (or "Freistaat Flaschenhals" in German) because it was the nearest unoccupied town to the Weimar Republic.

The municipal election held on 6 March 2016 yielded the following results:
The town's mayor is currently Marius Hahn (SPD).


In 1956, a "Patenschaft" – roughly, a sponsorship – was undertaken for Sudeten Germans driven out of the town of Mährisch Neustadt in the Sternberg district.

Limburg is a traditional transportation hub. Already in the Middle Ages, the "Via Publica" crossed the navigable Lahn here. Today the A 3 (Emmerich–Oberhausen–Cologne–Frankfurt–Nuremberg–Passau) and "Bundesstraße" 8, which both follow the "Via Publica's" alignment as closely as possible, run through the town. "Bundesstraße" 49 links Limburg to Koblenz towards the west and Wetzlar and Gießen towards the east. The section between Limburg and Wetzlar is currently being widened to four lanes. This section as far as Obertiefenbach is also known as "Die lange Meil" ("The Long Mile"). "Bundesstraße" 54 links Limburg on the one hand with Siegen to the north and on the other by way of Diez with Wiesbaden, which may likewise be reached over "Bundesstraße" 417 ("Hühnerstraße").

As early as 1248, a wooden bridge spanned the Lahn, but was replaced after the flooding in 1306 by a stone bridge, the "Alte Lahnbrücke". Other road bridges are the "Lahntalbrücke Limburg" (1964) on the A 3, the "Lahnbrücke" near Staffel and the "Neue Lahnbrücke" from 1968, over which run the "Bundesstraßen" before they cross under the inner town through the "Schiedetunnel", a bypass tunnel.

Once the "Lahntalbahn" had been built, Limburg was joined to the railway network in 1862. Limburg railway station developed into a transport hub. Eschhofen station is also in Limburg. Other railway lines are the "Unterwesterwaldbahn", the "Oberwesterwaldbahn" and the Main-Lahn Railway. At Niedernhausen station on the "Main-Lahn Railway", transfer to the "Ländchesbahn" to Wiesbaden is possible. With the exception of the upper section of the "Lahntalbahn" and express lines to Koblenz and Frankfurt, which are still served by Deutsche Bahn, all railway lines are run by Vectus Verkehrsgesellschaft mbH, based in Limburg.

Once the InterCityExpress Cologne-Frankfurt high-speed rail line had been built, Limburg acquired an ICE station. It is the only railway station in Germany at which exclusively ICE trains stop. The high-speed rail line crosses the Lahn over the "Lahntalbrücke" and then dives into the "Limburger Tunnel".

The nearest airport is Frankfurt Airport, 63 km away on the A 3. Travel time there on the ICE is roughly 20 minutes. Cologne Bonn Airport is 110 km away and can be reached on the ICE in 44 minutes.

The Lahn between Lahnstein and Wetzlar is a "Bundeswasserstraße" ("Federal waterway"). Since the "Lahntalbahn's" expansion, however, the waterway's importance has been declining. It is used mainly by tourists with small motorboats, canoes and rowboats. Limburg is the landing site of the tourboat "Wappen von Limburg".


Limburg has four schools which lead to, among other qualifications, the Abitur:

Professional training schools:

Hauptschulen and Realschulen:

Libraries:

The hospital perched on the Schafsberg overlooking the town has at its disposal 433 beds and 15 specialist departments.

In Limburg there are various sport clubs; some are even represented in "Bundesligen", and even at the world level.

The Evangelical Church offers with its "Jugendfreizeitstätte Limburg" (JFS for short, meaning "Youth Leisure Place") a meeting place for youth with many events. With table football, Internet café and many events, this institution is not only church-based, with two staff and a "Zivildienstleistender" supporting the visitors not only with their problems.

The "Mütterzentrum Limburg" is a family meeting place for those with or without children on Hospitalstraße. The club is supported by the town of Limburg and the "Bundesland" of Hesse and offers among other things a parents' service that looks after children, a broad array of course offerings for children and adults, a miniature kindergarten and a café.


The cabaret troupe "Thing", founded more than 25 years ago, moved after a short time from its initial home in the outlying centre of Staffel to the Josef-Kohlmaier-Halle, a civic event hall, where its stage can now be found in the hall's club rooms. The troupe is run by an independent acting club. On the programme are chanson, cabaret, literature and jazz as well as folk, Rock and performances by singer-songwriters. It makes a point of furthering young artists. Each month, three or four events are staged.

The dedication of "Thing" was recognized on 6 December 2003 when the "Kulturpreis Mittelhessen" ("Middle Hesse Culture Prize") was awarded to it.

Limburg Cathedral has a famous boys' choir, the Limburger Domsingknaben, which trains at Musical Boarding School in Hadamar, and an excellent girls' choir, the Mädchenkantorei Limburg, both singing at the Limburg Cathedral and internationally.

In Limburg there are several museums. The most important are:

Only a few towns, like Limburg, have been able to keep a full set of nearly unscathed mediaeval buildings. The formerly walled town core between St. George's Cathedral, Grabenstraße (a street marking the old town moat) and the 600-year-old Lahn Bridge thus stands today as a whole under monumental protection.

The "Altstadt" ("Old Town") boasts a fine cathedral and is full of narrow streets with timber-frame houses, dating mainly from the 17th and 18th centuries. That's why it is located on the German Timber-Frame Road.

(limited to those featuring in Wikipedia (EN) 






</doc>
<doc id="18390" url="https://en.wikipedia.org/wiki?curid=18390" title="Lavrentiy Beria">
Lavrentiy Beria

Lavrentiy Pavlovich Beria (; ; , ;  – 23 December 1953) was a Soviet politician, Marshal of the Soviet Union and state security administrator, chief of the Soviet security, and chief of the People's Commissariat for Internal Affairs (NKVD) under Joseph Stalin during World War II, and promoted to deputy premier under Stalin from 1941. He later officially joined the Politburo in 1946.

Beria was the longest-lived and most influential of Stalin's secret police chiefs, wielding his most substantial influence during and after World War II. Following the Soviet invasion of Poland in 1939, he was responsible for organising the Katyn massacre. He simultaneously administered vast sections of the Soviet state, and acted as the "de facto" Marshal of the Soviet Union in command of NKVD field units responsible for barrier troops and Soviet partisan intelligence and sabotage operations on the Eastern Front during World War II. Beria administered the vast expansion of the Gulag labour camps, and was primarily responsible for overseeing the secret detention facilities for scientists and engineers known as .

Beria attended the Yalta Conference with Stalin, who introduced him to U.S. President Franklin D. Roosevelt as "our Himmler". After the war, he organised the communist takeover of the state institutions in central and eastern Europe and political repressions in these countries. Beria's uncompromising ruthlessness in his duties and skill at producing results culminated in his success in overseeing the Soviet atomic bomb project. Stalin gave it absolute priority, and the project was completed in under five years.

After Stalin's death in March 1953, Beria became First Deputy Chairman of the Council of Ministers and head of the Ministry of Internal Affairs. In this dual capacity, he formed a troika, alongside Georgy Malenkov and Vyacheslav Molotov, that briefly led the country in Stalin's place. A coup d'état by Nikita Khrushchev, with help from Marshal of the Soviet Union Georgy Zhukov in June 1953, removed Beria from power. He was arrested on charges of 357 counts of rape and treason. He was sentenced to death and was executed on 23 December 1953.

Beria was born in Merkheuli, near Sukhumi, in the Sukhum Okrug of the Kutais Governorate (now Gulripshi District, "de facto" Republic of Abkhazia, or Georgia, then part of the Russian Empire). He was from the Mingrelian ethnic subgroup and grew up in a Georgian Orthodox family. Beria's mother, Marta Jaqeli (1868–1955), was deeply religious and church-going (she spent much time in church and died in a church building). She was previously married and widowed before marrying Beria's father, Pavel Khukhaevich Beria (1872–1922), a landowner from Abkhazia. Beria also had a brother (name unknown), and a deaf sister named Anna.

In his autobiography, Lavrentiy Beria mentioned only his sister and his niece, implying that his brother was (or any other siblings were) dead or had no relationship with Beria after he left Merkheuli. Beria attended a technical school in Sukhumi, and joined the Bolsheviks in March 1917 while a student in the Baku Polytechnicum (subsequently known as the Azerbaijan State Oil Academy). As a student, Beria distinguished himself in mathematics and the sciences. The Polytechnicum's curriculum concentrated on the petroleum industry.

Beria also worked for the anti-Bolshevik Mussavatists in Baku. After the Red Army captured the city on 28 April 1920, Beria was saved from execution because there was not enough time to arrange his shooting and replacement, and Sergei Kirov possibly intervened. While in prison, he formed a connection with Nina Gegechkori (1905–1991) his cellmate's niece, and they eloped on a train. She was 17, a trained scientist from an aristocratic family.

In 1919, at the age of twenty, Beria started his career in state security when the security service of the Azerbaijan Democratic Republic hired him while he was still a student at the Polytechnicum. In 1920 or 1921 (accounts vary) Beria joined the Cheka, the original Bolshevik secret police. At that time, a Bolshevik revolt took place in the Menshevik-controlled Democratic Republic of Georgia, and the Red Army subsequently invaded. The Cheka became heavily involved in the conflict, which resulted in the defeat of the Mensheviks and the formation of the Georgian SSR. By 1922, Beria was deputy head of the Georgian branch of Cheka's successor, the OGPU.

In 1924, he led the repression of a Georgian nationalist uprising, after which up to 10,000 people were executed. For this display of "Bolshevik ruthlessness", Beria was appointed head of the "secret-political division" of the Transcaucasian OGPU and was awarded the Order of the Red Banner.

In 1926, Beria became head of the Georgian OGPU; Sergo Ordzhonikidze, head of the Transcaucasian party, introduced him to fellow-Georgian Iosef Dzhughashvili, later known as Joseph Stalin. As a result, Beria became an ally in Stalin's rise to power. During his years at the helm of the Georgian OGPU, Beria effectively destroyed the intelligence networks that Turkey and Iran had developed in the Soviet Caucasus, while successfully penetrating the governments of these countries with his agents. He also took over Stalin's holiday security.

Beria was appointed Secretary of the Communist Party in Georgia in 1931, and for the whole Transcaucasian region in 1932. He became a member of the Central Committee of the Communist Party of the Soviet Union in 1934. During this time, he began to attack fellow members of the Georgian Communist Party, particularly Gaioz Devdariani, who served as Minister of Education of the Georgian SSR. Beria ordered the executions of Devdariani's brothers George and Shalva, who held important positions in the Cheka and the Communist Party respectively.

He reportedly won Stalin's favour in the early 1930s, after faking a conspiracy to assassinate the Soviet leader that he then claimed to have foiled. By 1935, Beria had become one of Stalin's most trusted subordinates. He cemented his place in Stalin's entourage with a lengthy oration titled, "On the History of the Bolshevik Organisations in Transcaucasia" (later published as a book), which emphasised Stalin's role. When Stalin's purge of the Communist Party and government began in 1934 after the assassination of Leningrad party boss Sergei Kirov (1 December 1934), Beria ran the purges in Transcaucasia. He used the opportunity to settle many old scores in the politically turbulent Transcaucasian republics.

In June 1937, he said in a speech, "Let our enemies know that anyone who attempts to raise a hand against the will of our people, against the will of the party of Lenin and Stalin, will be mercilessly crushed and destroyed."

In August 1938, Stalin brought Beria to Moscow as deputy head of the People's Commissariat for Internal Affairs (NKVD), the ministry which oversaw the state security and police forces. Under Nikolai Yezhov, the NKVD carried out the Great Purge: the imprisonment or execution of millions of people throughout the Soviet Union as alleged "enemies of the people". By 1938, however, the oppression had become so extensive that it was damaging the infrastructure, economy and even the armed forces of the Soviet state, prompting Stalin to wind the purge down. Stalin had voted to appoint Georgy Malenkov as head of the NKVD, but he was over-ruled. In September, Beria was appointed head of the Main Administration of State Security (GUGB) of the NKVD, and in November he succeeded Yezhov as NKVD head. Yezhov was executed in 1940, and one account says he was personally strangled by Beria. The NKVD was purged next, with half of its personnel replaced by Beria loyalists, many of them from the Caucasus.

Although Beria's name is closely identified with the Great Purge because of his activities while deputy head of the NKVD, his leadership of the organisation marked an easing of the repression begun under Yezhov. Over 100,000 people were released from the labour camps. The government officially admitted that there had been some injustice and "excesses" during the purges, which were blamed entirely on Yezhov. The liberalisation was only relative: arrests and executions continued, and in 1940, as war approached, the pace of the purges again accelerated. During this period, Beria supervised deportations of people identified as political enemies from Poland and the Baltic states after Soviet occupation of those regions.

In March 1939, Beria became a candidate member of the Communist Party's Politburo. Although he did not become a full member until 1946, he was already one of the senior leaders of the Soviet state. In 1941, Beria was made a Commissar General of State Security, the highest quasi-military rank within the Soviet police system of that time, effectively comparable to a Marshal of the Soviet Union.

On 5 March 1940, after the Gestapo–NKVD Third Conference was held in Zakopane, Beria sent a note (no. 794/B) to Stalin in which he stated that the Polish prisoners of war kept at camps and prisons in western Belarus and Ukraine were enemies of the Soviet Union, and recommended their execution. Most of them were military officers, but there were also intelligentsia, doctors, priests, and others in a total of 22,000 people. With Stalin's approval, Beria's NKVD executed them in what became known as the Katyn massacre.

From October 1940 to February 1942, the NKVD under Beria carried out a new purge of the Red Army and related industries. In February 1941, Beria became Deputy chairman of the Council of People's Commissars, and in June, following Nazi Germany's invasion of the Soviet Union, he became a member of the State Defense Committee (GKO). During World War II, he took on major domestic responsibilities and mobilised the millions of people imprisoned in NKVD Gulag camps into wartime production. He took control of the manufacture of armaments, and (with Georgy Malenkov) aircraft and aircraft engines. This was the beginning of Beria's alliance with Malenkov, which later became of central importance.

In 1944, as the Germans were driven from Soviet soil, Beria was in charge of dealing with the various ethnic minorities accused of anti-sovietism and/or collaboration with the invaders, including the Balkars, Karachays, Chechens, Ingush, Crimean Tatars, Kalmyks, Pontic Greeks, and Volga Germans. All these groups were deported to Soviet Central Asia (see "Population transfer in the Soviet Union").

In December 1944, Beria's NKVD was assigned to supervise the Soviet atomic bomb project ("Task No. 1"), which built and tested a bomb by 29 August 1949. The project was extremely labour-intensive. At least 330,000 people, including 10,000 technicians, were involved. The Gulag system provided tens of thousands of people for work in uranium mines and for the construction and operation of uranium processing plants. They also constructed test facilities, such as those at Semipalatinsk and in the Novaya Zemlya archipelago. The NKVD also ensured the necessary security for the project.

In July 1945, as Soviet police ranks were converted to a military uniform system, Beria's rank was officially converted to that of Marshal of the Soviet Union. Although he had never held a traditional military command, Beria made a significant contribution to the victory of the Soviet Union in World War II through his organisation of wartime production and his use of partisans. Stalin personally never thought much of it, and neither commented publicly on his performance nor awarded him recognition (i.e. Order of Victory), as he did for most other Soviet Marshals.

Abroad, Beria had met with Kim Il-sung, the future leader of North Korea, several times when the Soviet troops had declared war on Japan and occupied the northern half of Korea from August 1945. Beria recommended that Stalin install a communist leader in the occupied territories.

With Stalin nearing 70, a concealed struggle for succession amongst his entourage dominated Kremlin politics in the post-war years. At the end of the war, Andrei Zhdanov seemed the most likely candidate. Zhdanov had served as the Communist Party leader in Leningrad during the war, and by 1946 had charge of all cultural matters. After 1946, Beria formed an alliance with Malenkov to counter Zhdanov's rise.

In January 1946, Beria resigned as chief of the NKVD while retaining general control over national security matters as Deputy Prime Minister and Curator of the Organs of State Security under Stalin. However, the new NKVD chief, Sergei Kruglov, was not a Beria man. Also, by the summer of 1946, Beria's man Vsevolod Nikolayevich Merkulov was replaced as head of the Ministry for State Security (MGB) by Viktor Abakumov. Abakumov had headed SMERSH from 1943 to 1946; his relationship with Beria involved close collaboration (since Abakumov owed his rise to Beria's support and esteem), but also rivalry. Stalin had begun to encourage Abakumov to form his own network inside the MGB to counter Beria's dominance of the power ministries. Kruglov and Abakumov moved expeditiously to replace Beria's men in the security apparatus leadership with new people. Very soon, Deputy Minister Stepan Mamulov of the Soviet Ministry of Internal Affairs was the only close Beria ally left outside foreign intelligence, on which Beria kept a grip. In the following months, Abakumov started carrying out important operations without consulting Beria, often working in tandem with Zhdanov, and on Stalin's direct orders. These operations were aimed by Stalininitially tangentially, but with time more directlyat Beria.

One of the first such moves involved the Jewish Anti-Fascist Committee affair, which commenced in October 1946 and eventually led to the murder of Solomon Mikhoels and the arrest of many other members. This affair damaged Beria; not only had he championed the creation of the committee in 1942, but his own entourage included a substantial number of Jews.

After Zhdanov died suddenly in August 1948, Beria and Malenkov consolidated their power by means of a purge of Zhdanov's associates in the so-called "Leningrad Affair". Those executed included Zhdanov's deputy, Alexey Kuznetsov; the economic chief, Nikolai Voznesensky; the Party head in Leningrad, Pyotr Popkov; and the Prime Minister of the Russian Republic, Mikhail Rodionov.

During the postwar years, Beria supervised installation of Communist regimes in the countries of Eastern Europe and hand-picked the Soviet-backed leaders. Starting in 1948, Abakumov initiated several investigations against these leaders, which culminated with the arrest in November 1951 of Rudolf Slánský, Bedřich Geminder, and others in Czechoslovakia. These men were frequently accused of Zionism, "rootless cosmopolitanism", and providing weapons to Israel. Such charges deeply disturbed Beria, as he had directly ordered the sale of large amounts of Czech arms to Israel. Altogether, 14 Czechoslovak Communist leaders, 11 of them Jewish, were tried, convicted, and executed (see Slánský trial). Similar investigations in Poland and other Soviet satellite countries occurred at the same time.

In 1951, Abakumov was replaced by Semyon Ignatyev, who further intensified the anti-Semitic campaign. On 13 January 1953, the biggest anti-Semitic affair in the Soviet Union started with an article in "Pravda"it began what became known as the Doctors' plot, in which a number of the country's prominent Jewish physicians were accused of poisoning top Soviet leaders and arrested. Concurrently, the Soviet press began an anti-Semitic propaganda campaign, euphemistically termed the "struggle against rootless cosmopolitanism". Initially, 37 men were arrested, but the number quickly grew into hundreds. Scores of Soviet Jews were dismissed from their jobs, arrested, sent to the Gulag, or executed. The "Doctors' plot" was presumably invented by Stalin as an excuse to dismiss Beria and replace him with Ignatiev or some other MGB functionary. A few days after Stalin's death on 5 March 1953, Beria freed all the arrested doctors, announced that the entire matter was fabricated, and arrested the MGB functionaries directly involved.

In other international issues, Beria (along with Mikoyan) correctly foresaw the victory (1949–1950) of Mao Zedong in the Chinese Civil War and greatly helped the Chinese Communists by letting them use Soviet-occupied Manchuria as a staging area and arranging large weapons shipments to the People's Liberation Army, mainly from the recently captured equipment of the Japanese Kwantung Army.

At Beria's trial in 1953, it became known that he had committed numerous rapes during the years he was NKVD chief. Simon Sebag-Montefiore, a biographer of Stalin, concluded the information "reveals a sexual predator who used his power to indulge himself in obsessive depravity". After his death, charges of sexual abuse and rape were disputed by people close to him including his wife Nina and his son Sergo.

According to official testimony, in Soviet archives, of Colonel Rafael Semyonovich Sarkisov and Colonel Sardion Nikolaevich Nadaraiatwo of Beria's bodyguardson warm nights during the war Beria was often driven around Moscow in his limousine. He would point out young women to be taken to his mansion, where wine and a feast awaited them. After dining, Beria would take the women into his soundproofed office and rape them. Beria's bodyguards reported that their duties included handing each victim a flower bouquet as she left the house. Accepting it implied that the sex had been consensual; refusal would mean arrest. Sarkisov reported that after one woman rejected Beria's advances and ran out of his office, Sarkisov mistakenly handed her the flowers anyway. The enraged Beria declared, "Now it's not a bouquet, it's a wreath! May it rot on your grave!" The NKVD arrested the woman the next day.

Women also submitted to Beria's sexual advances in exchange for the promise of freedom for imprisoned relatives. In one case, Beria picked up Tatiana Okunevskaya, a well-known Soviet actress, under the pretence of bringing her to perform for the Politburo. Instead he took her to his dacha, where he offered to free her father and grandmother from prison if she submitted. He then raped her, telling her: "Scream or not, it doesn't matter." In fact Beria knew that Okunevskaya's relatives had been executed months earlier. Okunevskaya was arrested shortly afterwards and sentenced to solitary confinement in the Gulag, which she survived.

Beria's sexually predatory nature was well known to the Politburo, and though Stalin took an indulgent viewpoint (considering Beria's wartime importance), he expressed distrust of Beria. In one instance, when Stalin learned his daughter Svetlana was alone with Beria at his house, he telephoned her and told her to leave immediately. When Beria complimented Alexander Poskrebyshev's daughter on her beauty, Poskrebyshev quickly pulled her aside and instructed her, "Don't ever accept a lift from Beria." After taking an interest in Marshal of the Soviet Union Kliment Voroshilov's daughter-in-law during a party at their summer dacha, Beria shadowed their car closely all the way back to the Kremlin, terrifying Voroshilov's wife.

Before and during the war, Beria directed Sarkisov to keep a list of the names and phone numbers of his sexual encounters. Eventually, he ordered Sarkisov to destroy the list as a security risk, but Sarkisov retained a secret copy. When Beria's fall from power began, Sarkisov passed the list to Viktor Abakumov, the former wartime head of SMERSH and now chief of the MGBthe successor to the NKVD. Abakumov was already aggressively building a case against Beria. Stalin, who was also seeking to undermine Beria, was thrilled by the detailed records kept by Sarkisov, demanding: "Send me everything this asshole writes down!" Sarkisov reported that Beria had contracted syphilis during the war, for which he was secretly treated (a fact Beria later admitted during his interrogation). The Russian government acknowledged Sarkisov's handwritten list of Beria's victims in 2003; the victims' names will be released in 2028.

Evidence suggests that Beria murdered some of these women. In the mid 1990s, the skeletal remains of several young women were discovered in the garden of his Moscow villa (now the Tunisian Embassy). According to Martin Sixsmith, in a BBC documentary, "Beria spent his nights having teenagers abducted from the streets and brought here for him to rape. Those who resisted were strangled and buried in his wife's rose garden."

The testimony of Sarkisov and Nadaraia has been partially corroborated by Edward Ellis Smith, an American who served in the U.S. embassy in Moscow after the war. According to historian Amy Knight, "Smith noted that Beria's escapades were common knowledge among embassy personnel because his house was on the same street as a residence for Americans, and those who lived there saw girls brought to Beria's house late at night in a limousine."

Khrushchev wrote in his memoirs that Beria had, immediately after Stalin's stroke, gone about "spewing hatred against [Stalin] and mocking him". When Stalin showed signs of consciousness, Beria dropped to his knees and kissed his hand. When Stalin fell unconscious again, Beria immediately stood and spat. Stalin's aide Vasili Lozgachev reported that Beria and Malenkov were the first members of the Politburo to see Stalin's condition when he was found unconscious. They arrived at Stalin's dacha at Kuntsevo at 03:00 on 2 March, after being called by Khrushchev and Bulganin. The latter two did not want to risk Stalin's wrath by checking themselves. Lozgachev tried to explain to Beria that the unconscious Stalin (still in his soiled clothing) was "sick and needed medical attention". Beria angrily dismissed his claims as panic-mongering and quickly left, ordering him, "Don't bother us, don't cause a panic and don't disturb Comrade Stalin!" Calling a doctor was deferred for a full 12 hours after Stalin was rendered paralysed, incontinent and unable to speak. This decision is noted as "extraordinary" by the historian Simon Sebag-Montefiore but also consistent with the standard Stalinist policy of deferring all decision-making (no matter how necessary or obvious) without official orders from higher authority.

Beria's decision to avoid immediately calling a doctor was tacitly supported (or at least not opposed) by the rest of the Politburo, which was rudderless without Stalin's micromanagement and paralysed by a legitimate fear he would suddenly recover and take reprisals on anyone who had dared to act without his orders. Stalin's suspicion of doctors in the wake of the Doctors' Plot was well known at the time of his sickness; his private physician was already being tortured in the basement of the Lubyanka for suggesting the leader required more bed rest.

After Stalin's death, Beria claimed to have killed him. This aborted a final purge of Old Bolsheviks Mikoyan and Molotov, for which Stalin had been laying the groundwork in the year prior to his death. Shortly after Stalin's death, Beria announced triumphantly to the Politburo that he had "done [Stalin] in" and "saved [us] all", according to Molotov's memoirs. The assertion that Stalin was poisoned by Beria's associates has been supported by Edvard Radzinsky and other authors.

After Stalin's death, Beria's ambitions sprang into full force. In the uneasy silence following the cessation of Stalin's last agonies, Beria was the first to dart forward to kiss his lifeless form (a move likened by Sebag-Montefiore to "wrenching a dead King's ring off his finger"). While the rest of Stalin's inner circle (even Molotov, saved from certain liquidation) stood sobbing unashamedly over the body, Beria reportedly appeared "radiant", "regenerated" and "glistening with ill-concealed relish". When Beria left the room, he broke the sombre atmosphere by shouting loudly for his driver, his voice echoing with what Stalin's daughter Svetlana Alliluyeva called "the ring of triumph unconcealed". Alliluyeva noticed how the Politburo seemed openly frightened of Beria and unnerved by his bold display of ambition. "He's off to take power," Mikoyan recalled muttering to Khrushchev. That prompted a "frantic" dash for their own limousines to intercept him at the Kremlin.

After Stalin's death, Beria was appointed First Deputy Premier and reappointed head of the MVD, which he merged with the MGB. His close ally Malenkov was the new Premier and initially the most powerful man in the post-Stalin leadership. Beria was second most powerful, and given Malenkov's personal weakness, was poised to become the power behind the throne and ultimately leader himself. Khrushchev became Party Secretary. Voroshilov became Chairman of the Presidium of the Supreme Soviet (i.e., the head of state).

Beria undertook some measures of liberalisation immediately after Stalin's death. He reorganised the MVD and drastically reduced its economic power and penal responsibilities. A number of costly construction projects such as the Salekhard–Igarka Railway were scrapped, and the remaining industrial enterprises became affiliated under corresponding economic ministries. The Gulag system was transferred to the Ministry of Justice, and a mass release of over a million prisoners was announced, although only prisoners convicted for "non-political" crimes have been released. The amnesty, therefore, led to a substantial increase in crime and would later be used against Beria by his rivals.

To consolidate power, Beria also took steps to recognise the rights of non-Russian nationalities. He questioned the traditional policy of Russification and encouraged local officials to assert their own identities. He first turned to Georgia, where Stalin's fabricated Mingrelian affair was called off and the republic's key posts were replaced by pro-Beria Georgians. Beria's policies in Ukraine alarmed Khrushchev, for whom Ukraine was a power base. Khrushchev then tried to draw Malenkov to his side, warning that "Beria is sharpening his knives".

Khrushchev opposed the alliance between Beria and Malenkov but he was initially unable to challenge them. Khrushchev's opportunity came in June 1953 when a spontaneous uprising against the East German Communist regime broke out in East Berlin. Based on Beria's statements, other leaders suspected that in the wake of the uprising, he might be willing to trade the reunification of Germany and the end of the Cold War for massive aid from the United States, as had been received in World War II. The cost of the war still weighed heavily on the Soviet economy. Beria craved the vast financial resources that another (more sustained) relationship with the United States could provide. Beria gave Estonia, Latvia and Lithuania serious prospects of national autonomy, possibly similarly to other Soviet satellite states in Europe. Beria said of East Germany "It's not even a real state but one kept in being only by Soviet troops."

The East German uprising convinced Molotov, Malenkov, and Nikolai Bulganin that Beria's policies were dangerous and destabilising to Soviet power. Within days of the events in Germany, Khrushchev persuaded the other leaders to support a Party "coup" against Beria; Beria's principal ally Malenkov abandoned him.

Beria was chairman of the Council of Ministers and an influential Politburo member and saw himself as Stalin's successor, while wider Politburo members had contrasting thoughts on future leadership. On 26 June 1953, Beria was arrested and held in an undisclosed location near Moscow. Accounts of Beria's fall vary considerably. By the most likely account, Khrushchev prepared an elaborate ambush, convening a meeting of the Presidium on 26 June, where he suddenly launched a scathing attack on Beria, accusing him of being a traitor and spy in the pay of British intelligence. Beria was taken completely by surprise. He asked, "What's going on, Nikita Sergeyevich? Why are you picking fleas in my trousers?" Molotov and others quickly spoke against Beria one after the other, followed by a motion by Khrushchev for his instant dismissal.

When Beria finally realised what was happening and plaintively appealed to Malenkov (an old friend and crony) to speak for him, Malenkov silently hung his head and pressed a button on his desk. This was the pre-arranged signal to Marshal of the Soviet Union Georgy Zhukov and a group of armed officers in a nearby room, who burst in and arrested Beria.

Beria was taken first to the Moscow guardhouse and then to the bunker of the headquarters of Moscow Military District. Defence Minister Nikolai Bulganin ordered the Kantemirovskaya Tank Division and Tamanskaya Motor Rifle Division to move into Moscow to prevent security forces loyal to Beria from rescuing him. Many of Beria's subordinates, proteges and associates were also arrested, among them Vsevolod Merkulov, Bogdan Kobulov, Sergey Goglidze, Vladimir Dekanozov, Pavel Meshik, and Lev Vlodzimirskiy. "Pravda" did not announce Beria's arrest until 10 July, crediting it to Malenkov and referring to Beria's "criminal activities against the Party and the State".

Beria and the others were tried by a "special session" () of the Supreme Court of the Soviet Union on 23 December 1953 with no defence counsel and no right of appeal. Marshal of the Soviet Union Ivan Konev was the chairman of the court.

Beria was found guilty of:

Beria and all the other defendants were sentenced to death on 23 December 1953, the day of the trial. The other six defendants were shot immediately after the trial ended. Beria was executed separately; he allegedly pleaded on his knees before collapsing to the floor wailing. He was shot through the forehead by General Pavel Batitsky. His final moments bore great similarity to those of his own predecessor, NKVD Chief Nikolai Yezhov, who begged for his life before his execution in 1940. Beria's body was cremated and the remains buried in Communal Grave No. 3 at Donskoi Monastery Cemetery, Moscow, Moscow Federal City, Russia.

Beria was deprived of all titles and awards on December 23, 1953.




Beria is the central character in "Good Night, Uncle Joe" by Canadian playwright David Elendune. The play is a fictionalised account of the events leading up to Stalin's death.

Georgian film director Tengiz Abuladze based the character of dictator Varlam Aravidze on Beria in his 1984 film "Repentance". Although banned in the Soviet Union for its semi-allegorical critique of Stalinism, it premiered at the 1987 Cannes Film Festival, winning the FIPRESCI Prize, Grand Prize of the Jury, and the Prize of the Ecumenical Jury.

Beria was played by British actor Bob Hoskins in the 1991 film "Inner Circle", by Roshan Seth in the 1992 HBO television film "Stalin" and by David Suchet in "Red Monarch". Simon Russell Beale played Beria in the 2017 satirical film "The Death of Stalin".

In the 1958 CBS production of "The Plot to Kill Stalin" for "Playhouse 90", Beria was portrayed by E. G. Marshall. In the 1992 HBO movie "Stalin", Roshan Seth was cast as Beria.

Beria appears in the third episode ("Superbomb") of the four-part 2007 BBC docudrama series "Nuclear Secrets", played by Boris Isarov. In the 2008 BBC documentary series "", Beria was portrayed by Polish actor .

He was also an important character in the 2013 Russian mini-series "Kill Stalin", produced by Star Media.

Richard Condon's 1959 novel "The Manchurian Candidate" describes brainwashed Raymond Shaw, the "perfectly prefabricated assassin", as "this dream by Lavrenti Beria".

In the 1964 science fiction novel by Arkady and Boris Strugatsky, "Hard to Be a God", Beria is personified in the character Don Reba who serves as the king's minister of defence.

At the opening of Kingsley Amis' "The Alteration", Lavrentiy Beria figures as "Monsignor Laurentius", paired with the similarly black-clad cleric "Monsignor Henricus" of the Holy Office (i.e., the Inquisition); the one to whom Beria was compared by Stalin in our own timeline: Heinrich Himmler. In the novel, both men are on the same side, serving an alternate-world Catholic Empire.

Beria is a significant character in the alternate history/alien invasion novel series "Worldwar" by Harry Turtledove as well as the "Axis of Time" series by John Birmingham

In the 1981 novel "Noble House" by James Clavell set in 1963 Hong Kong, the main character Ian Dunross received from Alan Medford Grant a set of secret documents regarding a Soviet spy-ring in Hong Kong code-named "Sevrin". The document was signed by an LB, believed by Grant (and the mysterious Tip Tok-Toh) to be Lavrentiy Beria (written as Lavrenti Beria in the novel).

Beria is a significant character in the opening chapters of the 1998 novel "Archangel" by British novelist Robert Harris.

Beria is a minor character in the 2009 novel "The Hundred-Year-Old Man Who Climbed Out the Window and Disappeared" by Jonas Jonasson. Beria is described as the boss of the Soviet state's security and is in attendance at a meal with the main character and Stalin.

In 2012, his alleged personal diary from 1938 to 1953 was published in Russia.

As "der Kleine Groβe Mann" Beria appears as the lover of one of the leading characters, Christine, in the 2014 novel "Das achte Leben (Für Brilka)" (translated as "The Eighth Life (For Brilka)") by Nino Haratischwili.






</doc>
<doc id="18391" url="https://en.wikipedia.org/wiki?curid=18391" title="Lyonel Feininger">
Lyonel Feininger

Lyonel Charles Feininger (July 17, 1871January 13, 1956) was a German-American painter, and a leading exponent of Expressionism. He also worked as a caricaturist and comic strip artist. He was born and grew up in New York City, traveling to Germany at 16 to study and perfect his art. He started his career as a cartoonist in 1894 and met with much success in this area. He was also a commercial caricaturist for 20 years for magazines and newspapers in the USA and Germany. At the age of 36, he started to work as a fine artist. He also produced a large body of photographic works between 1928 and the mid 1950s, but he kept these primarily within his circle of friends. He was also a pianist and composer, with several piano compositions and fugues for organ extant.

Lyonel Feininger was born to German-American violinist and composer Karl Feininger and American singer Elizabeth Feininger. He was born and grew up in New York City, but traveled to Germany at the age of 16 in 1887 to study. In 1888, he moved to Berlin and studied at the Königliche Akademie Berlin under Ernst Hancke. He continued his studies at art schools in Berlin with Karl Schlabitz, and in Paris with sculptor Filippo Colarossi. He started as a caricaturist for several magazines including "Harper's Round Table", "Harper's Young People", "Humoristische Blätter", "Lustige Blätter", "Das Narrenschiff", "Berliner Tageblatt" and "Ulk".

In 1900, he met Clara Fürst, daughter of the painter . He married her in 1901, and they had two daughters. In 1905, he separated from his wife after meeting Julia Berg. He married Berg in 1908 and the couple had three boys.

The artist was represented with drawings at the exhibitions of the annual Berlin Secession in the years 1901 through 1903.

Feininger's career as cartoonist started in 1894. He was working for several German, French and American magazines. In February 1906, when a quarter of Chicago's population was of German descent, James Keeley, editor of The "Chicago Tribune" traveled to Germany to procure the services of the most popular humor artists. He recruited Feininger to illustrate two comic strips "The Kin-der-Kids" and "Wee Willie Winkie's World" for the "Chicago Tribune". The strips were noted for their fey humor and graphic experimentation. He also worked as a commercial caricaturist for 20 years for various newspapers and magazines in both the United States and Germany. Later, Art Spiegelman wrote in "The New York Times Book Review," that Feininger's comics have "achieved a breathtaking formal grace unsurpassed in the history of the medium."

Feininger started working as a fine artist at the age of 36. He was a member of the "Berliner Sezession" in 1909, and he was associated with German expressionist groups: Die Brücke, the Novembergruppe, Gruppe 1919, the Blaue Reiter circle and Die Blaue Vier (The Blue Four). His first solo exhibit was at Sturm Gallery in Berlin, 1917. When Walter Gropius founded the Bauhaus in Germany in 1919, Feininger was his first faculty appointment, and became the master artist in charge of the printmaking workshop.
From 1909 until 1921, Feininger spent summer vacations on the island of Usedom to recover and to get new inspiration. Typical of works from this period were marine settings from the shores of the Baltic See (Ostsee). He continued to create paintings and drawings of Benz for the rest of his life, even after returning to live in the United States. A tour of the sites appearing in the works of Feininger follows a path with markers in the ground to guide visitors.

He designed the cover for the Bauhaus 1919 manifesto: an expressionist woodcut 'cathedral'. He taught at the Bauhaus for several years. Among the students who attended his workshops were Ludwig Hirschfeld Mack (German/Australian (1893–1965), Hans Friedrich Grohs (German 1892 - 1981), and Margarete Koehler-Bittkow (German/American, 1898–1964).

When the Nazi Party came to power in 1933, the situation became unbearable for Feininger and his wife. The Nazi Party declared his work to be "degenerate". They moved to America after his work was exhibited in the 'degenerate art' ("Entartete Kunst") in 1936, but before the 1937 exhibition in Munich. He taught at Mills College before returning to New York. He was elected to the American Academy of Arts and Letters in 1955.

In addition to drawing, Feininger created art with painted toy figures being photographed in front of drawn backgrounds.

Feininger produced a large body of photographic works between 1928 and the mid-1950s. He kept his photographic work within his circle of friends, and it was not shared with the public in his lifetime. He gave some prints away to his colleagues Walter Gropius and Alfred H. Barr Jr..

Feininger also had intermittent activity as a pianist and composer, with several piano compositions and fugues for organ extant.

His sons, Andreas Feininger and T. Lux Feininger, both became noted artists, the former as a photographer and the latter as a photographer and painter. T. Lux Feininger died July 7, 2011 at the age of 101.

A major retrospective exhibition of Lyonel Feininger's work was put on in 2011-2012: it opened initially at the Whitney Museum of American Art, June 30 through October 16, 2011, subsequently at the Montreal Museum of Fine Arts, January 1 through May 13, 2012. The exhibition is described as "the first in Feininger's native country in more than forty-five years, and the first ever to include the full breadth of his art" and as "accompanied by a richly illustrated monograph with a feature essay that provides a broad overview of Feininger's career..." Many critics have argued that the artist's work was at its most mature around 1910 in works in which the power of Feininger as illustrator balance his abstract side; however, we have to consider the possibility that Feininger used cubism as a more artistically succinct tool to establish his version of the concept known as the objective correlative.

Feininger was also a composer: in tandem with the Whitney retrospective, the American Symphony Orchestra under Leon Botstein, at Carnegie Hall on 21 October 2011, performed three orchestral fugues written by Feininger. Barbara Haskell, curator of the Whitney exhibit, wrote that for his entire life, Feininger credited Bach with having been his "master in painting." 

At a 2001 Christie's auction in London, Feininger's painting "The Green Bridge" (1909) was sold for £2.42 million.

At a 2007 Sotheby’s auction in New York, Feininger’s oil painting "Jesuits III" (1915) sold for $23,280,000.

At a 2017 Sotheby's auction in New York, Feininger's oil painting "Fin de séance" (1910) sold for $5,637,500.






</doc>
<doc id="18393" url="https://en.wikipedia.org/wiki?curid=18393" title="Life">
Life

Life is a characteristic that distinguishes physical entities that have biological processes, such as signaling and self-sustaining processes, from those that do not, either because such functions have ceased (they have died), or because they never had such functions and are classified as inanimate. Various forms of life exist, such as plants, animals, fungi, protists, archaea, and bacteria. Biology is the science concerned with the study of life.
There is currently no consensus regarding the definition of life. One popular definition is that organisms are open systems that maintain homeostasis, are composed of cells, have a life cycle, undergo metabolism, can grow, adapt to their environment, respond to stimuli, reproduce and evolve. Other definitions sometimes include non-cellular life forms such as viruses and viroids.

Abiogenesis is the natural process of life arising from non-living matter, such as simple organic compounds. The prevailing scientific hypothesis is that the transition from non-living to living entities was not a single event, but a gradual process of increasing complexity. Life on Earth first appeared as early as 4.28 billion years ago, soon after ocean formation 4.41 billion years ago, and not long after the formation of the Earth 4.54 billion years ago. The earliest known life forms are microfossils of bacteria. Researchers generally think that current life on Earth descends from an RNA world, although RNA-based life may not have been the first life to have existed. The classic 1952 Miller–Urey experiment and similar research demonstrated that most amino acids, the chemical constituents of the proteins used in all living organisms, can be synthesized from inorganic compounds under conditions intended to replicate those of the early Earth. Complex organic molecules occur in the Solar System and in interstellar space, and these molecules may have provided starting material for the development of life on Earth.

Since its primordial beginnings, life on Earth has changed its environment on a geologic time scale, but it has also adapted to survive in most ecosystems and conditions. Some microorganisms, called extremophiles, thrive in physically or geochemically extreme environments that are detrimental to most other life on Earth. The cell is considered the structural and functional unit of life. There are two kinds of cells, prokaryotic and eukaryotic, both of which consist of cytoplasm enclosed within a membrane and contain many biomolecules such as proteins and nucleic acids. Cells reproduce through a process of cell division, in which the parent cell divides into two or more daughter cells.

In the past, there have been many attempts to define what is meant by "life" through obsolete concepts such as odic force, hylomorphism, spontaneous generation and vitalism, that have now been disproved by biological discoveries. Aristotle is considered to be the first person to classify organisms. Later, Carl Linnaeus introduced his system of binomial nomenclature for the classification of species. Eventually new groups and categories of life were discovered, such as cells and microorganisms, forcing dramatic revisions of the structure of relationships between living organisms. Though currently only known on Earth, life need not be restricted to it, and many scientists speculate in the existence of extraterrestrial life. Artificial life is a computer simulation or human-made reconstruction of any aspect of life, which is often used to examine systems related to natural life.

Death is the permanent termination of all biological functions which sustain an organism, and as such, is the end of its life. Extinction is the term describing the dying out of a group or taxon, usually a species. Fossils are the preserved remains or traces of organisms.

The definition of life has long been a challenge for scientists and philosophers, with many varied definitions put forward. This is partially because life is a process, not a substance. This is complicated by a lack of knowledge of the characteristics of living entities, if any, that may have developed outside of Earth. Philosophical definitions of life have also been put forward, with similar difficulties on how to distinguish living things from the non-living. Legal definitions of life have also been described and debated, though these generally focus on the decision to declare a human dead, and the legal ramifications of this decision.

Since there is no unequivocal definition of life, most current definitions in biology are descriptive. Life is considered a characteristic of something that preserves, furthers or reinforces its existence in the given environment. This characteristic exhibits all or most of the following traits:

These complex processes, called physiological functions, have underlying physical and chemical bases, as well as signaling and control mechanisms that are essential to maintaining life.

From a physics perspective, living beings are thermodynamic systems with an organized molecular structure that can reproduce itself and evolve as survival dictates. Thermodynamically, life has been described as an open system which makes use of gradients in its surroundings to create imperfect copies of itself. Hence, life is a self-sustained chemical system capable of undergoing Darwinian evolution. A major strength of this definition is that it distinguishes life by the evolutionary process rather than its chemical composition.

Others take a systemic viewpoint that does not necessarily depend on molecular chemistry. One systemic definition of life is that living things are self-organizing and autopoietic (self-producing). Variations of this definition include Stuart Kauffman's definition as an autonomous agent or a multi-agent system capable of reproducing itself or themselves, and of completing at least one thermodynamic work cycle. This definition is extended by the apparition of novel functions over time.

Whether or not viruses should be considered as alive is controversial. They are most often considered as just replicators rather than forms of life. They have been described as "organisms at the edge of life" because they possess genes, evolve by natural selection, and replicate by creating multiple copies of themselves through self-assembly. However, viruses do not metabolize and they require a host cell to make new products. Virus self-assembly within host cells has implications for the study of the origin of life, as it may support the hypothesis that life could have started as self-assembling organic molecules.

To reflect the minimum phenomena required, other biological definitions of life have been proposed, with many of these being based upon chemical systems. Biophysicists have commented that living things function on negative entropy. In other words, living processes can be viewed as a delay of the spontaneous diffusion or dispersion of the internal energy of biological molecules towards more potential microstates. In more detail, according to physicists such as John Bernal, Erwin Schrödinger, Eugene Wigner, and John Avery, life is a member of the class of phenomena that are open or continuous systems able to decrease their internal entropy at the expense of substances or free energy taken in from the environment and subsequently rejected in a degraded form.

Living systems are open self-organizing living things that interact with their environment. These systems are maintained by flows of information, energy, and matter.
Budisa, Kubyshkin and Schmidt defined cellular life as an organizational unit resting on four pillars/cornerstones: (i) energy, (ii) metabolism, (iii) information and (iv) form. This system is able to regulate and control metabolism and energy supply and contains at least one subsystem that functions as an information carrier (genetic information). Cells as self-sustaining units are parts of different populations that are involved in the unidirectional and irreversible open-ended process known as evolution.

Some scientists have proposed in the last few decades that a general living systems theory is required to explain the nature of life. Such a general theory would arise out of the ecological and biological sciences and attempt to map general principles for how all living systems work. Instead of examining phenomena by attempting to break things down into components, a general living systems theory explores phenomena in terms of dynamic patterns of the relationships of organisms with their environment.

The idea that the Earth is alive is found in philosophy and religion, but the first scientific discussion of it was by the Scottish scientist James Hutton. In 1785, he stated that the Earth was a superorganism and that its proper study should be physiology. Hutton is considered the father of geology, but his idea of a living Earth was forgotten in the intense reductionism of the 19th century. The Gaia hypothesis, proposed in the 1960s by scientist James Lovelock, suggests that life on Earth functions as a single organism that defines and maintains environmental conditions necessary for its survival. This hypothesis served as one of the foundations of the modern Earth system science.

The first attempt at a general living systems theory for explaining the nature of life was in 1978, by American biologist James Grier Miller. Robert Rosen (1991) built on this by defining a system component as "a unit of organization; a part with a function, i.e., a definite relation between part and whole." From this and other starting concepts, he developed a "relational theory of systems" that attempts to explain the special properties of life. Specifically, he identified the "nonfractionability of components in an organism" as the fundamental difference between living systems and "biological machines."

A systems view of life treats environmental fluxes and biological fluxes together as a "reciprocity of influence," and a reciprocal relation with environment is arguably as important for understanding life as it is for understanding ecosystems. As Harold J. Morowitz (1992) explains it, life is a property of an ecological system rather than a single organism or species. He argues that an ecosystemic definition of life is preferable to a strictly biochemical or physical one. Robert Ulanowicz (2009) highlights mutualism as the key to understand the systemic, order-generating behavior of life and ecosystems.

Complex systems biology (CSB) is a field of science that studies the emergence of complexity in functional organisms from the viewpoint of dynamic systems theory. The latter is also often called systems biology and aims to understand the most fundamental aspects of life. A closely related approach to CSB and systems biology called relational biology is concerned mainly with understanding life processes in terms of the most important relations, and categories of such relations among the essential functional components of organisms; for multicellular organisms, this has been defined as "categorical biology", or a model representation of organisms as a category theory of biological relations, as well as an algebraic topology of the functional organization of living organisms in terms of their dynamic, complex networks of metabolic, genetic, and epigenetic processes and signaling pathways. Alternative but closely related approaches focus on the interdependance of constraints, where constraints can be either molecular, such as enzymes, or macroscopic, such as the geometry of a bone or of the vascular system.

It has also been argued that the evolution of order in living systems and certain physical systems obeys a common fundamental principle termed the Darwinian dynamic. The Darwinian dynamic was formulated by first considering how macroscopic order is generated in a simple non-biological system far from thermodynamic equilibrium, and then extending consideration to short, replicating RNA molecules. The underlying order-generating process was concluded to be basically similar for both types of systems.

Another systemic definition called the operator theory proposes that "life is a general term for the presence of the typical closures found in organisms; the typical closures are a membrane and an autocatalytic set in the cell" and that an organism is any system with an organisation that complies with an operator type that is at least as complex as the cell. Life can also be modeled as a network of inferior negative feedbacks of regulatory mechanisms subordinated to a superior positive feedback formed by the potential of expansion and reproduction.

Some of the earliest theories of life were materialist, holding that all that exists is matter, and that life is merely a complex form or arrangement of matter. Empedocles (430 BC) argued that everything in the universe is made up of a combination of four eternal "elements" or "roots of all": earth, water, air, and fire. All change is explained by the arrangement and rearrangement of these four elements. The various forms of life are caused by an appropriate mixture of elements.

Democritus (460 BC) thought that the essential characteristic of life is having a soul ("psyche"). Like other ancient writers, he was attempting to explain what makes something a "living" thing. His explanation was that fiery atoms make a soul in exactly the same way atoms and void account for any other thing. He elaborates on fire because of the apparent connection between life and heat, and because fire moves.

The mechanistic materialism that originated in ancient Greece was revived and revised by the French philosopher René Descartes, who held that animals and humans were assemblages of parts that together functioned as a machine. In the 19th century, the advances in cell theory in biological science encouraged this view. The evolutionary theory of Charles Darwin (1859) is a mechanistic explanation for the origin of species by means of natural selection.

Hylomorphism is a theory first expressed by the Greek philosopher Aristotle (322 BC). The application of hylomorphism to biology was important to Aristotle, and biology is extensively covered in his extant writings. In this view, everything in the material universe has both matter and form, and the form of a living thing is its soul (Greek "psyche", Latin "anima"). There are three kinds of souls: the "vegetative soul" of plants, which causes them to grow and decay and nourish themselves, but does not cause motion and sensation; the "animal soul", which causes animals to move and feel; and the "rational soul", which is the source of consciousness and reasoning, which (Aristotle believed) is found only in man. Each higher soul has all of the attributes of the lower ones. Aristotle believed that while matter can exist without form, form cannot exist without matter, and that therefore the soul cannot exist without the body.

This account is consistent with teleological explanations of life, which account for phenomena in terms of purpose or goal-directedness. Thus, the whiteness of the polar bear's coat is explained by its purpose of camouflage. The direction of causality (from the future to the past) is in contradiction with the scientific evidence for natural selection, which explains the consequence in terms of a prior cause. Biological features are explained not by looking at future optimal results, but by looking at the past evolutionary history of a species, which led to the natural selection of the features in question.

Spontaneous generation was the belief that living organisms can form without descent from similar organisms. Typically, the idea was that certain forms such as fleas could arise from inanimate matter such as dust or the supposed seasonal generation of mice and insects from mud or garbage.

The theory of spontaneous generation was proposed by Aristotle, who compiled and expanded the work of prior natural philosophers and the various ancient explanations of the appearance of organisms; it held sway for two millennia. It was decisively dispelled by the experiments of Louis Pasteur in 1859, who expanded upon the investigations of predecessors such as Francesco Redi. Disproof of the traditional ideas of spontaneous generation is no longer controversial among biologists.

Vitalism is the belief that the life-principle is non-material. This originated with Georg Ernst Stahl (17th century), and remained popular until the middle of the 19th century. It appealed to philosophers such as Henri Bergson, Friedrich Nietzsche, and Wilhelm Dilthey, anatomists like Xavier Bichat, and chemists like Justus von Liebig. Vitalism included the idea that there was a fundamental difference between organic and inorganic material, and the belief that organic material can only be derived from living things. This was disproved in 1828, when Friedrich Wöhler prepared urea from inorganic materials. This Wöhler synthesis is considered the starting point of modern organic chemistry. It is of historical significance because for the first time an organic compound was produced in inorganic reactions.

During the 1850s, Hermann von Helmholtz, anticipated by Julius Robert von Mayer, demonstrated that no energy is lost in muscle movement, suggesting that there were no "vital forces" necessary to move a muscle. These results led to the abandonment of scientific interest in vitalistic theories, although the belief lingered on in pseudoscientific theories such as homeopathy, which interprets diseases and sickness as caused by disturbances in a hypothetical vital force or life force.

The age of the Earth is about 4.54 billion years. Evidence suggests that life on Earth has existed for at least 3.5 billion years, with the oldest physical traces of life dating back 3.7 billion years; however, some theories, such as the Late Heavy Bombardment theory, suggest that life on Earth may have started even earlier, as early as 4.1–4.4 billion years ago, and the chemistry leading to life may have begun shortly after the Big Bang, 13.8 billion years ago, during an epoch when the universe was only 10–17 million years old.

More than 99% of all species of life forms, amounting to over five billion species, that ever lived on Earth are estimated to be extinct.

Although the number of Earth's catalogued species of lifeforms is between 1.2 million and 2 million, the total number of species in the planet is uncertain. Estimates range from 8 million to 100 million, with a more narrow range between 10 and 14 million, but it may be as high as 1 trillion (with only one-thousandth of one percent of the species described) according to studies realized in May 2016. The total number of related DNA base pairs on Earth is estimated at 5.0 x 10 and weighs 50 billion tonnes. In comparison, the total mass of the biosphere has been estimated to be as much as 4 TtC (trillion tons of carbon). In July 2016, scientists reported identifying a set of 355 genes from the Last Universal Common Ancestor (LUCA) of all organisms living on Earth.

All known life forms share fundamental molecular mechanisms, reflecting their common descent; based on these observations, hypotheses on the origin of life attempt to find a mechanism explaining the formation of a universal common ancestor, from simple organic molecules via pre-cellular life to protocells and metabolism. Models have been divided into "genes-first" and "metabolism-first" categories, but a recent trend is the emergence of hybrid models that combine both categories.

There is no current scientific consensus as to how life originated. However, most accepted scientific models build on the Miller–Urey experiment and the work of Sidney Fox, which show that conditions on the primitive Earth favored chemical reactions that synthesize amino acids and other organic compounds from inorganic precursors, and phospholipids spontaneously form lipid bilayers, the basic structure of a cell membrane.

Living organisms synthesize proteins, which are polymers of amino acids using instructions encoded by deoxyribonucleic acid (DNA). Protein synthesis entails intermediary ribonucleic acid (RNA) polymers. One possibility for how life began is that genes originated first, followed by proteins; the alternative being that proteins came first and then genes.

However, because genes and proteins are both required to produce the other, the problem of considering which came first is like that of the chicken or the egg. Most scientists have adopted the hypothesis that because of this, it is unlikely that genes and proteins arose independently.

Therefore, a possibility, first suggested by Francis Crick, is that the first life was based on RNA, which has the DNA-like properties of information storage and the catalytic properties of some proteins. This is called the RNA world hypothesis, and it is supported by the observation that many of the most critical components of cells (those that evolve the slowest) are composed mostly or entirely of RNA. Also, many critical cofactors (ATP, Acetyl-CoA, NADH, etc.) are either nucleotides or substances clearly related to them. The catalytic properties of RNA had not yet been demonstrated when the hypothesis was first proposed, but they were confirmed by Thomas Cech in 1986.

One issue with the RNA world hypothesis is that synthesis of RNA from simple inorganic precursors is more difficult than for other organic molecules. One reason for this is that RNA precursors are very stable and react with each other very slowly under ambient conditions, and it has also been proposed that living organisms consisted of other molecules before RNA. However, the successful synthesis of certain RNA molecules under the conditions that existed prior to life on Earth has been achieved by adding alternative precursors in a specified order with the precursor phosphate present throughout the reaction. This study makes the RNA world hypothesis more plausible.

Geological findings in 2013 showed that reactive phosphorus species (like phosphite) were in abundance in the ocean before 3.5 Ga, and that Schreibersite easily reacts with aqueous glycerol to generate phosphite and glycerol 3-phosphate. It is hypothesized that Schreibersite-containing meteorites from the Late Heavy Bombardment could have provided early reduced phosphorus, which could react with prebiotic organic molecules to form phosphorylated biomolecules, like RNA.

In 2009, experiments demonstrated Darwinian evolution of a two-component system of RNA enzymes (ribozymes) "in vitro". The work was performed in the laboratory of Gerald Joyce, who stated "This is the first example, outside of biology, of evolutionary adaptation in a molecular genetic system."

Prebiotic compounds may have originated extraterrestrially. NASA findings in 2011, based on studies with meteorites found on Earth, suggest DNA and RNA components (adenine, guanine and related organic molecules) may be formed in outer space.

In March 2015, NASA scientists reported that, for the first time, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, have been formed in the laboratory under outer space conditions, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the universe, may have been formed in red giants or in interstellar dust and gas clouds, according to the scientists.

According to the panspermia hypothesis, microscopic life—distributed by meteoroids, asteroids and other small Solar System bodies—may exist throughout the universe.

The diversity of life on Earth is a result of the dynamic interplay between genetic opportunity, metabolic capability, environmental challenges, and symbiosis. For most of its existence, Earth's habitable environment has been dominated by microorganisms and subjected to their metabolism and evolution. As a consequence of these microbial activities, the physical-chemical environment on Earth has been changing on a geologic time scale, thereby affecting the path of evolution of subsequent life. For example, the release of molecular oxygen by cyanobacteria as a by-product of photosynthesis induced global changes in the Earth's environment. Because oxygen was toxic to most life on Earth at the time, this posed novel evolutionary challenges, and ultimately resulted in the formation of Earth's major animal and plant species. This interplay between organisms and their environment is an inherent feature of living systems.

The biosphere is the global sum of all ecosystems. It can also be termed as the zone of life on Earth, a closed system (apart from solar and cosmic radiation and heat from the interior of the Earth), and largely self-regulating. By the most general biophysiological definition, the biosphere is the global ecological system integrating all living beings and their relationships, including their interaction with the elements of the lithosphere, geosphere, hydrosphere, and atmosphere.

Life forms live in every part of the Earth's biosphere, including soil, hot springs, inside rocks at least deep underground, the deepest parts of the ocean, and at least high in the atmosphere. Under certain test conditions, life forms have been observed to thrive in the near-weightlessness of space and to survive in the vacuum of outer space. Life forms appear to thrive in the Mariana Trench, the deepest spot in the Earth's oceans. Other researchers reported related studies that life forms thrive inside rocks up to below the sea floor under of ocean off the coast of the northwestern United States, as well as beneath the seabed off Japan. In August 2014, scientists confirmed the existence of life forms living below the ice of Antarctica. According to one researcher, "You can find microbes everywhere—they're extremely adaptable to conditions, and survive wherever they are."

The biosphere is postulated to have evolved, beginning with a process of biopoesis (life created naturally from non-living matter, such as simple organic compounds) or biogenesis (life created from living matter), at least some 3.5 billion years ago. The earliest evidence for life on Earth includes biogenic graphite found in 3.7 billion-year-old metasedimentary rocks from Western Greenland and microbial mat fossils found in 3.48 billion-year-old sandstone from Western Australia. More recently, in 2015, "remains of biotic life" were found in 4.1 billion-year-old rocks in Western Australia. In 2017, putative fossilized microorganisms (or microfossils) were announced to have been discovered in hydrothermal vent precipitates in the Nuvvuagittuq Belt of Quebec, Canada that were as old as 4.28 billion years, the oldest record of life on earth, suggesting "an almost instantaneous emergence of life" after ocean formation 4.4 billion years ago, and not long after the formation of the Earth 4.54 billion years ago. According to biologist Stephen Blair Hedges, "If life arose relatively quickly on Earth ... then it could be common in the universe."

In a general sense, biospheres are any closed, self-regulating systems containing ecosystems. This includes artificial biospheres such as Biosphere 2 and BIOS-3, and potentially ones on other planets or moons.

The inert components of an ecosystem are the physical and chemical factors necessary for life—energy (sunlight or chemical energy), water, heat, atmosphere, gravity, nutrients, and ultraviolet solar radiation protection. In most ecosystems, the conditions vary during the day and from one season to the next. To live in most ecosystems, then, organisms must be able to survive a range of conditions, called the "range of tolerance." Outside that are the "zones of physiological stress," where the survival and reproduction are possible but not optimal. Beyond these zones are the "zones of intolerance," where survival and reproduction of that organism is unlikely or impossible. Organisms that have a wide range of tolerance are more widely distributed than organisms with a narrow range of tolerance.

To survive, selected microorganisms can assume forms that enable them to withstand freezing, complete desiccation, starvation, high levels of radiation exposure, and other physical or chemical challenges. These microorganisms may survive exposure to such conditions for weeks, months, years, or even centuries. Extremophiles are microbial life forms that thrive outside the ranges where life is commonly found. They excel at exploiting uncommon sources of energy. While all organisms are composed of nearly identical molecules, evolution has enabled such microbes to cope with this wide range of physical and chemical conditions. Characterization of the structure and metabolic diversity of microbial communities in such extreme environments is ongoing.

Microbial life forms thrive even in the Mariana Trench, the deepest spot in the Earth's oceans. Microbes also thrive inside rocks up to below the sea floor under of ocean.

Investigation of the tenacity and versatility of life on Earth, as well as an understanding of the molecular systems that some organisms utilize to survive such extremes, is important for the search for life beyond Earth. For example, lichen could survive for a month in a simulated Martian environment.

All life forms require certain core chemical elements needed for biochemical functioning. These include carbon, hydrogen, nitrogen, oxygen, phosphorus, and sulfur—the elemental macronutrients for all organisms—often represented by the acronym CHNOPS. Together these make up nucleic acids, proteins and lipids, the bulk of living matter. Five of these six elements comprise the chemical components of DNA, the exception being sulfur. The latter is a component of the amino acids cysteine and methionine. The most biologically abundant of these elements is carbon, which has the desirable attribute of forming multiple, stable covalent bonds. This allows carbon-based (organic) molecules to form an immense variety of chemical arrangements. Alternative hypothetical types of biochemistry have been proposed that eliminate one or more of these elements, swap out an element for one not on the list, or change required chiralities or other chemical properties.

Deoxyribonucleic acid is a molecule that carries most of the genetic instructions used in the growth, development, functioning and reproduction of all known living organisms and many viruses. DNA and RNA are nucleic acids; alongside proteins and complex carbohydrates, they are one of the three major types of macromolecule that are essential for all known forms of life. Most DNA molecules consist of two biopolymer strands coiled around each other to form a double helix. The two DNA strands are known as polynucleotides since they are composed of simpler units called nucleotides. Each nucleotide is composed of a nitrogen-containing nucleobase—either cytosine (C), guanine (G), adenine (A), or thymine (T)—as well as a sugar called deoxyribose and a phosphate group. The nucleotides are joined to one another in a chain by covalent bonds between the sugar of one nucleotide and the phosphate of the next, resulting in an alternating sugar-phosphate backbone. According to base pairing rules (A with T, and C with G), hydrogen bonds bind the nitrogenous bases of the two separate polynucleotide strands to make double-stranded DNA. The total amount of related DNA base pairs on Earth is estimated at 5.0 x 10, and weighs 50 billion tonnes. In comparison, the total mass of the biosphere has been estimated to be as much as 4 TtC (trillion tons of carbon).

DNA stores biological information. The DNA backbone is resistant to cleavage, and both strands of the double-stranded structure store the same biological information. Biological information is replicated as the two strands are separated. A significant portion of DNA (more than 98% for humans) is non-coding, meaning that these sections do not serve as patterns for protein sequences.

The two strands of DNA run in opposite directions to each other and are therefore anti-parallel. Attached to each sugar is one of four types of nucleobases (informally, "bases"). It is the sequence of these four nucleobases along the backbone that encodes biological information. Under the genetic code, RNA strands are translated to specify the sequence of amino acids within proteins. These RNA strands are initially created using DNA strands as a template in a process called transcription.

Within cells, DNA is organized into long structures called chromosomes. During cell division these chromosomes are duplicated in the process of DNA replication, providing each cell its own complete set of chromosomes. Eukaryotic organisms (animals, plants, fungi, and protists) store most of their DNA inside the cell nucleus and some of their DNA in organelles, such as mitochondria or chloroplasts. In contrast, prokaryotes (bacteria and archaea) store their DNA only in the cytoplasm. Within the chromosomes, chromatin proteins such as histones compact and organize DNA. These compact structures guide the interactions between DNA and other proteins, helping control which parts of the DNA are transcribed.

DNA was first isolated by Friedrich Miescher in 1869. Its molecular structure was identified by James Watson and Francis Crick in 1953, whose model-building efforts were guided by X-ray diffraction data acquired by Rosalind Franklin.

The first known attempt to classify organisms was conducted by the Greek philosopher Aristotle (384–322 BC), who classified all living organisms known at that time as either a plant or an animal, based mainly on their ability to move. He also distinguished animals with blood from animals without blood (or at least without red blood), which can be compared with the concepts of vertebrates and invertebrates respectively, and divided the blooded animals into five groups: viviparous quadrupeds (mammals), oviparous quadrupeds (reptiles and amphibians), birds, fishes and whales. The bloodless animals were also divided into five groups: cephalopods, crustaceans, insects (which included the spiders, scorpions, and centipedes, in addition to what we define as insects today), shelled animals (such as most molluscs and echinoderms), and "zoophytes" (animals that resemble plants). Though Aristotle's work in zoology was not without errors, it was the grandest biological synthesis of the time and remained the ultimate authority for many centuries after his death.

The exploration of the Americas revealed large numbers of new plants and animals that needed descriptions and classification. In the latter part of the 16th century and the beginning of the 17th, careful study of animals commenced and was gradually extended until it formed a sufficient body of knowledge to serve as an anatomical basis for classification.

In the late 1740s, Carl Linnaeus introduced his system of binomial nomenclature for the classification of species. Linnaeus attempted to improve the composition and reduce the length of the previously used many-worded names by abolishing unnecessary rhetoric, introducing new descriptive terms and precisely defining their meaning. The Linnaean classification has eight levels: domains, kingdoms, phyla, class, order, family, genus, and species.

The fungi were originally treated as plants. For a short period Linnaeus had classified them in the taxon Vermes in Animalia, but later placed them back in Plantae. Copeland classified the Fungi in his Protoctista, thus partially avoiding the problem but acknowledging their special status. The problem was eventually solved by Whittaker, when he gave them their own kingdom in his five-kingdom system. Evolutionary history shows that the fungi are more closely related to animals than to plants.

As new discoveries enabled detailed study of cells and microorganisms, new groups of life were revealed, and the fields of cell biology and microbiology were created. These new organisms were originally described separately in protozoa as animals and protophyta/thallophyta as plants, but were united by Haeckel in the kingdom Protista; later, the prokaryotes were split off in the kingdom Monera, which would eventually be divided into two separate groups, the Bacteria and the Archaea. This led to the six-kingdom system and eventually to the current three-domain system, which is based on evolutionary relationships. However, the classification of eukaryotes, especially of protists, is still controversial.

As microbiology, molecular biology and virology developed, non-cellular reproducing agents were discovered, such as viruses and viroids. Whether these are considered alive has been a matter of debate; viruses lack characteristics of life such as cell membranes, metabolism and the ability to grow or respond to their environments. Viruses can still be classed into "species" based on their biology and genetics, but many aspects of such a classification remain controversial.

In May 2016, scientists reported that 1 trillion species are estimated to be on Earth currently with only one-thousandth of one percent described.

The original Linnaean system has been modified over time as follows:
In the 1960s cladistics emerged: a system arranging taxa based on clades in an evolutionary or phylogenetic tree.

Cells are the basic unit of structure in every living thing, and all cells arise from pre-existing cells by division. Cell theory was formulated by Henri Dutrochet, Theodor Schwann, Rudolf Virchow and others during the early nineteenth century, and subsequently became widely accepted. The activity of an organism depends on the total activity of its cells, with energy flow occurring within and between them. Cells contain hereditary information that is carried forward as a genetic code during cell division.

There are two primary types of cells. Prokaryotes lack a nucleus and other membrane-bound organelles, although they have circular DNA and ribosomes. Bacteria and Archaea are two domains of prokaryotes. The other primary type of cells are the eukaryotes, which have distinct nuclei bound by a nuclear membrane and membrane-bound organelles, including mitochondria, chloroplasts, lysosomes, rough and smooth endoplasmic reticulum, and vacuoles. In addition, they possess organized chromosomes that store genetic material. All species of large complex organisms are eukaryotes, including animals, plants and fungi, though most species of eukaryote are protist microorganisms. The conventional model is that eukaryotes evolved from prokaryotes, with the main organelles of the eukaryotes forming through endosymbiosis between bacteria and the progenitor eukaryotic cell.

The molecular mechanisms of cell biology are based on proteins. Most of these are synthesized by the ribosomes through an enzyme-catalyzed process called protein biosynthesis. A sequence of amino acids is assembled and joined together based upon gene expression of the cell's nucleic acid. In eukaryotic cells, these proteins may then be transported and processed through the Golgi apparatus in preparation for dispatch to their destination.

Cells reproduce through a process of cell division in which the parent cell divides into two or more daughter cells. For prokaryotes, cell division occurs through a process of fission in which the DNA is replicated, then the two copies are attached to parts of the cell membrane. In eukaryotes, a more complex process of mitosis is followed. However, the end result is the same; the resulting cell copies are identical to each other and to the original cell (except for mutations), and both are capable of further division following an interphase period.

Multicellular organisms may have first evolved through the formation of colonies of identical cells. These cells can form group organisms through cell adhesion. The individual members of a colony are capable of surviving on their own, whereas the members of a true multi-cellular organism have developed specializations, making them dependent on the remainder of the organism for survival. Such organisms are formed clonally or from a single germ cell that is capable of forming the various specialized cells that form the adult organism. This specialization allows multicellular organisms to exploit resources more efficiently than single cells. In January 2016, scientists reported that, about 800 million years ago, a minor genetic change in a single molecule, called GK-PID, may have allowed organisms to go from a single cell organism to one of many cells.

Cells have evolved methods to perceive and respond to their microenvironment, thereby enhancing their adaptability. Cell signaling coordinates cellular activities, and hence governs the basic functions of multicellular organisms. Signaling between cells can occur through direct cell contact using juxtacrine signalling, or indirectly through the exchange of agents as in the endocrine system. In more complex organisms, coordination of activities can occur through a dedicated nervous system.

Though life is confirmed only on Earth, many think that extraterrestrial life is not only plausible, but probable or inevitable. Other planets and moons in the Solar System and other planetary systems are being examined for evidence of having once supported simple life, and projects such as SETI are trying to detect radio transmissions from possible alien civilizations. Other locations within the Solar System that may host microbial life include the subsurface of Mars, the upper atmosphere of Venus, and subsurface oceans on some of the moons of the giant planets.
Beyond the Solar System, the region around another main-sequence star that could support Earth-like life on an Earth-like planet is known as the habitable zone. The inner and outer radii of this zone vary with the luminosity of the star, as does the time interval during which the zone survives. Stars more massive than the Sun have a larger habitable zone, but remain on the Sun-like "main sequence" of stellar evolution for a shorter time interval. Small red dwarfs have the opposite problem, with a smaller habitable zone that is subject to higher levels of magnetic activity and the effects of tidal locking from close orbits. Hence, stars in the intermediate mass range such as the Sun may have a greater likelihood for Earth-like life to develop. The location of the star within a galaxy may also affect the likelihood of life forming. Stars in regions with a greater abundance of heavier elements that can form planets, in combination with a low rate of potentially habitat-damaging supernova events, are predicted to have a higher probability of hosting planets with complex life. The variables of the Drake equation are used to discuss the conditions in planetary systems where civilization is most likely to exist. Use of the equation to predict the amount of extraterrestrial life, however, is difficult; because many of the variables are unknown, the equation functions as more of a mirror to what its user already thinks. As a result, the number of civilizations in the galaxy can be estimated as low as 9.1 x 10, suggesting a minimum value of 1, or as high as 15.6 million (0.156 x 10); for the calculations, see Drake equation.

Artificial life is the simulation of any aspect of life, as through computers, robotics, or biochemistry. The study of artificial life imitates traditional biology by recreating some aspects of biological phenomena. Scientists study the logic of living systems by creating artificial environments—seeking to understand the complex information processing that defines such systems. While life is, by definition, alive, artificial life is generally referred to as data confined to a digital environment and existence.

Synthetic biology is a new area of biotechnology that combines science and biological engineering. The common goal is the design and construction of new biological functions and systems not found in nature. Synthetic biology includes the broad redefinition and expansion of biotechnology, with the ultimate goals of being able to design and build engineered biological systems that process information, manipulate chemicals, fabricate materials and structures, produce energy, provide food, and maintain and enhance human health and the environment.

Death is the permanent termination of all vital functions or life processes in an organism or cell. It can occur as a result of an accident, medical conditions, biological interaction, malnutrition, poisoning, senescence, or suicide. After death, the remains of an organism re-enter the biogeochemical cycle. Organisms may be consumed by a predator or a scavenger and leftover organic material may then be further decomposed by detritivores, organisms that recycle detritus, returning it to the environment for reuse in the food chain.

One of the challenges in defining death is in distinguishing it from life. Death would seem to refer to either the moment life ends, or when the state that follows life begins. However, determining when death has occurred is difficult, as cessation of life functions is often not simultaneous across organ systems. Such determination therefore requires drawing conceptual lines between life and death. This is problematic, however, because there is little consensus over how to define life. The nature of death has for millennia been a central concern of the world's religious traditions and of philosophical inquiry. Many religions maintain faith in either a kind of afterlife or reincarnation for the soul, or resurrection of the body at a later date.

Extinction is the process by which a group of taxa or species dies out, reducing biodiversity. The moment of extinction is generally considered the death of the last individual of that species. Because a species' potential range may be very large, determining this moment is difficult, and is usually done retrospectively after a period of apparent absence. Species become extinct when they are no longer able to survive in changing habitat or against superior competition. In Earth's history, over 99% of all the species that have ever lived are extinct; however, mass extinctions may have accelerated evolution by providing opportunities for new groups of organisms to diversify.

Fossils are the preserved remains or traces of animals, plants, and other organisms from the remote past. The totality of fossils, both discovered and undiscovered, and their placement in fossil-containing rock formations and sedimentary layers (strata) is known as the "fossil record". A preserved specimen is called a fossil if it is older than the arbitrary date of 10,000 years ago. Hence, fossils range in age from the youngest at the start of the Holocene Epoch to the oldest from the Archaean Eon, up to 3.4 billion years old.





</doc>
<doc id="18398" url="https://en.wikipedia.org/wiki?curid=18398" title="La Espero">
La Espero

"La Espero" () is a poem written by Polish-Jewish doctor L. L. Zamenhof (1859–1917), the initiator of the Esperanto language. The song is often used as the anthem of Esperanto, and is now usually sung to a triumphal march composed by Félicien Menu de Ménil in 1909 (although there is an earlier, less martial tune created in 1891 by Claes Adelsköld, as well as a number of others less well-known). It is sometimes referred to as the hymn of the Esperanto movement.

Some Esperantists object to the use of terms like "hymn" or "anthem" for "La Espero", arguing that these terms have religious and nationalist overtones respectively.



</doc>
<doc id="18400" url="https://en.wikipedia.org/wiki?curid=18400" title="Loonie">
Loonie

The loonie (), formally the Canadian one-dollar coin, is a gold-coloured coin that was introduced in 1987 and is produced by the Royal Canadian Mint at its facility in Winnipeg. The most prevalent versions of the coin show a common loon, a bird found throughout Canada, on the reverse and Queen Elizabeth II, the nation's head of state, on the obverse. Various commemorative and specimen-set editions of the coin with special designs replacing the loon on the reverse have been minted over the years.

The coin's outline is an 11-sided curve of constant width. Its diameter of 26.5 mm and its 11-sidedness matched that of the already-circulating Susan B. Anthony dollar in the United States, and its thickness of 1.95 mm was a close match to the latter's 2.0 mm. Its gold colour differed from the silver-coloured Anthony dollar; however, the succeeding Sacagawea and Presidential dollars matched the loonie's overall hue. Other coins using a curve of constant width include the 7-sided British twenty pence and fifty pence coins (the latter of which has similar size and value to the loonie, but is silver in colour).

After its introduction, the coin became a metonym for the Canadian dollar: media often discuss the rate at which the "loonie" is trading against other currencies. The nickname "loonie" became so widely recognized that in 2006, the Royal Canadian Mint secured the rights to it. When the Canadian two-dollar coin was introduced in 1996, it was in turn nicknamed the "toonie" (a portmanteau of "two" and "loonie").

Canada first minted a silver dollar coin in 1935 to celebrate the 25th anniversary of George V's reign as king. The voyageur dollar, so named because it featured an Indigenous person and a French voyageur paddling a canoe on the reverse, was minted in silver until 1967, after which it was composed primarily of nickel. The coins did not see wide circulation, mainly due to their size and weight; the nickel version weighed and was in diameter, and was itself smaller than the silver version.

By 1982, the Royal Canadian Mint had begun work on a new composition for the dollar coin that it hoped would lead to increased circulation. At the same time, vending machine operators and transit systems were lobbying the Government of Canada to replace the dollar banknotes with more widely circulating coins. A Commons committee recommended in 1985 that the dollar bill be eliminated despite a lack of evidence that Canadians would support the move. The government argued that it would save between $175 million and $250 million over 20 years by switching from bills that had a lifespan of less than a year to coins that would last two decades.

The government announced on March 25, 1986, that the new dollar coin would be launched the following year as a replacement for the dollar bill, which would be phased out. It was expected to cost $31.8 million to produce the first 300 million coins, but through seigniorage (the difference between the cost of production and the coin's value), expected to make up to $40 million a year on the coins. From the proceeds, a total of $60 million over five years was dedicated toward funding the 1988 Winter Olympics in Calgary.

The failure of the Susan B. Anthony dollar coin in the United States had been considered and it was believed Americans refused to support the coin due to its similarity to their quarter coin and its lack of esthetic appeal. In announcing the new Canadian dollar coin, the government stated it would be the same overall size as the Susan B. Anthony coin – slightly larger than a quarter – to allow for compatibility with American manufactured vending machines, but would be eleven-sided and gold-coloured.

It was planned that the coin would continue using the voyageur theme of its predecessor, but the master dies that had been struck in Ottawa were lost in transit en route to the Mint's facility at Winnipeg. A Commons committee struck to investigate the loss discovered that the Mint had no documented procedures for transport of master dies and that it had shipped them via a local courier in a bid to save $43.50. It was also found to be the third time that the Mint had lost master dies within five years. An internal review by the Royal Canadian Mint argued that while a policy existed to ship the obverse and reverse dies separately, the new coin dies were packaged separately but were part of the same shipment. The Mint also disagreed with the Royal Canadian Mounted Police's contention that the dies were simply lost in transit, believing instead that they were stolen. The dies were never recovered.

Fearing the possibility of counterfeiting, the government approved a new design for the reverse, replacing the voyageur with a Robert-Ralph Carmichael design of a common loon floating in water. The coin was immediately nicknamed the "loonie" across English Canada, and became known as a "huard", French for "loon", in Quebec. The loonie entered circulation on June 30, 1987, as 40 million coins were introduced into major cities across the country. Over 800 million loonies had been struck by the coin's 20th anniversary.

After a 21-month period in which the loonie and $1 note were produced concurrently with each other, the Bank of Canada ceased production of the dollar banknote. The final dollar bills were printed on June 30, 1989. Initial support for the coin was mixed, but withdrawing the banknote forced acceptance of the coin.

The loonie has subsequently gained iconic status within Canada, and is now regarded as a national symbol. The term "loonie" has since become synonymous with the Canadian dollar itself. The town of Echo Bay, Ontario, home of Robert-Ralph Carmichael, erected a large loonie monument in his honour in 1992 along the highway, similar to Sudbury's 'Big Nickel'.

Officials for the 2002 Salt Lake Winter Olympics invited the National Hockey League's ice making consultant, Dan Craig, to oversee the city's E Center arena, where the ice hockey tournament was being held. Craig invited a couple of members from the ice crew in his hometown of Edmonton to assist. One of them, Trent Evans, secretly placed a loonie at centre ice. He originally placed a dime, but added the loonie after the smaller coin quickly vanished as the ice surface was built up. He placed the coins after realizing there was no target at centre ice for referees to aim for when dropping the puck for a faceoff. A thin yellow dot was painted on the ice surface over the coins, though the loonie was faintly visible to those who knew to look for it.

Keeping the coin a secret, Evans told only a few people of its placement and swore them to secrecy. Among those told were the players of the men's and women's teams. Both Canadian teams went on to win gold medals. Several members of the women's team kissed the spot where the coin was buried following their victory. After the men won their final, the coin was dug up and given to Wayne Gretzky, the team's executive-director, who revealed the existence of the "lucky loonie" at a post-game press conference.

The lucky loonie quickly became a piece of Canadian lore. The original lucky loonie was donated to the Hockey Hall of Fame, and Canadians have subsequently hidden loonies at several international competitions. Loonies were buried in the foundations of facilities built for the 2010 Winter Olympics in Vancouver.

Capitalizing on the tradition, the Royal Canadian Mint has released a commemorative edition "lucky loonie" for each Olympic Games since 2004.

The weight of the coin was originally specified as 108 grains, equivalent to 6.998 grams. The coin's diameter is 26.5mm.

When introduced, loonie coins were made of Aureate, a bronze-electroplated nickel combination. Beginning in 2007, some loonie blanks also began to be produced with a cyanide-free brass plating process. In the spring of 2012, the composition switched to multi-ply brass-plated steel. As a result, the weight dropped from 7.00 to 6.27 grams. This has resulted in the 2012 loonie not being accepted in some vending machines. The Toronto Parking Authority estimates that at about $345 per machine, it will cost about $1 million to upgrade almost 3,000 machines to accept the new coins. The Mint states that multi-ply plated steel technology, already used in Canada's smaller coinage, produces an electromagnetic signature that is harder to counterfeit than that for regular alloy coins; also, using steel provides cost savings and avoids fluctuations in price or supply of nickel.

On April 10, 2012, the Royal Canadian Mint announced design changes to the loonie and toonie, which include new security features.

Alongside the regular minting of the loonie with the standard image of the common loon on the coin's reverse, the Royal Canadian Mint has also released commemorative editions of the one-dollar coin for a variety of occasions. These coins have a circulation-grade finish and have been made available to the public in five-coin packs and in 25-coin rolls in addition to being released directly into circulation.

The Terry Fox Loonie was unveiled in 2005 and designed by Senior Engraver Stanley Witten. The coin depicts the Canadian athlete, humanitarian, and cancer research activist Terry Fox.

Following his design of the 2005 Terry Fox loonie, Witten told the Ottawa Citizen that "while sculpting the design, I wanted to capture Terry fighting the elements, running against the wind, towering over wind-bent trees on a lonely stretch of Canadian wilderness."

In 1997, 2002, and each year since 2004, the Royal Canadian Mint has issued a one-dollar coin that depicts a different and unique image of a bird on the coin's reverse. These special loonies have limited mintages and are available only in the six-coin specimen sets.


Footnotes
Bibliography



</doc>
<doc id="18401" url="https://en.wikipedia.org/wiki?curid=18401" title="Laminar flow">
Laminar flow

In fluid dynamics, laminar flow is characterized by fluid particles following smooth paths in layers, with each layer moving smoothly past the adjacent layers with little or no mixing. At low velocities, the fluid tends to flow without lateral mixing, and adjacent layers slide past one another like playing cards. There are no cross-currents perpendicular to the direction of flow, nor eddies or swirls of fluids. In laminar flow, the motion of the particles of the fluid is very orderly with particles close to a solid surface moving in straight lines parallel to that surface.
Laminar flow is a flow regime characterized by high momentum diffusion and low momentum convection.

When a fluid is flowing through a closed channel such as a pipe or between two flat plates, either of two types of flow may occur depending on the velocity and viscosity of the fluid: laminar flow or turbulent flow. Laminar flow occurs at lower velocities, below a threshold at which the flow becomes turbulent. The velocity is determined by a dimensionless parameter characterizing the flow called the Reynolds number, which also depends on the viscosity and density of the fluid and dimensions of the channel. Turbulent flow is a less orderly flow regime that is characterized by eddies or small packets of fluid particles, which result in lateral mixing. In non-scientific terms, laminar flow is "smooth", while turbulent flow is "rough".

The type of flow occurring in a fluid in a channel is important in fluid-dynamics problems and subsequently affects heat and mass transfer in fluid systems. The dimensionless Reynolds number is an important parameter in the equations that describe whether fully developed flow conditions lead to laminar or turbulent flow. The Reynolds number is the ratio of the inertial force to the shearing force of the fluid: how fast the fluid is moving relative to how viscous it is, irrespective of the scale of the fluid system. Laminar flow generally occurs when the fluid is moving slowly or the fluid is very viscous. As the Reynolds number increases, such as by increasing the flow rate of the fluid, the flow will transition from laminar to turbulent flow at a specific range of Reynolds numbers, the laminar–turbulent transition range depending on small disturbance levels in the fluid or imperfections in the flow system. If the Reynolds number is very small, much less than 1, then the fluid will exhibit Stokes, or creeping, flow, where the viscous forces of the fluid dominate the inertial forces.

The specific calculation of the Reynolds number, and the values where laminar flow occurs, will depend on the geometry of the flow system and flow pattern. The common example is flow through a pipe, where the Reynolds number is defined as

where:

For such systems, laminar flow occurs when the Reynolds number is below a critical value of approximately 2,040, though the transition range is typically between 1,800 and 2,100.

For fluid systems occurring on external surfaces, such as flow past objects suspended in the fluid, other definitions for Reynolds numbers can be used to predict the type of flow around the object. The particle Reynolds number Re would be used for particle suspended in flowing fluids, for example. As with flow in pipes, laminar flow typically occurs with lower Reynolds numbers, while turbulent flow and related phenomena, such as vortex shedding, occur with higher Reynolds numbers.


Laminar airflow is used to separate volumes of air, or prevent airborne contaminants from entering an area. Laminar flow hoods are used to exclude contaminants from sensitive processes in science, electronics and medicine. Air curtains are frequently used in commercial settings to keep heated or refrigerated air from passing through doorways. A laminar flow reactor (LFR) is a reactor that uses laminar flow to study chemical reactions and process mechanisms.




</doc>
<doc id="18402" url="https://en.wikipedia.org/wiki?curid=18402" title="Luanda">
Luanda

Luanda (), is the capital and largest city in Angola. It is Angola's primary port, and its major industrial, cultural and urban centre. Located on Angola's northern Atlantic coast, Luanda is Angola's administrative centre, its chief seaport, and also the capital of the Luanda Province. Luanda and its metropolitan area is the most populous Portuguese-speaking capital city in the world, with over 8 million inhabitants in 2019 (a third of Angola's population). 

Among the oldest colonial cities of Africa, it was founded in January 1576 as "São Paulo da Assunção de Loanda" by Portuguese explorer Paulo Dias de Novais. The city served as the centre of the slave trade to Brazil before its prohibition. At the start of the Angolan Civil War in 1975, most of the white Portuguese left as refugees, principally for Portugal. Luanda's population increased greatly from refugees fleeing the war, but its infrastructure was inadequate to handle the increase. This also caused the exacerbation of slums, or musseques, around Luanda. The city is undergoing a major reconstruction, with many large developments taking place that will alter its cityscape significantly.

The industries present in the city include the processing of agricultural products, beverage production, textile, cement, newly car assembly plants, construction materials, plastics, metallurgy, cigarettes and shoes. The city is also notable as an economic centre for oil, and a refinery is located in the city. Luanda has been considered one of the most expensive cities in the world for expatriates. The inhabitants of Luanda are mostly members of the ethnic group of the Ambundu, but in recent times there has been an increase of the number of the Bakongo and the Ovimbundu. There exists a European population, consisting mainly of Portuguese. Luanda was the main host city for the matches of the 2010 African Cup of Nations.

Portuguese explorer Paulo Dias de Novais founded Luanda on 25 January 1576 as "São Paulo da Assumpção de Loanda", with one hundred families of settlers and four hundred soldiers. In 1618, the Portuguese built the fortress called "Fortaleza São Pedro da Barra", and they subsequently built two more: Fortaleza de São Miguel (1634) and Forte de São Francisco do Penedo (1765-6). Of these, the Fortaleza de São Miguel is the best preserved.

Luanda was Portugal's bridgehead from 1627, except during the Dutch rule of Luanda, from 1640 to 1648, as Fort Aardenburgh. The city served as the centre of slave trade to Brazil from circa 1550 to 1836. The slave trade was conducted mostly with the Portuguese colony of Brazil; Brazilian ships were the most numerous in the port of Luanda. This slave trade also involved local merchants and warriors who profited from the trade. During this period, no large scale territorial conquest was intended by the Portuguese; only a few minor settlements were established in the immediate hinterland of Luanda, some on the last stretch of the Kwanza River.

In the 17th century, the Imbangala became the main rivals of the Mbundu in supplying slaves to the Luanda market. In the 1750s, between 5,000 and 10,000 slaves were annually sold. By this time, Angola, a Portuguese colony, was in fact like a colony of Brazil, paradoxically another Portuguese colony. A strong degree of Brazilian influence was noted in Luanda until the Independence of Brazil in 1822. 

In the 19th century, still under Portuguese rule, Luanda experienced a major economic revolution. The slave trade was abolished in 1836, and in 1844, Angola's ports were opened to foreign shipping. By 1850, Luanda was one of the greatest and most developed Portuguese cities in the vast Portuguese Empire outside Continental Portugal, full of trading companies, exporting (together with Benguela) palm and peanut oil, wax, copal, timber, ivory, cotton, coffee, and cocoa, among many other products. Maize, tobacco, dried meat, and cassava flour are also produced locally. The Angolan bourgeoisie was born by this time.

In 1889, Governor Brito Capelo opened the gates of an aqueduct which supplied the city with water, a formerly scarce resource, laying the foundation for major growth.

Throughout Portugal's dictatorship, known as the Estado Novo, Luanda grew from a town of 61,208 with 14.6% of those inhabitants being white in 1940, to a wealthy cosmopolitan major city of 475,328 in 1970 with 124,814 Europeans (26.3%) and around 50,000 mixed race inhabitants.

Like most of Portuguese Angola, the cosmopolitan city of Luanda was not affected by the Portuguese Colonial War (1961–1974); economic growth and development in the entire region reached record highs during this period. In 1972, a report called Luanda the "Paris of Africa". 

By the time of Angolan independence in 1975, Luanda was a modern city. The majority of its population was African, but it was dominated by a strong minority of white Portuguese origin. 

After the Carnation Revolution in Lisbon on April 25, 1974, with the advent of independence and the start of the Angolan Civil War (1975–2002), most of the white Portuguese Luandans left as refugees, principally for Portugal, with many travelling overland to South Africa. There was an immediate crisis, however, as the local African population lacked the skills and knowledge needed to run the city and maintain its well-developed infrastructure.

The large numbers of skilled technicians among the force of Cuban soldiers sent in to support the Popular Movement for the Liberation of Angola (MPLA) government in the Angolan Civil War were able to make a valuable contribution to restoring and maintaining basic services in the city. 

In the following years, however, slums called "musseques" — which had existed for decades — began to grow out of proportion and stretched several kilometres beyond Luanda's former city limits as a result of the decades-long civil war, and because of the rise of deep social inequalities due to large-scale migration of civil war refugees from other Angolan regions. For decades, Luanda's facilities were not adequately expanded to handle this huge increase in the city's population.

After 2002, with the end of the civil war and high economic growth rates fuelled by the wealth provided by the increasing oil and diamond production, major reconstruction started. 

Luanda has also become one of the world's most expensive cities.

The central government supposedly allocates funds to all regions of the country, but the capital region receives the bulk of these funds. Since the end of the Angolan Civil War (1975–2002), stability has been widespread in the country, and major reconstruction has been going on since 2002 in those parts of the country that were damaged during the civil war. 

Luanda has been of major concern because its population had multiplied and had far outgrown the capacity of the city, especially because much of its infrastructure (water, electricity, roads etc.) had become obsolete and degraded.

Luanda has been undergoing major road reconstruction in the 21st century, and new highways are planned to improve connections to Cacuaco, Viana, Samba, and the new airport.

Major social housing is also being constructed to house those who reside in slums, which dominate the landscape of Luanda. A large Chinese firm has been given a contract to construct the majority of replacement housing in Luanda. The Angolan minister of health recently stated poverty in Angola will be overcome by an increase in jobs and the housing of every citizen.

Luanda is divided into two parts, the "Baixa de Luanda" (lower Luanda, the old city) and the "Cidade Alta" (upper city or the new part). The "Baixa de Luanda" is situated next to the port, and has narrow streets and old colonial buildings. However, new constructions have by now covered large areas beyond these traditional limits, and a number of previously independent nuclei — like Viana — were incorporated into the city.

Until 2011, the former Luanda Province comprised what now forms five municipalities. In 2011 the Province was enlarged by the addition of two additional municipalities transferred from Bengo Province, namely Icolo e Bengo, and Quiçama. Excluding these additions, the five municipalities comprise Greater Luanda:

Two new municipalities have been created within Greater Luanda since 2017: Talatona and Kilamba-Kiaxi

The city of Luanda is divided in six urban districts: Ingombota, Angola Quiluanje, Maianga, Rangel, Samba and Sambizanga.

In Samba and Sambizanga, more high-rise developments are to be built. The capital Luanda is growing constantly - and in addition, increasingly beyond the official city limits and even provincial boundaries.

Luanda is the seat of a Roman Catholic archbishop. It is also the location of most of Angola's educational institutions, including the private Catholic University of Angola and the public University of Agostinho Neto. It is also the home of the colonial Governor's Palace and the Estádio da Cidadela (the "Citadel Stadium"), Angola's main stadium, with a total seating capacity of 60,000.

Luanda has a hot semi-arid climate (Köppen climate classification: "BSh"). The climate is warm to hot but surprisingly dry, owing to the cool Benguela Current, which prevents moisture from easily condensing into rain. Frequent fog prevents temperatures from falling at night even during the completely dry months from May to October. Luanda has an annual rainfall of , but the variability is among the highest in the world, with a co-efficient of variation above 40 percent. Observed records since 1858 range from in 1958 to in 1916. The short rainy season in March and April depends on a northerly counter current bringing moisture to the city: it has been shown clearly that weakness in the Benguela Current can increase rainfall about sixfold compared with years when that current is strong.

The inhabitants of Luanda are primarily members of African ethnic groups, mainly Ambundu, Ovimbundu, and Bakongo. The official and the most widely used language is Portuguese, although several Bantu languages are also used, chiefly Kimbundu, Umbundu, and Kikongo. 

The population of Luanda has grown dramatically in recent years, due in large part to war-time migration to the city, which is safe compared to the rest of the country. Luanda, however, in 2006 saw an increase in violent crime, particularly in the shanty towns that surround the colonial urban core.

There is a sizable minority population of European origin, especially Portuguese (about 260,000), as well as Brazilians. In recent years, mainly since the mid-2000s, immigration from Portugal has increased due to greater opportunities present in Angola's booming economy. There is a sprinkling of immigrants from other African countries as well, including a small expatriate South African community. A small number of people of Luanda are of mixed race — European/Portuguese and native African. Over the last decades, a significant Chinese community has formed, as has a much smaller Vietnamese community.

Among the places of worship, they are predominantly Christian churches and temples: 

As the economic and political center of Angola, Luanda is similarly the epicenter of Angolan culture. The city is home to numerous cultural institutions, including the Sindika Dokolo Foundation.

The city hosts the annual Luanda International Jazz Festival, since 2009.

The city is home to numerous museums, including:

Other monuments in the city include:

Around one-third of Angolans live in Luanda, 53% of whom live in poverty. Living conditions in Luanda are poor for most of the people, with essential services such as safe drinking water and electricity still in short supply, and severe shortcomings in traffic conditions. On the other hand, luxury constructions for the benefit of the wealthy minority are booming. 

Luanda is one of the world's most expensive cities for resident foreigners.
New import tariffs imposed in March 2014 made Luanda even more expensive. As an example, a half-litre tub of vanilla ice-cream at the supermarket was reported to cost US$31. The higher import tariffs applied to hundreds of items, from garlic to cars. The stated aim was to try to diversify the heavily oil-dependent economy and nurture farming and industry, sectors which have remained weak. These tariffs have caused much hardship in a country where the average salary was US$260 per month in 2010, the latest year for which data was available. However, the average salary in the booming oil industry was over 20 times higher at US$5,400 per month.

Manufacturing includes processed foods, beverages, textiles, cement and other building materials, plastic products, metalware, cigarettes, and shoes/clothes. Petroleum (found in nearby off-shore deposits) is refined in the city, although this facility was repeatedly damaged during the Angolan Civil War of 1975–2002. Luanda has an excellent natural harbour; the chief exports are coffee, cotton, sugar, diamonds, iron, and salt.

The city also has a thriving building industry, an effect of the nationwide economic boom experienced since 2002, when political stability returned with the end of the civil war. Economic growth is largely supported by oil extraction activities, although great diversification is taking place. Large investment (domestic and international), along with strong economic growth, has dramatically increased construction of all economic sectors in the city of Luanda. In 2007, the first modern shopping mall in Angola was established in the city at Belas Shopping mall.

Luanda is the starting point of the Luanda railway that goes due east to Malanje. The civil war left the railway non-functional, but the railway has been restored up to Dondo and Malanje.

The main airport of Luanda is Quatro de Fevereiro Airport, which is the largest in the country. A new international airport, Angola International Airport is under construction southeast of the city, a few kilometres from Viana, which was expected to be opened in 2011. However, as the Angolan government did not continue to make the payments due to the Chinese enterprise in charge of the construction, the firm suspended its work in 2010.

The Port of Luanda serves as the largest port of Angola and is one of the busiest ports in Africa.. Major expansion of this port is also taking place. In 2014, a new port is being developed at Dande, about 30 km to the north.
Luanda's roads are in a poor state of repair, but are undergoing an extensive reconstruction process by the government in order to relieve traffic congestion in the city. Major road repairs can be found taking place in nearly every neighbourhood, including a major 6-lane highway connected Luanda to Viana.

Public transit is provided by the suburban services of the Luanda Railway, by the public company TCUL, and by a large fleet of privately owned collective taxis as white-blue painted minibuses called "Candongueiro". Candongueiros are usually Toyota Hiace vans, that are built to carry 12 people, although the candongueiros usually carry at least 15 people. They charge from 100 to 200 kwanzas per trip. They are known to disobey traffic rules, for example not stopping at signs and driving over pavements and aisles. 

In 2019, the Luanda Light Rail network with an estimated cost of US $3 billion was announced to begin construction in 2020.

International schools:

Universities:

In 2013 Luanda together with Namibe, today's Moçâmedes, hosted the 2013 FIRS Men's Roller Hockey World Cup, the first time that a World Cup of roller hockey was held in Africa. The city is home to the Desportivo do Bengo football club.

Luanda is twinned with:



</doc>
<doc id="18403" url="https://en.wikipedia.org/wiki?curid=18403" title="Logical positivism">
Logical positivism

Logical positivism, later called logical empiricism, and both of which together are also known as neopositivism, was a movement in Western philosophy whose central thesis was the verification principle (also known as the verifiability criterion of meaning). This theory of knowledge asserted that only statements verifiable through direct observation or logical proof are meaningful. Starting in the late 1920s, groups of philosophers, scientists, and mathematicians formed the Berlin Circle and the Vienna Circle, which, in these two cities, would propound the ideas of logical positivism.

Flourishing in several European centres through the 1930s, the movement sought to prevent confusion rooted in unclear language and unverifiable claims by converting philosophy into "scientific philosophy", which, according to the logical positivists, ought to share the bases and structures of empirical sciences' best examples, such as Albert Einstein's general theory of relativity. Despite its ambition to overhaul philosophy by studying and mimicking the extant conduct of empirical science, logical positivism became erroneously stereotyped as a movement to regulate the scientific process and to place strict standards on it.

After World War II, the movement shifted to a milder variant, logical empiricism, led mainly by Carl Hempel, who, during the rise of Nazism, had immigrated to the United States. In the ensuing years, the movement's central premises, still unresolved, were heavily criticised by leading philosophers, particularly Willard van Orman Quine and Karl Popper, and even, within the movement itself, by Hempel. By 1960, the movement had run its course. Soon, publication of Thomas Kuhn's landmark book, "The Structure of Scientific Revolutions", dramatically shifted academic philosophy's focus. By then, neopositivism was "dead, or as dead as a philosophical movement ever becomes".

Logical positivists culled from Ludwig Wittgenstein's early philosophy of language the verifiability principle or criterion of meaningfulness. As in Ernst Mach's phenomenalism, whereby the mind knows only actual or potential sensory experience, verificationists took all sciences' basic content to be only sensory experience. And some influence came from Percy Bridgman's musings that others proclaimed as operationalism, whereby a physical theory is understood by what laboratory procedures scientists perform to test its predictions. In verificationism, only the "verifiable" was scientific, and thus meaningful (or "cognitively meaningful"), whereas the unverifiable, being unscientific, were meaningless "pseudostatements" (just "emotively meaningful"). Unscientific discourse, as in ethics and metaphysics, would be unfit for discourse by philosophers, newly tasked to organize knowledge, not develop new knowledge.

Logical positivism is sometimes stereotyped as forbidding talk of unobservables, such as microscopic entities or such notions as causality and general principles, but that is an exaggeration. Rather, most neopositivists viewed talk of unobservables as metaphorical or elliptical: direct observations phrased abstractly or indirectly. So "theoretical terms" would garner meaning from "observational terms" via "correspondence rules", and thereby "theoretical laws" would be reduced to "empirical laws". Via Bertrand Russell's logicism, reducing mathematics to logic, physics' mathematical formulas would be converted to symbolic logic. Via Russell's logical atomism, ordinary language would break into discrete units of meaning. Rational reconstruction, then, would convert ordinary statements into standardized equivalents, all networked and united by a logical syntax. A scientific theory would be stated with its method of verification, whereby a logical calculus or empirical operation could verify its falsity or truth.

In the late 1930s, logical positivists fled Germany and Austria for Britain and the United States. By then, many had replaced Mach's phenomenalism with Otto Neurath's physicalism, whereby science's content is not actual or potential sensations, but instead is entities publicly observable. Rudolf Carnap, who had sparked logical positivism in the Vienna Circle, had sought to replace "verification" with simply "confirmation". With World War II's close in 1945, logical positivism became milder, "logical empiricism", led largely by Carl Hempel, in America, who expounded the covering law model of scientific explanation. Logical positivism became a major underpinning of analytic philosophy, and dominated philosophy in the English-speaking world, including philosophy of science, while influencing sciences, but especially social sciences, into the 1960s. Yet the movement failed to resolve its central problems, and its doctrines were increasingly criticized, most trenchantly by Willard Van Orman Quine, Norwood Hanson, Karl Popper, Thomas Kuhn, and Carl Hempel.

"Tractatus Logico-Philosophicus", by the young Ludwig Wittgenstein, introduced the view of philosophy as "critique of language", offering the possibility of a theoretically principled distinction of intelligible versus nonsensical discourse. "Tractatus" adhered to a correspondence theory of truth (versus a coherence theory of truth). Wittgenstein's influence also shows in some versions of the verifiability principle. In tractarian doctrine, truths of logic are tautologies, a view widely accepted by logical positivists who were also influenced by Wittgenstein's interpretation of probability although, according to Neurath, some logical positivists found "Tractatus" to contain too much metaphysics.

Gottlob Frege began the program of reducing mathematics to logic, continued it with Bertrand Russell, but lost interest in this logicism, and Russell continued it with Alfred North Whitehead in their "Principia Mathematica", inspiring some of the more mathematical logical posivists, such as Hans Hahn and Rudolf Carnap. Carnap's early anti-metaphysical works employed Russell's theory of types. Carnap envisioned a universal language that could reconstruct mathematics and thereby encode physics. Yet Kurt Gödel's incompleteness theorem showed this impossible except in trivial cases, and Alfred Tarski's undefinability theorem shattered all hopes of reducing mathematics to logic. Thus, a universal language failed to stem from Carnap's 1934 work "Logische Syntax der Sprache" ("Logical Syntax of Language"). Still, some logical positivists, including Carl Hempel, continued support of logicism.

In Germany, Hegelian metaphysics was a dominant movement, and Hegelian successors such as F H Bradley explained reality by postulating metaphysical entities lacking empirical basis, drawing reaction in the form of positivism. Starting in the late 19th century, there was a "back to Kant" movement. Ernst Mach's positivism and phenomenalism were a major influence.

The Vienna Circle, gathering around University of Vienna and Café Central, was led principally by Moritz Schlick. Schlick had held a neo-Kantian position, but later converted, via Carnap's 1928 book "Der logische Aufbau der Welt", that is, "The Logical Structure of the World". A 1929 pamphlet written by Otto Neurath, Hans Hahn, and Rudolf Carnap summarized the Vienna Circle's positions. Another member of Vienna Circle to later prove very influential was Carl Hempel. A friendly but tenacious critic of the Circle was Karl Popper, whom Neurath nicknamed the "Official Opposition".

Carnap and other Vienna Circle members, including Hahn and Neurath, saw need for a weaker criterion of meaningfulness than verifiability. A radical "left" wing—led by Neurath and Carnap—began the program of "liberalization of empiricism", and they also emphasized fallibilism and pragmatics, which latter Carnap even suggested as empiricism's basis. A conservative "right" wing—led by Schlick and Waismann—rejected both the liberalization of empiricism and the epistemological nonfoundationalism of a move from phenomenalism to physicalism. As Neurath and somewhat Carnap posed science toward social reform, the split in Vienna Circle also reflected political views.

The Berlin Circle was led principally by Hans Reichenbach.

Both Moritz Schlick and Rudolf Carnap had been influenced by and sought to define logical positivism versus the neo-Kantianism of Ernst Cassirer—the then leading figure of Marburg school, so called—and against Edmund Husserl's phenomenology. Logical positivists especially opposed Martin Heidegger's obscure metaphysics, the epitome of what logical positivism rejected. In the early 1930s, Carnap debated Heidegger over "metaphysical pseudosentences". Despite its revolutionary aims, logical positivism was but one view among many vying within Europe, and logical positivists initially spoke their language.

As the movement's first emissary to the New World, Moritz Schlick visited Stanford University in 1929, yet otherwise remained in Vienna and was murdered at the University, reportedly by a deranged student, in 1936. That year, a British attendee at some Vienna Circle meetings since 1933, A. J. Ayer saw his "Language, Truth and Logic", written in English, import logical positivism to the English-speaking world. By then, the Nazi Party's 1933 rise to power in Germany had triggered flight of intellectuals. In exile in England, Otto Neurath died in 1945. Rudolf Carnap, Hans Reichenbach, and Carl Hempel—Carnap's protégé who had studied in Berlin with Reichenbach—settled permanently in America. Upon Germany's annexation of Austria in 1938, remaining logical positivists, many of whom were also Jewish, were targeted and continued flight. Logical positivism thus became dominant in the English-speaking world.

Concerning reality, the necessary is a state true in all possible worlds—mere logical validity—whereas the contingent hinges on the way the particular world is. Concerning knowledge, the "a priori" is knowable before or without, whereas the "a posteriori" is knowable only after or through, relevant experience. Concerning statements, the "analytic" is true via terms' arrangement and meanings, thus a tautology—true by logical necessity but uninformative about the world—whereas the "synthetic" adds reference to a state of facts, a contingency.

In 1739, David Hume cast a fork aggressively dividing "relations of ideas" from "matters of fact and real existence", such that all truths are of one type or the other. By Hume's fork, truths by relations among ideas (abstract) all align on one side (analytic, necessary, "a priori"), whereas truths by states of actualities (concrete) always align on the other side (synthetic, contingent, "a posteriori"). Of any treatises containing neither, Hume orders, "Commit it then to the flames, for it can contain nothing but sophistry and illusion".

Thus awakened from "dogmatic slumber", Immanuel Kant quested to answer Hume's challenge—but by explaining how metaphysics is possible. Eventually, in his 1781 work, Kant crossed the tines of Hume's fork to identify another range of truths by necessity—synthetic "a priori", statements claiming states of facts but known true before experience—by arriving at transcendental idealism, attributing the mind a constructive role in phenomena by arranging sense data into the very experience "space", "time", and "substance". Thus, Kant saved Newton's law of universal gravitation from Hume's problem of induction by finding uniformity of nature to be "a priori" knowledge. Logical positivists rejected Kant's synthethic "a priori", and adopted Hume's fork, whereby a statement is either analytic and "a priori" (thus necessary and verifiable logically) or synthetic and "a posteriori" (thus contingent and verifiable empirically).

Early, most logical positivists proposed that all knowledge is based on logical inference from simple "protocol sentences" grounded in observable facts. In the 1936 and 1937 papers "Testability and meaning", individual terms replace sentences as the units of meaning. Further, theoretical terms no longer need to acquire meaning by explicit definition from observational terms: the connection may be indirect, through a system of implicit definitions. Carnap also provided an important, pioneering discussion of disposition predicates.

The logical positivists' initial stance was that a statement is "cognitively meaningful" only if some finite procedure conclusively determines its truth. By this verifiability principle, only statements verifiable either by their analyticity or by empiricism were "cognitively meaningful". Metaphysics, ontology, as well as much of ethics failed this criterion, and so were found "cognitively meaningless". Moritz Schlick, however, did not view ethical or aesthetic statements as cognitively meaningless. "Cognitive meaningfulness" was variously defined: having a truth value; corresponding to a possible state of affairs; intelligible or understandable as are scientific statements.

Ethics and aesthetics were subjective preferences, while theology and other metaphysics contained "pseudostatements", neither true nor false. This meaningfulness was cognitive, although other types of meaningfulness—for instance, emotive, expressive, or figurative—occurred in metaphysical discourse, dismissed from further review. Thus, logical positivism indirectly asserted Hume's law, the principle that "is" statements cannot justify "ought" statements, but are separated by an unbridgeable gap. A. J. Ayer's 1936 book asserted an extreme variant—the boo/hooray doctrine—whereby all evaluative judgments are but emotional reactions.

In an important pair of papers in 1936 and 1937, "Testability and meaning", Carnap replaced "verification" with "confirmation", on the view that although universal laws cannot be verified they can be confirmed. Later, Carnap employed abundant logical and mathematical methods in researching inductive logic while seeking to provide an account of probability as "degree of confirmation", but was never able to formulate a model. In Carnap's inductive logic, every universal law's degree of confirmation is always zero. In any event, the precise formulation of what came to be called the "criterion of cognitive significance" took three decades (Hempel 1950, Carnap 1956, Carnap 1961).

Carl Hempel became a major critic within the logical positivism movement. Hempel criticized the postivist thesis that empirical knowledge is restricted to "Basissätze"/"Beobachtungssätze"/"Protokollsätze" (basic statements or observation statements or protocol statements). Hempel elucidated the paradox of confirmation.

The second edition of A. J. Ayer's book arrived in 1946, and discerned "strong" versus "weak" forms of verification. Ayer concluded, "A proposition is said to be verifiable, in the strong sense of the term, if, and only if, its truth could be conclusively established by experience", but is verifiable in the weak sense "if it is possible for experience to render it probable". And yet, "no proposition, other than a tautology, can possibly be anything more than a probable hypothesis". Thus, all are open to weak verification.

Upon the global defeat of Nazism, and the removal from philosophy of rivals for radical reform—Marburg neo-Kantianism, Husserlian phenomenology, Heidegger's "existential hermeneutics"—and while hosted in the climate of American pragmatism and commonsense empiricism, the neopositivists shed much of their earlier, revolutionary zeal. No longer crusading to revise traditional philosophy into a new "scientific philosophy", they became respectable members of a new philosophy subdiscipline, "philosophy of science". Receiving support from Ernest Nagel, logical empiricists were especially influential in the social sciences.

Comtean positivism had viewed science as "description", whereas the logical positivists posed science as "explanation", perhaps to better realize the envisioned unity of science by covering not only fundamental science—that is, fundamental physics—but the special sciences, too, for instance biology, anthropology, psychology, sociology, and economics. The most widely accepted concept of scientific explanation, held even by neopositivist critic Karl Popper, was the deductive-nomological model (DN model). Yet DN model received its greatest explication by Carl Hempel, first in his 1942 article "The function of general laws in history", and more explicitly with Paul Oppenheim in their 1948 article "Studies in the logic of explanation".

In the DN model, the stated phenomenon to be explained is the "explanandum"—which can be an event, law, or theory—whereas premises stated to explain it are the "explanans". Explanans must be true or highly confirmed, contain at least one law, and entail the explanandum. Thus, given initial conditions "C, C . . . C" plus general laws "L, L . . . L", event "E" is a deductive consequence and scientifically explained. In the DN model, a law is an unrestricted generalization by conditional proposition—"If A, then B"—and has empirical content testable. (Differing from a merely true regularity—for instance, "George always carries only $1 bills in his wallet"—a law suggests what "must" be true, and is consequent of a scientific theory's axiomatic structure.)

By the Humean empiricist view that humans observe sequences of events, (not cause and effect, as causality and causal mechanisms are unobservable), the DN model neglects causality beyond mere constant conjunction, first event "A" and then always event "B". Hempel's explication of the DN model held natural laws—empirically confirmed regularities—as satisfactory and, if formulated realistically, approximating causal explanation. In later articles, Hempel defended the DN model and proposed a probabilistic explanation, inductive-statistical model (IS model). the DN and IS models together form the "covering law model", as named by a critic, William Dray. Derivation of statistical laws from other statistical laws goes to deductive-statistical model (DS model). Georg Henrik von Wright, another critic, named it "subsumption theory", fitting the ambition of theory reduction.

Logical positivists were generally committed to "Unified Science", and sought a common language or, in Neurath's phrase, a "universal slang" whereby all scientific propositions could be expressed. The adequacy of proposals or fragments of proposals for such a language was often asserted on the basis of various "reductions" or "explications" of the terms of one special science to the terms of another, putatively more fundamental. Sometimes these reductions consisted of set-theoretic manipulations of a few logically primitive concepts (as in Carnap's "Logical Structure of the World", 1928). Sometimes, these reductions consisted of allegedly analytic or "a priori" deductive relationships (as in Carnap's "Testability and meaning"). A number of publications over a period of thirty years would attempt to elucidate this concept.

As in Comtean positivism's envisioned unity of science, neopositivists aimed to network all special sciences through the covering law model of scientific explanation. And ultimately, by supplying boundary conditions and supplying bridge laws within the covering law model, all the special sciences' laws would reduce to fundamental physics, the fundamental science.

After World War II, key tenets of logical positivism, including its atomistic philosophy of science, the verifiability principle, and the fact/value gap, drew escalated criticism. It was clear that empirical claims cannot be verified to be universally true. Thus, as initially stated, the verifiability criterion made universal statements meaningless, and even made statements beyond empiricism for technological but not conceptual reasons meaningless, which would pose significant problems for science. These problems were recognized within the movement, which hosted attempted solutions—Carnap's move to "confirmation", Ayer's acceptance of "weak verification"—but the program drew sustained criticism from a number of directions by the 1950s. Even philosophers disagreeing among themselves on which direction general epistemology ought to take, as well as on philosophy of science, agreed that the logical empiricist program was untenable, and it became viewed as self-contradictory. The verifiability criterion of meaning was itself unverified. Notable critics included Nelson Goodman, Willard Van Orman Quine, Norwood Hanson, Karl Popper, Thomas Kuhn, J L Austin, Peter Strawson, Hilary Putnam, and Richard Rorty.

Although an empiricist, American logician Willard Van Orman Quine published the 1951 paper "Two Dogmas of Empiricism", which challenged conventional empiricist presumptions. Quine attacked the analytic/synthetic division, which the verificationist program had been hinged upon in order to entail, by consequence of Hume's fork, both necessity and aprioricity. Quine's ontological relativity explained that every term in any statement has its meaning contingent on a vast network of knowledge and belief, the speaker's conception of the entire world. Quine later proposed naturalized epistemology.

In 1958, Norwood Hanson's "Patterns of Discovery" undermined the division of observation versus theory, as one can predict, collect, prioritize, and assess data only via some horizon of expectation set by a theory. Thus, any dataset—the direct observations, the scientific facts—is laden with theory.

An early, tenacious critic was Karl Popper whose 1934 book "Logik der Forschung", arriving in English in 1959 as "The Logic of Scientific Discovery", directly answered verificationism. Popper heeded the problem of induction as rendering empirical verification logically impossible, and the deductive fallacy of affirming the consequent reveals any phenomenon's capacity to host more than one logically possible explanation. Accepting scientific method as hypotheticodeduction, whose inference form is denying the consequent, Popper finds scientific method unable to proceed without falsifiable predictions. Popper thus identifies falsifiability to demarcate not "meaningful" from "meaningless" but simply "scientific" from "unscientific"—a label not in itself unfavorable.

Popper finds virtue in metaphysics, required to develop new scientific theories. And an unfalsifiable—thus unscientific, perhaps metaphysical—concept in one era can later, through evolving knowledge or technology, become falsifiable, thus scientific. Popper also found science's quest for truth to rest on values. Popper disparages the "pseudoscientific", which occurs when an unscientific theory is proclaimed true and coupled with seemingly scientific method by "testing" the unfalsifiable theory—whose predictions are confirmed by necessity—or when a scientific theory's falsifiable predictions are strongly falsified but the theory is persistently protected by "immunizing stratagems", such as the appendage of "ad hoc" clauses saving the theory or the recourse to increasingly speculative hypotheses shielding the theory.

Popper's "scientific" epistemology is falsificationism, which finds that no number, degree, and variety of empirical successes can either verify or confirm scientific theory. Falsificationism finds science's aim as "corroboration" of scientific theory, which strives for scientific realism but accepts the maximal status of strongly corroborated verisimilitude ("truthlikeness"). Explicitly denying the positivist view that all knowledge is scientific, Popper developed the "general" epistemology critical rationalism, which finds human knowledge to evolve by "conjectures and refutations". Popper thus acknowledged the value of the positivist movement, driving evolution of human understanding, but claimed that he had "killed positivism".

With his landmark, "The Structure of Scientific Revolutions" (1962), Thomas Kuhn critically destabilized the verificationist program, which was presumed to call for foundationalism. (But already in the 1930s, Otto Neurath had argued for nonfoundationalism via coherentism by likening science to a boat (Neurath's boat) that scientists must rebuild at sea.) Although Kuhn's thesis itself was attacked even by opponents of neopositivism, in the 1970 postscript to "Structure", Kuhn asserted, at least, that there was no algorithm to science—and, on that, even most of Kuhn's critics agreed.

Powerful and persuasive, Kuhn's book, unlike the vocabulary and symbols of logic's formal language, was written in natural language open to the layperson. Kuhn's book was first published in a volume of "International Encyclopedia of Unified Science"—a project begun by logical positivists but co-edited by Neurath whose view of science was already nonfoundationalist as mentioned above—and some sense unified science, indeed, but by bringing it into the realm of historical and social assessment, rather than fitting it to the model of physics. Kuhn's ideas were rapidly adopted by scholars in disciplines well outside natural sciences, and, as logical empiricists were extremely influential in the social sciences, ushered academia into postpositivism or postempiricism.

The "received view" operates on the "correspondence rule" that states, "The observational terms are taken as referring to specified phenomena or phenomenal properties, and the only interpretation given to the theoretical terms is their explicit definition provided by the correspondence rules". According to Hilary Putnam, a former student of Reichenbach and of Carnap, the dichotomy of observational terms versus theoretical terms introduced a problem within scientific discussion that was nonexistent until this dichotomy was stated by logical positivists. Putnam's four objections:


Putnam also alleged that positivism was actually a form of metaphysical idealism by its rejecting scientific theory's ability to garner knowledge about nature's unobservable aspects. With his "no miracles" argument, posed in 1974, Putnam asserted scientific realism, the stance that science achieves true—or approximately true—knowledge of the world as it exists independently of humans' sensory experience. In this, Putnam opposed not only the positivism but other instrumentalism—whereby scientific theory is but a human tool to predict human observations—filling the void left by positivism's decline.

By the late 1960s, logical positivism had become exhausted. Interviewed in the late 1970s, A. J. Ayer supposed that "the most important" defect "was that nearly all of it was false". After some laughter, he says that "it was true in spirit." Although logical positivism tends to be recalled as a pillar of scientism, Carl Hempel was key in establishing the philosophy subdiscipline philosophy of science where Thomas Kuhn and Karl Popper brought in the era of postpositivism. John Passmore found logical positivism to be "dead, or as dead as a philosophical movement ever becomes".

Logical positivism's fall reopened debate over the metaphysical merit of scientific theory, whether it can offer knowledge of the world beyond human experience (scientific realism) versus whether it is but a human tool to predict human experience (instrumentalism). Meanwhile, it became popular among philosophers to rehash the faults and failures of logical positivism without investigation of it. Thereby, logical positivism has been generally misrepresented, sometimes severely. Arguing for their own views, often framed versus logical positivism, many philosophers have reduced logical positivism to simplisms and stereotypes, especially the notion of logical positivism as a type of foundationalism. In any event, the movement helped anchor analytic philosophy in the English-speaking world, and returned Britain to empiricism. Without the logical positivists, who have been tremendously influential outside philosophy, especially in psychology and social sciences, intellectual life of the 20th century would be unrecognizable.




Articles by logical positivists

Articles on logical positivism

Articles on related philosophical topics


</doc>
<doc id="18404" url="https://en.wikipedia.org/wiki?curid=18404" title="Lorentz transformation">
Lorentz transformation

In physics, the Lorentz transformations are a one-parameter family of linear transformations from a coordinate frame in spacetime to another frame that moves at a constant velocity (the parameter) relative to the former. The respective inverse transformation is then parametrized by the negative of this velocity. The transformations are named after the Dutch physicist Hendrik Lorentz.

The most common form of the transformation, parametrized by the real constant formula_1 representing a velocity confined to the -direction, is expressed as 
where and are the coordinates of an event in two frames, where the primed frame is seen from the unprimed frame as moving with speed along the -axis, is the speed of light, and formula_3 is the Lorentz factor. When speed "v" is much smaller than "c", the Lorentz factor is negligibly different from 1, but as "v" approaches "c", formula_4 grows without bound. The value of "v" must be smaller than "c" for the transformation to make sense.

Expressing the speed as formula_5 an equivalent form of the transformation is

Frames of reference can be divided into two groups: inertial (relative motion with constant velocity) and non-inertial (accelerating, moving in curved paths, rotational motion with constant angular velocity, etc.). The term "Lorentz transformations" only refers to transformations between "inertial" frames, usually in the context of special relativity.

In each reference frame, an observer can use a local coordinate system (usually Cartesian coordinates in this context) to measure lengths, and a clock to measure time intervals. An event is something that happens at a point in space at an instant of time, or more formally a point in spacetime. The transformations connect the space and time coordinates of an event as measured by an observer in each frame.

They supersede the Galilean transformation of Newtonian physics, which assumes an absolute space and time (see Galilean relativity). The Galilean transformation is a good approximation only at relative speeds much less than the speed of light. Lorentz transformations have a number of unintuitive features that do not appear in Galilean transformations. For example, they reflect the fact that observers moving at different velocities may measure different distances, elapsed times, and even different orderings of events, but always such that the speed of light is the same in all inertial reference frames. The invariance of light speed is one of the postulates of special relativity.

Historically, the transformations were the result of attempts by Lorentz and others to explain how the speed of light was observed to be independent of the reference frame, and to understand the symmetries of the laws of electromagnetism. The Lorentz transformation is in accordance with Albert Einstein's special relativity, but was derived first.

The Lorentz transformation is a linear transformation. It may include a rotation of space; a rotation-free Lorentz transformation is called a Lorentz boost. In Minkowski space, the mathematical model of spacetime in special relativity, the Lorentz transformations preserve the spacetime interval between any two events. This property is the defining property of a Lorentz transformation. They describe only the transformations in which the spacetime event at the origin is left fixed. They can be considered as a hyperbolic rotation of Minkowski space. The more general set of transformations that also includes translations is known as the Poincaré group.

Many physicists—including Woldemar Voigt, George FitzGerald, Joseph Larmor, and Hendrik Lorentz himself—had been discussing the physics implied by these equations since 1887. Early in 1889, Oliver Heaviside had shown from Maxwell's equations that the electric field surrounding a spherical distribution of charge should cease to have spherical symmetry once the charge is in motion relative to the aether. FitzGerald then conjectured that Heaviside's distortion result might be applied to a theory of intermolecular forces. Some months later, FitzGerald published the conjecture that bodies in motion are being contracted, in order to explain the baffling outcome of the 1887 aether-wind experiment of Michelson and Morley. In 1892, Lorentz independently presented the same idea in a more detailed manner, which was subsequently called FitzGerald–Lorentz contraction hypothesis. Their explanation was widely known before 1905.

Lorentz (1892–1904) and Larmor (1897–1900), who believed the luminiferous aether hypothesis, also looked for the transformation under which Maxwell's equations are invariant when transformed from the aether to a moving frame. They extended the FitzGerald–Lorentz contraction hypothesis and found out that the time coordinate has to be modified as well ("local time"). Henri Poincaré gave a physical interpretation to local time (to first order in "v"/"c", the relative velocity of the two reference frames normalized to the speed of light) as the consequence of clock synchronization, under the assumption that the speed of light is constant in moving frames. Larmor is credited to have been the first to understand the crucial time dilation property inherent in his equations.

In 1905, Poincaré was the first to recognize that the transformation has the properties of a mathematical group,
and named it after Lorentz.
Later in the same year Albert Einstein published what is now called special relativity, by deriving the Lorentz transformation under the assumptions of the principle of relativity and the constancy of the speed of light in any inertial reference frame, and by abandoning the mechanistic aether as unnecessary.

An "event" is something that happens at a certain point in spacetime, or more generally, the point in spacetime itself. In any inertial frame an event is specified by a time coordinate "ct" and a set of Cartesian coordinates to specify position in space in that frame. Subscripts label individual events.

From Einstein's second postulate of relativity follows

in all inertial frames for events connected by "light signals". The quantity on the left is called the "spacetime interval" between events and . The interval between "any two" events, not necessarily separated by light signals, is in fact invariant, i.e., independent of the state of relative motion of observers in different inertial frames, as is shown using homogeneity and isotropy of space. The transformation sought after thus must possess the property that

where are the spacetime coordinates used to define events in one frame, and are the coordinates in another frame. First one observes that is satisfied if an arbitrary -tuple of numbers are added to events and . Such transformations are called "spacetime translations" and are not dealt with further here. Then one observes that a "linear" solution preserving the origin of the simpler problem

solves the general problem too. (A solution satisfying the left formula automatically satisfies the right formula, see polarization identity.) Finding the solution to the simpler problem is just a matter of look-up in the theory of classical groups that preserve bilinear forms of various signature. First equation in can be written more compactly as

where refers to the bilinear form of signature on exposed by the right hand side formula in . The alternative notation defined on the right is referred to as the "relativistic dot product". Spacetime mathematically viewed as endowed with this bilinear form is known as Minkowski space . The Lorentz transformation is thus an element of the group Lorentz group , the Lorentz group or, for those that prefer the other metric signature, (also called the Lorentz group). One has

which is precisely preservation of the bilinear form which implies (by linearity of and bilinearity of the form) that is satisfied. The elements of the Lorentz group are rotations and "boosts" and mixes thereof. If the spacetime translations are included, then one obtains the "inhomogeneous Lorentz group" or the Poincaré group.

The relations between the primed and unprimed spacetime coordinates are the Lorentz transformations, each coordinate in one frame is a linear function of all the coordinates in the other frame, and the inverse functions are the inverse transformation. Depending on how the frames move relative to each other, and how they are oriented in space relative to each other, other parameters that describe direction, speed, and orientation enter the transformation equations.

Transformations describing relative motion with constant (uniform) velocity and without rotation of the space coordinate axes are called "boosts", and the relative velocity between the frames is the parameter of the transformation. The other basic type of Lorentz transformation is rotation in the spatial coordinates only, these like boosts are inertial transformations since there is no relative motion, the frames are simply tilted (and not continuously rotating), and in this case quantities defining the rotation are the parameters of the transformation (e.g., axis–angle representation, or Euler angles, etc.). A combination of a rotation and boost is a "homogeneous transformation", which transforms the origin back to the origin.

The full Lorentz group also contains special transformations that are neither rotations nor boosts, but rather reflections in a plane through the origin. Two of these can be singled out; spatial inversion in which the spatial coordinates of all events are reversed in sign and temporal inversion in which the time coordinate for each event gets its sign reversed.

Boosts should not be conflated with mere displacements in spacetime; in this case, the coordinate systems are simply shifted and there is no relative motion. However, these also count as symmetries forced by special relativity since they leave the spacetime interval invariant. A combination of a rotation with a boost, followed by a shift in spacetime, is an "inhomogeneous Lorentz transformation", an element of the Poincaré group, which is also called the inhomogeneous Lorentz group.

A "stationary" observer in frame defines events with coordinates . Another frame moves with velocity relative to , and an observer in this "moving" frame defines events using the coordinates .

The coordinate axes in each frame are parallel (the and axes are parallel, the and axes are parallel, and the and axes are parallel), remain mutually perpendicular, and relative motion is along the coincident axes. At , the origins of both coordinate systems are the same, . In other words, the times and positions are coincident at this event. If all these hold, then the coordinate systems are said to be in standard configuration, or synchronized.

If an observer in records an event , then an observer in records the "same" event with coordinates

where is the relative velocity between frames in the -direction, is the speed of light, and

(lowercase gamma) is the Lorentz factor.

Here, is the "parameter" of the transformation, for a given boost it is a constant number, but can take a continuous range of values. In the setup used here, positive relative velocity is motion along the positive directions of the axes, zero relative velocity is no relative motion, while negative relative velocity is relative motion along the negative directions of the axes. The magnitude of relative velocity cannot equal or exceed , so only subluminal speeds are allowed. The corresponding range of is .

The transformations are not defined if is outside these limits. At the speed of light () is infinite, and faster than light () is a complex number, each of which make the transformations unphysical. The space and time coordinates are measurable quantities and numerically must be real numbers.

As an active transformation, an observer in F′ notices the coordinates of the event to be "boosted" in the negative directions of the axes, because of the in the transformations. This has the equivalent effect of the "coordinate system" F′ boosted in the positive directions of the axes, while the event does not change and is simply represented in another coordinate system, a passive transformation.

The inverse relations ( in terms of ) can be found by algebraically solving the original set of equations. A more efficient way is to use physical principles. Here is the "stationary" frame while is the "moving" frame. According to the principle of relativity, there is no privileged frame of reference, so the transformations from to must take exactly the same form as the transformations from to . The only difference is moves with velocity relative to (i.e., the relative velocity has the same magnitude but is oppositely directed). Thus if an observer in notes an event , then an observer in notes the "same" event with coordinates

and the value of remains unchanged. This "trick" of simply reversing the direction of relative velocity while preserving its magnitude, and exchanging primed and unprimed variables, always applies to finding the inverse transformation of every boost in any direction.

Sometimes it is more convenient to use (lowercase beta) instead of , so that

which shows much more clearly the symmetry in the transformation. From the allowed ranges of and the definition of , it follows . The use of and is standard throughout the literature.

The Lorentz transformations can also be derived in a way that resembles circular rotations in 3d space using the hyperbolic functions. For the boost in the direction, the results are

where (lowercase zeta) is a parameter called "rapidity" (many other symbols are used, including ). Given the strong resemblance to rotations of spatial coordinates in 3d space in the Cartesian xy, yz, and zx planes, a Lorentz boost can be thought of as a hyperbolic rotation of spacetime coordinates in the xt, yt, and zt Cartesian-time planes of 4d Minkowski space. The parameter is the hyperbolic angle of rotation, analogous to the ordinary angle for circular rotations. This transformation can be illustrated with a Minkowski diagram.

The hyperbolic functions arise from the "difference" between the squares of the time and spatial coordinates in the spacetime interval, rather than a sum. The geometric significance of the hyperbolic functions can be visualized by taking or in the transformations. Squaring and subtracting the results, one can derive hyperbolic curves of constant coordinate values but varying , which parametrizes the curves according to the identity

Conversely the and axes can be constructed for varying coordinates but constant . The definition

provides the link between a constant value of rapidity, and the slope of the axis in spacetime. A consequence these two hyperbolic formulae is an identity that matches the Lorentz factor

Comparing the Lorentz transformations in terms of the relative velocity and rapidity, or using the above formulae, the connections between , , and are

Taking the inverse hyperbolic tangent gives the rapidity

Since , it follows . From the relation between and , positive rapidity is motion along the positive directions of the axes, zero rapidity is no relative motion, while negative rapidity is relative motion along the negative directions of the axes.

The inverse transformations are obtained by exchanging primed and unprimed quantities to switch the coordinate frames, and negating rapidity since this is equivalent to negating the relative velocity. Therefore,

The inverse transformations can be similarly visualized by considering the cases when and .

So far the Lorentz transformations have been applied to "one event". If there are two events, there is a spatial separation and time interval between them. It follows from the linearity of the Lorentz transformations that two values of space and time coordinates can be chosen, the Lorentz transformations can be applied to each, then subtracted to get the Lorentz transformations of the differences;

with inverse relations

where (uppercase delta) indicates a difference of quantities; e.g., for two values of coordinates, and so on.

These transformations on "differences" rather than spatial points or instants of time are useful for a number of reasons:


A critical requirement of the Lorentz transformations is the invariance of the speed of light, a fact used in their derivation, and contained in the transformations themselves. If in the equation for a pulse of light along the direction is , then in the Lorentz transformations give , and vice versa, for any .

For relative speeds much less than the speed of light, the Lorentz transformations reduce to the Galilean transformation

in accordance with the correspondence principle. It is sometimes said that nonrelativistic physics is a physics of "instantaneous action at a distance".

Three counterintuitive, but correct, predictions of the transformations are:

The use of vectors allows positions and velocities to be expressed in arbitrary directions compactly. A single boost in any direction depends on the full relative velocity vector with a magnitude that cannot equal or exceed , so that .

Only time and the coordinates parallel to the direction of relative motion change, while those coordinates perpendicular do not. With this in mind, split the spatial position vector as measured in , and as measured in , each into components perpendicular (⊥) and parallel ( ‖ ) to ,

then the transformations are

where · is the dot product. The Lorentz factor retains its definition for a boost in any direction, since it depends only on the magnitude of the relative velocity. The definition with magnitude is also used by some authors.

Introducing a unit vector in the direction of relative motion, the relative velocity is with magnitude and direction , and vector projection and rejection give respectively

Accumulating the results gives the full transformations,

\end{align}</math>

The projection and rejection also applies to . For the inverse transformations, exchange and to switch observed coordinates, and negate the relative velocity (or simply the unit vector since the magnitude is always positive) to obtain

\end{align}</math>

The unit vector has the advantage of simplifying equations for a single boost, allows either or to be reinstated when convenient, and the rapidity parametrization is immediately obtained by replacing and . It is not convenient for multiple boosts.

The vectorial relation between relative velocity and rapidity is

and the "rapidity vector" can be defined as

each of which serves as a useful abbreviation in some contexts. The magnitude of is the absolute value of the rapidity scalar confined to , which agrees with the range .

Defining the coordinate velocities and Lorentz factor by

taking the differentials in the coordinates and time of the vector transformations, then dividing equations, leads to

The velocities and are the velocity of some massive object. They can also be for a third inertial frame (say "F"′′), in which case they must be "constant". Denote either entity by X. Then X moves with velocity relative to F, or equivalently with velocity relative to F′, in turn F′ moves with velocity relative to F. The inverse transformations can be obtained in a similar way, or as with position coordinates exchange and , and change to .

The transformation of velocity is useful in stellar aberration, the Fizeau experiment, and the relativistic Doppler effect.

The Lorentz transformations of acceleration can be similarly obtained by taking differentials in the velocity vectors, and dividing these by the time differential.

In general, given four quantities and and their Lorentz-boosted counterparts and , a relation of the form

implies the quantities transform under Lorentz transformations similar to the transformation of spacetime coordinates;

The decomposition of (and ) into components perpendicular and parallel to is exactly the same as for the position vector, as is the process of obtaining the inverse transformations (exchange and to switch observed quantities, and reverse the direction of relative motion by the substitution ).

The quantities collectively make up a "four-vector", where is the "timelike component", and the "spacelike component". Examples of and are the following:

For a given object (e.g., particle, fluid, field, material), if or correspond to properties specific to the object like its charge density, mass density, spin, etc., its properties can be fixed in the rest frame of that object. Then the Lorentz transformations give the corresponding properties in a frame moving relative to the object with constant velocity. This breaks some notions taken for granted in non-relativistic physics. For example, the energy of an object is a scalar in non-relativistic mechanics, but not in relativistic mechanics because energy changes under Lorentz transformations; its value is different for various inertial frames. In the rest frame of an object, it has a rest energy and zero momentum. In a boosted frame its energy is different and it appears to have a momentum. Similarly, in non-relativistic quantum mechanics the spin of a particle is a constant vector, but in relativistic quantum mechanics spin depends on relative motion. In the rest frame of the particle, the spin pseudovector can be fixed to be its ordinary non-relativistic spin with a zero timelike quantity , however a boosted observer will perceive a nonzero timelike component and an altered spin.

Not all quantities are invariant in the form as shown above, for example orbital angular momentum does not have a timelike quantity, and neither does the electric field nor the magnetic field . The definition of angular momentum is , and in a boosted frame the altered angular momentum is . Applying this definition using the transformations of coordinates and momentum leads to the transformation of angular momentum. It turns out transforms with another vector quantity related to boosts, see relativistic angular momentum for details. For the case of the and fields, the transformations cannot be obtained as directly using vector algebra. The Lorentz force is the definition of these fields, and in it is while in it is . A method of deriving the EM field transformations in an efficient way which also illustrates the unit of the electromagnetic field uses tensor algebra, given below.

Throughout, italic non-bold capital letters are 4×4 matrices, while non-italic bold letters are 3×3 matrices.

Writing the coordinates in column vectors and the Minkowski metric as a square matrix

the spacetime interval takes the form (T denotes transpose)

and is invariant under a Lorentz transformation

where Λ is a square matrix which can depend on parameters.

The set of all Lorentz transformations Λ in this article is denoted formula_30. This set together with matrix multiplication forms a group, in this context known as the "Lorentz group". Also, the above expression is a quadratic form of signature (3,1) on spacetime, and the group of transformations which leaves this quadratic form invariant is the indefinite orthogonal group O(3,1), a Lie group. In other words, the Lorentz group is O(3,1). As presented in this article, any Lie groups mentioned are matrix Lie groups. In this context the operation of composition amounts to matrix multiplication.

From the invariance of the spacetime interval it follows

and this matrix equation contains the general conditions on the Lorentz transformation to ensure invariance of the spacetime interval. Taking the determinant of the equation using the product rule gives immediately

Writing the Minkowski metric as a block matrix, and the Lorentz transformation in the most general form,

carrying out the block matrix multiplications obtains general conditions on to ensure relativistic invariance. Not much information can be directly extracted from all the conditions, however one of the results

is useful; always so it follows that

The negative inequality may be unexpected, because multiplies the time coordinate and this has an effect on time symmetry. If the positive equality holds, then is the Lorentz factor.

The determinant and inequality provide four ways to classify Lorentz Transformations ("herein LTs for brevity"). Any particular LT has only one determinant sign "and" only one inequality. There are four sets which include every possible pair given by the intersections ("n"-shaped symbol meaning "and") of these classifying sets.

where "+" and "−" indicate the determinant sign, while "↑" for ≥ and "↓" for ≤ denote the inequalities.

The full Lorentz group splits into the union ("u"-shaped symbol meaning "or") of four disjoint sets

A subgroup of a group must be closed under the same operation of the group (here matrix multiplication). In other words, for two Lorentz transformations and from a particular set, the composite Lorentz transformations and must be in the same set as and . This is not always the case: the composition of two antichronous Lorentz transformations is orthochronous, and the composition of two improper Lorentz transformations is proper. In other words, while the sets formula_37, formula_38, formula_39, and formula_40 all form subgroups, the sets containing improper and/or antichronous transformations without enough proper orthochronous transformations (e.g. formula_41, formula_42, formula_43) do not form subgroups.

If a Lorentz covariant 4-vector is measured in one inertial frame with result formula_44, and the same measurement made in another inertial frame (with the same orientation and origin) gives result formula_45, the two results will be related by

where the boost matrix formula_47 represents the Lorentz transformation between the unprimed and primed frames and formula_48 is the velocity of the primed frame as seen from the unprimed frame. The matrix is given by

where formula_50 is the magnitude of the velocity and formula_51 is the Lorentz factor. This formula represents a passive transformation, as it describes how the coordinates of the measured quantity changes from the unprimed frame to the primed frame. The active transformation is given by formula_52.

If a frame is boosted with velocity relative to frame , and another frame is boosted with velocity relative to , the separate boosts are
and the composition of the two boosts connects the coordinates in and ,
Successive transformations act on the left. If and are collinear (parallel or antiparallel along the same line of relative motion), the boost matrices commute: . This composite transformation happens to be another boost, , where is collinear with and .

If and are not collinear but in different directions, the situation is considerably more complicated. Lorentz boosts along different directions do not commute: and are not equal. Also, each of these compositions is "not" a single boost, but they are still Lorentz transformations they each preserve the spacetime interval. It turns out the composition of any two Lorentz boosts is equivalent to a boost followed or preceded by a rotation on the spatial coordinates, in the form of or . The and are composite velocities, while and are rotation parameters (e.g. axis-angle variables, Euler angles, etc.). The rotation in block matrix form is simply

where is a 3d rotation matrix, which rotates any 3d vector in one sense (active transformation), or equivalently the coordinate frame in the opposite sense (passive transformation). It is "not" simple to connect and (or and ) to the original boost parameters and . In a composition of boosts, the matrix is named the Wigner rotation, and gives rise to the Thomas precession. These articles give the explicit formulae for the composite transformation matrices, including expressions for .

In this article the axis-angle representation is used for . The rotation is about an axis in the direction of a unit vector , through angle (positive anticlockwise, negative clockwise, according to the right-hand rule). The "axis-angle vector"
will serve as a useful abbreviation.

Spatial rotations alone are also Lorentz transformations they leave the spacetime interval invariant. Like boosts, successive rotations about different axes do not commute. Unlike boosts, the composition of any two rotations is equivalent to a single rotation. Some other similarities and differences between the boost and rotation matrices include:


The most general proper Lorentz transformation includes a boost and rotation together, and is a nonsymmetric matrix. As special cases, and . An explicit form of the general Lorentz transformation is cumbersome to write down and will not be given here. Nevertheless, closed form expressions for the transformation matrices will be given below using group theoretical arguments. It will be easier to use the rapidity parametrization for boosts, in which case one writes and .

The set of transformations

with matrix multiplication as the operation of composition forms a group, called the "restricted Lorentz group", and is the special indefinite orthogonal group SO(3,1). (The plus sign indicates that it preserves the orientation of the temporal dimension).

For simplicity, look at the infinitesimal Lorentz boost in the x direction (examining a boost in any other direction, or rotation about any axis, follows an identical procedure). The infinitesimal boost is a small boost away from the identity, obtained by the Taylor expansion of the boost matrix to first order about ,

where the higher order terms not shown are negligible because is small, and is simply the boost matrix in the "x" direction. The derivative of the matrix is the matrix of derivatives (of the entries, with respect to the same variable), and it is understood the derivatives are found first then evaluated at ,

For now, is defined by this result (its significance will be explained shortly). In the limit of an infinite number of infinitely small steps, the finite boost transformation in the form of a matrix exponential is obtained

where the limit definition of the exponential has been used (see also characterizations of the exponential function). More generally

The axis-angle vector and rapidity vector are altogether six continuous variables which make up the group parameters (in this particular representation), and the generators of the group are and , each vectors of matrices with the explicit forms

These are all defined in an analogous way to above, although the minus signs in the boost generators are conventional. Physically, the generators of the Lorentz group correspond to important symmetries in spacetime: are the "rotation generators" which correspond to angular momentum, and are the "boost generators" which correspond to the motion of the system in spacetime. The derivative of any smooth curve with in the group depending on some group parameter with respect to that group parameter, evaluated at , serves as a definition of a corresponding group generator , and this reflects an infinitesimal transformation away from the identity. The smooth curve can always be taken as an exponential as the exponential will always map smoothly back into the group via for all ; this curve will yield again when differentiated at .

Expanding the exponentials in their Taylor series obtains

which compactly reproduce the boost and rotation matrices as given in the previous section.

It has been stated that the general proper Lorentz transformation is a product of a boost and rotation. At the "infinitesimal" level the product

is commutative because only linear terms are required (products like and count as higher order terms and are negligible). Taking the limit as before leads to the finite transformation in the form of an exponential

The converse is also true, but the decomposition of a finite general Lorentz transformation into such factors is nontrivial. In particular,

because the generators do not commute. For a description of how to find the factors of a general Lorentz transformation in terms of a boost and a rotation "in principle" (this usually does not yield an intelligible expression in terms of generators and ), see Wigner rotation. If, on the other hand, "the decomposition is given" in terms of the generators, and one wants to find the product in terms of the generators, then the Baker–Campbell–Hausdorff formula applies.

Lorentz generators can be added together, or multiplied by real numbers, to obtain more Lorentz generators. In other words, the set of all Lorentz generators

together with the operations of ordinary matrix addition and multiplication of a matrix by a number, forms a vector space over the real numbers. The generators form a basis set of "V", and the components of the axis-angle and rapidity vectors, , are the coordinates of a Lorentz generator with respect to this basis.

Three of the commutation relations of the Lorentz generators are

where the bracket is known as the "commutator", and the other relations can be found by taking cyclic permutations of x, y, z components (i.e. change x to y, y to z, and z to x, repeat).

These commutation relations, and the vector space of generators, fulfill the definition of the Lie algebra formula_71. In summary, a Lie algebra is defined as a vector space "V" over a field of numbers, and with a binary operation [ , ] (called a Lie bracket in this context) on the elements of the vector space, satisfying the axioms of bilinearity, alternatization, and the Jacobi identity. Here the operation [ , ] is the commutator which satisfies all of these axioms, the vector space is the set of Lorentz generators "V" as given previously, and the field is the set of real numbers.

Linking terminology used in mathematics and physics: A group generator is any element of the Lie algebra. A group parameter is a component of a coordinate vector representing an arbitrary element of the Lie algebra with respect to some basis. A basis, then, is a set of generators being a basis of the Lie algebra in the usual vector space sense.

The exponential map from the Lie algebra to the Lie group,

provides a one-to-one correspondence between small enough neighborhoods of the origin of the Lie algebra and neighborhoods of the identity element of the Lie group. It the case of the Lorentz group, the exponential map is just the matrix exponential. Globally, the exponential map is not one-to-one, but in the case of the Lorentz group, it is surjective (onto). Hence any group element in the connected component of the identity can be expressed as an exponential of an element of the Lie algebra.

Lorentz transformations also include parity inversion

which negates all the spatial coordinates only, and time reversal

which negates the time coordinate only, because these transformations leave the spacetime interval invariant. Here is the 3d identity matrix. These are both symmetric, they are their own inverses (see involution (mathematics)), and each have determinant −1. This latter property makes them improper transformations.

If is a proper orthochronous Lorentz transformation, then is improper antichronous, is improper orthochronous, and is proper antichronous.

Two other spacetime symmetries have not been accounted for. For the spacetime interval to be invariant, it can be shown that it is necessary and sufficient for the coordinate transformation to be of the form

where "C" is a constant column containing translations in time and space. If "C" ≠ 0, this is an inhomogeneous Lorentz transformation or Poincaré transformation. If "C" = 0, this is a homogeneous Lorentz transformation. Poincaré transformations are not dealt further in this article.

Writing the general matrix transformation of coordinates as the matrix equation

allows the transformation of other physical quantities that cannot be expressed as four-vectors; e.g., tensors or spinors of any order in 4d spacetime, to be defined. In the corresponding tensor index notation, the above matrix expression is

where lower and upper indices label covariant and contravariant components respectively, and the summation convention is applied. It is a standard convention to use Greek indices that take the value 0 for time components, and 1, 2, 3 for space components, while Latin indices simply take the values 1, 2, 3, for spatial components. Note that the first index (reading left to right) corresponds in the matrix notation to a "row index". The second index corresponds to the column index.

The transformation matrix is universal for all four-vectors, not just 4-dimensional spacetime coordinates. If is any four-vector, then in tensor index notation

Alternatively, one writes

in which the primed indices denote the indices of A in the primed frame. This notation cuts risk of exhausting the Greek alphabet roughly in half.

For a general -component object one may write

where is the appropriate representation of the Lorentz group, an matrix for every . In this case, the indices should "not" be thought of as spacetime indices (sometimes called Lorentz indices), and they run from to . E.g., if is a bispinor, then the indices are called "Dirac indices".

There are also vector quantities with covariant indices. They are generally obtained from their corresponding objects with contravariant indices by the operation of "lowering an index"; e.g.,

where is the metric tensor. (The linked article also provides more information about what the operation of raising and lowering indices really is mathematically.) The inverse of this transformation is given by

where, when viewed as matrices, is the inverse of . As it happens, . This is referred to as "raising an index". To transform a covariant vector , first raise its index, then transform it according to the same rule as for contravariant -vectors, then finally lower the index;

But

I. e., it is the -component of the "inverse" Lorentz transformation. One defines (as a matter of notation),

and may in this notation write

Now for a subtlety. The implied summation on the right hand side of

is running over "a row index" of the matrix representing . Thus, in terms of matrices, this transformation should be thought of as the "inverse transpose" of acting on the column vector . That is, in pure matrix notation,

This means exactly that covariant vectors (thought of as column matrices) transform according to the dual representation of the standard representation of the Lorentz group. This notion generalizes to general representations, simply replace with .

If and are linear operators on vector spaces and , then a linear operator may be defined on the tensor product of and , denoted according to

From this it is immediately clear that if and are a four-vectors in , then transforms as

The second step uses the bilinearity of the tensor product and the last step defines a 2-tensor on component form, or rather, it just renames the tensor .

These observations generalize in an obvious way to more factors, and using the fact that a general tensor on a vector space can be written as a sum of a coefficient (component!) times tensor products of basis vectors and basis covectors, one arrives at the transformation law for any tensor quantity . It is given by

_\mu {\Lambda^{\beta'}}_\nu \cdots {\Lambda^{\zeta'}}_\rho
</math>               

where is defined above. This form can generally be reduced to the form for general -component objects given above with a single matrix () operating on column vectors. This latter form is sometimes preferred; e.g., for the electromagnetic field tensor.

Lorentz transformations can also be used to illustrate that the magnetic field and electric field are simply different aspects of the same force — the electromagnetic force, as a consequence of relative motion between electric charges and observers. The fact that the electromagnetic field shows relativistic effects becomes clear by carrying out a simple thought experiment.

The electric and magnetic fields transform differently from space and time, but exactly the same way as relativistic angular momentum and the boost vector.

The electromagnetic field strength tensor is given by

in SI units. In relativity, the Gaussian system of units is often preferred over SI units, even in texts whose main choice of units is SI units, because in it the electric field and the magnetic induction have the same units making the appearance of the electromagnetic field tensor more natural. Consider a Lorentz boost in the -direction. It is given by

where the field tensor is displayed side by side for easiest possible reference in the manipulations below.

The general transformation law becomes

For the magnetic field one obtains

For the electric field results

Here, is used. These results can be summarized by

and are independent of the metric signature. For SI units, substitute . refer to this last form as the view as opposed to the "geometric view" represented by the tensor expression
and make a strong point of the ease with which results that are difficult to achieve using the view can be obtained and understood. Only objects that have well defined Lorentz transformation properties (in fact under "any" smooth coordinate transformation) are geometric objects. In the geometric view, the electromagnetic field is a six-dimensional geometric object in "spacetime" as opposed to two interdependent, but separate, 3-vector fields in "space" and "time". The fields (alone) and (alone) do not have well defined Lorentz transformation properties. The mathematical underpinnings are equations and that immediately yield . One should note that the primed and unprimed tensors refer to the "same event in spacetime". Thus the complete equation with spacetime dependence is

Length contraction has an effect on charge density and current density , and time dilation has an effect on the rate of flow of charge (current), so charge and current distributions must transform in a related way under a boost. It turns out they transform exactly like the space-time and energy-momentum four-vectors,

or, in the simpler geometric view,

One says that charge density transforms as the time component of a four-vector. It is a rotational scalar. The current density is a 3-vector.

The Maxwell equations are invariant under Lorentz transformations.

Equation hold unmodified for any representation of the Lorentz group, including the bispinor representation. In one simply replaces all occurrences of by the bispinor representation ,

The above equation could, for instance, be the transformation of a state in Fock space describing two free electrons.

A general "noninteracting" multi-particle state (Fock space state) in quantum field theory transforms according to the rule
where is the Wigner rotation and is the representation of .








</doc>
<doc id="18406" url="https://en.wikipedia.org/wiki?curid=18406" title="Luminiferous aether">
Luminiferous aether

Luminiferous aether or ether ("luminiferous", meaning "light-bearing") was the postulated medium for the propagation of light. It was invoked to explain the ability of the apparently wave-based light to propagate through empty space, something that waves should not be able to do. The assumption of a spatial plenum of luminiferous aether, rather than a spatial vacuum, provided the theoretical medium that was required by wave theories of light.

The aether hypothesis was the topic of considerable debate throughout its history, as it required the existence of an invisible and infinite material with no interaction with physical objects. As the nature of light was explored, especially in the 19th century, the physical qualities required of an aether became increasingly contradictory. By the late 1800s, the existence of the aether was being questioned, although there was no physical theory to replace it.

The negative outcome of the Michelson–Morley experiment (1887) suggested that the aether did not exist, a finding that was confirmed in subsequent experiments through the 1920s. This led to considerable theoretical work to explain the propagation of light without an aether. A major breakthrough was the theory of relativity, which could explain why the experiment failed to see aether, but was more broadly interpreted to suggest that it was not needed. The Michelson-Morley experiment, along with the blackbody radiator and photoelectric effect, was a key experiment in the development of modern physics, which includes both relativity and quantum theory, the latter of which explains the particle-like nature of light.

In the 17th century, Robert Boyle was a proponent of an aether hypothesis. According to Boyle, the aether consists of subtle particles, one sort of which explains the absence of vacuum and the mechanical interactions between bodies, and the other sort of which explains phenomena such as magnetism (and possibly gravity) that are, otherwise, inexplicable on the basis of purely mechanical interactions of macroscopic bodies, "though in the ether of the ancients there was nothing taken notice of but a diffused and very subtle substance; yet we are at present content to allow that there is always in the air a swarm of steams moving in a determinate course between the north pole and the south".

Christiaan Huygens's "Treatise on Light" (1690) hypothesized that light is a wave propagating through an aether. He and Isaac Newton could only envision light waves as being longitudinal, propagating like sound and other mechanical waves in fluids. However, longitudinal waves necessarily have only one form for a given propagation direction, rather than two polarizations like transverse wave. Thus, longitudinal waves can not explain birefringence, in which two polarizations of light are refracted differently by a crystal. In addition, Newton rejected light as waves in a medium because such a medium would have to extend everywhere in space, and would thereby "disturb and retard the Motions of those great Bodies" (the planets and comets) and thus "as it is of no use, and hinders the Operation of Nature, and makes her languish, so there is no evidence for its Existence, and therefore it ought to be rejected".

Isaac Newton contended that light is made up of numerous small particles. This can explain such features as light's ability to travel in straight lines and reflect off surfaces. Newton imagined that light particles as non-spherical "corpuscles", with different "sides" that give rise to birefringence. But the particle theory of light can not satisfactorily explain refraction and diffraction. To explain refraction, Newton's Third Book of "Opticks" (1st ed. 1704, 4th ed. 1730) postulated an "aethereal medium" transmitting vibrations faster than light, by which light, when overtaken, is put into "Fits of easy Reflexion and easy Transmission", which caused refraction and diffraction. Newton believed that these vibrations were related to heat radiation:

Is not the Heat of the warm Room convey'd through the vacuum by the Vibrations of a much subtiler Medium than Air, which after the Air was drawn out remained in the Vacuum? And is not this Medium the same with that Medium by which Light is refracted and reflected, and by whose Vibrations Light communicates Heat to Bodies, and is put into Fits of easy Reflexion and easy Transmission?

In contrast to the modern understanding that heat radiation and light are both electromagnetic radiation, Newton viewed heat and light as two different phenomena. He believed heat vibrations to be excited "when a Ray of Light falls upon the Surface of any pellucid Body". He wrote, "I do not know what this Aether is", but that if it consists of particles then they must be exceedingly smaller than those of Air, or even than those of Light: The exceeding smallness of its Particles may contribute to the greatness of the force by which those Particles may recede from one another, and thereby make that Medium exceedingly more rare and elastic than Air, and by consequence exceedingly less able to resist the motions of Projectiles, and exceedingly more able to press upon gross Bodies, by endeavoring to expand itself.

In 1720, James Bradley carried out a series of experiments attempting to measure stellar parallax by taking measurements of stars at different times of the year. As the Earth moves around the sun, the apparent angle to a given distant spot changes. By measuring those angles the distance to the star can be calculated based on the known orbital circumference of the Earth around the sun. He failed to detect any parallax, thereby placing a lower limit on the distance to stars.

During these experiments, Bradley also discovered a related effect; the apparent positions of the stars did change over the year, but not as expected. Instead of the apparent angle being maximized when the Earth was at either end of its orbit with respect to the star, the angle was maximized when the Earth was at its fastest sideways velocity with respect to the star. This effect is now known as stellar aberration.

Bradley explained this effect in the context of Newton's corpuscular theory of light, by showing that the aberration angle was given by simple vector addition of the Earth's orbital velocity and the velocity of the corpuscles of light, just as vertically falling raindrops strike a moving object at an angle. Knowing the Earth's velocity and the aberration angle, this enabled him to estimate the speed of light.

Explaining stellar aberration in the context of an aether-based theory of light was regarded as more problematic. As the aberration relied on relative velocities, and the measured velocity was dependent on the motion of the Earth, the aether had to be remaining stationary with respect to the star as the Earth moved through it. This meant that the Earth could travel through the aether, a physical medium, with no apparent effect – precisely the problem that led Newton to reject a wave model in the first place.

A century later, Thomas Young and Augustin-Jean Fresnel revived the wave theory of light when they pointed out that light could be a transverse wave rather than a longitudinal wave; the polarization of a transverse wave (like Newton's "sides" of light) could explain birefringence, and in the wake of a series of experiments on diffraction the particle model of Newton was finally abandoned. Physicists assumed, moreover, that like mechanical waves, light waves required a medium for propagation, and thus required Huygens's idea of an aether "gas" permeating all space.

However, a transverse wave apparently required the propagating medium to behave as a solid, as opposed to a gas or fluid. The idea of a solid that did not interact with other matter seemed a bit odd, and Augustin-Louis Cauchy suggested that perhaps there was some sort of "dragging", or "entrainment", but this made the aberration measurements difficult to understand. He also suggested that the "absence" of longitudinal waves suggested that the aether had negative compressibility. George Green pointed out that such a fluid would be unstable. George Gabriel Stokes became a champion of the entrainment interpretation, developing a model in which the aether might be (by analogy with pine pitch) rigid at very high frequencies and fluid at lower speeds. Thus the Earth could move through it fairly freely, but it would be rigid enough to support light.

In 1856, Wilhelm Eduard Weber and Rudolf Kohlrausch measured the numerical value of the ratio of the electrostatic unit of charge to the electromagnetic unit of charge. They found that the ratio equals the product of the speed of light and the square root of two. The following year, Gustav Kirchhoff wrote a paper in which he showed that the speed of a signal along an electric wire was equal to the speed of light. These are the first recorded historical links between the speed of light and electromagnetic phenomena.

James Clerk Maxwell began working on Michael Faraday's lines of force. In his 1861 paper "" he modelled these magnetic lines of force using a sea of molecular vortices that he considered to be partly made of aether and partly made of ordinary matter. He derived expressions for the dielectric constant and the magnetic permeability in terms of the transverse elasticity and the density of this elastic medium. He then equated the ratio of the dielectric constant to the magnetic permeability with a suitably adapted version of Weber and Kohlrausch's result of 1856, and he substituted this result into Newton's equation for the speed of sound. On obtaining a value that was close to the speed of light as measured by Hippolyte Fizeau, Maxwell concluded that light consists in undulations of the same medium that is the cause of electric and magnetic phenomena.

Maxwell had, however, expressed some uncertainties surrounding the precise nature of his molecular vortices and so he began to embark on a purely dynamical approach to the problem. He wrote another paper in 1864, entitled "A Dynamical Theory of the Electromagnetic Field", in which the details of the luminiferous medium were less explicit. Although Maxwell did not explicitly mention the sea of molecular vortices, his derivation of Ampère's circuital law was carried over from the 1861 paper and he used a dynamical approach involving rotational motion within the electromagnetic field which he likened to the action of flywheels. Using this approach to justify the electromotive force equation (the precursor of the Lorentz force equation), he derived a wave equation from a set of eight equations which appeared in the paper and which included the electromotive force equation and Ampère's circuital law. Maxwell once again used the experimental results of Weber and Kohlrausch to show that this wave equation represented an electromagnetic wave that propagates at the speed of light, hence supporting the view that light is a form of electromagnetic radiation.

The apparent need for a propagation medium for such Hertzian waves can be seen by the fact that they consist of orthogonal electric (E) and magnetic (B or H) waves. The E waves consist of undulating dipolar electric fields, and all such dipoles appeared to require separated and opposite electric charges. Electric charge is an inextricable property of matter, so it appeared that some form of matter was required to provide the alternating current that would seem to have to exist at any point along the propagation path of the wave. Propagation of waves in a true vacuum would imply the existence of electric fields without associated electric charge, or of electric charge without associated matter. Albeit compatible with Maxwell's equations, electromagnetic induction of electric fields could not be demonstrated in vacuum, because all methods of detecting electric fields required electrically charged matter.

In addition, Maxwell's equations required that all electromagnetic waves in vacuum propagate at a fixed speed, "c". As this can only occur in one reference frame in Newtonian physics (see Galilean relativity), the aether was hypothesized as the absolute and unique frame of reference in which Maxwell's equations hold. That is, the aether must be "still" universally, otherwise "c" would vary along with any variations that might occur in its supportive medium. Maxwell himself proposed several mechanical models of aether based on wheels and gears, and George Francis FitzGerald even constructed a working model of one of them. These models had to agree with the fact that the electromagnetic waves are transverse but never longitudinal.

By this point the mechanical qualities of the aether had become more and more magical: it had to be a fluid in order to fill space, but one that was millions of times more rigid than steel in order to support the high frequencies of light waves. It also had to be massless and without viscosity, otherwise it would visibly affect the orbits of planets. Additionally it appeared it had to be completely transparent, non-dispersive, incompressible, and continuous at a very small scale. Maxwell wrote in "Encyclopædia Britannica":

Aethers were invented for the planets to swim in, to constitute electric atmospheres and magnetic effluvia, to convey sensations from one part of our bodies to another, and so on, until all space had been filled three or four times over with aethers. ... The only aether which has survived is that which was invented by Huygens to explain the propagation of light.

Contemporary scientists were aware of the problems, but aether theory was so entrenched in physical law by this point that it was simply assumed to exist. In 1908 Oliver Lodge gave a speech on behalf of Lord Rayleigh to the Royal Institution on this topic, in which he outlined its physical properties, and then attempted to offer reasons why they were not impossible. Nevertheless, he was also aware of the criticisms, and quoted Lord Salisbury as saying that "aether is little more than a nominative case of the verb "to undulate"". Others criticized it as an "English invention", although Rayleigh jokingly stated it was actually an invention of the Royal Institution.

By the early 20th century, aether theory was in trouble. A series of increasingly complex experiments had been carried out in the late 19th century to try to detect the motion of the Earth through the aether, and had failed to do so. A range of proposed aether-dragging theories could explain the null result but these were more complex, and tended to use arbitrary-looking coefficients and physical assumptions. Lorentz and FitzGerald offered within the framework of Lorentz ether theory a more elegant solution to how the motion of an absolute aether could be undetectable (length contraction), but if their equations were correct, the new special theory of relativity (1905) could generate the same mathematics without referring to an aether at all. Aether fell to Occam's Razor.

The two most important models, which were aimed to describe the relative motion of the Earth and aether, were Augustin-Jean Fresnel's (1818) model of the (nearly) stationary aether including a partial aether drag determined by Fresnel's dragging coefficient,
and George Gabriel Stokes' (1844)
model of complete aether drag. The latter theory was not considered as correct, since it was not compatible with the aberration of light, and the auxiliary hypotheses developed to explain this problem were not convincing. Also, subsequent experiments as the Sagnac effect (1913) also showed that this model is untenable. However, the most important experiment supporting Fresnel's theory was Fizeau's 1851 experimental confirmation of Fresnel's 1818 prediction that a medium with refractive index "n" moving with a velocity "v" would increase the speed of light travelling through the medium in the same direction as "v" from "c"/"n" to:

That is, movement adds only a fraction of the medium's velocity to the light (predicted by Fresnel in order to make Snell's law work in all frames of reference, consistent with stellar aberration). This was initially interpreted to mean that the medium drags the aether along, with a "portion" of the medium's velocity, but that understanding became very problematic after Wilhelm Veltmann demonstrated that the index "n" in Fresnel's formula depended upon the wavelength of light, so that the aether could not be moving at a wavelength-independent speed. This implied that there must be a separate aether for each of the infinitely many frequencies.

The key difficulty with Fresnel's aether hypothesis arose from the juxtaposition of the two well-established theories of Newtonian dynamics and Maxwell's electromagnetism. Under a Galilean transformation the equations of Newtonian dynamics are invariant, whereas those of electromagnetism are not. Basically this means that while physics should remain the same in non-accelerated experiments, light would not follow the same rules because it is travelling in the universal "aether frame". Some effect caused by this difference should be detectable.

A simple example concerns the model on which aether was originally built: sound. The speed of propagation for mechanical waves, the speed of sound, is defined by the mechanical properties of the medium. Sound travels 4.3 times faster in water than in air. This explains why a person hearing an explosion underwater and quickly surfacing can hear it again as the slower travelling sound arrives through the air. Similarly, a traveller on an airliner can still carry on a conversation with another traveller because the sound of words is travelling along with the air inside the aircraft. This effect is basic to all Newtonian dynamics, which says that everything from sound to the trajectory of a thrown baseball should all remain the same in the aircraft flying (at least at a constant speed) as if still sitting on the ground. This is the basis of the Galilean transformation, and the concept of frame of reference.

But the same was not supposed to be true for light, since Maxwell's mathematics demanded a single universal speed for the propagation of light, based, not on local conditions, but on two measured properties, the permittivity and permeability of free space, that were assumed to be the same throughout the universe. If these numbers did change, there should be noticeable effects in the sky; stars in different directions would have different colours, for instance.

Thus at any point there should be one special coordinate system, "at rest relative to the aether". Maxwell noted in the late 1870s that detecting motion relative to this aether should be easy enough—light travelling along with the motion of the Earth would have a different speed than light travelling backward, as they would both be moving against the unmoving aether. Even if the aether had an overall universal flow, changes in position during the day/night cycle, or over the span of seasons, should allow the drift to be detected.

Although the aether is almost stationary according to Fresnel, his theory predicts a positive outcome of aether drift experiments only to "second" order in formula_2, because Fresnel's dragging coefficient would cause a negative outcome of all optical experiments capable of measuring effects to "first" order in formula_2. This was confirmed by the following first-order experiments, which all gave negative results. The following list is based on the description of Wilhelm Wien (1898), with changes and additional experiments according to the descriptions of Edmund Taylor Whittaker (1910) and Jakob Laub (1910):


Besides those optical experiments, also electrodynamic first-order experiments were conducted, which should have led to positive results according to Fresnel. However, Hendrik Antoon Lorentz (1895) modified Fresnel's theory and showed that those experiments can be explained by a stationary aether as well:


While the "first"-order experiments could be explained by a modified stationary aether, more precise "second"-order experiments were expected to give positive results, however, no such results could be found.

The famous Michelson–Morley experiment compared the source light with itself after being sent in different directions, looking for changes in phase in a manner that could be measured with extremely high accuracy. In this experiment, their goal was to determine the velocity of the Earth through the aether. The publication of their result in 1887, the null result, was the first clear demonstration that something was seriously wrong with the aether hypothesis (Michelson's first experiment in 1881 was not entirely conclusive). In this case the MM experiment yielded a shift of the fringing pattern of about 0.01 of a fringe, corresponding to a small velocity. However, it was incompatible with the expected aether wind effect due to the Earth's (seasonally varying) velocity which would have required a shift of 0.4 of a fringe, and the error was small enough that the value may have indeed been zero. Therefore, the null hypothesis, the hypothesis that there was no aether wind, could not be rejected. More modern experiments have since reduced the possible value to a number very close to zero, about 10.

A series of experiments using similar but increasingly sophisticated apparatuses all returned the null result as well. Conceptually different experiments that also attempted to detect the motion of the aether were the Trouton–Noble experiment (1903), whose objective was to detect torsion effects caused by electrostatic fields, and the experiments of Rayleigh and Brace (1902, 1904), to detect double refraction in various media. However, all of them obtained a null result, like Michelson–Morley (MM) previously did.

These "aether-wind" experiments led to a flurry of efforts to "save" aether by assigning to it ever more complex properties, while only few scientists, like Emil Cohn or Alfred Bucherer, considered the possibility of the abandonment of the aether hypothesis. Of particular interest was the possibility of "aether entrainment" or "aether drag", which would lower the magnitude of the measurement, perhaps enough to explain the results of the Michelson-Morley experiment. However, as noted earlier, aether dragging already had problems of its own, notably aberration. In addition, the interference experiments of Lodge (1893, 1897) and Ludwig Zehnder (1895), aimed to show whether the aether is dragged by various, rotating masses, showed no aether drag. A more precise measurement was made in the Hammar experiment (1935), which ran a complete MM experiment with one of the "legs" placed between two massive lead blocks. If the aether was dragged by mass then this experiment would have been able to detect the drag caused by the lead, but again the null result was achieved. The theory was again modified, this time to suggest that the entrainment only worked for very large masses or those masses with large magnetic fields. This too was shown to be incorrect by the Michelson–Gale–Pearson experiment, which detected the Sagnac effect due to Earth's rotation (see Aether drag hypothesis).

Another, completely different attempt to save "absolute" aether was made in the Lorentz–FitzGerald contraction hypothesis, which posited that "everything" was affected by travel through the aether. In this theory the reason the Michelson–Morley experiment "failed" was that the apparatus contracted in length in the direction of travel. That is, the light was being affected in the "natural" manner by its travel through the aether as predicted, but so was the apparatus itself, cancelling out any difference when measured. FitzGerald had inferred this hypothesis from a paper by Oliver Heaviside. Without referral to an aether, this physical interpretation of relativistic effects was shared by Kennedy and Thorndike in 1932 as they concluded that the interferometer's arm contracts and also the frequency of its light source "very nearly" varies in the way required by relativity.

Similarly the Sagnac effect, observed by G. Sagnac in 1913, was immediately seen to be fully consistent with special relativity. In fact, the Michelson-Gale-Pearson experiment in 1925 was proposed specifically as a test to confirm the relativity theory, although it was also recognized that such tests, which merely measure absolute rotation, are also consistent with non-relativistic theories.

During the 1920s, the experiments pioneered by Michelson were repeated by Dayton Miller, who publicly proclaimed positive results on several occasions, although they were not large enough to be consistent with any known aether theory. However, other researchers were unable to duplicate Miller's claimed results. Over the years the experimental accuracy of such measurements has been raised by many orders of magnitude, and no trace of any violations of Lorentz invariance has been seen. (A later re-analysis of Miller's results concluded that he had underestimated the variations due to temperature.)

Since the Miller experiment and its unclear results there have been many more experimental attempts to detect the aether. Many experimenters have claimed positive results. These results have not gained much attention from mainstream science, since they contradict a large quantity of high-precision measurements, all the results of which were consistent with special relativity.

Between 1892 and 1904, Hendrik Lorentz developed an electron-aether theory, in which he introduced a strict separation between matter (electrons) and aether. In his model the aether is completely motionless, and won't be set in motion in the neighborhood of ponderable matter. Contrary to earlier electron models, the electromagnetic field of the aether appears as a mediator between the electrons, and changes in this field cannot propagate faster than the speed of light. A fundamental concept of Lorentz's theory in 1895 was the "theorem of corresponding states" for terms of order v/c. This theorem states that an observer moving relative to the aether makes the same observations as a resting observer, after a suitable change of variables. Lorentz noticed that it was necessary to change the space-time variables when changing frames and introduced concepts like physical length contraction (1892) to explain the Michelson–Morley experiment, and the mathematical concept of local time (1895) to explain the aberration of light and the Fizeau experiment. This resulted in the formulation of the so-called Lorentz transformation by Joseph Larmor (1897, 1900) and Lorentz (1899, 1904), whereby (it was noted by Larmor) the complete formulation of local time is accompanied by some sort of time dilation of electrons moving in the aether. As Lorentz later noted (1921, 1928), he considered the time indicated by clocks resting in the aether as "true" time, while local time was seen by him as a heuristic working hypothesis and a mathematical artifice. Therefore, Lorentz's theorem is seen by modern authors as being a mathematical transformation from a "real" system resting in the aether into a "fictitious" system in motion.

The work of Lorentz was mathematically perfected by Henri Poincaré, who formulated on many occasions the Principle of Relativity and tried to harmonize it with electrodynamics. He declared simultaneity only a convenient convention which depends on the speed of light, whereby the constancy of the speed of light would be a useful postulate for making the laws of nature as simple as possible. In 1900 and 1904 he physically interpreted Lorentz's local time as the result of clock synchronization by light signals. In June and July 1905 he declared the relativity principle a general law of nature, including gravitation. He corrected some mistakes of Lorentz and proved the Lorentz covariance of the electromagnetic equations. However, he used the notion of an aether as a perfectly undetectable medium and distinguished between apparent and real time, so most historians of science argue that he failed to invent special relativity.

Aether theory was dealt another blow when the Galilean transformation and Newtonian dynamics were both modified by Albert Einstein's special theory of relativity, giving the mathematics of Lorentzian electrodynamics a new, "non-aether" context. Unlike most major shifts in scientific thought, special relativity was adopted by the scientific community remarkably quickly, consistent with Einstein's later comment that the laws of physics described by the Special Theory were "ripe for discovery" in 1905. Max Planck's early advocacy of the special theory, along with the elegant formulation given to it by Hermann Minkowski, contributed much to the rapid acceptance of special relativity among working scientists.

Einstein based his theory on Lorentz's earlier work. Instead of suggesting that the mechanical properties of objects changed with their constant-velocity motion through an undetectable aether, Einstein proposed to deduce the characteristics that any successful theory must possess in order to be consistent with the most basic and firmly established principles, independent of the existence of a hypothetical aether. He found that the Lorentz transformation must transcend its connection with Maxwell's equations, and must represent the fundamental relations between the space and time coordinates of inertial frames of reference. In this way he demonstrated that the laws of physics remained invariant as they had with the Galilean transformation, but that light was now invariant as well.

With the development of the special theory of relativity, the need to account for a single universal frame of reference had disappeared – and acceptance of the 19th-century theory of a luminiferous aether disappeared with it. For Einstein, the Lorentz transformation implied a conceptual change: that the concept of position in space or time was not absolute, but could differ depending on the observer's location and velocity.

Moreover, in another paper published the same month in 1905, Einstein made several observations on a then-thorny problem, the photoelectric effect. In this work he demonstrated that light can be considered as particles that have a "wave-like nature". Particles obviously do not need a medium to travel, and thus, neither did light. This was the first step that would lead to the full development of quantum mechanics, in which the wave-like nature "and" the particle-like nature of light are both considered as valid descriptions of light. A summary of Einstein's thinking about the aether hypothesis, relativity and light quanta may be found in his 1909 (originally German) lecture "The Development of Our Views on the Composition and Essence of Radiation".

Lorentz on his side continued to use the aether hypothesis. In his lectures of around 1911, he pointed out that what "the theory of relativity has to say ... can be carried out independently of what one thinks of the aether and the time". He commented that "whether there is an aether or not, electromagnetic fields certainly exist, and so also does the energy of the electrical oscillations" so that, "if we do not like the name of 'aether', we must use another word as a peg to hang all these things upon". He concluded that "one cannot deny the bearer of these concepts a certain substantiality".

In later years there have been a few individuals who advocated a neo-Lorentzian approach to physics, which is Lorentzian in the sense of positing an absolute true state of rest that is undetectable and which plays no role in the predictions of the theory. (No violations of Lorentz covariance have ever been detected, despite strenuous efforts.) Hence these theories resemble the 19th century aether theories in name only. For example, the founder of quantum field theory, Paul Dirac, stated in 1951 in an article in Nature, titled "Is there an Aether?" that "we are rather forced to have an aether". However, Dirac never formulated a complete theory, and so his speculations found no acceptance by the scientific community.

When Einstein was still a student in the Zurich Polytechnic in 1900, he was very interested in the idea of aether. His initial proposal of research thesis was to do an experiment to measure how fast the Earth was moving through the aether. "The velocity of a wave is proportional to the square root of the elastic forces which cause [its] propagation, and inversely proportional to the mass of the aether moved by these forces."

In 1916, after Einstein completed his foundational work on general relativity, Lorentz wrote a letter to him in which he speculated that within general relativity the aether was re-introduced. In his response Einstein wrote that one can actually speak about a "new aether", but one may not speak of motion in relation to that aether. This was further elaborated by Einstein in some semi-popular articles (1918, 1920, 1924, 1930).

In 1918 Einstein publicly alluded to that new definition for the first time. Then, in the early 1920s, in a lecture which he was invited to give at Lorentz's university in Leiden, Einstein sought to reconcile the theory of relativity with Lorentzian aether. In this lecture Einstein stressed that special relativity took away the last mechanical property of the aether: immobility. However, he continued that special relativity does not necessarily rule out the aether, because the latter can be used to give physical reality to acceleration and rotation. This concept was fully elaborated within general relativity, in which physical properties (which are partially determined by matter) are attributed to space, but no substance or state of motion can be attributed to that "aether" (by which he meant curved space-time).

In another paper of 1924, named "Concerning the Aether", Einstein argued that Newton's absolute space, in which acceleration is absolute, is the "Aether of Mechanics". And within the electromagnetic theory of Maxwell and Lorentz one can speak of the "Aether of Electrodynamics", in which the aether possesses an absolute state of motion. As regards special relativity, also in this theory acceleration is absolute as in Newton's mechanics. However, the difference from the electromagnetic aether of Maxwell and Lorentz lies in the fact, that "because it was no longer possible to speak, in any absolute sense, of simultaneous states at different locations in the aether, the aether became, as it were, four-dimensional since there was no objective way of ordering its states by time alone". Now the "aether of special relativity" is still "absolute", because matter is affected by the properties of the aether, but the aether is not affected by the presence of matter. This asymmetry was solved within general relativity. Einstein explained that the "aether of general relativity" is not absolute, because matter is influenced by the aether, just as matter influences the structure of the aether.

The only similarity of this relativistic aether concept with the classical aether models lies in the presence of physical properties in space, which can be identified through geodesics. As historians such as John Stachel argue, Einstein's views on the "new aether" are not in conflict with his abandonment of the aether in 1905. As Einstein himself pointed out, no "substance" and no state of motion can be attributed to that new aether. Einstein's use of the word "aether" found little support in the scientific community, and played no role in the continuing development of modern physics.


Footnotes
Citations
 


</doc>
<doc id="18408" url="https://en.wikipedia.org/wiki?curid=18408" title="LAME">
LAME

LAME is a software encoder that converts a digitized WAV audio file into the MP3 audio coding file format. LAME is a free software project that was first released in 1998, and has incorporated many improvements since then, including an improved psychoacoustic model. The LAME encoder vastly outperforms early encoders like L3enc.

LAME is required by some programs. In many programs released as free software (e.g., Audacity), LAME must be linked for MP3 support. This avoided including LAME itself, which used patented techniques, and so required patent licenses in many countries.

The name LAME is a recursive acronym for "LAME Ain't an MP3 Encoder".

Around mid-1998, Mike Cheng created LAME 1.0 as a set of modifications against the "8Hz-MP3" encoder source code. After some quality concerns raised by others, he decided to start again from scratch based on the "dist10" MPEG reference software sources. His goal was only to speed up the dist10 sources, and leave its quality untouched. That branch (a patch against the reference sources) became Lame 2.0. The project quickly became a team project. Mike Cheng eventually left leadership and started working on tooLAME (an MP2 encoder).

Mark Taylor then started pursuing increased quality in addition to better speed, and released version 3.0 featuring gpsycho, a new psychoacoustic model he developed.

A few key improvements, in chronological order:

Like all MP3 encoders, LAME implemented techniques covered by patents owned by the Fraunhofer Society and others. The developers of LAME did not license the technology described by these patents. Distributing compiled binaries of LAME, its libraries, or programs that derive from LAME in countries where those patents have been granted may have constituted infringement, but since April 23, 2017, all of these patents have expired.

The LAME developers stated that, since their code was only released in source code form, it should only be considered as an educational description of an MP3 encoder, and thus did not infringe any patent in itself. They also advised users to obtain relevant patent licenses before including a compiled version of the encoder in a product. Some software was released using this strategy: companies used the LAME library, but obtained patent licenses.

In the course of the 2005 Sony BMG copy protection rootkit scandal, there were reports that the Extended Copy Protection rootkit included on some Sony Compact Discs had portions of the LAME library without complying with the terms of the LGPL.




</doc>
<doc id="18414" url="https://en.wikipedia.org/wiki?curid=18414" title="Leszek Miller">
Leszek Miller

Leszek Cezary Miller ( ; born 3 July 1946) is a Polish political scientist and politician. He has served as a Member of the European Parliament (MEP) since July 2019.

From 1989 to 1990 was a member of the Politburo of the Polish United Workers' Party. From 2001 to 2004 was a Prime Minister of Poland. Miller was the leader of the Democratic Left Alliance from 1999 to 2004 and again from 2011 to 2016. 

Leszek Miller is the great-grandson of Eliasz, son of Mośek and Sura Miller, born in 1840 in Kutno. Eliasz Miller converted from Judaism to Christianity in 1869 in Nieborów.

Leszek Miller was born in Żyrardów, Miller comes from a poor, working-class family: His father was a tailor and his mother a needlewoman. His parents broke up when Miller was six months old. His father, Florian Miller left the family and Leszek has never maintained any contact with him. His mother brought him up in a religious spirit – following her wish, he was even, for some time, an altar server in their church.

Due to hard living conditions, after graduation from a vocational school, 17-year-old Miller got a job in the Textile Linen Plant in Żyrardów, while continuing his education in the evenings at the Vocational Secondary School of Electric Power Engineering. He soon completed his military service on the ORP Bielik submarine.

In 1969, Miller married Aleksandra, three years his junior, in church. They had a son, Leszek Junior ( August 2018), and a granddaughter, Monika.

Miller started his political career as an activist of the Socialist Youth Union, where he held the position of Chairman of the Plant Board, soon becoming a member of the Town Committee. After the military service, in 1969, he joined the Polish United Workers' Party (PZPR), People's Poland's communist party.

Many people were pressured to join PZPR in order to advance in their careers or to pursue higher education. Miller used his affiliation with the Communist party to effectively advance in his studies and professional goals.

In 1973 and 1974, Miller was the Secretary of the PZPR Plant Committee. With the party's recommendation, he started political sciences studies at the party's Higher School of Political Sciences ("Wyższa Szkoła Nauk Społecznych"), graduating in 1977. After graduation, Miller worked at the PZPR Central Committee, supervising the Group, and later on the Department of Youth, Physical Education and Tourism.

In July 1986, Miller was elected as First Secretary of the PZPR Provincial Committee in Skierniewice. In December 1988, he returned to Warsaw, due to his promotion to the position of the Secretary of the PZPR Central Committee. As a representative of the government side, he took part in the session of the historic "Round Table", where, together with Andrzej Celiński, he co-chaired the sub-team for youth issues (the only one that closed the session without signing the agreement). In 1989, he became a member of the PZPR Political Bureau.

After the PZPR was dissolved, Miller became a co-founder of the Social Democracy of the Polish Republic (till March 1993, he was Secretary General, then Deputy Chairman and, from December 1997, the Chairman of that party). In December 1999, at the Founding Congress of the Democratic Left Alliance (SLD), he was elected its Chairman, holding the function continuously until February 2004. In 1997-2001 he was the Chairman of the SLD’s caucus.

In 1989, he ran unsuccessfully for the Senate as a representative of Skierniewice Province. In subsequent elections (1991), Miller was a leader on the election list of the Social Democracy of the Polish Republic in Łódź and, following a considerable success in elections, he won a seat in the Sejm, becoming Chairman of the Parliamentary Group of the Social Democracy of the Polish Republic. In three subsequent elections to the Sejm, he ran each time from Łódź, each time gaining more and more votes (from 50 thousand in 1991 up to 146 thousand in 2001); he held a seat in Parliament until 2005.

Through all that time he remained one of the leading politicians on the left wing. In the early 1990s, together with Mieczysław Rakowski, he was suspected in the case of the so-called "Moscow loan". After revealing that affair in 1991, Włodzimierz Cimoszewicz called Miller to abstain from taking an MP's oath due to accusations laid against him. When Miller was cleared of the charges, Prime Minister Cimoszewicz appointed him later as the Minister in Charge of the Office of the Council of Ministers and in 1997 the Minister of Internal Affairs and Administration in his government. In turn, Cimoszewicz later became the Minister of Foreign Affairs in Miller’s cabinet.

From 1993 to 1996, Miller was the Minister of Labour and Social Policy in the governments of Waldemar Pawlak and Józef Oleksy respectively. In 1996, he was nominated as Senior Minister in charge of the Office of the Council of Ministers. He then got the nickname “The Chancellor”.

Miller played an important role in concluding the case of Colonel Ryszard Kukliński, for which he was severely criticised within his political circle. A similar disapproval was expressed after Miller’s support for the Concordat and the candidature of Leszek Balcerowicz to the position of President of the National Bank of Poland.

During the period of the Solidarity Electoral Action’s government, Miller was in charge of the parliamentary opposition, leading the political fight with the governing party. He was also consolidating the majority of significant left-wing groups around his person. In 1999, he succeeded in establishing one uniform political party – the Democratic Left Alliance – which turned out to be very successful in following elections.

Following the victory of the Left (41% vs. 12% of the subsequent party) in the Parliamentary Election in 2001, on 19 October 2001, President Aleksander Kwaśniewski appointed Miller Prime Minister and obliged to nominate the government. The new government won the parliamentary vote of confidence on 26 October 2001 (306:140 votes with one abstention). The 16-person cabinet of Prime Minister Miller has been the smallest government of the Polish Republic so far.

Miller’s government faced a difficult economic situation in Poland, including an unemployment rate above 18%, a high level of public debt, and economic stagnation. At the end of Miller’s term, economic growth exceeded 6%; still, it was too slow to reduce the unemployment rate. During his term, the unpopular program of cuts in public expenses was implemented, together with a hardly successful reform of health care financing. The reforms of the tax system and of the Social Insurance Institution were continued, and the attempt to settle the mass-media market failed. Taxes were significantly lowered – to 19% for companies and for persons running business activity – and the act of freedom in business activity was voted through. A radical, structural reform of secret services was implemented (the State Security Office was dissolved and replaced by the Internal Security Agency and the Intelligence Agency).

Simultaneously, institutional and legal adjustments were continued, resulting from the accession to the European Union. The Accession conditions were negotiated, being the main strategic goal of Miller’s cabinet. On 13 December 2002, at the summit in Copenhagen (Denmark), Prime Minister Leszek Miller completed the negotiations with the European Union. On 16 April 2003 in Athens, Miller, together with Cimoszewicz, signed the Accession Treaty, bringing Poland into the European Union. Miller’s government, in collaboration with various political and social forces, organized the accession referendum with a successful outcome. On 7 and 8 June 2003, 77.45% of the referendum participants voted in favor of Poland’s accession to the European Union. The referendum turn-out reached 58.85%.

Miller’s government, together with President Kwaśniewski, made a decision (March 2003) to join the international coalition and deploy Polish troops to Iraq, targeting at overthrowing Saddam Hussein’s government. Miller was also a co-signatory of "the letter of 8", signed by eight European prime ministers, supporting the US position on Iraq. Already in 2002, Miller gave permission to the U.S. government to run a secret CIA prison at Stare Kiejkuty military training center, three hours north of Warsaw. Years later he is facing accusations of acting anti-constitutionally by having tolerated the imprisonment and torture of prisoners.
On 4 December 2003, Leszek Miller suffered injuries in a helicopter crash near Warsaw.

At the end of its term of office, Miller’s government had the lowest public support of any government since 1989. It was mainly caused by the continuing high unemployment rate, corruption scandals, with Rywingate on top, and by the attempt of fulfilling the plan of reducing social spending (the Hausner’s plan). In result of criticism in his own party, the Democratic Left Alliance, in February 2004, Miller resigned from chairing the party. Miller was criticized for an excessively liberal approach and for stressing the role of free-market mechanisms in economy. He was reproached for his acceptance of a flat tax, which ran counter to the left-wing doctrine. He was also identified with the “chieftain-like style” of leadership. On 26 March 2004, following the decision of the Speaker of the Parliament, Marek Borowski, to found a new dissenting party, the Social Democracy of Poland, Miller decided to resign from the position of Prime Minister on 2 May 2004, a day after Poland’s accession to the EU. On 1 May 2004, together with President Kwaśniewski, he was in Dublin, taking part in the Grand Ceremony of the accession of 10 states, including Poland, to the European Union.

In 2005, despite the support of the Łódź Branch of the Democratic Left Alliance, Miller was not registered on the election list to the Parliament. At the same time, he was offered to run for Senate but refused. Retirement of the old activists was presented in media as “inflow of new blood into the Democratic Left Alliance”. After the election, Miller became active in journalism, writing mainly for the “Wprost” weekly on liberal economic concepts and current political issues. In the first half of 2005, he stayed at the Woodrow Wilson International Center for Scholars in Washington, D.C., implementing a research project: “Status of the new Poland in the Eastern Europe’s space”.

In September 2007, the former Polish Prime Minister Leszek Miller become affiliated with Samoobrona, when he decided to run for the Sejm from their lists.



 


</doc>
<doc id="18420" url="https://en.wikipedia.org/wiki?curid=18420" title="Basis (linear algebra)">
Basis (linear algebra)

In mathematics, a set of elements (vectors) in a vector space is called a basis, if every element of may be written in a unique way as a (finite) linear combination of elements of . The coefficients of this linear combination are referred to as components or coordinates on of the vector. The elements of a basis are called .

Equivalently is a basis if its elements are linearly independent and every element of is a linear combination of elements of . In more general terms, a basis is a linearly independent spanning set.

A vector space can have several bases; however all the bases have the same number of elements, called the dimension of the vector space.

A basis of a vector space over a field (such as the real numbers or the complex numbers ) is a linearly independent subset of that spans .
This means that a subset of is a basis if it satisfies the two following conditions:

The scalars formula_8 are called the coordinates of the vector with respect to the basis , and by the first property they are uniquely determined.

A vector space that has a finite basis is called finite-dimensional. In this case, the finite subset can be taken as itself to check for linear independence in the above definition.

It is often convenient or even necessary to have an ordering on the basis vectors, e.g. for discussing orientation, or when one considers the scalar coefficients of a vector with respect to a basis, without referring explicitly to the basis elements. In this case, the ordering is necessary for associating each coefficient to the corresponding basis element. This ordering can be done by numbering the basis elements. For example, when dealing with ("m", "n")-matrices, the element (in the th row and th column) can be referred to the th element of a basis consisting of the ("m", "n")-unit-matrices (varying column-indices before row-indices). For emphasizing that an order has been chosen, one speaks of an ordered basis, which is therefore not simply an unstructured set, but e.g. a sequence, or an indexed family, or similar; see "Ordered bases and coordinates" below.


Many properties of finite bases result from the Steinitz exchange lemma, which states that, for any vector space , given a finite spanning set and a linearly independent set of elements of , one may replace well-chosen elements of by the elements of to get a spanning set containing , having its other elements in , and having the same number of elements as .

Most properties resulting from the Steinitz exchange lemma remain true when there is no finite spanning set, but their proofs in the infinite case generally require the axiom of choice or a weaker form of it, such as the ultrafilter lemma.

If is a vector space over a field , then:

If is a vector space of dimension , then:

Let be a vector space of finite dimension over a field , and 
be a basis of . By definition of a basis, every in may be written, in a unique way, as
where the coefficients formula_22 are scalars (that is, elements of ), which are called the "coordinates" of over . However, if one talks of the "set" of the coefficients, one loses the correspondence between coefficients and basis elements, and several vectors may have the same "set" of coefficients. For example, formula_23 and formula_24 have the same set of coefficients , and are different. It is therefore often convenient to work with an ordered basis; this is typically done by indexing the basis elements by the first natural numbers. Then, the coordinates of a vector form a sequence similarly indexed, and a vector is completely characterized by the sequence of coordinates. An ordered basis is also called a frame, a word commonly used, in various contexts, for referring to a sequence of data allowing defining coordinates.

Let, as usual, formula_13 be the set of the -tuples of elements of . This set is an -vector space, with addition and scalar multiplication defined component-wise. The map 
is a linear isomorphism from the vector space formula_13 onto . In other words, formula_13 is the coordinate space of , and the -tuple formula_29 is the coordinate vector of .

The inverse image by formula_30 of formula_31 is the -tuple formula_32 all of whose components are 0, except the th that is 1. The formula_32 form an ordered basis of formula_16 which is called its standard basis or canonical basis. The ordered basis is the image by formula_30 of the canonical basis of formula_17 

It follows from what precedes that every ordered basis is the image by a linear isomorphism of the canonical basis of formula_16 and that every linear isomorphism from formula_13 onto may be defined as the isomorphism that maps the canonical basis of formula_13 onto a given ordered basis of . In other words it is equivalent to define an ordered basis of , or a linear isomorphism from formula_13 onto .

Let be a vector space of dimension over a field . Given two (ordered) bases formula_41 and formula_42 of , it is often useful to express the coordinates of a vector with respect to formula_43 in terms of the coordinates with respect to formula_44 This can be done by the "change-of-basis formula", that is described below. The subscripts "old" and "new" have been chosen because it is customary to refer to formula_43 and formula_46 as the "old basis" and the "new basis", respectively. It is useful to describe the old coordinates in terms of the new ones, because, in general, one has expressions involving the old coordinates, and if one wants to obtain equivalent expressions in terms of the new coordinates; this is obtained by replacing the old coordinates by their expressions in terms of the new coordinates.

Typically, the new basis vectors are given by their coordinates over the old basis, that is, 
If formula_48 and formula_49 are the coordinates of a vector over the old and the new basis respectively, the change-of-basis formula is 
for .

This formula may be concisely written in matrix notation. Let be the matrix of the formula_51 and
be the column vectors of the coordinates of in the old and the new basis respectively, then the formula for changing coordinates is

The formula can be proven by considering the decomposition of the vector on the two bases: one has 
and

The change-of-basis formula results then from the uniqueness of the decomposition of a vector over a basis, here formula_57 that is
for .

If one replaces the field occurring in the definition of a vector space by a ring, one gets the definition of a module. For modules, linear independence and spanning sets are defined exactly as for vector spaces, although "generating set" is more commonly used than that of "spanning set".

Like for vector spaces, a "basis" of a module is a linearly independent subset that is also a generating set. A major difference with the theory of vector spaces is that not every module has a basis. A module that has a basis is called a "free module". Free modules play a fundamental role in module theory, as they may be used for describing the structure of non-free modules through free resolutions.

A module over the integers is exactly the same thing as an abelian group. Thus a free module over the integers is also a free abelian group. Free abelian groups have specific properties that are not shared by modules over other rings. Specifically, every subgroup of a free abelian group is a free abelian group, and, if is a subgroup of a finitely generated free abelian group (that is an abelian group that has a finite basis), there is a basis formula_15 of and an integer such that formula_60 is a basis of , for some nonzero integers formula_61 For details, see .

In the context of infinite-dimensional vector spaces over the real or complex numbers, the term (named after Georg Hamel) or algebraic basis can be used to refer to a basis as defined in this article. This is to make a distinction with other notions of "basis" that exist when infinite-dimensional vector spaces are endowed with extra structure. The most important alternatives are orthogonal bases on Hilbert spaces, Schauder bases, and Markushevich bases on normed linear spaces. In the case of the real numbers R viewed as a vector space over the field Q of rational numbers, Hamel bases are uncountable, and have specifically the cardinality of the continuum, which is the cardinal number formula_62 where formula_63 is the smallest infinite cardinal, the cardinal of the integers.

The common feature of the other notions is that they permit the taking of infinite linear combinations of the basis vectors in order to generate the space. This, of course, requires that infinite sums are meaningfully defined on these spaces, as is the case for topological vector spaces – a large class of vector spaces including e.g. Hilbert spaces, Banach spaces, or Fréchet spaces.

The preference of other types of bases for infinite-dimensional spaces is justified by the fact that the Hamel basis becomes "too big" in Banach spaces: If "X" is an infinite-dimensional normed vector space which is complete (i.e. "X" is a Banach space), then any Hamel basis of "X" is necessarily uncountable. This is a consequence of the Baire category theorem. The completeness as well as infinite dimension are crucial assumptions in the previous claim. Indeed, finite-dimensional spaces have by definition finite bases and there are infinite-dimensional ("non-complete") normed spaces which have countable Hamel bases. Consider formula_64, the space of the sequences formula_65 of real numbers which have only finitely many non-zero elements, with the norm formula_66 Its standard basis, consisting of the sequences having only one non-zero element, which is equal to 1, is a countable Hamel basis.

In the study of Fourier series, one learns that the functions {1} ∪ { sin("nx"), cos("nx") : "n" = 1, 2, 3, ... } are an "orthogonal basis" of the (real or complex) vector space of all (real or complex valued) functions on the interval [0, 2π] that are square-integrable on this interval, i.e., functions "f" satisfying

The functions {1} ∪ { sin("nx"), cos("nx") : "n" = 1, 2, 3, ... } are linearly independent, and every function "f" that is square-integrable on [0, 2π] is an "infinite linear combination" of them, in the sense that

for suitable (real or complex) coefficients "a", "b". But many square-integrable functions cannot be represented as "finite" linear combinations of these basis functions, which therefore "do not" comprise a Hamel basis. Every Hamel basis of this space is much bigger than this merely countably infinite set of functions. Hamel bases of spaces of this kind are typically not useful, whereas orthonormal bases of these spaces are essential in Fourier analysis.

The geometric notions of an affine space, projective space, convex set, and cone have related notions of "basis". An affine basis for an "n"-dimensional affine space is formula_69 points in general linear position. A ' is formula_70 points in general position, in a projective space of dimension "n". A ' of a polytope is the set of the vertices of its convex hull. A consists of one point by edge of a polygonal cone. See also a Hilbert basis (linear programming).

For a probability distribution in R with a probability density function, such as the equidistribution in an "n"-dimensional ball with respect to Lebesgue measure, it can be shown that "n" randomly and independently chosen vectors will form a basis with probability one, which is due to the fact that "n" linearly dependent vectors x, ..., x in R should satisfy the equation (zero determinant of the matrix with columns x), and the set of zeros of a non-trivial polynomial has zero measure. This observation has led to techniques for approximating random bases.

It is difficult to check numerically the linear dependence or exact orthogonality. Therefore, the notion of ε-orthogonality is used. For spaces with inner product, "x" is ε-orthogonal to "y" if formula_71 (that is, cosine of the angle between "x" and "y" is less than ε).

In high dimensions, two independent random vectors are with high probability almost orthogonal, and the number of independent random vectors, which all are with given high probability pairwise almost orthogonal, grows exponentially with dimension. More precisely, consider equidistribution in "n"-dimensional ball. Choose "N" independent random vectors from a ball (they are independent and identically distributed). Let "θ" be a small positive number. Then for

"N" random vectors are all pairwise ε-orthogonal with probability . This "N" growth exponentially with dimension "n" and formula_72 for sufficiently big "n". This property of random bases is a manifestation of the so-called "measure concentration phenomenon".

The figure (right) illustrates distribution of lengths N of pairwise almost orthogonal chains of vectors that are independently randomly sampled from the "n"-dimensional cube as a function of dimension, "n". A point is first randomly selected in the cube. The second point is randomly chosen in the same cube. If the angle between the vectors was within then the vector was retained. At the next step a new vector is generated in the same hypercube, and its angles with the previously generated vectors are evaluated. If these angles are within then the vector is retained. The process is repeated until the chain of almost orthogonality breaks, and the number of such pairwise almost orthogonal vectors (length of the chain) is recorded. For each "n", 20 pairwise almost orthogonal chains where constructed numerically for each dimension. Distribution of the length of these chains is presented.

Let V be any vector space over some field F.
Let X be the set of all linearly independent subsets of V.

The set X is nonempty since the empty set is an independent subset of V,
and it is partially ordered by inclusion, which is denoted, as usual, by .

Let Y be a subset of X that is totally ordered by ,
and let L be the union of all the elements of Y (which are themselves certain subsets of V).

Since (Y, ⊆) is totally ordered, every finite subset of L is a subset of an element of Y,
which is a linearly independent subset of V,
and hence L is linearly independent.
Thus L is an element of X.
Therefore, L is an upper bound for Y in (X, ⊆):
it is an element of X, that contains every element of Y.

As X is nonempty, and every totally ordered subset of (X, ⊆) has an upper bound in X, Zorn's lemma asserts that X has a maximal element. In other words, there exists some element L of X satisfying the condition that whenever L ⊆ L for some element L of X, then L = L.

It remains to prove that L is a basis of V. Since L belongs to X, we already know that L is a linearly independent subset of V.

If there were some vector w of V that is not in the span of L, then w would not be an element of L either.
Let L = L ∪ {w}. This set is an element of X, that is, it is a linearly independent subset of V (because w is not in the span of L, and L is independent). As L ⊆ L, and L ≠ L (because L contains the vector w that is not contained in L), this contradicts the maximality of L. Thus this shows that L spans V.

Hence L is linearly independent and spans V. It is thus a basis of V, and this proves that every vector space has a basis.

This proof relies on Zorn's lemma, which is equivalent to the axiom of choice. Conversely, it has been proved that if every vector space has a basis, then the axiom of choice is true. Thus the two assertions are equivalent.






</doc>
<doc id="18422" url="https://en.wikipedia.org/wiki?curid=18422" title="Linear algebra">
Linear algebra

Linear algebra is the branch of mathematics concerning linear equations such as: 
linear maps such as:
and their representations in vector spaces and through matrices.

Linear algebra is central to almost all areas of mathematics. For instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations. Also, functional analysis, a branch of mathematical analysis, may be viewed as basically the application of linear algebra to spaces of functions.

Linear algebra is also used in most sciences and fields of engineering, because it allows modeling many natural phenomena, and computing efficiently with such models. For nonlinear systems, which cannot be modeled with linear algebra, it is often used for dealing with first-order approximations, using the fact that the differential of a multivariate function at a point is the linear map that best approximates the function near that point.

The procedure for solving simultaneous linear equations now called Gaussian elimination appears in the ancient Chinese mathematical text Chapter Eight: "Rectangular Arrays" of "The Nine Chapters on the Mathematical Art". Its use is illustrated in eighteen problems, with two to five equations.

Systems of linear equations arose in Europe with the introduction in 1637 by René Descartes of coordinates in geometry. In fact, in this new geometry, now called Cartesian geometry, lines and planes are represented by linear equations, and computing their intersections amounts to solving systems of linear equations.

The first systematic methods for solving linear systems used determinants, first considered by Leibniz in 1693. In 1750, Gabriel Cramer used them for giving explicit solutions of linear systems, now called Cramer's rule. Later, Gauss further described the method of elimination, which was initially listed as an advancement in geodesy.

In 1844 Hermann Grassmann published his "Theory of Extension" which included foundational new topics of what is today called linear algebra. In 1848, James Joseph Sylvester introduced the term "matrix", which is Latin for "womb".

Linear algebra grew with ideas noted in the complex plane. For instance, two numbers "w" and "z" in ℂ have a difference "w" – "z", and the line segments formula_3 are of the same length and direction. The segments are equipollent. The four-dimensional system ℍ of quaternions was started in 1843. The term "vector" was introduced as "v" = "x" i + "y" j + "z" k representing a point in space. The quaternion difference "p" – "q" also produces a segment equipollent to formula_4 
Other hypercomplex number systems also used the idea of a linear space with a basis.

Arthur Cayley introduced matrix multiplication and the inverse matrix in 1856, making possible the general linear group. The mechanism of group representation became available for describing complex and hypercomplex numbers. Crucially, Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object. He also realized the connection between matrices and determinants, and wrote "There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants".

Benjamin Peirce published his "Linear Associative Algebra" (1872), and his son Charles Sanders Peirce extended the work later.

The telegraph required an explanatory system, and the 1873 publication of A Treatise on Electricity and Magnetism instituted a field theory of forces and required differential geometry for expression. Linear algebra is flat differential geometry and serves in tangent spaces to manifolds. Electromagnetic symmetries of spacetime are expressed by the Lorentz transformations, and much of the history of linear algebra is the history of Lorentz transformations.

The first modern and more precise definition of a vector space was introduced by Peano in 1888; by 1900, a theory of linear transformations of finite-dimensional vector spaces had emerged. Linear algebra took its modern form in the first half of the twentieth century, when many ideas and methods of previous centuries were generalized as abstract algebra. The development of computers led to increased research in efficient algorithms for Gaussian elimination and matrix decompositions, and linear algebra became an essential tool for modelling and simulations.

See also and .

Until the 19th century, linear algebra was introduced through systems of linear equations and matrices. In modern mathematics, the presentation through "vector spaces" is generally preferred, since it is more synthetic, more general (not limited to the finite-dimensional case), and conceptually simpler, although more abstract.

A vector space over a field (often the field of the real numbers) is a set equipped with two binary operations satisfying the following axioms. 
Elements of are called "vectors", and elements of "F" are called "scalars". The first operation, "vector addition", takes any two vectors and and outputs a third vector . The second operation, "scalar multiplication", takes any scalar and any vector and outputs a new . The axioms that addition and scalar multiplication must satisfy are the following. (In the list below, and are arbitrary elements of , and and are arbitrary scalars in the field .)

The first four axioms mean that is an abelian group under addition.

An element of a specific vector space may have various nature; for example, it could be a sequence, a function, a polynomial or a matrix. Linear algebra is concerned with those properties of such objects that are common to all vector spaces.

Linear maps are mappings between vector spaces that preserve the vector-space structure. Given two vector spaces and over a field , a linear map (also called, in some contexts, linear transformation or linear mapping) is a map

that is compatible with addition and scalar multiplication, that is

for any vectors in and scalar in .

This implies that for any vectors in and scalars in , one has

When "V" = "W" are the same vector space, a linear map formula_8 is also known as a "linear operator" on "V".

A bijective linear map between two vector spaces (that is, every vector from the second space is associated with exactly one in the first) is an isomorphism. Because an isomorphism preserves linear structure, two isomorphic vector spaces are "essentially the same" from the linear algebra point of view, in the sense that they cannot be distinguished by using vector space properties. An essential question in linear algebra is testing whether a linear map is an isomorphism or not, and, if it is not an isomorphism, finding its range (or image) and the set of elements that are mapped to the zero vector, called the kernel of the map. All these questions can be solved by using Gaussian elimination or some variant of this algorithm.

The study of those subsets of vector spaces that are in themselves vector spaces under the induced operations is fundamental, similarly as for many mathematical structures. These subsets are called linear subspaces. More precisely, a linear subspace of a vector space over a field is a subset of such that and are in , for every , in , and every in . (These conditions suffice for implying that is a vector space.)

For example, given a linear map formula_5, the image "T(V)" of "V", and the inverse image formula_10 of 0 (called kernel or null space), are linear subspaces of "W" and "V", respectively.

Another important way of forming a subspace is to consider linear combinations of a set of vectors: the set of all sums 
where are in , and are in form a linear subspace called the span of . The span of is also the intersection of all linear subspaces containing . In other words, it is the (smallest for the inclusion relation) linear subspace containing .

A set of vectors is linearly independent if none is in the span of the others. Equivalently, a set of vector is linearly independent if the only way to express the zero vector as a linear combination of elements of is to take zero for every coefficient formula_12

A set of vectors that spans a vector space is called a spanning set or generating set. If a spanning set is "linearly dependent" (that is not linearly independent), then some element of is in the span of the other elements of , and the span would remain the same if one remove from . One may continue to remove elements of until getting a "linearly independent spanning set". Such a linearly independent set that spans a vector space is called a basis of . The importance of bases lies in the fact that there are together minimal generating sets and maximal independent sets. More precisely, if is a linearly independent set, and is a spanning set such that formula_13 then there is a basis such that formula_14

Any two bases of a vector space have the same cardinality, which is called the dimension of ; this is the dimension theorem for vector spaces. Moreover, two vector spaces over the same field are isomorphic if and only if they have the same dimension.

If any basis of (and therefore every basis) has a finite number of elements, is a "finite-dimensional vector space". If is a subspace of , then . In the case where is finite-dimensional, the equality of the dimensions implies .

If "U" and "U" are subspaces of "V", then

where formula_16denotes the span of formula_17

Matrices allow explicit manipulation of finite-dimensional vector spaces and linear maps. Their theory is thus an essential part of linear algebra.

Let be a finite-dimensional vector space over a field , and be a basis of (thus is the dimension of ). By definition of a basis, the map
is a bijection from formula_19 the set of the sequences of elements of , onto . This is an isomorphism of vector spaces, if formula_20 is equipped of its standard structure of vector space, where vector addition and scalar multiplication are done component by component.

This isomorphism allows representing a vector by its inverse image under this isomorphism, that is by the coordinates vector formula_21 or by the column matrix

If is another finite dimensional vector space (possibly the same), with a basis formula_23 a linear map from to is well defined by its values on the basis elements, that is formula_24 Thus, is well represented by the list of the corresponding column matrices. That is, if 
for , then is represented by the matrix
with rows and columns.

Matrix multiplication is defined in such a way that the product of two matrices is the matrix of the composition of the corresponding linear maps, and the product of a matrix and a column matrix is the column matrix representing the result of applying the represented linear map to the represented vector. It follows that the theory of finite-dimensional vector spaces and the theory of matrices are two different languages for expressing exactly the same concepts.

Two matrices that encode the same linear transformation in different bases are called similar. It can be proved that two matrices are similar if and only if one can transform one in the other by elementary row and column operations. For a matrix representing a linear map from to , the row operations correspond to change of bases in and the column operations correspond to change of bases in . Every matrix is similar to an identity matrix possibly bordered by zero rows and zero columns. In terms of vector spaces, this means that, for any linear map from to , there are bases such that a part of the basis of is mapped bijectively on a part of the basis of , and that the remaining basis elements of , if any, are mapped to zero. Gaussian elimination is the basic algorithm for finding these elementary operations, and proving these results.

A finite set of linear equations in a finite set of variables, for example, formula_27 or formula_28 is called a system of linear equations or a linear system.

Systems of linear equations form a fundamental part of linear algebra. Historically, linear algebra and matrix theory has been developed for solving such systems. In the modern presentation of linear algebra through vector spaces and matrices, many problems may be interpreted in terms of linear systems.

For example, let

be a linear system.

To such a system, one may associate its matrix 
and its right member vector

Let be the linear transformation associated to the matrix . A solution of the system () is a vector 
such that 
that is an element of the preimage of by .

Let () be the associated homogeneous system, where the right-hand sides of the equations are put to zero:

The solutions of () are exactly the elements of the kernel of or, equivalently, .

The Gaussian-elimination consists of performing elementary row operations on the augmented matrix
for putting it in reduced row echelon form. These row operations do not change the set of solutions of the system of equations. In the example, the reduced echelon form is 
showing that the system () has the unique solution

It follows from this matrix interpretation of linear systems that the same methods can be applied for solving linear systems and for many operations on matrices and linear transformations, which include the computation of the ranks, kernels, matrix inverses.

A linear endomorphism is a linear map that maps a vector space to itself. 
If has a basis of elements, such an endomorphism is represented by a square matrix of size .

With respect to general linear maps, linear endomorphisms and square matrices have some specific properties that make their study an important part of linear algebra, which is used in many parts of mathematics, including geometric transformations, coordinate changes, quadratic forms, and many other part of mathematics.

The "determinant" of a square matrix is defined to be
where formula_37 is the group of all permutations of elements,
formula_38 is a permutation, and formula_39 the parity of the permutation.
A matrix is invertible if and only if the determinant is invertible (i.e., nonzero if the scalars belong to a field).

Cramer's rule is a closed-form expression, in terms of determinants, of the solution of a system of linear equations in unknowns. Cramer's rule is useful for reasoning about the solution, but, except for or , it is rarely used for computing a solution, since Gaussian elimination is a faster algorithm.

The "determinant of an endomorphism" is the determinant of the matrix representing the endomorphism in terms of some ordered basis. This definition makes sense, since this determinant is independent of the choice of the basis.

If is a linear endomorphism of a vector space over a field , an "eigenvector" of is a nonzero vector of such that for some scalar in . This scalar is an "eigenvalue" of .

If the dimension of is finite, and a basis has been chosen, and may be represented, respectively, by a square matrix and a column matrix ; the equation defining eigenvectors and eigenvalues becomes
Using the identity matrix , whose entries are all zero, except those of the main diagonal, which are equal to one, this may be rewritten
As is supposed to be nonzero, this means that is a singular matrix, and thus that its determinant formula_42 equals zero. The eigenvalues are thus the roots of the polynomial
If is of dimension , this is a monic polynomial of degree , called the characteristic polynomial of the matrix (or of the endomorphism), and there are, at most, eigenvalues.

If a basis exists that consists only of eigenvectors, the matrix of on this basis has a very simple structure: it is a diagonal matrix such that the entries on the main diagonal are eigenvalues, and the other entries are zero. In this case, the endomorphism and the matrix are said to be diagonalizable. More generally, an endomorphism and a matrix are also said diagonalizable, if they become diagonalizable after extending the field of scalars. In this extended sense, if the characteristic polynomial is square-free, then the matrix is diagonalizable.

A symmetric matrix is always diagonalizable. There are non-diagonalizable matrices, the simplest being
(it cannot be diagonalizable since its square is the zero matrix, and the square of a nonzero diagonal matrix is never zero).

When an endomorphism is not diagonalizable, there are bases on which it has a simple form, although not as simple as the diagonal form. The Frobenius normal form does not need of extending the field of scalars and makes the characteristic polynomial immediately readable on the matrix. The Jordan normal form requires to extend the field of scalar for containing all eigenvalues, and differs from the diagonal form only by some entries that are just above the main diagonal and are equal to 1.

A linear form is a linear map from a vector space over a field to the field of scalars , viewed as a vector space over itself. Equipped by pointwise addition and multiplication by a scalar, the linear forms form a vector space, called the dual space of , and usually denoted formula_45

If formula_46 is a basis of (this implies that is finite-dimensional), then one can define, for , a linear map formula_47 such that formula_48 and formula_49 if . These linear maps form a basis of formula_50 called the dual basis of formula_51 (If is not finite-dimensional, the formula_52 may be defined similarly; they are linearly independent, but do not form a basis.)

For in , the map
is a linear form on formula_45 This defines the canonical linear map from into formula_55 the dual of formula_50 called the bidual of . This canonical map is an isomorphism if is finite-dimensional, and this allows identifying with its bidual. (In the infinite dimensional case, the canonical map is injective, but not surjective.)

There is thus a complete symmetry between a finite-dimensional vector space and its dual. This motivates the frequent use, in this context, of the bra–ket notation
for denoting .

Let 
be a linear map. For every linear form on , the composite function is a linear form on . This defines a linear map
between the dual spaces, which is called the dual or the transpose of .

If and are finite dimensional, and is the matrix of in terms of some ordered bases, then the matrix of formula_60 over the dual bases is the transpose formula_61 of , obtained by exchanging rows and columns.

If elements of vector spaces and their duals are represented by column vectors, this duality may be expressed in bra–ket notation by 
For highlighting this symmetry, the two members of this equality are sometimes written 

Besides these basic concepts, linear algebra also studies vector spaces with additional structure, such as an inner product. The inner product is an example of a bilinear form, and it gives the vector space a geometric structure by allowing for the definition of length and angles. Formally, an "inner product" is a map

that satisfies the following three axioms for all vectors "u", "v", "w" in "V" and all scalars "a" in "F":

In R, it is symmetric.


We can define the length of a vector "v" in "V" by
and we can prove the Cauchy–Schwarz inequality:

In particular, the quantity
and so we can call this quantity the cosine of the angle between the two vectors.

Two vectors are orthogonal if formula_72. An orthonormal basis is a basis where all basis vectors have length 1 and are orthogonal to each other. Given any finite-dimensional vector space, an orthonormal basis could be found by the Gram–Schmidt procedure. Orthonormal bases are particularly easy to deal with, since if "v" = "a" "v" + ... + "a v", then formula_73.

The inner product facilitates the construction of many useful concepts. For instance, given a transform "T", we can define its Hermitian conjugate "T*" as the linear transform satisfying
If "T" satisfies "TT*" = "T*T", we call "T" normal. It turns out that normal matrices are precisely the matrices that have an orthonormal system of eigenvectors that span "V".

There is a strong relationship between linear algebra and geometry, which started with the introduction by René Descartes, in 1637, of Cartesian coordinates. In this new (at that time) geometry, now called Cartesian geometry, points are represented by Cartesian coordinates, which are sequences of three real numbers (in the case of the usual three-dimensional space). The basic objects of geometry, which are lines and planes are represented by linear equations. Thus, computing intersections of lines and planes amounts to solving systems of linear equations. This was one of the main motivations for developing linear algebra.

Most geometric transformation, such as translations, rotations, reflections, rigid motions, isometries, and projections transform lines into lines. It follows that they can be defined, specified and studied in terms of linear maps. This is also the case of homographies and Möbius transformations, when considered as transformations of a projective space.

Until the end of 19th century, geometric spaces were defined by axioms relating points, lines and planes (synthetic geometry). Around this date, it appeared that one may also define geometric spaces by constructions involving vector spaces (see, for example, Projective space and Affine space) It has been shown that the two approaches are essentially equivalent. In classical geometry, the involved vector spaces are vector spaces over the reals, but the constructions may be extended to vector spaces over any field, allowing considering geometry over arbitrary fields, including finite fields.

Presently, most textbooks, introduce geometric spaces from linear algebra, and geometry is often presented, at elementary level, as a subfield of linear algebra.

Linear algebra is used in almost all areas of mathematics, thus making it relevant in almost all scientific domains that use mathematics. These applications may be divided into several wide categories.

The modeling of ambient space is based on geometry. Sciences concerned with this space use geometry widely. This is the case with mechanics and robotics, for describing rigid body dynamics; geodesy for describing Earth shape; perspectivity, computer vision, and computer graphics, for describing the relationship between a scene and its plane representation; and many other scientific domains.

In all these applications, synthetic geometry is often used for general descriptions and a qualitative approach, but for the study of explicit situations, one must compute with coordinates. This requires the heavy use of linear algebra.

Functional analysis studies function spaces. These are vector spaces with additional structure, such as Hilbert spaces. Linear algebra is thus a fundamental part of functional analysis and its applications, which include, in particular, quantum mechanics (wave functions).

Most physical phenomena are modeled by partial differential equations. To solve them, one usually decomposes the space in which the solutions are searched into small, mutually interacting cells. For linear systems this interaction involves linear functions. For nonlinear systems, this interaction is often approximated by linear functions. In both cases, very large matrices are generally involved. Weather forecasting is a typical example, where the whole Earth atmosphere is divided in cells of, say, 100 km of width and 100 m of height.

Nearly all scientific computations involve linear algebra. Consequently, linear algebra algorithms have been highly optimized. BLAS and LAPACK are the best known implementations. For improving efficiency, some of them configure the algorithms automatically, at run time, for adapting them to the specificities of the computer (cache size, number of available cores, ...).

Some processors, typically graphics processing units (GPU), are designed with a matrix structure, for optimizing the operations of linear algebra.

This section presents several related topics that do not appear generally in elementary textbooks on linear algebra, but are commonly considered, in advanced mathematics, as parts of linear algebra.

The existence of multiplicative inverses in fields is not involved in the axioms defining a vector space. One may thus replace the field of scalars by a ring , and this gives a structure called module over , or -module.

The concepts of linear independence, span, basis, and linear maps (also called module homomorphisms) are defined for modules exactly as for vector spaces, with the essential difference that, if is not a field, there are modules that do not have any basis. The modules that have a basis are the free modules, and those that are spanned by a finite set are the finitely generated modules. Module homomorphisms between finitely generated free modules may be represented by matrices. The theory of matrices over a ring is similar to that of matrices over a field, except that determinants exist only if the ring is commutative, and that a square matrix over a commutative ring is invertible only if its determinant has a multiplicative inverse in the ring.

Vector spaces are completely characterized by their dimension (up to an isomorphism). In general, there is not such a complete classification for modules, even if one restricts oneself to finitely generated modules. However, every module is a cokernel of a homomorphism of free modules.

Modules over the integers can be identified with abelian groups, since the multiplication by an integer may identified to a repeated addition. Most of the theory of abelian groups may be extended to modules over a principal ideal domain. In particular, over a principal ideal domain, every submodule of a free module is free, and the fundamental theorem of finitely generated abelian groups may be extended straightforwardly to finitely generated modules over a principal ring.

There are many rings for which there are algorithms for solving linear equations and systems of linear equations. However, these algorithms have generally a computational complexity that is much higher than the similar algorithms over a field. For more details, see Linear equation over a ring.

In multilinear algebra, one considers multivariable linear transformations, that is, mappings that are linear in each of a number of different variables. This line of inquiry naturally leads to the idea of the dual space, the vector space "V" consisting of linear maps where "F" is the field of scalars. Multilinear maps can be described via tensor products of elements of "V".

If, in addition to vector addition and scalar multiplication, there is a bilinear vector product , the vector space is called an algebra; for instance, associative algebras are algebras with an associate vector product (like the algebra of square matrices, or the algebra of polynomials).

Vector spaces that are not finite dimensional often require additional structure to be tractable. A normed vector space is a vector space along with a function called a norm, which measures the "size" of elements. The norm induces a metric, which measures the distance between elements, and induces a topology, which allows for a definition of continuous maps. The metric also allows for a definition of limits and completeness - a metric space that is complete is known as a Banach space. A complete metric space along with the additional structure of an inner product (a conjugate symmetric sesquilinear form) is known as a Hilbert space, which is in some sense a particularly well-behaved Banach space. Functional analysis applies the methods of linear algebra alongside those of mathematical analysis to study various function spaces; the central objects of study in functional analysis are L spaces, which are Banach spaces, and especially the "L" space of square integrable functions, which is the only Hilbert space among them. Functional analysis is of particular importance to quantum mechanics, the theory of partial differential equations, digital signal processing, and electrical engineering. It also provides the foundation and theoretical framework that underlies the Fourier transform and related methods.










</doc>
<doc id="18423" url="https://en.wikipedia.org/wiki?curid=18423" title="Labia majora">
Labia majora

The labia majora (singular: "labium majus") are two prominent longitudinal cutaneous folds that extend downward and backward from the mons pubis to the perineum. Together with the labia minora they form the labia of the vulva.

The labia majora are homologous to the male scrotum.

"Labia majora" is the Latin plural for big ("major") lips; the singular is "labium majus." The Latin term "labium/labia" is used in anatomy for a number of usually paired parallel structures, but in English it is mostly applied to two pairs of parts of female external genitals (vulva)—labia majora and labia minora. Labia majora are commonly known as the outer lips, while labia minora (Latin for "small lips"), which run alongside between them, are referred to as the inner lips. Traditionally, to avoid confusion with other lip-like structures of the body, the labia of female genitals were termed by anatomists in Latin as "labia majora ("or "minora) pudendi."

Embryologically, they develop from labioscrotal folds. It means that they develop in the female foetus from the same previously sexually undifferentiated anatomical structure as the scrotum, the sac of skin below the penis in males.

The same process of sex differentiation concerns other male and female reproductive organs (see List of related male and female reproductive organs), with some organs of both sexes developing similar, yet not identical, structure and functions (like the gonads - male testicles and female ovaries, like male and female urethras, erectile corpus cavernosum penis and prepuce in the penis (foreskin) and the corpus cavernosum clitoridis in the clitoris and (clitoral hood) and their frenula). But other male and female sex organs become absolutely different and unique, like the internal female genitalia.

The scrotum and labia majora develop to have both similarities and crucial differences. Like the scrotum, labia majora after puberty may become of a darker color than the skin outside them, and, similarly, also grow pubic hair on their external surface (the female genitals on accompanying photos are shaved to show their structure clearer). But, during sexual differentiation of the foetus, labioscrotal folds in the males normally fuse longitudinally in the middle, forming a sack for male gonads (testicles) to descend into it from the pelvis, while in the females these folds normally do not fuse, forming the two labia majora and the pudendal cleft between them. Female gonads (ovaries) do not descend from the pelvis, thus the structure of labia majora may seem simpler (just fatty tissue covered with skin) and of lesser significance for functioning of the female body as a whole than the scrotum with testicles for males. The ridge or groove remaining of the fusion can be traced on the scrotum.

In some cases of intersex with disorders of sex development male/female genitalia may look ambiguous for either gender with phallus too small for a typical penis yet too big for a clitoris, with external urethral opening in an atypical location, and with labia/scrotum fully or partially fused but without descended gonads in them. Undescended testicles, though, may also occur in otherwise generally healthy male infants.

The labia majora constitute the lateral boundaries of the pudendal cleft, which contains the labia minora, interlabial sulci, clitoral hood, clitoral glans, frenulum clitoridis, the Hart's Line, and the vulval vestibule, which contains the external openings of the urethra and the vagina. Each labium majus has two surfaces, an outer, pigmented and covered with strong, pubic hair; and an inner, smooth and beset with large sebaceous follicles. The labia majora are covered with squamous epithelium. Between the two there is a considerable quantity of areolar tissue, fat, and a tissue resembling the dartos tunic of the scrotum, besides vessels, nerves, and glands. The labia majora are thicker in front, and form the anterior labial commissure where they meet below the mons pubis. Posteriorly, they are not really joined, but appear to become lost in the neighboring integument, ending close to, and nearly parallel to, each other. Together with the connecting skin between them, they form another commissure the posterior labial commissure which is also the posterior boundary of the pudendum. The interval between the posterior commissure and the anus, from 2.5 to 3 cm in length, constitutes the perineum. The anterior region of the perineum is known as the urogenital triangle which separates it from the anal region. Between the labia majora and the inner thighs are the labiocrural folds. Between the labia majora and labia minora are the interlabial sulci. Labia majora atrophy after menopause.

The fat pad of the labia majora can be used as a graft, often as a so-called "Martius labial fat pad graft", and can be used, for example, in urethrolysis.




</doc>
<doc id="18424" url="https://en.wikipedia.org/wiki?curid=18424" title="Labia minora">
Labia minora

The labia minora (Latin for "smaller lips," singular: "labium minus" "smaller lip"), also known as the inner labia, inner lips, vaginal lips or nymphae, are two flaps of skin on either side of the human vaginal opening in the vulva, situated between the labia majora (Latin for "larger lips;" also called outer labia, or outer lips). The labia minora vary widely in size, color and shape from individual to individual.

The labia minora are homologous to the male urethral surface of the penis.

The labia minora extend from the clitoris obliquely downward, laterally, and backward on either side of the vulval vestibule, ending between the bottom of the vulval vestibule and the labia majora. The posterior ends (bottom) of the labia minora are usually joined across the middle line by a fold of skin, named the frenulum of labia minora or fourchette.

On the front, each lip forks dividing into two portions surrounding the clitoris. The upper part of each lip passes above the clitoris to meet the upper part of the other lip—which will often be a little larger or smaller—forming a fold which overhangs the glans clitoridis (clitoral tip or head); this fold is named the clitoral hood. The lower part passes beneath the glans clitoridis and becomes united to its under surface, forming, with the inner lip of the opposite side, the "frenulum clitoridis".

The clitoral hood, analogously to the foreskin of the penis in men and also termed, like the latter, by the Latin word "prepuce", serves to cover most of the time the shaft and sometimes the glans (which is very sensitive to the touch) to protect the clitoris from mechanical irritation and from dryness. Yet the hood is movable and can slide during clitoral erection or be pulled upwards a little for greater exposure of the clitoris to sexual stimulation.

The frenulum (Latin for "little bridle") is an elastic band of tissue attached by its one end to the clitoral shaft and glans and by its other end to the prepuce. It allows two-way shifting of the clitoral hood: firstly, it can extend to let the hood be moved upwards to expose the glans for stimulation or hygienic cleansing, and secondly, it contracts to pull the hood back to protect it.

On the opposed surfaces of the labia minora are numerous sebaceous glands not associated with hair follicles. They are lined by stratified squamous epithelium on those surfaces.

Like the whole area of the vulval vestibule, the mucus secreted by those glands protects the labia from dryness and mechanical irritation.

Being thinner than the outer labia, the inner labia can be also more narrow than the former, or wider than labia majora, thus protruding in the pudendal cleft and making the term "minora" (Latin for smaller) essentially inapplicable in these cases. They can also be smooth or frilled, the latter being more typical of longer or wider inner labia.

From 2003 to 2004, researchers from the Department of Gynaecology, Elizabeth Garret Anderson Hospital in London, measured the labia and other genital structures of 50 women from the age of 18 to 50, with a mean age of 35.6. The study has since been criticized for its "small and homogenous sample group" consisting primarily of white women. The results were:

Due to the frequent portrayal of the pudendal cleft without protrusion in art and pornography, there has been a rise in the popularity of labiaplasty, surgery to alter the labia - usually, to make them smaller. On the other hand, there is an opposite movement of labia stretching. Its proponents stress the beauty of long labia and their positive role in sexual stimulation of both partners.

Labiaplasty is also sometimes sought by women who have asymmetrical labia minora to adjust the shape of the structures towards identical size.

Labia stretching has traditionally been practised in some African nations in the East and South and the South Pacific.

The inner lips serve to protect from mechanical irritation, dryness and infections the highly sensitive area of the vulval vestibule with vaginal and urethral openings in it between them. During vaginal sexual intercourse they may contribute to stimulation of the whole vestibule area, the clitoris and the vagina of the woman and the penis of her partner. Stimulation of the clitoris may occur through tension of the clitoral hood and its frenulum by inner labia pulling at them. During sexual arousal they are lubricated by the mucus secreted in the vagina and around it to make penetration painless and protect them from irritation.

As the female external urethral opening (meatus) is also situated between labia minora, they may play a role in guiding the stream of the urine during female urination.

Being very sensitive by their structure to any irritation, and situated in the excretion area where traces of urine, vaginal discharge, smegma and even feces may be present, the inner lips may be susceptible to inflammatory infections of the vulva such as vulvitis.

The likelihood of inflammation may be reduced through appropriate regular hygienic cleansing of the whole vulval vestibule, using water and medically tested cleansing agents designed for vulvas. To avoid contamination of the vulva with fecal bacteria, it is recommended that the vulva is washed only from front to back, from mons pubis to the perineum and anus. Apart from water and special liquid cleansing agents (lotions), there are commercially available wet wipes for female intimate hygiene. Some women wipe the vulval vestibule dry with toilet tissue after urination to avoid irritation and infections from residual drops of the urine in the area.

However, incorrect choice of cleansing agents, or their incorrect application, may itself cause labial irritation and require medical attention. Over-vigorous rubbing of the labia of little girls while washing, combined with the lack of estrogen in their bodies, may lead to the mostly pediatric condition known as labial fusion. If fused labia prevent urination, urine may accumulate and cause pain and inflammation.

In adult females, irritation of the area may be caused by wearing too-tight underwear (especially where wider inner labia protrude in the pudendal cleft); while G-strings, which rub against the labia during body movements, may cause irritation or lead to infection from bacteria transferred from either the external environment or the anus.



</doc>
<doc id="18425" url="https://en.wikipedia.org/wiki?curid=18425" title="Leopold von Sacher-Masoch">
Leopold von Sacher-Masoch

Leopold Ritter von Sacher-Masoch (27 January 1836 – 9 March 1895) was an Austrian nobleman, writer and journalist, who gained renown for his romantic stories of Galician life. The term "masochism" is derived from his name, invented by his contemporary, the Austrian psychiatrist Richard von Krafft-Ebing. Masoch did not approve of this use of his name.

During his lifetime, Sacher-Masoch was well known as a man of letters, in particular a utopian thinker who espoused socialist and humanist ideals in his fiction and non-fiction. Most of his works remain untranslated into English. Until recently, his novel "Venus in Furs" was his only book commonly available in English, but an English translation by William Holmes of "Die Gottesmutter" was released in 2015 as "The Mother of God".

Von Sacher-Masoch was born in the city of Lemberg (now Lviv, Ukraine), the capital of the Kingdom of Galicia and Lodomeria, at the time a province of the Austrian Empire, into the Roman Catholic family of an Austrian civil servant, Leopold Johann Nepomuk Ritter von Sacher, and Charlotte von Masoch, a Ukrainian noblewoman. The father later combined his surname with his wife's 'von Masoch', at the request of her family (she was the last of the line). Von Sacher served as a Commissioner of the Imperial Police Forces in Lemberg, and he was recognised with a new title of nobility as Sacher-Masoch awarded by the Austrian Emperor.

Leopold studied law, history and mathematics at Graz University, and after graduating moved back to Lemberg where he became a professor. His early, non-fictional publications dealt mostly with Austrian history. At the same time, Masoch turned to the folklore and culture of his homeland, Galicia. Soon he abandoned lecturing and became a free man of letters. Within a decade his short stories and novels prevailed over his historical non-fiction works, though historical themes continued to imbue his fiction.

Panslavist ideas were prevalent in Masoch's literary work, and he found a particular interest in depicting picturesque types among the various ethnicities that inhabited Galicia. From the 1860s to the 1880s he published a number of volumes of "Jewish Short Stories", "Polish Short Stories", "Galician Short Stories", "German Court Stories" and "Russian Court Stories". His works were published in translation in Ukrainian, Polish, Russian and French.

In 1869, Sacher-Masoch conceived a grandiose series of short stories under the collective title "Legacy of Cain" that would represent the author's aesthetic "Weltanschauung". The cycle opened with the manifesto "The Wanderer" that brought out misogynist themes that became peculiar to Masoch's writings. Of the six planned volumes, only the first two were ever completed. By the middle of the 1880s, Masoch abandoned the "Legacy of Cain". Nevertheless, the published volumes of the series included Masoch's best-known stories, and of them, "Venus in Furs" (1869) is the most famous today. The short novel expressed Sacher-Masoch's fantasies and fetishes (especially for dominant women wearing fur). He did his best to live out his fantasies with his mistresses and wives.

Sacher-Masoch edited the Leipzig-based monthly literary magazine "Auf der Höhe. Internationale Review" ("At the Pinnacle. International Review"), which was published from October, 1881 to September, 1885. This was a progressive magazine aimed at tolerance and integration for Jews in Saxony, as well as for the emancipation of women with articles on women's education and suffrage.

In his later years, he worked against local antisemitism through an association for adult education called the "Oberhessischer Verein für Volksbildung" (OVV), founded in 1893 with his second wife, Hulda Meister, who had also been his assistant for some years.

Fanny Pistor was an emerging literary writer. She met Sacher-Masoch after she contacted him, under the assumed name and fictitious title of Baroness Bogdanoff, for suggestions on improving her writing to make it suitable for publication.

On 9 December 1869, Sacher-Masoch and Pistor, who was by then his mistress, signed a contract making him her slave for a period of six months, with the stipulation that the Baroness wear furs as often as possible, especially when she was in a cruel mood. Sacher-Masoch took the alias of "Gregor", a stereotypical male servant's name, and assumed a disguise as the servant of the Baroness. The two travelled by train to Italy. As in "Venus in Furs", he traveled in the third-class compartment, while she had a seat in first-class, arriving in Venice (Florence, in the novel), where they were not known, and would not arouse suspicion.

Sacher-Masoch pressured his first wife – Aurora von Rümelin, whom he married in 1873 – to live out the experience of the book, against her preferences. Sacher-Masoch found his family life to be unexciting, and eventually got a divorce and married his assistant.

In 1875, Masoch wrote "The Ideals of Our Time", an attempt to give a portrait of German society during its Gründerzeit period.

In his late fifties, his mental health began to deteriorate, and he spent the last years of his life under psychiatric care. According to official reports, he died in Lindheim, Altenstadt, Hesse, in 1895. It is also claimed that he died in an asylum in Mannheim in 1905.

Sacher-Masoch is the great-great-uncle to the British singer and actress Marianne Faithfull on the side of her mother, the Viennese Baroness Eva Erisso.

The term "masochism" was coined in 1886 by the Austrian psychiatrist Richard Freiherr von Krafft-Ebing (1840–1902) in his book "Psychopathia Sexualis":

Sacher-Masoch was not pleased with Krafft-Ebing's assertions. Nevertheless, details of Masoch's private life were obscure until Aurora von Rümelin's memoirs, "Meine Lebensbeichte" (My Life Confession; 1906), were published in Berlin under the pseudonym Wanda v. Dunajew. The following year, a French translation, "Confession de Ma Vie" (1907) by "Wanda von Sacher-Masoch", was printed in Paris by Mercure de France. An English translation of the French edition was published as "The Confessions of Wanda von Sacher-Masoch" (1991) by RE/Search Publications.






</doc>
<doc id="18426" url="https://en.wikipedia.org/wiki?curid=18426" title="Lithography">
Lithography

Lithography () is a method of printing originally based on the immiscibility of oil and water. The printing is from a stone (lithographic limestone) or a metal plate with a smooth surface. It was invented in 1796 by German author and actor Alois Senefelder as a cheap method of publishing theatrical works. Lithography can be used to print text or artwork onto paper or other suitable material.

Lithography originally used an image drawn with oil, fat, or wax onto the surface of a smooth, level lithographic limestone plate. The stone was treated with a mixture of acid and gum arabic, "etching" the portions of the stone that were not protected by the grease-based image. When the stone was subsequently moistened, these etched areas retained water; an oil-based ink could then be applied and would be repelled by the water, sticking only to the original drawing. The ink would finally be transferred to a blank paper sheet, producing a printed page. This traditional technique is still used in some fine art printmaking applications.

In modern lithography, the image is made of a polymer coating applied to a flexible plastic or metal plate. The image can be printed directly from the plate (the orientation of the image is reversed), or it can be offset, by transferring the image onto a flexible sheet (rubber) for printing and publication.

As a printing technology, lithography is different from intaglio printing (gravure), wherein a plate is either engraved, etched, or stippled to score cavities to contain the printing ink; and woodblock printing or letterpress printing, wherein ink is applied to the raised surfaces of letters or images. Today, most types of high-volume books and magazines, especially when illustrated in colour, are printed with offset lithography, which has become the most common form of printing technology since the 1960s.

The related term "photolithography" refers to the use of photographic images in lithographic printing, whether these images are printed directly from a stone or from a metal plate, as in offset printing. "Photolithography" is used synonymously with "offset printing". The technique as well as the term were introduced in Europe in the 1850s. Beginning in the 1960s, photolithography has played an important role in the fabrication and mass production of integrated circuits in the microelectronics industry.

Lithography uses simple chemical processes to create an image. For instance, the positive part of an image is a water-repelling ("hydrophobic") substance, while the negative image would be water-retaining ("hydrophilic"). Thus, when the plate is introduced to a compatible printing ink and water mixture, the ink will adhere to the positive image and the water will clean the negative image. This allows a flat print plate to be used, enabling much longer and more detailed print runs than the older physical methods of printing (e.g., intaglio printing, letterpress printing).

Lithography was invented by Alois Senefelder in the Kingdom of Bavaria in 1796. In the early days of lithography, a smooth piece of limestone was used (hence the name "lithography": "lithos" () is the ancient Greek word for stone). After the oil-based image was put on the surface, a solution of gum arabic in water was applied, the gum sticking only to the non-oily surface. During printing, water adhered to the gum arabic surfaces and was repelled by the oily parts, while the oily ink used for printing did the opposite.

Lithography works because of the mutual repulsion of oil and water. The image is drawn on the surface of the print plate with a fat or oil-based medium (hydrophobic) such as a wax crayon, which may be pigmented to make the drawing visible. A wide range of oil-based media is available, but the durability of the image on the stone depends on the lipid content of the material being used, and its ability to withstand water and acid. After the drawing of the image, an aqueous solution of gum arabic, weakly acidified with nitric acid is applied to the stone. The function of this solution is to create a hydrophilic layer of calcium nitrate salt, , and gum arabic on all non-image surfaces. The gum solution penetrates into the pores of the stone, completely surrounding the original image with a hydrophilic layer that will not accept the printing ink. Using lithographic turpentine, the printer then removes any excess of the greasy drawing material, but a hydrophobic molecular film of it remains tightly bonded to the surface of the stone, rejecting the gum arabic and water, but ready to accept the oily ink.

Senefelder had experimented during the early 19th century with multicolor lithography; in his 1819 book, he predicted that the process would eventually be perfected and used to reproduce paintings. Multi-color printing was introduced by a new process developed by Godefroy Engelmann (France) in 1837 known as chromolithography. A separate stone was used for each color, and a print went through the press separately for each stone. The main challenge was to keep the images aligned ("in register"). This method lent itself to images consisting of large areas of flat color, and resulted in the characteristic poster designs of this period.
"Lithography, or printing from soft stone, largely took the place of engraving in the production of English commercial maps after about 1852. It was a quick, cheap process and had been used to print British army maps during the Peninsula War. Most of the commercial maps of the second half of the 19th century were lithographed and unattractive, though accurate enough."

High-volume lithography is used presently to produce posters, maps, books, newspapers, and packaging—just about any smooth, mass-produced item with print and graphics on it. Most books, indeed all types of high-volume text, are now printed using offset lithography.

For offset lithography, which depends on photographic processes, flexible aluminum, polyester, mylar or paper printing plates are used instead of stone tablets. Modern printing plates have a brushed or roughened texture and are covered with a photosensitive emulsion. A photographic negative of the desired image is placed in contact with the emulsion and the plate is exposed to ultraviolet light. After development, the emulsion shows a reverse of the negative image, which is thus a duplicate of the original (positive) image. The image on the plate emulsion can also be created by direct laser imaging in a CTP (Computer-To-Plate) device known as a platesetter. The positive image is the emulsion that remains after imaging. Non-image portions of the emulsion have traditionally been removed by a chemical process, though in recent times plates have come available that do not require such processing.

The plate is affixed to a cylinder on a printing press. Dampening rollers apply water, which covers the blank portions of the plate but is repelled by the emulsion of the image area. Hydrophobic ink, which is repelled by the water and only adheres to the emulsion of the image area, is then applied by the inking rollers.

If this image were transferred directly to paper, it would create a mirror-type image and the paper would become too wet. Instead, the plate rolls against a cylinder covered with a rubber "blanket", which squeezes away the water, picks up the ink and transfers it to the paper with uniform pressure. The paper passes between the blanket cylinder and a counter-pressure or impression cylinder and the image is transferred to the paper. Because the image is first transferred, or "offset" to the rubber blanket cylinder, this reproduction method is known as "offset lithography" or "offset printing".

Many innovations and technical refinements have been made in printing processes and presses over the years, including the development of presses with multiple units (each containing one printing plate) that can print multi-color images in one pass on both sides of the sheet, and presses that accommodate continuous rolls ("webs") of paper, known as web presses. Another innovation was the continuous dampening system first introduced by Dahlgren, instead of the old method (conventional dampening) which is still used on older presses, using rollers covered with molleton (cloth) that absorbs the water. This increased control of the water flow to the plate and allowed for better ink and water balance. Current dampening systems include a "delta effect or vario", which slows the roller in contact with the plate, thus creating a sweeping movement over the ink image to clean impurities known as "hickies".

This press is also called an ink pyramid because the ink is transferred through several layers of rollers with different purposes. Fast lithographic 'web' printing presses are commonly used in newspaper production.

The advent of desktop publishing made it possible for type and images to be modified easily on personal computers for eventual printing by desktop or commercial presses. The development of digital imagesetters enabled print shops to produce negatives for platemaking directly from digital input, skipping the intermediate step of photographing an actual page layout. The development of the digital platesetter during the late 20th century eliminated film negatives altogether by exposing printing plates directly from digital input, a process known as computer to plate printing.

Microlithography and nanolithography refer specifically to lithographic patterning methods capable of structuring material on a fine scale. Typically, features smaller than 10 micrometers are considered microlithographic, and features smaller than 100 nanometers are considered nanolithographic. Photolithography is one of these methods, often applied to semiconductor device fabrication. Photolithography is also commonly used for fabricating microelectromechanical systems (MEMS) devices. Photolithography generally uses a pre-fabricated photomask or reticle as a master from which the final pattern is derived.

Although photolithographic technology is the most commercially advanced form of nanolithography, other techniques are also used. Some, for example electron beam lithography, are capable of much greater patterning resolution (sometimes as small as a few nanometers). Electron beam lithography is also important commercially, primarily for its use in the manufacture of photomasks. Electron beam lithography as it is usually practiced is a form of maskless lithography, in that a mask is not required to generate the final pattern. Instead, the final pattern is created directly from a digital representation on a computer, by controlling an electron beam as it scans across a resist-coated substrate. Electron beam lithography has the disadvantage of being much slower than photolithography.

In addition to these commercially well-established techniques, a large number of promising microlithographic and nanolithographic technologies exist or are being developed, including nanoimprint lithography, interference lithography, X-ray lithography, extreme ultraviolet lithography, magnetolithography and scanning probe lithography. Some of these new techniques have been used successfully for small-scale commercial and important research applications.
Surface-charge lithography, in fact Plasma desorption mass spectrometry can be directly patterned on polar dielectric crystals via pyroelectric effect, 
Diffraction lithography.

During the first years of the 19th century, lithography had only a limited effect on printmaking, mainly because technical difficulties remained to be overcome. Germany was the main center of production in this period. Godefroy Engelmann, who moved his press from Mulhouse to Paris in 1816, largely succeeded in resolving the technical problems, and during the 1820s lithography was adopted by artists such as Delacroix and Géricault. After early experiments such as "Specimens of Polyautography" (1803), which had experimental works by a number of British artists including Benjamin West, Henry Fuseli, James Barry, Thomas Barker of Bath, Thomas Stothard, Henry Richard Greville, Richard Cooper, Henry Singleton, and William Henry Pyne, London also became a center, and some of Géricault's prints were in fact produced there. Goya in Bordeaux produced his last series of prints by lithography—"The Bulls of Bordeaux" of 1828. By the mid-century the initial enthusiasm had somewhat diminished in both countries, although the use of lithography was increasingly favored for commercial applications, which included the prints of Daumier, published in newspapers. Rodolphe Bresdin and Jean-François Millet also continued to practice the medium in France, and Adolf Menzel in Germany. In 1862 the publisher Cadart tried to initiate a portfolio of lithographs by various artists, which was not successful but included several prints by Manet. The revival began during the 1870s, especially in France with artists such as Odilon Redon, Henri Fantin-Latour and Degas producing much of their work in this manner. The need for strictly limited editions to maintain the price had now been realized, and the medium became more accepted.
In the 1890s, color lithography gained success in part by the emergence of Jules Chéret, known as the "father of the modern poster", whose work went on to inspire a new generation of poster designers and painters, most notably Toulouse-Lautrec, and former student of Chéret, Georges de Feure. By 1900 the medium in both color and monotone was an accepted part of printmaking.

During the 20th century, a group of artists, including Braque, Calder, Chagall, Dufy, Léger, Matisse, Miró, and Picasso, rediscovered the largely undeveloped artform of lithography thanks to the Mourlot Studios, also known as "Atelier Mourlot", a Parisian printshop founded in 1852 by the Mourlot family. The Atelier Mourlot originally specialized in the printing of wallpaper; but it was transformed when the founder's grandson, Fernand Mourlot, invited a number of 20th-century artists to explore the complexities of fine art printing. Mourlot encouraged the painters to work directly on lithographic stones in order to create original artworks that could then be executed under the direction of master printers in small editions. The combination of modern artist and master printer resulted in lithographs that were used as posters to promote the artists' work.

Grant Wood, George Bellows, Alphonse Mucha, Max Kahn, Pablo Picasso, Eleanor Coen, Jasper Johns, David Hockney, Susan Dorothea White, and Robert Rauschenberg are a few of the artists who have produced most of their prints in the medium. M. C. Escher is considered a master of lithography, and many of his prints were created using this process. More than other printmaking techniques, printmakers in lithography still largely depend on access to good printers, and the development of the medium has been greatly influenced by when and where these have been established.
As a special form of lithography, the serilith process is sometimes used. Seriliths are mixed media original prints created in a process in which an artist uses the lithograph and serigraph processes. The separations for both processes are hand-drawn by the artist. The serilith technique is used primarily to create fine art limited print editions.



</doc>
<doc id="18430" url="https://en.wikipedia.org/wiki?curid=18430" title="Library management">
Library management

Library management is a sub-discipline of institutional management that focuses on specific issues faced by libraries and library management professionals. Library management encompasses normal managerial tasks, as well as intellectual freedom and fundraising responsibilities. Issues faced in library management frequently overlap with those faced in managing non-profit organizations.

The basic functions of library management include, but are not limited to: planning and negotiating the acquisition of materials, Interlibrary Loan (ILL) requests, stacks maintenance, overseeing fee collection, event planning, fundraising, and human resources.
Most of the libraries that store physical media like books, periodicals, film, and other objects adhere to some derivative of the Dewey Decimal System as their method for tagging, storing, and retrieving materials based on unique identifiers. The use of such systems have caused librarians to develop and leverage common constructs that act as tools for both library professionals and library users alike. These constructs include master catalogs, domain catalogs, indexes, unique identifiers, unique identifier tokens, and artifacts .







An important aspect of library management is planning and maintaining library facilities. Successful planning is defined as "active planning that ensures an organization will have the right people in the right place at the right time for right job" Planning the construction of new libraries or remodeling those that exist is integral since user needs are often changing. To supplement their operating budget, managers often secure funding through donor gifts and fundraising. Many facilities have begun including cafes, Friends of the Library spaces, and even exhibits to help generate additional revenue. These areas should be taken into account when planning for building expansions.

The site for new construction must be found, then the building must be designed, constructed, and eventually evaluated. Once established, it is important that the building is regularly maintained. This may be completed by delegating tasks to maintenance personnel or by hiring an outside company through bids.

Disaster planning must be taken into account in the library context as well: not only the impact of a disaster on the library, but the library's potential role as a support service just after a disaster.

The Library Leadership and Management Association (LLAMA) is a division of the American Library Association that provides leaders with webinars, conferences, and a variety of industry publications, in addition to funding through awards and grants. LLAMA membership includes a free subscription to the online quarterly magazine "Library Leadership & Management", as well as discounts on other publications and related conferences.

The "Journal of Library Administration" began in 1980 and is currently published by Routledge eight times per year. It is a peer-reviewed academic journal that discusses issues pertaining to library management.




</doc>
<doc id="18432" url="https://en.wikipedia.org/wiki?curid=18432" title="English longbow">
English longbow

The English longbow was a powerful medieval type of longbow (a tall bow for archery) about long used by the English and Welsh for hunting and as a weapon in warfare. English use of longbows was effective against the French during the Hundred Years' War, particularly at the start of the war in the battles of Sluys (1340), Crécy (1346), and Poitiers (1356), and perhaps most famously at the Battle of Agincourt (1415). 
However they were less successful after this, with longbowmen having their lines broken at the Battle of Verneuil (1424) though the English won a decisive victory, and being completely routed at the Battle of Patay (1429) when they were charged by the French mounted men-at-arms before they had prepared the terrain and finished defensive arrangements. The Battle of Pontvallain (1370) had also previously shown longbowmen were not particularly effective when not given the time to set up defensive positions.

No English longbows survive from the period when the longbow was dominant (c. 1250–1450 AD), probably because bows became weaker, broke, and were replaced rather than being handed down through generations. More than 130 bows survive from the Renaissance period, however. More than 3,500 arrows and 137 whole longbows were recovered from the "Mary Rose", a ship of Henry VIII's navy that sank at Portsmouth in 1545.

A longbow must be long enough to allow its user to draw the string to a point on the face or body, and the length therefore varies with the user. In continental Europe it was generally seen as any bow longer than . The Society of Antiquaries of London says it is of in length. Richard Bartelot, of the Royal Artillery Institution, said that the bow was of yew, long, with a arrow. Gaston III, Count of Foix, wrote in 1388 that a longbow should be "of yew or boxwood, seventy inches [] between the points of attachment for the cord". Historian Jim Bradbury said they were an average of about 5 feet and 8 inches. All but the last estimate were made before the excavation of the "Mary Rose", where bows were found ranging in length from with an average length of .

Estimates for the draw of these bows varies considerably. Before the recovery of the "Mary Rose", Count M. Mildmay Stayner, Recorder of the British Long Bow Society, estimated the bows of the Medieval period drew , maximum, and W. F. Paterson, Chairman of the Society of Archer-Antiquaries, believed the weapon had a supreme draw weight of only . Other sources suggest significantly higher draw weights. The original draw forces of examples from the "Mary Rose" are estimated by Robert Hardy at at a draw length; the full range of draw weights was between . The draw length was used because that is the length allowed by the arrows commonly found on the "Mary Rose".

A modern longbow's draw is typically or less, and by modern convention measured at . Historically, hunting bows usually had draw weights of , which is enough for all but the very largest game and which most reasonably fit adults can manage with practice. Today, there are few modern longbowmen capable of using bows accurately.

A record of how boys and men trained to use the bows with high draw weights survives from the reign of Henry VII.
What Latimer meant when he describes laying his body into the bow was described thus:

The preferred material to make the longbow was yew, although ash, elm and other woods were also used. Gerald of Wales speaking of the bows used by the Welsh men of Gwent, says: "They are made neither of horn, ash nor yew, but of elm; ugly unfinished-looking weapons, but astonishingly stiff, large and strong, and equally capable of use for long or short shooting". The traditional construction of a longbow consists of drying the yew wood for 1 to 2 years, then slowly working the wood into shape, with the entire process taking up to four years. (This can be done far more quickly by working the wood down when wet, as a thinner piece of wood will dry much faster.) The bow stave is shaped into a D-section. The outer "back" of sapwood, approximately flat, follows the natural growth rings; modern bowyers often thin the sapwood, while in the "Mary Rose" bows the back of the bow was the natural surface of the wood, only the bark is removed. The inner side ("belly") of the bow stave consists of rounded heartwood. The heartwood resists compression and the outer sapwood performs better in tension. This combination in a single piece of wood (a self bow) forms a natural "laminate", somewhat similar in effect to the construction of a composite bow. Longbows will last a long time if protected with a water-resistant coating, traditionally of "wax, resin and fine tallow".

The trade of yew wood to England for longbows was such that it depleted the stocks of yew over a huge area. The first documented import of yew bowstaves to England was in 1294. In 1470 compulsory practice was renewed, and hazel, ash, and laburnum were specifically allowed for practice bows. Supplies still proved insufficient, until by the Statute of Westminster 1472, every ship coming to an English port had to bring four bowstaves for every tun. Richard III of England increased this to ten for every tun. This stimulated a vast network of extraction and supply, which formed part of royal monopolies in southern Germany and Austria. In 1483, the price of bowstaves rose from two to eight pounds per hundred, and in 1510 the Venetians obtained sixteen pounds per hundred.

In 1507 the Holy Roman Emperor asked the Duke of Bavaria to stop cutting yew, but the trade was profitable, and in 1532 the royal monopoly was granted for the usual quantity "if there are that many". In 1562, the Bavarian government sent a long plea to the Holy Roman Emperor asking him to stop the cutting of yew and outlining the damage done to the forests by its selective extraction, which broke the canopy and allowed wind to destroy neighbouring trees. In 1568, despite a request from Saxony, no royal monopoly was granted because there was no yew to cut, and the next year Bavaria and Austria similarly failed to produce enough yew to justify a royal monopoly.

Forestry records in this area in the 17th century do not mention yew, and it seems that no mature trees were to be had. The English tried to obtain supplies from the Baltic, but at this period bows were being replaced by guns in any case.

Bowstrings are made of hemp, flax or silk, and attached to the wood via horn "nocks" that fit onto the end of the bow. Modern synthetic materials (often Dacron) are now commonly also used for strings.

A wide variety of arrows were shot from the English longbow. Variations in length, fletchings and heads are all recorded. Perhaps the greatest diversity lies in hunting arrows, with varieties like broad-arrow, wolf-arrow, dog-arrow, Welsh arrow and Scottish arrow being recorded. War arrows were ordered in the thousands for medieval armies and navies, supplied in sheaves normally of 24 arrows. For example, between 1341 and 1359 the English crown is known to have obtained 51,350 sheaves (1,232,400 arrows).

Only one significant group of arrows, found at the wreck of the "Mary Rose", has survived. Over 3500 arrows were found, mainly made of poplar but also of ash, beech and hazel. Analysis of the intact specimens shows their length to vary from , with an average length of . Because of the preservation conditions of the "Mary Rose", no arrowheads survived. However, many heads have survived in other places, which has allowed typologies of arrowheads to be produced, the most modern being the Jessop typology. The most common arrowheads in military use were the short bodkin point (Jessop M10) and a small barbed arrow (Jessop M4).

Longbows were very difficult to master because the force required to deliver an arrow through the improving armour of medieval Europe was very high by modern standards. Although the draw weight of a typical English longbow is disputed, it was at least and possibly more than . Considerable practice was required to produce the swift and effective combat shooting required. Skeletons of longbow archers are recognisably affected, with enlarged left arms and often osteophytes on left wrists, left shoulders and right fingers.

It was the difficulty in using the longbow that led various monarchs of England to issue instructions encouraging their ownership and practice, including the Assize of Arms of 1252 and Edward III of England's declaration of 1363:

If the people practised archery, it would be that much easier for the king to recruit the proficient longbowmen he needed for his wars. Along with the improving ability of gunfire to penetrate plate armour, it was the long training needed by longbowmen that eventually led to their being replaced by musketeers.

The range of the medieval weapon is not accurately known, with much depending on both the power of the bow and the type of arrow. It has been suggested that a flight arrow of a professional archer of Edward III's time would reach but the longest mark shot at on the London practice ground of Finsbury Fields in the 16th century was . In 1542, Henry VIII set a minimum practice range for adults using flight arrows of ; ranges below this had to be shot with heavy arrows. Modern experiments broadly concur with these historical ranges. A 667 N (150 lbf) "Mary Rose" replica longbow was able to shoot a arrow and a a distance of . In 2012, Joe Gibbs shot a livery arrow with a 170 lbf yew bow. The effective combat range of longbowmen was generally lower than what could be achieved on the practice range as sustained shooting was tiring and the rigors of campaigning would sap soldiers' strength. Writing 30 years after the Mary Rose sank, Barnabe Rich estimated that if 1,000 English archers were mustered then after one week only 100 of them would be able to shoot farther than 200 paces, while 200 would not be able to shoot farther than 180 paces.

In an early modern test by Saxton Pope, a direct hit from a steel bodkin point penetrated Damascus mail armour.

A 2006 test was made by Matheus Bane using a draw (at 28") bow, shooting at 10 yards; according to Bane's calculations, this would be approximately equivalent to a bow at 250 yards. Measured against a replica of the thinnest contemporary gambeson (padded jacket) armour, a 905 grain needle bodkin and a 935 grain curved broadhead penetrated over . (gambeson armour could be up to twice as thick as the coat tested; in Bane's opinion such a thick coat would have stopped bodkin arrows but not the cutting force of broadhead arrows.) Against "high quality riveted maille", the needle bodkin and curved broadhead penetrated 2.8". Against a coat of plates, the needle bodkin achieved 0.3" penetration. The curved broadhead did not penetrate but caused 0.3" of deformation of the metal. Results against plate armour of "minimum thickness" (1.2mm) were similar to the coat of plates, in that the needle bodkin penetrated to a shallow depth, the other arrows not at all. In Bane's view, the plate armour would have kept out all the arrows if thicker or worn with more padding.

Other modern tests described by Bane include those by Williams (which concluded that longbows could "not" penetrate mail, but in Bane's view did not use a realistic arrow tip), Robert Hardy's tests (which achieved broadly similar results to Bane), and a "Primitive Archer" test which demonstrated that a longbow could penetrate a plate armour breastplate. However, the "Primitive Archer" test used a longbow at very short range, generating 160 joules (vs. 73 for Bane and 80 for Williams), so probably not representative of battles of the time.

Tests conducted by Mark Stretton examined the effects of heavier war shafts (as opposed to lighter hunting or distance-shooting 'flight arrows'). The quarrel-like 102-gram arrow from a yew 'self bow' (with a draw weight of 144lbs at 32 inches) while travelling at 47.23 metres per second yielded 113.76 joules, more kinetic energy than the lighter broad-heads while achieving 90% of the range. The short, heavy quarrel-form bodkin could penetrate a replica brigandine at up to 40° from perpendicular.

In 2011, Mike Loades conducted an experiment in which short bodkin arrows were shot at a range of by bows of - powerful bows at less than normal battlefield range. The target was covered in a riveted mail over a fabric armour of deerskin over 24 linen layers. While most arrows went through the mail layer, none fully penetrated the textile armour.

Other research has also concluded that later medieval armour, such as that of the Italian city-state mercenary companies, was effective at stopping contemporary arrows.

Computer analysis by Warsaw University of Technology in 2017 has estimated that heavy bodkin-point arrows could penetrate typical plate armour of the time at up to . However, the depth of penetration would be slight at that range, a mere 14mm on average; penetration increased as the range closed or against armour lesser than the best quality available at the time, but with 24mm being the highest penetration depth estimated at 25 m range, it was unlikely to be deadly.

In August 2019, Blacksmith and Youtuber Tod from Tod's Workshop together with historian Dr Tobias Capwell (curator at the Wallace collection), Joe Gibbs (Archer), Will Sherman (Fletcher) and Kevin Legg (armourer) ran a practical test using as close a recreation of 15th century plate armour (made with materials and techniques fitting to the time period) and a 160 lps longbow.
They fired a variety of arrows at the target and the results showed that the arrows shot by a 160 lbs longbow were unable to penetrate the front of the armour at any range.
Gerald of Wales commented on the power of the Welsh longbow in the 12th century:
Against massed men in armour, massed longbows were murderously effective on many battlefields.

Strickland and Hardy suggest that "even at a range of 240 yards, heavy war arrows shot from bows of poundages in the mid- to upper range possessed by the Mary Rose bows would have been capable of killing or severely wounding men equipped with armour of wrought iron. Higher-quality armour of steel would have given considerably greater protection, which accords well with the experience of Oxford's men against the elite French vanguard at Poitiers in 1356, and des Ursin's statement that the French knights of the first ranks at Agincourt, which included some of the most important (and thus best-equipped) nobles, remained comparatively unhurt by the English arrows".

Archery was described by contemporaries as ineffective against steel plate armour in the Battle of Neville's Cross (1346), the siege of Bergerac (1345), and the Battle of Poitiers (1356); such armour became available to European knights and men at arms of fairly modest means by the middle of the 14th century, though never to all soldiers in any army. Longbowmen were, however, effective at Poitiers, and this success stimulated changes in armour manufacture partly intended to make armoured men less vulnerable to archery. Nevertheless, at the battle of Agincourt in 1415 and for some decades thereafter, English longbowmen continued to be an effective battlefield force.

Following the Battle of Crécy, the longbow did not always prove as effective. For example, at the Battle of Poitiers (1356), the French men-at-arms formed a shield wall with which Geoffrey le Baker recounts "protecting their bodies with joined shields, [and] turned their faces away from the missiles. So the archers emptied their quivers in vain".

Modern tests and contemporary accounts agree therefore that well-made plate armour could protect against longbows. However, this did not necessarily make the longbow ineffective; thousands of longbowmen were deployed in the English victory at Agincourt against plate armoured French knights in 1415. Clifford Rogers has argued that while longbows might not have been able to penetrate steel breastplates at Agincourt they could still penetrate the thinner armour on the limbs. Most of the French knights advanced on foot but, exhausted by walking across wet muddy terrain in heavy armour enduring a "terrifying hail of arrow shot", they were overwhelmed in the melee.

Less heavily armoured soldiers were more vulnerable than knights. For example, enemy crossbowmen were forced to retreat at Crécy when deployed without their protecting pavises. Horses were generally less well protected than the knights themselves; shooting the French knights' horses from the side (where they were less well armoured) is described by contemporary accounts of the Battle of Poitiers (1356), and at Agincourt John Keegan has argued that the main effect of the longbow would have been in injuring the horses of the mounted French knights.

A typical military longbow archer would be provided with between 60 and 72 arrows at the time of battle. Most archers would not shoot arrows at the maximum rate, as it would exhaust even the most experienced man. "With the heaviest bows [a modern war bow archer] does not like to try for more than six a minute." Not only do the arms and shoulder muscles tire from the exertion, but the fingers holding the bowstring become strained; therefore, actual rates of shooting in combat would vary considerably. Ranged volleys at the beginning of the battle would differ markedly from the closer, aimed shots as the battle progressed and the enemy neared. On the battlefield English archers stored their arrows stabbed upright into the ground at their feet, reducing the time it took to nock, draw and loose.

Arrows were not unlimited, so archers and their commanders took every effort to ration their use to the situation at hand. Nonetheless, resupply during battle was available. Young boys were often employed to run additional arrows to longbow archers while in their positions on the battlefield. "The longbow was the machine gun of the Middle Ages: accurate, deadly, possessed of a long range and rapid rate of fire, the flight of its missiles was likened to a storm".

In tests against a moving target simulating a galloping knight it took some approximately seven seconds to draw, aim and loose an armour-piercing heavy arrow using a replica war bow. It was found that in the seven seconds between the first and second shots the target advanced 70 yards and that the second shot occurred at such close range that, if it was a realistic contest, running away was the only option.

A Tudor English author expects eight shots from a longbow in the same time as five from a musket. He points out that the musket also shoots at a flatter trajectory, so is more likely to hit its target and its shot is likely to be more damaging in the event of a hit. The advantage of early firearms lay in the lower training requirements, the opportunity to take cover while shooting, flatter trajectory, and greater penetration.

The only way to remove an arrow cleanly was to tie a piece of cloth soaked in water to the end of it and push it through the victim's wound and out the other side this was extremely painful. Specialised tools have existed since ancient times: Diocles (successor of Hippocrates) devised the graphiscos, a form of cannula with hooks, and the duck-billed forceps (allegedly invented by Heras of Cappadocia) was employed during the medieval period to extract arrows from places where bone prevented the arrow being pushed through.

Henry, Prince of Wales, later Henry V, was wounded in the face by an arrow at the Battle of Shrewsbury (1403). The royal physician John Bradmore had a tool made that consisted of a pair of smooth tongs. Once carefully inserted into the socket of the arrowhead, the tongs screwed apart until they gripped its walls and allowed the head to be extracted from the wound. Prior to the extraction, the hole made by the arrow shaft was widened by inserting larger and larger dowels of elder pith wrapped in linen down into the entry wound. The dowels were soaked in honey, now known to have antiseptic properties. The wound was then dressed with a poultice of barley and honey mixed in turpentine (pre-dating Ambroise Paré but whose therapeutic use of turpentine was inspired by Roman medical texts that may have been familiar to Bradmore). After 20 days, the wound was free of infection.

The word may have been coined to distinguish the longbow from the crossbow. The first recorded use of the term "longbow", as distinct from simply 'bow', is possibly in a 1386 administrative document which refers in Latin to "arcus vocati longbowes", "bows called 'longbows'", though unfortunately the reading of the last word in the original document is not certain. A 1444 will proved in York bequeaths "a sadil, alle my longe bowis, a bedde".

The origins of the English longbow are disputed. While it is hard to assess the significance of military archery in pre-Norman Conquest Anglo-Saxon warfare, it is clear that archery played a prominent role under the Normans, as the story of the Battle of Hastings shows. Their Anglo-Norman descendants also made use of military archery, as exemplified by their victory at the Battle of the Standard in 1138. During the Anglo-Norman invasions of Wales, Welsh bowmen took a heavy toll of the invaders and Welsh archers would feature in English armies from this point on. However, historians dispute whether this archery used a different kind of bow to the later English Longbow. Traditionally it has been argued that prior to the beginning of the 14th century, the weapon was a self bow between four and five feet in length, known since the 19th century as the shortbow. This weapon, drawn to the chest rather than the ear, was much weaker. However, in 1985, Jim Bradbury reclassified this weapon as the "ordinary wooden bow", reserving the term shortbow for short composite bows and arguing that longbows were a developed form of this ordinary bow. Strickland and Hardy in 2005 took this argument further, suggesting that the shortbow was a myth and all early English bows were a form of longbow. In 2011, Clifford Rogers forcefully restated the traditional case based upon a variety of evidence, including a large scale iconographic survey. In 2012, Richard Wadge added to the debate with an extensive survey of record, iconographic and archaeological evidence, concluding that longbows co-existed with shorter self-wood bows in England in the period between the Norman conquest and the reign of Edward III, but that powerful longbows shooting heavy arrows were a rarity until the later 13th century. Whether or not there was a technological revolution at the end of the 13th century therefore remains in dispute. What is agreed, however, is that the English longbow as an effective weapon system evolved in the late 13th and early 14th centuries.

The longbow decided many medieval battles fought by the English and Welsh, the most significant of which were the Battle of Crécy (1346) and the Battle of Agincourt (1415), during the Hundred Years' War and followed earlier successes, notably at the Battle of Falkirk (1298) and the Battle of Halidon Hill (1333) during the Wars of Scottish Independence. They were less successful after this, with longbowmen having their lines broken at the Battle of Verneuil (1424), and being routed at the Battle of Patay (1429) when they were charged before they had set up their defences, and with the war-ending Battle of Castillon (1453) being decided by the French artillery.

The longbow was also used against the English by their Welsh neighbours. The Welsh used the longbow mostly in a different manner than the English. In many early period English campaigns, the Welsh used the longbow in ambushes, often at point blank range that allowed their missiles to penetrate armour and generally do a lot of damage.

Although longbows were much faster and more accurate than the black-powder weapons which replaced them, longbowmen always took a long time to train because of the years of practice necessary before a war longbow could be used effectively (examples of longbows from the "Mary Rose" typically had draws greater than ). In an era in which warfare was usually seasonal, and non-noble soldiers spent part of the year working at farms, the year-round training required for the effective use of the longbow was a challenge. A standing army was an expensive proposition to a medieval ruler. Mainland European armies seldom trained a significant longbow corps. Due to their specialized training, English longbowmen were sought as mercenaries in other European countries, most notably in the Italian city-states and in Spain.
The White Company, comprising men-at-arms and longbowmen and commanded by Sir John Hawkwood, is the best known English Free Company of the 14th century. The powerful Hungarian king, Louis the Great, is an example of someone who used longbowmen in his Italian campaigns.

Longbows remained in use until around the 16th century, when advances in firearms made gunpowder weapons a significant factor in warfare and such units as arquebusiers and grenadiers began appearing. Despite this, the English Crown made numerous efforts to continue to promote archery practice by banning other sports and fining people for not possessing bows. Indeed, just before the English Civil War, a pamphlet by William Neade entitled "The Double-Armed Man" advocated that soldiers be trained in both the longbow and pike; although this advice was followed only by a few town militias.

The Battle of Flodden (1513) was "a landmark in the history of archery, as the last battle on English soil to be fought with the longbow as the principal weapon..." The last recorded use of bows in an English battle may have been a skirmish at Bridgnorth, in October 1642, during the Civil War, when an impromptu town militia, armed with bows, proved effective against un-armoured musketeers. Longbowmen remained a feature of the Royalist Army, but were not used by the Roundheads.

Longbows have been in continuous production and use for sport and for hunting to the present day, but since 1642 they have been a minority interest, and very few have had the high draw weights of the medieval weapons. Other differences include the use of a stiffened non-bending centre section, rather than a continuous bend.

Serious military interest in the longbow faded after the seventeenth century but occasionally schemes to resurrect its military use were proposed. Benjamin Franklin was a proponent in the 1770s; the Honourable Artillery Company had an archer company between 1784 and 1794, and a man named Richard Mason wrote a book proposing the arming of militia with pike and longbow in 1798. Donald Featherstone also records a Lt. Col. Richard Lee of 44th Foot advocated the military use of the longbow in 1792. There is a record of the use of the longbow in action as late as WWII, when Jack Churchill is credited with a longbow kill in France in 1940. The weapon was certainly considered for use by Commandos during the war but it is not known whether it was used in action.

The idea that there was a standard formation for English longbow armies was argued by Alfred Byrne in his influential work on the battles of the Hundred Years' War, "The Crecy War". This view was challenged by Jim Bradbury in his book "The Medieval Archer" and more modern works are more ready to accept a variety of formations.

In summary, however, the usual English deployment in the 14th and 15th centuries was as follows:

In the 16th century, these formations evolved in line with new technologies and techniques from the continent. Formations with a central core of pikes and bills were flanked by companies of "shot" made up of a mixture of archers and arquebusiers, sometimes with a skirmish screen of archers and arquebusiers in front.

More than 3,500 arrows and 137 whole longbows were recovered from the "Mary Rose", a ship of Henry VIII's navy that capsized and sank at Portsmouth in 1545. It is an important source for the history of the longbow, as the bows, archery implements and the skeletons of archers have been preserved. The bows range in length from with an average length of . The majority of the arrows were made of poplar, others were made of beech, ash and hazel. Draw lengths of the arrows varied between with the majority having a draw length of . The head would add 5–15 cm depending on type, though some 2–4.5 cm must be allowed for the insertion of the shaft into the socket.

The longbows on the "Mary Rose" were in excellent finished condition. There were enough bows to test some to destruction which resulted in draw forces of 450 N (100 lbf) on average. However, analysis of the wood indicated that they had degraded significantly in the seawater and mud, which had weakened their draw forces. Replicas were made and when tested had draw forces of from 445 N to 823 N (100 to 185 lbf).

In 1980, before the finds from the "Mary Rose", Robert E. Kaiser published a paper stating that there were five known surviving longbows:


The importance of the longbow in English culture can be seen in the legends of Robin Hood, which increasingly depicted him as a master archer, and also in the "Song of the Bow", a poem from "The White Company" by Sir Arthur Conan Doyle.

During the reign of Henry III the Assize of Arms of 1252 required that all "citizens, burgesses, free tenants, villeins and others from 15 to 60 years of age" should be armed. The poorest of them were expected to have a halberd and a knife, and a bow if they owned land worth more than £2. This made it easier for the King to raise an army, but also meant that the bow was a weapon commonly used by rebels during the Peasants' Revolt. From the time that the yeoman class of England became proficient with the longbow, the nobility in England had to be careful not to push them into open rebellion.

It has been conjectured that yew trees were commonly planted in English churchyards to have readily available longbow wood.









</doc>
<doc id="18433" url="https://en.wikipedia.org/wiki?curid=18433" title="Lee Marvin">
Lee Marvin

Lee Marvin (February 19, 1924 – August 29, 1987), was an American film and television actor.

Known for his distinctive voice and premature white hair, Marvin initially appeared in supporting roles, mostly villains, soldiers, and other hardboiled characters. A prominent television role was that of Detective Lieutenant Frank Ballinger in the crime series "M Squad" (1957–1960). Marvin is best remembered for his lead roles as "tough guy" characters such as Charlie Strom in "The Killers" (1964), Rico Fardan in "The Professionals" (1966), Major John Reisman in "The Dirty Dozen", Walker in "Point Blank" (both 1967), and the Sergeant in "The Big Red One" (1980).

One of Marvin's more notable movie projects was "Cat Ballou" (1965), a comedy Western in which he played dual roles. For portraying both gunfighter Kid Shelleen and criminal Tim Strawn, he won the Academy Award for Best Actor, along with a BAFTA Award, a Golden Globe Award, an NBR Award, and the Silver Bear for Best Actor.

Lee Marvin was born to Lamont Waltman Marvin on February 19, 1924, in New York City. As with his elder brother, Robert (1922–1999), he was named in honor of Confederate General Robert E. Lee, who was his first cousin, four times removed. His father was a direct descendant of Matthew Marvin Sr., who emigrated from Great Bentley, Essex, England in 1635, and helped found Hartford, Connecticut.
Marvin studied violin when he was young. As a teenager, Marvin "spent weekends and spare time hunting deer, puma, wild turkey, and bobwhite in the wilds of the then-uncharted Everglades".

He attended Manumit School, a Christian socialist boarding school in Pawling, New York, during the late 1930s, and later attended St. Leo College Preparatory School, a Catholic school in St. Leo, Florida after being expelled from several other schools for bad behavior.

Marvin left school at 18 to enlist in the United States Marine Corps Reserve on August 12, 1942. He served with the 4th Marine Division in the Pacific Theater during World War II. While serving as a member of "I" Company, 3rd Battalion, 24th Marines, 4th Marine Division, he was wounded in action on June 18, 1944, during the assault on Mount Tapochau in the Battle of Saipan, during which most of his company were casualties. He was hit by machine gun fire, which severed his sciatic nerve, and then was hit again in the foot by a sniper. After over a year of medical treatment in naval hospitals, Marvin was given a medical discharge with the rank of private first class. He previously held the rank of corporal, but had been demoted for troublemaking.

Marvin's military awards include the Purple Heart Medal, the Presidential Unit Citation, the American Campaign Medal, the Asiatic-Pacific Campaign Medal, the World War II Victory Medal, and the Combat Action Ribbon.

After the war, while working as a plumber's assistant at a local community theatre in upstate New York, Marvin was asked to replace an actor who had fallen ill during rehearsals. He caught the acting bug and got a job with the company at $7 a week. He moved to Greenwich Village and used the GI Bill to study at the American Theatre Wing.

He appeared on stage in a production of "Uniform of Flesh", an adaptation of "Billy Budd" (1949). It was done at the Experimental Theatre, where a few months later Marvin also appeared in "The Nineteenth Hole of Europe" (1949).

Marvin began appearing on television shows like "Escape", "The Big Story", and "Treasury Men in Action".

He made it to Broadway with a small role in a production of "Uniform of Flesh", now titled "Billy Budd" in February 1951.

Marvin's film debut was in "You're in the Navy Now" (1951), directed by Henry Hathaway, a movie that marked the debuts of Charles Bronson and Jack Warden. This required some filming in Hollywood. Marvin decided to stay there.

He had a similar small part in "Teresa" (1951) directed by Fred Zinnemann. As a decorated combat veteran, Marvin was a natural in war dramas, where he frequently assisted the director and other actors in realistically portraying infantry movement, arranging costumes, and the use of firearms.

He guest starred on episodes of "Fireside Theatre", "Suspense" and "Rebound". Hathaway used him again on "Diplomatic Courier" (1952) and he could be seen in "Down Among the Sheltering Palms" (1952), directed by Edmund Goulding, "We're Not Married!" (1952), also for Goulding, "The Duel at Silver Creek" (1952) directed by Don Siegel, and "Hangman's Knot" (1952), directed by Roy Huggins.

He guest starred on "Biff Baker, U.S.A." and "Dragnet", and had a decent role in a feature with "Eight Iron Men" (1952), a war film produced by Stanley Kramer (Marvin's role had been played on Broadway by Burt Lancaster).

He was a sergeant in "Seminole" (1953), a Western directed by Budd Boetticher, and was a corporal in "The Glory Brigade" (1953), a Korean War film.

Marvin guest starred in "The Doctor", "The Revlon Mirror Theater ", "Suspense" again and "The Motorola Television Hour".

He was now in much demand for Westerns: "The Stranger Wore a Gun" (1953) with Randolph Scott, and "Gun Fury" (1953) with Rock Hudson.

Marvin received much acclaim for his portrayal as villains in two films: "The Big Heat" (1953) where he played Gloria Grahame's vicious boyfriend, directed by Fritz Lang; and "The Wild One" (1953) opposite Marlon Brando (Marvin's gang in the film was named "The Beetles"), produced by Kramer.

He continued in TV shows such as "The Plymouth Playhouse" and "The Pepsi-Cola Playhouse". He had support roles in "Gorilla at Large" (1954) and had a notable small role as smart-aleck sailor Meatball in "The Caine Mutiny" (1954), produced by Kramer.

Marvin was in "The Raid" (1954), "Center Stage", "Medic" and "TV Reader's Digest".

He had an excellent part as Hector, the small-town hood in "Bad Day at Black Rock" (1955) with Spencer Tracy. Also in 1955, he played a conflicted, brutal bank-robber in "Violent Saturday". A latter-day critic wrote of the character, "Marvin brings a multi-faceted complexity to the role and gives a great example of the early promise that launched his long and successful career."

Marvin played Robert Mitchum's friend in "Not as a Stranger" (1955), a medical drama produced by Kramer. He had good supporting roles in "A Life in the Balance" (1955) (he was third billed), and "Pete Kelly's Blues" (1955) and appeared on TV in "Jane Wyman Presents The Fireside Theatre" and "Studio One in Hollywood."

Marvin was in "I Died a Thousand Times" (1955) with Jack Palance, "Shack Out on 101" (1955), "Kraft Theatre", and "Front Row Center."
Marvin was the villain in "7 Men from Now" (1956) with Randolph Scott directed by Boetticher. He was second-billed to Palance in "Attack" (1956) directed by Robert Aldrich.

Marvin had good roles in "Pillars of the Sky" (1956) with Jeff Chandler, "The Rack" (1956) with Paul Newman, "Raintree County" (1956) and "The Missouri Traveler" (1958). He also guest starred on "Climax!" (several times), "Studio 57", "The United States Steel Hour" and "Schlitz Playhouse".

Marvin finally got to be a leading man in 100 episodes as Chicago cop Frank Ballinger in the successful 1957–1960 television series "M Squad". One critic described the show as "a hyped-up, violent "Dragnet" ... with a hard-as-nails Marvin" playing a tough police lieutenant. Marvin received the role after guest-starring in a memorable "Dragnet" episode as a serial killer.

When the series ended Marvin appeared on "Westinghouse Desilu Playhouse", "Sunday Showcase", "The Barbara Stanwyck Show", "The Americans", "Wagon Train", "Checkmate", "General Electric Theater, Alcoa Premiere", "The Investigators", "Route 66" (he was injured during a fight scene), Ben Casey, "Bonanza", "The Untouchables" (several times), "The Virginian", "The Twilight Zone" ("The Grave", "Steel") and "The Dick Powell Theatre".

Marvin returned to features with a prominent role in "The Comancheros" (1961) starring John Wayne. He played in two more films with Wayne, both directed by John Ford: "The Man Who Shot Liberty Valance" (1962), and "Donovan's Reef" (1963). As the vicious Liberty Valance, Marvin played his first title role and held his own with two of the screen's biggest stars (Wayne and James Stewart).

In 1962 Marvin appeared as Martin Kalig on the TV western "The Virginian" in the episode titled "It Tolls for Thee." He continued to guest star on shows like "Combat!", "Dr. Kildare" and "The Great Adventure". He did "The Case Against Paul Ryker" for "Kraft Suspense Theatre."

For director Don Siegel, Marvin appeared in "The Killers" (1964) playing an efficient professional assassin alongside Clu Gulager. "The Killers" was also the first film in which Marvin received top billing.

He guest starred on "Bob Hope Presents the Chrysler Theatre".

Marvin finally became a star for his comic role in the offbeat Western "Cat Ballou" starring Jane Fonda. This was a surprise hit and Marvin won the 1965 Academy Award for Best Actor. He also won the 1965 Silver Bear for Best Actor at the 15th Berlin International Film Festival.

Playing alongside Vivien Leigh and Simone Signoret, Marvin won the 1966 National Board of Review Award for male actors for his role in "Ship of Fools" (1965) directed by Kramer.

Marvin next performed in the hit Western "The Professionals" (1966), in which he played the leader of a small band of skilled mercenaries (Burt Lancaster, Robert Ryan, and Woody Strode) rescuing a kidnap victim (Claudia Cardinale) shortly after the Mexican Revolution.

He followed that film with the hugely successful World War II epic "The Dirty Dozen" (1967) in which top-billed Marvin again portrayed an intrepid commander of a colorful group (future stars John Cassavetes, Charles Bronson, Telly Savalas, Jim Brown, and Donald Sutherland) performing an almost impossible mission. Robert Aldrich directed.

In the wake of these two films and after having received an Oscar, Marvin was a huge star, given enormous control over his next film "Point Blank". In "Point Blank", an influential film for director John Boorman, he portrayed a hard-nosed criminal bent on revenge. Marvin, who had selected Boorman for the director's slot, had a central role in the film's development, plot, and staging.

In 1968, Marvin also appeared in another Boorman film, the critically acclaimed but commercially unsuccessful World War II character study "Hell in the Pacific", also starring famed Japanese actor Toshiro Mifune. Boorman recounted his work with Lee Marvin on these two films and Marvin's influence on his career in the 1998 documentary "". "Paul Ryker", which Marvin shot for TV in 1963 was released theatrically as "Sergeant Ryker".

Marvin was originally cast as Pike Bishop (later played by William Holden) in "The Wild Bunch" (1969), but fell out with director Sam Peckinpah and pulled out to star in the Western musical "Paint Your Wagon" (1969), in which he was top-billed over a singing Clint Eastwood. Despite his limited singing ability, he had a hit song with "Wand'rin' Star". By this time, he was getting paid $1 million per film, $200,000 less than top star Paul Newman was making at the time, yet he was ambivalent about the movie business, even with its financial rewards:

You spend the first forty years of your life trying to get in this business, and the next forty years trying to get out. And then when you're making the bread, who needs it?

Marvin had a much greater variety of roles in the 1970s, with fewer 'bad-guy' roles than in earlier years. His 1970s movies included "Monte Walsh" (1970), a Western with Palance and Jeanne Moreau; the violent "Prime Cut" (1972) with Gene Hackman; "Pocket Money" (1972) with Paul Newman, for Stuart Rosenberg; "Emperor of the North" (1973) opposite Ernest Borgnine for Aldrich; as Hickey in "The Iceman Cometh" (1973) with Fredric March and Robert Ryan, for John Frankenheimer; "The Spikes Gang" (1974) with Noah Beery Jr. for Richard Fleischer; "The Klansman" (1974) with Richard Burton; "Shout at the Devil" (1976), a World War I adventure with Roger Moore, directed by Peter Hunt; "The Great Scout & Cathouse Thursday" (1976), a comic Western with Oliver Reed; and "Avalanche Express" (1978), a Cold War thriller with Robert Shaw who died during production. None of these films were big box office hits.

Marvin was offered the role of Quint in "Jaws" (1975) but declined, stating "What would I tell my fishing friends who'd see me come off a hero against a dummy shark?".

Marvin's last big role was in Samuel Fuller's "The Big Red One" (1980), a war film based on Fuller's own war experiences.

His remaining films were "Death Hunt" (1981), a Canadian action movie with Charles Bronson, directed by Hunt; "Gorky Park" (1983) with William Hurt; and "Dog Day" (1984), shot in France.

For TV he did "" (1985; a sequel with Marvin, Ernest Borgnine, and Richard Jaeckel picking up where they had left off despite being 18 years older).

His final appearance was in "The Delta Force" (1986) with Chuck Norris, playing a role turned down by Charles Bronson.

Marvin was a Democrat who opposed the Vietnam War. He publicly endorsed John F. Kennedy in the 1960 presidential election.

Marvin married Betty Ebeling in February 1951 and together they had four children, son Christopher Lamont (1952–2013), and three daughters: Courtenay Lee (b. 1954), Cynthia Louise (b. 1956), and Claudia Leslie (1958–2012). Married 16 years, they divorced in 1967.

Marvin reunited with his high school sweetheart, Pamela Feeley, following his divorce. They married in October 1970. She had four children with three previous marriages; they had no children together and remained married until his death in 1987.

In 1971, Marvin was sued by Michelle Triola, his live-in girlfriend from 1965 to 1970, who legally changed her surname to "Marvin". Although the couple never married, she sought financial compensation similar to that available to spouses under California's alimony and community property laws. Triola claimed Marvin made her pregnant three times and paid for two abortions, while one pregnancy ended in miscarriage. She claimed the second abortion left her unable to bear children. The result was the landmark "palimony" case, "Marvin v. Marvin", 18 Cal. 3d 660 (1976).

In 1979, Marvin was ordered to pay $104,000 to Triola for "rehabilitation purposes", but the court denied her community property claim for one-half of the $3.6 million which Marvin had earned during their six years of cohabitation – distinguishing nonmarital relationship contracts from marriage, with community property rights only attaching to the latter by operation of law. Rights equivalent to community property only apply in nonmarital relationship contracts when the parties expressly, whether orally or in writing, contract for such rights to operate between them. In August 1981, the California Court of Appeal found that no such contract existed between them and nullified the award she had received. Michelle Triola died of lung cancer on October 30, 2009, having been with actor Dick Van Dyke since 1976.

Later there was controversy after Marvin characterized the trial as a "circus", saying "everyone was lying, even I lied". There were official comments about possibly charging Marvin with perjury, but no charges were filed.

This case was used as fodder for a mock debate skit on "Saturday Night Live" called "Point Counterpoint", and on "The Tonight Show Starring Johnny Carson" as a skit with Carson as Adam, and Betty White as Eve.

In December 1986, Marvin was hospitalized for more than two weeks because of a condition related to coccidioidomycosis. He went into respiratory distress and was administered steroids to help his breathing. He had major intestinal ruptures as a result, and underwent a colectomy. Marvin died of a heart attack on August 29, 1987, aged 63. He was buried with full military honors at Arlington National Cemetery.




</doc>
<doc id="18434" url="https://en.wikipedia.org/wiki?curid=18434" title="Lead Belly">
Lead Belly

Huddie William Ledbetter (; January 23, 1888 – December 6, 1949), better known by the stage name Lead Belly, was an American folk and blues singer, musician and songwriter notable for his strong vocals, virtuosity on the twelve-string guitar, and the folk standards he introduced, including his renditions of "Goodnight, Irene", "Midnight Special", "Cotton Fields", and "Boll Weevil".

Lead Belly usually played a twelve-string guitar, but he also played the piano, mandolin, harmonica, violin, and windjammer. In some of his recordings, he sang while clapping his hands or stomping his foot.

Lead Belly's songs covered a wide range of genres and topics including gospel music; blues about women, liquor, prison life, and racism; and folk songs about cowboys, prison, work, sailors, cattle herding, and dancing. He also wrote songs about people in the news, such as Franklin D. Roosevelt, Adolf Hitler, Jean Harlow, Jack Johnson, the Scottsboro Boys and Howard Hughes. Lead Belly was posthumously inducted into the Rock and Roll Hall of Fame in 1988 and the Louisiana Music Hall of Fame in 2008.

Though many releases credit him as "Leadbelly", he himself wrote it as "Lead Belly", which is also the spelling on his tombstone and the spelling used by the Lead Belly Foundation.

Lead Belly pronounced his first name /ˈhuhdːee/. He can be heard pronouncing his name this way both speaking at the beginning and singing toward the end on one of his recordings of "Boll Weevil".

The younger of two children, Lead Belly was born Huddie William Ledbetter to Sallie Brown and Wesley Ledbetter on a plantation near Mooringsport, Louisiana. On his World War II draft registration card in 1942, he gave his birthplace as Freeport, Louisiana ("Shreveport").

There is uncertainty over his precise date and year of birth. The Lead Belly Foundation gives January 20, 1889, and his grave marker gives the year 1889. His 1942 draft registration card states January 23, 1889. However, the 1900 United States Census lists "Hudy Ledbetter" as 12 years old, born January 1888, and the 1910 and 1930 censuses also give his age as corresponding to a birth in 1888. The 1940 census lists his age as 51, with information supplied by wife Martha. The books "Blues: A Regional Experience" by Eagle and LeBlanc and "Encyclopedia of Louisiana Musicians" by Tomko gives January 23, 1888, while the "Encyclopedia of the Blues" gives January 20, 1888.

His parents had cohabited for several years, but they legally married on February 26, 1888. When Huddie was five years old, the family settled in Bowie County, Texas. The 1910 census of Harrison County, Texas, shows "Hudy Ledbetter" living next door to his parents with his first wife, Aletha "Lethe" Henderson. Aletha is registered as age 19 and married one year. Others say she was 15 when they married in 1908. It was in Texas that Ledbetter received his first instrument, an accordion, from his uncle Terrell. By his early twenties, having fathered at least two children, Ledbetter left home to make his living as a guitarist and occasional laborer.

Between 1915 and 1939, Ledbetter served several prison and jail terms for a variety of criminal charges. In 1934, when Lead Belly was released from one of his last incarcerations, the United States was deep in the Great Depression, and jobs were very scarce. In September of that year, in need of regular work in order to avoid cancellation of his release from prison, Lead Belly asked John Lomax to take him on as a driver. For three months, he assisted the 67-year-old in his folk song collecting around the South. Alan Lomax, his son, was ill and did not accompany him on this trip.

By 1903, Huddie was already a "musicianer", a singer and guitarist of some note. He performed for nearby Shreveport audiences in St. Paul's Bottoms, a notorious red-light district there. He began to develop his own style of music after exposure to various musical influences on Shreveport's Fannin Street, a row of saloons, brothels, and dance halls in the Bottoms, now referred to as Ledbetter Heights. While in prison, Lead Belly may have first heard the traditional prison song "Midnight Special". He was "discovered" there thirty years later during a visit by folklorists John Lomax and his son Alan Lomax.

Deeply impressed by Ledbetter's vibrant tenor and extensive repertoire, the Lomaxes recorded him in 1933 on portable aluminum disc recording equipment for the Library of Congress. They returned with new and better equipment in July 1934, recording hundreds of his songs. On August 1, Ledbetter was released after having again served nearly all of his minimum sentence, following a petition the Lomaxes had taken to Louisiana Governor Oscar K. Allen at his urgent request. It was on the other side of a recording of his signature song, "Goodnight Irene".

A prison official later wrote to John Lomax denying that Ledbetter's singing had anything to do with his release from Angola (state prison records confirm he was eligible for early release due to good behavior). However, both Ledbetter and the Lomaxes believed that the record they had taken to the governor had hastened his release from prison.

In December 1934, Lead Belly participated in a "smoker" (group sing) at a Modern Language Association meeting at Bryn Mawr College in Pennsylvania, where the senior Lomax had a prior lecturing engagement. He was written up in the press as a convict who had sung his way out of prison. On New Year's Day, 1935, the pair arrived in New York City, where Lomax was scheduled to meet with his publisher, Macmillan, about a new collection of folk songs. The newspapers were eager to write about the "singing convict," and "Time" magazine made one of its first "March of Time" newsreels about him. Lead Belly attained fame – although not fortune.

The following week, he began recording for the American Record Corporation, but these recordings achieved little commercial success. He recorded over 40 sides for ARC (intended to be released on their Banner, Melotone, Oriole, Perfect, and Romeo labels and their short-lived Paramount series), but only five sides were actually issued. Part of the reason for the poor sales may have been that ARC released only his blues songs rather than the folk songs for which he would later become better known. Lead Belly continued to struggle financially. Like many performers, what income he made during his career would come from touring, not from record sales. In February 1935, he married his girlfriend, Martha Promise, who came North from Louisiana to join him.

The month of February was spent recording his repertoire and those of other African Americans and interviews about his life with Alan Lomax for their forthcoming book, "Negro Folk Songs As Sung by Lead Belly" (1936). Concert appearances were slow to materialize. In March 1935, Lead Belly accompanied John Lomax on a previously scheduled two-week lecture tour of colleges and universities in the Northeast, culminating at Harvard.

At the end of the month, John Lomax decided he could no longer work with Lead Belly and gave him and Martha money to go back to Louisiana by bus. He gave Martha the money her husband had earned during three months of performing, but in installments, on the pretext Lead Belly would spend it all on drinking if given a lump sum. From Louisiana, Lead Belly successfully sued Lomax for both the full amount and release from his management contract. The quarrel was bitter, with hard feelings on both sides. Curiously, in the midst of the legal wrangling, Lead Belly wrote to Lomax proposing they team up again, but it was not to be. Further, the book about Lead Belly published by the Lomaxes in the fall of the following year proved a commercial failure.

In January 1936, Lead Belly returned to New York on his own, without John Lomax, in an attempted comeback. He performed twice a day at Harlem's Apollo Theater during the Easter season in a live dramatic recreation of the "March of Time" newsreel (itself a recreation) about his prison encounter with John Lomax, where he had worn stripes, though by this time he was no longer associated with Lomax.

"Life" magazine ran a three-page article titled "Lead Belly: Bad Nigger Makes Good Minstrel" in its issue of April 19, 1937. It included a full-page, color (rare in those days) picture of him sitting on grain sacks playing his guitar and singing. Also included was a striking picture of Martha Promise (identified in the article as his manager); photos showing Lead Belly's hands playing the guitar (with the caption "these hands once killed a man"); Texas Governor Pat M. Neff; and the "ramshackle" Texas State Penitentiary. The article attributes both of his pardons to his singing of his petitions to the governors, who were so moved that they pardoned him. The text of the article ends with "he... may well be on the brink of a new and prosperous period."

Lead Belly failed to stir the enthusiasm of Harlem audiences. Instead, he attained success playing at concerts and benefits for an audience of leftist folk music aficionados. He developed his own style of singing and explaining his repertoire in the context of Southern black culture having learned from his participation in Lomax's college lectures. He was especially successful with his repertoire of children's game songs (as a younger man in Louisiana he had sung regularly at children's birthday parties in the black community). He was written about as a heroic figure by the black novelist Richard Wright, then a member of the Communist Party, in the columns of the "Daily Worker," of which Wright was the Harlem editor. The two men became personal friends, though some say Lead Belly himself was apolitical and, if anything, was a supporter of Wendell Willkie, the centrist Republican candidate for President, for whom he wrote a campaign song. However, he also wrote the song "Bourgeois Blues", which has radical or left-wing lyrics.

In 1939, Lead Belly returned to prison. Alan Lomax, then 24, took him under his wing and helped raise money for his legal expenses, dropping out of graduate school to do so. After his release (in 1940–41), Lead Belly appeared as a regular on Alan Lomax and Nicholas Ray's groundbreaking CBS radio show "Back Where I Come From", broadcast nationwide. He also appeared in nightclubs with Josh White, becoming a fixture in New York City's surging folk music scene and befriending the likes of Sonny Terry, Brownie McGhee, Woody Guthrie, and a young Pete Seeger, all fellow performers on "Back Where I Come From". During the first half of the decade, he recorded for RCA, the Library of Congress, and Moe Asch (future founder of Folkways Records) and in 1944 went to California, where he recorded strong sessions for Capitol Records. He lodged with a studio guitar player on Merrywood Drive in Laurel Canyon. Lead Belly was the first American country blues musician to achieve success in Europe.

In 1949, Lead Belly had a regular radio show, "Folk Songs of America", broadcast on station WNYC in New York, on Henrietta Yurchenco's show on Sunday nights. Later in the year he began his first European tour with a trip to France, but fell ill before its completion and was diagnosed with amyotrophic lateral sclerosis (ALS), or Lou Gehrig's disease (a motor neuron disease). His final concert was at the University of Texas at Austin in a tribute to his former mentor, John Lomax, who had died the previous year. Martha also performed at that concert, singing spirituals with her husband.

Lead Belly died later that year in New York City and was buried in the Shiloh Baptist Church cemetery, in Mooringsport, Louisiana, west of Blanchard, in Caddo Parish. He is honored with a statue across from the Caddo Parish Courthouse, in Shreveport.

Lead Belly was imprisoned multiple times beginning in 1915 when he was convicted of carrying a pistol and sentenced to time on the Harrison County chain gang. He later escaped and found work in nearby Bowie County under the assumed name of Walter Boyd. Later, in January 1918, he was imprisoned at the Imperial Farm (now Central Unit) in Sugar Land, Texas, after killing one of his relatives, Will Stafford, in a fight over a woman. During his second prison term, another inmate stabbed him in the neck (leaving him with a fearsome scar he subsequently covered with a bandana); Ledbetter nearly killed his attacker with his own knife.

In 1925 he was pardoned and released after writing a song to Governor Pat Morris Neff seeking his freedom, having served the minimum seven years of a 7-to-35-year sentence. Combined with his good behavior, which included entertaining the guards and fellow prisoners, his appeal to Neff's strong religious beliefs proved sufficient. It was a testament to his persuasive powers, as Neff had run for governor on a pledge not to issue pardons (the only recourse for prisoners, since in most Southern prisons there was no provision for parole). According to Charles K. Wolfe and Kip Lornell in their book "The Life and Legend of Leadbelly" (1999), Neff had regularly brought guests to the prison on Sunday picnics to hear Ledbetter perform.

In 1930, Ledbetter was sentenced to Louisiana State Penitentiary after a summary trial for attempted homicide for stabbing a man in a fight. In 1939, Lead Belly served his final jail term for assault after stabbing a man in a fight in Manhattan.

There are several conflicting stories about how Ledbetter acquired the nickname "Lead Belly", but he probably acquired it while in prison. Some claim his fellow inmates called him "Lead Belly" as a play on his family name and his physical toughness. Others say he earned the name after being wounded in the stomach with buckshot. Another theory is that the name refers to his ability to drink moonshine, the homemade liquor that Southern farmers, black and white, made to supplement their incomes.

Blues singer Big Bill Broonzy thought it came from a supposed tendency to lie about as if "with a stomach weighted down by lead" in the shade when the chain gang was supposed to be working. Yet another theory is that it may be a corruption of his last name pronounced with a Southern accent. Whatever its origin, he adopted the nickname as a pseudonym while performing.

Lead Belly styled himself "King of the Twelve-String Guitar," and despite his use of other instruments like the accordion, the most enduring image of Lead Belly as a performer is wielding his unusually large Stella twelve-string. This guitar had a slightly longer scale length than a standard guitar, increasing the tension on the instrument, which, given the added tension of the six extra strings, meant that a trapeze-style tailpiece helped resist bridge lifting. It had slotted tuners and ladder bracing.

Lead Belly played with finger picks much of the time, using a thumb pick to provide walking bass lines described as "tricky" and "inventive" and occasionally to strum. This technique, combined with low tunings and heavy strings, gives many of his recordings a piano-like sound. In fact, scholars have suggested much of his guitar playing was inspired equally by barrelhouse piano and the Mexican Bajo sexto, an instrument he encountered in Texas and Louisiana.

Lead Belly's tunings are debated by both modern and contemporary musicians and blues enthusiasts alike exacerbated by the lack of film footage of his performing rendering chord decoding difficult but it seems to be a down-tuned variant of standard tuning; it is likely that he tuned his guitar strings relative to one another, so that the actual notes shifted as the strings wore. Such down-tuning was a common technique before the development of truss rods, and was intended to prevent the instrument's neck from warping. Lead Belly's playing style was popularized by Pete Seeger, who adopted the twelve-string guitar in the 1950s and released an instructional LP and book using Lead Belly as an exemplar of technique.

In some of the recordings in which Lead Belly accompanied himself, he would make an unusual type of grunt between his verses, sometimes described as "haah! " Songs such as "Looky Looky Yonder," "Take This Hammer," "Linin' Track" and "Julie Ann Johnson" feature this unusual vocalization. In "Take This Hammer," Lead Belly explained, "Every time the men say, 'Haah,' the hammer falls. The hammer rings, and we swing, and we sing." The "haah" sound can be heard in work chants sung by Southern railroad section workers, "gandy dancers," in which it was used to coordinate work crews as they laid and maintained tracks.

In 1976, a biopic entitled "Leadbelly" was released, directed by Gordon Parks and featuring Roger E. Mosley as Lead Belly.

Kurt Cobain promoted the legacy of Lead Belly, and some modern rock audiences often owe their familiarity with Lead Belly to Nirvana's performance of "Where Did You Sleep Last Night" on a televised concert later released as "MTV Unplugged in New York". Cobain refers to his attempt to convince David Geffen to purchase Lead Belly's guitar for him in an interval before the song is played. In his notebooks, Cobain listed Lead Belly's "Last Session Vol. 1" as one of the 50 albums most influential in the formation of Nirvana's sound. It was included in "NME's" "The 100 Greatest Albums You've Never Heard list".

Bob Dylan credits Lead Belly for getting him into Folk music. In his Nobel Prize Lecture, Dylan said "somebody – somebody I’d never seen before – handed me a Lead Belly record with the song "Cotton Fields" on it. And that record changed my life right then and there. Transported me into a world I’d never known. It was like an explosion went off. Like I’d been walking in darkness and all of the sudden the darkness was illuminated. It was like somebody laid hands on me. I must have played that record a hundred times." Dylan also pays homage to him in "Song to Woody" on his self-titled debut album.

Lonnie Donegan's recording of "Rock Island Line", released as a single in late 1955, signalled the start of the UK skiffle craze. George Harrison of The Beatles was quoted as saying, “if there was no Lead Belly, there would have been no Lonnie Donegan; no Lonnie Donegan, no Beatles. Therefore no Lead Belly, no Beatles.” In a BBC tribute in 1999, which marked the 50th anniversary of Lead Belly’s death, Van Morrison — while sitting alongside Ronnie Wood of The Rolling Stones — claimed that the British popular music scene of the 1960s wouldn’t have happened if it weren’t for Lead Belly’s influence. “I’d put my money on that,” he said. Wood concurred.

In 2001 English-Canadian blues singer Long John Baldry released his final studio album, "Remembering Leadbelly". It contains cover versions of Lead Belly songs, and features a six-minute Alan Lomax interview.

George Ezra developed his singing style from trying to sing like Lead Belly. "On the back of the record, it said his voice was so big, you had to turn your record player down," Ezra says. "I liked the idea of singing with a big voice, so I tried it, and I could."

In 2015, in celebration of Lead Belly's 125th birthday, several events were held. The Kennedy Center, in collaboration with the Grammy Museum held "Lead Belly at 125: A Tribute to an American Songster," a musical event featuring Robert Plant, Alison Krauss, and Buddy Miller with Viktor Krauss as headliners and Dom Flemons as host, with special appearances by Lucinda Williams, Alvin Youngblood Hart, Billy Hector, Valerie June, Shannon McNally, Josh White Jr., and Dan Zanes, among others Also in Washington, D.C., "Bourgeois Town: Lead Belly in Washington DC" by the Library of Congress was held where Todd Harvey interviewed Lead Belly family members about their relative, his contributions to American culture and world music and an overview of the significant Lead Belly materials in the Center's archive In London, England, the Royal Albert Hall held "Lead Belly Fest", a musical event featuring Van Morrison, Eric Burdon, Jools Holland, Billy Bragg, Paul Jones, and more.

Influenced by the sinking of the "Titanic" in April 1912, Ledbetter wrote the song "The Titanic", his first composition on the twelve-string guitar, which later became his signature instrument. Initially played when performing with Blind Lemon Jefferson (1893–1929) in and around Dallas, Texas, the song is about champion African-American boxer Jack Johnson's being denied passage on the "Titanic". Johnson had in fact been denied passage on a ship for being black, but it was not the "Titanic". Still, the song includes the lyric "Jack Johnson tried to get on board. The Captain, he says, 'I ain't haulin' no coal!' Fare thee, "Titanic"! Fare thee well!" Ledbetter later noted he had to leave out this passage when playing in front of white audiences.



The Library of Congress recordings, made by John and Alan Lomax from 1934 to 1943, were released in a six-volume series by Rounder Records:


The Folkways recordings, done for Moses Asch from 1941 to 1947, were released in a three-volume series by Smithsonian Folkways:

Smithsonian Folkways has released several other collections of his recordings:







</doc>
<doc id="18435" url="https://en.wikipedia.org/wiki?curid=18435" title="Lower Saxony">
Lower Saxony

Lower Saxony ( ; ) is a German state ("Land") situated in northwestern Germany. It is the second-largest state by land area, with , and fourth-largest in population (7.9 million) among the 16 "Länder" federated as the Federal Republic of Germany. In rural areas, Northern Low Saxon (a dialect of Low German) and Saterland Frisian (a variety of the Frisian language) are still spoken, but the number of speakers is declining.

Lower Saxony borders on (from north and clockwise) the North Sea, the states of Schleswig-Holstein, Hamburg, Mecklenburg-Vorpommern, Brandenburg, Saxony-Anhalt, Thuringia, Hesse and North Rhine-Westphalia, and the Netherlands (Drenthe, Groningen and Overijssel). Furthermore, the state of Bremen forms two enclaves within Lower Saxony, one being the city of Bremen, the other, its seaport city of Bremerhaven. In fact, Lower Saxony borders more neighbours than any other single "Bundesland." The state's principal cities include the state capital Hanover, Braunschweig (Brunswick), Lüneburg, Osnabrück, Oldenburg, Hildesheim, Wolfenbüttel, Wolfsburg, and Göttingen.

The northwestern area of Lower Saxony, which lies on the coast of the North Sea, is called East Frisia and the seven East Frisian Islands offshore are popular with tourists. In the extreme west of Lower Saxony is the Emsland, a traditionally poor and sparsely populated area, once dominated by inaccessible swamps. The northern half of Lower Saxony, also known as the North German Plains, is almost invariably flat except for the gentle hills around the Bremen geestland. Towards the south and southwest lie the northern parts of the German Central Uplands: the Weser Uplands and the Harz mountains. Between these two lie the Lower Saxon Hills, a range of low ridges. Thus, Lower Saxony is the only "Bundesland" that encompasses both maritime and mountainous areas.

Lower Saxony's major cities and economic centres are mainly situated in its central and southern parts, namely Hanover, Braunschweig, Osnabrück, Wolfsburg, Salzgitter, Hildesheim, and Göttingen. Oldenburg, near the northwestern coastline, is another economic centre. The region in the northeast is called the Lüneburg Heath ("Lüneburger Heide"), the largest heathland area of Germany and in medieval times wealthy due to salt mining and salt trade, as well as to a lesser degree the exploitation of its peat bogs until about the 1960s. To the north, the Elbe River separates Lower Saxony from Hamburg, Schleswig-Holstein, Mecklenburg-Vorpommern, and Brandenburg. The banks just south of the Elbe are known as "Altes Land" (Old Country). Due to its gentle local climate and fertile soil, it is the state's largest area of fruit farming, its chief produce being apples.

Most of the state's territory was part of the historic Kingdom of Hanover; the state of Lower Saxony has adopted the coat of arms and other symbols of the former kingdom. It was created by the merger of the State of Hanover with three smaller states on 1 November 1946.

Lower Saxony has a natural boundary in the north in the North Sea and the lower and middle reaches of the River Elbe, although parts of the city of Hamburg lie south of the Elbe. The state and city of Bremen is an enclave entirely surrounded by Lower Saxony. The Bremen/Oldenburg Metropolitan Region is a cooperative body for the enclave area. To the southeast, the state border runs through the Harz, low mountains that are part of the German Central Uplands. The northeast and west of the state, which form roughly three-quarters of its land area, belong to the North German Plain, while the south is in the Lower Saxon Hills, including the Weser Uplands, Leine Uplands, Schaumburg Land, Brunswick Land, Untereichsfeld, Elm, and Lappwald. In northeast, Lower Saxony is Lüneburg Heath. The heath is dominated by the poor, sandy soils of the geest, whilst in the central east and southeast in the loess "börde" zone, productive soils with high natural fertility occur. Under these conditions—with loam and sand-containing soils—the land is well-developed agriculturally. In the west lie the County of Bentheim, Osnabrück Land, Emsland, Oldenburg Land, Ammerland, Oldenburg Münsterland, and on the coast East Frisia.
The state is dominated by several large rivers running northwards through the state: the Ems, Weser, Aller, and Elbe.

The highest mountain in Lower Saxony is the Wurmberg (971 m) in the Harz. For other significant elevations see: List of mountains and hills in Lower Saxony. Most of the mountains and hills are found in the southeastern part of the state. The lowest point in the state, at about 2.5 m below sea level, is a depression near Freepsum in East Frisia.

The state's economy, population, and infrastructure are centred on the cities and towns of Hanover, Stadthagen, Celle, Braunschweig, Wolfsburg, Hildesheim, and Salzgitter. Together with Göttingen in southern Lower Saxony, they form the core of the Hannover–Braunschweig–Göttingen–Wolfsburg Metropolitan Region.

Lower Saxony has clear regional divisions that manifest themselves geographically, as well as historically and culturally. In the regions that used to be independent, especially the heartlands of the former states of Brunswick, Hanover, Oldenburg and Schaumburg-Lippe, a marked local regional awareness exists. By contrast, the areas surrounding the Hanseatic cities of Bremen and Hamburg are much more oriented towards those centres.

Sometimes, overlaps and transition areas happen between the various regions of Lower Saxony. Several of the regions listed here are part of other, larger regions, that are also included in the list.
Just under 20% of the land area of Lower Saxony is designated as nature parks, i.e.: Dümmer, Elbhöhen-Wendland, Elm-Lappwald, Harz, Lüneburger Heide, Münden, Terra.vita, Solling-Vogler, Lake Steinhude, Südheide, Weser Uplands, Wildeshausen Geest, Bourtanger Moor-Bargerveen.
Lower Saxony falls climatically into the north temperate zone of central Europe that is affected by prevailing Westerlies and is located in a transition zone between the maritime climate of Western Europe and the continental climate of Eastern Europe. This transition is clearly noticeable within the state: whilst the northwest experiences an Atlantic (North Sea coastal) to Sub-Atlantic climate, with comparatively low variations in temperature during the course of the year and a surplus water budget, the climate towards the southeast is increasingly affected by the Continent. This is clearly shown by greater temperature variations between the summer and winter halves of the year and in lower and more variable amounts of precipitation across the year. This sub-continental effect is most sharply seen in the Wendland, in the Weser Uplands (Hamelin to Göttingen) and in the area of Helmstedt. The highest levels of precipitation are experienced in the Harz because the Lower Saxon part forms the windward side of this mountain range against which orographic rain falls. The average annual temperature is 8 °C (7.5 °C in the Altes Land and 8.5 °C in the district of Cloppenburg).

Lower Saxony is divided into 37 districts ("Landkreise" or simply "Kreise"):

Furthermore, there are eight urban districts and two cities with special status:
¹ "following the "Göttingen Law" of 1 January 1964, the town of Göttingen is incorporated into the rural district ("Landkreis") of Göttingen, but is treated as an urban district unless other rules apply. On 1 November 2016 the districts of Osterode and Göttingen were merged under the name Göttingen, not influencing the city's special status."
² "following the "Law on the region of Hanover", Hanover merged with the district of Hanover to form the Hanover Region, which has been treated mostly as a rural district, but Hanover is treated as an urban district since 1 November 2001 unless other rules apply."

The name of Saxony derives from that of the Germanic tribe of the Saxons. Before the late medieval period, there was a single Duchy of Saxony. The term "Lower Saxony" was used after the dissolution of the stem duchy in the late 13th century to disambiguate the parts of the former duchy ruled by the House of Welf from the Electorate of Saxony on one hand, and from the Duchy of Westphalia on the other.

The name and coat of arms of the present state go back to the Germanic tribe of Saxons. During the Migration Period some of the Saxon peoples left their homeland in Holstein about the 3rd century and pushed southwards over the Elbe, where they expanded into the sparsely populated regions in the rest of the lowlands, in the present-day Northwest Germany and the northeastern part of what is now the Netherlands. From about the 7th century the Saxons had occupied a settlement area that roughly corresponds to the present state of Lower Saxony, of Westphalia and a number of areas to the east, for example, in what is now west and north Saxony-Anhalt. The land of the Saxons was divided into about 60 "Gaue". The Frisians had not moved into this region; for centuries they preserved their independence in the most northwesterly region of the present-day Lower Saxon territory. The original language of the folk in the area of Old Saxony was West Low German, one of the varieties of language in the Low German dialect group.

The establishment of permanent boundaries between what later became Lower Saxony and Westphalia began in the 12th century. In 1260, in a treaty between the Archbishopric of Cologne and the Duchy of Brunswick-Lüneburg the lands claimed by the two territories were separated from each other. The border ran along the Weser to a point north of Nienburg. The northern part of the Weser-Ems region was placed under the rule of Brunswick-Lüneburg.

The word "Niedersachsen" was first used before 1300 in a Dutch rhyming chronicle ("Reimchronik"). From the 14th century it referred to the Duchy of Saxe-Lauenburg (as opposed to Saxe-Wittenberg). On the creation of the imperial circles in 1500, a Lower Saxon Circle was distinguished from a Lower Rhenish–Westphalian Circle. The latter included the following territories that, in whole or in part, belong today to the state of Lower Saxony: the Bishopric of Osnabrück, the Bishopric of Münster, the County of Bentheim, the County of Hoya, the Principality of East Frisia, the Principality of Verden, the County of Diepholz, the County of Oldenburg, the County of Schaumburg and the County of Spiegelberg. At the same time a distinction was made with the eastern part of the old Saxon lands from the central German principalities later called Upper Saxony for dynastic reasons. (see also → Electorate of Saxony, History of Saxony).

The close historical links between the domains of the Lower Saxon Circle now in modern Lower Saxony survived for centuries especially from a dynastic point of view. The majority of historic territories whose land now lies within Lower Saxony were sub-principalities of the medieval, Welf estates of the Duchy of Brunswick-Lüneburg. All the Welf princes called themselves dukes "of Brunswick and Lüneburg" despite often ruling parts of a duchy that was forever being divided and reunited as various Welf lines multiplied or died out.

Over the course of time two great principalities survived east of the Weser: the Kingdom of Hanover and the Duchy of Brunswick (after 1866 Hanover became a Prussian province; after 1919 Brunswick became a free state). Historically a close tie exists between the royal house of Hanover (Electorate of Hanover) to the United Kingdom of Great Britain and Northern Ireland as a result of their personal union in the 18th century.

West of the River Hunte a "de-Westphalianising process" began in 1815: After the Congress of Vienna the territories of the later administrative regions ("Regierungsbezirke") of Osnabrück and Aurich transferred to the Kingdom of Hanover. Until 1946, the Grand Duchy of Oldenburg and the Principality of Schaumburg-Lippe retained their stately authority. Nevertheless, the entire Weser-Ems region (including the city of Bremen) were grouped in 1920 into a Lower Saxon Constituency Association ("Wahlkreisverband IX (Niedersachsen)"). This indicates that at that time the western administrations of the Prussian Province of Hanover and the state of Oldenburg were perceived as being "Lower Saxon".

The forerunners of today's state of Lower Saxony were lands that were geographically and, to some extent, institutionally interrelated from very early on. The County of Schaumburg (not to be confused with the Principality of Schaumburg-Lippe) around the towns of Rinteln and Hessisch Oldendorf did indeed belong to the Prussian province of Hesse-Nassau until 1932, a province that also included large parts of the present state of Hesse, including the cities of Kassel, Wiesbaden and Frankfurt am Main; but in 1932, however, the County of Schaumburg became part of the Prussian Province of Hanover. Also before 1945, namely 1937, the city of Cuxhaven has been fully integrated into the Prussian Province of Hanover by the Greater Hamburg Act, so that in 1946, when the state of Lower Saxony was founded, only four states needed to be merged. With the exception of Bremen and the areas that were ceded to the Soviet Occupation Zone in 1945, all those areas allocated to the new state of Lower Saxony in 1946, had already been merged into the "Constituency Association of Lower Saxony" in 1920.

In a lecture on 14 September 2007, Dietmar von Reeken described the emergence of a "Lower Saxony consciousness" in the 19th century, the geographical basis of which was used to invent a territorial construct: the resulting local heritage societies ("Heimatvereine") and their associated magazines routinely used the terms "Lower Saxony" or "Lower Saxon" in their names. At the end of the 1920s in the context of discussions about a reform of the Reich, and promoted by the expanding local heritage movement ("Heimatbewegung"), a 25-year conflict started between "Lower Saxony" and "Westphalia". The supporters of this dispute were administrative officials and politicians, but regionally focussed scientists of various disciplines were supposed to have fuelled the arguments. In the 1930s, a real Lower Saxony did not yet exist, but there was a plethora of institutions that would have called themselves "Lower Saxon". The motives and arguments in the disputes between "Lower Saxony" and "Westphalia" were very similar on both sides: economic interests, political aims, cultural interests and historical aspects.

After the Second World War most of Northwest Germany lay within the British Zone of Occupation. On 23 August 1946, the British Military Government issued Ordinance No. 46 ""Concerning the dissolution of the provinces of the former state of Prussia in the British Zone and their reconstitution as independent states"", which initially established the State of Hanover on the territory of the former Prussian Province of Hanover. Its minister president, Hinrich Wilhelm Kopf, had already suggested in June 1945 the formation of a state of Lower Saxony, that was to include the largest possible region in the middle of the British Zone. In addition to the regions that actually became Lower Saxony subsequently, Kopf asked, in a memorandum dated April 1946, for the inclusion of the former Prussian district of Minden-Ravensberg (i.e. the Westphalian city of Bielefeld as well as the Westphalian districts of Minden, Lübbecke, Bielefeld, Herford and Halle), the district of Tecklenburg and the state of Lippe. Kopf's plan was ultimately based on a draft for the reform of the German Empire from the late 1920s by Georg Schnath and Kurt Brüning. The strong Welf connotations of this draft, according to Thomas Vogtherr, did not simplify the development of a Lower Saxon identity after 1946.

An alternative model, proposed by politicians in Oldenburg and Brunswick, envisaged the foundation of the independent state of "Weser-Ems", that would be formed from the state of Oldenburg, the Hanseatic City of Bremen and the administrative regions of Aurich and Osnabrück. Several representatives of the state of Oldenburg even demanded the inclusion of the Hanoverian districts of Diepholz, Syke, Osterholz-Scharmbeck and Wesermünde in the proposed state of "Weser-Ems". Likewise an enlarged State of Brunswick was proposed in the southeast to include the "Regierungsbezirk" of Hildesheim and the district of Gifhorn. Had this plan come to fruition, the territory of the present Lower Saxony would have consisted of three states of roughly equal size.

The district council of Vechta protested on 12 June 1946 against being incorporated into the metropolitan area of Hanover ("Großraum Hannover"). If the State of Oldenburg was to be dissolved, Vechta District would much rather be included in the Westphalian region. Particularly in the districts where there was a political Catholicism the notion was widespread, that Oldenburg Münsterland and the "Regierungsbezirk" of Osnabrück should be part of a newly formed State of Westphalia.

Since the foundation of the states of North Rhine-Westphalia and Hanover on 23 August 1946 the northern and eastern border of North Rhine-Westphalia has largely been identical with that of the Prussian Province of Westphalia. Only the Free State of Lippe was not incorporated into North Rhine-Westphalia until January 1947. With that the majority of the regions left of the Upper Weser became North Rhine-Westphalian.

In the end, at the meeting of the Zone Advisory Board on 20 September 1946, Kopf's proposal with regard to the division of the British occupation zone into three large states proved to be capable of gaining a majority. Because this division of their occupation zone into relatively large states also met the interests of the British, on 8 November 1946 Regulation No. 55 of the British military government was issued, by which the State of Lower Saxony with its capital Hanover were founded, backdated to 1 November 1946. The state was formed by a merger of the Free States of Brunswick, of Oldenburg and of Schaumburg-Lippe with the previously formed State of Hanover. But there were exceptions:

The demands of Dutch politicians that the Netherlands should be given the German regions east of the Dutch-German border as war reparations, were roundly rejected at the London Conference of 26 March 1949. In fact only about 1.3 km² of West Lower Saxony was transferred to the Netherlands, in 1949.

"→ see main article Dutch annexation of German territory after World War II"

The first Lower Saxon parliament or "Landtag" met on 9 December 1946. It was not elected; rather it was established by the British Occupation Administration (a so-called "appointed parliament"). That same day the parliament elected the Social Democrat, Hinrich Wilhelm Kopf, the former Hanoverian president ("Regierungspräsident") as their first minister president. Kopf led a five-party coalition, whose basic task was to rebuild a state afflicted by the war's rigours. Kopf's cabinet had to organise an improvement of food supplies and the reconstruction of the cities and towns destroyed by Allied air raids during the war years. Hinrich Wilhelm Kopf remained – interrupted by the time in office of Heinrich Hellwege (1955–1959) – as the head of government in Lower Saxony until 1961.

The greatest problem facing the first state government in the immediate post-war years was the challenge of integrating hundreds of thousands of refugees from Germany's former territories in the east (such as Silesia and East Prussia), which had been annexed by Poland and the Soviet Union. Lower Saxony was at the western end of the direct escape route from East Prussia and had the longest border with the Soviet Zone. On 3 October 1950 Lower Saxony took over the sponsorship of the very large number of refugees from Silesia. In 1950 there was still a shortage of 730,000 homes according to official figures.

During the period when Germany was divided, the Lower Saxon border crossing at Helmstedt found itself on the main transport artery to West Berlin and, from 1945 to 1990 was the busiest European border crossing point.

Of economic significance for the state was the "Volkswagen" concern, that restarted the production of civilian vehicles in 1945, initially under British management, and in 1949 transferred into the ownership of the newly founded country of West Germany and state of Lower Saxony. Overall, Lower Saxony, with its large tracts of rural countryside and few urban centres, was one of the industrially weaker regions of the federal republic for a long time. In 1960, 20% of the working population worked on the land. In the rest of the federal territory the figure was just 14%. Even in economically prosperous times the jobless totals in Lower Saxony are constantly higher than the federal average.

In 1961 Georg Diederichs took office as the minister president of Lower Saxony as the successor to Hinrich Wilhelm Kopf. He was replaced in 1970 by Alfred Kubel. The arguments about the Gorleben Nuclear Waste Repository, that began during the time in office of minister president Ernst Albrecht (1976–1990), have played an important role in state and federal politics since the end of the 1970s.

In 1990 Gerhard Schröder entered the office of minister president. On 1 June 1993 the new Lower Saxon constitution entered force, replacing the "Provisional Lower Saxon Constitution" of 1951. It enables referenda and plebiscites and establishes environmental protection as a fundamental state principle.

The former Hanoverian Amt Neuhaus with its parishes of Dellien, Haar, Kaarßen, Neuhaus (Elbe), Stapel, Sückau, Sumte and Tripkau as well as the villages of Neu Bleckede, Neu Wendischthun and Stiepelse in the parish of Teldau and the historic Hanoverian region in the forest district of Bohldamm in the parish of Garlitz transferred with effect from 30 June 1993 from Mecklenburg-Vorpommern to Lower Saxony (Lüneburg district). From these parishes the new municipality of Amt Neuhaus was created on 1 October 1993.

In 1998 Gerhard Glogowski succeeded Gerhard Schröder who became Federal Chancellor. Because he had been linked with various scandals in his home city of Brunswick, he resigned in 1999 and was replaced by Sigmar Gabriel.

From 2003 to his election as Federal President in 2010 Christian Wulff was minister president in Lower Saxony. The Osnabrücker headed a CDU-led coalition with the FDP as does his successor, David McAllister. After the elections on 20 January 2013 McAllister was deselected.

Between 1946 and 2004, the state's districts and independent towns were grouped into eight regions, with different status for the two regions ("Verwaltungsbezirke") comprising the formerly free states of Brunswick and Oldenburg. In 1978 the regions were merged into four governorates ("Regierungsbezirke"): Since 2004 the Bezirksregierungen (regional governments) have been broken up again.

1946–1978:
1978–2004:

On 1 January 2005 the four administrative regions or governorates ("Regierungsbezirke"), into which Lower Saxony had been hitherto divided, were dissolved. These were the governorates of Braunschweig, Hanover, Lüneburg and Weser-Ems.

The 300.000-year-old nearly entire remains of a female straight-tusked elephant were revealed by University of Tübingen researchers and the Senckenberg Centre for Human Evolution in May in 2020. According to the archaeozoologist Ivo Verheijen, 6.8 tones older skeleton with battered teeth had a shoulder height of about 3.2 metres. Researchers also uncovered two long bones and 30 small flint flakes that were used as tools for knapping among the elephant bones.

"We found both 2.3-m-long tusks, the complete lower jaw, numerous vertebrae and ribs as well as large bones belonging to three of the legs and even all five delicate hyoid bones" said archaeologist Dr. Jordi Serangeli.

At the end of 2014, there were almost 571.000 non-German citizens in Lower Saxony. The following table illustrates the largest minority groups in Lower Saxony:


The 2011 census stated that a majority of the population were Christians (71.93%); 51.48% of the total population were member of the Evangelical Church in Germany, 18.34% were Catholics, 2.11% were member of other Christian denominations, 2.27% were member of other religions. 25.8% have no denomination. Even there is a high level of official belonging to a Christian denomination, the people - especially in the cities - are highly secular in faith and behavior.

As of 2018, the Evangelical Church in Germany was the faith of 43.0% of the population. It is organised in the five Landeskirchen named Evangelical Lutheran State Church in Brunswick (comprising the former Free State of Brunswick), Evangelical Lutheran Church of Hanover (comprising the former Province of Hanover), Evangelical Lutheran Church in Oldenburg (comprising the former Free State of Oldenburg), Evangelical Lutheran Church of Schaumburg-Lippe (comprising the former Free State of Schaumburg-Lippe), and Evangelical Reformed Church (covering all the state).

Together, these member churches of the Evangelical Church in Germany gather a substantial part of the Protestant population in Germany.

The Catholic Church was the faith of 16.8% of the population in 2018. It is organised in the three dioceses of Osnabrück (western part of the state), Münster (comprising the former Free State of Oldenburg) and Hildesheim (northern and eastern part of the state). The Catholic faith is mainly concentrated to the regions of Oldenburger Münsterland, region of Osnabrück, region of Hildesheim and in the Western Eichsfeld.
40.2% of the Low Saxons were irreligious or adhere to other religions. Judaism, Islam and Buddhism are minority faiths.

The Gross domestic product (GDP) of the state was 229.5 billion euros in 2018, accounting for 8.7% of German economic output. GDP per capita adjusted for purchasing power was 33,700 euros or 112% of the EU27 average in the same year. The GDP per employee was 100% of the EU average.

Agriculture, strongly weighted towards the livestock sector, has always been a very important economic factor in the state. The north and northwest of Lower Saxony are mainly made up of coarse sandy soil that makes crop farming difficult and therefore grassland and cattle farming are more prevalent in those areas. Lower Saxony is home, in 2017, to one in five of Germany's cattle, one in three of the country's pigs, and 50% of its hens. Wheat, potatoes, rye, and oats are among the state's present-day arable crops. Towards the south and southeast, extensive loess layers in the soil left behind by the last ice age allow high-yield crop farming. One of the principal crops there is sugar beet. Consequently, the Land has a big food industry, mainly organized in small and medium-sized enterprises (SME). Big players are Deutsches Milchkontor and PHW Group (biggest German poultry farmer and producer).

Mining has also been an important source of income in Lower Saxony for centuries. Silver ore became a foundation of notable economic prosperity in the Harz Mountains as early as the 12th century, while iron mining in the Salzgitter area and salt mining in various areas of the state became another important economic backbone. Although overall yields are comparatively low, Lower Saxony is also an important supplier of crude oil in the European Union. Mineral products still mined today include iron and lignite.

Radioactive waste is frequently transported in the area to the city of Salzgitter, for the deep geological repository Schacht Konrad and between Schacht Asse II in the Wolfenbüttel district and Lindwedel and Höfer.

Manufacturing is another large part of the regional economy. Despite decades of gradual downsizing and restructuring, the car maker Volkswagen with its five production plants within the state's borders still remains the single biggest private-sector employer, its world headquarters in Wolfsburg. Due to the Volkswagen Law, which has recently been ruled illegal by the European Union's high court, the state of Lower Saxony is still the second largest shareholder, owning 20.3% of the company. Thanks to the importance of car manufacturing in Lower Saxony, a thriving supply industry is centred around its regional focal points. Other mainstays of the Lower Saxon industrial sector include aviation (the region of Stade is called CFK-Valley), shipbuilding (e.g. Meyer Werft), biotechnology, and steel. Medicine plays a major role: Hanover and Göttingen have two large University Medical Schools and hospitals and Otto Bock in Duderstadt is the word leader in prosthetics.

The service sector has gained importance following the demise of manufacturing in the 1970s and 1980s. Important branches today are the tourism industry with TUI AG in Hanover, one of Europe's largest travel companies, as well as trade and telecommunication. Hanover is one of Germany's main location of insurance companies e. g. Talanx, Hannover Re .

In October 2018 the unemployment rate stood at 5.0% and was marginally higher than the national average.

Lower Saxony has four World Heritage Sites.


Since 1948, politics in the state has been dominated by the rightist Christian Democratic Union (CDU) and the leftist Social Democratic Party. Lower Saxony was one of the origins of the German environmentalist movement in reaction to the state government's support for underground nuclear waste disposal. This led to the formation of the German Green Party in 1980.

The former Minister-President, Christian Wulff, led a coalition of his CDU with the Free Democratic Party between 2003 and 2010. In the 2008 election, the ruling CDU held on to its position as the leading party in the state, despite losing votes and seats. The CDU's coalition with the Free Democratic Party retained its majority although it was cut from 29 to 10. The election also saw the entry into the state parliament for the first time of the leftist The Left party. On 1 July 2010 David McAllister was elected Minister-President.

After the state election on 20 January 2013, Stephan Weil of the Social Democrats was elected as the new Minister-President. He governed in coalition with the Greens.

After the state election in September 2017, Stephan Weil of the Social Democrats was again elected as the new Minister-President. He governs in coalition with the CDU.

The state of Lower Saxony was formed after World War II by merging the former states of Hanover, Oldenburg, Brunswick and Schaumburg-Lippe. Hanover, a former kingdom, is by far the largest of these contributors by area and population and has been a province of Prussia since 1866. The city of Hanover is the largest and capital city of Lower Saxony.

The constitution states that Lower Saxony be a free, republican, democratic, social and environmentally sustainable state inside the Federal Republic of Germany; universal human rights, peace and justice are preassigned guidelines of society, and the human rights and civil liberties proclaimed by the constitution of the Federal Republic are genuine constituents of the constitution of Lower Saxony. Each citizen is entitled to education and there is universal compulsory school attendance.

All government authority is to be sanctioned by the will of the people, which expresses itself via elections and plebiscites. The legislative assembly is a unicameral parliament elected for terms of five years. The composition of the parliament obeys to the principle of proportional representation of the participating political parties, but it is also ensured that each constituency delegates one directly elected representative. If a party wins more constituency delegates than their statewide share among the parties would determine, it can keep all these constituency delegates.

The governor of the state (prime minister) and his ministers are elected by the parliament. As there is a system of five political parties in Germany and so also in Lower Saxony, it is usually the case that two or more parties negotiate for a common political agenda and a commonly determined composition of government where the party with the biggest share of the electorate fills the seat of the governor.

The states of the Federal Republic of Germany, and so Lower Saxony, have legislative responsibility and power mainly reduced to the policy fields of the school system, higher education, culture and media and police, whereas the more important policy fields like economic and social policies, foreign policy etc. are a prerogative of the federal government. Hence the probably most important function of the federal states is their representation in the Federal Council (Bundesrat), where their approval on many crucial federal policy fields, including the tax system, is required for laws to become enacted.

The Minister-President heads the state government, acting as a head of state (even if the federated states have the status of a state, they don't established the office of a head of state but merged the functions with the head of the executive branch) as well as the government leader. They are elected by the Landtag of Lower Saxony.

The coat of arms shows a white horse (Saxon Steed) on red ground, which is an old symbol of the Saxon people. Legend has it that the horse was a symbol of the Saxon leader Widukind. But this one should have been black. The colour has been changed by Christian baptism of Widukind into white. White and red are the other colours (despite to Gold and black) of the Holy Roman Empire symbolizing Christ as the Saviour, who is still shown with a white flag with a red cross.




</doc>
<doc id="18439" url="https://en.wikipedia.org/wiki?curid=18439" title="LTJ Bukem">
LTJ Bukem

Daniel Williamson (born 20 September 1967), better known as LTJ Bukem (), is a British drum and bass musician, producer and DJ. He and his record label Good Looking are most associated with the jazzy, atmospheric side of drum and bass music.

Bukem was trained as a classical pianist and discovered jazz fusion in his teenage years, having a jazz funk band at one stage. By the late 1980s, he decided to become a DJ and gained fame in the rave scene of the early 1990s. As a producer, he released a series of drum and bass tracks such as "Logical Progression" (1991), "Demon's Theme" (1992), "Atlantis" and "Music" (1993). His most notable release was the track "Horizons" (1995) which attained considerable popularity, using the main melody from Lemon Sol's song "Sunflash".

He then dipped in visibility as a producer, with his work running the London club night Speed and his record label Good Looking Records, coming to the fore. A series of compilations entitled "Logical Progression" highlighted a jazz and ambient influenced side of drum and bass; the style became widely known as intelligent drum and bass. Bukem also explored the downtempo end of electronic lounge music, with sister label Cookin' and the "Earth" series of compilations. Some of the artists who rose to fame under Good Looking in this period include Blame, Seba, Big Bud, Blu Mar Ten, DJ Dream (Aslan Davis), Future Engineers, Tayla, Aquarius (an alias of Photek), Peshay, Source Direct and Artemis.

On 16 July 1995, he did an Essential Mix alongside MC Conrad. In 1997, he remixed the James Bond theme for David Arnold's concept album of James Bond music "". In 2000, he finally released a debut solo album, the double-CD "Journey Inwards". The album heavily emphasised his jazz fusion influences. 2001 saw a remix of Herbie Hancock.

He ran the Speed clubnight in London with fellow drum and bass DJ Fabio.

He DJs extensively around the world, often under the 'Progression Sessions' or 'Bukem in Session' banners. However, his former companion and vocalist, MC Conrad left the label and ultimately their musical partnership in 2012.

Daniel Williamson was adopted from birth. In 2007, he revealed that he had found his biological birth mother, a Ugandan woman living in Paris. She told him that his father was Egyptian.

Viewed as an innovator in the drum and bass style, Bukem is known for developing an accessible alternative to that hardcore genre's speedy, assaultive energies. His style pays homage to the Detroit-based sound of early techno, but Bukem also incorporates still earlier influences, particularly the mellow, melodic sonorities of 1970s era jazz fusion as exemplified by Lonnie Liston Smith and Roy Ayers. Early in his career, Bukem was identified for his response to the "almost paranoid hyperkinesis" of breakbeat-based house music, and specifically for his reservations regarding the overbearing force of the hardcore mentality.

Bukem's music from the early 1990s onward represents his efforts to map out an alternative future for drum and bass by incorporating softer-edged influences culled from London's 1980s rare groove and acid jazz scenes. Music on "Logical Progression" reveals these influences, as does his approach on 1993's "Music / Enchanted", which features string arrangements and sounds from nature. His use of keyboards, live vocals and slow-motion breaks on these and future releases earned Bukem's music the tag intelligent drum and bass. While this designation caused controversy within the drum and bass community, it also influenced the popularization of hardcore music in the UK during the mid-1990s.







</doc>
<doc id="18443" url="https://en.wikipedia.org/wiki?curid=18443" title="Lindsay Anderson">
Lindsay Anderson

Lindsay Gordon Anderson (17 April 1923 – 30 August 1994) was a British feature-film, theatre and documentary director, film critic, and leading-light of the Free Cinema movement and of the British New Wave. He is most widely remembered for his 1968 film "if...", which won the "Palme d'Or" at Cannes Film Festival in 1969 and marked Malcolm McDowell's cinematic debut.
He is also notable, though not a professional actor, for playing a minor role in the Academy Award-winning 1981 film "Chariots of Fire". McDowell produced a 2007 documentary about his experiences with Anderson, "Never Apologize".

Lindsay Gordon Anderson was born in Bangalore, South India, where his father had been stationed with the Royal Engineers, on 17 April 1923. His father Captain (later Major General) Alexander Vass Anderson was a British Army Officer who had been born in North India, and his mother Estelle Bell Gasson was born in Queenstown, South Africa, the daughter of a wool merchant. Lindsay's parents separated in 1926 and Estelle returned to England with her sons; however, they tried to reconcile in 1932 in Bangalore, and when Estelle returned to England she was pregnant with her third son, Alexander Vass Anderson. The Andersons divorced and Estelle remarried Major Cuthbert Sleigh in 1936. Lindsay's father remarried in India; although Gavin Lambert writes, in 'Mainly About Lindsay Anderson: A Memoir' (Faber and Faber, 2000, p. 18), that Alexander Vass Anderson 'cut (his first family) out of his life', making no reference to them in his 'Who's Who' entry, Lindsay often saw his father and looked after his house and dogs when he was away.

Both Lindsay and his older brother Murray Anderson (1919-2016) were educated at Saint Ronan's School in Worthing, West Sussex, and at Cheltenham College. It was at Cheltenham that Lindsay had met his lifelong friend and biographer, the screenwriter and novelist Gavin Lambert. Lindsay won a scholarship for classical studies at Wadham College at the University of Oxford, in 1942.

Anderson served in the Army from 1943 until 1946, first with the 60th King's Royal Rifle Groups, and then in the final year of World War II as a cryptographer for the Intelligence Corps, at the Wireless Experimental Centre in Delhi. Anderson assisted in nailing the Red flag to the roof of the Junior Officers' mess in Annan Parbat, in August 1945, after the victory of the Labour Party in the general election was confirmed. The colonel did not approve, he recalled a decade later, but no disciplinary action was taken against them.

Lindsay returned to Oxford in 1946 but changed from classical studies to English; he graduated with an MA in 1948.

Before going into film-making, Anderson was a prominent film critic writing for the influential "Sequence" magazine (1947–52), which he co-founded with Gavin Lambert, Peter Ericsson and Karel Reisz; later writing for the British Film Institute's journal "Sight and Sound" and the left-wing political weekly the "New Statesman". In a 1956 polemical article, "Stand Up, Stand Up" for "Sight and Sound", he attacked contemporary critical practices, in particular the pursuit of objectivity. Taking as an example some comments made by Alistair Cooke in 1935, where Cooke claimed to be without politics as a critic, Anderson responded:
Following a series of screenings which he and the National Film Theatre programmer Karel Reisz organized for the venue of independently produced short films by himself and others, he developed a philosophy of cinema which found expression in what became known, by the late-1950s, as the Free Cinema movement. This was the belief that the British cinema must break away from its class-bound attitudes and that non-metropolitan Britain ought to be shown on the nation's screens. He had already begun to make films himself, starting in 1948 with "Meet the Pioneers", a documentary about a conveyor-belt factory.

Along with Karel Reisz, Tony Richardson, and others, he secured funding from a variety of sources (including Ford of Britain) and they each made a series of short documentaries on a variety of subjects. One of Anderson's early short films, "Thursday's Children" (1954), concerning the education of deaf children, made in collaboration with Guy Brenton, a friend from his Oxford days, won an Oscar for Best Documentary Short in 1954. "Thursday's Children" was preserved by the Academy Film Archive in 2005.

These films, influenced by one of Anderson' heroes, the French filmmaker Jean Vigo, and made in the tradition of the British documentaries of Humphrey Jennings, foreshadowed much of the social realism of British cinema that emerged in the next decade, with Reisz's "Saturday Night and Sunday Morning" (1960), Richardson's "The Loneliness of the Long Distance Runner" (1962) and Anderson's own "This Sporting Life" (1963), produced by Reisz. Anderson's film met with mixed reviews at the time, and was not a commercial success.

Anderson is perhaps best remembered as a filmmaker for his "Mick Travis trilogy", all of which star Malcolm McDowell as the title character: "if..." (1968), a satire on public schools; "O Lucky Man!" (1973) a "Pilgrim's Progress" inspired road movie; and "Britannia Hospital" (1982), a fantasia taking stylistic influence from the populist wing of British cinema represented by Hammer horror films and Carry On comedies.

In 1981, Anderson played the role of the Master of Caius College at Cambridge University in the film "Chariots of Fire".

Anderson developed an acquaintance from 1950 with John Ford, which led to what has come to be regarded as one of the standard books on that director, Anderson's "About John Ford" (1983). Based on half a dozen meetings over more than two decades, and a lifetime's study of the man's work, the book has been described as "One of the best books published by a film-maker on a film-maker".

In 1985, producer Martin Lewis invited Anderson to chronicle Wham!'s visit to China, among the first-ever visits by Western pop artists, which resulted in Anderson's film "Foreign Skies: Wham! In China". He admitted in his diary on 31 March 1985, to having "no interest in Wham!", or China, and he was simply "'doing this for the money'". In 1986, he was a member of the jury at the 36th Berlin International Film Festival.

Anderson was also a significant British theatre director. He was long associated with London's Royal Court Theatre, where he was Co-Artistic Director 1969–70, and Associate Artistic Director 1971–75, directing premiere productions of plays by David Storey, among others.

In 1992, as a close friend of actresses Jill Bennett and Rachel Roberts, Anderson included a touching episode in his autobiographical BBC film "Is That All There Is?", with a boat trip down the River Thames (several of their professional colleagues and friends aboard) to scatter their ashes on the waters while musician Alan Price sang the song "Is That All There Is?".

Every year, the International Documentary Festival in Amsterdam (IDFA) gives an acclaimed filmmaker the chance to screen his or her personal Top 10 favorite films. In 2007, Iranian filmmaker Maziar Bahari selected "O Dreamland" and "Every Day Except Christmas" (1957), a record of a day in the old Covent Garden market, for his top 10 classics from the history of documentary.[3]

Gavin Lambert's memoir, "Mainly About Lindsay Anderson", in which he wrote that Anderson repressed his homosexuality, was seen as a betrayal by his other friends. In November 2006 Malcolm McDowell told "The Independent":
Anderson died from a heart attack on 30 August 1994 at the age of 71.

All Royal Court, London, unless otherwise indicated:






</doc>
<doc id="18444" url="https://en.wikipedia.org/wiki?curid=18444" title="Loch">
Loch

Loch () is the Irish, Scottish Gaelic and Scots word for a lake or for a sea inlet. It is cognate with the Manx lough, Cornish logh, and one of the Welsh words for lake, llwch.

In English English and Hiberno-English, the anglicised spelling lough is commonly found in place names; in Lowland Scots and Scottish English, the spelling "loch" is always used. Many loughs are connected to stories of lake-bursts, signifying their mythical origin.

Sea-inlet lochs are often called sea lochs or sea loughs. Some such bodies of water could also be called firths, fjords, estuaries, straits or bays. 

This name for a body of water is Insular Celtic in origin and is applied to most lakes in
Scotland and to many sea inlets in the west and north of Scotland. The word is Indo-European in origin; cf. Latin "lacus".

Lowland Scots orthography, like Scottish Gaelic, Welsh and Irish, represents with "ch", so the word was borrowed with identical spelling.

English borrowed the word separately from a number of loughs in the previous Cumbric language areas of Northumbria and Cumbria. Earlier forms of English included the sound as "gh" (compare Scots "bricht" with English "bright"). However, by the time Scotland and England joined under a single parliament, English had lost the sound. This form was therefore used when the English settled Ireland. The Scots convention of using "ch" remained, hence the modern Scottish English "loch".

In Welsh, what corresponds to "lo" is "lu" in Old Welsh and "llw" in Middle Welsh such as in today's Welsh placenames Llanllwchaiarn, Llwchwr, Llyn Cwm Llwch, Amlwch, Maesllwch, the Goidelic "lo" being taken into Scottish Gaelic by the gradual replacement of much Brittonic orthography with Goidelic orthography in Scotland.

Many of the loughs in Northern England have also previously been called "meres" (a Northern English dialect word for "lake" and an archaic Standard English word meaning "a lake that is broad in relation to its depth") such as the "Black Lough" in Northumberland. However, reference to the latter as "loughs" (lower case initial), rather than as "lakes", "inlets" and so on, is unusual.

Some lochs in Southern Scotland have a Brythonic rather than Goidelic etymology, such as Loch Ryan where the Gaelic "loch" has replaced a Cumbric equivalent of Welsh "llwch". The same is perhaps the case for water bodies in Northern England named with 'Low' or 'Lough' or otherwise it represents a borrowing of the Brythonic word into the Northumbrian dialect of Old English.

Although there is no strict size definition, a small loch is often known as a lochan (so spelled also in Scottish Gaelic; in Irish it is spelled lochán).

Perhaps the most famous Scottish loch is Loch Ness, although there are other large examples such as Loch Awe, Loch Lomond and Loch Tay.

Examples of sea lochs in Scotland include Loch Long, Loch Fyne, Loch Linnhe, and Loch Eriboll.

Some new reservoirs for hydroelectric schemes have been given names faithful to the names for natural bodies of water—for example, the Loch Sloy scheme, and Lochs Laggan and Treig (which form part of the Lochaber hydroelectric scheme near Fort William). Other expanses are simply called reservoirs, e.g. Blackwater Reservoir above Kinlochleven.

Scotland has very few bodies of water called lakes. The Lake of Menteith, an Anglicisation of the Scots "Laich o Menteith" meaning a "low-lying bit of land in Menteith", is applied to the loch there because of the similarity of the sounds of the words "laich" and "lake". Until the 19th century the body of water was known as the "Loch of Menteith". The Lake of the Hirsel, Pressmennan Lake and Lake Louise are man-made bodies of water in Scotland known as lakes.

The word "loch" is sometimes used as a shibboleth to identify natives of England, because the fricative sound is used in Scotland whereas most English people pronounce the word like "lock".

As "loch" is a common Gaelic word, it is found as the root of several Manx place names.

The United States naval port of Pearl Harbor, on the south coast of the main Hawaiian island of Oahu, is one of a complex of sea inlets. Several are named as lochs, including South East Loch, Merry Loch, East Loch, Middle Loch and West Loch.

Loch Raven Reservoir is a reservoir in Baltimore County, Maryland.

Brenton Loch in the Falkland Islands is a sea loch, near Lafonia, East Falkland.

In the Scottish settlement of Glengarry County in present-day Eastern Ontario, there is a lake called Loch Garry. Loch Garry was named by those who settled in the area, Clan MacDonell of Glengarry, after the well-known loch their clan is from, Loch Garry in Scotland. Similarly, lakes named Loch Broom, Big Loch, Greendale Loch, and Loch Lomond can be found in Nova Scotia, along with Loch Leven in Newfoundland, and Loch Leven in Saskatchewan.

Loch Fyne is a fjord in Greenland named by Douglas Clavering in 1823.



</doc>
<doc id="18446" url="https://en.wikipedia.org/wiki?curid=18446" title="Leo Marks">
Leo Marks

Leopold Samuel Marks, (24 September 1920 – 15 January 2001) was an English writer, screenwriter, and cryptographer. During the Second World War he headed the codes office supporting resistance agents in occupied Europe for the secret Special Operations Executive organisation. After the war, Marks became a playwright and screenwriter, writing scripts that frequently utilised his war-time cryptographic experiences. He wrote the script for "Peeping Tom", the controversial film directed by Michael Powell which had a disastrous effect on Powell's career, but was later described by Martin Scorsese as a masterpiece. In 1998, towards the end of his life, Marks published a personal history of his experiences during the war, "Between Silk and Cyanide", which was critical of the leadership of SOE.

Marks was the son of Benjamin Marks, the joint owner of Marks & Co, an antiquarian bookseller in Charing Cross Road, London. He was introduced at an early age to cryptography when his father showed him Edgar Allan Poe's story, "The Gold-Bug".

From this early interest, he demonstrated his skill at codebreaking by deciphering the secret price codes which his father wrote inside the covers of books. The bookshop subsequently became famous as a result of the book "84, Charing Cross Road", which was based on correspondence between American writer Helene Hanff and the shop's chief buyer, Frank Doel.

Marks was conscripted in January 1942, and trained as a cryptographer; apparently he demonstrated the ability to complete one week's work in decipherment exercise in a few hours. Unlike the rest of his intake, who were sent to the main British codebreaking centre at Bletchley Park, Marks was regarded as a misfit and he was assigned to the newly formed Special Operations Executive (SOE) in Baker Street, which was set up to train agents to operate behind enemy lines in occupied Europe and to assist local resistance groups. SOE has been described as "a mixture of brilliant brains and bungling amateurs". Marks wrote that he had an inauspicious arrival at SOE when it took him all day to decipher a code he had been expected to finish in 20 minutes, because, not atypically, SOE had forgotten to supply the cipher key, and he had to break the code which SOE had regarded as secure.

Marks briefed many Allied agents sent into occupied Europe, including Noor Inayat Khan, the Grouse/Swallow team of four Norwegian Telemark saboteurs and his own close friend 'Tommy' Yeo-Thomas, nicknamed "the White Rabbit". In an interview which accompanied the DVD of the film "Peeping Tom", Marks quoted General Eisenhower as saying that his group's work shortened the war by three months, saving countless lives.

Marks was portrayed by Anton Lesser in David Morley's BBC Radio drama "A Cold Supper Behind Harrods". The fiction play was inspired by conversations between Marks and David Morley and real events in SOE. It featured David Jason, and Stephanie Cole as Vera Atkins.

One of Marks's first challenges was to phase out double transposition ciphers using keys based on poems. These poem ciphers had the limited advantage of being easy to memorise, but significant disadvantages, including limited cryptographic security, substantial minimum message sizes (short ones were easy to crack), and the fact that the method's complexity caused encoding errors. Cryptographic security was enhanced by Marks's innovations, especially "worked-out keys". He was credited with inventing the letter one-time pad, but while he did independently discover the method, he later found it already in use at Bletchley.

While attempting to relegate poem codes to emergency use, he enhanced their security by promoting the use of original poems in preference to widely known ones, forcing a cryptanalyst to work it out the hard way for each message instead of guessing an agent's entire set of keys after breaking the key to a single message (or possibly just part of the key.) Marks wrote many poems later used by agents, the most famous being one he gave to the agent Violette Szabo, "The Life That I Have", which gained popularity when it was used in the 1958 film about her, "Carve Her Name With Pride". According to his book, Marks wrote the poem in Christmas 1943 about a girlfriend, Ruth, who had recently died in an air crash in Canada; supposedly the god-daughter of the head of SOE, Sir Charles Jocelyn Hambro.
The life that I have 
Is all that I have 
And the life that I have 
Is yours. 

The love that I have 
Of the life that I have 
Is yours and yours and yours. 

A sleep I shall have
A rest I shall have 
Yet death will be but a pause. 

For the peace of my years 
In the long green grass 
Will be yours and yours and yours.
Gestapo signal tracers endangered clandestine radio operators, and their life expectancy averaged about six weeks. Therefore, short and less frequent transmissions from the codemaster were of value. The pressure could cause agents to make mistakes encoding messages, and the practice was for the home station to tell them to recode it (usually a safe activity) and retransmit it (dangerous, and increasingly so the longer it took). In response to this problem, Marks established, staffed and trained a group based at Grendon Underwood, Buckinghamshire to cryptanalyse garbled messages ("indecipherables") so they could be dealt with in England without forcing the agent to risk retransmitting from the field. Other innovations of his simplified encoding in the field, which reduced errors and made shorter messages possible, both of which reduced transmission time.

The Germans generally did not execute captured radio operators out of hand. The goal was to turn and use them, or to extract enough information to imitate them. For the safety of entire underground "circuits", it was important to determine if an operator was genuine and still free, but means of independently checking were primitive. Marks claims that he became convinced (but was unable to prove) that their agents in the Netherlands had been compromised by the German counter-intelligence Abwehr. The Germans referred to their operation as "a game"—Das Englandspiel. Marks's warnings fell on deaf ears and perhaps as many as 50 further agents were sent to meet their deaths in Holland. The other side of this story was published in 1953 by Marks's German opposite number in the Netherlands, Hermann Giskes, in his book "London Calling North Pole".

In his book (pp. 222–3), Marks describes the memorandum he wrote detailing his conviction that messages from the Netherlands were being sent either by Germans or by agents who had been turned. He argued that, despite harrowing circumstances, "not a single Dutch agent has been so overwrought that he's made a mistake in his coding..." Marks had to face Brigadier (later Sir) Colin Gubbins:

Gubbins grills Marks. In particular he wants to know who has seen this report, who typed it (Marks did):

After the war, Marks went on to write plays and films, including "The Girl Who Couldn't Quite!" (1947), "Cloudburst" (1951), "The Best Damn Lie" (1957), "Guns at Batasi" (co-writer) (1964), "Sebastian" (1968) and "Twisted Nerve" (1968).

Marks wrote the script for Michael Powell's film "Peeping Tom" (1960), the story of a serial killer who films his victims while stabbing them. The film provoked critical revulsion, at the time, and was described as "evil and pornographic". The film was critically rehabilitated when younger directors, including Martin Scorsese, expressed admiration for Marks's script. Scorsese subsequently asked Marks to supply the voice of Satan in his 1988 film "The Last Temptation of Christ".

In 1998, Marks published his account of his work in SOE – "". The book was written in the early 1980s, but didn't receive UK Government approval for publication until 1998. Three of the poems published in the book were scrambled into the song "Dead Agents" by John Cale performed at the Institute of Contemporary Arts, London, in April 1999.

Marks describes himself as an agnostic in "Between Silk and Cyanide", but frequently refers to his Jewish heritage.

He married the portrait painter Elena Gaussen in 1966. The marriage lasted until shortly before his death at home from cancer in January 2001.




</doc>
<doc id="18448" url="https://en.wikipedia.org/wiki?curid=18448" title="Livonia">
Livonia

Livonia (, , , German and Scandinavian languages: "", archaic German: "Liefland", , Latvian and , , archaic English "Livland", "Liwlandia"; ) is a historical region on the eastern shores of the Baltic Sea. It is named after the Livonians, who lived on the shores of present-day Latvia.

By the end of the 13th century, the name was extended to most of present-day Estonia and Latvia that had been conquered during the Livonian Crusade (1193–1290) by the Livonian Brothers of the Sword. Medieval Livonia, or Terra Mariana, reached its greatest extent after Saint George's Night Uprising that in 1346 forced Denmark to sell the Duchy of Estonia (northern Estonia conquered by Denmark in the 13th century) to the State of the Teutonic Order. Livonia, as understood after the retreat of Denmark in 1346, bordered on the Gulf of Finland in the north, Lake Peipus and Russia to the east, and Lithuania to the south.

As a consequence of the Livonian War in the 16th century, the territory of Livonia was reduced to the southern half of Estonia and the northern half of Latvia.

The indigenous inhabitants of Livonia were various Finnic tribes in the north and Baltic tribes in the south. The descendants of the crusaders formed the nucleus of the new ruling class of Livonia after the Livonian Crusade, and eventually became known as Baltic Germans.

Beginning in the 12th century CE, Livonia became a target for economic and political expansion by Danes and Germans, particularly for the Hanseatic League and the Cistercian Order. 
Around 1160, Hanseatic traders from Lübeck established a trading post on the site of the future city of Riga, which Bishop Albrecht von Buxthoeven founded in 1201. He ordered (1215) the construction of a cathedral and became the first Prince-Bishop of Livonia.

Bishop Albert of Riga (Albert of Buxhoeveden) founded the military order of the Livonian Brothers of the Sword (, ) in 1202; Pope Innocent III sanctioned the establishment in 1204. The membership of the order comprised German "warrior monks". Alternative names of the order include the Christ Knights, Sword Brethren, and the Militia of Christ of Livonia. Following their defeat by Lithuanian forces in the Battle of Saule in 1236, the surviving Brothers merged into the Teutonic Order as an autonomous branch (1237) and became known as the Livonian Order.

Albert, bishop of Riga (or Prince-Bishop of Livonia), founded the Brotherhood to aid the Bishopric of Riga in the conversion of the pagan Curonians, Livonians, Semigallians, and Latgalians living on the shores of the Gulf of Riga. From its foundation, the undisciplined Order tended to ignore its supposed vassalage to the bishops. In 1218 Albert asked King Valdemar II of Denmark for assistance, but Valdemar instead arranged a deal with the Brotherhood and conquered the north of Estonia for Denmark. The Brotherhood had its headquarters at Fellin (Viljandi) in present-day Estonia, where the walls of the Master's castle stand. Other strongholds included Wenden (Cēsis), Segewold (Sigulda) and Ascheraden (Aizkraukle). The commanders of Fellin, Goldingen (Kuldīga), Marienburg (Alūksne), Reval (Tallinn), and the bailiff of Weißenstein (Paide) belonged to the five-member entourage of the Order's Master.

Pope Gregory IX asked the Brothers to defend Finland from Novgorodian attacks in his letter of 24 November 1232;
however, no known information regarding the knights' possible activities in Finland has survived. (Sweden eventually took over Finland after the Second Swedish Crusade in 1249.) In the Battle of Saule in 1236 the Lithuanians and Semigallians decimated the Order. This disaster led the surviving Brothers to become incorporated into the Order of Teutonic Knights in the following year, and from that point on they became known as the Livonian Order. They continued, however, to function in all respects (rule, clothing and policy) as an autonomous branch of the Teutonic Order, headed by their own Master (himself "de jure" subject to the Teutonic Order's Grand Master).

The Chronicle of Henry of Livonia from the 1220s gives a firsthand account of the Christianization of Livonia, granted as a fief by the Hohenstaufen ("de facto" but not known as) the King of Germany, Philip of Swabia (), to Bishop Albert of Buxthoeven, nephew of the Hartwig II, Archbishop of Bremen, who sailed (1200) with a convoy of ships filled with armed crusaders to carve out a Catholic territory in the east as part of the Livonian Crusade.

Livonia consisted of the following subdivisions: 

The Livonian Rhymed Chronicle describes the conquest of Livonia by the Germans.

The Livonian Order was a largely autonomous branch of the Teutonic Knights (or Teutonic Order) and a member of the Livonian Confederation from 1418 to 1561. After being defeated by Lithuanian forces in the 1236 Battle of Saule, the remnants of the Livonian Brothers of the Sword were incorporated into the Teutonic Knights as the Livonian Order in 1237. Between 1237 and 1290, the Livonian Order conquered all of Courland, Livonia, and Semigallia, but their attack on northern Russia was repelled in the Battle of Rakvere (1268). In 1346, after the St. George's Night Uprising the Order purchased the rest of Estonia from King Valdemar IV of Denmark. The Chronicle of Henry of Livonia and the Livonian Rhymed Chronicle describe conditions within the Order's territory. The Teutonic Order fell into decline following its defeat in the Battle of Grunwald in 1410 and the secularization of its Prussian territories by Albert of Brandenburg in 1525, but the Livonian Order managed to maintain an independent existence. During the many years of the Livonian War (1558–1582), however, they suffered a decisive defeat at the hands of troops of Muscovite Russia in the Battle of Ergeme in 1560 and continued living under great threat. Letters to the Holy Roman Emperor arrived from many European countries, warning, "that Moscow has its eyes on much more than only a few harbors or the province of Liefland" ... the East Sea (Ostsee-Baltic Sea and the West Sea (Atlantic) are equally in danger. Duke Barnim the Elder, 50 years duke of Pomerania, warned, "that never before did he experience the fear than now, where even in his land, where people send by Moscow are everywhere". At stake was the Narva-trade-route and practically all trade in the North, and with that all of Europe. Due to the religious upheavals of the Reformation the distant Holy Roman Empire could not send troops, which it could not afford anyway. The Duchy of Prussia was not able to help for much of the same reason, and Duke Albrecht () was under continuous ban by the Empire. The Hanseatic League was greatly weakened by this and the city state of Luebeck fought its last great war. The emperor Maximilian II () diffused the greatest threat by remaining on friendly terms with Tsar Ivan IV of Russia (), but not sending Ivan troops as requested in his struggles with the Polish–Lithuanian Commonwealth.

In 1570 Tsar Ivan of Russia installed Duke Magnus as King of Livonia. The other forces opposed this appointment. The Livonian Order saw no other way than to seek protection from Sigismund II Augustus (King of Poland and Grand Duke of Lithuania), who had intervened in a war between Bishop William of Riga and the Brothers in 1557. After coming to an agreement with Sigismund II Augustus and his representatives (especially Mikołaj "the Black" Radziwiłł), the last Livonian Master, Gotthard Kettler, secularized the Order and converted to Lutheranism. In the southern part of the Brothers' lands he set up the Duchy of Courland and Semigallia for his family. Most of the remaining lands were seized by the Grand Duchy of Lithuania. Denmark and Sweden re-occupied the north of Estonia.

From the 14th to the 16th centuries, Middle Low German - as spoken in the towns of the Hanseatic League - functioned as the established language of the Livonian lands, but High German subsequently succeeded it as the official language in the course of the 16th and 17th centuries.
In 1418, the Archbishop of Riga, Johannes Ambundii, organised the five ecclesiastical states of the Holy Roman Empire in Medieval Livonia (Livonian Order, Courland, Ösel–Wiek, Dorpat and Riga) into the Livonian Confederation.
A diet or "Landtag" was formed in 1419. The city of Walk was chosen as the site of the diet.

Ferdinand I, Holy Roman Emperor once again asked for help of Gustav I of Sweden, and the Kingdom of Poland also began direct negotiations with Gustav, but nothing resulted because on 29 September 1560, Gustav I Vasa died. The chances for success of Magnus, (who had become Bishop of Courland and of Ösel-Wiek) in 1560 and his supporters looked particularly good in 1560 (and in 1570). In 1560 he had been recognised as their sovereign by the Bishopric of Ösel-Wiek and by the Bishopric of Courland, and as their prospective ruler by the authorities of the Bishopric of Dorpat; the Bishopric of Reval with the Harrien-Wierland gentry were on his side; the Livonian Order conditionally recognised his right of ownership of Estonia (Principality of Estonia). Then along with Archbishop Wilhelm von Brandenburg of the Archbishopric of Riga and his Coadjutor Christoph von Mecklenburg, Kettler, the last Master of the Teutonic Order, gave to Magnus the portions of the Kingdom of Livonia which he had taken possession of, but they refused to give him any more land.

Once Eric XIV of Sweden became king in September 1560 he took quick actions to get involved in the war. He negotiated a continued peace with Muscovy and spoke to the burghers of Reval city. He offered them goods to submit to him as well as threatening them. By 6 June 1561 they submitted to him contrary to the persuasions of Kettler to the burghers. King Eric's brother Johan married the Polish princess Catherine Jagiellon in 1562. Wanting to obtain his own land in Livonia, he loaned Poland money and then claimed the castles they had pawned as his own instead of using them to pressure Poland. After Johan returned to Finland, Erik XIV forbade him to deal with any foreign countries without his consent.

Shortly after that Erik XIV quickly lost any allies he was about to obtain, either in the form of Magnus or of the Archbishop of Riga. Magnus was upset he had been tricked out of his inheritance of Holstein. After Sweden occupied Reval, Frederick II of Denmark made a treaty with Erik XIV of Sweden in August 1561. Magnus and his brother Frederick II were in great disagreement, and Frederick II negotiated a treaty with Ivan IV on 7 August 1562 to help his brother obtain more land and to stall further Swedish advances. Erik XIV did not like this and the Northern Seven Years' War (1563-1570) broke out, with Sweden pitted against the Free City of Lübeck, Denmark, and Poland. While only losing land and trade, Frederick II and Magnus were not faring well. But in 1568 Erik XIV became insane and his brother Johan took his place as King John III of Sweden.

Johan III, due to his friendship with Poland, began a policy against Muscovy. He would try to obtain more land in Livonia and to dominate Denmark. After all parties had been financially drained, Frederick II let his ally, King Sigismund II Augustus of Polish–Lithuanian Commonwealth, know that he was ready for peace. On 15 December 1570, the Treaty of Stettin concluded the Northern Seven Years' War.

It is, however, more difficult to estimate the scope and magnitude of the support Magnus received in Livonian cities. Compared to the Harrien-Wierland gentry, the Reval city council, and hence probably the majority of citizens, demonstrated a much more reserved attitude towards Denmark and towards King Magnus of Livonia. Nevertheless, there is no reason to speak about any strong pro-Swedish sentiments among the residents of Reval. The citizens who had fled to the Bishopric of Dorpat or had been deported to Muscovy hailed Magnus as their saviour until 1571. Analysis indicates that during the Livonian War a pro-independence wing emerged among the Livonian gentry and townspeople, forming the so-called "Peace Party". Dismissing hostilities, these forces perceived an agreement with Muscovy as a chance to escape the atrocities of war and to avoid the division of Livonia. Thus Magnus, who represented Denmark and later struck a deal with Ivan IV the Terrible, proved a suitable figurehead for this faction.

The Peace Party, however, had its own armed forces – scattered bands of household troops ("Hofleute") under diverse command, which only united in action in 1565 (Battle of Pärnu and Siege of Reval), in 1570–1571 (Siege of Reval; 30 weeks), and in 1574–1576 (first on Sweden's side, then came the sale of Ösel–Wiek to the Danish Crown, and the loss of territory to Tsardom of Russia). In 1575, after Muscovy attacked Danish claims in Livonia, Frederick II dropped out of the competition, as did the Holy Roman Emperor. After this Johan III held off on his pursuit for more land due to Muscovy obtaining lands that Sweden controlled. He used the next two years of truce to get in a better position. In 1578 he resumed the fight, not only for Livonia, but also for everywhere due to an understanding he made with the Rzeczpospolita. In 1578 Magnus retired to the Rzeczpospolita and his brother all but gave up the land in Livonia.

In 1561, during the Livonian War, Livonia fell to the Grand Duchy of Lithuania and became a dependent vassal of Lithuania. Eight years later, in 1569, when the Grand Duchy of Lithuania and the Kingdom of Poland formed the Polish–Lithuanian Commonwealth, Livonia became a joint domain administered directly by the king and grand duke. 
Having rejected peace proposals from its enemies, Ivan the Terrible found himself in a difficult position by 1579, when Crimean Khanate devastated Muscovian territories and burnt down Moscow (see Russo-Crimean Wars), the drought and epidemics have fatally affected the economy, Oprichnina had thoroughly disrupted the government, while The Grand Principality of Lithuania had united with The Kingdom of Poland (1385–1569) and acquired an energetic leader, Stefan Batory, supported by Ottoman Empire (1576). Stefan Batory replied with a series of three offensives against Muscovy, trying to cut The Kingdom of Livonia from Muscovian territories. During his first offensive in 1579, with 22,000 men, he retook Polotsk; during the second, in 1580, with 29,000-strong army, he took Velikie Luki, and in 1581 with a 100,000-strong army he started the Siege of Pskov. Frederick II of Denmark and Norway had trouble continuing the fight against Muscovy unlike Sweden and Poland. He came to an agreement with John III in 1580 giving him the titles in Livonia. That war would last from 1577 to 1582. Muscovy recognized Polish–Lithuanian control of Ducatus Ultradunensis only in 1582. After Magnus von Lyffland died in 1583, Poland invaded his territories in The Duchy of Courland and Frederick II decided to sell his rights of inheritance. Except for the island of Œsel, Denmark was out of the Baltic by 1585. As of 1598 Inflanty Voivodeship was divided onto:

Based on a guarantee by Sigismund II Augustus from the 1560s, the German language retained its official status.

The armies of Ivan the Terrible were initially successful, taking Polotsk (1563) and Parnawa (1575) and overrunning much of Grand Duchy of Lithuania up to some proximity of Vilnius. Eventually, the Grand Duchy of Lithuania and Kingdom of Poland formed the Polish–Lithuanian Commonwealth in 1569 under the Union of Lublin. Eric XIV of Sweden did not like this and the Northern Seven Years' War between the Free City of Lübeck, Denmark, Poland, and Sweden broke out. While only losing land and trade, Frederick II of Denmark and Magnus von Lyffland of the Œsel-Wiek did not fare well. But in 1569, Erik XIV became insane and his brother John III of Sweden took his place. After all parties had been financially drained, Frederick II let his ally, King Zygmunt II August, know that he was ready for peace. On 15 December 1570, the Treaty of Stettin was concluded.

In the next phase of the conflict, in 1577 Ivan IV took advantage of the Commonwealth's internal strife (called the war against Gdańsk in Polish historiography), and during the reign of Stefan Batory in Poland, invaded Livonia, quickly taking almost the entire territory, with the exception of Riga and Reval. In 1578, Magnus of Livonia recognized the sovereignty of the Polish–Lithuanian Commonwealth (not ratified by the Sejm of Poland-Lithuania, or recognized by Denmark). The Kingdom of Livonia was beaten back by Muscovy on all fronts. In 1578, Magnus of Livonia retired to The Bishopric of Courland and his brother all but gave up the land in Livonia.

Sweden was given roughly the same area as the former Duchy of Livonia after the 1626–1629 Polish–Swedish War. The area, usually known as Swedish Livonia, became a very important Swedish dominion, with Riga being the second largest Swedish city and Livonia paying for one third of the Swedish war costs. Sweden lost Swedish Livonia, Swedish Estonia and Ingria to the Russian Empire almost 100 years later, by the Capitulation of Estonia and Livonia in 1710 and the Treaty of Nystad in 1721.

The Livonian Voivodeship (; ) was a unit of administrative division and local government in the Duchy of Livonia, part of the Polish–Lithuanian Commonwealth, since it was formed in the 1620s out of the Wenden Voivodeship till the First Partition of Poland in 1772.

The Russian Empire conquered Swedish Livonia during the course of the Great Northern War and acquired the province in the Capitulation of Estonia and Livonia in 1710, confirmed by the Treaty of Nystad in 1721. Peter the Great confirmed German as the exclusive official language. Russia then added Polish Livonia in 1772 during the Partitions of Poland.

In 1796 the Riga Governorate was renamed as the Governorate of Livonia ( / , , ). Livonia remained within the Russian Empire until the end of World War I, when it was split between the newly independent states of Latvia and Estonia. In 1918–1920, both Soviet troops and German Freikorps fought against Latvian and Estonian troops for control over Livonia, but their attempts were defeated.

From 1845 to 1876, the Baltic governorates of Estonia, Livonia, and Courland—an area roughly corresponding to the historical medieval Livonia—were administratively subordinated to a common Governor-General. Amongst the holders of this post were Count Alexander Arkadyevich Suvorov and Count Pyotr Andreyevich Shuvalov.

In independent Latvia between the World Wars, southern Livonia became an administrative region under the traditional Latvian name Vidzeme, encompassing the then much larger counties of Riga, Cēsis, Valmiera, and Valka.

Ostland was one of the Reichskommissariats established, by a Decree of the Führer dated 17 July 1941, as administrative units of the "Großdeutsches Reich" (Greater Germany). The structure of the Reichskommissariats was defined by the same decree. Local administration in the Reichskommissariats was to be organized under a "National Director" ("Reichskomissar") in Estonia, a "General Director" in Latvia and a "General Adviser" in Lithuania. The local administration of the Reichskommissariat Ostland was under "Reichskomissar" Hinrich Lohse. Below him there was an administrative hierarchy: a "Generalkomissar" led each "Generalbezirke", "Gebietskomissars" and "Hauptkommissars" administered "Kreigsbietes" and "Hauptgenbietes", respectively. Alfred Rosenberg's (Minister für die besetzten Ostgebiete (Reich Ministry for the Occupied Eastern Territories) ministerial authority was, in practice, severely limited. The first reason was that many of the practicalities were commanded elsewhere: the Wehrmacht and the SS managed the military and security aspects, Fritz Saukel (Reich Director of Labour) had control over manpower and working areas, Hermann Göring and Albert Speer had total management of economic aspects in the territories and the Reich postal service administered the East territories' postal services. These German central government interventions in the affairs of Ostland, overriding the appropriate ministries was known as "Sonderverwaltungen" (special administration). Later, from September, the civil administration that had been decreed in the previous July was actually set up. Lohse and, for that matter, Koch would not bow to his authority seeking to administer their territories with the independence and authority of gauleiters. on 1 April 1942 an "arbeitsbereich" (lit. "working sphere", a name for the party cadre organisation outside the reich proper) was established in the civil administration part of the occupied Soviet territories, whereupon Koch and Lohse gradually ceased communication with him, preferring to deal directly with Hitler through Martin Bormann and the party chancellery. In the process they also displaced all other actors including notably the SS, except in central Belarus where HSSPF Erich von dem Bach-Zelewsky had a special command encompassing both military and civil administration territories and engaged in "anti-partisan" atrocities.

The historical land of Livonia has been split between Latvia and Estonia ever since. The Livonian language is spoken by fewer than 100 individuals as a second language, and is understood to be fast approaching extinction. The last native Livonian speaker died in June 2013.
The anthem (unofficial) of Livonians is "Min izāmō, min sindimō" sharing the melody of Finnish and Estonian anthems.



</doc>
<doc id="18450" url="https://en.wikipedia.org/wiki?curid=18450" title="Lung cancer">
Lung cancer

Lung cancer, also known as lung carcinoma, is a malignant lung tumor characterized by uncontrolled cell growth in tissues of the lung. This growth can spread beyond the lung by the process of metastasis into nearby tissue or other parts of the body. Most cancers that start in the lung, known as primary lung cancers, are carcinomas. The two main types are small-cell lung carcinoma (SCLC) and non-small-cell lung carcinoma (NSCLC). The most common symptoms are coughing (including coughing up blood), weight loss, shortness of breath, and chest pains.
The vast majority (85%) of cases of lung cancer are due to long-term tobacco smoking. About 10–15% of cases occur in people who have never smoked. These cases are often caused by a combination of genetic factors and exposure to radon gas, asbestos, second-hand smoke, or other forms of air pollution. Lung cancer may be seen on chest radiographs and computed tomography (CT) scans. The diagnosis is confirmed by biopsy which is usually performed by bronchoscopy or CT-guidance.
Avoidance of risk factors, including smoking and air pollution, is the primary method of prevention. Treatment and long-term outcomes depend on the type of cancer, the stage (degree of spread), and the person's overall health. Most cases are not curable. Common treatments include surgery, chemotherapy, and radiotherapy. NSCLC is sometimes treated with surgery, whereas SCLC usually responds better to chemotherapy and radiotherapy.
Worldwide in 2012, lung cancer occurred in 1.8 million people and resulted in 1.6 million deaths. This makes it the most common cause of cancer-related death in men and second most common in women after breast cancer. The most common age at diagnosis is 70 years. In the United States, five-year survival rate is 19.4%, while in Japan it is 41.4%. Outcomes on average are worse in the developing world.
Signs and symptoms which may suggest lung cancer include:

If the cancer grows in the airways, it may obstruct airflow, causing breathing difficulties. The obstruction can also lead to accumulation of secretions behind the blockage, and increase the risk of pneumonia.

Depending on the type of tumor, paraneoplastic phenomena — symptoms not due to the local presence of cancer — may initially attract attention to the disease. In lung cancer, these phenomena may include hypercalcemia, syndrome of inappropriate antidiuretic hormone (SIADH, abnormally concentrated urine and diluted blood), ectopic ACTH production, or Lambert–Eaton myasthenic syndrome (muscle weakness due to autoantibodies). Tumors in the top of the lung, known as Pancoast tumors, may invade the local part of the sympathetic nervous system, resulting in Horner's syndrome (dropping of the eyelid and a small pupil on that side), as well as damage to the brachial plexus.

Many of the symptoms of lung cancer (poor appetite, weight loss, fever, fatigue) are not specific. In many people, the cancer has already spread beyond the original site by the time they have symptoms and seek medical attention. Symptoms that suggest the presence of metastatic disease include weight loss, bone pain, and neurological symptoms (headaches, fainting, convulsions, or limb weakness). Common sites of spread include the brain, bone, adrenal glands, opposite lung, liver, pericardium, and kidneys. About 10% of people with lung cancer do not have symptoms at diagnosis; these cancers are incidentally found on routine chest radiography.

Cancer develops after genetic damage to DNA and epigenetic changes. Those changes affect the cell's normal functions, including cell proliferation, programmed cell death (apoptosis), and DNA repair. As more damage accumulates, the risk for cancer increases.

Tobacco smoking is by far the main contributor to lung cancer. Cigarette smoke contains at least 73 known carcinogens, including benzo["a"]pyrene, NNK, 1,3-butadiene, and a radioactive isotope of polonium – polonium-210. Across the developed world, 90% of lung cancer deaths in men and 70% of those in women during the year 2000 were attributed to smoking. Smoking accounts for about 85% of lung cancer cases. A 2014 review found that vaping may be a risk factor for lung cancer but less than that of cigarettes.

Passive smoking – the inhalation of smoke from another's smoking – is a cause of lung cancer in nonsmokers. A passive smoker can be defined as someone either living or working with a smoker. Studies from the US, the UK and other European countries have consistently shown a significantly-increased risk among those exposed to passive smoking. Those who live with someone who smokes have a 20–30% increase in risk while those who work in an environment with secondhand smoke have a 16–19% increase in risk. Investigations of sidestream smoke suggest that it is more dangerous than direct smoke. Passive smoking results in roughly 3,400 lung cancer-related deaths each year in the US.

Marijuana smoke contains many of the same carcinogens as those found in tobacco smoke, however, the effect of smoking cannabis on lung cancer risk is not clear. A 2013 review did not find an increased risk from light to moderate use. A 2014 review found that smoking cannabis doubled the risk of lung cancer, though cannabis is in many countries commonly mixed with tobacco.

Radon is a colorless and odorless gas generated by the breakdown of radioactive radium, which in turn is the decay product of uranium, found in the Earth's crust. The radiation decay products ionize genetic material, causing mutations that sometimes become cancerous. Radon is the second most-common cause of lung cancer in the US, causing about 21,000 deaths each year. The risk increases 8–16% for every 100 Bq/m³ increase in the radon concentration. Radon gas levels vary by locality and the composition of the underlying soil and rocks. About one in 15 homes in the US have radon levels above the recommended guideline of 4 picocuries per liter (pCi/l) (148 Bq/m³).

Asbestos can cause a variety of lung diseases such as lung cancer. Tobacco smoking and asbestos both have synergistic effects on the development of lung cancer. In smokers who work with asbestos, the risk of lung cancer is increased 45-fold compared to the general population. Asbestos can also cause cancer of the pleura, called mesothelioma – which actually is different from lung cancer.

Outdoor air pollutants, especially chemicals released from the burning of fossil fuels, increase the risk of lung cancer. Fine particulates (PM) and sulfate aerosols, which may be released in traffic exhaust fumes, are associated with a slightly-increased risk. For nitrogen dioxide, an incremental increase of 10 parts per billion increases the risk of lung cancer by 14%. Outdoor air pollution is estimated to cause 1–2% of lung cancers.

Tentative evidence supports an increased risk of lung cancer from indoor air pollution in relation to the burning of wood, charcoal, dung, or crop residue for cooking and heating. Women who are exposed to indoor coal smoke have roughly twice the risk, and many of the by-products of burning biomass are known or suspected carcinogens. This risk affects about 2.4 billion people worldwide, and it is believed to result in 1.5% of lung cancer deaths.

About 8% of lung cancer is caused by inherited factors. In relatives of people that are diagnosed with lung cancer, the risk is doubled, likely due to a combination of genes. Polymorphisms on chromosomes 5, 6, and 15 are known to affect the risk of lung cancer. Single-nucleotide polymorphisms (SNPs) of the genes encoding the nicotinic acetylcholine receptor (nAChR) – "CHRNA5", "CHRNA3", and "CHRNB4" – are of those associated with an increased risk of lung cancer, as well as "RGS17" – a gene regulating G-protein signaling.

Numerous other substances, occupations, and environmental exposures have been linked to lung cancer. The International Agency for Research on Cancer (IARC) states that there is some "sufficient evidence" to show that the following are carcinogenic in the lungs:

Similar to many other cancers, lung cancer is initiated by either the activation of oncogenes or the inactivation of tumor suppressor genes. Carcinogens cause mutations in these genes that induce the development of cancer.

Mutations in the "K-ras" proto-oncogene cause roughly 10–30% of lung adenocarcinomas. Nearly 4% of non-small-cell lung carcinomas involve an EML4-ALK tyrosine kinase fusion gene.

Epigenetic changes such as alteration of DNA methylation, histone tail modification, or microRNA regulation may result in the inactivation of tumor suppressor genes. Importantly, cancer cells develop resistance to oxidative stress, which enables them to withstand and exacerbate inflammatory conditions that inhibit the activity of the immune system against the tumor.

The epidermal growth factor receptor (EGFR) regulates cell proliferation, apoptosis, angiogenesis, and tumor invasion. Mutations and amplification of EGFR are common in non-small-cell lung carcinoma, and they provide the basis for treatment with EGFR-inhibitors. "Her2/neu" is affected less frequently. Other genes that are often mutated or amplified include "c-MET", "NKX2-1", "LKB1", "PIK3CA", and "BRAF".

The cell lines of origin are not fully understood. The mechanism may involve the abnormal activation of stem cells. In the proximal airways, stem cells that express keratin 5 are more likely to be affected, typically leading to squamous-cell lung carcinoma. In the middle airways, implicated stem cells include club cells and neuroepithelial cells that express club cell secretory protein. Small-cell lung carcinoma may originate from these cell lines or neuroendocrine cells, and it may express CD44.

Metastasis of lung cancer requires transition from epithelial to mesenchymal cell type. This may occur through the activation of signaling pathways such as Akt/GSK3Beta, MEK-ERK, Fas, and Par6.

Performing a chest radiograph is one of the first investigative steps if a person reports symptoms that may be suggestive of lung cancer. This may reveal an obvious mass, the widening of the mediastinum (suggestive of spread to lymph nodes there), atelectasis (lung collapse), consolidation (pneumonia), or pleural effusion. CT imaging of the chest may reveal a spiculated mass which is highly suggestive of lung cancer, and is also used to provide more information about the type and extent of disease. Bronchoscopic or CT-guided biopsy is often used to sample the tumor for histopathology.

Lung cancer often appears as a solitary pulmonary nodule on a chest radiograph. However, the differential diagnosis is wide. Many other diseases can also give this appearance, including metastatic cancer, hamartomas, and infectious granulomas caused by tuberculosis, histoplasmosis or coccidioidomycosis. Lung cancer can also be an incidental finding, as a solitary pulmonary nodule on a chest radiograph or CT scan done for an unrelated reason. The definitive diagnosis of lung cancer is based on the histological examination of the suspicious tissue in the context of the clinical and radiological features.

Clinical practice guidelines recommend frequencies for pulmonary nodule surveillance. CT imaging should not be used for longer or more frequently than indicated, as the extended surveillance exposes people to increased radiation and is costly.

Lung cancers are classified according to histological type. This classification is important for determining both the management and predicting outcomes of the disease. Lung cancers are carcinomas – malignancies that arise from epithelial cells. Lung carcinomas are categorized by the size and appearance of the malignant cells seen by a histopathologist under a microscope. For therapeutic purposes, two broad classes are distinguished: non-small-cell lung carcinoma and small-cell lung carcinoma.

The three main subtypes of NSCLC are adenocarcinoma, squamous-cell carcinoma, and large-cell carcinoma. Rare subtypes include pulmonary enteric adenocarcinoma.

Nearly 40% of lung cancers are adenocarcinoma, which usually comes from peripheral lung tissue. Although most cases of adenocarcinoma are associated with smoking, adenocarcinoma is also the most-common form of lung cancer among people who have smoked fewer than 100 cigarettes in their lifetimes ("never-smokers") and ex-smokers with a modest smoking history. A subtype of adenocarcinoma, the bronchioloalveolar carcinoma, is more common in female never-smokers, and may have a better long-term survival.

Squamous-cell carcinoma causes about 30% of lung cancers. They typically occur close to large airways. A hollow cavity and associated cell death are commonly found at the center of the tumor.

About 10 to 15% of lung cancers are large-cell carcinoma. These are so named because the cancer cells are large, with excess cytoplasm, large nuclei, and conspicuous nucleoli.

In SCLC, the cells contain dense neurosecretory granules (vesicles containing neuroendocrine hormones), which give this tumor an endocrine or paraneoplastic syndrome association. Most cases arise in the larger airways (primary and secondary bronchi). Sixty to seventy percent have extensive disease (which cannot be targeted within a single radiation therapy field) at presentation.

Four main histological subtypes are recognised, although some cancers may contain a combination of different subtypes, such as adenosquamous carcinoma. Rare subtypes include carcinoid tumors, bronchial gland carcinomas, and sarcomatoid carcinomas.

The lungs are a common place for the spread of tumors from other parts of the body. Secondary cancers are classified by the site of origin; for example, breast cancer that has been spread to the lung is called metastatic breast cancer. Metastases often have a characteristic round appearance on chest radiograph.

Primary lung cancers also most commonly metastasize to the brain, bones, liver, and adrenal glands. Immunostaining of a biopsy usually helps determine the original source. The presence of Napsin-A, TTF-1, CK7, and CK20 help confirm the subtype of lung carcinoma. SCLC that originates from neuroendocrine cells may express CD56, neural cell adhesion molecule, synaptophysin, or chromogranin.

Lung cancer staging is an assessment of the degree of spread of the cancer from its original source. It is one of the factors affecting both the prognosis and the potential treatment of lung cancer.

The evaluation of non-small-cell lung carcinoma (NSCLC) staging uses the TNM classification (tumor, node, metastasis). This is based on the size of the primary tumor, lymph node involvement, and distant metastasis.

Using the TNM descriptors, a group is assigned, ranging from occult cancer, through stages 0, IA (one-A), IB, IIA, IIB, IIIA, IIIB, and IV (four). This stage group assists with the choice of treatment and estimation of prognosis.

SCLC has traditionally been classified as "limited stage" (confined to one-half of the chest and within the scope of a single tolerable radiotherapy field) or "extensive stage" (more widespread disease). However, the TNM classification and grouping are useful in estimating prognosis.

For both NSCLC and SCLC, the two general types of staging evaluations are clinical staging and surgical staging. Clinical staging is performed before definitive surgery. It is based on the results of imaging studies (such as CT scans and PET scans) and biopsy results. Surgical staging is evaluated either during or after the operation. It is based on the combined results of surgical and clinical findings, including surgical sampling of thoracic lymph nodes.

Smoking prevention and smoking cessation are effective ways of preventing the development of lung cancer.

While in most countries industrial and domestic carcinogens have been identified and banned, tobacco smoking is still widespread. Eliminating tobacco smoking is a primary goal in the prevention of lung cancer, and smoking cessation is an important preventive tool in this process.

Policy interventions to decrease passive smoking in public areas such as restaurants and workplaces have become more common in many Western countries. Bhutan has had a complete smoking ban since 2005 while India introduced a ban on smoking in public in October 2008. The World Health Organization has called for governments to institute a total ban on tobacco advertising to prevent young people from taking up smoking. They assess that such bans have reduced tobacco consumption by 16% where instituted.

Cancer screening uses medical tests to detect disease in large groups of people who have no symptoms. For individuals with high risk of developing lung cancer, computed tomography (CT) screening can detect cancer and give a person options to respond to it in a way that prolongs life. This form of screening reduces the chance of death from lung cancer by an absolute amount of 0.3% (relative amount of 20%). High risk people are those age 55–74 who have smoked equivalent amount of a pack of cigarettes daily for 30 years including time within the past 15 years.

CT screening is associated with a high rate of falsely positive tests which may result in unneeded treatment. For each true positive scan there are about 19 falsely positives scans. Other concerns include radiation exposure and the cost of testing along with follow up. Research has not found two other available tests—sputum cytology or chest radiograph (CXR) screening tests—to have any benefit.

The United States Preventive Services Task Force (USPSTF) recommends yearly screening using low-dose computed tomography in those who have a total smoking history of 30 pack-years and are between 55 and 80 years old until a person has not been smoking for more than 15 years. Screening should not be done in those with other health problems that would make treatment of lung cancer if found not an option. The English National Health Service was in 2014 re-examining the evidence for screening.

The long-term use of supplemental vitamin A, vitamin C, vitamin D or vitamin E does not reduce the risk of lung cancer. Some studies have found vitamin A, B, and E may increase the risk of lung cancer in those who have a history of smoking.

Some studies suggest that people who eat diets with a higher proportion of vegetables and fruit tend to have a lower risk, but this may be due to confounding—with the lower risk actually due to the association of a high fruit and vegetables diet with less smoking. Several rigorous studies have not demonstrated a clear association between diet and lung cancer risk, although meta-analysis that accounts for smoking status may show benefit from a healthy diet.

Treatment for lung cancer depends on the cancer's specific cell type, how far it has spread, and the person's performance status. Common treatments include palliative care, surgery, chemotherapy, and radiation therapy. Targeted therapy of lung cancer is growing in importance for advanced lung cancer. People who have lung cancer should be encouraged to stop smoking. There is no clear evidence which smoking cessation program is most effective for people who have been diagnosed with lung cancer. It is unclear if exercise training is beneficial for people living with advanced lung cancer. Exercise training may benefit people with NSCLC who are recovering from lung surgery. In addition, exercise training can benefit people with NSCLC who have received radiotherapy, chemotherapy, chemoradiotherapy, or palliative care.

Exercise training before lung cancer surgery improves outcomes. A home-based component in rehabilitation is also useful. Even though it is uncertain if home-based prehabilitation leads to less adverse events or hospitalization time, rehabilitation with a home-based component may improve recovery after treatment and overall lung health.

If investigations confirm NSCLC, the stage is assessed to determine whether the disease is localized and amenable to surgery or if it has spread to the point where it cannot be cured surgically. CT scan and positron emission tomography (PET-CT), non-invasive tests, can be used to help rule out malignancy or mediastinal lymph node involvement. If mediastinal lymph node involvement is suspected using PET-CT, the nodes should be sampled (using a biopsy) to assist staging, a PET-CT scan is not accurate enough to be used alone. Techniques used for obtaining a sample include transthoracic needle aspiration, transbronchial needle aspiration (with or without endobronchial ultrasound), endoscopic ultrasound with needle aspiration, mediastinoscopy, and thoracoscopy. Blood tests and pulmonary function testing are used to assess whether a person is well enough for surgery. If pulmonary function tests reveal poor respiratory reserve, surgery may not be possible.

In most cases of early-stage NSCLC, removal of a lobe of lung (lobectomy) is the surgical treatment of choice. In people who are unfit for a full lobectomy, a smaller sublobar excision (wedge resection) may be performed. However, wedge resection has a higher risk of recurrence than lobectomy. Radioactive iodine brachytherapy at the margins of wedge excision may reduce the risk of recurrence. Rarely, removal of a whole lung (pneumonectomy) is performed. Video-assisted thoracoscopic surgery (VATS) and VATS lobectomy use a minimally invasive approach to lung cancer surgery. VATS lobectomy is equally effective compared to conventional open lobectomy, with less postoperative illness.

In SCLC, chemotherapy and/or radiotherapy is typically used. However the role of surgery in SCLC is being reconsidered. Surgery might improve outcomes when added to chemotherapy and radiation in early stage SCLC.

The effectiveness of lung cancer surgery (resection) for people with stage I - IIA NSCLC is not clear, however, weak evidence suggests that a combined approach of lung cancer resection and removing the mediastinal lymph nodes (mediastinal lymph node dissection) may improve survival compared to lung resection and a sample of mediastinal nodes (not a complete node dissection).

Radiotherapy is often given together with chemotherapy, and may be used with curative intent in people with NSCLC who are not eligible for surgery. This form of high-intensity radiotherapy is called radical radiotherapy. A refinement of this technique is continuous hyperfractionated accelerated radiotherapy (CHART), in which a high dose of radiotherapy is given in a short time period. Radiosurgery refers to the radiotherapy technique of giving a precise high-dose of radiotherapy that is guided by a computer. Postoperative (adjuvant) thoracic radiotherapy generally should not be used after curative-intent surgery for NSCLC. Some people with mediastinal N2 lymph node involvement might benefit from post-operative radiotherapy.

For potentially curable SCLC cases, chest radiotherapy is often recommended in addition to chemotherapy. The ideal timing of these therapies (the optimal time to give radiotherapy and chemotherapy for improving survival) is not known.

If cancer growth blocks a short section of bronchus, brachytherapy (localized radiotherapy) may be given directly inside the airway to open the passage. Compared to external beam radiotherapy, brachytherapy allows a reduction in treatment time and reduced radiation exposure to healthcare staff. Evidence for brachytherapy, however, is less than that for external beam radiotherapy.

Prophylactic cranial irradiation (PCI) is a type of radiotherapy to the brain, used to reduce the risk of metastasis. PCI is most useful in SCLC. In limited-stage disease, PCI increases three-year survival from 15% to 20%; in extensive disease, one-year survival increases from 13% to 27%. For people who have NSCLC and a single brain metastasis, it is not clear if surgery is more effective than radiosurgery.

Recent improvements in targeting and imaging have led to the development of stereotactic radiation in the treatment of early-stage lung cancer. In this form of radiotherapy, high doses are delivered over a number of sessions using stereotactic targeting techniques. Its use is primarily in patients who are not surgical candidates due to medical comorbidities.

For both NSCLC and SCLC patients, smaller doses of radiation to the chest may be used for symptom control (palliative radiotherapy). The use of higher doses of radiotherapy for palliative care are not shown to prolong survival.

The chemotherapy regimen depends on the tumor type. SCLC, even relatively early stage disease, is treated primarily with chemotherapy and radiation. In SCLC, cisplatin and etoposide are most commonly used. Combinations with carboplatin, gemcitabine, paclitaxel, vinorelbine, topotecan, and irinotecan are also used. In advanced NSCLC, chemotherapy improves survival and is used as first-line treatment, provided the person is well enough for the treatment. Typically, two drugs are used, of which one is often platinum-based (either cisplatin or carboplatin). Other commonly used drugs are gemcitabine, paclitaxel, docetaxel, pemetrexed, etoposide or vinorelbine. Platinum-based drugs and combinations that include platinum therapy do not appear to be more beneficial for prolonging survival compared to other non-platinum medications, and may lead to a higher risk of serious adverse effects such as nausea, vomiting, anaemia, and thrombocytopenia, especially in people over the age of 70 years. There is not enough evidence to determine which chemotherapy approach is associated with the highest quality of life. There is also insufficient evidence to determine if treating people with NSCLC a second time when the first round of chemotherapy was not successful (second-line chemotherapy) causes more benefit or harm.

Adjuvant chemotherapy refers to the use of chemotherapy after apparently curative surgery to improve the outcome. In NSCLC, samples are taken of nearby lymph nodes during surgery to assist staging. If stage II or III disease is confirmed, adjuvant chemotherapy (including or not including postoperative radiotherapy) improves survival by 4% at five years. The combination of vinorelbine and cisplatin is more effective than older regimens. Adjuvant chemotherapy for people with stage IB cancer is controversial, as clinical trials have not clearly demonstrated a survival benefit. Chemotherapy before surgery in NSCLC that can be removed surgically may improve outcomes.

Chemotherapy may be combined with palliative care in the treatment of the NSCLC. In advanced cases, appropriate chemotherapy improves average survival over supportive care alone, as well as improving quality of life. With adequate physical fitness maintaining chemotherapy during lung cancer palliation offers 1.5 to 3 months of prolongation of survival, symptomatic relief, and an improvement in quality of life, with better results seen with modern agents. The NSCLC Meta-Analyses Collaborative Group recommends if the recipient wants and can tolerate treatment, then chemotherapy should be considered in advanced NSCLC.

Several drugs that target molecular pathways in lung cancer are available, especially for the treatment of advanced disease. Erlotinib, gefitinib and afatinib inhibit tyrosine kinase at the epidermal growth factor receptor (EGFR). These EGFR inhibitors may help delay the spread of cancer cells for people with EGFR M+ lung cancer and may improve a person's quality of life. EGFR inhibitors have not been shown to help people survive longer. For people with EGFR mutations, treatment with gefitinib may result in an improved quality of life compared to treatment with chemotherapy. Denosumab is a monoclonal antibody directed against receptor activator of nuclear factor kappa-B ligand and may be useful in the treatment of bone metastases.

Immunotherapy may be used for both SCLC and NSCLC. Vaccine-based immunotherapy treatment after surgery or radiotherapy may not lead to improved survival for people with Stage I-III NSCLC.

Several treatments can be provided via bronchoscopy for the management of airway obstruction or bleeding. If an airway becomes obstructed by cancer growth, options include rigid bronchoscopy, balloon bronchoplasty, stenting, and microdebridement. Laser photosection involves the delivery of laser light inside the airway via a bronchoscope to remove the obstructing tumor.

Palliative care when added to usual cancer care benefits people even when they are still receiving chemotherapy. These approaches allow additional discussion of treatment options and provide opportunities to arrive at well-considered decisions. Palliative care may avoid unhelpful but expensive care not only at the end of life, but also throughout the course of the illness. For individuals who have more advanced disease, hospice care may also be appropriate.

There is weak evidence to suggest that supportive care interventions (non-invasive interventions) that focus on well-being for people with lung cancer may improve quality of life. Interventions such as nurse follow-ups, psychotherapy, psychosocial therapy, and educational programs may be beneficial, however, the evidence is not strong (further research is needed). Counselling may help people cope with emotional symptoms related to lung cancer. Reflexology may be effective in the short-term, however more research is needed. There is no evidence to suggest that nutritional interventions or exercise programs result in an improvement in the quality of life for a person with lung cancer.

Of all people with lung cancer in the US, 16.8% survive for at least five years after diagnosis. In England and Wales, between 2010 and 2011, overall five-year survival for lung cancer was estimated at 9.5%. Outcomes are generally worse in the developing world. Stage is often advanced at the time of diagnosis. At presentation, 30–40% of cases of NSCLC are stage IV, and 60% of SCLC are stage IV. Survival for lung cancer falls as the stage at diagnosis becomes more advanced: the English data suggest that around 70% of patients survive at least a year when diagnosed at the earliest stage, but this falls to just 14% for those diagnosed with the most advanced disease (stage IV).

Prognostic factors in NSCLC include presence of pulmonary symptoms, large tumor size (>3 cm), non-squamous cell type (histology), degree of spread (stage) and metastases to multiple lymph nodes, and vascular invasion. For people with inoperable disease, outcomes are worse in those with poor performance status and weight loss of more than 10%. Prognostic factors in small cell lung cancer include performance status, biological sex, stage of disease, and involvement of the central nervous system or liver at the time of diagnosis.

For NSCLC, the best prognosis is achieved with complete surgical resection of stage IA disease, with up to 70% five-year survival. People with extensive-stage SCLC have an average five-year survival rate of less than 1%. The average survival time for limited-stage disease is 20 months, with a five-year survival rate of 20%.

According to data provided by the National Cancer Institute, the median age at diagnosis of lung cancer in the US is 70 years, and the median age at death is 72 years. In the US, people with medical insurance are more likely to have a better outcome.

Worldwide, lung cancer is the most-common cancer among men in terms of both incidence and mortality, and among women has the third-highest incidence, and is second after breast cancer in mortality. In 2012, there were 1.82 million new cases worldwide, and 1.56 million deaths due to lung cancer, representing 19.4% of all deaths from cancer. The highest rates are in North America, Europe, and East Asia, with over a third of new cases in China that year. Rates in Africa and South Asia are much lower.

The population segment that is most likely to develop lung cancer is people aged over 50 who have a history of smoking. Unlike the mortality rate in men – which began declining more than 20 years ago, women's lung cancer mortality rates have risen over the last decades, and are just recently beginning to stabilize. In the US, the lifetime risk of developing lung cancer is 8% in men and 6% in women.

For every 3–4 million cigarettes smoked, one lung cancer death can occur. The influence of "Big Tobacco" plays a significant role in smoking. Young nonsmokers who see tobacco advertisements are more likely to smoke. The role of passive smoking is increasingly being recognized as a risk factor for lung cancer, resulting in policy interventions to decrease the undesired exposure of nonsmokers to others' tobacco smoke.

From the 1960s, the rates of lung adenocarcinoma started to rise in relation to other kinds of lung cancer, partially due to the introduction of filter cigarettes. The use of filters removes larger particles from tobacco smoke, thus reducing deposition in larger airways. However, the smoker has to inhale more deeply to receive the same amount of nicotine, increasing particle deposition in small airways where adenocarcinoma tends to arise. Rates of lung adenocarcinoma continues to rise.

In the US, both black men and black women have a higher incidence. Lung cancer rates are currently lower in developing countries. With increased smoking in developing countries, the rates are expected to increase in the next few years, notably in both China and India.

Also in the US, military veterans have a 25–50% higher rate of lung cancer primarily due to higher rates of smoking. During World War II and the Korean War, asbestos also played a role, and Agent Orange may have caused some problems during the Vietnam War.

Lung cancer is the third most-common cancer in the UK (around 46,400 people were diagnosed with the disease in 2014), and it is the most common cause of cancer-related death (around 35,900 people died in 2014).

Lung cancer was uncommon before the advent of cigarette smoking; it was not even recognized as a distinct disease until 1761. Different aspects of lung cancer were described further in 1810. Malignant lung tumors made up only 1% of all cancers seen at autopsy in 1878, but had risen to 10–15% by the early 1900s. Case reports in the medical literature numbered only 374 worldwide in 1912, but a review of autopsies showed the incidence of lung cancer had increased from 0.3% in 1852 to 5.66% in 1952. In Germany in 1929, physician Fritz Lickint recognized the link between smoking and lung cancer, which led to an aggressive antismoking campaign. The British Doctors' Study, published in the 1950s, was the first solid epidemiological evidence of the link between lung cancer and smoking. As a result, in 1964 the Surgeon General of the United States recommended smokers should stop smoking.

The connection with radon gas was first recognized among miners in the Ore Mountains near Schneeberg, Saxony. Silver has been mined there since 1470, and these mines are rich in uranium, with its accompanying radium and radon gas. Miners developed a disproportionate amount of lung disease, eventually recognized as lung cancer in the 1870s. Despite this discovery, mining continued into the 1950s, due to the USSR's demand for uranium. Radon was confirmed as a cause of lung cancer in the 1960s.

The first successful pneumonectomy for lung cancer was performed in 1933. Palliative radiotherapy has been used since the 1940s. Radical radiotherapy, initially used in the 1950s, was an attempt to use larger radiation doses in patients with relatively early-stage lung cancer, but who were otherwise unfit for surgery. In 1997, CHART was seen as an improvement over conventional radical radiotherapy. With SCLC, initial attempts in the 1960s at surgical resection and radical radiotherapy were unsuccessful. In the 1970s, successful chemotherapy regimens were developed.

Current research directions for lung cancer treatment include immunotherapy, which encourages the body's immune system to attack the tumor cells, epigenetics, and new combinations of chemotherapy and radiotherapy, both on their own and together. Many of these new treatments work through immune checkpoint blockade, disrupting cancer's ability to evade the immune system.

Ipilimumab blocks signaling through a receptor on T cells known as CTLA-4 which dampens down the immune system. It has been approved by the US Food and Drug Administration (FDA) for treatment of melanoma and is undergoing clinical trials for both NSCLC and SCLC.

Other immunotherapy treatments interfere with the binding of programmed cell death 1 (PD-1) protein with its ligand PD-1 ligand 1 (PD-L1), and have been approved as first- and subsequent-line treatments for various subsets of lung cancers. Signaling through PD-1 inactivates T cells. Some cancer cells appear to exploit this by expressing PD-L1 in order to switch off T cells that might recognise them as a threat. Monoclonal antibodies targeting both PD-1 and PD-L1, such as pembrolizumab, nivolumab, atezolizumab, and durvalumab are currently in clinical trials for treatment for lung cancer.

Epigenetics is the study of small, usually heritable, molecular modifications—or "tags"—that bind to DNA and modify gene expression levels. Targeting these tags with drugs can kill cancer cells. Early-stage research in NSCLC using drugs aimed at epigenetic modifications shows that blocking more than one of these tags can kill cancer cells with fewer side effects. Studies also show that giving patients these drugs before standard treatment can improve its effectiveness. Clinical trials are underway to evaluate how well these drugs kill lung cancer cells in humans. Several drugs that target epigenetic mechanisms are in development. Histone deacetylase inhibitors in development include valproic acid, vorinostat, belinostat, panobinostat, entinostat, and romidepsin. DNA methyltransferase inhibitors in development include decitabine, azacytidine, and hydralazine.

The TRACERx project is looking at how NSCLC develops and evolves, and how these tumors become resistant to treatment. The project will look at tumor samples from 850 NSCLC patients at various stages including diagnosis, after first treatment, post-treatment, and relapse. By studying samples at different points of tumor development, the researchers hope to identify the changes that drive tumor growth and resistance to treatment. The results of this project will help scientists and doctors gain a better understanding of NSCLC and potentially lead to the development of new treatments for the disease.

For lung cancer cases that develop resistance to epidermal growth factor receptor (EGFR) and anaplastic lymphoma kinase (ALK) tyrosine kinase inhibitors, new drugs are in development. EGFR inhibitors include afatinib and dacomitinib. An alternative signaling pathway, c-Met, can be inhibited by tivantinib and onartuzumab. New ALK inhibitors include crizotinib and ceritinib. If the MAPK/ERK pathway is involved, the BRAF kinase inhibitor dabrafenib and the MAPK/MEK inhibitor trametinib may be beneficial.

The PI3K pathway has been investigated as a target for lung cancer therapy. The most promising strategies for targeting this pathway seem to be selective inhibition of one or more members of the class I PI3Ks, and co-targeted inhibition of this pathway with others such as MEK.

Lung cancer stem cells are often resistant to conventional chemotherapy and radiotherapy. This may lead to relapse after treatment. New approaches target protein or glycoprotein markers that are specific to the stem cells. Such markers include CD133, CD90, ALDH1A1, CD44 and ABCG2. Signaling pathways such as Hedgehog, Wnt and Notch are often implicated in the self-renewal of stem cell lines. Thus treatments targeting these pathways may help to prevent relapse.



</doc>
<doc id="18452" url="https://en.wikipedia.org/wiki?curid=18452" title="Lists of office-holders">
Lists of office-holders

These are lists of incumbents (individuals holding offices or positions), including heads of states or of subnational entities.

A historical discipline, archontology, focuses on the study of past and current office holders.

Incumbents may also be found in the countries' articles (main article and "Politics of") and the list of national leaders, recent changes in 2007 in politics, and past leaders on State leaders by year and Colonial governors by year.

Various articles group lists by title, function or topic: e.g. abdication, assassinated persons, cabinet (government), chancellor, ex-monarchs (20th century), head of government, head of state, lieutenant governor, mayor, military commanders, minister (and ministers by portfolio below), order of precedence, peerage, president, prime minister, Reichstag participants (1792), Secretary of State.










































</doc>
<doc id="18453" url="https://en.wikipedia.org/wiki?curid=18453" title="Liberal Party of Australia">
Liberal Party of Australia

The Liberal Party of Australia is a major centre-right political party in Australia, one of the two major parties in Australian politics, along with the centre-left Australian Labor Party (ALP). It was founded in 1944 as the successor to the United Australia Party (UAP).

The Liberal Party is the larger and dominant party in the Coalition with the National Party of Australia. Except for a few short periods, the Liberal Party and its predecessors have operated in similar coalitions at federal level since the 1920s. The party's leader is Scott Morrison, the incumbent prime minister, and its deputy leader is Josh Frydenberg. The pair were elected to their positions at the August 2018 Liberal leadership ballot, with Morrison and Frydenberg replacing Malcolm Turnbull and Julie Bishop respectively. The Coalition has been in power since the 2013 federal election, forming the Abbott (2013–2015), Turnbull (2015–2018) and Morrison Governments.

The Liberal Party has a federal structure, with autonomous divisions in all six states and the Australian Capital Territory (ACT). The Country Liberal Party (CLP) of the Northern Territory is an affiliate. Both the CLP and the Liberal National Party (LNP), the Queensland state division, were formed through mergers of the local Liberal and National parties. At state and territory level, the Liberal Party is in office in three states: Peter Gutwein, Premier of Tasmania since 2020, Gladys Berejiklian, Premier of New South Wales since 2017 and Steven Marshall, Premier of South Australia since 2018. The party is in opposition in the states of Victoria, Queensland and Western Australia, and in both the ACT and Northern Territory.

The party's ideology has been referred to as conservative, liberal-conservative, conservative-liberal, and classical liberal. The Liberal Party tends to promote economic liberalism (which in the Australian usage refers to free markets and small government). Two past leaders of the party, Sir Robert Menzies and John Howard, are Australia's two longest-serving Prime Ministers.

The Liberals' immediate predecessor was the United Australia Party (UAP). More broadly, the Liberal Party's ideological ancestry stretched back to the anti-Labor groupings in the first Commonwealth parliaments. The Commonwealth Liberal Party was a fusion of the Free Trade Party and the Protectionist Party in 1909 by the second prime minister, Alfred Deakin, in response to Labor's growing electoral prominence. The Commonwealth Liberal Party merged with several Labor dissidents (including Billy Hughes) to form the Nationalist Party of Australia in 1917. That party, in turn, merged with Labor dissidents to form the UAP in 1931.

The UAP had been formed as a new conservative alliance in 1931, with Labor defector Joseph Lyons as its leader. The stance of Lyons and other Labor rebels against the more radical proposals of the Labor movement to deal the Great Depression had attracted the support of prominent Australian conservatives. With Australia still suffering the effects of the Great Depression, the newly formed party won a landslide victory at the 1931 Election, and the Lyons Government went on to win three consecutive elections. It largely avoided Keynesian pump-priming and pursued a more conservative fiscal policy of debt reduction and balanced budgets as a means of stewarding Australia out of the Depression. Lyons' death in 1939 saw Robert Menzies assume the Prime Ministership on the eve of war. Menzies served as Prime Minister from 1939 to 1941 but resigned as leader of the minority World War II government amidst an unworkable parliamentary majority. The UAP, led by Billy Hughes, disintegrated after suffering a heavy defeat in the 1943 election. In New South Wales, the party merged with the Commonwealth Party to form the Democratic Party, In Queensland the state party was absorbed into the Queensland People's Party.

From 1942 onward Menzies had maintained his public profile with his series of "The Forgotten People" radio talks—similar to Franklin D. Roosevelt's "fireside chats" of the 1930s—in which he spoke of the middle class as the "backbone of Australia" but as nevertheless having been "taken for granted" by political parties.

Menzies called a conference of conservative parties and other groups opposed to the ruling Australian Labor Party, which met in Canberra on 13 October 1944 and again in Albury, New South Wales in December 1944. Outlining his vision for a new political movement, Menzies said:

The formation of the party was formally announced at Sydney Town Hall on 31 August 1945. It took the name "Liberal" in honour of the old Commonwealth Liberal Party. The new party was dominated by the remains of the old UAP; with few exceptions, the UAP party room became the Liberal party room. The Australian Women's National League, a powerful conservative women's organisation, also merged with the new party. A conservative youth group Menzies had set up, the Young Nationalists, was also merged into the new party. It became the nucleus of the Liberal Party's youth division, the Young Liberals. By September 1945 there were more than 90,000 members, many of whom had not previously been members of any political party.

In New South Wales, the New South Wales division of the Liberal Party replaced the Liberal Democratic Party and Democratic Party between January and April 1945. In Queensland, the Queensland People's Party did not become part of the Liberal Party until July 1949, when it became the Queensland division of the Liberal Party.

After an initial loss to Labor at the 1946 election, Menzies led the Liberals to victory at the 1949 election, and the party stayed in office for a record 23 years— the longest unbroken run ever in government at the federal level. Australia experienced prolonged economic growth during the post-war boom period of the Menzies Government (1949–1966) and Menzies fulfilled his promises at the 1949 election to end rationing of butter, tea and petrol and provided a five-shilling endowment for first-born children, as well as for others. While himself an unashamed anglophile, Menzies' government concluded a number of major defence and trade treaties that set Australia on its post-war trajectory out of Britain's orbit; opened up Australia to multi-ethnic immigration; and instigated important legal reforms regarding Aboriginal Australians.

Menzies was strongly opposed to Labor's plans to nationalise the Australian banking system and, following victory at the 1949 election, secured a double dissolution election for April 1951, after the Labor-controlled Senate rejected his banking legislation. The Liberal-Country Coalition was returned with control of the Senate. The Government was re-elected again at the 1954 election; the formation of the anti-Communist Democratic Labor Party (DLP) and the consequent split in the Australian Labor Party early in 1955 helped the Liberals to secure another victory in December 1955. John McEwen replaced Arthur Fadden as leader of the Country Party in March 1958 and the Menzies-McEwen Coalition was returned again at elections in November 1958—their third victory against Labor's H. V. Evatt. The Coalition was narrowly returned against Labor's Arthur Calwell in the December 1961 election, in the midst of a credit squeeze. Menzies stood for office for the last time at the November 1963 election, again defeating Calwell, with the Coalition winning back its losses in the House of Representatives. Menzies went on to resign from parliament on 26 January 1966.

Menzies came to power the year the Communist Party of Australia had led a coal strike to improve pit miners' working conditions. That same year Joseph Stalin's Soviet Union exploded its first atomic bomb, and Mao Zedong led the Communist Party of China to power in China; a year later came the invasion of South Korea by Communist North Korea. Anti-communism was a key political issue of the 1950s and 1960s. Menzies was firmly anti-Communist; he committed troops to the Korean War and attempted to ban the Communist Party of Australia in an unsuccessful referendum during the course of that war. The Labor Party split over concerns about the influence of the Communist Party over the Trade Union movement, leading to the foundation of the breakaway Democratic Labor Party whose preferences supported the Liberal and Country parties.

In 1951, during the early stages of the Cold War, Menzies spoke of the possibility of a looming third world war. The Menzies Government entered Australia's first formal military alliance outside of the British Commonwealth with the signing of the ANZUS Treaty between Australia, New Zealand and the United States in San Francisco in 1951. External Affairs Minister Percy Spender had put forward the proposal to work along similar lines to the NATO Alliance. The Treaty declared that any attack on one of the three parties in the Pacific area would be viewed as a threat to each, and that the common danger would be met in accordance with each nation's constitutional processes. In 1954, the Menzies Government signed the South East Asia Collective Defence Treaty (SEATO) as a South East Asian counterpart to NATO. That same year, Soviet diplomat Vladimir Petrov and his wife defected from the Soviet embassy in Canberra, revealing evidence of Russian spying activities; Menzies called a Royal Commission to investigate.

In 1956, a committee headed by Sir Keith Murray was established to inquire into the financial plight of Australia's universities, and Menzies injected funds into the sector under conditions which preserved the autonomy of universities.

Menzies continued the expanded immigration program established under Chifley, and took important steps towards dismantling the White Australia Policy. In the early-1950s, external affairs minister Percy Spender helped to establish the Colombo Plan for providing economic aid to underdeveloped nations in Australia's region. Under that scheme many future Asian leaders studied in Australia. In 1958, the government replaced the Immigration Act's arbitrarily applied European language dictation test with an entry permit system, that reflected economic and skills criteria. In 1962, Menzies' "Commonwealth Electoral Act" provided that all Indigenous Australians should have the right to enrol and vote at federal elections (prior to this, indigenous people in Queensland, Western Australia and some in the Northern Territory had been excluded from voting unless they were ex-servicemen). In 1949, the Liberals appointed Dame Enid Lyons as the first woman to serve in an Australian Cabinet. Menzies remained a staunch supporter of links to the monarchy and British Commonwealth but formalised an alliance with the United States and concluded the Agreement on Commerce between Australia and Japan which was signed in July 1957 and launched post-war trade with Japan, beginning a growth of Australian exports of coal, iron ore and mineral resources that would steadily climb until Japan became Australia's largest trading partner.

Menzies retired in 1966 as Australia's longest-serving Prime Minister.

Harold Holt replaced the retiring Robert Menzies in 1966 and the Holt Government went on to win 82 seats to Labor's 41 at the 1966 election. Holt remained Prime Minister until 19 December 1967, when he was declared presumed dead two days after disappearing in rough surf in which he had gone for a swim. As of 2018, his body has still not been found.

Holt increased Australian commitment to the growing War in Vietnam, which met with some public opposition. His government oversaw conversion to decimal currency. Holt faced Britain's withdrawal from Asia by visiting and hosting many Asian leaders and by expanding ties to the United States, hosting the first visit to Australia by an American president, his friend Lyndon B. Johnson. Holt's government introduced the "Migration Act 1966", which effectively dismantled the White Australia Policy and increased access to non-European migrants, including refugees fleeing the Vietnam War. Holt also called the 1967 Referendum which removed the discriminatory clause in the Australian Constitution which excluded Aboriginal Australians from being counted in the census – the referendum was one of the few to be overwhelmingly endorsed by the Australian electorate (over 90% voted "Yes"). By the end of 1967, the Liberals' initially popular support for the war in Vietnam was causing increasing public protest.

The Liberals chose John Gorton to replace Holt. Gorton, a former World War II Royal Australian Air Force pilot, with a battle scarred face, said he was "Australian to the bootheels" and had a personal style which often affronted some conservatives.

The Gorton Government increased funding for the arts, setting up the Australian Council for the Arts, the Australian Film Development Corporation and the National Film and Television Training School. The Gorton Government passed legislation establishing equal pay for men and women and increased pensions, allowances and education scholarships, as well as providing free health care to 250,000 of the nation's poor (but not universal health care). Gorton's government kept Australia in the Vietnam War but stopped replacing troops at the end of 1970.

Gorton maintained good relations with the United States and Britain, but pursued closer ties with Asia. The Gorton government experienced a decline in voter support at the 1969 election. State Liberal leaders saw his policies as too Centralist, while other Liberals didn't like his personal behaviour. In 1971, Defence Minister Malcolm Fraser, resigned and said Gorton was "not fit to hold the great office of Prime Minister". In a vote on the leadership the Liberal Party split 50/50, and although this was insufficient to remove him as the leader, Gorton decided this was also insufficient support for him, and he resigned.

Former treasurer, William McMahon, replaced Gorton as Prime Minister. Gorton remained a front bencher but relations with Fraser remained strained. The McMahon Government ended when Gough Whitlam led the Australian Labor Party out of its 23-year period in Opposition at the 1972 election.

The economy was weakening. McMahon maintained Australia's diminishing commitment to Vietnam and criticised Opposition leader, Gough Whitlam, for visiting Communist China in 1972—only to have the US President Richard Nixon announce a planned visit soon after.

During McMahon's period in office, Neville Bonner joined the Senate and became the first Indigenous Australian in the Australian Parliament. Bonner was chosen by the Liberal Party to fill a Senate vacancy in 1971 and celebrated his maiden parliamentary speech with a boomerang throwing display on the lawns of Parliament. Bonner went on to win election at the 1972 election and served as a Liberal Senator for 12 years. He worked on Indigenous and social welfare issues and proved an independent minded Senator, often crossing the floor on Parliamentary votes.

Following Whitlam's victory, John Gorton played a further role in reform by introducing a Parliamentary motion from Opposition supporting the legalisation of same-gender sexual relations. Billy Snedden led the party against Whitlam in the 1974 federal election, which saw a return of the Labor government. When Malcolm Fraser won the Liberal Party leadership from Snedden in 1975, Gorton walked out of the Party Room.

Following the 1974–75 Loans Affair, the Malcolm Fraser led Liberal-Country Party Coalition argued that the Whitlam Government was incompetent and delayed passage of the Government's money bills in the Senate, until the government would promise a new election. Whitlam refused, yet Fraser insisted leading to the divisive 1975 Australian constitutional crisis. The deadlock came to an end when the Whitlam government was controversially dismissed by the Governor-General, Sir John Kerr on 11 November 1975 and Fraser was installed as caretaker Prime Minister, pending an election. Fraser won in a landslide at the resulting 1975 election.

Fraser maintained some of the social reforms of the Whitlam era, while seeking increased fiscal restraint. His government included the first Aboriginal federal parliamentarian, Neville Bonner, and in 1976, Parliament passed the Aboriginal Land Rights Act 1976, which, while limited to the Northern Territory, affirmed "inalienable" freehold title to some traditional lands. Fraser established the multicultural broadcaster SBS, accepted Vietnamese refugees, opposed minority white rule in apartheid South Africa and Rhodesia and opposed Soviet expansionism. A significant program of economic reform, however, was not pursued. By 1983, the Australian economy was suffering with the early 1980s recession and amidst the effects of a severe drought. Fraser had promoted "states' rights" and his government refused to use Commonwealth powers to stop the construction of the Franklin Dam in Tasmania in 1982. Liberal minister Don Chipp split off from the party to form a new social liberal party, the Australian Democrats in 1977. Fraser won further substantial majorities at the 1977 and 1980 elections, before losing to the Bob Hawke-led Australian Labor Party in the 1983 election.

A period of division for the Liberals followed, with former Treasurer John Howard competing with former Foreign Minister Andrew Peacock for supremacy. The Australian economy was facing the early 1990s recession. Unemployment reached 11.4% in 1992. Under Dr John Hewson, in November 1991, the opposition launched the 650-page Fightback! policy document—a radical collection of "dry", economic liberal measures including the introduction of a Goods and Services Tax (GST), various changes to Medicare including the abolition of bulk billing for non-concession holders, the introduction of a nine-month limit on unemployment benefits, various changes to industrial relations including the abolition of awards, a $13 billion personal income tax cut directed at middle and upper income earners, $10 billion in government spending cuts, the abolition of state payroll taxes and the privatisation of a large number of government owned enterprises − representing the start of a very different future direction to the keynesian economic policies practiced by previous Liberal/National Coalition governments. The 15 percent GST was the centerpiece of the policy document. Through 1992, Labor Prime Minister Paul Keating mounted a campaign against the Fightback package, and particularly against the GST, which he described as an attack on the working class in that it shifted the tax burden from direct taxation of the wealthy to indirect taxation as a broad-based consumption tax. Pressure group activity and public opinion was relentless, which led Hewson to exempt food from the proposed GST—leading to questions surrounding the complexity of what food was and wasn't to be exempt from the GST. Hewson's difficulty in explaining this to the electorate was exemplified in the infamous birthday cake interview, considered by some as a turning point in the election campaign. Keating won a record fifth consecutive Labor term at the 1993 election. A number of the proposals were later adopted into law in some form, to a small extent during the Keating Labor government, and to a larger extent during the Howard Liberal government (most famously the GST), while unemployment benefits and bulk billing were re-targeted for a time by the Abbott Liberal government.

At the state level, the Liberals have been dominant for long periods in all states except Queensland, where they have always held fewer seats than the National party (not to be confused with the old Nationalist Party). The Liberals were in power in Victoria from 1955 to 1982. Jeff Kennett led the party back to office in that state in 1992, and remained Premier until 1999.

In South Australia, initially a Liberal and Country Party affiliated party, the Liberal and Country League (LCL), mostly led by Premier of South Australia Tom Playford, was in power from the 1933 election to the 1965 election, though with assistance from an electoral malapportionment, or gerrymander, known as the Playmander. The LCL's Steele Hall governed for one term from the 1968 election to the 1970 election and during this time began the process of dismantling the Playmander. David Tonkin, as leader of the South Australian Division of the Liberal Party of Australia, became Premier at the 1979 election for one term, losing office at the 1982 election. The Liberals returned to power at the 1993 election, led by Premiers Dean Brown, John Olsen and Rob Kerin through two terms, until their defeat at the 2002 election. They remained in opposition for 16 years, under a record five Opposition Leaders, until Steven Marshall led the party to victory in 2018.

The dual aligned Country Liberal Party ruled the Northern Territory from 1978 to 2001.

The party has held office in Western Australia intermittently since 1947. Liberal Richard Court was Premier of the state for most of the 1990s.

In New South Wales, the Liberal Party has not been in office as much as its Labor rival, and just three leaders have led the party from opposition to government in that state: Sir Robert Askin, who was premier from 1965 to 1975, Nick Greiner, who came to office in 1988 and resigned in 1992, and Barry O'Farrell who would lead the party out of 16 years in opposition in 2011.

The Liberal Party does not officially contest most local government elections, although many members do run for office in local government as independents. An exception is the Brisbane City Council, where both Sallyanne Atkinson and Campbell Newman have been elected Lord Mayor of Brisbane.

Labor's Paul Keating lost the 1996 Election to the Liberals' John Howard. The Liberals had been in Opposition for 13 years. With John Howard as Prime Minister, Peter Costello as Treasurer and Alexander Downer as Foreign Minister, the Howard Government remained in power until their electoral defeat to Kevin Rudd in 2007.

Howard generally framed the Liberals as being conservative on social policy, debt reduction and matters like maintaining Commonwealth links and the American Alliance but his premiership saw booming trade with Asia and expanding multiethnic immigration. His government concluded the Australia-United States Free Trade Agreement with the Bush Administration in 2004.

Howard differed from his Labor predecessor Paul Keating in that he supported traditional Australian institutions like the Monarchy in Australia, the commemoration of ANZAC Day and the design of the Australian flag, but like Keating he pursued privatisation of public utilities and the introduction of a broad based consumption tax (although Keating had dropped support for a GST by the time of his 1993 election victory). Howard's premiership coincided with Al Qaeda's 11 September attacks on the United States. The Howard Government invoked the ANZUS treaty in response to the attacks and supported America's campaigns in Afghanistan and Iraq.

In the 2004 Federal elections the party strengthened its majority in the Lower House and, with its coalition partners, became the first federal government in twenty years to gain an absolute majority in the Senate. This control of both houses permitted their passing of legislation without the need to negotiate with independents or minor parties, exemplified by industrial relations legislation known as WorkChoices, a wide-ranging effort to increase deregulation of industrial laws in Australia.

In 2005, Howard reflected on his government's cultural and foreign policy outlook in oft repeated terms:

The 2007 federal election saw the defeat of the Howard federal government, and the Liberal Party was in opposition throughout Australia at the state and federal level; the highest Liberal office-holder at the time was Lord Mayor of Brisbane Campbell Newman. This ended after the 2008 Western Australian state election, when Colin Barnett became Premier of that state.

Following the 2007 federal election, Dr Brendan Nelson was elected leader by the Parliamentary Liberal Party. On 16 September 2008, in a second contest following a spill motion, Nelson lost the leadership to Malcolm Turnbull. On 1 December 2009, a subsequent leadership election saw Turnbull lose the leadership to Tony Abbott by 42 votes to 41 on the second ballot. Abbott led the party to the 2010 federal election, which saw an increase in the Liberal Party vote and resulted in the first hung parliament since the 1940 election.

Through 2010, the party remained in opposition at the Tasmanian and South Australian state elections and achieved state government in Victoria. In March 2011, the New South Wales Liberal-National Coalition led by Barry O'Farrell won government with the largest election victory in post-war Australian history at the State Election. In Queensland, the Liberal and National parties merged in 2008 to form the new Liberal National Party of Queensland (registered as the Queensland Division of the Liberal Party of Australia). In March 2012, the new party achieved Government in an historic landslide, led by former Brisbane Lord Mayor, Campbell Newman.

In March 2013, the Western Australian Liberal-National government won re-election while the party won government in Tasmania in 2014 and lost their fourth election in a row at the South Australian election. However, the Victorian Liberal-National government, now led by Denis Napthine, became the first one term government in Victoria in 60 years. Similarly, just two months later, the Liberal National government in Queensland was defeated just three years after its historic landslide victory. The New South Wales Liberal-National Coalition, however, managed to win re-election in March 2015. In 2016 the Federal Liberals narrowly won re-election in July 2016 while the Liberal-affiliated Country Liberals suffered a historic defeat in the Northern Territory and Canberra Liberals lost their fifth election in a row in October 2016. The Liberals fared little better in 2017 with the Barnett-led Liberal-National government in Western Australia also suffered a landslide defeat in March.

Turbull's time in office saw tensions between Moderate and Conservative factions within the Liberal Party.

On 21 August 2018 after a week of mounting pressure on Mr Turnbull's leadership over his handling of energy policy and election strategy, the prime minister used the regular party-room meeting to spill the party leadership in an attempt to head off a growing conservative-led move against him by Conservative Home Affairs Minister Peter Dutton. MrTurnbull survived the challenge, winning 48 votes to Mr Dutton's 35. 
A further spill was called by Mr Turnbull, in which he declined to stand and the leadership of the party was decided in favour of Treasurer Scott Morrison, over Mr Dutton.

Turnbull resigned from parliament on 31 August 2018, triggering a by-election in the seat of Wentworth. The Liberals lost the by-election to an Independent, the Coalition also losing its majority in the House of Representatives.

Further dissatisfaction within the Liberal Party has seen a number of centrist and economically-liberal candidates announce that they will be nominating as independents in wealthy electorates for the 2019 federal election, with a specific focus on "addressing climate change" since the Liberal Party has recently promoted the construction of more coal mines and are not following climate scientists advice.

The contemporary Liberal Party generally advocates economic liberalism. Historically, the party has supported a higher degree of economic protectionism and interventionism than it has in recent decades. However, from its foundation the party has identified itself as an anti-socialist grouping of liberals and conservatives. Strong opposition to socialism and communism in Australia and abroad was one of its founding principles. The party's founder and longest-serving leader Robert Menzies envisaged that Australia's middle class would form its main constituency.

Towards the end of his term as Prime Minister of Australia and in a final address to the Liberal Party Federal Council in 1964, Menzies spoke of the "Liberal Creed" as follows:

Soon after the election of the Howard Government the new Prime Minister John Howard, who was to become the second-longest serving Liberal Prime Minister, spoke of his interpretation of the "Liberal Tradition" in a Robert Menzies Lecture in 1996:

Throughout their history, the Liberals have been in electoral terms largely the party of the middle class (whom Menzies, in the era of the party's formation called "The forgotten people"), though such class-based voting patterns are no longer as clear as they once were. In the 1970s a left-wing middle class emerged that no longer voted Liberal. One effect of this was the success of a breakaway party, the Australian Democrats, founded in 1977 by former Liberal minister Don Chipp and members of minor liberal parties. On the other hand, the Liberals have done increasingly well in recent years among socially conservative working-class voters. However, the Liberal Party's key support base remains the upper-middle classes—16 of the 20 richest federal electorates are held by the Liberals, most of which are safe seats. In country areas they either compete with or have a truce with the Nationals, depending on various factors.

Menzies was an ardent constitutional monarchist, who supported the monarchy in Australia and links to the Commonwealth of Nations. Today the party is divided on the question of republicanism, with some (such as incumbent leader Scott Morrison) being monarchists, while others (such as his predecessor Malcolm Turnbull) are republicans. The Menzies Government formalised Australia's alliance with the United States in 1951 and the party has remained a strong supporter of the mutual defence treaty.

Domestically, Menzies presided over a fairly regulated economy in which utilities were publicly owned, and commercial activity was highly regulated through centralised wage-fixing and high tariff protection. Liberal leaders from Menzies to Malcolm Fraser generally maintained Australia's high tariff levels. At that time the Liberals' coalition partner, the Country Party, the older of the two in the coalition (now known as the "National Party"), had considerable influence over the government's economic policies. It was not until the late 1970s and through their period out of power federally in the 1980s that the party came to be influenced by what was known as the "New Right"—a conservative liberal group who advocated market deregulation, privatisation of public utilities, reductions in the size of government programs and tax cuts.

Socially, while liberty and freedom of enterprise form the basis of its beliefs, elements of the party have wavered between what is termed "small-l liberalism" and social conservatism. Historically, Liberal Governments have been responsible for the carriage of a number of notable "socially liberal" reforms, including the opening of Australia to multiethnic immigration under Menzies and Harold Holt; Holt's 1967 Referendum on Aboriginal Rights; John Gorton's support for cinema and the arts; selection of the first Aboriginal Senator, Neville Bonner, in 1971; and Malcolm Fraser's Aboriginal Land Rights Act 1976. A West Australian Liberal, Ken Wyatt, became the first Indigenous Australian elected to the House of Representatives in 2010.

The Prime Minister of Australia, Scott Morrison, stated the following in his 2019 victory speech;

This is, this is the best country in the world in which to live. It is those Australians that we have been working for, for the last five and a half years since we came to Government, under Tony Abbott's leadership back in 2013. It has been those Australians who have worked hard every day, they have their dreams, they have their aspirations; to get a job, to get an apprenticeship, to start a business, to meet someone amazing. To start a family, to buy a home, to work hard and provide the best you can for your kids. To save your retirement and to ensure that when you're in your retirement, that you can enjoy it because you've worked hard for it. These are The Quiet Australians who have won a great victory tonight.

The Liberal Party is a member of the International Democrat Union.

The Liberal Party's organisation is dominated by the six state divisions, reflecting the party's original commitment to a federalised system of government (a commitment which was strongly maintained by all Liberal governments bar the Gorton government until 1983, but was to a large extent abandoned by the Howard Government, which showed strong centralising tendencies). Menzies deliberately created a weak national party machine and strong state divisions. Party policy is made almost entirely by the parliamentary parties, not by the party's rank-and-file members, although Liberal party members do have a degree of influence over party policy.

The Liberal Party's basic organisational unit is the "branch", which consists of party members in a particular locality. For each electorate there is a "conference"—notionally above the branches—which coordinates campaigning in the electorate and regularly communicates with the member (or candidate) for the electorate. As there are three levels of government in Australia, each branch elects delegates to a local, state, and federal conference.

All the branches in an Australian state are grouped into a "Division". The ruling body for the Division is a "State Council". There is also one "Federal Council" which represents the entire organisational Liberal Party in Australia. Branch executives are delegates to the Councils "ex-officio" and additional delegates are elected by branches, depending on their size.

Preselection of electoral candidates is performed by a special electoral college convened for the purpose. Membership of the electoral college consists of head office delegates, branch officers, and elected delegates from branches.


For the 2015–2016 financial year, the top ten disclosed donors to the Liberal Party were: Paul Marks (Nimrod resources) ($1,300,000), Pratt Holdings ($790,000), Hong Kong Kingson Investment Company ($710,000), Aus Gold Mining Group ($410,000), Village Roadshow ($325,000), Waratah Group ($300,000), Walker Corporation ($225,000), Australian Gypsum Industries ($196,000), National Automotive Leasing and Salary Packaging Association ($177,000) and Westfield Corporation ($150,000).

The Liberal Party also receives undisclosed funding through several methods, such as "associated entities". Cormack Foundation, Eight by Five, Free Enterprise Foundation, Federal Forum and Northern Sydney Conservative forum are entities which have been used to funnel donations to the Liberal Party without disclosing the source.




</doc>
<doc id="18454" url="https://en.wikipedia.org/wiki?curid=18454" title="Lindisfarne">
Lindisfarne

The Holy Island of Lindisfarne, commonly known as either Holy Island or Lindisfarne, is a tidal island off the northeast coast of England, which constitutes the civil parish of Holy Island in Northumberland. Holy Island has a recorded history from the 6th century AD; it was an important centre of Celtic Christianity under Saints Aidan of Lindisfarne, Cuthbert, Eadfrith of Lindisfarne and Eadberht of Lindisfarne. After the Viking invasions and the Norman conquest of England, a priory was reestablished. A small castle was built on the island in 1550.

Both the "Parker Chronicle" and "Peterborough Chronicle" annals of AD 793 record the Old English name, "Lindisfarena". 

In the 9th century "Historia Brittonum" the island appears under its Old Welsh name "Medcaut". Andrew Breeze (following up on a suggestion by Richard Coates) proposes that the name ultimately derives from Latin "Medicata [Insula]" (), owing perhaps to the island's reputation for medicinal herbs).

The soubriquet "Holy Island" was in use by the 11th century when it appears in Latin as "Insula Sacra". The reference was to Saints Aidan and Cuthbert.

In the present day 'Holy Island' is the name of the civil parish and native inhabitants are known as 'Islanders'. The Ordnance Survey uses 'Holy Island' for both the island and the village, with 'Lindisfarne' listed either as an alternative name for the island or as a name of 'non-Roman antiquity'. "Locally the island is rarely referred to by its Anglo-Saxon name of 'Lindisfarne'" (according to the local community website www.lindisfarne.org.uk). More widely, the two names are used somewhat interchangeably. 'Lindisfarne' is invariably used when referring to the pre-conquest monastic settlement, the Priory ruins and the Castle. The combined phrase 'The Holy Island of Lindisfarne' has begun to be used more frequently in recent times, particularly when promoting the island as a tourist or pilgrim destination.

The name Lindisfarne has an uncertain origin. The "-farne" part may be Old English "–fearena" meaning "traveller". The first part, "Lindis-", may refer to people from the Kingdom of Lindsey in modern Lincolnshire, referring to either regular visitors or settlers. Another possibility is that "Lindisfarne" is Brittonic in origin, containing the element "Lind-" meaning "stream or pool" (Welsh "llyn"), with the nominal morpheme "-as(t)" and an unknown element identical to that in the Farne Islands. Further suggested is that the name may be a wholly Irish formation, from corresponding "*lind-is-", plus "–fearann" meaning "land, domain, territory". Such an Irish formation, however, could have been based on a pre-existing Brittonic name.

There is also a supposition that the nearby Farne Islands are fern-like in shape and the name may have come from there.

The island measures from east to west and from north to south, and comprises approximately at high tide. The nearest point of the island is about from the mainland of England. The island of Lindisfarne is located along the northeast coast of England, close to the border with Scotland. It is accessible, most times, at low tide by crossing sand and mudflats which are covered with water at high tides. These sand and mud flats carry an ancient pilgrims' path, and in more recent times, a modern causeway. Lindisfarne is surrounded by the Lindisfarne National Nature Reserve, which protects the island's sand dunes and the adjacent intertidal habitats. , the island had a population of 180.

A February 2020 report provided an update on the island. At the time, three pubs and a hotel were operating; the store had closed but the post office remained in operation. No professional or medical services were available and residents were driving to Berwick-upon-Tweed for groceries and other supplies. Points of interest for visitors included Lindisfarne Castle operated by the National Trust, the priory, the historic church, the nature reserve and the beaches. At certain times of year, numerous migratory birds can be seen.

Warning signs urge visitors walking to the island to keep to the marked path, to check tide times and weather carefully, and to seek local advice if in doubt. For drivers, tide tables are prominently displayed at both ends of the causeway and also where the Holy Island road leaves the A1 Great North Road at Beal. The causeway is generally open from about three hours after high tide until two hours before the next high tide, but the period of closure may be extended during stormy weather. Tide tables giving the safe crossing periods are published by Northumberland County Council.

Despite these warnings, about one vehicle each month is stranded on the causeway, requiring rescue by HM Coastguard and/or Seahouses RNLI lifeboat. A sea rescue costs approximately £1,900 (quoted in 2009, ), while an air rescue costs more than £4,000 (also quoted in 2009, ). Local people have opposed a causeway barrier primarily on convenience grounds.

Trinity House operates two light beacons (which it lists as lighthouses) to guide vessels entering Holy Island Harbour. Until 1 November 1995 both were operated by Newcastle-upon-Tyne Trinity House (a separate corporation, which formerly had responsibility for navigation marks along the coast from Berwick-upon-Tweed to Whitby). On that day, responsibility for marking the approach to the harbour was assumed by the London-based Corporation. 

Heugh Hill Light is a metal framework tower with a black triangular day mark, situated on Heugh Hill (a ridge on the south edge of Lindisfarne). Prior to its installation, a wooden beacon with a triangle topmark had stood on the centre of Heugh Hill for many decades. Nearby is a former coastguard station, recently refurbished and opened to the public as a viewing platform. An adjacent ruin is known as the Lantern Chapel; its origin is unknown, but the name may indicate an earlier navigation light on this site.

Guile Point East and Guile Point West are a pair of stone obelisks standing on a small tidal island on the other side of the channel. The obelisks are leading marks which, when aligned, indicate the safe channel over the bar. When Heugh Hill bears 310° (in line with the church belfry) the bar is cleared and there is a clear run into the harbour. The beacons were established in 1826 by Newcastle-upon-Tyne Trinity House (in whose ownership they remain). Since the early 1990s, a sector light has been fixed about one-third of the way up Guile Point East. 

Not a lighthouse but simply a daymark for maritime navigation, a white brick pyramid, 35 feet high and built in 1810, stands at Emmanuel Head, the north-eastern point of Lindisfarne. It is said to be Britain's earliest purpose-built daymark.

The northeast coast of England was largely unsettled by Roman civilians apart from the Tyne valley and Hadrian's Wall. The area had been little affected during the centuries of nominal Roman occupation. The countryside had been subject to raids from both Scots and Picts and was "not one to attract early Germanic settlement". King Ida (reigned from 547) started the sea-borne settlement of the coast, establishing an "urbs regia" at Bamburgh across the bay from Lindisfarne. The conquest was not straightforward, however. The "Historia Brittonum" recounts how, in the 6th century, Urien, prince of Rheged, with a coalition of North British kingdoms, besieged Angles led by Theodric of Bernicia at the island for three days and nights, until internal power struggles led to the Britons' defeat.

The monastery of Lindisfarne was founded circa 634 by Irish monk Saint Aidan, who had been sent from Iona off the west coast of Scotland to Northumbria at the request of King Oswald. The priory was founded before the end of 634 and Aidan remained there until his death in 651. The priory remained the only seat of a bishopric in Northumbria for nearly thirty years. Finian (bishop 651–661) built a timber church "suitable for a bishop's seat". St Bede, however, was critical of the fact that the church was not built of stone but only of hewn oak thatched with reeds. A later bishop, Eadbert, removed the thatch and covered both walls and roof in lead. An abbot, who could be the bishop, was elected by the brethren and led the community. Bede comments on this:
Lindisfarne became the base for Christian evangelism in the North of England and also sent a successful mission to Mercia. Monks from the Irish community of Iona settled on the island. Northumbria's patron saint, Saint Cuthbert, was a monk and later abbot of the monastery, and his miracles and life are recorded by the Venerable Bede. Cuthbert later became Bishop of Lindisfarne. An anonymous life of Cuthbert written at Lindisfarne is the oldest extant piece of English historical writing. From its reference to "Aldfrith, who now reigns peacefully" it must date to between 685 and 704. Cuthbert was buried here, his remains later translated to Durham Cathedral (along with the relics of Saint Eadfrith of Lindisfarne). Eadberht of Lindisfarne, the next bishop (and saint) was buried in the place from which Cuthbert's body was exhumed earlier the same year when the priory was abandoned in the late 9th century.

Cuthbert's body was carried with the monks, eventually settling in Chester-le-Street before a final move to Durham. The saint's shrine was the major pilgrimage centre for much of the region until its despoliation by Henry VIII's commissioners in 1539 or 1540. The grave was preserved, however, and when opened in 1827 yielded a number of remarkable artefacts dating back to Lindisfarne. The inner (of three) coffins was of incised wood, the only decorated wood to survive from the period. It shows Jesus surrounded by the Four Evangelists. Within the coffin was a pectoral cross across made of gold and mounted with garnets and intricate tracery. There was a comb made of elephant ivory, a rare and expensive item in Northern England. Also inside was an embossed silver covered travelling altar. All were contemporary with the original burial on the island. When the body was placed in the shrine in 1104 other items were removed: a paten, scissors and a chalice of gold and onyx. Most remarkable of all was a gospel (known as the St Cuthbert Gospel or Stonyhurst Gospel from its association with the college). The manuscript is in an early, probably original, binding beautifully decorated with deeply embossed leather.

Following Finian's death, Colman became Bishop of Lindisfarne. Up to this point the Northumbrian (and latterly Mercian) churches had looked to Lindisfarne as the mother church. There were significant liturgical and theological differences with the fledgling Roman party based at Canterbury. According to Stenton: "There is no trace of any intercourse between these bishops [the Mercians] and the see of Canterbury". The Synod of Whitby in 663 changed this. Allegiance switched southwards to Canterbury and thence to Rome. Colman departed his see for Iona and Lindisfarne ceased to be of such major importance.

In 735 the northern ecclesiastical province of England was established with the archbishopric at York. There were only three bishops under York: Hexham, Lindisfarne and Whithorn whereas Canterbury had the twelve envisaged by St Augustine. The Diocese of York encompassed roughly the counties of Yorkshire and Lancashire. Hexham covered County Durham and the southern part of Northumberland up to the River Coquet and eastwards into the Pennines. Whithorn covered most of Dumfries and Galloway region west of Dumfries itself. The remainder, Cumbria, northern Northumbria, Lothian and much of the Kingdom of Strathclyde formed the diocese of Lindisfarne.

In 737 Saint Ceolwulf of Northumbria abdicated as King of Northumbria and entered the Prior at Lindisfarne. He died in 764 and was buried alongside Cuthbert. In 830 his body was moved to Norham-upon-Tweed and later his head was translated to Durham Cathedral.

At some point in the early 8th century, the famous illuminated manuscript known as the Lindisfarne Gospels, an illustrated Latin copy of the Gospels of Matthew, Mark, Luke and John, was made probably at Lindisfarne and the artist was possibly Eadfrith, who later became Bishop of Lindisfarne. It is also speculated that a team of illuminators and calligraphers (monks of Lindisfarne Priory) worked on the text however, their identities are unknown. Sometime in the second half of the 10th century a monk named Aldred added an Anglo-Saxon (Old English) gloss to the Latin text, producing the earliest surviving Old English copies of the Gospels. Aldred attributed the original to Eadfrith (bishop 698–721). The Gospels were written with a good hand, but it is the illustrations done in an insular style containing a fusion of Celtic, Germanic and Roman elements that are truly outstanding. According to Aldred, Eadfrith's successor Æthelwald was responsible for pressing and binding it and then it was covered with a fine metal case made by a hermit called Billfrith. The Lindisfarne Gospels now reside in the British Library in London, somewhat to the annoyance of some Northumbrians. In 1971 professor Suzanne Kaufman of Rockford, Illinois presented a facsimile copy of the Gospels to the clergy of the island.

In 793, a Viking raid on Lindisfarne caused much consternation throughout the Christian west and is now often taken as the beginning of the Viking Age. There had been some other Viking raids, but according to English Heritage this one was particularly significant, because "it attacked the sacred heart of the Northumbrian kingdom, desecrating ‘the very place where the Christian religion began in our nation’".

The D and E versions of the "Anglo-Saxon Chronicle" record:

Her wæron reðe forebecna cumene ofer Norðhymbra land, ⁊ þæt folc earmlic bregdon, þæt wæron ormete þodenas ⁊ ligrescas, ⁊ fyrenne dracan wæron gesewene on þam lifte fleogende. Þam tacnum sona fyligde mycel hunger, ⁊ litel æfter þam, þæs ilcan geares on .vi. Idus Ianuarii, earmlice hæþenra manna hergunc adilegode Godes cyrican in Lindisfarnaee þurh hreaflac ⁊ mansliht.

In this year fierce, foreboding omens came over the land of the Northumbrians, and the wretched people shook; there were excessive whirlwinds, lightning, and fiery dragons were seen flying in the sky. These signs were followed by great famine, and a little after those, that same year on 6th ides of January, the ravaging of wretched heathen men destroyed God's church at Lindisfarne.

The generally accepted date for the Viking raid on Lindisfarne is in fact 8 June; Michael Swanton writes: ""vi id Ianr", presumably [is] an error for "vi id Iun" (8 June) which is the date given by the "Annals of Lindisfarne" (p. 505), when better sailing weather would favour coastal raids."

Alcuin, a Northumbrian scholar in Charlemagne's court at the time, wrote:

Never before has such terror appeared in Britain as we have now suffered from a pagan race ... The heathens poured out the blood of saints around the altar, and trampled on the bodies of saints in the temple of God, like dung in the streets.

The English seemed to have turned their back on the sea as they became more settled. Many monasteries were established on islands, peninsulas, river mouths and cliffs. Isolated communities were less susceptible to interference and the politics of the heartland. The amazement of the English at the raids from the sea must have been matched by the amazement of the raiders at such (to them) vulnerable, wealthy and unarmed settlements.

These preliminary raids, unsettling as they were, were not followed up. The main body of the raiders passed north around Scotland. The 9th-century invasions came not from Norway, but from the Danes from around the entrance to the Baltic. The first Danish raids into England were in the Isle of Sheppey, Kent during 835 and from there their influence spread north. During this period religious art continued to flourish on Lindisfarne, and the "Liber Vitae" of Durham began in the priory.

By 866 the Danes were in York and in 873 the army was moving into Northumberland. With the collapse of the Northumbrian kingdom the monks of Lindisfarne fled the island in 875 taking with them St Cuthbert's bones (which are now buried at the cathedral in Durham).

Prior to the 9th century Lindisfarne Priory had, in common with other such establishments, held large tracts of land which were managed directly or leased to farmers with a life interest only. Following the Danish occupation land was increasingly owned by individuals and could be bought, sold and inherited. Following the Battle of Corbridge in 914 Ragnald seized the land giving some to his followers Scula and Onlafbal.

William of St Calais, the first Norman Bishop of Durham, endowed his new Benedictine monastery at Durham with land and property in Northumberland, including Holy Island and much of the surrounding mainland. Durham Priory re-established a monastic house on the island in 1093, as a cell of Durham, administered from Norham. The standing remains date from this time (whereas the site of the original priory is now occupied by the parish church).

Monastic records from the 14th to the 16th century provide evidence of an already well-established fishing economy on the island. Both line fishing and net fishing were practised, inshore in shallow waters and in the deep water offshore, using a variety of vessels: contemporary accounts differentiate between small 'cobles' and larger 'boats', as well as singling out certain specialised vessels (such as a 'herynger', sold for £2 in 1404). As well as supplying food for the monastic community, the island's fisheries (together with those of nearby Farne) provided the mother house at Durham with fish, on a regular (sometimes weekly) basis. Fish caught included cod, haddock, herring, salmon, porpoise and mullet, among others. Shellfish of various types were also fished for, with lobster nets and oyster dredges being mentioned in the accounts. Fish surplus to the needs of the monastery was traded, but subject to a tithe. 

There is also evidence that the monks operated a lime kiln on the island.

In 1462, during the Wars of the Roses, Margaret of Anjou made an abortive attempt to seize the Northumbrian castles. Following a storm at sea 400 troops had to seek shelter on Holy Island, where they surrendered to the Yorkists.

The Benedictine monastery continued until its suppression in 1536 under Henry VIII, after which the buildings surrounding the church were used as a naval storehouse. In 1613 ownership of the island (and other land in the area formerly pertaining to Durham Priory) was transferred to the Crown.

An early scholarly description of the priory was compiled by Dr Henry George Charles Clarke (presumed son of Admiral Sir Erasmus Gower) in 1838 during his term as president of the Berwickshire Naturalists' Club. Dr Clarke surmised that this Norman priory was unique in that the centre aisle had a vault of stone. Of the six arches, Dr Clarke stated "as if the architect had not previously calculated the space to be occupied by his arcade. The effect here has been to produce a horse-shoe instead of a semicircular arch, from its being of the same height, but lesser span, than the others. This arch is very rare, even in Norman buildings". The Lindisfarne Priory (ruin) is a grade I listed building, List Entry Number 1042304. Other parts of the priory are a Scheduled ancient monument, List Entry Number 1011650. The latter are described as "the site of the pre-Conquest monastery of Lindisfarne and the Benedictine cell of Durham Cathedral that succeeded it in the 11th century".

Recent work by archeologists was continuing in 2019, for the fourth year. Artifacts recovered included a rare board game piece, copper-alloy rings and Anglo-Saxon coins from both Northumbria and Wessex. The discovery of a cemetery led to finding commemorative markers "unique to the 8th and 9th centuries". The group also found evidence of an early medieval building, "which seems to have been constructed on top of an even earlier industrial oven" which was used to make copper or glass.

Lindisfarne Castle was built in 1550, around the time that Lindisfarne Priory went out of use, and stones from the priory were used as building material. It is very small by the usual standards, and was more of a fort. The castle sits on the highest point of the island, a whinstone hill called Beblowe.

After Henry VIII suppressed the priory, his troops used the remains as a naval store. In 1542 Henry VIII ordered the Earl of Rutland to fortify the site against possible Scottish invasion. By December 1547, Ralph Cleisbye, Captain of the fort, had guns including a wheel-mounted demi-culverin, two brass sakers, a falcon, and another fixed demi-culverin. However, Beblowe Crag itself was not fortified until 1549 and Sir Richard Lee saw only a decayed platform and turf rampart there in 1565. Elizabeth I then had work carried out on the fort, strengthening it and providing gun platforms for the new developments in artillery technology. When James VI and I came to power in England, he combined the Scottish and English thrones, and the need for the castle declined. At that time the castle was still garrisoned from Berwick and protected the small Lindisfarne Harbour.

During the Jacobite Rising of 1715, Lancelot Errington, one of a number of locals who supported the Jacobite cause, visited the castle. Some sources say that he asked the Master Gunner, who also served as the unit's barber, for a shave. Once Errington was inside, it became clear that most of the garrison were away; later that day he returned with his nephew Mark Errington, claiming that he had lost the key to his watch. They were allowed in, overpowered the three soldiers present, and claimed the castle as a landing site for the Jacobite group led by Thomas Forster, Member of Parliament for the county of Northumberland. Reinforcements did not arrive to support the Erringtons, so when a detachment of 100 men arrived from Berwick to retake the castle they were only able to hold out for one day. Fleeing, they were captured at the tollbooth at Berwick and imprisoned, but were later able to tunnel out of their gaol and escape.

A Dundee firm built lime kilns on Lindisfarne in the 1860s, and lime was burnt on the island until at least the end of the 19th century. The kilns are among the most complex in Northumberland. Horses carried limestone, along the Holy Island Waggonway, from a quarry on the north side of the island to the lime kilns, where it was burned with coal transported from Dundee on the east coast of Scotland. There are still some traces of the jetties by which the coal was imported and the lime exported close by at the foot of the crags. The remains of the waggonway between the quarries and the kilns makes for a pleasant and easy walk. At its peak over 100 men were employed. Crinoid columnals extracted from the quarried stone and threaded into necklaces or rosaries became known as St Cuthbert's beads. The large-scale quarrying in the 19th century had a devastating effect on the interesting limestone caves, but eight sea caves remain at Coves Haven.

Workings on the lime kilns stopped by the start of the 20th century. The lime kilns on Lindisfarne are among the few being actively preserved in Northumberland.

Holy Island Golf Club was founded in 1907 but later closed in the 1960s.

The island is within an "Area of Outstanding Natural Beauty" on the Northumberland Coast. The ruined monastery is in the care of English Heritage, which also runs a museum/visitor centre nearby. The neighbouring parish church (see below) is still in use.

Lindisfarne also has the small Lindisfarne Castle, based on a Tudor fort, which was refurbished in the Arts and Crafts style by Sir Edwin Lutyens for the editor of Country Life, Edward Hudson. Lutyens also designed the island's Celtic-cross war-memorial on the Heugh. Lutyens' upturned herring busses near the foreshore provided the inspiration for Spanish architect Enric Miralles' Scottish Parliament Building in Edinburgh.

One of the most celebrated gardeners of modern times, Gertrude Jekyll (1843–1932), laid out a tiny garden just north of the castle in 1911. The castle, garden and nearby lime kilns are in the care of the National Trust and open to visitors.

Turner, Thomas Girtin and Charles Rennie Mackintosh all painted on Holy Island.

Holy Island was considered part of the Islandshire unit along with several mainland parishes. This came under the jurisdiction of the County Palatine of Durham until the Counties (Detached Parts) Act 1844.

Lindisfarne was mainly a fishing community for many years, with farming and the production of lime also of some importance.

The Holy Island of Lindisfarne is well known for mead. In the mediaeval days when monks inhabited the island, it was thought that if the soul was in God's keeping, the body must be fortified with Lindisfarne mead. The monks have long vanished, and the mead's recipe remains a secret of the family which still produces it; Lindisfarne Mead is produced at St Aidan's Winery, and sold throughout the UK and elsewhere.

The isle of Lindisfarne was featured on the television programme "Seven Natural Wonders" as one of the wonders of the North. The Lindisfarne Gospels have also featured on television among the top few Treasures of Britain. It also features in an ITV Tyne Tees programme "Diary of an Island" which started on 19 April 2007 and on a DVD of the same name.

When the abbey was rebuilt by the Normans, the site was moved. The site of the original priory church was redeveloped in stone as the parish church. As such it is now the oldest building on the island still with a roof on. Remains of the Saxon church exist as the chancel wall and arch. A Norman apse (subsequently replaced in the 13th century) led eastwards from the chancel. The nave was extended in the 12th century with a northern arcade, and in the following century with a southern arcade.

After the Reformation the church slipped into disrepair until the restoration of 1860. The church is built of coloured sandstone which has had the Victorian plaster removed from it. The north aisle is known as the "fishermen's aisle" and houses the altar of St. Peter. The south aisle used to hold the altar of St. Margaret of Scotland, but now houses the organ.

The church is a Grade I listed building number 1042304, listed as part of the whole priory. The church forms most of the earliest part of the site and is a scheduled ancient monument number 1011650.

For several years in the late 20th century ( 1980~1990), religious author and cleric David Adam ministered to thousands of pilgrims and other visitors as rector of Holy Island.

In response to the perceived lack of affordable housing on the isle of Lindisfarne, in 1996 a group of islanders established a charitable foundation known as the Holy Island of Lindisfarne Community Development Trust. They built a visitor centre on the island using the profits from sales. In addition, eleven community houses were built and are rented out to community members who want to continue to live on the island. The trust is also responsible for management of the inner harbour. The Holy Island Partnership was formed in 2009 by members of the community as well as organisations and groups operating on the island.

Tourism grew steadily throughout the 20th century, and the isle of Lindisfarne is now a popular destination for visitors. Those tourists staying on the island while it is cut off by the tide experience the island in a much quieter state, as most day trippers leave before the tide rises. At low tide it is possible to walk across the sands following an ancient route known as the Pilgrims' Way (see the note about safety, above). This route is marked with posts and has refuge boxes for stranded walkers, just as the road has a refuge box for those who have left their crossing too late. The isle of Lindisfarne is surrounded by the Lindisfarne National Nature Reserve which attracts bird-watchers to the tidal island. The island's prominent position and varied habitat make it particularly attractive to tired avian migrants, and 330 bird species had been recorded on the island.



</doc>
<doc id="18456" url="https://en.wikipedia.org/wiki?curid=18456" title="Literacy">
Literacy

Literacy is popularly understood as an ability to read, write and use numeracy in at least one method of writing, an understanding reflected by mainstream dictionary and handbook definitions. Starting in the 1980s, however, literacy researchers have maintained that defining literacy as an ability apart from any actual event of reading and writing ignores the complex ways reading and writing always happen in a specific context and in tandem with the values associated with that context. The view that literacy always involves social and cultural elements is reflected in UNESCO's stipulation that literacy is an "ability to identify, understand, interpret, create, communicate and compute, using printed and written materials "associated with varying contexts"." Modern attention to literacy as a "context-dependent assemblage of social practices" reflects the understanding that individuals' reading and writing practices develop and change over the lifespan as their cultural, political, and historical contexts change. For example, in Scotland, literacy has been defined as: "The ability to read, write and use numeracy, to handle information, to express ideas and opinions, to make decisions and solve problems, as family members, workers, citizens and lifelong learners."

Such expanded definitions have altered long-standing "rule of thumb" measures of literacy, e.g., the ability to read the newspaper, in part because the increasing involvement of computers and other digital technologies in communication necessitates additional skills (e.g. interfacing with web browsers and word processing programs; organizing and altering the configuration of files, etc.). By extension, the expansion of these necessary skill-sets became known, variously, as computer literacy, information literacy, and technological literacy. Elsewhere definitions of literacy extend the original notion of "acquired ability" into concepts like "arts literacy," visual literacy (the ability to understand visual forms of communication such as body language, pictures, maps, and video), statistical literacy, critical literacy, media literacy, ecological literacy and health literacy. 

Literacy emerged with the development of numeracy and computational devices as early as 8000 BCE. Script developed independently at least five times in human history Mesopotamia, Egypt, the Indus civilization, lowland Mesoamerica, and China.
The earliest forms of written communication originated in Sumer, located in southern Mesopotamia about 3500-3000 BCE. During this era, literacy was "a largely functional matter, propelled by the need to manage the new quantities of information and the new type of governance created by trade and large scale production". Writing systems in Mesopotamia first emerged from a recording system in which people used impressed token markings to manage trade and agricultural production. The token system served as a precursor to early cuneiform writing once people began recording information on clay tablets. Proto-cuneiform texts exhibit not only numerical signs, but also ideograms depicting objects being counted.

Egyptian hieroglyphs emerged from 3300-3100 BCE and depicted royal iconography that emphasized power amongst other elites. The Egyptian hieroglyphic writing system was the first notation system to have phonetic values.

Writing in lowland Mesoamerica was first put into practice by the Olmec and Zapotec civilizations in 900-400 BCE. These civilizations used glyphic writing and bar-and-dot numerical notation systems for purposes related to royal iconography and calendar systems.

The earliest written notations in China date back to the Shang Dynasty in 1200 BCE. These systematic notations were found inscribed on bones and recorded sacrifices made, tributes received, and animals hunted, which were activities of the elite. These oracle-bone inscriptions were the early ancestors of modern Chinese script and contained logosyllabic script and numerals.

Indus script is largely pictorial and has not been deciphered yet. It may or may not include abstract signs. It is thought that they wrote from right to left and that the script is thought to be logographic. Because it has not been deciphered, linguists disagree on whether it is a complete and independent writing system; however, it is genuinely thought to be an independent writing system that emerged in the Harappa culture.

These examples indicate that early acts of literacy were closely tied to power and chiefly used for management practices, and probably less than 1% of the population was literate, as it was confined to a very small ruling elite.

According to social anthropologist Jack Goody, there are two interpretations that regard the origin of the alphabet. Many classical scholars, such as historian Ignace Gelb, credit the Ancient Greeks for creating the first alphabetic system (c. 750 BCE) that used distinctive signs for consonants and vowels. But Goody contests, "The importance of Greek culture of the subsequent history of Western Europe has led to an over-emphasis, by classicists and others, on the addition of specific vowel signs to the set of consonantal ones that had been developed earlier in Western Asia".

Thus, many scholars argue that the ancient Semitic-speaking peoples of northern Canaan (modern-day Syria) invented the consonantal alphabet as early as 1500 BCE. Much of this theory's development is credited to English archeologist Flinders Petrie, who, in 1905, came across a series of Canaanite inscriptions located in the turquoise mines of Serabit el-Khadem. Ten years later, English Egyptologist Alan Gardiner reasoned that these letters contain an alphabet, as well as references to the Canaanite goddess Asherah. In 1948, William F. Albright deciphered the text using additional evidence that had been discovered subsequent to Goody's findings. This included a series of inscriptions from Ugarit, discovered in 1929 by French archaeologist Claude F. A. Schaeffer. Some of these inscriptions were mythological texts (written in an early Canaanite dialect) that consisted of a 32-letter cuneiform consonantal alphabet.

Another significant discovery was made in 1953 when three arrowheads were uncovered, each containing identical Canaanite inscriptions from twelfth century BCE. According to Frank Moore Cross, these inscriptions consisted of alphabetic signs that originated during the transitional development from pictographic script to a linear alphabet. Moreover, he asserts, "These inscriptions also provided clues to extend the decipherment of earlier and later alphabetic texts".

The consonantal system of the Canaanite script inspired alphabetical developments in subsequent systems. During the Late Bronze Age, successor alphabets appeared throughout the Mediterranean region and were employed for Phoenician, Hebrew and Aramaic.

According to Goody, these cuneiform scripts may have influenced the development of the Greek alphabet several centuries later. Historically, the Greeks contended that their writing system was modeled after the Phoenicians. However, many Semitic scholars now believe that Ancient Greek is more consistent with an early form Canaanite that was used c. 1100 BCE. While the earliest Greek inscriptions are dated c. eighth century BCE, epigraphical comparisons to Proto-Canaanite suggest that the Greeks may have adopted the consonantal alphabet as early as 1100 BCE, and later "added in five characters to represent vowels".

Phoenician, which is considered to contain the first "linear alphabet", rapidly spread to the Mediterranean port cities in northern Canaan. Some archeologists believe that Phoenician scripture had some influence on the developments of the Hebrew and Aramaic alphabets based on the fact that these languages evolved during the same time period, share similar features, and are commonly categorized into the same language group.

When the Israelites migrated to Canaan between 1200 and 1001 BCE, they also adopted a variation of the Canaanite alphabet. Baruch ben Neriah, Jeremiah's scribe, used this alphabet to create the later scripts of the Old Testament. The early Hebrew alphabet was prominent in the Mediterranean region until Chaldean Babylonian rulers exiled the Jews to Babylon in the sixth century BCE. It was then that the new script ("Square Hebrew") emerged and the older one rapidly died out.

The Aramaic alphabet also emerged sometime between 1200 and 1000 BCE. As the Bronze Age collapsed, the Aramaeans moved into Canaan and Phoenician territories and adopted their scripts. Although early evidence of this writing is scarce, archeologists have uncovered a wide range of later Aramaic texts, written as early as the seventh century BCE. Due to its longevity and prevalence in the region, Achaemenid rulers would come to adopt it as a "diplomatic language". The modern Aramaic alphabet rapidly spread east to the Kingdom of Nabataea, then to Sinai and the Arabian Peninsula, eventually making its way to Africa. Aramaean merchants carried older variations of Aramaic as far as India, where it later influenced the development of the Brahmi script. It also led to the developments of Arabic and Pahlavi (an Iranian adaptation), "as well as for a range of alphabets used by early Turkish and Mongol tribes in Siberia, Mongolia and Turkestan". Literacy at this period spread with the merchant classes and may have grown to number 15-20% of the total population.

The Aramaic language declined with the spread of Islam, which was accompanied by the spread of Arabic.

Until recently it was thought that the majority of people were illiterate in ancient times. However, recent work challenges this perception. Anthony DiRenzo asserts that Roman society was "a civilization based on the book and the register", and "no one, either free or slave, could afford to be illiterate". Similarly Dupont points out, "The written word was all around them, in both public and private life: laws, calendars, regulations at shrines, and funeral epitaphs were engraved in stone or bronze. The Republic amassed huge archives of reports on every aspect of public life". The imperial civilian administration produced masses of documentation used in judicial, fiscal and administrative matters as did the municipalities. The army kept extensive records relating to supply and duty rosters and submitted reports. Merchants, shippers, and landowners (and their personal staffs) especially of the larger enterprises must have been literate.

In the late fourth century the Desert Father Pachomius would expect literacy of a candidate for admission to his monasteries:
they shall give him twenty Psalms or two of the Apostles' epistles or some other part of Scripture. And if he is illiterate he shall go at the first, third and sixth hours to someone who can teach and has been appointed for him. He shall stand before him and learn very studiously and with all gratitude. The fundamentals of a syllable, the verbs and nouns shall all be written for him and even if he does not want to he shall be compelled to read.

In the course of the 4th and 5th century the Churches made efforts to ensure a better clergy in particular among the bishops who were expected to have a classical education, which was the hallmark of a socially acceptable person in higher society (and possession of which allayed the fears of the pagan elite that their cultural inheritance would be destroyed). Even after the remnants of the Western Roman Empire fell in the 470s, literacy continued to be a distinguishing mark of the elite as communications skills were still important in political and Church life (bishops were largely drawn from the senatorial class) in a new cultural synthesis that made "Christianity the Roman religion". However, these skills were less needed than previously in the absence of the large imperial administrative apparatus whose middle and top echelons the elite had dominated as if by right. Even so, in pre-modern times it is unlikely that literacy was found in more than about 30-40% of the population. The highest percentage of literacy during the Dark Ages was among the clergy and monks who supplied much of the staff needed to administer the states of western Europe.

Post-Antiquity illiteracy was made much worse by the lack of a suitable writing medium. When the Western Roman Empire collapsed, the import of papyrus to Europe ceased. Since papyrus perishes easily and does not last well in the wetter European climate, parchment was used, which was expensive and accessible only by the Church and the wealthy. Paper was introduced into Europe in Spain in the 11th century. Its use spread north slowly over the next four centuries. Literacy saw a resurgence as a result, and by the 15th century paper had largely replaced parchment except for luxury manuscripts.

The Reformation stressed the importance of literacy and being able to read the Bible. The Protestant countries were the first to attain full literacy; Scandinavian countries were fully literate in the early 17th century. The Church demanded literacy as the pre-requisite for marriage in Sweden, further propagating full literacy.

Modern industrialization began in England and Scotland in the 18th century, where there were relatively high levels of literacy among farmers, especially in Scotland. This permitted the recruitment of literate craftsman, skilled workers, foremen and managers who supervised the emerging textile factories and coal mines. Much of a labor was unskilled, and especially in textile mills children as young as eight proved useful in handling chores and adding to the family income. Indeed, children were taken out of school to work alongside their parents in the factories. However by the mid-nineteenth century, unskilled labor forces were common in Western Europe, and British industry moved upscale, needing many more engineers and skilled workers who could handle technical instructions and handle complex situations. Literacy was essential to be hired. A senior government official told Parliament in 1870:

Literacy data published by UNESCO displays that since 1950, the adult literacy rate at the world level has increased by 5 percentage points every decade on average, from 55.7 per cent in 1950 to 86.2 per cent in 2015. However, for four decades, the population growth was so rapid that the number of illiterate adults kept increasing, rising from 700 million in 1950 to 878 million in 1990. Since then, the number has fallen markedly to 745 million in 2015, although it remains higher than in 1950 despite decades of universal education policies, literacy interventions and the spread of print material and information and communications technology (ICT). However, these trends have been far from uniform across regions.

Available global data indicates significant variations in literacy rates between world regions. North America, Europe, West Asia, and Central Asia have achieved almost full adult literacy (individuals at or over the age of 15) for both men and women. Most countries in East Asia and the Pacific, as well as Latin America and the Caribbean, are above a 90% literacy rate for adults. Illiteracy persists to a greater extent in other regions: 2013 UNESCO Institute for Statistics (UIS) data indicates adult literacy rates of only 67.55% in South Asia and North Africa, 59.76% in Sub-Saharan Africa. 

In much of the world, high youth literacy rates suggest that illiteracy will become less and less common as younger generations with higher educational attainment levels replace older ones. However, in sub-Saharan Africa and South Asia, where the vast majority of the world's illiterate youth live, lower school enrollment implies that illiteracy will persist to a greater degree. According to 2013 UIS data, the youth literacy rate (individuals ages 15 to 24) is 84.03% in South Asia and North Africa, and 70.06% in Sub-Saharan Africa. Yet the literate/illiterate distinction is not clear-cut: for example, given that a large part of the benefits of literacy can be obtained by having access to a literate person in the household, some recent literature in economics, starting with the work of Kaushik Basu and James Foster, distinguishes between a "proximate illiterate" and an "isolated illiterate". The former refers to an illiterate person who lives in a household with literates and the latter to an illiterate who lives in a household of all illiterates. What is of concern is that many people in poor nations are not proximate illiterates but rather isolated illiterates.

That being said, literacy has rapidly spread in several regions in the last twenty-five years (see image).

According to 2015 UIS data collected by the UNESCO Institute for Statistics, about two-thirds (63%) of the world's illiterate adults are women. This disparity was even starker in previous decades: from 1970 to 2000, the global gender gap in literacy would decrease by roughly 50%. In recent years, however, this progress has stagnated, with the remaining gender gap holding almost constant over the last two decades. In general, the gender gap in literacy is not as pronounced as the regional gap; that is, differences between countries in overall literacy are often larger than gender differences within countries. However, the gap between men and women would narrow from 1990 onwards, after the increase of male adult literacy rates at 80 per cent (see image).

Sub-Saharan Africa, the region with the lowest overall literacy rates, also features the widest gender gap: just 52% of adult females are literate, and 68% among adult men. Similar gender disparity persists in two other regions, North Africa (86% adult male literacy, 70% adult female literacy) and South Asia (77% adult male literacy, 58% adult female literacy).

The 1990 World Conference on Education for All, held in Jomtien, Thailand, would bring attention to the literacy gender gap and prompt many developing countries to prioritize women's literacy.

In many contexts, female illiteracy co-exists with other aspects of gender inequality. Martha Nussbaum suggests illiterate women are more vulnerable to becoming trapped in an abusive marriage, given that illiteracy limits their employment opportunities and worsens their intra-household bargaining position. Moreover, Nussbaum links literacy to the potential for women to effectively communicate and collaborate with one another in order "to participate in a larger movement for political change."

Social barriers prevent expanding literacy skills among women and girls. Making literacy classes available can be ineffective when it conflicts with the use of the valuable limited time of women and girls. School age girls, in many contexts, face stronger expectations than their male counterparts to perform household work and care after younger siblings. Generational dynamics can also perpetuate these disparities: illiterate parents may not readily appreciate the value of literacy for their daughters, particularly in traditional, rural societies with expectations that girls will remain at home.

A 2015 World Bank and the International Center for Research on Women review of academic literature would conclude that child marriage, which predominantly impacts girls, tends to reduce literacy levels. A 2008 analysis of the issue in Bangladesh found that for every additional year of delay in a girl's marriage, her likelihood of literacy would increase by 5.6 percent. Similarly, a 2014 study found that in sub-Saharan Africa, marrying early would significantly decrease a girl's probability of literacy, holding other variables constant. A 2015 review of the child marriage literature therefore would recommend marriage postponement as part of a strategy to increase educational attainment levels, including female literacy in particular.

While women and girls comprise the majority of the global illiterate population, in many developed countries a literacy gender gap exists in the opposite direction. Data from the Programme for International Student Assessment (PISA) has consistently indicated the literacy underachievement of boys within member countries of the Organisation for Economic Co-operation and Development (OECD). In view of such findings, many education specialists have recommended changes in classroom practices to better accommodate boys' learning styles, and to remove any gender stereotypes that may create a perception of reading and writing as feminine activities.

Many policy analysts consider literacy rates as a crucial measure of the value of a region's human capital. For example, literate people can be more easily trained than illiterate people, and generally have a higher socioeconomic status; thus they enjoy better health and employment prospects. The international community has come to consider literacy as a key facilitator and goal of development. In regard to the Sustainable Development Goals adopted by the UN in 2015, the UNESCO Institute for Lifelong Learning has declared the "central role of literacy in responding to sustainable development challenges such as health, social equality, economic empowerment and environmental sustainability." A majority of prisoners have been found to be illiterate: In Edinburgh prison, winner of the 2010 Libraries Change Lives Award, "the library has become the cornerstone of the prison's literacy strategy" and thus recidivism and reoffending can be reduced, and incarcerated persons can work toward attaining higher socioconomic status once released.

Print illiteracy generally corresponds with less knowledge about modern hygiene and nutritional practices, an unawareness which can exacerbate a wide range of health issues. Within developing countries in particular, literacy rates also have implications for child mortality; in these contexts, children of literate mothers are 50% more likely to live past age 5 than children of illiterate mothers. Public health research has thus increasingly concerned itself with the potential for literacy skills to allow women to more successfully access health care systems, and thereby facilitate gains in child health.

For example, a 2014 descriptive research survey project correlates literacy levels with the socioeconomic status of women in Oyo State, Nigeria. The study claims that developing literacy in this area will bring "economic empowerment and will encourage rural women to practice hygiene, which will in turn lead to the reduction of birth and death rates."

Literacy can increase job opportunities and access to higher education. In 2009, the National Adult Literacy agency (NALA) in Ireland commissioned a cost benefit analysis of adult literacy training. This concluded that there were economic gains for the individuals, the companies they worked for, and the Exchequer, as well as the economy and the country as a whole—for example, increased GDP. Korotayev and coauthors have revealed a rather significant correlation between the level of literacy in the early 19th century and successful modernization and economic breakthroughs in the late 20th century, as "literate people could be characterized by a greater innovative-activity level, which provides opportunities for modernization, development, and economic growth".

While informal learning within the home can play an important role in literacy development, gains in childhood literacy often occur in primary school settings. Continuing the global expansion of public education is thus a frequent focus of literacy advocates. These kinds of broad improvements in education often require centralized efforts undertaken by national governments; alternatively, local literacy projects implemented by NGOs can play an important role, particularly in rural contexts.

Funding for both youth and adult literacy programs often comes from large international development organizations. USAID, for example, steered donors like the Bill and Melinda Gates Foundation and the Global Partnership for Education toward the issue of childhood literacy by developing the Early Grade Reading Assessment. Advocacy groups like the National Institute of Adult Continuing Education have frequently called upon international organizations such as UNESCO, the International Labour Organization, the World Health Organization, and the World Bank to prioritize support for adult women's literacy. Efforts to increase adult literacy often encompass other development priorities as well; for example, initiatives in Ethiopia, Morocco, and India have combined adult literacy programs with vocational skills trainings in order to encourage enrollment and address the complex needs of women and other marginalized groups who lack economic opportunity.

In 2013, the UNESCO Institute for Lifelong Learning published a set of case studies on programs that successfully improved female literacy rates. The report features countries from a variety of regions and of differing income levels, reflecting the general global consensus on "the need to empower women through the acquisition of literacy skills." Part of the impetus for UNESCO's focus on literacy is a broader effort to respond to globalization and "the shift towards knowledge-based societies" that it has produced. While globalization presents emerging challenges, it also provides new opportunities: many education and development specialists are hopeful that new ICTs will have the potential to expand literacy learning opportunities for children and adults, even those in countries that have historically struggled to improve literacy rates through more conventional means.

The Human Development Index, produced by the United Nations Development Programme (UNDP), uses education as one of its three indicators; originally, adult literacy represented two-thirds of this education index weight. In 2010, however, the UNDP replaced the adult literacy measure with mean years of schooling. A 2011 UNDP research paper framed this change as a way to "ensure current relevance," arguing that gains in global literacy already achieved between 1970 and 2010 meant that literacy would be "unlikely to be as informative of the future." Other scholars, however, have since warned against overlooking the importance of literacy as an indicator and a goal for development, particularly for marginalized groups such as women and rural populations.

Unlike medieval times, when reading and writing skills were restricted to a few elites and the clergy, these literacy skills are now expected from every member of a society. Literacy is a human right essential for lifelong learning and social change. As supported by the 1996 Report of the International Commission on Education for the Twenty-First Century, and the 1997 Hamburg Declaration: ‘Literacy, broadly conceived as the basic knowledge and skills needed by all in a rapidly changing world, is a fundamental human right. (...) There are millions, the majority of whom are women, who lack opportunities to learn or who have insufficient skills to be able to assert this right. The challenge is to enable them to do so. This will often imply the creation of preconditions for learning through awareness raising and empowerment. Literacy is also a catalyst for participation in social, cultural, political and economic activities, and for learning throughout life’.

The public library has long been a force promoting literacy in many countries. In the U.S. context, the American Library Association promotes literacy through the work of the Office for Literacy and Outreach Services. This committee's charge includes ensuring equitable access to information and advocating for adult new and non-readers. The Public Library Association recognizes the importance of early childhood in the role of literacy development and created, in collaboration with the Association for Library Service to Children, Every Child Ready to Read @your library in order to inform and support parents and caregivers in their efforts to raise children who become literate adults. The release of the National Assessment of Adult Literacy (NAAL) report in 2005 revealed that approximately 14% of U.S. adults function at the lowest level of literacy; 29% of adults function at the basic functional literacy level and cannot help their children with homework beyond the first few grades. The lack of reading skills hinders adults from reaching their full potential. They might have difficulty getting and maintaining a job, providing for their families, or even reading a story to their children. For adults, the library might be the only source of a literacy program.

Dia! Which stand for Diversity in Action and is also known as "El Día de los Niños/El día de los libros (Children's Day/Book Day)" is a program which celebrates the importance of reading to children from all cultural and linguistic backgrounds. Dia! is celebrated every year on 30 April in schools, libraries, and homes and this website provides tools and programs to encourage reading in children. Parents, caregivers, and educators can even start a book club.

This community literacy program was initiated in 1992 by the Orange County Public Library in California. The mission of READ/Orange County is to "create a more literate community by providing diversified services of the highest quality to all who seek them." Potential tutors train during an extensive 23-hour tutor training workshop in which they learn the philosophy, techniques and tools they will need to work with adult learns. After the training, the tutors invest at least 50 hours a year to tutoring their student.The organization builds on people's experience as well as education rather than trying to make up for what has not been learned. The program seeks to equip students with skills to continue learning in the future. The guiding philosophy is that an adult who learns to read creates a ripple effect in the community. The person becomes an example to children and grandchildren and can better serve the community.

Located in Boulder, Colorado, the program recognized the difficulty that students had in obtaining child care while attending tutoring sessions, and joined with the University of Colorado to provide reading buddies to the children of students. Reading Buddies matches children of adult literacy students with college students who meet with them once a week throughout the semester for an hour and a half. The college students receive course credit to try to enhance the quality and reliability of their time. Each Reading Buddies session focuses primarily on the college student reading aloud with the child. The goal is to help the child gain interest in books and feel comfortable reading aloud. Time is also spent on word games, writing letters, or searching for books in the library. Throughout the semester the pair work on writing and illustrating a book together. The college student's grade is partly dependent on the completion of the book. Although Reading Buddies began primarily as an answer to the lack of child care for literacy students, it has evolved into another aspect of the program. Participating children show marked improvement in their reading and writing skills throughout the semester.

Approximately 120,000 adults in Hillsborough County are illiterate or read below the fourth-grade level; According to 2003 Census statistics, 15 percent of Hillsborough County residents age 16 and older lacked basic prose literacy skills. Since 1986, the Hillsborough Literacy Council is "committed to improving literacy by empowering adults through education". Sponsored by the statewide Florida Literacy Coalition and affiliated with Tampa-Hillsborough Public Library System, HLC strives to improve the literacy ability of adults in Hillsborough County, Florida. Using library space, the HLC provides tutoring for English for speakers of other languages (ESOL) in small groups or one-on-one tutoring. Through one-on-one tutoring, the organization works to help adult students reach at least the fifth-grade level in reading. The organization also provides volunteer-run conversation groups for English practice.

Critiques of autonomous models of literacy notwithstanding, the belief that reading development is key to literacy remains dominant, at least in the United States, where it is understood as progression of skills that begins with the ability to understand spoken words and decode written words, and that culminates in the deep understanding of text. Reading development involves a range of complex language-underpinnings including awareness of speech sounds (phonology), spelling patterns (orthography), word meaning (semantics), grammar, (syntax) and patterns of word formation (morphology), all of which provide a necessary platform for reading fluency and comprehension.

Once these skills are acquired, it is maintained, a reader can attain full language literacy, which includes the abilities to apply to printed material critical analysis, inference and synthesis; to write with accuracy and coherence; and to use information and insights from text as the basis for informed decisions and creative thought.

For this reason, teaching English literacy in the United States is dominated by a focus on a set of discrete decoding skills. From this perspective, literacy—or, rather, reading—comprises a number of subskills that can be taught to students. These skill sets include phonological awareness, phonics (decoding), fluency, comprehension, and vocabulary. Mastering each of these subskills is necessary for students to become proficient readers.

From this same perspective, readers of alphabetic languages must understand the alphabetic principle to master basic reading skills. For this purpose a writing system is "alphabetic" if it uses symbols to represent individual language sounds, though the degree of correspondence between letters and sounds varies between alphabetic languages. Syllabic writing systems (such as Japanese kana) use a symbol to represent a single syllable, and logographic writing systems (such as Chinese) use a symbol to represent a morpheme.

There are any number of approaches to teaching literacy; each is shaped by its informing assumptions about what literacy is and how it is best learned by students. Phonics instruction, for example, focuses on reading at the level of the word. It teaches readers to observe and interpret the letters or groups of letters that make up words. A common method of teaching phonics is synthetic phonics, in which a novice reader pronounces each individual sound and "blends" them to pronounce the whole word. Another approach is embedded phonics instruction, used more often in whole language reading instruction, in which novice readers learn about the individual letters in words on a just-in-time, just-in-place basis that is tailored to meet each student's reading and writing learning needs. That is, teachers provide phonics instruction opportunistically, within the context of stories or student writing that feature many instances of a particular letter or group of letters. Embedded instruction combines letter-sound knowledge with the use of meaningful context to read new and difficult words. Techniques such as directed listening and thinking activities can be used to aid children in learning how to read and reading comprehension.

In a 2012 proposal, it has been claimed that reading can be acquired naturally if print is constantly available at an early age in the same manner as spoken language. If an appropriate form of written text is made available before formal schooling begins, reading should be learned inductively, emerge naturally, and with no significant negative consequences. This proposal challenges the commonly held belief that written language requires formal instruction and schooling. Its success would change current views of literacy and schooling. Using developments in behavioral science and technology, an interactive system (Technology Assisted Reading Acquisition, TARA) would enable young pre-literate children to accurately perceive and learn properties of written language by simple exposure to the written form.

In Australia a number of State governments have introduced Reading Challenges to improve literacy. The Premier's Reading Challenge in South Australia, launched by Premier Mike Rann has one of the highest participation rates in the world for reading challenges. It has been embraced by more than 95% of public, private and religious schools.

Programs have been implemented in regions that have an ongoing conflict or in a post-conflict stage. The Norwegian Refugee Council Pack program has been used in 13 post-conflict countries since 2003. The program organizers believe that daily routines and other wise predictable activities help the transition from war to peace. Learners can select one area in vocational training for a year-long period. They complete required courses in agriculture, life skills, literacy and numeracy. Results have shown that active participation and management of the members of the program are important to the success of the program. These programs share the use of integrated basic education, e.g. literacy, numeracy, scientific knowledge, local history and culture, native and mainstream language skills, and apprenticeships.

Although there is considerable awareness that language deficiencies (lacking proficiency) are disadvantageous to immigrants settling in a new country, there appears to be a lack of pedagogical approaches that address the instruction of literacy to migrant English language learners (ELLs). Harvard scholar Catherine Snow (2001) called for a gap to be addresses: "The TESOL field needs a concerted research effort to inform literacy instruction for such children ... to determine when to start literacy instruction and how to adapt it to the LS reader's needs". The scenario becomes more complex when there is no choice in such decisions as in the case of the current migration trends with citizens from the Middle East and Africa being relocated to English majority nations due to various political or social reasons. Recent developments to address the gap in teaching literacy to second or foreign language learners has been ongoing and promising results have been shown by Pearson and Pellerine (2010) which integrates Teaching for Understanding, a curricular framework from the Harvard Graduate School of Education. A series of pilot projects had been carried out in the Middle East and Africa (see Patil, 2016). In this work significant interest from the learners perspective have been noticed through the integration of visual arts as springboards for literacy oriented instruction. In one case migrant women had been provided with cameras and a walking tour of their local village was provided to the instructor as the women photographed their tour focusing on places and activities that would later be used for writings about their daily life. In essence a narrative of life. Other primers for writing activities include: painting, sketching, and other craft projects (e.g. gluing activities).

A series of pilot studies were carried out to investigate alternatives to instructing literacy to migrant ELLs, starting from simple trials aiming to test the teaching of photography to participants with no prior photography background, to isolating painting and sketching activities that could later be integrated into a larger pedagogical initiative. In efforts to develop alternative approaches for literacy instruction utilising visual arts, work was carried out with Afghan labourers, Bangladeshi tailors, Emirati media students, internal Ethiopian migrants (both labourers and university students), and a street child.
It should be pointed out that in such challenging contexts sometimes the teaching of literacy may have unforeseen barriers. The "EL Gazette" reported that in the trials carried out in Ethiopia, for example, it was found that all ten of the participants had problems with vision. In order to overcome this, or to avoid such challenges, preliminary health checks can help inform pre-teaching in order to better assist in the teaching/learning of literacy.

In a visual arts approach to literacy instruction a benefit can be the inclusion of both a traditional literacy approach (reading and writing) while at the same time addressing 21st Century digital literacy instruction through the inclusion of digital cameras and posting images onto the web. Many scholars feel that the inclusion of digital literacy is necessary to include under the traditional umbrella of literacy instruction specifically when engaging second language learners. (Also see: Digital literacy.) 

Other ways in which visual arts have been integrated into literacy instruction for migrant populations include integrating aspects of visual art with the blending of core curricular goals.

A more pressing challenge in education is the instruction of literacy to Migrant English Language Learners (MELLs), a term coined by Pellerine. It is not just limited to English. “Due to the growing share of immigrants in many Western societies, there has been increasing concern for the degree to which immigrants acquire language that is spoken in the destination country” (Tubergen 2006). Remembering that teaching literacy to a native in their L1 can be challenging, and the challenge becomes more cognitively demanding when in a second language (L2), the task can become considerably more difficult when confronted with a migrant who has made a sudden change (migrated) and requires the second language upon arrival in the country of destination. In many instances a migrant will not have the opportunity, for many obvious reasons, to start school again at grade one and acquire the language naturally. In these situations alternative interventions need to take place.

In working with illiterate people (and individuals with low-proficiency in an L2) following the composition of some artifact like in taking a photo, sketching an event, or painting an image, a stage of orality has been seen as an effective way to understand the intention of the learner.

In the accompanying image from left to right a) an image taken during a phototour of the participant's village. This image is of the individual at her shop, and this is one of her products that she sells, dung for cooking fuel. The image helps the interlocutor understand the realities of the participants daily life and most importantly it allows the participant the opportunity to select what they feel is important to them. b) This is an image of a student explaining and elaborating the series of milestones in her life to a group. In this image the student had a very basic ability and with some help was able to write brief captions under the images. While she speaks a recording of her story takes place to understand her story and to help develop it in the L2. The third image is of a painting that had been used with a composite in Photoshop. With further training participants can learn how to blend images they would like to therefore introducing elements of digital literacies, beneficial in many spheres of life in the 21st century.

In the following image (see right) you can see two samples 1) One in Ethiopia from stencil to more developed composition based on a village tour, photography, and paintings. 2) In the Middle East at a tailor's shop focusing English for Specific Purposes (ESP) and in this example the writing has evolved from photography, sketching, and in situ exposure for the instructor (much like the village tour in sample one).

From the work based in Ethiopia, participants were asked to rate preference of activity, on a scale of 1-10. The survey prompt was: On a scale of 1 - 10 how would you rate photography as an activity that helped you get inspiration for your writing activities (think of enjoyment and usefulness). The following activities were rated, in order of preference - activities used as primers for writing:


More research would need to be conducted to confirm such trends.

In bringing work together from students in culminating projects, authorship programs have been successful in bringing student work together in book format. Such artifacts can be used to both document learning, but more importantly reinforce language and content goals.

The culmination of such writings, into books can evoke both intrinsic and extrinsic motivation. Form feedback by students involved in such initiatives the responses have indicated that the healthy pressures of collective and collaborative work was beneficial.

Teaching people to read and write, in a traditional sense of the meaning (literacy) is a very complex task in a native language. To do this in a second language becomes increasingly more complex, and in the case of migrants relocating to another country there can be legal and policy driven boundaries that prohibit the naturalization and acquisition of citizen ship based on language proficiency. In Canada for example despite a debate, language tests are required years after settling into Canada. Similar exists globally, see:, and for example.

The "EL Gazette" reviewed Pellerine's work with migrant English language learners and commented: "Handing English language learners a sponge and some paint and asking them to ‘paint what comes’ might not appear like a promising teaching method for a foreign language. But Canadian EL instructor and photographer Steve Pellerine has found that the technique, along with others based around the visual arts, has helped some of his most challenging groups to learn". Visual arts have been viewed as an effective way to approach literacy instruction - the art being primers for subsequent literacy tasks within a scaffolded curricular design, such at Teaching for Understanding (TfU) or Understanding by Design (UbD).

Nearly one in ten young adult women have poor reading and writing skills in the UK in the 21st century. This seriously damages their employment prospects and many are trapped in poverty. Lack of reading skill is a social stigma and women tend to hide their difficulty rather than seeking help. Girls on average do better than boys at English in school. A quarter of British adults would struggle to read a bus timetable.

Literacy is first documented in the area of modern England on 24 September 54 BCE, on which day Julius Caesar and Quintus Cicero wrote to Cicero "from the nearest shores of Britain". Literacy was widespread under Roman rule, but became very rare, limited almost entirely to churchmen, after the fall of the Western Roman Empire. In 12th and 13th century England, the ability to recite a particular passage from the Bible in Latin entitled a common law defendant to the so-called benefit of clergy: i.e. trial before an ecclesiastical court, where sentences were more lenient, instead of a secular one, where hanging was a likely sentence. Thus literate lay defendants often claimed benefit of clergy, while an illiterate person who had memorized the psalm used as the literacy test, Psalm 51 ("O God, have mercy upon me..."), could also claim benefit of clergy. Despite lacking a system of free and compulsory primary schooling, England reached near universal literacy in the 19th century as a result of shared, informal learning provided by family members, fellow workers, and/or benevolent employers. Even with near universal literacy rates, the gap between male and female literacy rates persisted until the early 20th century. Many women in the West during the 19th century were able to read, but unable to write.

Formal higher education in the arts and sciences in Wales, from the Middle Ages to the 18th century, was the preserve of the wealthy and the clergy. As in England, Welsh history and archaeological finds dating back to the Bronze Age reveal not only reading and writing, but also alchemy, botany, advanced maths and science. Following the Roman occupation and the conquest by the English, education in Wales was at a very low ebb in the early modern period; in particular, formal education was only available in English while the majority of the population spoke only Welsh. The first modern grammar schools were established in Welsh towns such as Ruthin, Brecon, and Cowbridge. One of the first modern national education methods to use the native Welsh language was started by Griffith Jones in 1731. Jones was the rector of Llanddowror from 1716 and remained there for the rest of his life. He organized and introduced a Welsh medium circulating school system, which was attractive and effective for Welsh speakers, while also teaching them English, which gave them access to broader educational sources. The circulating schools may have taught half the country's population to read. Literacy rates in Wales by the mid-18th century were one of the highest.
The ability to read did not necessarily imply the ability to write. The 1686 church law ("kyrkolagen") of the Kingdom of Sweden (which at the time included all of modern Sweden, Finland, Latvia and Estonia) enforced literacy on the people, and by 1800 the ability to read was close to 100%. This was directly dependent on the need to read religious texts in the Lutheran faith in Sweden and Finland. As a result, literacy in these countries was inclined towards reading, specifically. But as late as the 19th century, many Swedes, especially women, could not write. The exception to this rule were the men and women of Iceland who achieved widespread literacy without formal schooling, libraries, or printed books via informal tuition by religious leaders and peasant teachers. That said, the situation in England was far worse than in Scandinavia, France, and Prussia: as late as 1841, 33% of all Englishmen and 44% of Englishwomen signed marriage certificates with their mark as they were unable to write (government-financed public education was not available in England until 1870 and, even then, on a limited basis).

Historian Ernest Gellner argues that Continental European countries were far more successful in implementing educational reform precisely because their governments were more willing to invest in the population as a whole. Government oversight allowed countries to standardize curriculum and secure funding through legislation thus enabling educational programs to have a broader reach.

Although the present-day concepts of literacy have much to do with the 15th-century invention of the movable type printing press, it was not until the Industrial Revolution of the mid-19th century that paper and books became affordable to all classes of industrialized society. Until then, only a small percentage of the population were literate as only wealthy individuals and institutions could afford the materials. Even , the cost of paper and books is a barrier to universal literacy in some less-industrialized nations.

On the other hand, historian Harvey Graff argues that the introduction of mass schooling was in part an effort to control the type of literacy that the working class had access to. According to Graff, literacy learning was increasing outside of formal settings (such as schools) and this uncontrolled, potentially critical reading could lead to increased radicalization of the populace. In his view, mass schooling was meant to temper and control literacy, not spread it. Graff also points out, using the example of Sweden, that mass literacy can be achieved without formal schooling or instruction in writing.

Research on the literacy rates of Canadians in the colonial days rested largely on examinations of the proportion of signatures to marks on parish acts (birth, baptismal, and marriage registrations). Although some researchers have concluded that signature counts drawn from marriage registers in nineteenth century France corresponded closely with literacy tests given to military conscripts, others regard this methodology as a "relatively unimaginative treatment of the complex practices and events that might be described as literacy" (Curtis, 2007, p. 1-2). But censuses (dating back to 1666) and official records of New France offer few clues of their own on the population's levels of literacy, therefore leaving few options in terms of materials from which to draw literary rate estimates.

In his research of literacy rates of males and females in New France, Trudel found that in 1663, of 1,224 persons in New France who were of marriageable age, 59% of grooms and 46% of brides wrote their name; however, of the 3,000-plus colony inhabitants, less than 40% were native born. Signature rates were therefore likely more reflective of rates of literacy among French immigrants. Magnuson's (1985) research revealed a trend: signature rates for the period of 1680–1699 were 42% for males, 30% for females; in 1657-1715, they were 45% for males and 43% for females; in 1745-1754, they were higher for females than for males. He believed that this upward trend in rates of females’ ability to sign documents was largely attributed to the larger number of female religious orders, and to the proportionately more active role of women in health and education, while the roles of male religious orders were largely to serve as parish priests, missionaries, military chaplains and explorers. 1752 marked the date that Canada's first newspaper—the "Halifax Gazette"—began publication.

The end of the Seven Years' War in 1763 allowed two Philadelphia printers to come to Québec City and to begin printing a bilingual "Quebec Gazette" in 1764, while in 1785 Fleury Mesplet started publication of the "Montreal Gazette", which is now the oldest continuing newspaper in the country.

In the 19th century, everything about print changed, and literature in its many forms became much more available. But educating the Canadian population in reading and writing was nevertheless a huge challenge. Concerned about the strong French Canadian presence in the colony, the British authorities repeatedly tried to help establish schools that were outside the control of religious authorities, but these efforts were largely undermined by the Catholic Church and later the Anglican clergy.

From the early 1820s in Lower Canada, classical college curriculum, which was monopolized by the Church, was also subject to growing liberal and lay criticism, arguing it was fit first and foremost to produce priests, when Lower Canadians needed to be able to compete effectively with foreign industry and commerce and with the immigrants who were monopolizing trade (Curtis, 1985). Liberal and lay attempts to promote parish schools generated a reaction from the Catholic and later the Anglican clergy in which the dangers of popular literacy figured centrally. Both churches shared an opposition to any educational plan that encouraged lay reading of the Bible, and spokesmen for both warned of the evil and demoralizing tendencies of unregulated reading in general. Granted the power to organize parish schooling through the Vestry School Act of 1824, the Catholic clergy did nothing effective.

Despite this, the invention of the printing press had laid the foundation for the modern era and universal social literacy, and so it is that with time, "technologically, literacy had passed from the hands of an elite to the populace at large. Historical factors and sociopolitical conditions, however, have determined the extent to which universal social literacy has come to pass".

In 1871 only about half of French Canadian men in Canada self-reported that they were literate, whereas 90 percent of other Canadian men said they could read and write, but information from the Canadian Families Project sample of the 1901 Census of Canada indicated that literacy rates for French Canadians and other Canadians increased, as measured by the ability of men between the ages of 16 and 65 to answer literacy questions. Compulsory attendance in schools was legislated in the late 19th century in all provinces but Quebec, but by then, a change in parental attitudes towards educating the new generation meant that many children were already attending regularly. Unlike the emphasis of school promoters on character formation, the shaping of values, the inculcation of political and social attitudes, and proper behaviour, many parents supported schooling because they wanted their children to learn to read, write, and do arithmetic. Efforts were made to exert power and religious, moral, economic/professional, and social/cultural influence over children who were learning to read by dictating the contents of their school readers accordingly. But educators broke from these spheres of influence and also taught literature from a more child-centred perspective: for the pleasure of it.

Educational change in Québec began as a result of a major commission of inquiry at the start of what came to be called the "Quiet Revolution" in the early 1960s. In response to the resulting recommendations, the Québec government revamped the school system in an attempt to enhance the francophone population's general educational level and to produce a better-qualified labour force. Catholic Church leadership was rejected in favour of government administration and vastly increased budgets were given to school boards across the province.

With time, and with continuing inquiry into the literacy achievement levels of Canadians, the definition of literacy moved from a dichotomous one (either a person could, or couldn't write his or her name, or was literate or illiterate), to ones that considered its multidimensionality, along with the qualitative and quantitative aspects of literacy. In the 1970s, organizations like the Canadian Association for Adult Education (CAAE) believed that one had to complete the 8th grade to achieve functional literacy. Examination of 1976 census data, for example, found that 4,376,655, or 28.4% of Canadians 15 years of age and over reported a level of schooling of less than grade 9 and were thus deemed not functionally literate. But in 1991, UNESCO formally acknowledged Canada's findings that assessment of educational attainment as proxy measure of literacy was not as reliable as was direct assessment. This dissatisfaction manifested itself in the development of actual proficiency tests that measure reading literacy more directly.

Canada conducted its first literacy survey in 1987 which discovered that there were more than five million functionally illiterate adults in Canada, or 24 per cent of the adult population. Statistics Canada then conducted three national and international literacy surveys of the adult population — the first one in 1989 commissioned by the Human Resources and Skills Development Canada (HRSDC) department.

This first survey was called the "Literacy Skills Used in Daily Activities" (LSUDA) survey, and was modeled on the 1985 U.S. survey of young adults (YALS). It represented a first attempt in Canada to produce skill measures deemed comparable across languages. Literacy, for the first time, was measured on a continuum of skills. The survey found that 16% of Canadians had literacy skills too limited to deal with most of the printed material encountered in daily life whereas 22% were considered "narrow" readers.

In 1994-95, Canada participated in the first multi-country, multi-language assessment of adult literacy, the International Adult Literacy Survey (IALS). A stratified multi-stage probability sample design was used to select the sample from the Census Frame. The sample was designed to yield separate samples for the two Canadian official languages, English and French, and participants were measured on the dimensions of prose literacy, document literacy and quantitative literacy. The survey found that 42.2%, 43% and 42.2% of Canadians between the ages of 16 and 65 scored at the lowest two levels of Prose Literacy, Document Literacy and Quantitative Literacy, respectively. The survey presented many important correlations, among which was a strong plausible link between literacy and a country's economic potential.

In 2003, Canada participated in the Adult Literacy and Life Skills Survey (ALL). This survey contained identical measures for assessing the prose and document literacy proficiencies, allowing for comparisons between survey results on these two measures and found that 41.9% and 42.6% of Canadians between the ages of 16 and 65 scored at the lowest two levels of Prose Literacy and document literacy respectively. Further, Canadians’ mean scores also improved on both the prose and the document literacy scales. Energy production:36%, transportation: 24%, homes and businesses: 12%, industry: 11%, agriculture: 10%, and waste: 7%.

The OECD's Programme for the International Assessment of Adult Competencies (PIAAC) is expected to produce new comparative skill profiles in late 2013.

In the last 40 years, the rate of illiteracy in Mexico has been steadily decreasing. In the 1960s, because the majority of the residents of the federal capital were illiterate, the planners of the Mexico City Metro designed a system of unique icons to identify each station in the system in addition to its formal name. However, The INEGI's census data of 1970 showed a national average illiteracy rate of 25.8%; the last census data puts the national average at 6.9%. Mexico still has a gender educational bias. The illiteracy rate for women in the last census was 8.1% compared with 5.6% for men. Rates differ across regions and states. Chiapas, Guerrero and Oaxaca, the states with the highest poverty rate, had greater than 15% illiteracy in 2010(17.8%, 16.7% and 16.3 respectively). In contrast, the illiteracy rates in the Federal District (D.F. / Mexico City) and in some northern states like Nuevo León, Baja California, and Coahuila were below 3% in the 2010 census (2.1%, 2.2%, 2.6% and 2.6% respectively).

Before the 20th century white illiteracy was not uncommon and many of the slave states made it illegal to teach slaves to read. By 1900 the situation had improved somewhat, but 44% of black people remained illiterate. There were significant improvements for African American and other races in the early 20th century as the descendants of former slaves, who had had no educational opportunities, grew up in the post Civil War period and often had some chance to obtain a basic education. The gap in illiteracy between white and black adults continued to narrow through the 20th century, and in 1979 the rates were about the same.

Full prose proficiency, as measured by the ability to process complex and challenging material such as would be encountered in everyday life, is achieved by about 13% of the general, 17% of the white, and 2% of the African American population. However 86% of the general population had basic or higher prose proficiency as of 2003, with a decrease distributed across all groups in the full proficiency group vs. 1992 of more than 10% consistent with trends, observed results in the SAT reading score to the present (2015).

Before colonization, oral storytelling and communication composed most if not all Native American literacy. Native people communicated and retained their histories verbally—it was not until the beginning of American Indian boarding schools that reading and writing forms of literacy were forced onto Native Americans. While literacy rates of English increased, forced assimilation exposed Native children to physical and sexual abuse, unsanitary living conditions, and even death. Many students ran away in an attempt to hold on to their cultural identity and literary traditions that were relevant to their community. While these formalized forms of literacy prepared Native youth to exist in the changing society, they destroyed all traces of their cultural literacy. Native children would return to their families unable to communicate with them due to the loss of their indigenous language. In the 20th and 21st century, there is still a struggle to learn and maintain cultural language. But education initiatives and programs have increased overall—according to the 2010 census, 86 percent of the overall population of Native Americans and Alaska Natives have high school diplomas, and 28 percent have a bachelor's degree or higher.

In 1964 in Brazil, Paulo Freire was arrested and exiled for teaching peasants to read. Since democracy returned to Brazil, however, there has been a steady increase in the percentage of literate people. Educators with the Axé project within the city of Salvador, Bahía attempt to improve literacy rates among urban youth, especially youth living on the streets, through the use of music and dances of the local culture. They are encouraged to continue their education and become professionals.

The literacy rates in Africa vary significantly between countries. The registered literacy rate in Libya was 86.1% in 2004 and Unesco says that literacy rate in the region of Equatorial Guinea is approximately 95% while the literacy rate is in South Sudan is approximately (27%). Poorer youth in sub-Saharan Africa have fewer educational opportunities to become literate compared with wealthier families. They often must leave school because of being needed at home to farm or care for siblings.

In sub-Saharan Africa, the rate of literacy has not improved enough to compensate for the effects of demographic growth. As a result, the number of illiterate adults has risen by 27% over the last 20 years, reaching 169 million in 2010. Thus, out of the 775 million illiterate adults in the world in 2010, more than one fifth were in sub- Saharan Africa – in other words, 20% of the adult population. The countries with the lowest levels of literacy in the world are also concentrated in this region. These include Niger (28.7%), Burkina Faso (28.7%), Mali (33.4%), Chad (35.4%) and Ethiopia (39%), where adult literacy rates are well below 50%. There are, however, certain exceptions, like Equatorial Guinea, with a literacy rate of 94%.

The literacy rate of Algeria is around 70%: education is compulsory and free in Algeria up to age of 17.

Botswana has among the highest literacy rates in the developing world with around 85% of its population being literate.

Burkina Faso has a very low literacy rate of 28.7%. The government defines literacy as anyone at least 15 years of age and up who can read and write. To improve the literacy rate, the government has received at least 80 volunteer teachers. A severe lack of primary school teachers causes problems for any attempt to improve the literacy rate and school enrollment.

Djibouti has an estimated literacy rate of 70%.

Egypt has a relatively high literacy rate. The adult literacy rate in 2010 was estimated at 72%.
Education is compulsory from ages 6 to 15 and free for all children to attend. 93% of children enter primary school today, compared with 87% in 1994.

According to the Ministry of Information of Eritrea, the nation has an estimated literacy rate of 80%.

The Ethiopians are among the first literate people in the world, having written, read, and created manuscripts in their ancient language of Ge'ez (Amharic) since the second century CE. All boys learned to read the Psalms around the age of 7. National literacy campaign introduced in 1978 increased literacy rates to between 37% (unofficial) and 63% (official) by 1984.

Guinea has a literacy rate of 41%. The Guinea government defines literacy as anyone who can read or write who is at least 15 years old. Guinea was the first to use the Literacy, Conflict Resolution, and Peacebuilding project. This project was developed to increase agriculture production, develop key skills, resolve conflict, improve literacy, and numeracy skills. The LCRP worked within refugee camps near the border of Sierra Leone, however this project only lasted from 1999 to 2001. There are several other international projects working within the country that have similar goals.

The literacy rate in Kenya among people below 20 years of age is over 70%, as the first 8 years of primary school are provided tuition-free by the government. In January 2008, the government began offering a restricted program of free secondary education. Literacy is much higher among the young than the old population, with the total being about 53% for the country. Most of this literacy, however, is elementary—not secondary or advanced.

Mali has one of the lowest literacy rates in the world, at 33.4%, with males having a 43.1% literacy rate and females having a 24.6% literacy rate. In 2015, the adult literacy rate was 33%. The government defines literacy as anyone who is at least 15 and over who can read or write. The government of Mali and international organizations in recent years has taken steps to improve the literacy rate. The government recognized the slow progress in literacy rates and began created ministries for basic education and literacy for their national languages in 2007. To also improve literacy the government planned to increase its education budget by 3%, when this was purposed it was at 35% in 2007. The lack of literate adults causes the programs to be slowed. The programs need qualified female trainers is a major problem because most men refuse to send female family members to be trained under male teachers.

Free education in Mauritius didn't proceed beyond the primary level until 1976, so many women now in their 50s or older left school at age 12. The younger generation (below 50) are however extremely well educated with very high educational expectations placed upon pupils. Education is today free from pre-primary to tertiary (only admission fees remain at University level). Most professional people have at least a bachelor's degree. Mauritian students consistently rank top in the world each year for the Cambridge International O Level, International A and AS level examinations. Most Mauritian children, even at primary level, attend tuition after school and at weekends to cope with the highly competitive public school system where admission to prestigious public colleges (secondary) and most sought after university courses depend on merit based academic performance.

The adult literacy rate was estimated at 89.8% in 2011. Male literacy was 92.3% and Female literacy 87.3%.

Niger has an extremely low literacy rate at 28.7%. However, the gender gap between males and females is a major problem for the country, men have a literacy rate of 42.9% and women a literacy rate of 15.1%. The Nigerien government defines literacy as anyone who can read or write over the age of 15. The Niass Tijaniyya, a predominant group of the Sufi brotherhoods, has started anti-poverty, empowerment, and literacy campaigns. The women in Kiota had not attempted to improve their education, or economic standing. Saida Oumul Khadiri Niass, known as Maman, through talking to men and women throughout the community changed the community's beliefs on appropriate behavior for women because the community recognized she was married to a leader of the Niass Tijaniyya. Maman's efforts has allowed women in Kiota to own small businesses, sell in the market place, attend literacy classes, and organize small associations that can give micro loans. Maman personally teaches children in and around Kiota, with special attention to girls. Maman has her students require instructor permission to allow the girls' parents to marry their daughters early. This increases the amount of education these girls receive, as well as delaying marriage, pregnancy, and having children.

Senegal has a literacy rate of 49.7%; the government defines literacy as anyone who is at least 15 years of age and over who can read and write. However, many students do not attend school long enough to be considered literate. The government did not begin actively attempting to improve the literacy rate until 1971 when it gave the responsibility to Department for Vocational Training at the Secretariat for Youth and Sports. This department and subsequent following departments had no clear policy on literacy until the Department of Literacy and Basic Education was formed in 1986. The government of Senegal relies heavily on funding from the World Bank to fund its school system.

There is no reliable data on the nationwide literacy rate in Somalia. A 2013 FSNAU survey indicates considerable differences per region, with the autonomous northeastern Puntland region having the highest registered literacy rate at 72%.

The Sierra Leone government defines literacy as anyone over the age of 15 who can read and write in English, Mende, Temne, or Arabic. Official statics put the literacy rate at 43.3%. Sierra Leone was the second country to use the Literacy, Conflict Resolution and Peacebuilding project. However, due to fighting near the city where the project was centered causing the project to be delayed until an arms amnesty was in place.

Uganda has a literacy rate of 72.2%.

Zimbabwe has a high literacy rate of 86.5% (2016 est.).

<nowiki> </nowiki>Afghanistan has one of the lowest literacy rates in the world at 28.1% with males having a literacy rate of 43.1% and females with a literacy rate of 12.6%. The Afghan government considers someone literate if they are 15 years of age or older, and if they can read and write. To improve the literacy rate U.S. military trainers have been teaching Afghan Army recruits how to read before teaching to fire a weapon. U.S. commanders in the region estimate that as many as 65% of recruits may be illiterate.

The PRC conducts standardized testing to assess proficiency in Standard Chinese, known as "putonghua," but it is primarily for foreigners or those needing to demonstrate professional proficiency in the Beijing dialect. Literacy in languages like Chinese can be assessed by reading comprehension tests, just as in other languages, but historically has often been graded on the number of Chinese characters introduced during the speaker's schooling, with a few thousand considered the minimum for practical literacy. Social science surveys in China have repeatedly found that just more than half the population of China is conversant in spoken putonghua.

Literacy is defined by the Registrar General and Census Commissioner of India, as "[the ability of] a person aged 7 years and above [to]... both write and read with understanding in any language." According to the 2011 census, 74.04 percent.

Laos has the lowest level of adult literacy in all of Southeast Asia other than East Timor.

Obstacles to literacy vary by country and culture as writing systems, quality of education, availability of written material, competition from other sources (television, video games, cell phones, and family work obligations), and culture all influence literacy levels. In Laos, which has a phonetic alphabet, reading is relatively easy to learn—especially compared to English, where spelling and pronunciation rules are filled with exceptions, and Chinese, with thousands of symbols to be memorized. But a lack of books and other written materials has hindered functional literacy in Laos, where many children and adults read so haltingly that the skill is hardly beneficial.

A literacy project in Laos addresses this by using what it calls "books that make literacy fun!" The project, Big Brother Mouse, publishes colorful, easy-to-read books, then delivers them by holding book parties at rural schools. Some of the books are modeled on successful western books by authors such as Dr. Seuss; the most popular, however, are traditional Lao fairy tales. Two popular collections of folktales were written by Siphone Vouthisakdee, who comes from a village where only five children finished primary school.

Big Brother Mouse has also created village reading rooms, and published books for adult readers about subjects such as Buddhism, health, and baby care.

In Pakistan, the National Commission for Human Development (NCHD) aims to bring literacy to adults, especially women.
ISLAMABAD - UNESCO Islamabad Director Kozue Kay Nagata has said, "Illiteracy in Pakistan has fallen over two decades, thanks to the government and people of Pakistan for their efforts working toward meeting the Millennium Development Goals". "Today, 70 percent of Pakistani youths can read and write. In 20 years, illiterate population has been reduced significantly", she said while speaking at a function held in connection with International Literacy Day.

However, she also emphasised on the need to do more to improve literacy in the country and said, "The proportion of population in Pakistan lacking basic reading and writing is too high. This is a serious obstacle for individual fulfillment, to the development of societies, and to mutual understanding between peoples." Referring to the recent national survey carried out by the Ministry of Education, Trainings and Standards in Higher Education with support of UNESCO, UNICEF, and provincial and areas departments of education, Nagata pointed out that, in Pakistan, although primary school survival rate is 70 percent, gender gap still exists with only 68 percent of girls’ survival rate compared to 71 percent for boys. Specifically in the case of Punjab, she said, primary school survival rate today is better with 76 percent, but not without a gender gap of 8 percent points with 72 percent girls’ survival rate compared to 80 percent for boys. She also pointed out that average per student spending in primary level (age 5-9) was better in Punjab: Rs 6,998, compared to the national average. In Balochistan, although almost the same amount (Rs 6,985) as in Punjab is spent per child, the primary school survival rate is only 53 percent. Girls’ survival rate is slightly better with 54 percent than that of boys which is 52 percent. Literate Pakistan Foundation, a non-profit organization, which was established in 2003, is a case study, which brings to light the solutions for removing this menace from its roots. It works to improve rate of literacy in Pakistan.

The data of the survey shows that in Khyber Pakhtunkhwa, primary school survival rate is 67 percent which is lower than the national average of 70 percent. Furthermore, gender gap also exists with only 65 percent of girls’ survival rate compared to that of boys which is 68 percent. Per-student education expenditure in primary level (age 5-9) in Khyber Pakhtunkhwa is Rs 8,638. In Sindh, primary school survival rate is 63percent, with a gender gap of only 67 percent of girls’ survival rate compared to 60 percent for boys. Per student education expenditure in primary level (age 5-9) in Sindh is Rs 5,019. Nagata made reference to the survey report and mentioned that the most common reason in Pakistan for children (both boys and girls) of age 10 to 18 years leaving school before completing primary grade is "the child not willing to go to school", which may be related to quality and learning outcome. She said, however, and sadly, for the girls living in rural communities the second highest reason for dropout is "parents did not allow" which might be related to prejudice and cultural norm against girls.

Early Filipinos devised and used their own system of writings from 300 BC, which derived from the Brahmic family of scripts of Ancient India. Baybayin became the most widespread of these derived scripts by the 11th century.

Early chroniclers, who came during the first Spanish expeditions to the islands, noted the proficiency of some of the natives, especially the chieftain and local kings, in Sanskrit, Old Javanese, Old Malay, and several other languages. During the Spanish colonization of the islands, reading materials were destroyed to a far much less extent compared to the Spanish colonization of the Americas. Education and literacy was introduced only to the Peninsulares and remained a privilege until the Americans came. The Americans introduced the public schools system to the country which drove literacy rates up. English became the lingua franca in the Philippines. It was only during a brief period in the Japanese occupation of the Philippines that the Japanese were able to teach their language in the Philippines and teach the children their written language.

After World War II, the Philippines had the highest literacy rates in Asia. It nearly achieved universal literacy once again in the 1980s and 1990s. Ever since then, the literacy rate has plummeted only to start regaining a few percentage years back. The DepEd, CHED, and other academic institutions encourage children to improve literacy skills and knowledge. The government has a program of literacy teaching starting in kindergarten. New reforms are being brought in shifting to a K-12 system which will teach children their regional languages before English, as opposed to the ten-year basic education program which teaches English and Filipino, the country's two official languages, from Grade 1.

With a literacy rate of 92.5%, Sri Lanka has one of the most literate populations amongst developing nations. Its youth literacy rate stands at 98%, computer literacy rate at 35%, and primary school enrollment rate at over 99%. An education system which dictates 9 years of compulsory schooling for every child is in place. The free education system established in 1945, is a result of the initiative of C. W. W. Kannangara and A. Ratnayake. It is one of the few countries in the world that provide universal free education from primary to tertiary stage.

Approximately 56% of Australians aged 15 to 74 achieve Level 3 literacy or above Australian Bureau of Statistics 2011–2012 and 83% of five-year-olds are on track to develop good language and cognitive skills Australian Early Development Census 2012 summary report. In 2012–2013, Australia had 1515 public library service points, lending almost 174 million items to 10 million members of Australian public library services, at an average per capita cost of just under AU$45 Australian Public Library Statistics 2012–2013.





</doc>
<doc id="18457" url="https://en.wikipedia.org/wiki?curid=18457" title="Local-loop unbundling">
Local-loop unbundling

Local loop unbundling (LLU or LLUB) is the regulatory process of allowing multiple telecommunications operators to use connections from the telephone exchange to the customer's premises. The physical wire connection between the local exchange and the customer is known as a "local loop", and is owned by the incumbent local exchange carrier (also referred to as the "ILEC", "local exchange", or in the United States either a "Baby Bell" or an independent telephone company). To increase competition, other providers are granted unbundled access.

LLU is generally opposed by the ILECs, which in most cases are either former investor-owned (North America) or state-owned monopoly enterprises forced to open themselves to competition. ILECs argue that LLU amounts to a regulatory taking, that they are forced to provide competitors with essential business inputs, that LLU stifles infrastructure-based competition and technical innovation because new entrants prefer to 'parasitise' the incumbent's network instead of building their own and that the regulatory interference required to make LLU work (e.g., to set the LLU access price) is detrimental to the market.

New entrants, on the other hand, argue that since they cannot economically duplicate the incumbent's local loop, they cannot actually provide certain services, such as ADSL without LLU, thus allowing the incumbent to monopolise the respective potentially competitive market(s) and stifle innovation. They point out that alternative access technologies, such as wireless local loop, have proven uncompetitive and/or impractical, and that under current pricing models, the incumbent is in many cases, depending on the regulatory model, guaranteed a fair price for the use of its facilities, including an appropriate return on investment. Finally, they argue that the ILECs generally did not construct their local loop in a competitive, risky, market environment, but under legal monopoly protection and using taxpayer's money, which means, according to the new entrants, that ILECs ought not to be entitled to continue to extract regulated rates of return, which often include monopoly rents from the local loop.

Most industrially developed nations, including the US, Australia and the European Union Member States, and India have introduced regulatory frameworks providing for LLU. Given the above-mentioned problems, regulators face the challenging task of regulating a market that is changing very rapidly, without stifling any type of innovation, and without improperly disadvantaging any competitor.

The process has been long - the first action in the EU resulted from a report written for the European Commission in 1993. It took several years for the EU legislation to require unbundling and then in individual EU countries the process took further time to mature to become practical and economic rather than simply being a legal possibility.

In 1996 the United States Telecommunication Act (in section 251) defined the unbundled access as: 
The 1993 report referred to the logical requirement to unbundle optical fibre access but recommended deferral to a later date when fibre access had become more common. In 2006 there were the first signs that (as a result of the municipal fibre networks movement and example such as Sweden where unbundled local loop fibre is commercially available from both the incumbent and competitors) policy may yet evolve in this direction.

Some provisions of WTO telecommunications law can be read to require unbundling:

The question has not been settled before a WTO judicial body, and, at any rate, these obligations only apply where the respective WTO Member has committed itself to open its basic telecommunications market to competition. About 80 (mostly developed) Members have done so since 1998.

LLU has not been implemented in Indian cities yet. However, BSNL recently stated that it will open up its copper loops for private participation. In addition to this, the proliferation of WiMax and cable broadband has increased broadband penetration and market competition. By 2008 the price war had reduced basic broadband prices to INR 250 (US$6), including line rental without any long-term contracts. In rural areas, the state player, BSNL, is still the leading, and often the only supplier.
Although BSNL is a monopoly, it is used as a tool to ensure competition by the government.

The implementation of local loop unbundling is a requirement of European Union policy on competition in the telecommunications sector and has been introduced, at various stages of development, in all member states (Operators with Significant Market Power shall publish (from 31 December 2000, and keep updated) a postreference offer for unbundled access to their local loops and related facilities. The offer shall be sufficiently unbundled so that the beneficiary does not have to pay for network elements or facilities which are not necessary for the supply of its services, and shall contain a description of the components of the offer, associated terms and conditions, including charges).

European States that have been approved for membership to the EU have an obligation to introduce LLU as part of the liberalisation of their communications sector.

By 14 January 2006, 210,000 local loop connections had been unbundled from BT operation under local loop unbundling. Ofcom had hoped that 1 million local loop connections would be unbundled by June 2006. However, as reported by The Register, on 15 June 2006, the figure had reached only 500,000, but was growing by 20,000 a week. Ofcom announced in November 2006 that 1,000,000 connections had been unbundled. By April 2007, the figure was 2,000,000.

By June 2006, AOL UK had unbundled 100,000 lines through its £120 million investment, making it the largest single LLU operator in the UK market.

On 10 October 2006, Carphone Warehouse announced the purchase of AOL UK, the leading LLU operator, for £370m.
This made Carphone Warehouse the third largest broadband provider and the largest LLU operator with more than 150,000 LLU customers.

On 8 May 2009, TalkTalk, who were owned by Carphone Warehouse, announced that they would purchase ailing Tiscali UK's assets for £235 million. On 30 June 2009, Tiscali sold its UK subsidiary to Carphone Warehouse following regulatory approval from the European Union. This purchase made TalkTalk the biggest home broadband supplier in the UK, with 4.25 million home broadband subscribers, compared with BT's 3.9 million. The service was rebranded as TalkTalk in January 2010.

Most LLU operators only unbundle the broadband service leaving the traditional telephone service using BT's core equipment (with or without the provision of carrier preselect). Where the traditional telephone service is also unbundled (full LLU), operators usually prohibit the facility where selected calls can be made using the networks of other telephone providers (i.e. accessed using a three- to five-digit prefix beginning with '1'). These calls can usually still be made by using an 0800 or other non-geographic (NGN) access code.

Although regulators in the UK admitted that the market could provide competitive offerings in due time, the purpose of mandatory local loop unbundling in the United Kingdom was to speed the delivery of advanced services to consumers.

Pursuant to the Telecommunications Act of 1996, the Federal Communications Commission (FCC) requires that ILECs lease local loops to competitors (CLECs). Prices are set through a market mechanism.

The Commerce Commission recommended against local loop unbundling in late 2003 as Telecom New Zealand (now Spark New Zealand) offered a market-led solution. In May 2004 this was confirmed by the Government, despite the intense "call4change" campaign by some of Telecom's competitors. Part of Telecom's commitment to the Commerce Commission to avoid unbundling was a promise to deliver 250,000 new residential broadband connections by the end of 2005, one-third of which were to be wholesaled through other providers. Telecom failed to achieve the number of wholesale connections required, despite an attempt by management to claim that the agreement had been for only one-third of the growth rather than one-third of the total. That claim was rejected by the Commerce Commission, and the publicised figure of 83,333 wholesale connections out of 250,000 was held to be the true target. The achieved number was less than 50,000 wholesale connections, despite total connections exceeding 300,000.

On 3 May 2006 the Government announced it would require the unbundling of the local loop. This was in response to concerns about the low levels of broadband uptake. Regulatory action such as information disclosure, the separate accounting of Telecom New Zealand business operations, and enhanced Commerce Commission monitoring was announced.

On 9 August 2007 Telecom released the keys to exchanges in Glenfield and Ponsonby in Auckland. In March 2008 Telecom activated ADSL 2+ services from five Auckland exchanges – Glenfield, Browns Bay, Ellerslie, Mt Albert and Ponsonby – with further plans for the rest of Auckland and other major centres, allowing other ISPs to take advantage.

Switzerland is one of the last OECD nations to provide for unbundling, because the Swiss Federal Supreme Court held in 2001 that the 1996 Swiss Telecommunications Act did not require it. The government then enacted an ordinance providing for unbundling in 2003, and Parliament amended the act in 2006. While infrastructure-based access is now generally available, unbundled fast bitstream access is limited to a period of four years after the entry into force of the act.

Unbundling requests tend to be tied up before the courts, however, because unlike in the EU, Swiss law does not provide for an "ex ante" regulation of access conditions by the regulator. Instead, under the Swiss "ex post" regulation system, each new entrant must first try to reach an individual agreement with Swisscom, the state-owned ILEC.

Mandatory local loop unbundling policy (termed Type II Interconnection (Traditional Chinese:第二類互連) in Hong Kong) started on July 1, 1995 (the same day of telephone market liberalisation), to ensure choice to customers. After 10 years, new operators have built their networks covering a large region of Hong Kong; the government considered it a good time to withdraw mandatory local loop unbundling policy, to persuade operators to build their own networks and let businesses run themselves with a minimum of government intervention. At the meeting of the Executive Council on 6 July 2004, the government decided that the regulatory intervention under the current Type II interconnection policy applicable to telephone exchanges for individual buildings covered by such exchanges should be withdrawn, subject to conditions documented in this Statement of the Telecommunications Authority. After that, the terms of interconnection will be negotiated between telephone operators. Hong Kong is the only advanced economy that has withdrawn the mandatory local loop unbundling policy.

On 25 May 2006 the Minister of Communications of South Africa Dr Ivy Matsepe-Casaburri established the Local Loop Unbundling Committee chaired by Professor Tshilidzi Marwala to recommend the appropriate local loop unbundling models. The Local Loop Unbundling Committee submitted a report to Minister Matsepe-Casaburri on 25 May 2007. This report recommends that models that permit customers to access both voice and data be offered by many different companies. The models recommended are Full Unbundling, Line Sharing and Bitstream Access. It is recommended that customers should exercise carrier pre-selection and thus be able to switch between service providers. It is also recommended that an organisation be created to manage the local loop and that this organisation should be under the guidance of the regulator Icasa and that Icasa be capacitated in terms of resources. The committee recommended that service providers approved by Icasa should have access to the telephone exchange infrastructure whenever necessary. The committee recommended that a regulatory guideline be established and be managed by Icasa to guarantee that strategic issues like quality of the local loop be optimised for regulation and delivery of services. Based on this report the Minister has issued policy directives to Icasa to move swiftly with the unbundling process. At the end of March 2010 nothing has happened yet, however a deadline of November 1, 2011 was set by the Minister of Communications for monopoly holder, Telkom SA to finalise the unbundling process.





</doc>
<doc id="18459" url="https://en.wikipedia.org/wiki?curid=18459" title="Leda">
Leda

Leda and similar may refer to:









</doc>
<doc id="18460" url="https://en.wikipedia.org/wiki?curid=18460" title="Lysithea (moon)">
Lysithea (moon)

Lysithea is a prograde irregular satellite of Jupiter. It was discovered by Seth Barnes Nicholson in 1938 at Mount Wilson Observatory and is named after the mythological Lysithea, daughter of Oceanus and one of Zeus' lovers.

Lysithea did not receive its present name until 1975; before then, it was simply known as . It was sometimes called "Demeter" from 1955 to 1975.

It belongs to the Himalia group, five moons orbiting between 11 and 13 Gm from Jupiter at an inclination of about 28.3°. Its orbital elements are as of January 2000. They are continuously changing due to solar and planetary perturbations.




</doc>
<doc id="18461" url="https://en.wikipedia.org/wiki?curid=18461" title="Leda and the Swan">
Leda and the Swan

Leda and the Swan is a story and subject in art from Greek mythology in which the god Zeus, in the form of a swan, seduces (or in some versions, rapes) Leda. According to later Greek mythology, Leda bore Helen and Polydeuces, children of Zeus, while at the same time bearing Castor and Clytemnestra, children of her husband Tyndareus, the King of Sparta. In the W. B. Yeats version, it is subtly suggested that Clytemnestra, although being the daughter of Tyndareus, has somehow been traumatized by what the swan has done to her mother (see below). According to many versions of the story, Zeus took the form of a swan and seduced Leda on the same night she slept with her husband King Tyndareus. In some versions, she laid two eggs from which the children hatched. In other versions, Helen is a daughter of Nemesis, the goddess who personified the disaster that awaited those suffering from the pride of Hubris.

The subject was rarely seen in the large-scale sculpture of antiquity, although a representation of Leda in sculpture has been attributed in modern times to Timotheus ("compare illustration, below left"); small-scale sculptures survive showing both reclining and standing poses, in cameos and engraved gems, rings, and terracotta oil lamps. Thanks to the literary renditions of Ovid and Fulgentius it was a well-known myth through the Middle Ages, but emerged more prominently as a classicizing theme, with erotic overtones, in the Italian Renaissance.

The subject undoubtedly owed its sixteenth-century popularity to the paradox that it was considered more acceptable to depict a woman in the act of copulation with a swan than with a man. The earliest depictions show the pair love-making with some explicitness—more so than in any depictions of a human pair made by artists of high quality in the same period.
The fate of the erotic album "I Modi" some years later shows why this was so. The theme remained a dangerous one in the Renaissance, as the fates of the three best known paintings on the subject demonstrate. The earliest depictions were all in the more private medium of the old master print, and mostly from Venice. They were often based on the extremely brief account in the "Metamorphoses" of Ovid (who does not imply a rape), though Lorenzo de' Medici had both a Roman sarcophagus and an antique carved gem of the subject, both with reclining Ledas.

The earliest known explicit Renaissance depiction is one of the many woodcut illustrations to "Hypnerotomachia Poliphili", a book published in Venice in 1499. This shows Leda and the Swan making love with gusto, despite being on top of a triumphal car, being pulled along and surrounded by a considerable crowd. An engraving dating to 1503 at the latest, by Giovanni Battista Palumba, also shows the couple in coitus, but in deserted countryside. Another engraving, certainly from Venice and attributed by many to Giulio Campagnola, shows a love-making scene, but there Leda's attitude is highly ambiguous. Palumba made another engraving, perhaps in about 1512, presumably influenced by Leonardo's sketches for his earlier composition, showing Leda seated on the ground and playing with her children.

There were also significant depictions in the smaller decorative arts, also private media. Benvenuto Cellini made a medallion, now in Vienna, early in his career, and Antonio Abondio one on the obverse of a medal celebrating a Roman courtesan.

Leonardo da Vinci began making studies in 1504 for a painting, apparently never executed, of Leda seated on the ground with her children. In 1508 he painted a different composition of the subject, with a nude standing Leda cuddling the Swan, with the two sets of infant twins (also nude), and their huge broken egg-shells. The original of this is lost, probably deliberately destroyed, and was last recorded in the French royal Château de Fontainebleau in 1625 by Cassiano dal Pozzo. However it is known from many copies, of which the earliest are probably the "Spiridon Leda", perhaps by a studio assistant and now in the Uffizi, and the one at Wilton House in the United Kingdom (illustrated).

Also lost, and probably deliberately destroyed, is Michelangelo's tempera painting of the pair making love, commissioned in 1529 by Alfonso d'Este for his palazzo in Ferrara, and taken to France for the royal collection in 1532; it was at Fontainebleau in 1536. Michelangelo's cartoon for the work—given to his assistant Antonio Mini, who used it for several copies for French patrons before his death in 1533—survived for over a century. This composition is known from many copies, including an by Cornelis Bos, c. 1563; the marble sculpture by Bartolomeo Ammanati in the Bargello, Florence; two copies by the young Rubens on his Italian voyage, and the painting after Michelangelo, ca. 1530, in the National Gallery, London. The Michelangelo composition, of about 1530, shows Mannerist tendencies of elongation and twisted pose (the "figura serpentinata") that were popular at the time. In addition, a sculptural group, similar to the Prado Roman group illustrated, was believed until at least the 19th century to be by Michelangelo.

The last very famous Renaissance painting of the subject is Correggio's elaborate composition of c. 1530 (Berlin); this too was damaged whilst in the collection of Philippe II, Duke of Orléans, the Regent of France in the minority of Louis XV. His son Louis, though a great lover of painting, had periodic crises of conscience about his way of life, in one of which he attacked the figure of Leda with a knife. The damage has been repaired, though full restoration to the original condition was not possible. Both the Leonardo and Michelangelo paintings also disappeared when in the collection of the French Royal Family, and are believed to have been destroyed by more moralistic widows or successors of their owners.

There were many other depictions in the Renaissance, including cycles of book illustrations to Ovid, but most were derivative of the compositions mentioned above. The subject remained largely confined to Italy, and sometimes France – Northern versions are rare. After something of a hiatus in the 18th and early 19th centuries (apart from a very sensuous Boucher,), Leda and the Swan became again a popular motif in the later 19th and 20th centuries, with many Symbolist and Expressionist treatments.

Also from that era were sculptures of the theme by Antonin Mercié and Max Klinger.

Cy Twombly executed an abstract version of "Leda and the Swan" in 1962. It was purchased by Larry Gagosian for $52.9 million at Christie's May 2017 Post-War and Contemporary Art Evening Sale.

Avant-garde filmmaker Kurt Kren along with other members of the Viennese Actionist movement, including Otto Muehl and Hermann Nitsch, made a film-performance called "7/64 Leda mit der Schwan" in 1964. The film retains the classical motif, portraying, for most of its duration, a young woman embracing a swan.

There is a life-sized marble statue of "Leda and the Swan" at the Jai Vilas Palace Museum in Gwalior, Northern Madhya Pradesh, India.

American artist and photographer Carole Harmel created the "Bird" series (1983), a Jean Cocteau-influenced collection of photographs that explored the "Leda and the Swan" myth in tightly cropped, voyeuristic images of a nude female and an undefinable birdlike creature hinting at intimacy. 

Bristol Museum and Art Gallery currently exhibits Karl Weschke's "Leda and the Swan", painted in 1986. The Winnipeg Art Gallery in Canada has, in its permanent collection, a ceramic "Leda and the Swan" by Japanese-born American artist Akio Takamori. Genieve Figgis painted her version of Leda and the Swan in 2018 after an earlier work by François Boucher. Figgis’ contemporary version reinvents the idyllic romantic scene of lavish playfulness with a dark humor creating a scene of profanity and horror. There is a sculpture in neon lights depicting Leda and the Swan in Berlin, near Sonnenallee metro station and the Estrel hotel, designed by AES+F. Photographer Charlie White included a portrait of Leda in his "And Jeopardize the Integrity of the Hull" series. Zeus, as the swan, only appears metaphorically.

Ronsard wrote a poem on "La Défloration de Lède", perhaps inspired by the Michelangelo, which he may well have known. Like many artists, he imagines the beak penetrating Leda's vagina.

"Leda and the Swan" is a sonnet by William Butler Yeats first published in the "Dial" in 1923. Combining psychological realism with a mystic vision, it describes the swan's rape of Leda. It also alludes to the Trojan war, which will be provoked by the abduction of Helen, who will be begotten by Zeus on Leda (along with Castor and Pollux, in some versions of the myth). Clytaemnestra, who killed her husband, Agamemnon, leader of the Greeks at Troy, was also supposed to have hatched from one of Leda's eggs. The poem is regularly praised as one of Yeats's masterpieces. Camille Paglia, who called the poem "the greatest poem of the twentieth century," and said "all human beings, like Leda, are caught up moment by moment in the 'white rush' of experience. For Yeats, the only salvation is the shapeliness and stillness of art." See external links for a bas relief arranged in the position as described by Yeats.

Nicaraguan poet Rubén Darío's 1892 poem "Leda" contains an oblique description of the rape, watched over by the god Pan.

H.D. (Hilda Doolittle) also wrote a poem called "Leda" in 1919, suggested to be from the perspective of Leda. The description of the sexual action going on makes it seem almost beautiful, as if Leda had given her consent.

In the song "Power and Glory" from Lou Reed's 1992 album "Magic and Loss", Reed recalls the experience of seeing his friend dying of cancer and makes reference to the myth, "I saw isotopes introduced into his lungs / trying to stop the cancerous spread / And it made me think of Leda and The Swan / and gold being made from lead"

Sylvia Plath alludes to the myth in her radio play "Three Women" written for the BBC in 1962. The play features the voices of three women. The first is a married woman who keeps her baby. The second is a secretary who suffers a miscarriage. The third voice, a girl who is pregnant and leaves her baby, mentions "the great swan, with its terrible look,/ Coming at me," insinuating that the girl was raped. The play is about the disconnection of women in society and challenges societal expectations of childbirth.

Several references to the myth are presented in novels by Angela Carter, including "Nights at the Circus" and "The Magic Toyshop". In the latter novel, the myth is brought to life in the form of a performance in which a frightened young girl is forced to act as Leda in accompaniment with a large mechanical swan.

There is a reference to Leda and the Swan in Dorothea Benton Frank's 2016 book "All Summer Long".

The myth is also mentioned in Richard Yates' 1962 novel "Revolutionary Road". The character Frank Wheeler, married to April Wheeler, after having had sex with an office secretary ponders what to say as he is leaving: "Did the swan apologize to Leda? Did an eagle apologize? Did a lion apologize? Hell no!"
A version of the Leda and the Swan story is the foundation myth in the Canadian futuristic thriller television series "Orphan Black" which aired over 5 seasons from 2013 to 2017. A corporation uses genetic engineering to create a series of female clones (Leda) and a series of male clones (Castor) who are also brothers and sisters clones as they derive from one mother who is a chimera with male and female genomes. 

In April 2012 an art gallery in London, England, was instructed by the police to remove a modern exhibit of Leda and the Swan. The law concerned was Section 63 of the Criminal Justice and Immigration Act 2008, condemning 'violent pornography', brought in by the Labour Party government of 2005–2010.




</doc>
<doc id="18463" url="https://en.wikipedia.org/wiki?curid=18463" title="Lions Clubs International">
Lions Clubs International

Lions Clubs International (LCI) is an international non-political service organization established originally in 1917 in Chicago, Illinois, by Melvin Jones. It is now headquartered in Oak Brook, Illinois. , it had over 46,000 local clubs and more than 1.4 million members (including the youth wing Leo) in more than 200 countries around the world.

Lions Clubs International was founded in Evansville, Indiana on 24 October 1916 by William Perry Woods and subsequently evolved as an international service organization under the guidance and supervision of its secretary, Melvin Jones.

In 1917, Jones was a 38-year-old Chicago business leader who told members of his local business club they should reach beyond business issues and address the betterment of their communities and the world. Jones' group, the Business Circle of Chicago, agreed. After contacting similar groups around the United States, an organizational meeting was held on June 7, 1917, in Chicago. The Business Circle subsequently joined one of the invited groups, the "International Association of Lions Clubs" and at a national convention held in Dallas, Texas, later that year, those who were assembled: (1) adopted a Constitution, By-Laws, Code of Ethics and an Emblem; (2) established as a main tenet "unselfish service to others", (3) unanimously elected Woods as its first president ,effectively securing his leadership for the first two years of the existence of the International Association of Lions, and (4) selected Jones to serve as the organization's secretary-treasurer.
The Lions motto is "We Serve". Local Lions Club programs include sight conservation, hearing and speech conservation, diabetes awareness, youth outreach, international relations, environmental issues, and many other programs. The discussion of politics and religion is forbidden. The LIONS acronym also stands for Liberty, Intelligence, Our Nations' Safety.

The stated purposes of Lions Clubs International are:

Lions Clubs plan and participate in a wide variety of service projects that meet the international goals of Lions Clubs International as well as the needs of their local communities. Examples include donations to hospices, or community campaigns such as "Message in a bottle", a United Kingdom and Ireland initiative which places a plastic bottle with critical medical information inside the refrigerators of vulnerable people. Money is also raised for international purposes. Some of this is donated in reaction to events such as the 2004 Indian Ocean earthquake and the 2013 Typhoon Haiyan (Yolanda) where Lions and LCIF provided disaster relief locally and from around the world, with donations and commitments surpassing US$1 million. Other money is used to support international campaigns, coordinated by the Lions Clubs International Foundation (LCIF), such as Sight First and Lions World Sight Day, which was launched in 1998 to draw world media attention to the plight of sight loss in the developing world. Lions take on all sorts of various fundraisers to fund these projects.

Lions focus on work for the blind and visually impaired began when Helen Keller addressed the international convention at Cedar Point, Ohio, on 30 June 1925 and charged Lions to be "Knights of the Blind".

Lions also have a strong commitment to community hearing- and cancer-screening projects. In Perth, Western Australia, they have conducted hearing screening for over 30 years and provided seed funding for the Lions Ear and Hearing Institute established September 9, 2001, a center of excellence in the diagnosis, management, and research of ear and hearing disorders. In Perth, Lions have also been instrumental in the establishment of the Lions Eye Institute. In Brisbane, Queensland, the Lions Medical Research Foundation provides funding to a number of researchers. Ian Frazer's initial work, leading to the development of a HPV vaccine for the human papillomavirus which could lead to cervical cancer, was funded by the Lions Medical Research Foundation.

Lions Clubs International has supported the work of the United Nations since that organization's inception in 1945, when it was one of the non-governmental organizations invited to assist in the drafting of the United Nations Charter in San Francisco, California.

Lions Clubs International Foundation is "Lions helping Lions serve the world". Donations provide funding in the form of grants to financially assist Lions districts with large-scale humanitarian projects that are too expensive and costly for Lions to finance on their own.
The Foundation aids Lions in making a greater impact in their local communities, as well as around the world. Through LCIF, Lions ease pain and suffering and bring healing and hope to people worldwide. Major initiatives of the foundation include the following:

Upon endorsing the biggest ever collaborative disease eradication programme called the London Declaration on Neglected Tropical Diseases launched on 30 January 2012 in London, the organization has implemented SightFirst program by which it aims to eradicate blindness due to trachoma, one of the neglected tropical diseases. It has allocated over US$11 million in 10 countries for eye surgeries, medical training, distribution of Zithromax and tetracycline, and sanitary services. It has also announced US$6.9 million funding to support the Government of China for the same cause.

Membership in the Lions Club is by "invitation only" as mandated by its constitution and by-laws. All member applicants need a sponsor who is an active member and of good standing in the club they intend to join. While sponsorship may be obtained by an applicant in order to become a legitimate member, sponsorship is no guarantee of membership. Acceptance of membership is still subject to the approval of the majority of the club's board of directors. A Lions Club chooses its members diligently as it requires time and financial commitments. Prospective applicants must be a person of good moral character in his or her community. Attendance at meetings is encouraged on a monthly or fortnightly basis. Due to the hierarchical nature of Lions Clubs International, members have the opportunity to advance from a local club to an office at the zone, district, multiple district, and international levels.

In 1987 the constitution of Lions Clubs International was amended to allow for women to become members. Since then many clubs have admitted women, but some all-male clubs still exist. In 2003, 8 out of 17 members at the Lions Club in Worcester, England, resigned when a woman joined the club. Women's membership numbers continue to grow throughout the association. 

Among the famous and noteworthy members of Lions International are former U.S. President Jimmy Carter; Her Royal Highness Sophie, Countess of Wessex, a member of the Wokingham Lions Club and Royal Patron of the Lions Clubs of the British Isles and Ireland; Amelia Earhart, pioneer U. S. aviator, author, and advocate for women's rights; Richard E. Byrd, admiral in the United States Navy, aviator, pioneer and polar explorer; and Helen Keller, American writer, lecturer and social activist.

Lions Clubs International gives various awards for outstanding merits.

The Medal of Merit (MM) is the highest award from "Lions Clubs International" to non-members for outstanding contributions to Lions Clubs International and its goals.

The District Governor Award (DGA) is one of the highest awards from Lions Clubs International to its members having done exceptional services.

The President's Appreciation Award (PAA) is the highest award that can be awarded to an outstanding club.

The Melvin Jones Fellowship (MJF) Award is the highest recognition from the "Lions Clubs International Foundation" being given to members who have rendered outstanding community services.

The organization became international on 12 March 1920, when the first club in Canada was established in Windsor, Ontario. In 1937, it was founded in San Juan, Puerto Rico. Lions Clubs have since spread across the globe and have a current membership roster of 1.4 million members worldwide.

In addition to adult Lions Clubs, the Lions family includes Lioness Clubs, Leo Clubs, Campus Lions Clubs and Lion Cubs. These divisions are parts of Lions Clubs International.

Lioness Club Membership is generally for women, with exceptions of men also becoming Lioness members nowadays. They are formed under a parent Lions Club. The Lions Club thus becomes the Parent Club for the Lioness Club. Naming of the Club is also like that of the Lions Club—e.g., Lions Club of Satara United Dist 323D-1 forming and sponsoring a Lioness Club Satara United District 323D-1. In many areas, particularly the United States, Lioness clubs have disbanded and merged into their parent clubs to make a more effective club as a whole.

Leo Clubs are an extension of the Lions service organization which aims to encourage community service and involvement from a young age. Much like Lioness Clubs, Leo Clubs are sponsored by a parent Lions Club. Leo Clubs are a common school-based organization with members between the ages of 12 and 18 from the same school, these are commonly referred to as "Alpha Leo Clubs". Community based clubs also exist; these generally cater for 18- to 30-year-olds, and are referred to as "Omega Leo Clubs". Leo Clubs are required to have a Leo Club Advisor, a member of the sponsoring Lions Club who attends meetings and provides general advice to the club. Lions International includes more than 250,000 Leo club members in over 150 countries.

Many Leos join a Campus Lions Club if they attend a university or college after high school graduation. There are more than 600 Campus Lions clubs in the world including nearly 13,000 members on college and university campuses in Australia, Brazil, Canada, China, Dominican Republic, Ecuador, El Salvador, England, Ethiopia, Germany, India, Indonesia, Italy, Mongolia, Nepal, Nigeria, Norway, Pakistan, Panama, Peru, Philippines, Russia, Sri Lanka, Thailand, Uganda, United States, Venezuela, Kenya, Zimbabwe, Legon Lions Club in University of Ghana, KNUST Campus Lions and the University of KwaZulu-Natal Campus Lions Club which is the only active Campus Lions Club currently operating in the Republic of South Africa.

Specialty Lions Clubs
These are chartered to focus on a specific need. For instance, Diabetes Clubs, Champion Lions Clubs (Special Olympics), Vision, hearing, homeless/hunger etc. Specialty clubs may also be ethnic based or its members may have similar interestes - for instance a gardening Lions Club whose members all have interests in gardening.

Lion Cubs is a youth service organization for the elementary aged students (ages eight to twelve). The first club was chartered in the Owen J. Roberts School District in Pottstown, Pennsylvania, United States. It was developed for students in 4th through 6th grade, and therefore too young to be a Leo Club member. The clubs (one club in each of five elementary schools) started their meetings and activities in September 2008 and were officially chartered March 24, 2009. The club is sponsored by the Coventry Lions Club of District 14P. The Lion Cubs first year (2008–09) had 179 charter members.

An international convention is held annually in cities across the globe for members to meet other Lions, elect the coming year's officers, and partake in the many activities planned. At the convention, Lions can participate in elections and parades, display and discuss fundraisers and service projects, and trade pins and other souvenirs. The first convention was held in 1917, the first year of the club's existence, in Dallas, Texas. The 2006 convention was due to be held in New Orleans, but damage sustained during Hurricane Katrina meant that the convention had to be relocated to Boston.

Indonesian Islamic hardliners have called for a ban on the Lions Club, saying it is part of a Zionist conspiracy. The club has been called an "infidel" front for Freemasonry and the world Zionist movement and threatened Islam in the world's most populous Muslim country.

Given that many Freemasons are members of Lions Clubs, and its founder, Melvin Jones, was also a Freemason, modern conspiracy theories have claimed that the Lions are connected to and act cohesively with Freemasonry. One example is found on Martha F. Lee's "Conspiracy Rising: Conspiracy Thinking and American Public Life". It says that the "Freemasons are apparently in cahoots with the Lions Clubs and involved in plots ranging from the distribution of aspartame to control the human mind, to the death of John Paul I, to an apparent plot to spread Zionism."

This perception, according to a Freemasonry website, can be traced to John Robison and the Abbé Barruel's unfounded writings on the causes of the French Revolution, Léo Taxil's late 19th-century hoax and the Protocols of the Elders of Zion.

While there is no direct link between the Lions and the Masons, they are compatible and may have overlapping membership, as evidenced by a speech delivered in 2004 to a Lions Club by a Mason named James F. Kirk-White. The topic of the talk was "Sharing Freemasonry Within Your Community". Their compatibility, moreover, is evidenced by the Masons in Albion, New York offering space for the Lions at a Masonic Lodge. Others also believe that the Lions Clubs actually are a "secret society" that has a great deal of secret ritual within its structure. According to them the Lions are one of those social groups belonging to a secret society that demand an oath of allegiance to join.

Controversial German author Jan Udo Holey, often known by his penname Jan van Helsing, wrote in his 1995 book "Geheimgesellschaften und ihre Macht im 20. Jahrhundert" ("Secret Societies and Their Power in the 20th Century") that the Lions was founded by the B'nai B'rith in Chicago in 1917 and that, like the Freemasons and other secret societies, 90% of its members are used by the elites and have no inkling of what happens in the upper echelons. Holey explained that the lower degrees of the hierarchy of these organizations are much into social work and present good programs.



</doc>
<doc id="18465" url="https://en.wikipedia.org/wiki?curid=18465" title="Laches (equity)">
Laches (equity)

Laches ( "latches", }; Law French: "remissness", "dilatoriness", from Old French "laschesse") refers to a lack of diligence and activity in making a legal claim, or moving forward with legal enforcement of a right, particularly in regard to equity; hence, it is an "unreasonable" delay that can be viewed as prejudicing the opposing [defending] party. When asserted in litigation, it is an equity defense, that is, a defense to a claim for an equitable remedy. The person invoking laches is asserting that an opposing party has "slept on its rights", and that, as a result of this delay, circumstances have changed, witnesses or evidence may have been lost or no longer available, etc., such that it is no longer a just resolution to grant the plaintiff's claim. Laches is associated with the maxim of equity, "Equity aids the vigilant, not the sleeping ones [that is, those who sleep on their rights]." Put another way, failure to assert one's rights in a timely manner can result in a claim being barred by laches.

Laches is a legal term derived from the Old French "laschesse," meaning "remissness" or "dilatoriness," and is viewed as the opposite of "vigilance." The United States Supreme Court case "Costello v. United States" 365 US 265, 282 (1961) is often cited for a definition of laches. Invoking laches is a reference to a lack of diligence and activity in making a legal claim, or moving forward with legal enforcement of a right, in particular with regard to equity, and so is an "unreasonable delay pursuing a right or claim, in a way that prejudices the [opposing] party". When asserted in litigation, it is an equitable defense, that is, a defense to a claim for an equitable remedy. The essential element of "laches" is an unreasonable delay by the plaintiff in bringing the claim; because laches is an equitable defense, it is ordinarily applied only to claims for equitable relief (such as injunctions), and not to claims for legal relief (such as damages). The person invoking laches is asserting that an opposing party has "slept on its rights", and that, as a result of this delay, witnesses and/or evidence may have been lost or no longer available, and circumstances have changed such that it is no longer just to grant the plaintiff's original claim; hence, laches is associated with the maxim of equity: "Vigilantibus non dormientibus æquitas subvenit" ("Equity aids the vigilant, not the sleeping ones [that is, those who sleep on their rights]"). Put another way, failure to assert one's rights in a timely manner can result in a claim being barred by laches. Sometimes courts will also require that the party invoking the doctrine has changed its position as a result of the delay, but that requirement is more typical of the related (but more stringent) defense and equally cause of action of estoppel.

A claim of laches requires the following components:

The period of delay begins when the plaintiff knew, or reasonably ought to have known, that the cause of action existed; the period of delay ends only when the legal action is formally filed. Informing or warning the defendant of the cause of action (for example by sending a cease-and-desist letter or merely "threatening" a lawsuit) does "not", by itself, end the period of delay.

To invoke laches the delay by the opposing party in initiating the lawsuit must be unreasonable. The courts have recognized the following causes of delay as reasonable:

By contrast, it is "not" reasonable to delay a lawsuit to "capitalize on the value of the infringer's labor". In "Danjaq v. Sony", the Ninth Circuit decided that a screenwriter who waited for a film studio to publicize and distribute a film based on a script he allegedly owned had delayed his lawsuit unreasonably.

Unreasonable delay must prejudice the defendant. Examples of such prejudice include:

Unreasonable delay may also prejudice the rights of third-parties who were unknown in the case, earlier but whose rights got created in the intervening period of the delay (e.g.: the defendant inducts new persons on a disputed property by sale, or by lease)

A defense lawyer raising the defense of "laches" against a motion for injunctive relief (a form of equitable relief) might argue that the plaintiff comes "waltzing in at the eleventh hour" when it is now too late to grant the relief sought, at least not without causing great harm that the plaintiff could have avoided. In certain types of cases (for example, cases involving time-sensitive matters, such as elections), a delay of even a few days is likely to be met with a defense of "laches", even where the applicable statute of limitations might allow the type of action to be commenced within a much longer time period. In courts in the United States, laches has often been applied even where a statute of limitations exists, although there is a division of authority on this point.

If a court does accept the laches defense, it can decide either to deny the request for equitable relief or to narrow the equitable relief that it would otherwise give. Even if the court denies equitable relief to a plaintiff because of laches, the plaintiff may still have a claim for legal relief if the statute of limitations has not run out.

Under the United States Federal Rules of Civil Procedure, "laches" is an affirmative defense, which means that the burden of asserting "laches" is on the party responding to the claim to which it applies.

The "laches" defense does not apply if the claimant was a minor during the time that the claim was not brought, so a party can bring a claim against an historical injustice when they reach their majority.

The defense of "laches" resembles a statute of limitations since both are concerned with ensuring that plaintiffs bring their claims in a timely fashion.

However, a statute of limitations is concerned only with the time that has passed. Laches is concerned with the reasonableness of the delay in a particular situation and so is more case-specific and more focused on the equitable conduct of the plaintiff. Those considerations are not unique to the laches defense because they are characteristic of equitable reasoning and equitable remedies. Whereas, limitation is a statutory remedy.

In the US, the proper disposal of claims in light of those two areas of law has required attention through to the Supreme Court. In "Petrella v. Metro-Goldwyn-Mayer" (2014), the US Supreme Court rebuffed a defendant's claim that laches barred a copyright infringement suit because Congress had established a detailed statutory scheme, including a statute of limitations.

In the Virginia Republican primary for the 2012 US presidential election, several candidates did not appear on the ballot because they failed to obtain sufficient petition signatures in time; four of the unsuccessful candidates—Rick Perry, Jon Huntsman, Newt Gingrich, and Rick Santorum—sued, claiming that restrictions on the persons allowed to gather signatures were unconstitutional. Their claim was dismissed by the district court on the grounds of laches, because, in the words of the appellate court: The appeals court upheld the dismissal on grounds of laches, but it added that the challenge would likely have succeeded if it had been brought in a timely fashion.

In Grand Haven, Michigan, the Northwest Ottawa Community Health System sued Grand Haven Township and Health Pointe, which was in the process of building a competing medical facility in the township, arguing that the township ignored its own zoning ordinance in approving the project. On March 24, 2017, as part of the ruling dismissing the lawsuit, Circuit Court Judge Jon A. Van Allsburg noted that the Northwest Ottawa Community Health System delayed more than eight months from the date the project was approved before filing the lawsuit and that during that time, plaintiff Health Pointe had purchased construction materials. Therefore, the doctrine of laches invalidated a lawsuit that was filed so long after the fact.




</doc>
<doc id="18467" url="https://en.wikipedia.org/wiki?curid=18467" title="Legion">
Legion

Legion may refer to:














</doc>
<doc id="18470" url="https://en.wikipedia.org/wiki?curid=18470" title="Lyman Abbott">
Lyman Abbott

Lyman J. Abbott (December 18, 1835 – October 22, 1922) was an American Congregationalist theologian, editor, and author.

Lyman J. Abbott was born at Roxbury, Massachusetts on December 18, 1835, the son of the prolific author, educator and historian Jacob Abbott. Lyman Abbott grew up in Farmington, Maine and later in New York City. Abbott's ancestors were from England, and came to America roughly twenty years after Plymouth Rock.

He graduated from the New York University in 1853, where he was a member of the Eucleian Society, studied law, and was admitted to the bar in 1856. Abbott soon abandoned the legal profession, however, and after studying theology with his uncle, John Stevens Cabot Abbott, was ordained a minister of the Congregational Church in 1860.

He was pastor of the Congregational Church in Terre Haute, Indiana from 1860 to 1865 and of the New England Church in New York City in 1865–1869. From 1865 to 1868 he was secretary of the American Union Commission (later called the American Freedmen's Bureau). In 1869 he resigned his pastorate to devote himself to literature.
Abbot worked variously in the publishing profession as an associate editor of "Harper's Magazine", and was the founder of a publication called the "Illustrated Christian Weekly," which he edited for six years. He was also the co-editor of "The Christian Union" with Henry Ward Beecher from 1876 to 1881. Abbott later succeeded Beecher in 1888 as pastor of Plymouth Church, Brooklyn. He also wrote the official biography of Beecher and edited his papers.

From 1881 Abbott was editor-in-chief of "The Christian Union", renamed "The Outlook" in 1893; this periodical reflected his efforts toward social reform, and, in theology, a liberality, humanitarianism and nearly unitarian. The latter characteristics marked his published works also.

Abbott's opinions differed from those of Beecher. Abbott was a constant advocate of Industrial Democracy, and was an advocate of Theodore Roosevelt's progressivism for almost 20 years. He would later adopt a pronouncedly liberal theology. He was also a pronounced Christian Evolutionist. In two of his books, "The Evolution of Christianity" and "The Theology of an Evolutionist", Abbott applied the concept of evolution in a Christian theological perspective. Although he himself objected to being called an advocate of Darwinism, he was an optimistic advocate of evolution who thought that "what Jesus saw, humanity is becoming."

Abbott was a religious figure of some public note and was called upon on October 30, 1897, to deliver an address in New York at the funeral of economist, Henry George. He ultimately resigned his pastorate in November 1898.

His son, Lawrence Fraser Abbott, accompanied President Roosevelt on a tour of Europe and Africa (1909–10). In 1913 Lyman Abbott was expelled from the American Peace Society because military preparedness was vigorously advocated in the "Outlook", which he edited, and because he was a member of the Army and Navy League. During the World War I he was a strong supporter of the government's war policies.

Lyman Abbott died on October 22, 1922 and was buried in the New Windsor Cemetery in Cornwall-on-Hudson, New York.

The editors of "The Outlook" kept their normal routine, publishing without “departure from the normal course of publication” since that was what their departed colleague would have wanted. The issue asked readers for understanding as the paper “wait[ed] until [the] next week to give to his friends, known and unknown, a record of his life and of the tributes which marked his passing.” A brief tribute appeared in that issue, but the November 8th edition contained the official remembrance and tributes. Fifteen pages in that issue dealt with Abbott, and the publishers included "several long essays in Abbott’s honor from close relatives, shorter tributes from friends and past associates, and blurbs from many American press companies." 

The many diverse and prominent author who contributed tributes "demonstrated the scope and magnitude of Lyman Abbott’s influence within American religious and intellectual culture during his long career." Prominent examples include a re-published 1915 tribute from former United States president Theodore Roosevelt and articles from prestigious newspapers such as "The New York Times" and the "New York Herald". Roosevelt praised Abbott for being “one of those men whose work and life give strength to all who believe in this country,” and the New York Herald recalled Abbott’s ability to “convey his valuable opinions to the entire intellectual public.” Dr. Henry Sloane Coffin noted at a later memorial service, "Measured by the number of people he reached, Dr. Abbott was unquestionably the greatest teacher of religion of this generation.” 

Abbott's lasting influence and widespread appeal is readily apparent in later evaluations of his life. Abbott’s one biographer, Ira V. Brown, confirmed Abbott’s importance via “testimonials by the dozen,” and added that Abbott “directly reached several hundred thousands of people” through his work as a “minister, lecturer, author, and editor.” Abbott was “something of a national patriarch” by the time of his death, and according to Brown, he was “no less than a modern oracle” to thousands of followers. Abbott influenced hundreds every week through his sermons at the prestigious Plymouth Avenue Congregationalist Church. He also gave speeches at many American colleges, published several books that sold between five and ten thousand copies, and edited the Outlook that, at its peak, sold “about 125,000 copies a week.” The magazine "was a prominent news source for Protestant ministers and laypeople all over the United States, demonstrating Abbott's lasting influence."





</doc>
<doc id="18472" url="https://en.wikipedia.org/wiki?curid=18472" title="Leap second">
Leap second

A leap second is a one-second adjustment that is occasionally applied to Coordinated Universal Time (UTC), to accommodate the difference between precise time (as measured by atomic clocks) and imprecise observed solar time (known as UT1 and which varies due to irregularities and long-term slowdown in the Earth's rotation). The UTC time standard, widely used for international timekeeping and as the reference for civil time in most countries, uses precise atomic time and consequently would run ahead of observed solar time unless it is reset to UT1 as needed. The leap second facility exists to provide this adjustment. 

Because the Earth's rotation speed varies in response to climatic and geological events, UTC leap seconds are irregularly spaced and unpredictable. Insertion of each UTC leap second is usually decided about six months in advance by the International Earth Rotation and Reference Systems Service (IERS), to ensure that the difference between the UTC and UT1 readings will never exceed 0.9 seconds.

This practice has proved disruptive, particularly in the twenty-first century and especially in services that depend on precise time stamping or time-critical process control. The relevant international standards body has been debating whether or not to continue the practice, with an increasing number of nations supporting its abolition.

About 140 AD, Ptolemy, the Alexandrian astronomer, sexagesimally subdivided both the mean solar day and the true solar day to at least six places after the sexagesimal point, and he used simple fractions of both the equinoctial hour and the seasonal hour, none of which resemble the modern second. Muslim scholars, including al-Biruni in 1000, subdivided the mean solar day into 24 equinoctial hours, each of which was subdivided sexagesimally, that is into the units of minute, second, third, fourth and fifth, creating the modern second as of the mean solar day in the process. With this definition, the second was proposed in 1874 as the base unit of time in the CGS system of units. Soon afterwards Simon Newcomb and others discovered that Earth's rotation period varied irregularly, so in 1952, the International Astronomical Union (IAU) defined the second as a fraction of the sidereal year. In 1955, considering the tropical year to be more fundamental than the sidereal year, the IAU redefined the second as the fraction of the 1900.0 mean tropical year. In 1956, a slightly more precise value of was adopted for the definition of the second by the International Committee for Weights and Measures, and in 1960 by the General Conference on Weights and Measures, becoming a part of the International System of Units (SI).

Eventually, this definition too was found to be inadequate for precise time measurements, so in 1967, the SI second was again redefined as 9,192,631,770 periods of the radiation emitted by a caesium-133 atom in the transition between the two hyperfine levels of its ground state. That value agreed to 1 part in 10 with the astronomical (ephemeris) second then in use. It was also close to of the mean solar day as averaged between years 1750 and 1892.

However, for the past several centuries, the length of the mean solar day has been increasing by about 1.4–1.7 ms per century, depending on the averaging time. By 1961, the mean solar day was already a millisecond or two longer than SI seconds. Therefore, time standards that change the date after precisely SI seconds, such as the International Atomic Time (TAI), will get increasingly ahead of time standards tied to the mean solar day, such as Universal Time (UT1).

When the Coordinated Universal Time standard was instituted in 1960, based on atomic clocks, it was felt necessary to maintain agreement with the GMT time of day, which, until then, had been the reference for broadcast time services. From 1960 to 1971, the rate of UTC atomic clocks was slowed by the BIH to remain synchronized with UT2, a practice known as the "rubber second". The rate of UTC was decided at the start of each year, and was slower than the rate of atomic time by −150 parts per 10 for 1960–1962, by −130 parts per 10 for 1962–63, by −150 parts per 10 again for 1964–65, and by −300 parts per 10 for 1966–1971. Alongside the shift in rate, an occasional 0.1 s step (0.05 s before 1963) was needed. This predominately frequency shifted rate of UTC was broadcast by MSF, WWV, and CHU among other time stations. In 1966, the CCIR approved "stepped atomic time" (SAT), which adjusted atomic time with more frequent 0.2 s adjustments to keep it within 0.1 s of UT2, because it had no rate adjustments. SAT was broadcast by WWVB among other time stations.

In 1972, the leap-second system was introduced so that the UTC seconds could be set exactly equal to the standard SI second, while still maintaining the UTC time of day and changes of UTC date synchronized with those of UT1 (the solar time standard that superseded GMT). By then, the UTC clock was already 10 seconds behind TAI, which had been synchronized with UT1 in 1958, but had been counting true SI seconds since then. After 1972, both clocks have been ticking in SI seconds, so the difference between their displays at any time is 10 seconds plus the total number of leap seconds that have been applied to UTC as of that time; , 27 leap seconds have been applied to UTC, so the difference is 10 + 27 = 37 seconds.

The scheduling of leap seconds was initially delegated to the Bureau International de l'Heure (BIH), but passed to the International Earth Rotation and Reference Systems Service (IERS) on January 1, 1988. IERS usually decides to apply a leap second whenever the difference between UTC and UT1 approaches 0.6 s, in order to keep the difference between UTC and UT1 from exceeding 

The UTC standard allows leap seconds to be applied at the end of any UTC month, with first preference to June and December and second preference to March and September. , all of them have been inserted at the end of either June 30 or December 31. IERS publishes announcements every six months, whether leap seconds are to occur or not, in its "Bulletin C". Such announcements are typically published well in advance of each possible leap second date – usually in early January for June 30 and in early July for December 31. Some time signal broadcasts give voice announcements of an impending leap second.
Between 1972 and 2020, a leap second has been inserted about every 21 months, on average. However, the spacing is quite irregular and apparently increasing: there were no leap seconds in the six-year interval between January 1, 1999 and December 31, 2004, but there were nine leap seconds in the eight years 1972–1979.

Unlike leap days, which begin after February 28 23:59:59 local time, UTC leap seconds occur simultaneously worldwide; for example, the leap second on December 31, 2005 23:59:60 UTC was December 31, 2005 18:59:60 (6:59:60 p.m.) in U.S. Eastern Standard Time and January 1, 2006 08:59:60 (a.m.) in Japan Standard Time.

When it is mandated, a positive leap second is inserted between second 23:59:59 of a chosen UTC calendar date and second 00:00:00 of the following date. The definition of UTC states that the last day of December and June are preferred, with the last day of March or September as second preference, and the last day of any other month as third preference. All leap seconds (as of 2019) have been scheduled for either June 30 or December 31. The extra second is displayed on UTC clocks as 23:59:60. On clocks that display local time tied to UTC, the leap second may be inserted at the end of some other hour (or half-hour or quarter-hour), depending on the local time zone. A negative leap second would suppress second 23:59:59 of the last day of a chosen month, so that second 23:59:58 of that date would be followed immediately by second 00:00:00 of the following date. Since the introduction of leap seconds, the mean solar day has outpaced atomic time only for very brief periods, and has not triggered a negative leap second.

Leap seconds are irregularly spaced because the Earth's rotation speed changes irregularly. Indeed, the Earth's rotation is quite unpredictable in the long term, which explains why leap seconds are announced only six months in advance.

A mathematical model of the variations in the length of the solar day was developed by F. R. Stephenson and L. V. Morrison, based on records of eclipses for the period 700 BC to 1623 AD, telescopic observations of occultations for the period 1623 until 1967 and atomic clocks thereafter. The model shows a steady increase of the mean solar day by per century, plus a periodic shift of about 4 ms amplitude and period of about 1,500 yr. Over the last few centuries, rate of lengthening of the mean solar day has been about per century, being the sum of the periodic component and the overall rate.

The main reason for the slowing down of the Earth's rotation is tidal friction, which alone would lengthen the day by 2.3 ms/century. Other contributing factors are the movement of the Earth's crust relative to its core, changes in mantle convection, and any other events or processes that cause a significant redistribution of mass. These processes change the Earth's moment of inertia, affecting the rate of rotation due to conservation of angular momentum. Some of these redistributions increase Earth's rotational speed, shorten the solar day and oppose tidal friction. For example, glacial rebound shortens the solar day by 0.6 ms/century and the 2004 Indian Ocean earthquake is thought to have shortened it by 2.68 microseconds. It is evident from the figure that the Earth's rotation has slowed at a decreasing rate since the initiation of the current system in 1971, and the rate of leap second insertions has therefore been decreasing.

The TAI and UT1 time scales are precisely defined, the former by atomic clocks (and thus independent of Earth's rotation) and the latter by astronomical observations (that measure actual planetary rotation and thus the solar time at the Greenwich meridian). UTC (on which civil time is usually based) is a compromise, stepping with atomic seconds but periodically reset by a leap second to match UT1. 

The irregularity and unpredictability of UTC leap seconds is problematic for several areas, especially computing (see below). With increasing requirements for accuracy in automation systems and high-speed trading, this raises a number of issues, since a leap second represents a jump as much as a million times larger than the accuracy required for industry clocks. Consequently, the long-standing practice of inserting leap seconds is under review by the relevant international standards body.

On July 5, 2005, the Head of the Earth Orientation Center of the IERS sent a notice to IERS Bulletins C and D subscribers, soliciting comments on a U.S. proposal before the ITU-R Study Group 7's WP7-A to eliminate leap seconds from the UTC broadcast standard before 2008 (the ITU-R is responsible for the definition of UTC). It was expected to be considered in November 2005, but the discussion has since been postponed. Under the proposal, leap seconds would be technically replaced by leap hours as an attempt to satisfy the legal requirements of several ITU-R member nations that civil time be astronomically tied to the Sun.

A number of objections to the proposal have been raised. Dr. P. Kenneth Seidelmann, editor of the Explanatory Supplement to the Astronomical Almanac, wrote a letter lamenting the lack of consistent public information about the proposal and adequate justification. Steve Allen of the University of California, Santa Cruz cited what he claimed to be the large impact on astronomers in a "Science News" article. He has an extensive online site devoted to the issues and the history of leap seconds, including a set of references about the proposal and arguments against it.

At the 2014 General Assembly of the International Union of Radio Scientists (URSI), Dr. Demetrios Matsakis, the United States Naval Observatory's Chief Scientist for Time Services, presented the reasoning in favor of the redefinition and rebuttals to the arguments made against it. He stressed the practical inability of software programmers to allow for the fact that leap seconds make time appear to go backwards, particularly when most of them do not even know that leap seconds exist. The possibility of leap seconds being a hazard to navigation was presented, as well as the observed effects on commerce.

The United States formulated its position on this matter based upon the advice of the National Telecommunications and Information Administration and the Federal Communications Commission (FCC), which solicited comments from the general public. This position is in favor of the redefinition.

In 2011, Chunhao Han of the Beijing Global Information Center of Application and Exploration said China had not decided what its vote would be in January 2012, but some Chinese scholars consider it important to maintain a link between civil and astronomical time due to Chinese tradition. The 2012 vote was ultimately deferred. At an ITU/BIPM-sponsored workshop on the leap second, Dr. Han expressed his personal view in favor of abolishing the leap second, and similar support for the redefinition was again expressed by Dr. Han, along with other Chinese timekeeping scientists, at the URSI General Assembly in 2014.

At a special session of the Asia-Pacific Telecommunity Meeting on February 10, 2015, Chunhao Han indicated China was now supporting the elimination of future leap seconds, as were all the other presenting national representatives (from Australia, Japan, and the Republic of Korea). At this meeting, Bruce Warrington (NMI, Australia) and Tsukasa Iwama (NICT, Japan) indicated particular concern for the financial markets due to the leap second occurring in the middle of a workday in their part of the world. Subsequent to the CPM15-2 meeting in March/April 2015 the draft gives four methods which the WRC-15 might use to satisfy Resolution 653 from WRC-12.

Arguments against the proposal include the unknown expense of such a major change and the fact that universal time will no longer correspond to mean solar time. It is also answered that two timescales that do not follow leap seconds are already available, International Atomic Time (TAI) and Global Positioning System (GPS) time. Computers, for example, could use these and convert to UTC or local civil time as necessary for output. Inexpensive GPS timing receivers are readily available, and the satellite broadcasts include the necessary information to convert GPS time to UTC. It is also easy to convert GPS time to TAI, as TAI is always exactly 19 seconds ahead of GPS time. Examples of systems based on GPS time include the CDMA digital cellular systems IS-95 and CDMA2000. In general, computer systems use UTC and synchronize their clocks using Network Time Protocol (NTP). Systems that cannot tolerate disruptions caused by leap seconds can base their time on TAI and use Precision Time Protocol. However, the BIPM has pointed out that this proliferation of timescales leads to confusion.

At the 47th meeting of the Civil Global Positioning System Service Interface Committee in Fort Worth, Texas in September 2007, it was announced that a mailed vote would go out on stopping leap seconds. The plan for the vote was:
In January 2012, rather than decide yes or no per this plan, the ITU decided to postpone a decision on leap seconds to the World Radiocommunication Conference in November 2015. At this conference, it was again decided to continue using leap seconds, pending further study and consideration at the next conference in 2023.

In October 2014, Dr. Włodzimierz Lewandowski, chair of the timing subcommittee of the Civil GPS Interface Service Committee and a member of the ESA Navigation Program Board, presented a CGSIC-endorsed resolution to the ITU that supported the redefinition and described leap seconds as a "hazard to navigation".

Some of the objections to the proposed change have been answered by its opponents. For example, Dr. Felicitas Arias, who, as Director of the International Bureau of Weights and Measures (BIPM)'s Time, Frequency, and Gravimetry Department, is responsible for generating UTC, noted in a press release that the drift of about one minute every 60–90 years could be compared to the 16-minute annual variation between true solar time and mean solar time, the one hour offset by use of daylight time, and the several-hours offset in certain geographically extra-large time zones.

To compute the elapsed time in seconds between two given UTC dates requires the consultation of a table of leap seconds, which needs to be updated whenever a new leap second is announced. Since leap seconds are known only 6 months in advance, time intervals for UTC dates farther in the future cannot be computed.

Although BIPM announces a leap second 6 months in advance, most time distribution systems (SNTP, IRIG-B, PTP) announce leap seconds at most 12 hours in advance, sometimes only in the last minute and some even not at all (DNP 03). Clocks that are not regularly synchronized can miss a leap second, but still can claim to be perfectly synchronized.

Not all clocks implement leap seconds in the same manner. Leap seconds in Unix time are commonly implemented by repeating 23:59:59 or adding 23:59:60. Network Time Protocol (SNTP) freezes time during the leap second, some time servers declare "alarm condition". Other schemes smear time in the vicinity of a leap second.

While the textual representation of leap seconds is defined by BIPM as "23:59:60", most computer operating systems and most time distribution systems derive this human-readable text from a binary counter indicating the number of seconds elapsed since an arbitrary epoch; for instance, since 00:00:00 in Unix machines or since 00:00:00 in NTP. This counter has no indicator that a leap second has been inserted, therefore two seconds in sequence will have the same counter value. Some computer operating systems, in particular Linux, assign to the leap second the counter value of the preceding, 23:59:59 second ( sequence), while other computers (and the IRIG-B time distribution) assign to the leap second the counter value of the next, 00:00:00 second ( sequence). Since there is no standard governing this sequence, the time stamp of values sampled at exactly the same time can vary by one second. This may explain flaws in time-critical systems that rely on time-stamped values.

The textual representation is not always accepted. Entering "2016-12-31 23:59:60" in a POSIX converter will fail and XML will reject such entry as "invalid time". This can cause an exception status in application programs.

A number of organizations reported problems caused by flawed software following the June 30, 2012, leap second. Among the sites which reported problems were Reddit (Apache Cassandra), Mozilla (Hadoop), Qantas, and various sites running Linux.

Older versions of Motorola Oncore VP, UT, GT, and M12 GPS receivers had a software bug that would cause a single timestamp to be off by a day if no leap second was scheduled for 256 weeks. On November 28, 2003, this happened. At midnight, the receivers with this firmware reported November 29, 2003 for one second and then reverted to November 28, 2003.

Older Trimble GPS receivers had a software flaw that would insert a leap second immediately after the GPS constellation started broadcasting the next leap second insertion time (some months in advance of the actual leap second), rather than waiting for the next leap second to happen. This left the receiver's time off by a second in the interim.

Older Datum Tymeserve 2100 GPS receivers and Symmetricom Tymeserve 2100 receivers also have a similar flaw to that of the older Trimble GPS receivers, with the time being off by one second. The advance announcement of the leap second is applied as soon as the message is received, instead of waiting for the correct date. A workaround has been described and tested, but if the GPS system rebroadcasts the announcement, or the unit is powered off, the problem will occur again.

On January 21, 2015, several models of GPS receivers implemented the leap second as soon as the announcement was broadcast by GPS, instead of waiting until the implementation date of June 30.

The NTP protocol specifies a flag to inform the receiver that a leap second is imminent. However, some NTP servers have failed to set their leap second flag correctly. Some NTP servers have responded with the wrong time for up to a day after a leap second insertion.

Four different brands of marketed navigational receivers that use data from GPS or Galileo along with the Chinese BeiDou satellites, and even some receivers that use BeiDou satellites alone, were found to implement leap seconds one day early. This was traced to the fact that BeiDou numbers the days of the week from 0 to 6, while GPS and Galileo number them from 1 to 7.

The effect of leap seconds on the commercial sector has been described as "a nightmare". Because financial markets are vulnerable to both technical and legal leap second problems, the Intercontinental Exchange, parent body to 7 clearing houses and 11 stock exchanges including the New York Stock Exchange, ceased operations for 61 minutes at the time of the June 30, 2015 leap second.

Despite the publicity given to the 2015 leap second, a small number of network failures occurred due to leap second-related software errors of some routers. Also, interruptions of around 40 minutes' duration occurred with Twitter, Instagram, Pinterest, Netflix, Amazon, and Apple's music streaming series Beats 1.

Several older versions of the Cisco Systems NEXUS 5000 Series Operating System NX-OS (versions 5.0, 5.1, 5.2) are affected by leap second bugs.

Leap second software bugs have affected the Altea airlines reservation system used by Qantas and Virgin Australia.

Cloudflare was affected by a leap second software bug. Its DNS resolver implementation incorrectly calculated a negative number when subtracting two timestamps obtained from the Go programming language's codice_1 function, which then used only a real-time clock source. This could have been avoided by using a monotonic clock source, which has since been added to Go 1.9.

There were misplaced concerns that farming equipment using GPS during harvests occurring on December 31, 2016, would be affected by the 2016 leap second. GPS navigation makes use of GPS time, which is not impacted by the leap second.

The most obvious workaround is to use the TAI scale for all operational purposes and convert to UTC for human-readable text. UTC can always be derived from TAI with a suitable table of leap seconds; the reverse is unsure. The Society of Motion Picture and Television Engineers (SMPTE) video/audio industry standards body selected TAI for deriving time stamps of media. 
IEC/IEEE 60802 (Time sensitive networks) specifies TAI for all operations. Grid automation is planning to switch to TAI for global distribution of events in electrical grids. Bluetooth mesh networking also
uses TAI.

Instead of inserting a leap second at the end of the day, Google servers implement a "leap smear", extending seconds slightly over a 24-hour period centered on the leap second. Amazon followed a similar, but slightly different, pattern for the introduction of the June 30, 2015 leap second, leading to another case of the proliferation of timescales. They later released an NTP service for EC2 instances which performs leap smearing. UTC-SLS was proposed as a version of UTC with linear leap smearing, but it never became standard.

It has been proposed that media clients using the Real-time Transport Protocol inhibit generation or use of NTP timestamps during the leap second and the second preceding it.

NIST has established a special NTP time server to deliver UT1 instead of UTC. Such a server would be particularly useful in the event the ITU resolution passes and leap seconds are no longer inserted. Those astronomical observatories and other users that require UT1 could run off UT1 – although in many cases these users already download UT1-UTC from the IERS, and apply corrections in software.





</doc>
<doc id="18473" url="https://en.wikipedia.org/wiki?curid=18473" title="Luca Pacioli">
Luca Pacioli

Fra Luca Bartolomeo de Pacioli (sometimes "Paccioli" or "Paciolo"; 1447 – 19 June 1517) was an Italian mathematician, Franciscan friar, collaborator with Leonardo da Vinci, and an early contributor to the field now known as accounting. He is referred to as "The Father of Accounting and Bookkeeping" in Europe and he was the second person to publish a work on the double-entry system of book-keeping on the continent. He was also called Luca di Borgo after his birthplace, Borgo Sansepolcro, Tuscany.

Luca Pacioli was born between 1446 and 1448 in the Tuscan town of Sansepolcro where he received an abbaco education. This was education in the vernacular ("i.e.", the local tongue) rather than Latin and focused on the knowledge required of merchants. His father was Bartolomeo Pacioli; however, Luca Pacioli was said to have lived with the Befolci family as a child in his birth town Sansepolcro. He moved to Venice around 1464, where he continued his own education while working as a tutor to the three sons of a merchant. It was during this period that he wrote his first book, a treatise on arithmetic for the boys he was tutoring. Between 1472 and 1475, he became a Franciscan friar. Thus, he could be referred to as Fra ('Friar') Luca.

In 1475, he started teaching in Perugia as a private teacher before becoming first chair in mathematics in 1477. During this time, he wrote a comprehensive textbook in the vernacular for his students. He continued to work as a private tutor of mathematics and was instructed to stop teaching at this level in Sansepolcro in 1491. In 1494, his first book, "Summa de arithmetica, geometria, Proportioni et proportionalita", was published in Venice. In 1497, he accepted an invitation from Duke Ludovico Sforza to work in Milan. There he met, taught mathematics to, collaborated, and lived with Leonardo da Vinci. In 1499, Pacioli and da Vinci were forced to flee Milan when Louis XII of France seized the city and drove out their patron. Their paths appear to have finally separated around 1506. Pacioli died at about the age of 70 on 19 June 1517, most likely in Sansepolcro, where it is thought that he had spent much of his final years.

Pacioli published several works on mathematics, including:

The majority of the second volume of "Summa de arithmetica, geometria. Proportioni et proportionalita" was a slightly rewritten version of one of Piero della Francesca's works. The third volume of Pacioli's "Divina proportione" was an Italian translation of Piero della Francesca's Latin writings "On [the] Five Regular Solids". In neither case did Pacioli include an attribution to Piero. He was severely criticized for this and accused of plagiarism by sixteenth-century art historian and biographer Giorgio Vasari. R. Emmett Taylor (1889–1956) said that Pacioli may have had nothing to do with the translated volume "Divina proportione", and that it may just have been appended to his work. However, no such defense can be presented concerning the inclusion of Piero della Francesca's material in Pacioli's Summa.

Pacioli dramatically affected the practice of accounting by describing the double-entry accounting method used in parts of Italy. This revolutionized how businesses oversaw their operations, enabling improved efficiency and profitability. The "Summa"'s section on accounting was used internationally as an accounting textbook up to the mid-16th century. The essentials of double-entry accounting have for the most part remain unchanged for over 500 years. "Accounting practitioners in public accounting, industry, and not-for-profit organizations, as well as investors, lending institutions, business firms, and all other users for financial information are indebted to Luca Pacioli for his monumental role in the development of accounting."

The ICAEW Library's rare book collection at Chartered Accountants' Hall holds the complete published works of Luca Pacioli. Sections of two of Pacioli's books, 'Summa de arithmetica' and 'Divina proportione' can be viewed online using Turning the Pages, an interactive tool developed by the British Library.

Luca Pacioli also wrote an unpublished treatise on chess, "De ludo scachorum" ("On the Game of Chess"). Long thought to have been lost, a surviving manuscript was rediscovered in 2006, in the 22,000-volume library of Count Guglielmo Coronini-Cronberg in Gorizia. A facsimile edition of the book was published in Pacioli's home town of Sansepolcro in 2008. Based on Leonardo da Vinci's long association with the author and his having illustrated "Divina proportione", some scholars speculate that Leonardo either drew the chess problems that appear in the manuscript or at least designed the chess pieces used in the problems.


Footnotes
Citations



</doc>
<doc id="18474" url="https://en.wikipedia.org/wiki?curid=18474" title="Lower Mainland">
Lower Mainland

The Lower Mainland is a name commonly applied to the region surrounding and including Vancouver, British Columbia, Canada. As of 2016, 2,832,000 people (60 percent of British Columbia's total population) lived in the region; sixteen of the province's thirty most populous municipalities are located there. Islands contained within rivers in the region are considered to be part of the Lower Mainland.

While the term "Lower Mainland" has been recorded from the earliest period of non-native settlement in British Columbia, it has never been officially defined in legal terms. The British Columbia Geographical Names Information System (BCGNIS) comments that most residents of Vancouver might consider it to be only areas west of Mission and Abbotsford, while residents in the rest of the province consider it to be the whole region south of Whistler and west of Hope. However, the term has historically been in popular usage for over a century to describe a region that extends from Horseshoe Bay south to the Canada–United States border and east to Hope at the eastern end of the Fraser Valley.

The climate, ecology and geology of the Lower Mainland are consistent enough that it has been classified as a separate ecoregion (the Lower Mainland Ecoregion) within the ecological framework of Canada, used by both the federal and provincial environment ministries.

The region is the traditional territory of the , a Halkomelem-speaking people of the Coast Salish linguistic and cultural grouping. There are two regional districts within the region, Metro Vancouver and the Fraser Valley.

The region is bounded to the north by the Coast Mountains and to the southeast by the Cascade Mountains, and is traversed from east to west by the Fraser River. Due to its consistency of climate, flora and fauna, geology and land use, "Lower Mainland" is also the name of an ecoregion—a biogeoclimatic region—that comprises the eastern part of the Georgia Depression and extends from Powell River on the Sunshine Coast to Hope at the eastern end of the Fraser Valley.

One of the mildest climates in Canada, the region has a mean annual temperature of with a summer mean of and a winter mean of . Annual precipitation ranges from an annual mean of in the west end to in the eastern end of the Fraser Valley and at higher elevations. Maximum precipitation occurs as rain in winter. Less than ten percent falls as snow at sea level but the amount of snowfall increases significantly with elevation.

As of the 2016 census, the population of the Lower Mainland totals 2,759,385:

These figures are slightly inflated due to the inclusion of areas within the Regional Districts which are not normally considered to be part of the Lower Mainland, notably the lower Fraser Canyon and the heads of Harrison and Pitt Lakes, which are within the FVRD, and Lions Bay and Bowen Island, which are within the Greater Vancouver Regional District.

The population of the Lower Mainland was up 9.2 percent from the 2006 census. This is among the highest growth rates in the continent.

The Lower Mainland is among the most diverse regions in Canada. Europeans form a slight majority at 51.5 percent, followed by East Asians at 20.8 percent and South Asians at 12.2 percent.

The Lower Mainland includes large Christian, Irreligious, Sikh and Buddhist communities. The Sikh population, numbering 185,000 or 7.2 percent of the total population, is significant across Metro Vancouver and the Fraser Valley; proportionally, it is more than five times the national average of 1.4 percent.
Regional districts were first created across British Columbia in 1966–1967 to form bodies for inter-municipal coordination and to extend municipal-level powers to areas outside existing municipalities. Today, the Lower Mainland includes two regional districts: the Metro Vancouver Regional District (MVRD) and the Fraser Valley Regional District (FVRD). Both regional districts, however, include areas outside the traditional limits of the Lower Mainland. Metro Vancouver includes areas like Surrey and Langley that are geographically in the Fraser Valley.

The Metro Vancouver Regional District is made up of 21 municipalities. The MVRD is bordered on the west by the Strait of Georgia, to the north by the Squamish-Lillooet Regional District, on the east by the Fraser Valley Regional District, and to the south by Whatcom County, Washington, in the United States.

The Fraser Valley Regional District lies east of the Greater Vancouver Regional District, and comprises the cities of Abbotsford and Chilliwack, the district municipalities of Mission, Kent, and Hope, and the village of Harrison Hot Springs. It also includes many unincorporated areas in the Fraser Valley and along the west side of the Fraser Canyon (the Fraser Canyon is not in the Lower Mainland).

Regional district powers are very limited and other localized provincial government services are delivered through other regionalization systems.

The traditional territories of the Musqueam and Tsleil'waututh lie completely within the region; the southern portion of Squamish traditional territory is also in the region. Its claims overlap those of the Tsleil-waututh, Musqueam, and Kwikwetlem. Other peoples whose territories lie within the region are the , Chehalis, Katzie, Kwantlen, Tsawwassen, and Semiahmoo; many of their territories overlap with those of the Musqueam, and with each other. Many other peoples of the Georgia Strait region also frequented the lower Fraser, including those from Vancouver Island and what is now Whatcom County, Washington.

Sto:lo traditional territory, known as "Solh Temexw" in Halkomelem, more or less coincides with the traditional conception of the Lower Mainland, except for the inclusion of Port Douglas at the head of Harrison Lake, which is in In-SHUCK-ch territory, and the lands around Burrard Inlet.

Health system services and governance in the Lower Mainland are provided by Vancouver Coastal Health, serving Vancouver, Richmond and the North Shore, and the mainland coast as far north as the Central Coast region, and Fraser Health, which serves the area of the Lower Mainland east of Vancouver and Richmond.

The Lower Mainland is considered to have a high vulnerability to flood risk. There have been two major floods, the largest in 1894 and the second largest in 1948. According to the Fraser Basin Council, scientists predict a one-in-three chance of a similar-sized flood occurring in the next 50 years.

In the spring of 2007, the Lower Mainland was on high alert for flooding. Higher than normal snow packs in the British Columbia Interior prompted municipal governments to start taking emergency measures in the region. Dikes along the Fraser River are regulated to handle about 8.5 m at the Mission Gauge (the height above sea level of the dykes at Mission). Warmer than normal weather in the interior caused large amounts of snow to melt prematurely, resulting in higher than normal water levels, which, nevertheless, remained well below flood levels.

Flooding can cover much of the Lower Mainland. Cloverdale, Barnston Island, low-lying areas of Maple Ridge, areas west of Hope, White Rock, Richmond, parts of Vancouver, and parts of Surrey are potentially at risk. In 2007, the Lower Mainland was largely spared, although northern regions of the province, along the Skeena and Nechako Rivers, experienced floods. Climate scientists predict that increasing temperatures will mean wetter winters and more snow at the high elevations. This will increase the likelihood of snowmelt floods.

The provincial government maintains an integrated flood hazard management program and extensive flood protection infrastructure in the Lower Mainland. The infrastructure consists of dikes, pump stations, floodboxes, riprap, and relief wells.

While earthquakes are common in British Columbia and adjacent coastal waters, most are minor in energy release or are sufficiently remote to have little effect on populated areas. Nevertheless, earthquakes with a magnitude of up to 7.3 have occurred within of the Lower Mainland.

Based on geological evidence, however, stronger earthquakes appear to have occurred at approximately 600-year intervals. Therefore, there is a probability that there will be a major earthquake in the region within the next 200 years.

In April 2008, the United States Geological Survey released information concerning a newly found fault south of downtown Abbotsford, called the Boulder Creek Fault. Scientists now believe this fault is active and capable of producing earthquakes in the 6.8 magnitude range.

Much of the Lower Mainland is vulnerable to explosive eruptions from the Garibaldi Volcanic Belt. Volcanoes in this zone are capable of producing large quantities of volcanic ash that may cause short and long term water supply problems for Lower Mainland communities. All airports covered by the accompanying eruption column would be closed, heavy ash falls would damage electrical equipment and weak structures could collapse under the weight of the ash.

The Lower Mainland's communities includes large cities in Metro Vancouver, and smaller cities, towns and villages along both banks of the Fraser River. Neighbourhoods within cities are not listed unless historically or otherwise notable and/or separate. Only some of the many Indian Reserves are listed.






</doc>
<doc id="18475" url="https://en.wikipedia.org/wiki?curid=18475" title="Lucius Afranius (poet)">
Lucius Afranius (poet)

Lucius Afranius was an ancient Roman comic poet, who lived at the beginning of the 1st century BC. 

Afranius' comedies described Roman scenes and manners (the genre called "comoediae togatae") and the subjects were mostly taken from the life of the lower classes ("comoediae tabernariae"). They were considered by some ancients to be frequently polluted with disgraceful amours, which, according to Quintilian, were only a representation of the conduct of Afranius. He depicted, however, Roman life with such accuracy that he is classed with Menander, from whom indeed he borrowed largely. He imitated the style of Gaius Titius, and his language is praised by Cicero. His comedies are spoken of in the highest terms by the ancient writers, and under the Empire they not only continued to be read, but were even acted, of which an example occurs in the time of Nero. They seem to have been well known even at the latter end of the 4th century AD.

The Spanish-Roman teacher of rhetoric Quintilian wrote of Afranius's plays:

Such is the generally accepted interpretation of this sentence. An alternative view is proposed by Welsh (2010), who, noting that there is no trace of pederasty or any lewdness in any of the quoted fragments of Afranius, proposed to translate the sentence "if only he hadn't polluted his plots with disreputable love affairs (conducted) by boys", something which Quintilian perhaps thought unsuited to the moralising tone of Roman comedies. A problem with this interpretation, as Welsh himself admits, is that in Roman literature the word "pueri" is usually used for the boys who are object of love affairs, not the young men who conduct them.

Afranius wrote many comedies. The titles of forty-two of his plays are still preserved, along with associated fragments and quotations:



</doc>
<doc id="18476" url="https://en.wikipedia.org/wiki?curid=18476" title="London Post Office Railway">
London Post Office Railway

The Post Office Railway, known as Mail Rail since 1987, is a narrow gauge, driverless underground railway in London that was built by the Post Office with assistance from the Underground Electric Railways Company of London, to transport mail between sorting offices. Inspired by the Chicago Tunnel Company, it opened in 1927 and operated for 76 years until it closed in 2003. A museum within the former railway was opened in September 2017.

The line ran from Paddington Head District Sorting Office in the west to the Eastern Head District Sorting Office at Whitechapel in the east, a distance of . It had eight stations, the largest of which was underneath Mount Pleasant, but by 2003 only three stations remained in use because the sorting offices above the other stations had been relocated.

In 1911, a plan evolved to build an underground railway long from Paddington to Whitechapel serving the main sorting offices along the route; road traffic congestion was causing unacceptable delays. The contract to build the tunnels was won by John Mowlem and Co. Construction of the tunnels started in February 1915 from a series of shafts. Most of the line was constructed using the Greathead shield system, with limited amounts of hand-mining for connecting tunnels at stations.

The main line has a single diameter tube with two tracks. Just before stations, tunnels diverge into two single-track diameter tunnels leading to two parallel diameter station tunnels. The main tube is at a depth of around . Stations are at a much shallower depth, with a 1-in-20 gradient into the stations. The gradients assist in slowing the trains when approaching stations, and accelerating them away. There is also less distance to lift mail from the stations to the surface. At Oxford Circus the tunnel runs close to the Bakerloo line tunnel of the London Underground.
During 1917, work was suspended due to the shortage of labour and materials. By June 1924, track laying had started. In February 1927, the first section, between Paddington and the West Central District Office, was made available for training. The line became available for the Christmas parcel post in 1927 and letters were carried from February 1928.

In 1954, plans were developed for a new Western District Office at Rathbone Place, which required a diversion, opening in 1958. It was not until 3 August 1965 that the new station and office were opened by the Postmaster General, Anthony Wedgwood-Benn. The disused section was used as a store tunnel; some parts of it still have the track in place.

A Royal Mail press release in April 2003 said that the railway would be closed and mothballed at the end of May that year. Royal Mail had earlier stated that using the railway was five times more expensive than using road transport for the same task. The Communication Workers Union (CWU) claimed the actual figure was closer to three times more expensive but argued that this was the result of a deliberate policy of running the railway down and using it at only one-third of its capacity. A local governmental report by the Greater London Authority was in support of continued use and criticized the increase of lorries on local roads, estimated to be 80 more trucks per week. The railway was closed on 31 May 2003.

In April 2011, an urban exploration group called the "Consolidation Crew" published accounts of illicit access to the tunnels. Detailed photography and text revealed that the railway is still largely in good condition, despite some natural decay. More recently, media have been admitted to the tunnels as part of the pre-launch publicity for the Postal Museum. Photographs show much of the infrastructure in place.

A team from the University of Cambridge has taken over a short, double track section of unused Post Office tunnel near Liverpool Street Station, where a newly built tunnel for Crossrail is situated some two metres beneath. The study is to establish how the original cast-iron lining sections, which are similar to those used for many miles of railway under London, resist possible deformation and soil movement caused by the new works. Digital cameras, fibre optic deformation sensors, laser scanners and other low-cost instruments, reporting in real time, have been installed in the vacated tunnel. As well as providing information about the behaviour of the old construction materials, the scheme can also provide an early warning if the new tunnel bores are creating dangerous soil movement.

In October 2013, the British Postal Museum & Archive announced that it intended opening part of the network to the public. After approval was granted by Islington Council, work on the new museum and the railway began in 2014. Special tourist trains were installed in late 2016. It was planned to open a circular route, running beneath the depot at Mount Pleasant with a journey time of around 15 minutes, by mid-2017. The museum opened on 5 September.

In its first year of operation (2017–2018), the trains performed 9,000 trips totalling , with the railway and museum hosting over 198,000 visitors.

The first stock was delivered in 1926 with the opening of the system. All stock used was electrically powered.


Some trains have been preserved at the Launceston Steam Railway.


A pneumatic underground railway was used by the Post Office in London between 1863 and 1874 using individual wheeled capsules, operated by the London Pneumatic Despatch Company.

In 1910, a tunnel railway opened in Munich, Germany between München Hauptbahnhof and the nearby Post office. The tunnels were damaged in World War II, restored in 1948 and partially rebuilt in 1966 to allow for the first Munich S-Bahn tunnel. Operations ceased in 1988.

Postal Telegraph and Telephone (Switzerland) () opened the Post-U-Bahn (underground railway) in Zürich in 1938. It ran between Zürich Hauptbahnhof and the Sihlpost (), Zürich's main post office. The track gauge was 60 cm, and the small electric railcar, which could carry 250 kg of mail, collected power from wires between the tracks. Operations ceased on 11 October 1980 when a rubber-tired system replaced the train.

The Chicago Tunnel Company, in operation between 1906 and 1959, delivered freight, parcels, and coal, and disposed of ash and excavation debris. It operated an elaborate network of narrow gauge track in tunnels running under the streets throughout the central business district including and surrounding the "Loop".

The only known instance of a human travelling on the London Post Office Railway is that of the cricket commentator and entertainer Brian Johnston, who presented the radio show "In Town Tonight" in which he was "travelling as a parcel" on one of the trains on the railway.




</doc>
<doc id="18477" url="https://en.wikipedia.org/wiki?curid=18477" title="Lulach">
Lulach

Lulach mac Gille Coemgáin (Modern Gaelic: "Lughlagh mac Gille Chomghain", known in English simply as Lulach, and nicknamed Tairbith, "the Unfortunate" and Fatuus, "the Simple-minded" or "the Foolish"; before 1033 – 17 March 1058) was King of Scots between 15 August 1057 and 17 March 1058.

Lulach was the son of Gruoch of Scotland, from her first marriage to Gille Coemgáin, Mormaer of Moray, and thus the stepson of Macbeth (Mac Bethad mac Findlaích). Following the death of Macbeth at the Battle of Lumphanan on 15 August 1057, the king's followers placed Lulach on the throne. He has the distinction of being the first king of Scotland of whom there are coronation details available: he was crowned, probably on 8 September 1057 at Scone. Lulach appears to have been a weak king, as his nicknames suggest, and ruled only for a few months before being assassinated and usurped by Malcolm III.

Lulach's son Máel Snechtai was Mormaer of Moray, while Óengus of Moray was the son of Lulach's daughter. 

He is believed to be buried on Saint Columba's Holy Island of Iona in or around the monastery. The exact position of his grave is unknown.

Lulach is an important secondary character in Dorothy Dunnett's historical novel "King Hereafter", where he is portrayed as a seer. In the novel, Dunnett used Lulach as a mouthpiece for researched information about the real Macbeth.

Lulach is also one of the protagonists in Jackie French's children's novel "Macbeth and Son" and in Susan Fraser King's novel "Lady MacBeth".


</doc>
<doc id="18483" url="https://en.wikipedia.org/wiki?curid=18483" title="Lexicography">
Lexicography

Lexicography is divided into two separate but equally important groups:


There is some disagreement on the definition of lexicology, as distinct from lexicography. Some use "lexicology" as a synonym for theoretical lexicography; others use it to mean a branch of linguistics pertaining to the inventory of words in a particular language.

A person devoted to lexicography is called a lexicographer.

General lexicography focuses on the design, compilation, use and evaluation of general dictionaries, i.e. dictionaries that provide a description of the language in general use. Such a dictionary is usually called a general dictionary or LGP dictionary (Language for General Purpose). Specialized lexicography focuses on the design, compilation, use and evaluation of specialized dictionaries, i.e. dictionaries that are devoted to a (relatively restricted) set of linguistic and factual elements of one or more specialist subject fields, e.g. legal lexicography. Such a dictionary is usually called a specialized dictionary or Language for specific purposes dictionary and following Nielsen 1994, specialized dictionaries are either multi-field, single-field or sub-field dictionaries.

It is now widely accepted that lexicography is a scholarly discipline in its own right and not a sub-branch of applied linguistics, as the chief object of study in lexicography is the dictionary (see e.g. Bergenholtz/Nielsen/Tarp 2009).

Coined in English 1680, the word "lexicography" derives from the Greek λεξικογράφος "lexikographos", "lexicographer", from λεξικόν "lexicon", neut. of λεξικός "lexikos", "of or for words", from λέξις "lexis", "speech", "word", (in turn from λέγω "lego", "to say", "to speak") and γράφω "grapho", "to scratch, to inscribe, to write".

Practical lexicographic work involves several activities, and the compilation of well-crafted dictionaries requires careful consideration of all or some of the following aspects:

One important goal of lexicography is to keep the lexicographic information costs incurred by dictionary users as low as possible. Nielsen (2008) suggests relevant aspects for lexicographers to consider when making dictionaries as they all affect the users' impression and actual use of specific dictionaries.

Theoretical lexicography concerns the same aspects as lexicography, but aims to develop principles that can improve the quality of future dictionaries, for instance in terms of access to data and lexicographic information costs. Several perspectives or branches of such academic dictionary research have been distinguished: 'dictionary criticism' (or evaluating the quality of one or more dictionaries, e.g. by means of reviews (see Nielsen 1999), 'dictionary history' (or tracing the traditions of a type of dictionary or of lexicography in a particular country or language), 'dictionary typology' (or classifying the various genres of reference works, such as dictionary versus encyclopedia, monolingual versus bilingual dictionary, general versus technical or pedagogical dictionary), 'dictionary structure' (or formatting the various ways in which the information is presented in a dictionary), 'dictionary use' (or observing the reference acts and skills of dictionary users), and 'dictionary IT' (or applying computer aids to the process of dictionary compilation).

One important consideration is the status of 'bilingual lexicography', or the compilation and use of the bilingual dictionary in all its aspects (see e.g. Nielsen 1894). In spite of a relatively long history of this type of dictionary, it is often said to be less developed in a number of respects than its unilingual counterpart, especially in cases where one of the languages involved is not a major language. Not all genres of reference works are available in interlingual versions, e.g. LSP, learners' and encyclopedic types, although sometimes these challenges produce new subtypes, e.g. 'semi-bilingual' or 'bilingualised' dictionaries such as Hornby's "(Oxford) Advanced Learner's Dictionary English-Chinese", which have been developed by translating existing monolingual dictionaries (see Marello 1998).





</doc>
<doc id="18486" url="https://en.wikipedia.org/wiki?curid=18486" title="Law enforcement">
Law enforcement

Law enforcement is the activity of some members of government who act in an organized manner to enforce the law by discovering, deterring, rehabilitating, or punishing people who violate the rules and norms governing that society. Although the term encompasses police, courts, and corrections, it is most frequently applied to those who directly engage in patrols or surveillance to dissuade and discover criminal activity, and those who investigate crimes and apprehend offenders, a task typically carried out by the police, sheriff or another law enforcement organization. 

Modern state legal codes use the term peace officer, or law enforcement officer to include every person vested by the legislating state with police power or authority, traditionally, anyone "sworn or badged,  who can arrest, or any public official authorized by statute, to detain, any person for a violation of criminal law, is included under the umbrella term of law enforcement. 

Although law enforcement may be most concerned with the prevention and punishment of crimes, organizations exist to discourage a wide variety of non-criminal violations of rules and norms, effected through the imposition of less severe consequences such as probation.

Most law enforcement is conducted by some type of law enforcement agency, with the most typical agency fulfilling this role being the police. Social investment in enforcement through such organizations can be massive, both in terms of the resources invested in the activity, and in the number of people professionally engaged to perform those functions.

Law enforcement agencies tend to be limited to operating within a specified jurisdiction. In some cases, jurisdiction may overlap in between organizations; for example, in the United States, each state has its own statewide law enforcement arms, but the Federal Bureau of Investigation is able to act against certain types of crimes occurring in any state. Various specialized segments of society may have their own internal law enforcement arrangements. For example, military organizations may have military police.



</doc>
<doc id="18488" url="https://en.wikipedia.org/wiki?curid=18488" title="Libido">
Libido

Libido (; colloquial: sex drive) is a person's overall sexual drive or desire for sexual activity. Libido is influenced by biological, psychological, and social factors. Biologically, the sex hormones and associated neurotransmitters that act upon the nucleus accumbens (primarily testosterone and dopamine, respectively) regulate libido in humans. Social factors, such as work and family, and internal psychological factors, such as personality and stress, can affect libido. Libido can also be affected by medical conditions, medications, lifestyle and relationship issues, and age (e.g., puberty). A person who has extremely frequent or a suddenly increased sex drive may be experiencing hypersexuality, while the opposite condition is hyposexuality.

A person may have a desire for sex, but not have the opportunity to act on that desire, or may on personal, moral or religious reasons refrain from acting on the urge. Psychologically, a person's urge can be repressed or sublimated. Conversely, a person can engage in sexual activity without an actual desire for it. Multiple factors affect human sex drive, including stress, illness, pregnancy, and others. A 2001 review found that, on average, men have a higher desire for sex than women.

Sexual desires are often an important factor in the formation and maintenance of intimate relationships in humans. A lack or loss of sexual desire can adversely affect relationships. Changes in the sexual desires of any partner in a sexual relationship, if sustained and unresolved, may cause problems in the relationship. The infidelity of a partner may be an indication that a partner's changing sexual desires can no longer be satisfied within the current relationship. Problems can arise from disparity of sexual desires between partners, or poor communication between partners of sexual needs and preferences.

Sigmund Freud, who is considered the originator of the modern use of the term, defined libido as "the energy, regarded as a quantitative magnitude... of those instincts which have to do with all that may be comprised under the word 'love'." It is the instinct energy or force, contained in what Freud called the id, the strictly unconscious structure of the psyche. He also explained that it is analogous to hunger, the will to power, and so on insisting that it is a fundamental instinct that is innate in all humans.

Freud developed the idea of a series of developmental phases in which the libido fixates on different erogenous zones—first in the oral stage (exemplified by an infant's pleasure in nursing), then in the anal stage (exemplified by a toddler's pleasure in controlling his or her bowels), then in the phallic stage, through a latency stage in which the libido is dormant, to its reemergence at puberty in the genital stage. (Karl Abraham would later add subdivisions in both oral and anal stages.)

Freud pointed out that these libidinal drives can conflict with the conventions of civilised behavior, represented in the psyche by the superego. It is this need to conform to society and control the libido that leads to tension and disturbance in the individual, prompting the use of ego defenses to dissipate the psychic energy of these unmet and mostly unconscious needs into other forms. Excessive use of ego defenses results in neurosis. A primary goal of psychoanalysis is to bring the drives of the id into consciousness, allowing them to be met directly and thus reducing the patient's reliance on ego defenses.

Freud viewed libido as passing through a series of developmental stages within the individual. Failure to adequately adapt to the demands of these different stages could result in libidinal energy becoming 'dammed up' or fixated in these stages, producing certain pathological character traits in adulthood. Thus the psychopathologized individual for Freud was an immature individual, and the goal of psychoanalysis was to bring these fixations to conscious awareness so that the libido energy would be freed up and available for conscious use in some sort of constructive sublimation.

According to Swiss psychiatrist Carl Gustav Jung, the libido is identified as the totality of psychic energy, not limited to sexual desire. As Jung states in "The Concept of Libido," "[libido] denotes a desire or impulse which is unchecked by any kind of authority, moral or otherwise. Libido is appetite in its natural state. From the genetic point of view it is bodily needs like hunger, thirst, sleep, and sex, and emotional states or affects, which constitute the essence of libido." The Duality (opposition) creates the energy (or libido) of the psyche, which Jung asserts expresses itself only through symbols: "It is the energy that manifests itself in the life process and is perceived subjectively as striving and desire." (Ellenberger, 697) These symbols may manifest as "fantasy-images" in the process of psychoanalysis which embody the contents of the libido, otherwise lacking in any definite form. Desire, conceived generally as a psychic longing, movement, displacement and structuring, manifests itself in definable forms which are apprehended through analysis.

Defined more narrowly, libido also refers to an individual's urge to engage in sexual activity, and its antonym is the force of destruction termed mortido or destrudo.

Libido is governed primarily by activity in the mesolimbic dopamine pathway (ventral tegmental area and nucleus accumbens). Consequently, dopamine and related trace amines (primarily phenethylamine) that modulate dopamine neurotransmission play a critical role in regulating libido.

Other neurotransmitters, neuropeptides, and sex hormones that affect sex drive by modulating activity in or acting upon this pathway include:

A woman's desire for sex is correlated to her menstrual cycle, with many women experiencing a heightened sexual desire in the several days immediately before ovulation, which is her peak fertility period, which normally occurs two days before until two days after the ovulation. This cycle has been associated with changes in a woman's testosterone levels during the menstrual cycle. According to Gabrielle Lichterman, testosterone levels have a direct impact on a woman's interest in sex. According to her, testosterone levels rise gradually from about the 24th day of a woman's menstrual cycle until ovulation on about the 14th day of the next cycle, and during this period the woman's desire for sex increases consistently. The 13th day is generally the day with the highest testosterone levels. In the week following ovulation, the testosterone level is the lowest and as a result women will experience less interest in sex.

Also, during the week following ovulation, progesterone levels increase, resulting in a woman experiencing difficulty achieving orgasm. Although the last days of the menstrual cycle are marked by a constant testosterone level, women's libido may get a boost as a result of the thickening of the uterine lining which stimulates nerve endings and makes a woman feel aroused. Also, during these days, estrogen levels decline, resulting in a decrease of natural lubrication.

Although some specialists disagree with this theory, menopause is still considered by the majority a factor that can cause decreased sex desire in women. The levels of estrogen decrease at menopause and this usually causes a lower interest in sex and vaginal dryness which makes intercourse painful. However, the levels of testosterone increase at menopause and this may be why some women may experience a contrary effect of an increased libido.

Certain psychological or social factors can reduce the desire for sex. These factors can include lack of privacy or intimacy, stress or fatigue, distraction or depression. Environmental stress, such as prolonged exposure to elevated sound levels or bright light, can also affect libido. Other causes include experience of sexual abuse, assault, trauma, or neglect, body image issues, and anxiety about engaging in sexual activity.

Individuals with PTSD may find themselves with reduced sexual desire. Struggling to find pleasure, as well as having trust issues, many with PTSD experience feelings of vulnerability, rage and anger, and emotional shutdowns, which have been shown to inhibit sexual desire in those with PTSD. Reduced sex drive may also be present in trauma victims due to issues arising in sexual function. For women, it has been found that treatment can improve sexual function, thus helping restore sexual desire. Depression and libido decline often coincide, with reduced sex drive being one of the symptoms of depression. Those suffering from depression often report the decline in libido to be far reaching and more noticeable than other symptoms. In addition, those with depression often are reluctant to report their reduced sex drive, often normalizing it with cultural/social values, or by the failure of the physician to inquire about it.

Physical factors that can affect libido include endocrine issues such as hypothyroidism, the effect of certain prescription medications (for example flutamide), and the attractiveness and biological fitness of one's partner, among various other lifestyle factors.

In males, the frequency of ejaculations affects the levels of serum testosterone, a hormone which promotes libido. A study of 28 males aged 21–45 found that all but one of them had a peak (145.7% of baseline [117.8%–197.3%]) in serum testosterone on the 7th day of abstinence from ejaculation.

Anemia is a cause of lack of libido in women due to the loss of iron during the period.

Smoking, alcohol abuse, and the use of certain drugs can also lead to a decreased libido. Moreover, specialists suggest that several lifestyle changes such as exercising, quitting smoking, lowering consumption of alcohol or using prescription drugs may help increase one's sexual desire.

Some people purposefully attempt to decrease their libido through the usage of anaphrodisiacs. Aphrodisiacs, such as dopaminergic psychostimulants, are a class of drugs which can increase libido. On the other hand, a reduced libido is also often iatrogenic and can be caused by many medications, such as hormonal contraception, SSRIs and other antidepressants, antipsychotics, opioids and beta blockers.

Many SSRIs can cause a long term decrease in libido and other sexual functions, even after users of those drugs have shown improvement in their depression and have stopped usage. Multiple studies have shown that with the exception of bupropion (Wellbutrin), trazodone (Desyrel) and nefazodone (Serzone), antidepressants generally will lead to lowered libido. SSRIs that typically lead to decreased libido are fluoxetine (Prozac), paroxetine (Paxil), fluvoxamine (Luvox), citalopram (Celexa) and sertraline (Zoloft). There are several ways to try and reap the benefits of the antidepressants while maintaining high enough sex drive levels. Some antidepressant users have tried decreasing their dosage in the hopes of maintaining an adequate sex drive. Results of this are often positive, with both drug effectiveness not reduced and libido preserved. Other users try enrolling in psychotherapy to solve depression-related issues of libido. However, the effectiveness of this therapy is mixed, with lots reporting that it had no or little effect on sexual drive.

Testosterone is one of the hormones controlling libido in human beings. Emerging research is showing that hormonal contraception methods like oral contraceptive pills (which rely on estrogen and progesterone together) are causing low libido in females by elevating levels of sex hormone binding globulin (SHBG). SHBG binds to sex hormones, including testosterone, rendering them unavailable. Research is showing that even after ending a hormonal contraceptive method, SHBG levels remain elevated and no reliable data exists to predict when this phenomenon will diminish.

Oral contraceptives lower androgen levels in users, and lowered androgen levels generally lead to a decrease in sexual desire. However, usage of oral contraceptives has shown to typically not have a connection with lowered libido in women. Multiple studies have shown that usage of oral contraceptives is associated with either a small increase or decrease in libido, with most users reporting a stable sex drive.

Males reach the peak of their sex drive in their teenage years, while females reach it in their thirties. The surge in testosterone hits the male at puberty resulting in a sudden and extreme sex drive which reaches its peak at age 15–16, then drops slowly over his lifetime. In contrast, a female's libido increases slowly during adolescence and peaks in her mid-thirties.
Actual testosterone and estrogen levels that affect a person's sex drive vary considerably.

Some boys and girls will start expressing romantic or sexual interest by age 10–12. The romantic feelings are not necessarily sexual, but are more associated with attraction and desire for another. For boys and girls in their preteen years (ages 11–12), at least 25% report "thinking a lot about sex". By the early teenage years (ages 13–14), however, boys are much more likely to have sexual fantasies than girls. In addition, boys are much more likely to report an interest in sexual intercourse at this age than girls. Masturbation among youth is common, with prevalence among the population generally increasing until the late 20s and early 30s. Boys generally start masturbating earlier, with less than 10% boys masturbating around age 10, around half participating by age 11–12, and over a substantial majority by age 13–14. This is in sharp contrast to girls where virtually none are engaging in masturbation before age 13, and only around 20% by age 13–14.

People in their 60s and early 70s generally retain a healthy sex drive, but this may start to decline in the early to mid-70s. Older adults generally develop a reduced libido due to declining health and environmental or social factors. In contrast to common belief, postmenopausal women often report an increase in sexual desire and an increased willingness to satisfy their partner. Women often report family responsibilities, health, relationship problems, and well-being as inhibitors to their sexual desires. Aging adults often have more positive attitudes towards sex in older age due to being more relaxed about it, freedom from other responsibilities, and increased self-confidence. Those exhibiting negative attitudes generally cite health as one of the main reasons. Stereotypes about aging adults and sexuality often regard seniors as asexual beings, doing them no favors when they try to talk about sexual interest with caregivers and medical professionals. Non-western cultures often follow a narrative of older women having a much lower libido, thus not encouraging any sort of sexual behavior for women. Residence in retirement homes has affects on residents libidos. In these homes, sex occurs, but it is not encouraged by the staff or other residents. Lack of privacy and resident gender imbalance are the main factors lowering desire. Generally, for older adults, being excited about sex, good health, sexual self-esteem and having a sexually talented partner.

There is no widely accepted measure of what is a healthy level for sex desire. Some people want to have sex every day, or more than once a day; others once a year or not at all. However, a person who lacks a desire for sexual activity for some period of time may be experiencing a hypoactive sexual desire disorder or may be asexual.

A sexual desire disorder is more common in women than in men, and women tend to exhibit less frequent and less intense sexual desires than men. Erectile dysfunction may happen to the penis because of lack of sexual desire, but these two should not be confused. For example, large recreational doses of amphetamine or methamphetamine can simultaneously cause erectile dysfunction and significantly increase libido. However, men can also experience a decrease in their libido as they age.

The American Medical Association has estimated that several million US women suffer from a female sexual arousal disorder, though arousal is not at all synonymous with desire, so this finding is of limited relevance to the discussion of libido. Some specialists claim that women may experience low libido due to some hormonal abnormalities such as lack of luteinising hormone or androgenic hormones, although these theories are still controversial. Also, women commonly lack sexual desire in the period immediately after giving birth. Moreover, any condition affecting the genital area can make women reject the idea of having intercourse. It has been estimated that half of women experience different health problems in the area of the vagina and vulva, such as thinning, tightening, dryness or atrophy. Frustration may appear as a result of these issues and because many of them lead to painful sexual intercourse, many women prefer not having sex at all. Surgery or major health conditions such as arthritis, cancer, diabetes, high blood pressure, coronary artery disease or infertility may have the same effect in women. Surgery that affects the hormonal levels in women include oophorectomies.



</doc>
<doc id="18490" url="https://en.wikipedia.org/wiki?curid=18490" title="Larissa">
Larissa

Larissa (; , , ) is the capital and largest city of the Thessaly region in Greece. It is the fourth-most populous city in Greece with a population of 144,651 according to the 2011 census (181,713 est. 2018). It is also capital of the Larissa regional unit. It is a principal agricultural centre and a national transport hub, linked by road and rail with the port of Volos, the cities of Thessaloniki and Athens. The municipality of Larissa has 162,591 inhabitants, while the regional unit of Larissa reached a population of 284,325 ().

Legend has it that Achilles was born here. Hippocrates, the "Father of Medicine", died here. Today, Larissa is an important commercial, transportation, educational, agricultural and industrial centre of Greece.

There are a number of highways including E75 and the main railway from Athens to Thessaloniki (Salonika) crossing through Thessaly. The region is directly linked to the rest of Europe through the International Airport of Central Greece located in Nea Anchialos a short distance from Larissa (about 60 km). Larissa lies on the river Pineios.

The municipality Larissa has an area of , the municipal unit Larissa has an area of , and the community Larissa has an area of .

The Larissa Chasma, a deep gash in the surface of Dione, a natural satellite of Saturn, was named after Larissa.

The climate of Larissa is semi-arid in the cool version (Köppen: "BSk") but it is close to a hot summer Mediterranean climate ("Csa"). The winter is fairly mild, and some snowstorms may occur. The summer is particularly hot, and temperatures of may occur. Thunderstorms or heavy rain may cause agricultural damage. Larissa receives of rain per year.

According to Greek mythology it is said that the city was founded by Acrisius, who was killed accidentally by his grandson, Perseus. There lived Peleus, the hero beloved by the gods, and his son Achilles.

In mythology, the nymph Larissa was a daughter of the primordial man Pelasgus.

The city of Larissa is mentioned in Book II of "Iliad" by Homer:

"Hippothous led the tribes of Pelasgian spearsmen, who dwelt in fertile Larissa- Hippothous, and Pylaeus of the race of Mars, two sons of the Pelasgian Lethus, son of Teutamus." 

In this paragraph, Homer shows that the Pelasgians, Trojan allies, used to live in the city of Larissa. It is likely that this city of Larissa was different to the city that was the birthplace of Achilles. The Larissa that features as a Trojan ally in the "Iliad" was likely to be located in the Troad, on the other side of the Aegean Sea.

Traces of Paleolithic human settlement have been recovered from the area, but it was peripheral to areas of advanced culture. The area around Larissa was extremely fruitful; it was agriculturally important and in antiquity was known for its horses.

The name Larissa (Λάρισα "Lárīsa") is in origin a Pelasgian word for "fortress". There were many ancient Greek cities with this name.
The name of Thessalian Larissa is first recorded in connection with the aristocratic Aleuadai family. It was also a polis (city-state).

Larissa was a polis (city-state) during the Classical Era. Larissa is thought to be where the famous Greek physician Hippocrates and the famous philosopher Gorgias of Leontini died.
When Larissa ceased minting the federal coins it shared with other Thessalian towns and adopted its own coinage in the late 5th century BC, it chose local types for its coins. The obverse depicted the nymph of the local spring, Larissa, for whom the town was named; probably the choice was inspired by the famous coins of Kimon depicting the Syracusan nymph Arethusa. The reverse depicted a horse in various poses. The horse was an appropriate symbol of Thessaly, a land of plains, which was well known for its horses. Usually there is a male figure; he should perhaps be seen as the eponymous hero of the Thessalians, Thessalos, who is probably also to be identified on many of the earlier, federal coins of Thessaly.

Larissa, sometimes written Larisa on ancient coins and inscriptions, is near the site of the Homeric Argissa. It appears in early times, when Thessaly was mainly governed by a few aristocratic families, as an important city under the rule of the Aleuadae, whose authority extended over the whole district of Pelasgiotis. This powerful family possessed for many generations before 369 BC the privilege of furnishing the "tagus", the local term for the "strategos" of the combined Thessalian forces. The principal rivals of the Aleuadae were the Scopadae of Crannon, the remains of which are about 14 miles south west.

Larissa was the birthplace of Meno, who thus became, along with Xenophon and a few others, one of the generals leading several thousands Greeks from various places, in the ill-fated expedition of 401 (retold in Xenophon's "Anabasis") meant to help Cyrus the Younger, son of Darius II, king of Persia, overthrow his elder brother Artaxerxes II and take over the throne of Persia (Meno is featured in Plato's dialogue bearing his name, in which Socrates uses the example of ""the way to Larissa"" to help explain Meno the difference between true opinion and science (Meno, 97a–c); this "way to Larissa" might well be on the part of Socrates an attempt to call to Meno's mind a "way home", understood as the way toward one's true and "eternal" home reached only at death, that each man is supposed to seek in his life).

The constitution of the town was democratic, which explains why it sided with Athens in the Peloponnesian War. In the neighbourhood of Larissa was celebrated a festival which recalled the Roman Saturnalia, and at which the slaves were waited on by their masters. As the chief city of ancient Thessaly, Larissa was taken by the Thebans and later directly annexed by Philip II of Macedon in 344. It remained under Macedonian control afterwards, except for a brief period when Demetrius Poliorcetes captured it in 302 BC.

It was in Larissa that Philip V of Macedon signed in 197 BC a treaty with the Romans after his defeat at the Battle of Cynoscephalae, and it was there also that Antiochus III the Great, won a great victory in 192 BC. In 196 BC Larissa became an ally of Rome and was the headquarters of the Thessalian League.

Larissa is frequently mentioned in connection with the Roman civil wars which preceded the establishment of the Roman Empire and Pompey sought refuge there after the defeat of Pharsalus.

Larissa was sacked by the Ostrogoths in the late 5th century, and rebuilt under the Byzantine emperor Justinian I.

In the 8th century, the city became the metropolis of the theme of Hellas. The city was captured in 986 by Tsar Samuel of Bulgaria, who carried off the relics of its patron saint, Saint Achilleios, to Prespa. It was again unsuccessfully besieged by the Italo-Normans under Bohemond I in 1082/3.

After the Fourth Crusade, the King of Thessalonica, Boniface of Montferrat, gave the city to Lombard barons, but they launched a rebellion in 1209 that had to be subdued by the Latin Emperor Henry of Flanders himself. The city was recovered by Epirus soon after.

It was conquered by the Ottoman Empire in 1386/87 and again in the 1390s, but only came under permanent Ottoman control in 1423, by Turahan Bey. Under Ottoman rule, the city was known as "Yeni-şehir i-Fenari", "new citadel". As the chief town and military base of Ottoman Thessaly, Larissa was a predominantly Muslim city. During Ottoman rule the administration of the Metropolis of Larissa was transferred to nearby Trikala where it remained until 1734, when Metropolitan Iakovos II returned the see from Trikala to Larissa and established the present-day metropolis of Larissa and Tyrnavos.

The town was noted for its trade fair in the 17th and 18th centuries, while the seat of the pasha of Thessaly was also transferred there in 1770. Larissa was the headquarters of Hursid Pasha during the Greek War of Independence. It was also renowned for its mosques (four of which were still in use in the late 19th century) and its muslim cemeteries.

The city remained in Ottoman hands until Thessaly became part of the independent Kingdom of Greece in 1881, except for a period where Ottoman forces re-occupied it during the Greco-Turkish War of 1897. In the late 19th century, there was still a small village in the outskirts of the town inhabited by Africans from Sudan, a curious remnant of the forces collected by Ali Pasha.

In the 19th century, the town produced leather, cotton, silk and tobacco. Fevers and agues were prevalent owing to bad drainage and the overflowing of the river; and the death rate was higher than the birth rate.

In 1881, the city, along with the rest of Thessaly, was incorporated into the Kingdom of Greece during the prime ministry of Alexandros Koumoundouros. On 31 August 1881 a unit of the Greek Army headed by General Skarlatos Soutsos entered the city. A considerable portion of the Turkish population emigrated into the Ottoman Empire at that point. In this new era the city starts gradually to expand and to be rebuilt by the Greek authorities.

During the Greco-Turkish War of 1897, the city was the headquarters of Greek Crown Prince Constantine. The flight of the Greek army from here to Farsala took place on April 23, 1897. Turkish troops entered the city two days later. After a treaty for peace was signed, they withdrew and Larissa remained permanently in Greece. This was followed by a further exodus of Turks in 1898. The Hassan Bey mosque (which was built in the early 16th century) was demolished in 1908.

During the Axis Occupation of the country, the Jewish community of the city (dated back to 2nd BC, see Romaniotes) suffered heavy losses. Today in the city there is a Holocaust memorial and a synagogue.

After WWII the city was expanded rapidly. Today Larissa is the fourth largest Greek city with many squares, taverns and cafes. It has three public hospitals with one being a military hospital. It hosts the Hellenic Air Force Headquarters and NATO Headquarters in Greece. It has a School of Medicine and a School of Biochemistry – Biotechnology and the third largest in the country Institute of Technology. It occupies the first place among Greek cities into green coverage rate per square-metre urban space and the first place with the highest percentance of bars-taverns-restaurants per capita in Greece. It also has two public libraries and five museums.

Christianity penetrated early to Larissa, though its first bishop is recorded only in 325 at the Council of Nicaea. St. Achillius of the 4th century, is celebrated for his miracles. Le Quien cites twenty-nine bishops from the fourth to the 18th centuries; the most famous is Jeremias II, who occupied the see until 733, when the Emperor Leo III the Isaurian transferred it from the jurisdiction of the Pope of Rome to the Patriarchate of Constantinople. In the first years of the 10th century it had ten suffragan sees; subsequently the number increased and about the year 1175 under the Emperor Manuel I Comnenus, it reached twenty-eight. At the close of the 15th century, under the Ottoman domination, there were only ten suffragan sees, which gradually grew less and finally disappeared.

Larissa is an Orthodox Metropolis of the Church of Greece.

It was also briefly a Latin archbishopric in the early 13th century, and remains a Latin Metropolitan (top-ranking) titular see of the Roman Catholic Church, which must not be confused with the Latin episcopal (low-ranking) titular see Larissa in Syria. Today there is a Catholic church in the city (Sacred Heart of Jesus).

The municipality Larissa was formed at the 2011 local government reform by the merger of the following 3 former municipalities, that became municipal units:

The municipal unit of Larissa is divided into four city-districts or municipal communities (29 city areas) plus 2 suburban communities (Amphithea and Koulourion). The municipality includes also the Community of Terpsithèa (with the suburban community of Argyssa).
1st Municipal District

2nd Municipal District

3rd Municipal District

4th Municipal District

Community of Terpsithèa
From 1 January 2011, in accordance with the Kallikratis Plan (new administrative division of Greece), the new municipality of Larissa includes also the former municipalities of Giannouli and Koilada.

The province of Larissa () was one of the provinces of the Larissa Prefecture. Its territory corresponded with that of the current municipalities Larissa (except the municipal unit Giannouli) and Tempi (except the municipal units Gonnoi and Kato Olympos). It was abolished in 2006.


Larissa is a major agricultural center of Greece, due to the plain of Thessaly.

In manufacturing sector, Larissa is among others home to Biokarpet carpet company (whose owners were also major shareholders of AEL FC in the past) and Orient Bikes.

It comes also in first place with the highest percentance of bars-taverns-restaurants per capita in Greece. Mikel Coffee Company chain started and has its base in the city.



Some historical buildings that have been listed as architecturally preserved, include the Cine Palace (architect Colonello), the Charokopos tower (arch. Anastasios Metaxas, endangered to collapse as 2019), the old Mills of Pappas, such as the complex of the Averofeios Agricultural School.

Local specialities:




Among the notable festivals that the city hosts, is the "Pineiou Festival" (music) and "AgroThessaly", a major agricultural fair.


Larissa sits in the middle of the plain of Thessaly, with connections to Motorway A1 and national roads EO3 and EO6.

The city is in close proximity of many interesting destinations in the region (Mount Olympus, Mount Kissavos, Meteora, Lake Plastira, Pilio, etc.) suitable for daily trips.

The local football club AEL FC currently participates in Superleague Greece. The team won the Greek Championship, in 1988, and won the Greek Cup in 1985 and 2007. These titles place AEL among the five most important football clubs in Greece. AEL has hosted its home games at the AEL FC Arena, a UEFA 3-star-rated football ground, since November 2010.

Other important sport venues are the "National Sport Center of Larissa" (EAK Larissas), which includes the Alcazar Stadium and the Neapoli Indoor Hall.

The National Sports Center of Larissa can accommodate a number of sports and events (football, basketball, wrestling, swimming, boxing, martial arts, handball, water polo, etc.), while the Sports Hall has hosted important athletic events (the 1995 FIBA Under-19 World Cup, the 1997 Women's EuroLeague Final Four, the 2003 Greek Basketball Cup Final Four, martial arts events, etc.), and it is also used for cultural events, such as dance festivals.





Larissa is twinned with:






</doc>
<doc id="18491" url="https://en.wikipedia.org/wiki?curid=18491" title="Lead and follow">
Lead and follow

In some types of partner dance, lead and follow are designations for the two dancers' roles in a dance pairing. The leader is responsible for guiding the couple and initiating transitions to different dance steps and, in improvised dances, for choosing the dance steps to perform. The leader communicates choices to the follower, and directs the follower by means of subtle physical and visual signals, thereby allowing the pair to be smoothly coordinated.

The amount of direction given by the leader depends on several factors, including dance style, social context of the dance, and experience and personalities of the dancers.

Traditionally, the male dance partner is the leader and the female dance partner is the follower, though this is not always the case, such as in Schottische danced in the Madrid style where women lead and men follow (although this is not totally true: during the dance there is an exchange of roles, the leader becomes the follower and vice versa.). Many social dance forms have a long history of same-sex (e.g. tango) and role-crossing partnerships, and there have been some changes to the strict gendering of partner dances in some competition or performance contexts. An example is a "Jack and Jack" dance contest.

Partner dancing requires awareness and clear communication; this is essential both for safety and for the overall success of the dance. If following in the dance, it helps to maintain a centered readiness to the leader. This helps the follower be ready for cues both visually and physically. The leader in the dance will best support the follower by giving clear directions.

For the leader and follower to interact with each other, communication needs to occur between the dance couple. Dancers take cues through physical connection, with the follower using it to communicate feedback to the leader just as the leader uses it to suggest moves to the follower. The most accomplished dancers use connection as a line of communication which allows the leader to incorporate the follower's ideas, abilities, and creative suggestions into their own styling and selection of moves.

In many partner dances, the leader's steps differ from the follower's. In face-to-face positions, the follower generally "mirrors" the leader's footwork. For example, if the leader begins on their left foot, the follower will begin on their right foot. In choreographed pieces and other situations where the follower is in a tandem position or shadow position, the leader and follower will use the same footwork. Usually both partners move together as a unit, but in some dances the partners move in opposite directions - together and apart again.

In partner dancing, dancers seek to work together to create synchronized or complementary movements. The leader is largely responsible for "initiating" movement, whereas the follower's role is to "maintain" this movement (though they may choose not to). This process can be described as involving the initiation of momentum or 'energy' (by the leader) and then the subsequent maintenance, exaggeration, decreasing or dissolving of this momentum by both partners.
This momentum or energy may be manifested as movement (in its most obvious form), or in a range of more complex interactions between partners:

It is also helpful for dancers to regard their partners in terms of their "points of balance" to help the leader initiate movements for their follower. These points of balance include the front-facing side of the shoulders, the front facing side of the hips, and the follower's center (the abdomen). If the leader wants to bring the follower close, the leader is to apply tension and draw the hand in and down toward the leader's own hip; to send the follower away, the leader would guide the hand toward the follower and add compression, signaling the move away.

A general rule is that both leader and follower watch each other's back in a dance hall situation. Collision avoidance is one of the cases when the follower is required to "backlead" or at least to communicate about the danger to the leader. In travelling dances, such as waltz, common follower signals of danger are an unusual resistance to the leader, or a slight tap by the shoulder. In open-position dances, such as swing or Latin dances, maintaining eye contact with the partner is an important safety communication link.

For partner dancers, using weight transfers is a way for a leader to communicate a 'lead' for a dance step to a follower.

In another example, for a leader to have their follower walk forwards while connected, the leader begins by taking his or her center back, indicating a backward walking move. As the partners' arms/points of contact move away from each other, they develop tension, which the follower may either break by dropping their arms or breaking the hold, or 'follow' by moving.

A more experienced leader may realize (if only on an unconscious level) that the most effective execution of even this "simple" step is achieved by preparing for movement before the step begins.

The leader-follower connection facilitates this. The principles of leading and following are explored in contact improvisation of modern dance.

Sometimes a miscommunication will occur between the leader and follower. Techniques of the recovery of connection and synchronization vary from dance to dance, but below are a few common examples.



A body lead occurs where the leader initiates a lead by moving their body, which moves their arm(s), and thus transmits a lead to the follower. 'Body lead' means much the same as 'weight transfer'.
An arm lead occurs where the leader moves their arm(s) without moving their body, or moves their body in a different direction to their arm.
While an 'arm lead' without the transfer of weight (or movement of the body) on the part of the leader is often a marker of an inexperienced or poorly taught dancer, the process of leading and following, particularly at an advanced level, often involves the contrasting uses of weight transfers and 'arm moves'. As an example, a leader may lead a follower back onto their right foot through the leader's own weight transfer forwards onto their left foot; yet at the same time turn the follower's torso to the left from above the hips.

The leader has to communicate the direction of the movement to the follower. Traditionally, the leader's right hand is on the follower's back, near the lowest part of the shoulder-blade. This is the strongest part of the back and the leader can easily pull the follower's body inwards. To enable the leader to communicate a step forward (backward for the follower) the follower has to constantly put a little weight against the leader's right hand. When the leader goes forward, the follower will naturally go backwards.

An important leading mechanism is the leader's left hand, which usually holds the follower's right hand. At no point should it be necessary for any partner to firmly grab the other's hand. It is sufficient to press the hand or even only finger tips slightly against each other, the follower's hand following the leader's hand.

Another important leading mechanism is hip contact. Though not possible in traditional Latin dances like Rumba, Cha-cha, Tango Argentino because of partner separation, hip contact is a harmonious and sensual way of communicating movement to the partner, used primarily in Standard or Ballroom dances (English / slow waltz, European tango, quickstep etc.) and Caribbean dances.


Backleading is when a follower is executing steps without waiting for, or contrary to the lead's lead. Both are considered bad dancing habits because it makes the follower difficult to lead and dance with.

Backleading can be a teaching tool that is often used intentionally by an instructor when dancing with a student lead, in order to help them learn the desired technique.

Backleading sounds similar to "hijacking", and indeed it is often used in place of "hijacking". However the two terms have significant differences, stemming from intentions. The first difference is superficial; hijacking is usually an occasional "outburst" from the follower, who otherwise diligently follows the lead, while a "backlead" may refer to a consistent habit. The second difference is more significant; hijacking is an actual reversal of roles, meaning that the hijacker leads the leader and takes control of the dance, while backleading only takes care of the follower.

Sometimes the follower steals the lead and the couple reverses roles for some time. This is called "hijacking" (also known as "lead stealing"). Hijacking requires experience and good connection, since without proper timing it may look like sloppy dancing. A signal for hijacking is typically an unusually changed (mostly, increased) stress in the connection from the follower's side. "Unusually" meaning more than typically required for the execution of the current step (by these partners). For a follower to hijack, they must be sure that the lead will understand or at least guess the follower's intentions.



</doc>
<doc id="18492" url="https://en.wikipedia.org/wiki?curid=18492" title="Lexeme">
Lexeme

A lexeme () is a unit of lexical meaning that underlies a set of words that are related through inflection. It is a basic abstract unit of meaning, a unit of morphological analysis in linguistics that roughly corresponds to a set of forms taken by a single root word. For example, in English, "run", "runs", "ran" and "running" are forms of the same lexeme, which can be represented as RUN.

One form, the lemma (or citation form), is chosen by convention as the canonical form of a lexeme. The lemma is the form used in dictionaries as an entry's headword. Other forms of a lexeme are often listed later in the entry if they are uncommon or irregularly inflected.

The notion of the lexeme is central to morphology, the basis for defining other concepts in that field. For example, the difference between inflection and derivation can be stated in terms of lexemes:

A lexeme belongs to a particular syntactic category, has a certain meaning (semantic value) and, in inflecting languages, has a corresponding inflectional paradigm. That is, a lexeme in many languages will have many different forms. For example, the lexeme RUN has a present third person singular form "runs", a present non-third-person singular form "run" (which also functions as the past participle and non-finite form), a past form "ran", and a present participle "running". (It does not include "runner, runners, runnable" etc.) The use of the forms of a lexeme is governed by rules of grammar. In the case of English verbs such as RUN, they include subject-verb agreement and compound tense rules, which determine the form of a verb that can be used in a given sentence.

In many formal theories of language, lexemes have subcategorization frames to account for the number and types of complements. They occur within sentences and other syntactic structures.

A language's lexemes are often composed of smaller units with individual meaning called morphemes, according to root morpheme + derivational morphemes + suffix (not necessarily in that order), where:

The compound root morpheme + derivational morphemes is often called the stem. The decomposition stem + desinence can then be used to study inflection.



</doc>
<doc id="18494" url="https://en.wikipedia.org/wiki?curid=18494" title="Lord's Prayer">
Lord's Prayer

The Lord's Prayer, also called the Our Father (), is a central Christian prayer which, according to the New Testament, Jesus taught as the way to pray:
Two versions of this prayer are recorded in the gospels: a longer form within the Sermon on the Mount in the Gospel of Matthew, and a shorter form in the Gospel of Luke when "one of his disciples said to him, 'Lord, teach us to pray, as John taught his disciples. ( NRSV). Lutheran theologian Harold Buls suggested that both were original, the Matthean version spoken by Jesus early in his ministry in Galilee, and the Lucan version one year later, "very likely in Judea".

The first three of the seven petitions in Matthew address God; the other four are related to human needs and concerns. The Matthew account alone includes the "Your will be done" and the "Rescue us from the evil one" (or "Deliver us from evil") petitions. Both original Greek texts contain the adjective "epiousios", which does not appear in any other classical or Koine Greek literature; while controversial, "daily" has been the most common English-language translation of this word. Protestants usually conclude the prayer with a doxology, a later addendum appearing in some manuscripts of Matthew.

Initial words on the topic from the "Catechism of the Catholic Church" teach that it "is truly the summary of the whole gospel". The prayer is used by most Christian churches in their worship; with few exceptions, the liturgical form is the Matthean. Although theological differences and various modes of worship divide Christians, according to Fuller Seminary professor Clayton Schmit, "there is a sense of solidarity in knowing that Christians around the globe are praying together ... and these words always unite us."

In biblical criticism, the prayer's absence in the Gospel of Mark together with its occurrence in Matthew and Luke has caused scholars who accept the two-source hypothesis (against other document hypotheses) to conclude that it is probably a "logion" original to Q.

Standard edition of Greek text

<br>
"(pater hēmōn ho en tois ouranois)"

<br>
"(hagiasthētō to onoma sou)"

<br>
"(elthetō hē basileia sou)"

<br>
"(genēthētō to thelēma sou hōs en ouranō(i) kai epi gēs)"

<br>
"(ton arton hēmōn ton epiousion dos hēmin sēmeron)"

<br>
"(kai aphes hēmin ta opheilēmata hēmōn hōs kai hēmeis aphēkamen tois opheiletais hēmōn)"

<br>
"(kai mē eisenegkēs hēmas eis peirasmon alla rhusai hēmas apo tou ponērou)"

Patriarchal Edition 1904

,<br>
,<br>
,<br>
.<br>
<br>
.<br>
.<br>
ὅτι σοῦ ἐστιν ἡ βασιλεία καὶ ἡ δύναμις καὶ ἡ δόξα εἰς τοὺς αἰῶνας ἀμήν.

Roman Missal

There are several different English translations of the Lord's Prayer from Greek or Latin, beginning around AD 650 with the Northumbrian translation. Of those in current liturgical use, the three best-known are:
The square brackets in two of the texts below indicate the doxology often added at the end of the prayer by Protestants and, in a slightly different form, by the Byzantine Rite ("For thine is the kingdom and the power and the glory: of the Father, and of the Son, and of the Holy Spirit, now and ever, and unto ages of ages. Amen."), among whom the prayer proper is usually recited by the cantors and congregation in unison, and the doxology by the priest as the conclusion of the prayer. The 1662 Book of Common Prayer (BCP) of the Church of England adds it in some services, but not in all. For example, the doxology is not used in the 1662 BCP at Morning and Evening Prayer when it is preceded by the Kyrie eleison. Older English translations of the Bible, based on late Byzantine Greek manuscripts, included it, but it is excluded in critical editions of the New Testament, such as that of the United Bible Societies. It is absent in the oldest manuscripts and is not considered to be part of the original text of –.

Latin Rite Roman Catholic usage has never attached the doxology to the Lord's Prayer. The doxology does appear in the Roman Rite Mass as revised in 1969. After the conclusion of the Lord's Prayer, the priest says a prayer known as the embolism. In the official ICEL English translantion, the embolism reads: "Deliver us, Lord, we pray, from every evil, graciously grant peace in our days, that, by the help of your mercy, we may be always free from sin and safe from all distress, as we await the blessed hope and the coming of our Saviour, Jesus Christ." This elaborates on the final petition, "Deliver us from evil." The people then respond to this with the doxology: "For the kingdom, the power, and the glory are yours, now and forever."

The translators of the 1611 King James Bible assumed that a Greek manuscript they possessed was ancient and therefore adopted the phrase "For thine is the kingdom, the power, and the glory forever" into the Lord's Prayer of Matthew's Gospel. However, the use of the doxology in English dates from at least 1549 with the First Prayer Book of Edward VI which was influenced by William Tyndale's New Testament translation in 1526. Later scholarship demonstrated that inclusion of the doxology in New Testament manuscripts was actually a later addition based in part on Eastern liturgical tradition.

Authorized Version (known also as the King James Version)

Although uses the term "debts", most older English versions of the Lord's Prayer use the term "trespasses", while ecumenical versions often use the term "sins". The latter choice may be due to , which uses the word "sins", while the former may be due to (immediately after the text of the prayer), where Jesus speaks of "trespasses". As early as the third century, Origen of Alexandria used the word "trespasses" () in the prayer. Although the Latin form that was traditionally used in Western Europe has "debita" ("debts"), most English-speaking Christians (except Scottish Presbyterians and some others of the Dutch Reformed tradition) use "trespasses". For example, the Church of Scotland, the Presbyterian Church (U.S.A.), the Reformed Church in America, as well as some Congregational heritage churches in the United Church of Christ follow the version found in Matthew 6 in the Authorized Version (known also as the King James Version), which in the prayer uses the words "debts" and "debtors".

All these versions are based on the text in Matthew, rather than Luke, of the prayer given by Jesus:

St. Augustine gives the following analysis of the Lord's Prayer, which elaborates on Jesus' words just before it in Matthew's gospel: "Your Father knows what you need before you ask him. Pray then in this way" (Mt. 6:8-9):We need to use words (when we pray) so that we may remind ourselves to consider carefully what we are asking, not so that we may think we can instruct the Lord or prevail on him.

When we say: "Hallowed be your name," we are reminding ourselves to desire that his name, which in fact is always holy, should also be considered holy among men. ...But this is a help for men, not for God. ...And as for our saying: "Your kingdom come," it will surely come whether we will it or not. But we are stirring up our desires for the kingdom so that it can come to us and we can deserve to reign there. ...When we say: "Deliver us from evil," we are reminding ourselves to reflect on the fact that we do not yet enjoy the state of blessedness in which we shall suffer no evil. ...It was very appropriate that all these truths should be entrusted to us to remember in these very words. Whatever be the other words we may prefer to say (words which the one praying chooses so that his disposition may become clearer to himself or which he simply adopts so that his disposition may be intensified), we say nothing that is not contained in the Lord’s Prayer, provided of course we are praying in a correct and proper way. This excerpt from Augustine is included in the Office of Readings in the Catholic Liturgy of the Hours.

Many have written Biblical commentaries on the Lord's prayer. Contained below are a variety of selections from some of those commentaries.

"Our" indicates that the prayer is that of a group of people who consider themselves children of God and who call God their "Father". "In heaven" indicates that the Father who is addressed is distinct from human fathers on earth.

Augustine interpreted "heaven" ("coelum", sky) in this context as meaning "in the hearts of the righteous, as it were in His holy temple".

Former Archbishop of Canterbury Rowan Williams explains this phrase as a petition that people may look upon God's name as holy, as something that inspires awe and reverence, and that they may not trivialize it by making God a tool for their purposes, to "put other people down, or as a sort of magic to make themselves feel safe". He sums up the meaning of the phrase by saying: "Understand what you're talking about when you're talking about God, this is serious, this is the most wonderful and frightening reality that we could imagine, more wonderful and frightening than we can imagine."

"This petition has its parallel in the Jewish prayer, 'May he establish his Kingdom during your life and during your days.'" In the gospels Jesus speaks frequently of God's kingdom, but never defines the concept: "He assumed this was a concept so familiar that it did not require definition." Concerning how Jesus' audience in the gospels would have understood him, G. E. Ladd turns to the concept's Hebrew Biblical background: "The Hebrew word "malkuth" […] refers first to a reign, dominion, or rule and only secondarily to the realm over which a reign is exercised. […] When "malkuth" is used of God, it almost always refers to his authority or to his rule as the heavenly King." This petition looks to the perfect establishment of God's rule in the world in the future, an act of God resulting in the eschatological order of the new age.

Some see the coming of God's kingdom as a divine gift to be prayed for, not a human achievement. Others believe that the Kingdom will be fostered by the hands of those faithful who work for a better world. These believe that Jesus' commands to feed the hungry and clothe the needy make the seeds of the kingdom already present on earth (Lk 8:5-15; Mt 25:31-40).

Hilda C. Graef notes that the operative Greek word, basileia, means both kingdom and kingship (i.e., reign, dominion, governing, etc.), but that the English word kingdom loses this double meaning. Kingship adds a psychological meaning to the petition: one is also praying for the condition of soul where one follows God's will.

According to William Barclay, this phrase is a couplet with the same meaning as "Thy kingdom come." Barclay argues: "The kingdom is a state of things on earth in which God's will is as perfectly done as it is in heaven. ...To do the will of God and to be in the Kingdom of God are one and the same thing."

John Ortberg interprets this phrase as follows: "Many people think our job is to get my afterlife destination taken care of, then tread water till we all get ejected and God comes back and torches this place. But Jesus never told anybody – neither his disciples nor us – to pray, 'Get me out of here so I can go up there.' His prayer was, 'Make up there come down here.' Make things down here run the way they do up there." The request that "thy will be done" is God's invitation to "join him in making things down here the way they are up there."

As mentioned earlier in this article, the original word ("epiousios"), commonly characterized as "daily", is unique to the Lord's Prayer in all of ancient Greek literature. The word is almost a "hapax legomenon", occurring only in Luke and Matthew's versions of the Lord's Prayer, and nowhere else in any other extant Greek texts. While "epiousios" is often substituted by the word "daily," all other New Testament translations from the Greek into "daily" otherwise reference "hemeran" (ἡμέραν, "the day"), which does not appear in this usage.

Via linguistic parsing, Jerome translated "ἐπιούσιον" ("epiousios") as ""supersubstantialem"" in the Gospel of Matthew, but chose ""cotidianum"" ("daily") in the Gospel of Luke. This wide-ranging difference with respect to meaning of "epiousios" is discussed in detail in the current Catechism of the Catholic Church by way of an inclusive approach toward tradition as well as a literal one for meaning: "Taken in a temporal sense, this word is a pedagogical repetition of 'this day', to confirm us in trust 'without reservation'. Taken in the qualitative sense, it signifies what is necessary for life, and more broadly every good thing sufficient for subsistence. Taken literally ("epi-ousios": "super-essential"), it refers directly to the Bread of Life, the Body of Christ, the 'medicine of immortality,' without which we have no life within us."

"Epiousios" is translated as "supersubstantialem" in the Vulgate () and accordingly as "supersubstantial" in the Douay-Rheims Bible ().

Barclay M. Newman's "A Concise Greek-English Dictionary of the New Testament", published in a revised edition in 2010 by the United Bible Societies, has the following entry:

It thus derives the word from the preposition ἐπί ("epi") and the verb εἰμί ("eimi"), from the latter of which are derived words such as οὐσία ("ousia"), the range of whose meanings is indicated in "A Greek–English Lexicon".

The Presbyterian and other Reformed churches tend to use the rendering "forgive us our debts, as we forgive our debtors". Roman Catholics, Lutherans, Anglicans and Methodists are more likely to say "trespasses… those who trespass against us". The "debts" form appears in the first English translation of the Bible, by John Wycliffe in 1395 (Wycliffe spelling "dettis"). The "trespasses" version appears in the 1526 translation by William Tyndale (Tyndale spelling "treaspases"). In 1549 the first Book of Common Prayer in English used a version of the prayer with "trespasses". This became the "official" version used in Anglican congregations. On the other hand, the 1611 King James Version, the version specifically authorized for the Church of England, has "forgive us our debts, as we forgive our debtors".

After the request for bread, Matthew and Luke diverge slightly. Matthew continues with a request for debts to be forgiven in the same manner as people have forgiven those who have debts against them. Luke, on the other hand, makes a similar request about sins being forgiven in the manner of debts being forgiven between people. The word "debts" () does not necessarily mean financial obligations, as shown by the use of the verbal form of the same word () in passages such as . The Aramaic word "ḥôbâ" can mean "debt" or "sin". This difference between Luke's and Matthew's wording could be explained by the original form of the prayer having been in Aramaic. The generally accepted interpretation is thus that the request is for forgiveness of sin, not of supposed loans granted by God. Asking for forgiveness from God was a staple of Jewish prayers (e.g., Psalm 51). It was also considered proper for individuals to be forgiving of others, so the sentiment expressed in the prayer would have been a common one of the time.

Anthony C. Deane, Canon of Worcester Cathedral, suggested that the choice of the word "ὀφειλήματα" (debts), rather than "ἁμαρτίας" (sins), indicates a reference to failures to use opportunities of doing good. He linked this with the parable of the sheep and the goats (also in Matthew's Gospel), in which the grounds for condemnation are not wrongdoing in the ordinary sense, but failure to do right, missing opportunities for showing love to others.

"As we forgive ...". Divergence between Matthew's "debts" and Luke's "sins" is relatively trivial compared to the impact of the second half of this statement. The verses immediately following the Lord's Prayer, show Jesus teaching that the forgiveness of our sin/debt (by God) is linked with how we forgive others, as in the Parable of the Unforgiving Servant , which Matthew gives later. R. T. France comments:

The point is not so much that forgiving is a prior condition of being forgiven, but that forgiving cannot be a one-way process. Like all God's gifts it brings responsibility; it must be passed on. To ask for forgiveness on any other basis is hypocrisy. There can be question, of course, of our forgiving being in proportion to what we are forgiven, as 18:23–35 makes clear.
Interpretations of the penultimate petition of the prayer – not to be led by God into "peirasmos –" vary considerably. The range of meanings of the Greek word "πειρασμός" ("peirasmos") is illustrated in New Testament Greek lexicons. In different contexts it can mean temptation, testing, trial, experiment. Although the traditional English translation uses the word "temptation" and Carl Jung saw God as actually leading people astray, Christians generally interpret the petition as not contradicting : "Let no one say when he is tempted, 'I am being tempted by God', for God cannot be tempted with evil, and he himself tempts no one. But each person is tempted when he is lured and enticed by his own desire." Some see the petition as an eschatological appeal against unfavourable Last Judgment, a theory supported by the use of the word ""peirasmos"" in this sense in . Others see it as a plea against hard "tests" described elsewhere in scripture, such as those of Job. It is also read as: "Do not let us be led (by ourselves, by others, by Satan) into temptations". Since it follows shortly after a plea for daily bread (i.e., material sustenance), it is also seen as referring to not being caught up in the material pleasures given. A similar phrase appears in and in connection with the prayer of Jesus in Gethsemane.

Joseph Smith, the founder of The Church of Jesus Christ of Latter-day Saints, in a translation of the Holy Bible which was not completed before his death, used: "And suffer us not to be led into temptation".

In a conversation on the Italian TV channel "TV2000" on 6 December 2017, Pope Francis commented that the then Italian wording of this petition (similar to the traditional English) was a poor translation. He said "the French" (i.e., the Bishops' Conference of France) had changed the petition to "Do not let us fall in/into temptation". He was referring to the 2017 change to a new French version, "Et ne nous laisse pas entrer en tentation" ("Do not let us enter into temptation"), but spoke of it in terms of the Spanish translation, "no nos dejes caer en la tentación" ("do not let us fall in/into temptation"), that he was accustomed to recite in Argentina before his election as Pope. He explained: "I am the one who falls; it's not him [God] pushing me into temptation to then see how I have fallen". Anglican theologian Ian Paul said that such a proposal was "stepping into a theological debate about the nature of evil".

In January 2018, the German Bishops' Conference rejected any rewording of their translation of the Lord's Prayer.

In November 2018, the Episcopal Conference of Italy adopted a new edition of the "Messale Romano", the Italian translation of the Roman Missal. One of the changes made from the older (1983) edition was to render this petition as "non abbandonarci alla tentazione" ("do not abandon us to temptation"). The Italian-speaking Waldensian Evangelical Church maintains its translation of the petition: "non esporci alla tentazione" ("do not expose us to temptation").

Translations and scholars are divided over whether the final word here refers to "evil" in general or "the evil one" (the devil) in particular. In the original Greek, as well as in the Latin translation, the word could be either of neuter (evil in general) or masculine (the evil one) gender. Matthew's version of the prayer appears in the Sermon on the Mount, in earlier parts of which the term is used to refer to general evil. Later parts of Matthew refer to the devil when discussing similar issues. However, the devil is never referred to as "the evil one" in any known Aramaic sources. While John Calvin accepted the vagueness of the term's meaning, he considered that there is little real difference between the two interpretations, and that therefore the question is of no real consequence. Similar phrases are found in and .

The doxology sometimes attached to the prayer in English is similar to a passage in – "Yours, O LORD, is the greatness and the power and the glory and the victory and the majesty, for all that is in the heavens and in the earth is yours. Yours is the kingdom, O LORD, and you are exalted as head above all." It is also similar to the paean to King Nebuchadnezzar of Babylon in – "You, O king, the king of kings, to whom the God of heaven has given the kingdom, the power, and the might, and the glory,"

The doxology has been interpreted as connected with the final petition: "Deliver us from evil". The kingdom, the power and the glory are the Father's, not of our antagonist's, who is subject to him to whom Christ will hand over the kingdom after he has destroyed all dominion, authority and power (1 Corinthians 15:24). It makes the prayer end as well as begin with the vision of God in heaven, in the majesty of his name and kingdom and the perfection of his will and purpose.

The doxology is not included in Luke's version of the Lord's Prayer, nor is it present in the earliest manuscripts (papyrus or parchment) of Matthew, representative of the Alexandrian text, although it is present in the manuscripts representative of the later Byzantine text. Most scholars do not consider it part of the original text of Matthew. The Codex Washingtonensis, which adds a doxology (in the familiar text), is of the early fifth or late fourth century. New translations generally omit it except as a footnote.

The "Didache", generally considered a first-century text, has a doxology, "for yours is the power and the glory forever", as a conclusion for the Lord's Prayer ("Didache", 8:2). C. Clifton Black, although regarding the "Didache" as an "early second century" text, nevertheless considers the doxology it contains to be the "earliest additional ending we can trace". Of a longer version, Black observes: "Its earliest appearance may have been in Tatian's "Diatessaron", a second-century harmony of the four Gospels". The first three editions of the UBS text cited the "Diatessaron" for inclusion of the familiar doxology in Matthew 6:13, but in the later editions it cites the "Diatessaron" for excluding it. The "Apostolic Constitutions" added "the kingdom" to the beginning of the formula in the "Didache", thus establishing the now familiar doxology.

In the Divine Liturgy of the Byzantine Rite, the priest sings, after the last line of the prayer, the doxology, "For thine is the kingdom and the power and the glory, of the Father, and of the Son, and of the Holy Spirit, now and ever and unto ages of ages." 

Adding a doxology to the Our Father is not part of the liturgical tradition of the Roman Rite nor does the Latin Vulgate of St. Jerome contain the doxology that appears in late Greek manuscripts. However, it is recited since 1970 in the Roman Rite Order of Mass, not as part of the Lord's Prayer but separately as a response acclamation after the embolism developing the seventh petition in the perspective of the Final Coming of Christ.

The Anglican Book of Common Prayer sometimes gives the Lord's Prayer with the doxology, sometimes without.

Most Protestants append it to the Lord's Prayer.

In the course of Christianization, one of the first texts to be translated between many languages has historically been the Lord's Prayer, long before the full Bible would be translated into the respective languages.
Since the 16th century, collections of translations of the prayer have often been used for a quick comparison of languages.

The first such collection, with 22 versions, was "Mithridates, de differentiis linguarum" by Conrad Gessner (1555; the title refers to Mithridates VI of Pontus who according to Pliny the Elder was an exceptional polyglot).

Gessner's idea of collecting translations of the prayer was taken up by authors of the 17th century, including Hieronymus Megiserus (1603) and Georg Pistorius (1621).
Thomas Lüdeken in 1680 published an enlarged collection of 83 versions of the prayer, of which three were in fictional philosophical languages.
Lüdeken quotes as a "Barnum Hagius" as his source for the exotic scripts used, while their true (anonymous) author was Andreas Müller.
In 1700, Lüdeken's collection was re-edited by B. Mottus as "Oratio dominica plus centum linguis versionibus aut characteribus reddita et expressa".
This edition was comparatively inferior, but a second, revised edition was published in 1715 by John Chamberlain.
This 1715 edition was used by Gottfried Hensel in his "Synopsis Universae Philologiae" (1741) to compile "geographico-polyglot maps" where the beginning of the prayer was shown in the geographical area where the respective languages were spoken.
Johann Ulrich Kraus also published a collection with more than 100 entries.

These collections continued to be improved and expanded well into the 19th century; Johann Christoph Adelung and Johann Severin Vater in 1806–1817 published the prayer in "well-nigh five hundred languages and dialects".

Samples of scripture, including the Lord's Prayer, were published in 52 oriental languages, most of them not previously found in such collections, translated by the brethren of the Serampore Mission and printed at the mission press there in 1818.

The book "The Comprehensive New Testament", by T.E. Clontz and J. Clontz, points to similarities between elements of the Lord's Prayer and expressions in writings of other religions as diverse as the "Dhammapada", the "Epic of Gilgamesh", the "Golden Verses", and the Egyptian "Book of the Dead". These elements include both biblical and post-biblical material in Jewish prayer, especially Kiddushin 81a (Babylonian). "Our Father which art in heaven" (אבינו שבשמים, "Avinu shebashamayim") is the beginning of many Hebrew prayers. "Hallowed be thy name" is reflected in the Kaddish. "Lead us not into sin" is echoed in the "morning blessings" of Jewish prayer. A blessing said by some Jewish communities after the evening "Shema" includes a phrase quite similar to the opening of the Lord's Prayer: "Our God in heaven, hallow thy name, and establish thy kingdom forever, and rule over us for ever and ever. Amen." There are parallels also in .

Rabbi Aron Mendes Chumaceiro has said that nearly all the elements of the prayer have counterparts in the Jewish Bible and Deuterocanonical books: the first part in ("Look down from heaven and see, from your holy and beautiful habitation ... for you are our Father ...") and ("I will vindicate the holiness of my great name ...") and ("I will show my greatness and my holiness and make myself known in the eyes of many nations ..."), the second part in ("Saviours shall go up to Mount Zion to rule Mount Esau, and the kingdom shall be the L's") and ("... It is the L. Let him do what seems good to him."), the third part in ("... feed me with my apportioned bread..."), the fourth part in ("Forgive your neighbour the wrong he has done, and then your sins will be pardoned when you pray."). "Deliver us from evil" can be compared with ("... let no iniquity get dominion over me."). Chumaceiro says that, because the idea of God leading a human into temptation contradicts the righteousness and love of God, "Lead us not into temptation" has no counterpart in the Jewish Bible/Christian Old Testament.

The word "πειρασμός", which is translated as "temptation", could also be translated as "test" or "trial", making evident the attitude of someone's heart. Well-known examples in the Old Testament are God's test of Abraham (); his "moving" (the Hebrew word means basically "to prick, as by weeds, thorns") David to do (numbering Israel) what David later acknowledged as sin (; see also ); and the testing of Job in the "Book of Job".

In modern times, various composers have incorporated "The Lord's Prayer" into a musical setting for utilization during liturgical services for a variety of religious traditions as well as interfaith ceremonies. Included among them are:


As with other prayers, the Lord's Prayer was used by cooks to time their recipes before the spread of clocks.
For example, a step could be "simmer the broth for three Lord's Prayers".

American songwriter and arranger Brian Wilson set the text of the Lord's Prayer to an elaborate close-harmony arrangement loosely based on Malotte's melody. Wilson's group, The Beach Boys, would return to the piece several times throughout their recording career, most notably as the B-side to their 1964 single "Little Saint Nick."

The band Yazoo used the prayer interspersed with the lyrics of "In My Room" on the album "Upstairs at Eric's".

The 2005 game Civilization IV uses a Swahili-language version of the prayer as its main theme: "Baba Yetu".


Text

Comment


</doc>
<doc id="18495" url="https://en.wikipedia.org/wiki?curid=18495" title="Lightworks">
Lightworks

Lightworks is a non-linear editing system (NLE) for editing and mastering digital video. It was an early developer of computer-based non-linear editing systems, and has been in development since 1989 and won a 2017 EMMY Award for pioneering digital nonlinear editing. Lightworks has millions of adoptors worldwide due to the software being available across three platforms in Windows, Mac and Linux. The development of an open-source version was announced in May 2010. No source code of the program has yet been released. In July 2020, a Lightworks product manager confirmed that they "Still hope to announce something in the future" about it becoming open source.

The free version comes with a limited number of features:

The free version cannot export to DVD or Blu-ray, but can export to a hard drive (since Lightworks 14).

OLE Limited was founded in 1989 by Paul Bamborough, Nick Pollock and Neil Harris. In 1994 it was sold to Tektronix, who were not successful at developing the company's products. In 1999 it was sold on to the newly formed Lightworks Inc., then owned by Fairlight Japan, and then purchased by Gee Broadcast in May 2004.

Under Gee Broadcast ownership, new product releases resumed with the release of the Lightworks "Touch" range, and the "Alacrity" and "Softworks" ranges for SD & HD editing. Softworks offered the Lightworks User Interface and toolset in a software only package for laptops or office workstations. Softworks and Alacrity supported mixed formats and resolutions in real time and project output in different resolutions without re-rendering. Alacrity supported dual outputs while the same facility was available for Softworks users as an option.

In August 2009 the UK and US based company "EditShare" acquired Gee Broadcast and the Lightworks editing platform from, along with their video server system "GeeVS".

At the annual convention of the National Association of Broadcasters, NAB Show, on 11 April 2010, EditShare announced that they plan to transform Lightworks into Lightworks Open Source. It was presented at IBC in Amsterdam September 2010.

On 9 November 2010, EditShare announced that Lightworks would be downloadable on 29 November of the same year, at first exclusively for the users who had registered during the initial announcement, but subsequently publishing the software as "public beta".

EditShare planned the release of the open source version in Q4 of 2011, after they finished code review. They plan to make money from proprietary plugins offered in their associated online shop, including plugins needed to access professional video formats. Shortly before the scheduled release date of 29 November 2011, EditShare announced that an open source release of the software would be temporarily delayed, but did not announce a new release date. The announcement noted that they were not yet satisfied with the stability of the new version.

After an 18-month beta program, EditShare released Lightworks 11, for Windows only, on 28 May 2012. The non beta release of Lightworks includes a host of new features for editors, and runs on wide range of PC hardware. The software was re-designed and re-written for portability (versions for GNU/Linux and Mac OS X have also been released) and now supports many more codecs including AVCHD, H.264, AVC-Intra, DNxHD, ProRes, Red R3D, DPX, XDCAM HD 50, XDCAM EX, DVD, Blu-ray, and 4K, but only for the paid Pro version. The free version supports DV, MPEG, Uncompressed and other codecs for both import and export.

On 29 May 2013, v11.1 stable release was made available for download. A major development in the Pro version is much improved performance of the H.264/AVC codec in MP4 and MOV containers. This makes it possible to edit this format natively, even with less powerful CPUs. This should interest HDSLR and GoPro camera users. Native editing of H.264 MTS files has been possible since version 11.0.3.

This version of Lightworks has also replaced HASP with the new EditShare Licensing System (ELS), which eliminates some installation problems. Lightworks Free users can now download the 64 bit version, which was previously limited to Pro users. The Free version now also comes with a 30-day Pro Trial period.

EditShare demonstrated the Linux version at the NAB in Las Vegas in April 2012, and posted a video of it running on Ubuntu on their YouTube channel. At IBC in Amsterdam in September, an updated Linux demo was presented, and EditShare announced that the initial Linux alpha version would become available on 30 October . Lightworks 11 alpha for Linux was released on 30 April 2012, but only to a limited audience. The Linux version of Lightworks was made available as a Public Beta on 30 April 2013.

On 8 August 2014, the first beta of Lightworks version 12 working on Windows, Linux and Mac was released.

On 29 August 2015, Lightworks version 12.5 for Windows, Linux and Mac was released.

On 4 February 2016, Lightworks version 12.6 for Windows, Linux and Mac was released.

In October 2018, Lightworks released version 14.5 for Windows, Linux and Mac platforms. 14.5 added a vast array of new features including variable frame rate support, a huge amount of codec support including Red Cinema R3D, Cineform and Blackmagic Q1 codecs.

Lightworks confirmed that over 4-million downloads and registrations had taken place.




</doc>
<doc id="18496" url="https://en.wikipedia.org/wiki?curid=18496" title="Love Parade">
Love Parade

The Love Parade () was a popular electronic dance music festival and technoparade that originated in 1989 in West Berlin, Germany. It was held annually in Berlin from 1989 to 2003 and in 2006, then from 2007 to 2010 in the Ruhr region. Events scheduled for 2004 and 2005 in Berlin and for 2009 in Bochum were canceled.

On 24 July 2010, a crowd crush at the Love Parade caused the death of 21 people, with at least 500 others injured. As a consequence, the organizer of the festival announced that no further Love Parades would be held and that the festival was permanently canceled.

The parade first occurred in July 1989, when 150 people took to the streets in Berlin. It was started by the Berlin underground at the initiative of Matthias Roeingh (also known as "Dr Motte") and his then girlfriend Danielle de Picciotto. It was conceived as a political demonstration for peace and international understanding through love and music. It was supposed to be a bigger birthday party for Roeingh, and the motto "Friede, Freude, Eierkuchen" (in English — "Peace, Joy, Pancakes") stood for disarmament (peace), music (joy) and a fair food production/distribution (pancakes). Roeingh dissociated himself from the parade in 2006 because of the commercialization of the event.

The parade was held on the Berlin Kurfürstendamm until 1996. Because of overcrowding on this street, the festival moved to the Straße des 17. Juni in the Großer Tiergarten park in the center of Berlin. The festival became centered around the "Siegessäule" in the middle of the park; and the golden angel atop the column became the parade's emblem.

Many people from Germany and abroad traveled to Berlin to take part in the Parade — over a million attended in the years 1997 through 2000 and 800,000 in 2001. Attendance at the 2001 festival was significantly lower because the date of the parade was changed with little advance notice. 2002 and 2003 also saw lower figures, and in 2004 and 2005 the parade was canceled because of funding difficulties. The parade had inspired opposition because of the damage to the Tiergarten by attendees, who were provided with insufficient toilet facilities. Opponents allegedly complicated matters for organisers by booking their own events in Berlin and so to exclude the parade from being able to register with city police. In 2004, however, a scaled-down version took place which served more as a mini-protest and was promoted with the title "Love Weekend". Dozens of clubs promoted the weekend-long event all over the city, with various clubs staying open for three days straight without closing. In 2006, the parade made a comeback with the help of German exercise studio McFit.

The Love Parade 2007 was planned for 7 July 2007 in Berlin. However, the Berlin event was canceled in February because the Senate of Berlin did not issue the necessary permits at that time. After negotiations with several German cities, on 21 July, it was announced that the parade would move to the Ruhr Area for the next five years. The first event took place in Essen on 25 August. The parade in Essen saw 1.2 million visitors in comparison to the 500,000 who attended the 2006 parade in Berlin.

In 2008, the festival took place in Dortmund on 19 July on the Bundesstraße 1 under the motto "Highway of Love". The event was planned as a "Love Weekend", with parties throughout the region. For the first time the Turkish electronic scene was represented by its own float, called "Turkish Delights". The official estimate is that 1.6 million visitors attended, making it the largest parade to date.

The 2009 event, planned for Bochum, was canceled; a year later, the deaths of 21 attendees at the Duisburg venue prompted the parade's organiser Rainer Schaller to declare an end to the festival. "The Love Parade has always been a peaceful party, but it will forever be overshadowed by the accident, so out of respect for the victims the Love Parade will never take place again," Schaller said. The parade was one of the oldest and largest festivals of electronic music, together with Zürich's Streetparade, Mayday and Nature One.

The music played at the events was predominantly electronic dance music — in this case mainly trance, house, techno, and schranz music. Attempts to introduce other music styles, such as hip hop, have failed. Hardcore and gabber music were part of the parade in early years, but were later removed. They are now celebrated separately on a counter-demonstration called "Fuckparade".

The parade was seen to be louder and more crowded than most concerts. With its water-cooled sound systems on every truck, the parade produced an extremely loud sound floor. After the 2001 arrangement, veterinarians at the Berlin Zoo blamed the parade for giving more than half of its animals diarrhea. Chairman Heiner Kloes said veterinarians told him the heavy bass was to blame for disturbing the animals. The parade consisted of the sound trucks that usually featured local, or important, clubs and their DJs. It had become a rule that only trucks that had sponsors from a techno-related field, such as clubs, labels or stores, were allowed, but advertising space was increased after the 2006 event to offset the high costs of equipping a truck. The trucks were usually open on top and featured dancers, with box-systems mounted on the side or rear.

The parade was a place where some exhibited and enjoyed other people's exhibitionist tendencies. Some attendees enjoyed carrying around toys such or other items such as dummies (pacifiers) or face masks. Often the crowd was imaginative in terms of clothing (or lack thereof) and appearance.

One famous picture from the parade is people sitting and dancing on streetlamps, trees, commercial signs, telephone booths, which gave the event's nickname "the greatest amateur circus on earth".

The demonstration concluded with the so-called "Abschlusskundgebung" which were sets of the world's leading top DJs such as DJ Tiesto, Paul Van Dyk, Carl Cox, Armin Van Buuren, DJ Rush, DJ Hell, Westbam, Drum Connection, Miss Djax, Marusha or Chris Liebing. During this time all trucks (usually about 40) were connected to each other and set online to the statue of victory where the turntables are. This was one of the few chances a DJ can ever have to play for a crowd of about one million people.

The parade was quite peaceful for an event of its size, seeing few arrests. In 2008, for example, charges were pressed for six robberies, three sexually related offences and forty thefts. Twenty-three attendees were caught with drugs and forty-nine were charged with bodily harm. There were 177 parade visitors provisionally arrested by the police. Arrests were usually related to drug crimes and most other incidents featured people passing out due to dehydration or hyperthermia. In 2000, after the parade, a girl under the influence of ecstasy was run over by an S-Bahn after she had been leaning on the door too hard.

At the 2010 Love Parade in Duisburg, the number of people attending allegedly reached 1.4 million – the original expectation was around 800,000 – whereas police believed around 400,000 people were present. 21 people were killed, and more than 500 injured, in an incident on an overcrowded ramp leading from a tunnel into the festival. At least 20 casualties resulted from suffocation, caused by crowd pressure.

Safety experts and a fire service investigator had previously warned that the site was not suitable for the numbers expected to attend. Rainer Schaller, the festival's organizer and chief executive officer, later said the festival would not continue in future.

A preliminary investigation of the ministry of the interior placed heavy blame on the organizers around Rainer Schaller. Schaller in turn claimed that errors by the police in controlling streams of visitors led to the accident.

Similar festivals have taken place in other cities of Germany and many other countries worldwide. Large spin-off festivals in Europe include Zürich's Street Parade, Geneva's Lake Parade, Paris's Techno Parade, Rotterdam's FFWD Dance Parade, Munich's Union Move, Hamburg's Generation Move, Hannover's Reincarnation, Bremen's Vision Parade and the Love Parade and the Freeparade in Vienna. In 1994, 1995 and 1996 an event called Love Parade was held in Melbourne, Australia. Unlike its overseas counterparts, this was a smaller "rave party" version of the festival. In 1996 it was held at Festival Hall in West Melbourne and included a parade that made the evening news. It was followed in 1997 by a Love Parade in Sydney, Australia, likewise a smaller rave party, held at the infamous Graffiti Hall of Fame in Redfern. In 1999 and 2000 technoparades named "Buenos Aires Energy Parade" took place in Buenos Aires, Argentina under the motto "Love, Peace and Dance". On Saturday 8 July 2000 a Love Parade was held in Roundhay Park, Leeds, United Kingdom sponsored by BBC Radio 1. In 2001, the official UK parade had moved to Newcastle upon Tyne which was to have seen a parade through the streets of Newcastle before ending up on Town Moor but was canceled after the police refused a license: BBC Radio 1 still hosted a more contained event, however. Since then no Love Parade has taken place in the United Kingdom. In Summer 2000 one of the first public events that took place in post-war Sarajevo, Bosnia and Herzegovina, was Futura, Festival of Electronic Music. Some of the world's most famous DJs, including the organizers of the Berlin Love Parade, performed in a bombed and burnt out factory.
After being held in the North-American Continent for the first time in Mexico (2002), in the fall of 2004 the Love Parade was held in San Francisco. They had held their inaugural Parade in September 2004 with 37,000 attending. The parade was held again in San Francisco in September 2005 as a rousing success drawing over 50–60,000 people. In 2006, the parade was held on 23 September and was renamed Love Fest because the Loveparade Berlin organization did not renew any of their worldwide licenses not already under contract so they could focus on their own event. 2009 was the biggest success of the parade now renamed Lovevolution with over 100,000 people. The first Love Parade in Santiago was held in 2005 and gathered over 100,000 people; the 2006 version gathered over 200,000 people. The first Love Parade in Caracas was held in June 2007 and gathered over 25,000 people.

Spin-off festivals of the Love Parade have taken place in:
Under German law the state has to pay for security during political demonstrations as well as cleaning up the streets after the demonstration. In the case of a commercial event however, the organizer must cover these expenses. For a large event like the Love Parade the costs are quite high: an estimated €300,000 to €400,000.

The Love Parade was initially held as a political demonstration to save costs; however it was organized by two companies set up just for the Love Parade. Due to this there was a dispute between the organizers and the city of Berlin every year about the status of the Love Parade and who should bear what costs. Finally in 2001, the courts ruled that the Love Parade had to be held as commercial event.

Every German parade has had its own anthem.

According to media reports, the attendance figures had been forged by the organizers for years. Accurate counts are not available since entry is free and uncontrolled. The mayor of Dortmund and the police confirmed the number of attendees in Dortmund.




</doc>
<doc id="18497" url="https://en.wikipedia.org/wiki?curid=18497" title="Lost Generation">
Lost Generation

The Lost Generation was the social generational cohort that came of age during World War I. "Lost" in this context refers to the "disoriented, wandering, directionless" spirit of many of the war's survivors in the early postwar period. The term is also particularly used to refer to a group of American expatriate writers living in Paris during the 1920s. Gertrude Stein is credited with coining the term, and it was subsequently popularized by Ernest Hemingway who used it in the epigraph for his 1926 novel "The Sun Also Rises": "You are all a lost generation".

In a more general sense, the Lost Generation is considered to be made up of individuals born between 1883 and 1900. The last surviving person known to have been born in the 19th century died in 2018.

In his memoir "A Moveable Feast" (1964), published after Hemingway's and Stein's deaths, Hemingway writes that Stein heard the phrase from a French garage owner who serviced Stein's car. When a young mechanic failed to repair the car quickly enough, the garage owner shouted at the young man, "You are all a ""génération perdue."" While telling Hemingway the story, Stein added: "That is what you are. That's what you all are ... all of you young people who served in the war. You are a lost generation." Hemingway thus credits the phrase to Stein, who was then his mentor and patron.

The 1926 publication of Hemingway's "The Sun Also Rises" popularized the term; the novel serves to epitomize the post-war expatriate generation. However, Hemingway later wrote to his editor Max Perkins that the "point of the book" was not so much about a generation being lost, but that "the earth abideth forever." Hemingway believed the characters in "The Sun Also Rises" may have been "battered" but were not lost.

Consistent with this ambivalence, Hemingway employs "Lost Generation" as one of two contrasting epigraphs for his novel. In "A Moveable Feast", Hemingway writes, "I tried to balance Miss Stein's quotation from the garage owner with one from Ecclesiastes." A few lines later, recalling the risks and losses of the war, he adds: "I thought of Miss Stein and Sherwood Anderson and egotism and mental laziness versus discipline and I thought 'who is calling who a lost generation?

The writings of the Lost Generation literary figures often pertained to the writers' experiences in World War I and the years following it. It is said that the work of these writers was autobiographical based on their use of mythologized versions of their lives. One of the themes that commonly appears in the authors' works is decadence and the frivolous lifestyle of the wealthy. Both Hemingway and Fitzgerald touched on this theme throughout the novels "The Sun Also Rises" and "The Great Gatsby". Another theme commonly found in the works of these authors was the death of the American dream, which is exhibited throughout many of their novels. It is particularly prominent in "The Great Gatsby", in which the character Nick Carraway comes to realize the corruption that surrounds him.

The term is also used in a broader context for the generation of young people who came of age during and shortly after World War I. Authors William Strauss and Neil Howe define the Lost Generation as the cohort born from 1883 to 1900, who came of age during World War I and the Roaring Twenties. In Europe, they are mostly known as the "Generation of 1914", for the year World War I began. In France, the country in which many expatriates settled, they were sometimes called the "Génération du feu", the "(gun)fire generation". In Great Britain, the term was originally used for those who died in the war, and often implicitly referred to upper-class casualties who were perceived to have died disproportionately, robbing the country of a future elite. Many felt that "the flower of youth and the best manhood of the peoples [had] been mowed down," for example such notable casualties as the poets Isaac Rosenberg, Rupert Brooke, Edward Thomas and Wilfred Owen, composer George Butterworth and physicist Henry Moseley.

Notable figures of the Lost Generation include F. Scott Fitzgerald, Gertrude Stein, Ernest Hemingway, T. S. Eliot, Ezra Pound, Jean Rhys and Sylvia Beach.





</doc>
<doc id="18499" url="https://en.wikipedia.org/wiki?curid=18499" title="Left-wing politics">
Left-wing politics

Left-wing politics supports social equality and egalitarianism, often in opposition to social hierarchy. It typically involves a concern for those in society whom its adherents perceive as disadvantaged relative to others as well as a belief that there are unjustified inequalities that need to be reduced or abolished.

The political terms "Left" and "Right" were coined during the French Revolution (1789–1799), referring to the seating arrangement in the French Estates General. Those who sat on the left generally opposed the monarchy and supported the revolution, including the creation of a republic and secularization, while those on the right were supportive of the traditional institutions of the Old Regime. Use of the term "Left" became more prominent after the restoration of the French monarchy in 1815 when it was applied to the "Independents". The word "wing" was appended to Left and Right in the late 19th century, usually with disparaging intent and "left-wing" was applied to those who were unorthodox in their religious or political views.

The term was later applied to a number of movements, especially republicanism during the French Revolution in the 18th century, followed by socialism, including anarchism, communism and social democracy, in the 19th and 20th centuries. Since then, the term "left-wing" has been applied to a broad range of movements including civil rights movements, feminist movements, anti-war movements and environmental movements, as well as a wide range of parties. According to emeritus professor of economics, Barry Clark, "[leftists] claim that human development flourishes when individuals engage in cooperative, mutually respectful relations that can thrive only when excessive differences in status, power, and wealth are eliminated".

In politics, the term "Left" derives from the French Revolution, as the political groups opposed to the royal veto privilege (Montagnard and Jacobin deputies from the Third Estate) generally sat to the left of the presiding member's chair in parliament, while the ones in favour of the royal veto privilege sat on its right. That habit began in the French Estates General of 1789. Throughout the 19th century in France, the main line dividing Left and Right was between supporters of the French Republic and those of the monarchy's privileges. The June Days uprising during the Second Republic was an attempt by the Left to assert itself after the 1848 Revolution, but only a small portion of the population supported this.

In the mid-19th century, nationalism, socialism, democracy and anti-clericalism became features of the French Left. After Napoleon III's 1851 coup and the subsequent establishment of the Second Empire, Marxism began to rival radical republicanism and utopian socialism as a force within left-wing politics. The influential "Communist Manifesto" by Karl Marx and Friedrich Engels, published in 1848, asserted that all human history is the history of class struggle. They predicted that a proletarian revolution would eventually overthrow bourgeois capitalism and create a classless, stateless, post-monetary communist society. It was in this period that the word "wing" was appended to both Left and Right.
In the United States, many leftists, social liberals, progressives and trade unionists were influenced by the works of Thomas Paine, who introduced the concept of asset-based egalitarianism, which theorises that social equality is possible by a redistribution of resources.

The International Workingmen's Association (1864–1876), sometimes called the First International, brought together delegates from many different countries, with many different views about how to reach a classless and stateless society. Following a split between supporters of Marx and Mikhail Bakunin, anarchists formed the International Workers' Association. The Second International (1888–1916) became divided over the issue of World War I. Those who opposed the war, such as Vladimir Lenin and Rosa Luxemburg, saw themselves as further to the left.

In the United States after Reconstruction, the phrase "the Left" was used to describe those who supported trade unions, the civil rights movement and the anti-war movement. More recently in the United States, left-wing and right-wing have often been used as synonyms for Democratic and Republican, or as synonyms for liberalism and conservatism respectively.

Since the Right was populist, both in the Western and the Eastern Bloc anything viewed as avant-garde art was called leftist in all Europe, thus the identification of Picasso's "Guernica" as "leftist" in Europe and the condemnation of the Russian composer Shostakovich's opera ("The Lady Macbeth of Mtsensk District") in "Pravda" as follows: "Here we have 'leftist' confusion instead of natural, human music".
The following positions are typically associated with left-wing politics.

Leftist economic beliefs range from Keynesian economics and the welfare state through industrial democracy and the social market to nationalization of the economy and central planning, to the anarcho-syndicalist advocacy of a council- and assembly-based self-managed anarchist communism. During the Industrial Revolution, leftists supported trade unions. At the beginning of the 20th century, many leftists advocated strong government intervention in the economy. Leftists continue to criticize what they perceive as the exploitative nature of globalization, the "race to the bottom" and unjust lay-offs. In the last quarter of the 20th century, the belief that government (ruling in accordance with the interests of the people) ought to be directly involved in the day-to-day workings of an economy declined in popularity amongst the center-left, especially social democrats who became influenced by "Third Way" ideology.

Other leftists believe in Marxian economics, which are based on the economic theories of Karl Marx. Some distinguish Marx's economic theories from his political philosophy, arguing that Marx's approach to understanding the economy is independent of his advocacy of revolutionary socialism or his belief in the inevitability of proletarian revolution. Marxian economics does not exclusively rely upon Marx, but it draws from a range of Marxist and non-Marxist sources. The "dictatorship of the proletariat" or "workers' state" are terms used by some Marxists, particularly Leninists and Marxist–Leninists, to describe what they see as a temporary state between the capitalist state of affairs and a communist society. Marx defined the proletariat as salaried workers, in contrast to the "lumpenproletariat", who he defined as outcasts of society, such as beggars, tricksters, entertainers, buskers, criminals and prostitutes. The political relevance of farmers has divided the left. In "Das Kapital", Marx scarcely mentioned the subject. Mao Zedong believed that it would be rural peasants, not urban workers, who would bring about the proletarian revolution.

Left-libertarians, libertarian socialists and anarchists believe in a decentralized economy run by trade unions, workers' councils, cooperatives, municipalities and communes and oppose both state and private control of the economy, preferring social ownership and local control, in which a nation of decentralized regions is united in a confederation.

The global justice movement, also known as the anti-globalization movement or alter-globalization movement, protests against corporate economic globalization due to its negative consequences for the poor, workers, the environment and small businesses.

One of the foremost left-wing advocates was Thomas Paine, one of the first individuals since "left" and "right" became political terms to describe the collective human ownership of the world which he speaks of in Agrarian Justice. As such, most of left-wing thought concerning environmentalism stems from this duty of ownership, and this cooperative ownership means that we need to take care of it. This is reflected in much of the historical left-wing thought that came after, although there were disagreements about what this entailed.

Both Karl Marx and the early socialist William Morris arguably had a concern for environmental matters. According to Marx: "Even an entire society, a nation, or all simultaneously existing societies taken together [...] are not owners of the earth. They are simply its possessors, its beneficiaries, and have to bequeath it in an improved state to succeeding generations". Following the Russian Revolution, environmental scientists such as revolutionary Alexander Bogdanov and the Proletkult organisation made efforts to incorporate environmentalism into Bolshevism and "integrate production with natural laws and limits" in the first decade of Soviet rule, before Joseph Stalin attacked ecologists and the science of ecology, purged environmentalists and promoted the pseudo-science of Trofim Lysenko. Similarly, Mao Zedong rejected environmentalism and believed that based on the laws of historical materialism all of nature must be put into the service of revolution.

From the 1970s onwards, environmentalism became an increasing concern of the left, with social movements and some unions campaigning over environmental issues. For example, the left-wing Builders Labourers Federation in Australia, led by the communist Jack Mundy, united with environmentalists to place green bans on environmentally destructive development projects. Some segments of the socialist and Marxist left consciously merged environmentalism and anti-capitalism into an eco-socialist ideology. Barry Commoner articulated a left-wing response to "The Limits to Growth" model that predicted catastrophic resource depletion and spurred environmentalism, postulating that capitalist technologies were chiefly responsible for environmental degradation, as opposed to population pressures. Environmental degradation can be seen as a class or equity issue, as environmental destruction disproportionately affects poorer communities and countries.

Several left-wing or socialist groupings have an overt environmental concern and several green parties contain a strong socialist presence. For example, the Green Party of England and Wales features an eco-socialist group, Green Left, that was founded in June 2005. Its members held some influential positions within the party, including both the former Principal Speakers Siân Berry and Dr. Derek Wall, himself an eco-socialist and Marxist academic. In Europe, some Green left political parties combine traditional social-democratic values such as a desire for greater economic equality and workers rights with demands for environmental protection, such as the Nordic Green Left.

Well-known socialist Bolivian President Evo Morales has traced environmental degradation to consumerism. He has said: "The Earth does not have enough for the North to live better and better, but it does have enough for all of us to live well". James Hansen, Noam Chomsky, Raj Patel, Naomi Klein, The Yes Men and Dennis Kucinich have had similar views.
In the 21st century, questions about the environment have become increasingly politicized, with the Left generally accepting the findings of environmental scientists about global warming and many on the Right disputing or rejecting those findings. However, the left is divided over how to effectively and equitably reduce carbon emissions: the center-left often advocates a reliance on market measures such as emissions trading or a carbon tax, while those further to the left tend to support direct government regulation and intervention either alongside or instead of market mechanisms.

The question of nationality and nationalism has been a central feature of political debates on the Left. During the French Revolution, nationalism was a policy of the Republican Left. The Republican Left advocated civic nationalism and argued that the nation is a "daily plebiscite" formed by the subjective "will to live together". Related to "revanchism", the belligerent will to take revenge against Germany and retake control of Alsace-Lorraine, nationalism was sometimes opposed to imperialism. In the 1880s, there was a debate between those, such as Georges Clemenceau (Radical), Jean Jaurès (Socialist) and Maurice Barrès (nationalist), who argued that colonialism diverted France from the "blue line of the Vosges" (referring to Alsace-Lorraine); and the "colonial lobby", such as Jules Ferry (moderate republican), Léon Gambetta (republican) and Eugène Etienne, the president of the parliamentary colonial group. After the Dreyfus Affair, nationalism instead became increasingly associated with the far-right.

The Marxist social class theory of proletarian internationalism asserts that members of the working class should act in solidarity with working people in other countries in pursuit of a common class interest, rather than focusing on their own countries. Proletarian internationalism is summed up in the slogan: "Workers of the world, unite!", the last line of "The Communist Manifesto". Union members had learned that more members meant more bargaining power. Taken to an international level, leftists argued that workers ought to act in solidarity to further increase the power of the working class.

Proletarian internationalism saw itself as a deterrent against war, because people with a common interest are less likely to take up arms against one another, instead focusing on fighting the ruling class. According to Marxist theory, the antonym of proletarian internationalism is bourgeois nationalism. Some Marxists, together with others on the left, view nationalism, racism (including anti-Semitism) and religion as divide and conquer tactics used by the ruling classes to prevent the working class from uniting against them. Left-wing movements therefore have often taken up anti-imperialist positions. Anarchism has developed a critique of nationalism that focuses on nationalism's role in justifying and consolidating state power and domination. Through its unifying goal, nationalism strives for centralization, both in specific territories and in a ruling elite of individuals, while it prepares a population for capitalist exploitation. Within anarchism, this subject has been treated extensively by Rudolf Rocker in "Nationalism and Culture" and by the works of Fredy Perlman, such as "Against His-Story, Against Leviathan" and "The Continuing Appeal of Nationalism".

The failure of revolutions in Germany and Hungary ended Bolshevik hopes for an imminent world revolution and led to promotion of "Socialism in One Country" by Joseph Stalin. In the first edition of the book "Osnovy Leninizma" ("Foundations of Leninism", 1924), Stalin argued that revolution in one country is insufficient, but by the end of that year in the second edition of the book he argued that the "proletariat can and must build the socialist society in one country". In April 1925, Nikolai Bukharin elaborated the issue in his brochure "Can We Build Socialism in One Country in the Absence of the Victory of the West-European Proletariat?", whose position was adopted as state policy after Stalin's January 1926 article "On the Issues of Leninism" (К вопросам ленинизма). This idea was opposed by Leon Trotsky and his followers who declared the need for an international "permanent revolution". Various Fourth Internationalist groups around the world who describe themselves as Trotskyist see themselves as standing in this tradition, while Maoist China supported Socialism in One Country.

European social democrats strongly support Europeanism and supranational integration, although there is a minority of nationalists and eurosceptics also in the left. Some link this left-wing nationalism to the pressure generated by economic integration with other countries encouraged by free trade agreements. This view is sometimes used to justify hostility towards supranational organizations. Left-wing nationalism can also refer to any nationalism which emphasises a working-class populist agenda which seeks to overcome perceived exploitation or oppression by other nations. Many Third World anti-colonial movements adopted left-wing and socialist ideas.

Third-Worldism is a tendency within leftist thought that regards the division between First World developed countries and Third World developing countries as being of high political importance. This tendency supports national liberation movements against what it considers imperialism by capitalists. Third-Worldism is closely connected with African socialism, Latin American socialism, Maoism, Pan-Africanism and Pan-Arabism. Some left-wing groups in the developing world – such as the Zapatista Army of National Liberation in Mexico, the Abahlali baseMjondolo in South Africa and the Naxalites in India – argue that the First World Left takes a racist and paternalistic attitude towards liberation movements in the Third World.

The original French left-wing was anti-clerical, opposing the influence of the Roman Catholic Church and supporting the separation of church and state. Karl Marx asserted that "[r]eligion is the sigh of the oppressed creature, the heart of a heartless world, and the soul of soulless conditions. It is the opium of the people". In Soviet Russia, the Bolsheviks originally embraced "an ideological creed which professed that all religion would atrophy" and "resolved to eradicate Christianity as such". In 1918, "ten Orthodox hierarchs were summarily shot" and "children were deprived of any religious education outside the home". Today in the Western world those on the Left usually support secularization and the separation of church and state.

However, religious beliefs have also been associated with some left-wing movements, such as the civil rights movement and the anti-capital punishment movement. Early socialist thinkers such as Robert Owen, Charles Fourier and the Comte de Saint-Simon based their theories of socialism upon Christian principles. From St. Augustine of Hippo's "City of God" through St. Thomas More's "Utopia", major Christian writers defended ideas that socialists found agreeable. Other common leftist concerns such as pacifism, social justice, racial equality, human rights and the rejection of excessive wealth can be found in the Bible. In the late 19th century, the Social Gospel movement arose (particularly among some Anglicans, Lutherans, Methodists and Baptists in North America and Britain) which attempted to integrate progressive and socialist thought with Christianity in faith-based social activism, promoted by movements such as Christian socialism. In the 20th century, the theology of liberation and Creation Spirituality was championed by such writers as Gustavo Gutierrez and Matthew Fox.

Other left-wing religious movements include Islamic socialism and Buddhist socialism. There have been alliances between the left and anti-war Muslims, such as the Respect Party and the Stop the War Coalition in Britain. In France, the left has been divided over moves to ban the hijab from schools, with some supporting a ban based on separation of church and state and others opposing the prohibition based on personal freedom.

Social progressivism is another common feature of modern leftism, particularly in the United States, where social progressives played an important role in the abolition of slavery, women's suffrage, civil rights and multiculturalism. Progressives have both advocated prohibition legislation and worked towards its repeal. Current positions associated with social progressivism in the West include opposition to the death penalty and the War on Drugs, support for abortion rights, cognitive liberty, LGBT rights including legal recognition of same-sex marriage, distribution of contraceptives, and public funding of embryonic stem-cell research. Public education was a subject of great interest to groundbreaking social progressives such as Lester Frank Ward and John Dewey, who believed that a democratic system of government was impossible without a universal and comprehensive system of education.

Various counterculture movements in the 1960s and 1970s were associated with the "New Left". Unlike the earlier leftist focus on union activism, the New Left instead adopted a broader definition of political activism commonly called social activism. The New Left in the United States is associated with the hippie movement, college campus mass protest movements, and a broadening of focus from protesting class-based oppression to include issues such as gender, race and sexual orientation. The British New Left was an intellectually driven movement which attempted to correct the perceived errors of "Old Left".

The New Left opposed prevailing authority structures in society, which it termed "The Establishment" and became known as "anti-Establishment". The New Left did not seek to recruit industrial workers but rather concentrated on a social activist approach to organization, convinced that they could be the source for a better kind of social revolution. This view has been criticised by some Marxists (especially Trotskyists) who characterized this approach as "substitutionism", which they described as a misguided and non-Marxist belief that other groups in society could "substitute" for the revolutionary agency of the working class.

Many early feminists and advocates of women's rights were considered left-wing by contemporaries. Feminist pioneer Mary Wollstonecraft was influenced by the radical thinker Thomas Paine. Many notable leftists have been strong supporters of gender equality, such as the Marxists Rosa Luxemburg, Clara Zetkin, and Alexandra Kollontai; anarchists such as Virginia Bolten, Emma Goldman, and Lucía Sánchez Saornil; and socialists Helen Keller and Annie Besant. However, Marxists such as Rosa Luxemburg, Clara Zetkin and Alexandra Kollontai, though supporters of radical social equality for women, opposed feminism because they considered it to be a bourgeois ideology. Marxists were responsible for organizing the first International Working Women's Day events.

The women's liberation movement is closely connected to the New Left and other new social movements that challenged the orthodoxies of the Old Left. Socialist feminism, as exemplified by the Freedom Socialist Party and Radical Women; and Marxist feminism, as with Selma James, saw themselves as a part of the left that challenged what they perceive to be male-dominated and sexist structures within the Left. Liberal feminism is closely connected with social liberalism, and in America, with the left wing of mainstream politics (e.g., National Organization for Women).

The connection between left-leaning ideologies and LGBT rights struggles also has an important history. Prominent socialists who were involved in early struggles for LGBT rights include Edward Carpenter, Oscar Wilde, Harry Hay, Bayard Rustin, and Daniel Guérin, among others.

The spectrum of left-wing politics ranges from center-left to far-left (or ultra-left). The term "center-left" describes a position within the political mainstream. The terms "far-left" and "ultra-left" refer to positions that are more radical. The center-left includes social democrats, social liberals, progressives and also some democratic socialists and greens (including some eco-socialists). Center-left supporters accept market allocation of resources in a mixed economy with a significant public sector and a thriving private sector. Center-left policies tend to favour limited state intervention in matters pertaining to the public interest.

In several countries, the terms "far-left" and "radical left" have been associated with varieties of communism, autonomism and anarchism. They have been used to describe groups that advocate anti-capitalism or eco-terrorism. In France, a distinction is made between the left (Socialist Party and Communist Party) and the far-left (Trotskyists, Maoists and anarchists). The United States Department of Homeland Security defines left-wing extremism as groups that want "to bring about change through violent revolution rather than through established political processes".

In China, the term "Chinese New Left" denotes those who oppose the current economic reforms and favour the restoration of more socialist policies. In the Western world, the term "New Left" refers to cultural politics. In the United Kingdom in the 1980s, the term "hard left" was applied to supporters of Tony Benn, such as the Campaign Group and those involved in the "London Labour Briefing" newspaper, as well as Trotskyist groups such as Militant and the Alliance for Workers' Liberty. In the same period, the term "soft left" was applied to supporters of the British Labour Party who were perceived to be more moderate. Under the leadership of Tony Blair and Gordon Brown, the British Labour Party rebranded itself as New Labour in order to promote the notion that it was less left-wing than it had been in the past. One of the first actions of the Labour Party leader who succeeded them, Ed Miliband, was the rejection of the "New Labour" label. However, Labour's voting record in parliament would indicate that under Miliband it had maintained the same distance from the left as it had with Blair. Likewise, the election of Jeremy Corbyn as Labour Party leader was viewed by some as Labour turning back toward its socialist roots.

Leftist postmodernism opposes attempts to supply universal explanatory theories, including Marxism, deriding them as grand narratives. It views culture as a contested space and via deconstruction seeks to undermine all pretensions to absolute truth. Left-wing critics of post-modernism assert that cultural studies inflates the importance of culture by denying the existence of an independent reality.



</doc>
<doc id="18504" url="https://en.wikipedia.org/wiki?curid=18504" title="Los Angeles-class submarine">
Los Angeles-class submarine

The "Los Angeles" class boats are nuclear-powered fast attack submarines (SSN) in service with the United States Navy. The submarines are also known as the 688 class (pronounced "Six-Eighty-Eight"), after the hull number of lead vessel . They represent two generations and close to half a century of the Navy's attack submarine fleet, which currently has a total of 53 fast attack submarines in all classes. As of 2020, 33 of the "Los Angeles" class are still in commission and 29 are retired from service. The class has more active nuclear submarines than any other class in the world.

Of the retired boats, a few were in commission for nearly 40 years, including , and . With a wide variance in longevity, 12 of the 688s were laid up halfway through their projected lifespans, being the youngest-retired at 16 years. Another five also laid up early (20–25 years), due to their midlife reactor refueling being cancelled, and one was lost during overhaul due to arson. Two are being converted to moored training ships, and all others are being scrapped per the Navy's Ship-Submarine Recycling Program. A further four boats were proposed by the Navy, but later cancelled. Submarines of this class are named after American towns and cities, such as Albany, New York; Los Angeles, California; and Tucson, Arizona, with the exception of , named after a US Navy Admiral. This was a change from traditionally naming attack submarines after marine animals, such as or .

"Los Angeles"-class submarines were built in three successive flights: SSNs 688–718, SSNs 719–750, and SSNs 751–773. In 1982 after building 31 boats, the class underwent a minor redesign. The following eight that made up the second "flight" of subs had 12 new vertical launch tubes that could fire Tomahawk missiles. The last 23 had a significant upgrade with the 688i improvement program. These boats are quieter, with more advanced electronics, sensors, and noise-reduction technology. The diving planes are placed at the bow rather than on the sail, and are retractable.

According to the U.S. Department of Defense, the top speed of the submarines of the "Los Angeles" class is over , although the actual maximum is classified. Some published estimates have placed their top speed at . In his book "Submarine: A Guided Tour Inside a Nuclear Warship", Tom Clancy estimated the top speed of "Los Angeles"-class submarines at about .

The U.S. Navy gives the maximum operating depth of the "Los Angeles" class as , while Patrick Tyler, in his book "Running Critical", suggests a maximum operating depth of . Although Tyler cites the 688-class design committee for this figure, the government has not commented on it. The maximum diving depth is according to "Jane's Fighting Ships, 2004–2005 Edition", edited by Commodore Stephen Saunders of the Royal Navy.

"Los Angeles"-class submarines carry about 25 torpedo tube-launched weapons, as well as Mark 67 and Mark 60 CAPTOR mines and were designed to launch Tomahawk cruise missiles, and Harpoon missiles horizontally (from the torpedo tubes). The last 31 boats of this class (Flight II/688i) also have 12 dedicated vertical launching system tubes for launching Tomahawks. The tube configuration for the first two boats of Flight II differed from the later ones: "Providence" and "Pittsburgh" have four rows of three tubes vs. the inner two rows of four and outer two rows of two tubes found on other examples.

Over close to 40 years, the control suite of the class has changed dramatically. The class was originally equipped with the Mk 113 mod 10 fire control system, also known as the Pargo display program. The Mk 113 runs on a UYK-7 computer.

The Mk 117 FCS, the first "all digital" fire control system replaced the Mk 113. The Mk 117 transferred the duties of the analog Mk 75 attack director to the UYK-7, and the digital Mk 81 weapon control consoles, removing the two analog conversions, and allowing "all digital" control of the digital mk 48 control. The first 688 sub to be built with the Mk 117 was .

The Mark 1 Combat Control System/All Digital Attack Center replaced the Mk 117 FCS, on which it was based. The Mk 1 CCS was built by Lockheed Martin, and gave the class the ability to fire Tomahawk missiles. The CSS internal tracker model provides processing for both towed-array and spherical-array trackers. Trackers are signal followers that generate bearing, arrival angle, and frequency reports based on information received by an acoustic sensor. It incorporated the Gyro Static Navigator into the system in replacement of the DMINS of the earlier 688 class.

The Mk 1 CCS was replaced by the Mk 2, which was built by Raytheon. Mk 2 provides Tomahawk Block III vertical launch capability as well as fleet-requested improvements to Mk 48 ADCAP torpedo and Towed Array Target Motion Analysis operability. The Mk 2 CCS paired with the AN/BQQ-5E system is referred to as the QE-2" system. The CCS MK2 Block 1 A/B system architecture extends the CCS MK2 tactical system with a network of tactical advanced computers (TAC-3). These TAC-3s are configured to support the SFMPL, NTCS-A, LINK-11 and ATWCS subsystems.

 sensor suite consists of the AN/BQS-13 spherical sonar array and AN/UYK-44 computer. The AN/BQQ-5 was developed from the AN/BQQ-2 sonar system. The BQS 11, 12, and 13 spherical arrays have 1,241 transducers. Also equipped are a conformal hull array with 104 to 156 hydrophones and two towed arrays: the TB-12 (later replaced by the TB-16) and TB-23 or TB-29, of which there are multiple variants. There are 5 versions of the AN/BQQ-5 system, sequentially identified by letters A-E.

The 688i (Improved) subclass was initially equipped with the AN/BSY-1 SUBACS submarine advanced combat system that used an AN/BQQ-5E sensor system with updated computers and interface equipment. Development of the AN/BSY-1 and its sister the AN/BSY-2 for the was widely reported as one of the most problematic programs for the Navy, its cost and schedule suffering many setbacks.

A series of conformal passive hydrophones are hard-mounted to each side of the hull, using the AN/BQR-24 internal processor. The system uses FLIT (frequency line integration tracking) which homes in on precise narrowband frequencies of sound and, using the Doppler principle, can accurately provide firing solutions against very quiet submarines. The AN/BQQ-5's hull array doubled the performance of its predecessors.

The AN/BQQ-5 system was replaced by the AN/BQQ-10 system. Acoustic Rapid Commercial Off-The-Shelf Insertion (A-RCI), designated AN/BQQ-10, is a four-phase program for transforming existing submarine sonar systems (AN/BSY-1, AN/BQQ-5, and AN/BQQ-6) from legacy systems to a more capable and flexible COTS/Open System Architecture (OSA) and also provide the submarine force with a common sonar system. A single A-RCI Multi-Purpose Processor (MPP) has as much computing power as the entire "Los Angeles" (SSN-688/688I) submarine fleet combined and will allow the development and use of complex algorithms previously beyond the reach of legacy processors. The use of COTS/OSA technologies and systems will enable rapid periodic updates to both software and hardware. COTS-based processors will allow computer power growth at a rate commensurate with the commercial industry.

Two watertight compartments are used in the "Los Angeles"-class submarines. The forward compartment contains crew living spaces, weapons-handling spaces, and control spaces not critical to recovering propulsion. The aft compartment contains the bulk of the submarine's engineering systems, power generation turbines, and water-making equipment. Some submarines in the class are capable of delivering Navy SEALs through either a SEAL Delivery Vehicle deployed from the Dry Deck Shelter or the Advanced SEAL Delivery System mounted on the dorsal side, although the latter was canceled in 2006 and removed from service in 2009. A variety of atmospheric control devices are used to allow the vessel to remain submerged for long periods of time without ventilating, including an electrolytic oxygen generator, which produces oxygen for the crew and hydrogen as a byproduct. The hydrogen is pumped overboard but there is always a risk of fire or explosion from this process.
While on the surface or at snorkel depth, the submarine may use the submarine's auxiliary or emergency diesel generator for power or ventilation (e.g., following a fire). The diesel engine in a 688 class can be quickly started by compressed air during emergencies or to evacuate noxious (nonvolatile) gases from the boat, although 'ventilation' requires raising a snorkel mast. During nonemergency situations, design constraints call for operators to allow the engine to reach normal operating temperatures before it is capable of producing full power, a process that may take from 20 to 30 minutes. However, the diesel generator can be immediately loaded to 100% power output, despite design criteria cautions, at the discretion of the submarine commander on the recommendation of the submarine's engineer, if necessity dictates such actions to: (a) restore electrical power to the submarine, (b) prevent a reactor incident from occurring or escalating, or (c) to protect the lives of the crew or others as determined necessary by the commanding officer.

The "Los Angeles" class is powered by the General Electric S6G pressurized water reactor. The hot reactor coolant water heats water in the steam generators, producing steam to power the propulsion turbines and ship service turbine generators (SSTGs), which generate the submarine's electrical power. The high-speed propulsion turbines drive the shaft and propeller through a reduction gear. In the case of a reactor plant casualty, the submarine has a diesel generator and a bank of batteries to provide electrical power. An emergency propulsion motor on the shaft line or a retractable 325-hp secondary propulsion motor power the submarine off the battery or diesel generator.

The S6G reactor plant was originally designed to use the D1G-2 core, similar to the D2G reactor used on the guided missile cruiser. The D1G-2 core had a rated thermal power of 150 MW and the turbines were rated at 30,000 shp. All "Los Angeles"-class submarines from on were built with a D2W core and older submarines with D1G-2 cores have been refueled with D2W cores. The D2W core is rated at 165 MW and turbine power rose to approximately 33,500 shp.





</doc>
<doc id="18507" url="https://en.wikipedia.org/wiki?curid=18507" title="Lucretia">
Lucretia

According to Roman tradition, Lucretia ( , ; died ), anglicized as Lucrece, was a noblewoman in ancient Rome whose rape by Sextus Tarquinius (Tarquin), son of the last king of Rome, was the cause of a rebellion that overthrew the Roman monarchy and led to the transition of Roman government from a kingdom to a republic. There are no contemporary sources; information regarding Lucretia, her rape and suicide, and the consequence of this being the start of the Roman Republic, come from the later accounts of Roman historian Livy and Greco-Roman historian Dionysius of Halicarnassus.

The incident kindled the flames of dissatisfaction over the tyrannical methods of the last king of Rome, Lucius Tarquinius Superbus. As a result, the prominent families instituted a republic, drove the extensive royal family of Tarquin from Rome, and successfully defended the republic against attempted Etruscan and Latin intervention. The rape itself became a major theme in European art and literature.

One of the first two consuls of the Roman Republic, Lucius Tarquinius Collatinus was Lucretia's husband. All the numerous sources on the establishment of the republic reiterate the basic events of Lucretia's story, though accounts vary slightly. Lucretia's story is not considered a myth by most historians, but rather a historical legend about an early history that was already a major part of Roman folklore before it was first written about. The evidence points to the historical existence of a woman named Lucretia and a historical incident that played a critical part in the real downfall of a real monarchy. Many of the specific details, though, are debatable, and vary depending on the writer. Post-Roman uses of the legend typically became mythical in portrayal, being of artistic rather than historical merit.

As the events of the story move rapidly, the date of the incident is probably the same year as the first of the "fasti". Dionysius of Halicarnassus, a major source, sets this year "at the beginning of the sixty-eighth Olympiad ... Isagoras being the annual archon at Athens"; that is, 508/507 BC (the ancient calendars split years over modern ones). According to this source Lucretia therefore died in 508 BC. The other historical sources tend to support this date, but the year is debatable within a range of about five years.

Lucretia is the daughter of Spurius Lucretius and wife of Lucius Tarquinius Collatinus. Prior to the rape, while her husband was a man of excellent social standing, he had no political power or standing in Rome. He lacked both the power and the wealth of his Tarquinius relatives. Her marriage was depicted as being the ideal Roman marriage as both Lucretia and Lucius were faithfully devoted to one another. She was described as beautiful and virtuous by Roman writer Livy. While her husband was away at battle, Lucretia would stay at home and pray for his safe return. Similar to Livy, Dionysius' depiction of Lucretia, separates her from the rest of Roman women in a story about the men returning home from a battle. The narrative begins with a bet between the sons of Tarquinius and their kinsmen, Brutus and Collantinus. The men fight over which of their wives best exemplified sophrosyne. The men return home to find the women socializing with each other, presumably drinking and in conversation. In contrast, they find Lucretia home alone working with her wool in silence. Roman writers such as Livy, Ovid and Dionysus, described Lucretia as being the role model for Roman girls because of her devotion to her husband.

Lucius Tarquinius Superbus, last king of Rome, being engaged in the siege of Ardea, sent his son, Tarquin, on a military errand to Collatia. Tarquin was received with great hospitality at the governor's mansion, home of Lucius Tarquinius Collatinus, son of the king's nephew, Arruns Tarquinius, former governor of Collatia and first of the Tarquinii Collatini. Collatinus' wife, Lucretia, daughter of Spurius Lucretius, prefect of Rome, "a man of distinction," made sure that the king's son was treated as became his rank, although her husband was away at the siege.

In a variant of the story, Tarquin and Collatinus, at a wine party on furlough, were debating the virtues of wives when Collatinus volunteered to settle the debate by all of them riding to his home to see what Lucretia was doing. She was weaving with her maids. The party awarded her the palm of victory and Collatinus invited them to visit, but for the time being they returned to camp.

At night, Tarquin entered her bedroom by stealth, quietly going around the slaves who were sleeping at her door. She awakened. He identified himself and offered her two choices: she could submit to his sexual advances and become his wife and future queen, or he would kill her and one of her slaves and place the bodies together, then claim he had caught her having adulterous sex (see sexuality in ancient Rome for Roman attitudes toward sex). In the alternative story, he returned from camp a few days later with one companion to take Collatinus up on his invitation to visit and was lodged in a guest bedroom. He entered Lucretia's room while she lay naked in her bed and started to wash her belly with water, which woke her up.

In Dionysius of Halicarnassus' account, the following day Lucretia dressed in black and went to her father's house in Rome and cast herself down in the supplicant's position (embracing the knees), weeping in front of her father and husband. She asked to explain herself and insisted on summoning witnesses before she told them about her rape. After disclosing the rape, she asked them for vengeance, a plea that could not be ignored because she was speaking to the chief magistrate of Rome. While the men debated the proper course of action, Lucretia drew a concealed dagger and stabbed herself in the heart. She died in her father's arms, with the women present keening and lamenting on her death. Dionysius' depiction of her suicide he stated that: "This dreadful scene struck the Romans who were present with so much horror and compassion that they all cried out with one voice that they would rather die a thousand deaths in defense of their liberty than suffer such outrages to be committed by the tyrants."

In Livy's version, Lucretia did not go to Rome, but instead sent for her father and her husband asking them to bring one friend each to act as an witness. Those selected were Publius Valerius Publicola from Rome and Lucius Junius Brutus from the camp at Ardea. The men found Lucretia in her room and she explained what had happened to her. After exacting an oath of vengeance—"Pledge me your solemn word that the adulterer shall not go unpunished."— and while the men were discussing the matter, she drew a poignard and stabbed herself in her heart.

In Dio's version, Lucretia's request for revenge is: "And, whereas I (for I am a woman) shall act in a manner which is fitting for me: you, if you are men, and if you care for your wives and children, exact vengeance on my behalf and free your selves and show the tyrants what sort of woman they outraged, and what sort of men were her menfolk!" She follows her statement by plunging the dagger into her chest and promptly dying.

In this version Collatinus and Brutus were encountered returning to Rome unaware of the incident, were briefed, and were brought to the death scene. Brutus happened to be a politically motivated participant. By kinship he was a Tarquin on his mother's side, the son of Tarquinia, daughter of Lucius Tarquinius Priscus, the third king before last. He was a candidate for the throne if anything should happen to Superbus. By law, however, as he was a Junius on his father's side, and thus he was not a Tarquin and therefore could later propose the exile of the Tarquins without fear for himself. Superbus had taken his inheritance and left him a pittance, keeping him at court for entertainment.
Collatinus, seeing his wife dead, became distraught. He held her, kissed her, called her name and spoke to her. Dio stated that after seeing the hand of Destiny in these events, Brutus called the grieving party to order, explained that his simplicity had been a sham, and proposed that they drive the Tarquins from Rome. Grasping the bloody dagger, he swore by Mars and all the other gods that he would do everything in his power to overthrow the dominion of the Tarquinii and that he would neither be reconciled to the tyrants himself nor tolerate any who should be reconciled to them, but would look upon every man who thought otherwise as an enemy and til his death would pursue with unrelenting hatred both the tyranny and its abettors; and if he should violate his oath, he prayed that he and his children might meet with the same end as Lucretia.

He passed the dagger around and each mourner swore the same oath by it. The primary sources of both Dio and Livy agree on this point: Livy's version is:

By this blood—most pure before the outrage wrought by the king's son—I swear, and you, O gods, I call to witness that I will drive hence Lucius Tarquinius Superbus, together with his cursed wife and his whole blood, with fire and sword and every means in my power, and I will not suffer them or anyone else to reign in Rome.

The newly sworn revolutionary committee paraded the bloody corpse of Lucretia to the Roman Forum where it remained on display as a reminder of the dishonor committed. At the form, the committee heard grievances against the Tarquins and began to enlist an army to abolish the monarchy. Brutus "urged them to act as men and Romans and take up arms against their insolent foes." The gates of Rome were blockaded by the new revolutionary soldiers and more were sent to guard Collatia. By now a crowd had gathered in the forum; the presence of the magistrates among the revolutionaries kept them in good order.

Brutus was the Tribune of the Celeres, a minor office of some religious duties, which as a magistracy gave him the theoretical power to summon the curiae, an organization of patrician families used mainly to ratify the decrees of the king. Summoning them on the spot he transformed the crowd into an authoritative legislative assembly and began to address them in one of the more noted and effective speeches of ancient Rome.

He began by revealing that his pose as fool was a sham designed to protect him against an evil king. He levelled a number of charges against the king and his family: the outrage against Lucretia, whom everyone could see on the dais, the king's tyranny, the forced labor of the plebeians in the ditches and sewers of Rome. In his speech, he pointed out that Superbus had come to rule by the murder of Servius Tullius, his wife's father, next-to-the-last king of Rome. He "solemnly invoked the gods as the avengers of murdered parents." He suggested that as the king's wife, Tullia, was in fact in Rome and probably was a witness to the proceedings from her palace near the forum. Seeing herself the target of so much animosity she fled from the palace in fear of her life and proceeded to the camp at Ardea.

Brutus opened a debate on the form of government Rome ought to have, a debate at which many patricians spoke. In summation he proposed the banishment of the Tarquins from all the territories of Rome and appointment of an interrex to nominate new magistrates and conduct an election of ratification. They decided on a republican form of government with two consuls in place of a king executing the will of a patrician senate. This was a temporary measure until they could consider the details more carefully. Brutus renounced all right to the throne. In subsequent years the powers of the king were divided among various elected magistracies.

A final vote of the curiae carried the interim constitution. Spurius Lucretius was swiftly elected interrex; he was prefect of the city already. He proposed Brutus and Collatinus as the first two consuls and that choice was ratified by the curiae. Needing to acquire the assent of the population as a whole, they paraded Lucretia through the streets summoning the plebeians to legal assembly in the forum. Once there they heard a constitutional speech by Brutus not unlike many speeches and documents of western civilization subsequently. It began:

A general election was held and the vote won in favor of the republic. This ended the monarchy, and during these proceedings Lucretia was still displayed in the forum.

The constitutional consequences of this event were, prevented Rome from having another hereditary "king," later emperors were absolute rulers in all but name. This constitutional tradition prevented both Julius Caesar and Octavian Augustus from accepting a crown; instead they had to devise a confluence of several republican offices onto their persons in order to secure absolute power. Their successors both in Rome and in Constantinople adhered to this tradition in form if not in essence, and the office of German Holy Roman Emperor remained elective rather than hereditary—up to its abolition in the Napoleonic Wars, over 2300 years later.

Livy's account in "Ab Urbe Condita Libri" (c. 25–8 BC) is the earliest surviving full historical treatment. In his account her husband has boasted of the virtue of his wife to Tarquin and others. Livy contrasts the virtue of the Roman Lucretia, who remained in her room weaving, with the Etruscan ladies who feasted with friends. Ovid recounts the story of Lucretia in Book II of his "Fasti", published in 8 AD, concentrating on the bold over-reaching character of Tarquin. Later, St. Augustine made use of the figure of Lucretia in "The City of God" (published 426 AD) to defend the honour of Christian women who had been raped in the sack of Rome and had not committed suicide.

The story of Lucretia was a popular moral tale in the later Middle Ages. Lucretia appears to Dante in the section of Limbo reserved to the nobles of Rome and other "virtuous pagans" in Canto IV of the "Inferno". Christine de Pizan used Lucretia just as St. Augustine of Hippo did in her "City of Ladies", defending a woman's sanctity.

The myth is recounted in Geoffrey Chaucer's "The Legend of Good Women", and it follows a similar story line to Livy's. Lucretia calls for her father and husband, but Chaucer's tale also has her call for her mother and attendants as well, whereas Livy's has both her father and husband bring an friend as witness. The tale also deviated from Livy's account as it begins with her husband coming home to surprise her, rather than the men placing a bet on the virtue of their wives.

John Gower's "Confessio Amantis" (Book VII), and John Lydgate's "Fall of Princes recount the myth of Lucretia". Gower's work is a collection of narritive poems. In Book VII, he tells the "Tale of the Rape of Lucrece." Lydgate's work is a long poem containing stories and myths about various kings and princes who fell from power. It follows their lives from their rise into power and their fall into adversity. Lydgate's poem mentions the fall of Tarquin, the rape and suicide of Lucretia and her speech prior to death.

Lucretia's rape and suicide is also the subject of William Shakespeare's 1594 long poem "The Rape of Lucrece", which draws extensively on Ovid's treatment of the story; he also mentioned her in "Titus Andronicus", in "As You Like It", and in "Twelfth Night" in which Malvolio authenticates his fateful letter by spotting Olivia's Lucrece seal, and alludes to her in "Macbeth", and in "Cymbeline" he further refers to the story, though without mentioning Lucretia by name. Shakespeare's poem, based on the rape of Lucretia, draws on the beginning of the Livy's account of the incident. The poem begins with a bet between husbands about the virtuousness of their wives. Shakespeare draws on the idea of Lucretia as a moral agent, as Livy did, when he explores his characters response to death and her unwillingness to yield to her rapist. A direct excerpt from Livy is used when Shakespeare prefaces his poem with a brief prose called "Argument". This is the internal deliberation Lucrece suffered from, following the rape.

Niccolò Machiavelli's comedy "La Mandragola" is loosely based on the Lucretia story.

She is also mentioned in the poem "Appius and Virginia" by John Webster and Thomas Heywood, which includes the following lines:

Thomas Heywood's play "The Rape of Lucretia" dates from 1607. The subject also enjoyed a revival in the mid twentieth century; André Obey's 1931 play "" was adapted by librettist Ronald Duncan for "The Rape of Lucretia", a 1946 opera by Benjamin Britten which premiered at Glyndebourne. Ernst Krenek set Emmet Lavery's libretto "Tarquin" (1940), a version in a contemporary setting.

Jacques Gallot (died ) composed the allemandes "Lucrèce" and "Tarquin" for baroque lute.

In Samuel Richardson's 1740 novel "Pamela", Mr. B. cites the story of Lucretia as a reason why Pamela ought not fear for her reputation, should he rape her. Pamela quickly sets him straight with a better reading of the story. Colonial Mexican poet Sor Juana Inés de la Cruz also mentions Lucrecia in her poem "Redondillas," a commentary on prostitution and who is to blame.

In 1769, doctor Juan Ramis wrote a tragedy in Menorca entitled "Lucrecia". The play is written in the Catalan language using a neoclassical style and is the most important work of the eighteenth century written in this language.

In 1932, the play "Lucrece" was produced on Broadway starring legendary actress Katharine Cornell in the title part. It was mostly performed in pantomime.

In Donna Leon's 2009 Venetian novel, "About Face", Franca Marinello refers to the tale of Tarquin and Lucrezia, as recounted in Ovid's " Fasti " (Book II, for February 24, "Regifugium") to explain her actions to Commissario Brunetti.

Since the Renaissance, the suicide of Lucretia has been an enduring subject for visual artists, including Titian, Rembrandt, Dürer, Raphael, Botticelli, Jörg Breu the Elder, Johannes Moreelse, Artemisia Gentileschi, Damià Campeny, Eduardo Rosales, Lucas Cranach the Elder, and others. Most commonly, either the moment of the rape is shown or Lucretia is shown alone at the moment of her suicide. In either situation, her clothing is loosened or absent, while Tarquin is normally clothed.

The subject was one of a group showing women from legend or the Bible who were either powerless, such as Susanna and Verginia, or only able to escape their situations by suicide, such as Dido of Carthage and Lucretia. These formed a counterpoint to, or sub-group of, the set of subjects known as the Power of Women, showing female violence against, or domination of, men. These were often depicted by the same artists, and especially popular in Northern Renaissance art. The story of Esther lay somewhere between these two extremes.

The subject of Lucretia spinning, with her ladies, is sometimes depicted, as in a series of four engravings of her story by Hendrick Goltzius, which also includes a banquet.







</doc>
