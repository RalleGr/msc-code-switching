<doc id="14071" url="https://en.wikipedia.org/wiki?curid=14071" title="Humanae vitae">
Humanae vitae

Humanae vitae (Latin: "Of Human Life") is an encyclical written by Pope Paul VI and dated 25 July 1968. The text was issued at a Vatican press conference on 29 July. Subtitled "On the Regulation of Birth", it re-affirmed the teaching of the Catholic Church regarding married love, responsible parenthood, and the rejection of artificial contraception. In formulating his teaching he explained why he did not accept the conclusions of the Pontifical Commission on Birth Control established by his predecessor, Pope John XXIII, a commission he himself had expanded.

Mainly because of its restatement of the Church's opposition to artificial contraception, the encyclical was politically controversial. It affirmed traditional Church moral teaching on the sanctity of life and the procreative and unitive nature of conjugal relations.

It was the last of Paul's seven encyclicals.

In this encyclical Paul VI reaffirmed the Catholic Church's view of marriage and marital relations and a continued condemnation of "artificial" birth control. There were two Papal committees and numerous independent experts looking into the latest advancement of science and math on the question of artificial birth control, which were noted by the Pope in his encyclical. The expressed views of Paul VI reflected the teachings of his predecessors, especially Pius XI, Pius XII and John XXIII, all of whom had insisted on the divine obligations of the marital partners in light of their partnership with God the creator.

Paul VI himself, even as commission members issued their personal views over the years, always reaffirmed the teachings of the Church, repeating them more than once in the first years of his Pontificate.

To Pope Paul VI, marital relations are much more than a union of two people. In his view, they constitute a union of the loving couple with a loving God, in which the two persons generate the matter for the body, while God creates the unique soul of a person. For this reason, Paul VI teaches in the first sentence of "Humanae Vitae", that the "transmission of human life is a most serious role in which married people collaborate freely and responsibly with God the Creator." This is divine partnership, so Paul VI does not allow for arbitrary human decisions, which may limit divine providence. According to Paul VI, marital relations are a source of great joy, but also of difficulties and hardships. The question of human procreation with God, exceeds in the view of Paul VI specific disciplines such as biology, psychology, demography or sociology. According to Paul VI, married love takes its origin from God, who is love, and from this basic dignity, he defines his position:
The encyclical opens with an assertion of the competency of the magisterium of the Catholic Church to decide questions of morality. It then goes on to observe that circumstances often dictate that married couples should limit the number of children, and that the sexual act between husband and wife is still worthy even if it can be foreseen not to result in procreation. Nevertheless, it is held that the sexual act must retain its intrinsic relationship to the procreation of human life.

Every action specifically intended to prevent procreation is forbidden, except in medically necessary circumstances. Therapeutic means necessary to cure diseases are exempted, even if a foreseeable impediment to procreation should result, but only if infertility is not directly intended. This is held to directly contradict the moral order which was established by God. Abortion, even for therapeutic reasons, is absolutely forbidden, as is sterilization, even if temporary. Therapeutic means which induce infertility are allowed ("e.g.", hysterectomy), if they are not specifically intended to cause infertility (e.g., the uterus is cancerous, so the preservation of life is intended). If there are well grounded reasons (arising from the physical or psychological condition of husband or wife, or from external circumstances), Natural family planning methods (abstaining from intercourse during certain parts of the menstrual cycle) are allowed, since they take advantage of a faculty provided by nature.

The acceptance of artificial methods of birth control is then claimed to result in several negative consequences, among them a general lowering of moral standards resulting from sex without consequences, and the danger that men may reduce women to being a mere instrument for the satisfaction of [their] own desires; finally, abuse of power by public authorities, and a false sense of autonomy.

Public authorities should oppose laws which undermine natural law; scientists should further study effective methods of natural birth control; doctors should further familiarize themselves with this teaching, in order to be able to give advice to their patients, priests must spell out clearly and completely the Church's teaching on marriage. The encyclical acknowledges that "perhaps not everyone will easily accept this particular teaching", but that "...it comes as no surprise to the church that she, no less than her Divine founder is destined to be a sign of contradiction." Noted is the duty of proclaiming the entire moral law, "both natural and evangelical." The encyclical also points out that the Roman Catholic Church cannot "declare lawful what is in fact unlawful", because she is concerned with "safeguarding the holiness of marriage, in order to guide married life to its full human and Christian perfection." This is to be the priority for his fellow bishops and priests and lay people. The Pope predicts that future progress in social cultural and economic spheres will make marital and family life more joyful, provided God's design for the world is faithfully followed. The encyclical closes with an appeal to observe the natural laws of the Most High God. "These laws must be wisely and lovingly observed."

There had been a long-standing general Christian prohibition on contraception and abortion, with such Church Fathers as Clement of Alexandria and Saint Augustine condemning the practices. It was not until the 1930 Lambeth Conference that the Anglican Communion allowed for contraception in limited circumstances. Mainline Protestant denominations have since removed prohibitions against artificial contraception. In a partial reaction, Pope Pius XI wrote the encyclical "Casti connubii" ("On Christian Marriage") in 1930, reaffirming the Catholic Church's belief in various traditional Christian teachings on marriage and sexuality, including the prohibition of artificial birth control even within marriage. "Casti connubii" is against contraception and regarding natural family planning allowed married couples to use their nuptial rights "in the proper manner" when because of either time or defects, new life could not be brought forth.

With the appearance of the first oral contraceptives in 1960, dissenters in the Church argued for a reconsideration of the Church positions. In 1963 Pope John XXIII established a commission of six European non-theologians to study questions of birth control and population. It met once in 1963 and twice in 1964. As Vatican Council II was concluding, Pope Paul VI enlarged it to fifty-eight members, including married couples, laywomen, theologians and bishops. The last document issued by the council ("Gaudium et spes") contained a section titled "Fostering the Nobility of Marriage" (1965, nos. 47-52), which discussed marriage from the personalist point of view. The "duty of responsible parenthood" was affirmed, but the determination of licit and illicit forms of regulating birth was reserved to Pope Paul VI. In the spring of 1966, following the close of the council, the commission held its fifth and final meeting, having been enlarged again to include sixteen bishops as an executive committee. The commission was only consultative but it submitted a report approved by a majority of 64 members to Paul VI. It proposed he approve of artificial contraception without distinction of the various means. A minority of four members opposed this report and issued a parallel report to the Pope. Arguments in the minority report, against change in the church's teaching, were that "we should have to concede frankly that the Holy Spirit had been on the side of the Protestant churches in 1930" (when "Casti connubii" was promulgated) and that "it should likewise have to be admitted that for a half a century the Spirit failed to protect Pius XI, Pius XII, and a large part of the Catholic hierarchy from a very serious error."

After two more years of study and consultation, the pope issued "Humanae vitae", which removed any doubt that the Church views hormonal anti-ovulants as contraceptive. He explained why he did not accept the opinion of the majority report of the commission (1968, #6). Arguments were raised in the decades that followed that his decision has never passed the condition of "reception" to become church doctrine.

In his role as Theologian of the Pontifical Household Mario Luigi Ciappi advised Pope Paul VI during the drafting of "Humanae vitae". Ciappi, a doctoral graduate of the "Pontificium Athenaeum Internationale Angelicum", the future Pontifical University of Saint Thomas Aquinas, "Angelicum", served as professor of dogmatic theology there and was Dean of the "Angelicum's" Faculty of Theology from 1935 to 1955.

According to George Weigel, Paul VI named Archbishop Karol Wojtyła (later Pope John Paul II) to the commission, but Polish government authorities would not permit him to travel to Rome. Wojtyła had earlier defended the church's position from a philosophical standpoint in his 1960 book "Love and Responsibility". Wojtyła's position was strongly considered and it was reflected in the final draft of the encyclical, although much of his language and arguments were not incorporated. Weigel attributes much of the poor reception of the encyclical to the omission of many of Wojtyła's arguments.

In 2017, anticipating the 50th anniversary of the encyclical, four theologians led by Mgr. Gilfredo Marengo, a professor of theological anthropology at the Pontifical John Paul II Institute for Studies on Marriage and Family, launched a research project he called "a work of historical-critical investigation without any aim other than reconstructing as well as possible the whole process of composing the encyclical". Using the resources of the Vatican Secret Archives and the Congregation for the Doctrine of the Faith, they hope to detail the writing process and the interaction between the commission, publicity surrounding the commission's work, and Paul's own authorship.

13. Men rightly observe that a conjugal act imposed on one's partner without regard to his or her condition or personal and reasonable wishes in the matter, is no true act of love, and therefore offends the moral order in its particular application to the intimate relationship of husband and wife. If they further reflect, they must also recognize that an act of mutual love which impairs the capacity to transmit life which God the Creator, through specific laws, has built into it, frustrates His design which constitutes the norm of marriage, and contradicts the will of the Author of life. Hence to use this divine gift while depriving it, even if only partially, of its meaning and purpose, is equally repugnant to the nature of man and of woman, and is consequently in opposition to the plan of God and His holy will. But to experience the gift of married love while respecting the laws of conception is to acknowledge that one is not the master of the sources of life but rather the minister of the design established by the Creator. Just as man does not have unlimited dominion over his body in general, so also, and with more particular reason, he has no such dominion over his specifically sexual faculties, for these are concerned by their very nature with the generation of life, of which God is the source. "Human life is sacred—all men must recognize that fact," Our predecessor Pope John XXIII recalled. "From its very inception it reveals the creating hand of God."

15. ...the Church does not consider at all illicit the use of those therapeutic means necessary to cure bodily diseases, even if a foreseeable impediment to procreation should result therefrom — provided such impediment is not directly intended.

16. ...If therefore there are well-grounded reasons for spacing births, arising from the physical or psychological condition of husband or wife, or from external circumstances, the Church teaches that married people may then take advantage of the natural cycles immanent in the reproductive system and engage in marital intercourse only during those times that are infertile, thus controlling birth in a way which does not in the least offend the moral principles which We have just explained.

18. It is to be anticipated that perhaps not everyone will easily accept this particular teaching. There is too much clamorous outcry against the voice of the Church, and this is intensified by modern means of communication. But it comes as no surprise to the Church that it, no less than its divine Founder, is destined to be a "sign of contradiction.") The Church does not, because of this, evade the duty imposed on it of proclaiming humbly but firmly the entire moral law, both natural and evangelical. Since the Church did not make either of these laws, it cannot be their arbiter—only their guardian and interpreter. It could never be right for the Church to declare lawful what is in fact unlawful, since that, by its very nature, is always opposed to the true good of man. In preserving intact the whole moral law of marriage, the Church is convinced that it is contributing to the creation of a truly human civilization. The Church urges man not to betray his personal responsibilities by putting all his faith in technical expedients. In this way it defends the dignity of husband and wife. This course of action shows that the Church, loyal to the example and teaching of the divine Savior, is sincere and unselfish in its regard for men whom it strives to help even now during this earthly pilgrimage "to share God's life as sons of the living God, the Father of all men".

23. We are fully aware of the difficulties confronting the public authorities in this matter, especially in the developing countries. In fact, We had in mind the justifiable anxieties which weigh upon them when We published Our encyclical letter "Populorum Progressio". But now We join Our voice to that of Our predecessor John XXIII of venerable memory, and We make Our own his words: "No statement of the problem and no solution to it is acceptable which does violence to man's essential dignity; those who propose such solutions base them on an utterly materialistic conception of man himself and his life. The only possible solution to this question is one which envisages the social and economic progress both of individuals and of the whole of human society, and which respects and promotes true human values." No one can, without being grossly unfair, make divine Providence responsible for what clearly seems to be the result of misguided governmental policies, of an insufficient sense of social justice, of a selfish accumulation of material goods, and finally of a culpable failure to undertake those initiatives and responsibilities which would raise the standard of living of peoples and their children.

Cardinal Leo Joseph Suenens, a moderator of the ecumenical council, questioned, "whether moral theology took sufficient account of scientific progress, which can help determine, what is according to nature. I beg you my brothers let us avoid another Galileo affair. One is enough for the Church." In an interview in "Informations Catholiques Internationales" on 15 May 1969, he criticized the Pope’s decision again as frustrating the collegiality defined by the Council, calling it a non-collegial or even an anti-collegial act. He was supported by Vatican II theologians such as Karl Rahner, Hans Küng, several Episcopal conferences, e.g. the Episcopal Conference of Austria, Germany, and Switzerland, as well as several bishops, including Christopher Butler, who called it one of the most important contributions to contemporary discussion in the Church.

The publication of the encyclical marks the first time in the twentieth century that open dissent from the laity about teachings of the Church was voiced widely and publicly. The teaching has been criticized by development organizations and others who claim that it limits the methods available to fight worldwide population growth and struggle against HIV/AIDS. Within two days of the encyclical's release, a group of dissident theologians, led by Rev. Charles Curran, then of The Catholic University of America, issued a statement stating, "spouses may responsibly decide according to their conscience that artificial contraception in some circumstances is permissible and indeed necessary to preserve and foster the value and sacredness of marriage.

Two months later, the controversial "Winnipeg Statement" issued by the Canadian Conference of Catholic Bishops stated that those who cannot accept the teaching should not be considered shut off from the Catholic Church, and that individuals can in good conscience use contraception as long as they have first made an honest attempt to accept the difficult directives of the encyclical.

The Dutch Catechism of 1966, based on the Dutch bishops' interpretation of the just completed Vatican Council, and the first post-Council comprehensive Catholic catechism, noted the lack of mention of artificial contraception in the Council. "As everyone can ascertain nowadays, there are several methods of regulating births. The Second Vatican Council did not speak of any of these concrete methods… This is a different standpoint than that taken under Pius XI some thirty years which was also maintained by his successor ... we can sense here a clear development in the Church, a development, which is also going on outside the Church."

In the Soviet Union, "Literaturnaja Gazeta", a publication of Soviet intellectuals, included an editorial and statement by Russian physicians against the encyclical.

Ecumenical reactions were mixed. Liberal and Moderate Lutherans and the World Council of Churches were disappointed. Eugene Carson Blake criticised the concepts of nature and natural law, which, in his view, still dominated Catholic theology, as outdated. This concern dominated several articles in Catholic and non-Catholic journals at the time. Patriarch Athenagoras I stated his full agreement with Pope Paul VI: “He could not have spoken in any other way.”

In Latin America, much support developed for the Pope and his encyclical. As World Bank President Robert McNamara declared at the 1968 Annual Meeting of the International Monetary Fund and the World Bank Group that countries permitting birth control practices will get preferential access to resources, doctors in La Paz, Bolivia, called it insulting that money should be exchanged for the conscience of a Catholic nation. In Colombia, Cardinal Aníbal Muñoz Duque declared, if American conditionality undermines Papal teachings, we prefer not to receive one cent. The Senate of Bolivia passed a resolution, stating that "Humanae vitae" can be discussed in its implications on individual consciences, but is of greatest significance because it defends the rights of developing nations to determine their own population policies. The Jesuit Journal "Sic" dedicated one edition to the encyclical with supportive contributions. However, against eighteen insubordinate priests, professors of theology at Pontifical Catholic University of Chile, and the ensuing conspiracy of silence practiced by the Chilean Episcopate, which had to be censured by the Nuncio in Santiago at the behest of Cardinal Gabriel-Marie Garrone, prefect of the Congregation for Catholic Education, triggering eventually a media conflict with , Plinio Corrêa de Oliveira expressed his affliction with the lamentations of Jeremiah: "O ye all that pass through the way…" ().

In the book "Nighttime conversations in Jerusalem. On the risk of faith." well-known liberal Cardinal Carlo Maria Martini accused Paul VI of deliberately concealing the truth, leaving it to theologians and pastors to fix things by adapting precepts to practice:
"I knew Paul VI well. With the encyclical, he wanted to express consideration for human life. He explained his intention to some of his friends by using a comparison: although one must not lie, sometimes it is not possible to do otherwise; it may be necessary to conceal the truth, or it may be unavoidable to tell a lie. It is up to the moralists to explain where sin begins, especially in the cases in which there is a higher duty than the transmission of life."

Pope Paul VI was troubled by the encyclical's reception in the West. Acknowledging the controversy, Paul VI in a letter to the Congress of German Catholics (30 August 1968), stated: "May the lively debate aroused by our encyclical lead to a better knowledge of God’s will." In March 1969, he had a meeting with one of the main critics of "Humanae vitae", Cardinal Leo Joseph Suenens. Paul heard him out and said merely, "Yes, pray for me; because of my weaknesses, the Church is badly governed." And to jog the memory of his critics, he put in their minds the experience of no less a figure than Pope Saint Peter: "[n]ow I understand St Peter: he came to Rome twice, the second time to be crucified", — herewith directing their attention to his rejoicing in glorifying the Lord. Increasingly convinced, that "the smoke of Satan entered the temple of God from some fissure", Paul VI reaffirmed, on 23 June 1978, weeks before his death, in an address to the College of Cardinals, his "Humanae vitae": "following the confirmations of serious science", and which sought to affirm the principle of respect for the laws of nature and of "a conscious and ethically responsible paternity".

Polls show that most Catholics use artificial means of contraception, and very few use natural family planning, However, John L. Allen, Jr. wrote in 2008: "Three decades of bishops’ appointments by John Paul II and Benedict XVI, both unambiguously committed to "Humanae Vitae", mean that senior leaders in Catholicism these days are far less inclined than they were in 1968 to distance themselves from the ban on birth control, or to soft-pedal it. Some Catholic bishops have brought out documents of their own defending "Humanae Vitae"." Also, developments in fertility awareness since the 1960s have given rise to natural family planning organizations such as the Billings Ovulation Method, Couple to Couple League and the Creighton Model FertilityCare System, which actively provide formal instruction on the use and reliability of natural methods of birth control.

Albino Luciani's views on "Humanae vitae" have been debated. Journalist John L. Allen, Jr. claims that "it's virtually certain that John Paul I would not have reversed Paul VI’s teaching, particularly since he was no doctrinal radical. Moreover, as Patriarch in Venice some had seen a hardening of his stance on social issues as the years went by." According to Allen, "...it is reasonable to assume that John Paul I would not have insisted upon the negative judgment in "Humanae Vitae" as aggressively and publicly as John Paul II did, and probably would not have treated it as a quasi-infallible teaching. It would have remained a more 'open' question". Other sources take a different view and note that during his time as Patriarch of Venice that "Luciani was intransigent with his upholding of the teaching of the Church and severe with those, through intellectual pride and disobedience paid no attention to the Church's prohibition of contraception", though while not condoning the sin, he was tolerant of those who sincerely tried and failed to live up to the Church's teaching. The book states that "...if some people think that his compassion and gentleness in this respect implies he was against Humane Vitae one can only infer it was wishful thinking on their part and an attempt to find an ally in favor of artificial contraception."

After he became pope in 1978, John Paul II continued on the Catholic Theology of the Body of his predecessors with a series of lectures, entitled "Theology of the Body", in which he talked about an "original unity between man and women", purity of heart (on the Sermon on the Mount), marriage and celibacy and reflections on "Humanae vitae", focusing largely on responsible parenthood and marital chastity.

In 1981, the Pope's Apostolic exhortation, Familiaris consortio restated the Church's opposition to artificial birth control stated previously in Humanae vitae.

John Paul II readdressed some of the same issues in his 1993 encyclical "Veritatis splendor". He reaffirmed much of "Humanae vitae", and specifically described the practice of artificial contraception as an act not permitted by Catholic teaching in any circumstances. The same encyclical also clarifies the use of conscience in arriving at moral decisions, including in the use of contraception. However, John Paul also said, “It is not right then to regard the moral conscience of the individual and the magisterium of the Church as two contenders, as two realities in conflict. The authority which the magisterium enjoys by the will of Christ exists so that the moral conscience can attain the truth with security and remain in it.” John Paul quoted "Humanae vitae" as a compassionate encyclical, "Christ has come not to judge the world but to save it, and while he was uncompromisingly stern towards sin, he was patient and rich in mercy towards sinners".

Pope John Paul's 1995 encyclical, "Evangelium vitae" ("The Gospel of Life"), affirmed the Church's position on contraception and multiple topics related to the culture of life.

On 12 May 2008, Benedict XVI accepted an invitation to talk to participants in the International Congress organized by the Pontifical Lateran University on the 40th anniversary of "Humanae vitae". He put the encyclical in the broader view of love in a global context, a topic he called "so controversial, yet so crucial for humanity's future." "Humanae vitae" became "a sign of contradiction but also of continuity of the Church's doctrine and tradition... What was true yesterday is true also today." The Church continues to reflect "in an ever new and deeper way on the fundamental principles that concern marriage and procreation." The key message of "Humanae vitae" is love. Benedict states, that the fullness of a person is achieved by a unity of soul and body, but neither spirit nor body alone can love, only the two together. If this unity is broken, if only the body is satisfied, love becomes a commodity.

On 16 January 2015, Pope Francis said to a meeting with families in Manila, insisting on the need to protect the family: "The family is ...threatened by growing efforts on the part of some to redefine the very institution of marriage, by relativism, by the culture of the ephemeral, by a lack of openness to life. I think of Blessed Paul VI. At a time when the problem of population growth was being raised, he had the courage to defend openness to life in families. He knew the difficulties that are there in every family, and so in his Encyclical he was very merciful towards particular cases, and he asked confessors to be very merciful and understanding in dealing with particular cases. But he also had a broader vision: he looked at the peoples of the earth and he saw this threat of the destruction of the family through the privation of children [original Spanish: destrucción de la familia por la privación de los hijos]. Paul VI was courageous; he was a good pastor and he warned his flock of the wolves who were coming."

A year before, on 1 May 2014, Pope Francis, in an interview given to Italian newspaper "Corriere della Sera", expressed his opinion and praise for "Humanae Vitae": "Everything depends on how "Humanae Vitae" is interpreted. Paul VI himself, in the end, urged confessors to be very merciful and pay attention to concrete situations. But his genius was prophetic, he had the courage to take a stand against the majority, to defend moral discipline, to exercise a cultural restraint, to oppose present and future neo-Malthusianism. The question is not of changing doctrine, but of digging deep and making sure that pastoral care takes into account situations and what it is possible for persons to do."




</doc>
<doc id="14072" url="https://en.wikipedia.org/wiki?curid=14072" title="History of Wikipedia">
History of Wikipedia

Wikipedia began with its first edit on 15 January 2001, two days after the domain was registered by Jimmy Wales and Larry Sanger. Its technological and conceptual underpinnings predate this; the earliest known proposal for an online encyclopedia was made by Rick Gates in 1993, and the concept of a free-as-in-freedom online encyclopedia (as distinct from mere open source) was proposed by Richard Stallman in December 2000.

Crucially, Stallman's concept specifically included the idea that no central organization should control editing. This characteristic greatly contrasted with contemporary digital encyclopedias such as Microsoft Encarta, "Encyclopædia Britannica", and even Bomis's Nupedia, which was Wikipedia's direct predecessor. In 2001, the license for Nupedia was changed to GFDL, and Wales and Sanger launched Wikipedia using the concept and technology of a wiki pioneered in 1995 by Ward Cunningham. Initially, Wikipedia was intended to complement Nupedia, an online encyclopedia project edited solely by experts, by providing additional draft articles and ideas for it. In practice, Wikipedia quickly overtook Nupedia, becoming a global project in multiple languages and inspiring a wide range of other online reference projects.

According to Alexa Internet, , Wikipedia is the world's ninth most popular website in terms of global internet engagement. Wikipedia's worldwide monthly readership is approximately 495 million. Worldwide in September 2018, WMF Labs tallied 15.5 billion page views for the month. According to comScore, Wikipedia receives over 117 million monthly unique visitors from the United States alone.

The concept of compiling the world's knowledge in a single location dates back to the ancient Libraries of Alexandria and Pergamum, but the modern concept of a general-purpose, widely distributed, printed encyclopedia originated with Denis Diderot and the 18th-century French encyclopedists. The idea of using automated machinery beyond the printing press to build a more useful encyclopedia can be traced to Paul Otlet's 1934 book "Traité de documentation"; Otlet also founded the Mundaneum, an institution dedicated to indexing the world's knowledge, in 1910. This concept of a machine-assisted encyclopedia was further expanded in H. G. Wells' book of essays "World Brain" (1938) and Vannevar Bush's future vision of the microfilm-based Memex in his essay "As We May Think" (1945). Another milestone was Ted Nelson's hypertext design Project Xanadu, which was begun in 1960.

Advances in information technology in the late 20th century led to changes in the form of encyclopedias. While previous encyclopedias, notably the "Encyclopædia Britannica", were book-based, Microsoft's Encarta, published in 1993, was available on CD-ROM and hyperlinked. The development of the World Wide Web led to many attempts to develop internet encyclopedia projects. An early proposal for an online encyclopedia was Interpedia in 1993 by Rick Gates; this project died before generating any encyclopedic content. Free software proponent Richard Stallman described the usefulness of a "Free Universal Encyclopedia and Learning Resource" in 1999. His published document "aims to lay out what the free encyclopedia needs to do, what sort of freedoms it needs to give the public, and how we can get started on developing it." On Wednesday 17 January 2001, two days after the founding of Wikipedia, the Free Software Foundation's (FSF) GNUPedia project went online, competing with Nupedia, but today the FSF encourages people "to visit and contribute to [Wikipedia]".

Wikipedia co-founder Jimmy Wales has stated that the germ of the concept for Wikipedia, for him, came back when he was a graduate student at Indiana University where he was impressed with the successes of the open-source movement and found Richard Stallman's Emacs Manifesto promoting free software and a sharing economy to be quite interesting. At the time, Wales was studying finance and was intrigued by the incentives of the many people who contributed as volunteers toward creating free software where there were many examples having excellent results.

Wikipedia was initially conceived as a feeder project for the Wales-founded Nupedia, an earlier project to produce a free online encyclopedia, volunteered by Bomis, a web-advertising firm owned by Jimmy Wales, Tim Shell and Michael E. Davis. Nupedia was founded upon the use of highly qualified volunteer contributors and an elaborate multi-step peer review process. Despite its mailing list of interested editors, and the presence of a full-time editor-in-chief, Larry Sanger, a graduate philosophy student hired by Wales, the writing of content for Nupedia was extremely slow, with only 12 articles written during the first year.

Wales and Sanger discussed various ways to create content more rapidly. The idea of a wiki-based complement originated from a conversation between Larry M. Sanger and Ben Kovitz. Ben Kovitz was a computer programmer and regular on Ward Cunningham's revolutionary wiki "the WikiWikiWeb". He explained to Sanger what wikis were, at that time a difficult concept to understand, over a dinner on Tuesday 2 January 2001. Wales first stated, in October 2001, that "Larry had the idea to use Wiki software", though he later stated in December 2005 that Jeremy Rosenfeld, a Bomis employee, introduced him to the concept. Sanger thought a wiki would be a good platform to use, and proposed on the Nupedia mailing list that a wiki based upon UseModWiki (then v. 0.90) be set up as a "feeder" project for Nupedia. Under the subject "Let's make a wiki", he wrote:
Wales set one up and put it online on Wednesday 10 January 2001.

There was considerable resistance on the part of Nupedia's editors and reviewers to the idea of associating Nupedia with a wiki-style website. Sanger suggested giving the new project its own name, "Wikipedia", and Wikipedia was soon launched on its own domain, wikipedia.com, on Monday 15 January 2001. The bandwidth and server (located in San Diego) used for these initial projects were donated by Bomis. Many former Bomis employees later contributed content to the encyclopedia: notably Tim Shell, co-founder and later CEO of Bomis, and programmer Jason Richey.

Wales stated in December 2008 that he made Wikipedia's first edit, a test edit with the text "Hello, World!", but this edit may have been to an old version of Wikipedia which soon after was scrapped and replaced by a restart; see [WikiEN-l] "Hello world?". The existence of the project was formally announced and an appeal for volunteers to engage in content creation was made to the Nupedia mailing list on 17 January 2001.

The project received many new participants after being mentioned on the Slashdot website in July 2001, having already earned two minor mentions in March 2001. It then received a prominent pointer to a story on the community-edited technology and culture website Kuro5hin on 25 July. Between these relatively rapid influxes of traffic, there had been a steady stream of traffic from other sources, especially Google, which alone sent hundreds of new visitors to the site every day. Its first major mainstream media coverage was in "The New York Times" on Thursday 20 September 2001.

The project gained its 1,000th article around Monday 12 February 2001, and reached 10,000 articles around 7 September. In the first year of its existence, over 20,000 encyclopedia entries were created – a rate of over 1,500 articles per month. On Friday 30 August 2002, the article count reached 40,000.

Wikipedia's earliest edits were long believed lost, since the original UseModWiki software deleted old data after about a month. On Tuesday 14 December 2010, developer Tim Starling found backups on SourceForge containing every change made to Wikipedia from its creation in January 2001 to 17 August 2001. It showed the first edit as being to HomePage on 15 January 2001, reading "This is the new WikiPedia!". That edit was imported in 2019 and can be found .

The first three edits that were known of before Tim Starling's discovery, are:
For more information see and .

Early in Wikipedia's development, it began to expand internationally, with the creation of new namespaces, each with a distinct set of usernames. The first subdomain created for a non-English Wikipedia was "deutsche.wikipedia.com" (created on Friday 16 March 2001, 01:38 UTC), followed after a few hours by "Catalan.wikipedia.com" (at 13:07 UTC). The Japanese Wikipedia, started as nihongo.wikipedia.com, was created around that period, and initially used only Romanized Japanese. For about two months Catalan was the one with the most articles in a non-English language, although statistics of that early period are imprecise. The French Wikipedia was created on or around 11 May 2001, in a wave of new language versions that also included Chinese, Dutch, Esperanto, Hebrew, Italian, Portuguese, Russian, Spanish, and Swedish. These languages were soon joined by Arabic and Hungarian. In September 2001, an announcement pledged commitment to the multilingual provision of Wikipedia, notifying users of an upcoming roll-out of Wikipedias for all major languages, the establishment of core standards, and a push for the translation of core pages for the new wikis. At the end of that year, when international statistics first began to be logged, Afrikaans, Norwegian, and Serbian versions were announced.

In January 2002, 90% of all Wikipedia articles were in English. By January 2004, fewer than 50% were English, and this internationalization has continued to increase as the encyclopedia grows. , about 85.5% of all Wikipedia articles are contained within non-English Wikipedia versions.

In March 2002, following the withdrawal of funding by Bomis during the dot-com bust, Larry Sanger left both Nupedia and Wikipedia. By 2002, Sanger and Wales differed in their views on how best to manage open encyclopedias. Both still supported the open-collaboration concept, but the two disagreed on how to handle disruptive editors, specific roles for experts, and the best way to guide the project to success.

Wales went on to establish self-governance and bottom-up self-direction by editors on Wikipedia. He made it clear that he would not be involved in the community's day-to-day management, but would encourage it to learn to self-manage and find its own best approaches. , Wales mostly restricts his own role to occasional input on serious matters, executive activity, advocacy of knowledge, and encouragement of similar reference projects.

Sanger says he is an "inclusionist" and is open to almost anything. He proposed that experts still have a place in the Web 2.0 world. He returned briefly to academia, then joined the Digital Universe Foundation. In 2006, Sanger founded Citizendium, an open encyclopedia that used real names for contributors in an effort to reduce disruptive editing, and hoped to facilitate "gentle expert guidance" to increase the accuracy of its content. Decisions about article content were to be up to the community, but the site was to include a statement about "family-friendly content". He stated early on that he intended to leave Citizendium in a few years, by which time the project and its management would presumably be established.

The Wikipedia project has grown rapidly in the course of its life, at several levels. Content has grown organically through the addition of new articles, new wikis have been added in English and non-English languages, and entire new projects replicating these growth methods in other related areas (news, quotations, reference books and so on) have been founded as well. Wikipedia itself has grown, with the creation of the Wikimedia Foundation to act as an umbrella body and the growth of software and policies to address the needs of the editorial community. These are documented below:

In March 2000, the Nupedia project was started. Its intention was to publish articles written by experts which would be licensed as free content. Nupedia was founded by Jimmy Wales, with Larry Sanger as editor-in-chief, and funded by the web-advertising company Bomis.

In January 2001, Wikipedia began as a side-project of Nupedia, to allow collaboration on articles prior to entering the peer-review process. The name was suggested by Sanger on 11 January 2001 as a portmanteau of the words "wiki" (Hawaiian for "quick") and "encyclopedia". The "wikipedia.com" and "wikipedia.org" domain names were registered on 12 and 13 January, respectively, with "wikipedia.org" being brought online on the same day. The project formally opened on 15 January (""), with the first international Wikipedias – the French, German, Catalan, Swedish, and Italian editions – being created between March and May. The "neutral point of view" (NPOV) policy was officially formulated at this time, and Wikipedia's first slashdotter wave arrived on 26 July. The first media report about Wikipedia appeared in August 2001 in the newspaper "Wales on Sunday". The September 11 attacks spurred the appearance of breaking news stories on the homepage, as well as information boxes linking related articles.

2002 saw the end of funding for Wikipedia from Bomis and the departure of Larry Sanger. The forking of the Spanish Wikipedia also took place with the establishment of the "Enciclopedia Libre". The first portable MediaWiki software went live on 25 January. Bots were introduced, Jimmy Wales confirmed that Wikipedia would never run commercial advertising, and the first sister project (Wiktionary) and first formal Manual of Style were launched. A separate board of directors to supervise the project was proposed and initially discussed at Meta-Wikipedia. Close to 200 contributors were editing Wikipedia daily.

The English Wikipedia passed 100,000 articles in 2003, while the next largest edition, the German Wikipedia, passed 10,000. The Wikimedia Foundation was established, and Wikipedia adopted its jigsaw world logo. Mathematical formulae using TeX were reintroduced to the website. The took place in Munich, Germany, in October. The basic principles of Wikipedia's (known colloquially as "ArbCom") were developed, mostly by , and other early Wikipedians.

Wikisource was created as a separate project on 24 November 2003, to host free textual sources.

The worldwide Wikipedia article pool continued to grow rapidly in 2004, doubling in size in 12 months, from under 500,000 articles in late 2003 to over 1 million in over 100 languages by the end of 2004. The English Wikipedia accounted for just under half of these articles. The website's server farms were moved from California to Florida, and CSS style configuration sheets were introduced, and the first attempt to block Wikipedia occurred, with the website being blocked in China for two weeks in June. The formal election of a board and Arbitration Committee began. The first formal projects were proposed to deliberately balance content and seek out systemic bias arising from Wikipedia's community structure.

"Bourgeois v. Peters", (11th Cir. 2004), a court case decided by the United States Court of Appeals for the Eleventh Circuit was one of the earliest . It stated: "We also reject the notion that the Department of Homeland Security's threat advisory level somehow justifies these searches. Although the threat level was 'elevated' at the time of the protest, 'to date, the threat level has stood at yellow (elevated) for the majority of its time in existence. It has been raised to orange (high) six times."

Wikimedia Commons was created on 7 September 2004 to host media files for Wikipedia in all languages.

In 2005, Wikipedia became the most popular reference website on the Internet, according to Hitwise, with the English Wikipedia alone exceeding 750,000 articles. Wikipedia's first multilingual and subject portals were established in 2005. A formal fundraiser held in the first quarter of the year raised almost US$100,000 for system upgrades to handle growing demand. China again blocked Wikipedia in October 2005.

The first major Wikipedia scandal, the Seigenthaler incident, occurred in 2005, when a well-known figure was found to have a vandalized biography which had gone unnoticed for months. In the wake of this and other concerns, the first policy and system changes specifically designed to counter this form of abuse were established. These included a new privilege policy update to assist in sock puppetry investigations, a new feature called , a more strict policy on biographies of living people and the tagging of such articles for stricter review. A restriction of new article creation to registered users only was put in place in December 2005, after the Seigenthaler incident where an anonymous user posted a hoax.
Wikimania 2005, the first Wikimania conference, was held from 4 to 8 August 2005 at the "Haus der Jugend" in Frankfurt, Germany, attracting about 380 attendees.

The English Wikipedia gained its one-millionth article, Jordanhill railway station, on 1 March 2006. The first approved Wikipedia article selection was made freely available to download, and "Wikipedia" became registered as a trademark of the Wikimedia Foundation. The congressional aides biography scandals – multiple incidents in which congressional staffers and a campaign manager were caught trying to covertly alter Wikipedia biographies – came to public attention, leading to the resignation of the campaign manager. Nonetheless, Wikipedia was rated as one of the top five global brands of 2006.

Jimmy Wales indicated at Wikimania 2006 that Wikipedia had achieved sufficient volume and called for an emphasis on quality, perhaps best expressed in the call for . A new privilege, "oversight", was created, allowing specific versions of archived pages with unacceptable content to be marked as non-viewable. Semi-protection against anonymous vandalism, introduced in 2005, proved more popular than expected, with over 1,000 pages being semi-protected at any given time in 2006.

Wikipedia continued to grow rapidly in 2007, possessing over 5 million registered editor accounts by 13 August. The 250 language editions of Wikipedia contained a combined total of 7.5 million articles, totalling 1.74 billion words, by 13 August. The English Wikipedia gained articles at a steady rate of 1,700 a day, with the wikipedia.org domain name ranked the 10th-busiest in the world. Wikipedia continued to garner visibility in – the Essjay controversy broke when a prominent member of Wikipedia was found to have lied about his credentials. Citizendium, a competing online encyclopedia, launched publicly. A new trend developed in Wikipedia, with the encyclopedia addressing people whose notability stemmed from being a participant in a news story by adding a redirect from their name to the larger story, rather than creating a distinct biographical article. On 9 September 2007, the English Wikipedia gained its two-millionth article, El Hormiguero. There was some controversy in late 2007 when the Volapük Wikipedia jumped from 797 to over 112,000 articles, briefly becoming the 15th-largest Wikipedia edition, due to automated stub generation by an enthusiast for the Volapük constructed language.

According to the "MIT Technology Review", the number of regularly active editors on the English-language Wikipedia peaked in 2007 at more than 51,000, and has since been declining.

Various in many areas continued to expand and refine article contents within their scope. In April 2008, the 10-millionth Wikipedia article was created, and by the end of the year the English Wikipedia exceeded 2.5 million articles.

On 25 June 2009 at 3:15 pm PDT (22:15 UTC), following the death of pop icon Michael Jackson, the website temporarily crashed.

The Wikimedia Foundation reported nearly a million visitors to Jackson's biography within one hour, probably the most visitors in a one-hour period to any article in Wikipedia's history. By late August 2009, the number of articles in all Wikipedia editions had exceeded 14 million. The three-millionth article on the English Wikipedia, Beate Eriksen, was created on 17 August 2009 at 04:05 UTC. On 27 December 2009, the German Wikipedia exceeded one million articles, becoming the second edition after the English Wikipedia to do so. A "TIME" article listed Wikipedia among 2009's best websites.

Wikipedia content became licensed under Creative Commons in 2009.

On 24 March, the European Wikipedia servers went offline due to an overheating problem. Failover to servers in Florida turned out to be broken, causing DNS resolution for Wikipedia to fail across the world. The problem was resolved quickly, but due to DNS caching effects, some areas were slower to regain access to Wikipedia than others.

On 13 May, the site released a new interface. New features included an updated logo, new navigation tools, and a link wizard. However, the classic interface remained available for those who wished to use it. On 12 December, the English Wikipedia passed the 3.5-million-article mark, while the French Wikipedia's millionth article was created on 21 September. The 1-billionth Wikimedia project edit was performed on 16 April.

Wikipedia and its users held many celebrations worldwide to commemorate the site's 10th anniversary on 15 January. The site began efforts to expand its growth in India, holding its first Indian conference in Mumbai in November 2011. The English Wikipedia passed the 3.6-million-article mark on 2 April, and reached 3.8 million articles on 18 November. On 7 November 2011, the German Wikipedia exceeded 100 million page edits, becoming the second language edition to do so after the English edition, which attained 500 million page edits on 24 November 2011. The Dutch Wikipedia exceeded 1 million articles on 17 December 2011, becoming the fourth Wikipedia edition to do so.

The "Wikimania 2011 – Haifa, Israel" stamp was issued by Israel Post on 2 August 2011. This was the first-ever stamp dedicated to a Wikimedia-related project.

Between 4 and 6 October 2011, the Italian Wikipedia became intentionally inaccessible in protest against the Italian Parliament's proposed DDL intercettazioni law, which, if approved, would allow any person to force websites to remove information that is perceived as untrue or offensive, without the need to provide evidence.

Also in October 2011, Wikimedia announced the launch of Wikipedia Zero, an initiative to enable free mobile access to Wikipedia in developing countries through partnerships with mobile operators.

On 16 January, Wikipedia co-founder Jimmy Wales announced that the English Wikipedia would shut down for 24 hours on 18 January as part of a protest meant to call public attention to the proposed Stop Online Piracy Act and PROTECT IP Act, two anti-piracy laws under debate in the United States Congress. Calling the blackout a "community decision", Wales and other opponents of the laws believed that they would endanger free speech and online innovation. A similar blackout was staged on 10 July by the Russian Wikipedia, in protest against a proposed Russian internet regulation law.

In late March 2012, the announced Wikidata, a universal platform for sharing data between all Wikipedia language editions. The US$1.7-million Wikidata project was partly funded by Google, the Gordon and Betty Moore Foundation, and the Allen Institute for Artificial Intelligence. Wikimedia Deutschland assumed responsibility for the first phase of Wikidata, and initially planned to make the platform available to editors by December 2012. Wikidata's first phase became fully operational in March 2013.
In April 2012, Justin Knapp became the first single contributor to make over one million edits to Wikipedia. Jimmy Wales congratulated Knapp for his work and presented him with the site's "Special Barnstar" medal and the "Golden Wiki" award for his achievement. Wales also declared that 20 April would be "Justin Knapp Day".

On 13 July 2012, the English Wikipedia gained its 4-millionth article, Izbat al-Burj. In October 2012, historian and Wikipedia editor Richard J. Jensen opined that the English Wikipedia was "nearing completion", noting that the number of regularly active editors had fallen significantly since 2007, despite Wikipedia's rapid growth in article count and readership.

According to Alexa Internet, Wikipedia was the world's sixth-most-popular website as of November 2012. Dow Jones ranked Wikipedia fifth worldwide as of December 2012.

On 22 January 2013, the Italian Wikipedia became the fifth language edition of Wikipedia to exceed 1 million articles, while the Russian and Spanish Wikipedias gained their millionth articles on 11 and 16 May respectively. On 15 July the Swedish and on 24 September the Polish Wikipedias gained their millionth articles, becoming the eighth and ninth Wikipedia editions to do so.

On 27 January, the main belt asteroid 274301 was officially renamed "Wikipedia" by the Committee for Small Body Nomenclature.

The first phase of the Wikidata database, automatically providing interlanguage links and other data, became available for all language editions in March 2013.

In April 2013, the French secret service was accused of attempting to censor Wikipedia by threatening a Wikipedia volunteer with arrest unless "classified information" about a military radio station was deleted.
In July, the VisualEditor editing system was launched, forming the first stage of an effort to allow articles to be edited with a word processor-like interface instead of using wikimarkup. An editor specifically designed for smartphones and other mobile devices was also launched.

In February 2014, a project to make a print edition of the English Wikipedia, consisting of 1,000 volumes and over 1,100,000 pages, was launched by German Wikipedia contributors. The project sought funding through Indiegogo, and was intended to honor the contributions of Wikipedia's editors. On 22 October 2014, the first monument to Wikipedia was unveiled in the Polish town of Slubice.

On 8 June, 15 June, and 16 July 2014, the Waray Wikipedia, the Vietnamese Wikipedia and the Cebuano Wikipedia each exceeded the one million article mark. They were the tenth, eleventh and twelfth Wikipedias to reach that milestone. Despite having very few active users, the Waray and Cebuano Wikipedias had a high number of automatically generated articles created by bots.

In mid-2015, Wikipedia was the world's seventh-most-popular website according to Alexa Internet, down one place from the position it held in November 2012. At the start of 2015, Wikipedia remained the largest general-knowledge encyclopedia online, with a combined total of over 36 million mainspace articles across all 291 language editions. On average, Wikipedia receives a total of 10 billion global pageviews from around 495 million unique visitors every month, including 85 million visitors from the United States alone, where it is the sixth-most-popular site.
"Print Wikipedia" was an art project by Michael Mandiberg that printed out the 7473 volumes of Wikipedia as it existed on 7 April 2015. Each volume has 700 pages.

On 1 November 2015, the English Wikipedia reached 5,000,000 articles with the creation of an article on "Persoonia terminalis", a type of shrub.

On 19 January 2016, the Japanese Wikipedia exceeded the one million article mark, becoming the thirteenth Wikipedia to reach that milestone. The millionth article was (a World War II submarine of the Imperial Japanese Navy).

In mid-2016, Wikipedia was once again the world's sixth-most-popular website according to Alexa Internet, up one place from the position it held in the previous year.

In October 2016, the mobile version of Wikipedia got a new look.

In mid-2017, Wikipedia was listed as the world's fifth-most-popular website according to Alexa Internet, rising one place from the position it held in the previous year. Wikipedia Zero was made available in Iraq and Afghanistan.

On 29 April 2017, online access to Wikipedia was blocked across all language editions in Turkey by the Turkish authorities. This block lasted until 15 January 2020, as the court of Turkey ruled that the block violated human rights. The encrypted Japanese Wikipedia has been blocked in China since 28 December 2017.

During 2018, Wikipedia retained its listing as the world's fifth-most-popular website according to Alexa Internet. One notable development was the use of Artificial Intelligence to create draft articles on overlooked topics.

On 13 April 2018, the number of Chinese Wikipedia articles exceeded 1 million, becoming the fourteenth Wikipedia to reach that milestone. The Chinese Wikipedia has been blocked in Mainland China since May 2015. Later in the year, on 26 June, the Portuguese Wikipedia exceeded the one million article mark, becoming the fifteenth Wikipedia to reach that milestone. The millionth article was "" (the Pardon of Richard Nixon).

In August 2019, according to Alexa.com, Wikipedia fell from fifth placed to seventh placed website in the world for global internet engagement.

On 23 April 2019, Chinese authorities expanded the block of Wikipedia to versions in all languages. The timing of the block coincided with the 30th anniversary of the Tiananmen Square Protests and the 100th anniversary of the May Fourth Movement, resulting in stricter internet censorship in China.

On 23 January 2020, the six millionth article, the biography of Maria Elise Turner Lauder, was added to the English Wikipedia. Despite this growth in articles, Wikipedia's global internet engagement, as measured by Alexa, continued to decline. By February 2020, Wikipedia fell to the eleventh placed website in the world for global internet engagement. Both Wikipedia's coverage of the COVID-19 pandemic crisis and the supporting edits, discussions and even deletions were thought to be a useful resource for future historians seeking to understand the period in detail.






Every year, Wikipedia runs a fundraising campaign to support its operations.


Because Wikipedia biographies are often updated as soon as new information comes to light, they are often used as a reference source on the lives of . This has led to attempts to manipulate and falsify Wikipedia articles for promotional or defamatory purposes (see Controversies). It has also led to novel uses of the biographical material provided. Some notable people's lives are being affected by their Wikipedia biography.

Sanger played an important role in the early stages of creating Wikipedia. Wales says that Sanger was his subordinate employee. Sanger initially brought the wiki concept to Wales and suggested it be applied to Nupedia and then, after some initial skepticism, Wales agreed to try it. It was Jimmy Wales, along with other people, who came up with the broader idea of an open-source, collaborative encyclopedia that would accept contributions from ordinary people and it was Wales who invested in it. Wales stated in October 2001 that "Larry had the idea to use Wiki software." Sanger coined the portmanteau "Wikipedia" as the project name. In review, Larry Sanger conceived of a wiki-based encyclopedia as a strategic solution to Nupedia's inefficiency problems. In terms of project roles, Sanger spearheaded and pursued the project as its leader in its first year, and did most of the early work in formulating policies (including "Ignore all rules" and "Neutral point of view") and building up the community. Upon departure in March 2002, Sanger emphasized the main issue was purely the cessation of Bomis' funding for his role, which was not viable part-time, and his changing personal priorities; however, by 2004, the two had drifted apart and Sanger became more critical. Two weeks after the launch of Citizendium, Sanger criticized Wikipedia, describing the latter as "broken beyond repair." By 2005 Wales began to dispute Sanger's role in the project, three years after Sanger left.

In 2005, Wales described himself simply as the founder of Wikipedia; however, according to Brian Bergstein of the Associated Press, "Sanger has long been cited as a co-founder." There is evidence that Sanger was called co-founder, along with Wales, as early as 2001, and he is referred to as such in early Wikipedia press releases and Wikipedia articles and in a September 2001 "New York Times" article for which both were interviewed. In 2006, Wales said, "He used to work for me [...] I don't agree with calling him a co-founder, but he likes the title"; nonetheless, before January 2004, Wales did not dispute Sanger's status as co-founder and, indeed, identified himself as "co-founder" as late as August 2002. In Sanger's introductory message to the Nupedia mailing list, he said that "Jimmy Wales contacted me and asked me to apply as editor-in-chief of Nupedia. Apparently, Bomis, Inc. (which owns Nupedia)... who could manage this sort of long-term project, he thought I would be perfect for the job. This is indeed my dream job". Sanger said "He [Wales] had had the idea for Nupedia since at least last fall".

As of March 2007: Wales emphasized this employer–employee relationship and his ultimate authority, terming himself Wikipedia's sole founder; and Sanger emphasized their statuses as co-founders, referencing earlier versions of Wikipedia pages (2004, 2006), press releases (2002–2004), and media coverage from the time of his involvement routinely terming them in this manner.



The goals which led to GNUpedia were published at least as early as 18 December 2000, and these exact goals were finalized on the 12th and 13th of January 2001, albeit with a copyright of 1999, from when Stallman had first started considering the problem. The only sentence added between 18 December and the unveiling of GNUpedia the week of 12–16 January was this: "The GNU Free Documentation License would be a good license to use for courses."

GNUpedia was "formally" announced on the "slashdot" website, on 16 January, the same day that their mailing list first went online with a test-message. Wales posted to the list on 17 January, the first full day of messages, explaining the discussions with Stallman concerning the change in Nupedia content-licensing, and suggesting cooperation. Stallman himself first posted on 19 January, and, in his second post on 22 January, mentioned that discussions about merging Wikipedia and GNUpedia were ongoing. Within a couple of months, Wales had changed his email signature from the open source encyclopedia to the free encyclopedia; both Nupedia and Wikipedia had adopted the GFDL; and the merger of GNUpedia into Wikipedia was effectively accomplished.

In a separate but similar incident, the campaign manager for Cathy Cox, Morton Brilliant, resigned after being found to have added negative information to the Wikipedia entries of political opponents. Following media publicity, the incidents tapered off around August 2006.

There are a number of . Other sites also use the MediaWiki software and concept, popularized by Wikipedia. No list of them is maintained.

Specialized foreign language forks using the Wikipedia concept include Enciclopedia Libre (Spanish), "Wikiweise" (German), WikiZnanie (Russian), Susning.nu (Swedish), and Baidu Baike (Chinese). Some of these (such as "Enciclopedia Libre") use GFDL or compatible licenses as used by Wikipedia, leading to exchange of material with their respective language Wikipedias.

In 2006, Larry Sanger founded Citizendium, based upon a modified version of MediaWiki. The site said it aimed 'to improve on the Wikipedia model with "gentle expert oversight", among other things'. (See also Nupedia).

The German Wikipedia was the first to be partly published also using other media (rather than online on the internet), including releases on CD in November 2004 and more extended versions on CDs or DVD in April 2005 and December 2006. In December 2005, the publisher Zenodot Verlagsgesellschaft mbH, a sister company of Directmedia, published a 139-page book explaining Wikipedia, its history and policies, which was accompanied by a 7.5 GB DVD containing 300,000 articles and 100,000 images from the German Wikipedia. Originally, Directmedia also announced plans to print the German Wikipedia in its entirety, in 100 volumes of 800 pages each. Publication was due to begin in October 2006, and finish in 2010. In March 2006, however, this project was called off.

In September 2008, Bertelsmann published a 1000 pages volume with a selection of popular German Wikipedia articles. Bertelsmann paid voluntarily 1 Euro per sold copy to Wikimedia Deutschland.

The first CD version containing a selection of articles from the English Wikipedia was published in April 2006 by as the "2006 Wikipedia CD Selection". In April 2007, "Wikipedia Version 0.5", a CD containing around 2000 articles selected from the online encyclopedia was published by the Wikimedia Foundation and Linterweb. The selection of articles included was based on both the quality of the online version and the importance of the topic to be included. This CD version was created as a test-case in preparation for a DVD version including far more articles. The CD version can be purchased online, downloaded as a DVD image file or , or accessed online at the project's website.

A free software project has also been launched to make a static version of Wikipedia available for use on iPods. The "Encyclopodia" project was started around March 2006 and can currently be used on 1st to 4th generation iPods.

In limited ways, the Wikimedia Foundation is protected by Section 230 of the Communications Decency Act. In the defamation action "Bauer et al. v. Glatzer et al.", it was held that Wikimedia had no case to answer because of this section. A similar law in France caused a lawsuit to be dismissed in October 2007. In 2013, a German appeals court (the Higher Regional Court of Stuttgart) ruled that Wikipedia is a "service provider" not a "content provider", and as such is immune from liability as long as it takes down content that is accused of being illegal.


Historical summaries

Size and statistics

Discussion and debate archives

Other


</doc>
<doc id="14073" url="https://en.wikipedia.org/wiki?curid=14073" title="Hydropower">
Hydropower

Hydropower or water power (from , "water") is power derived from the energy of falling or fast-running water, which may be harnessed for useful purposes. Since ancient times, hydropower from many kinds of watermills has been used as a renewable energy source for irrigation and the operation of various mechanical devices, such as gristmills, sawmills, textile mills, trip hammers, dock cranes, domestic lifts, and ore mills. A trompe, which produces compressed air from falling water, is sometimes used to power other machinery at a distance.

In the late 19th century, hydropower became a source for generating electricity. Cragside in Northumberland was the first house powered by hydroelectricity in 1878 and the first commercial hydroelectric power plant was built at Niagara Falls in 1879. In 1881, street lamps in the city of Niagara Falls were powered by hydropower.

Since the early 20th century, the term has been used almost exclusively in conjunction with the modern development of hydroelectric power. International institutions such as the World Bank view hydropower as a means for economic development without adding substantial amounts of carbon to the atmosphere,
but dams can have significant negative social and environmental impacts.

The earliest evidence of water wheels and watermills date back to the ancient Near East in the 4th century BC, specifically in the Persian Empire before 350 BCE, in the regions of Iraq, Iran, and Egypt.

In the Roman Empire water-powered mills were described by Vitruvius by the first century BC. The Barbegal mill had sixteen water wheels processing up to 28 tons of grain per day. Roman waterwheels were also used for sawing marble such as the Hierapolis sawmill of the late 3rd century AD. Such sawmills had a waterwheel which drove two crank-and-connecting rods to power two saws. It also appears in two 6th century Eastern Roman saw mills excavated at Ephesus and Gerasa respectively. The crank and connecting rod mechanism of these Roman watermills converted the rotary motion of the waterwheel into the linear movement of the saw blades.

In China, it was theorized that its water-powered trip hammers and bellows from as early as the Han dynasty (202 BC - 220 AD) were powered by water scoops, but later historians believed that they were powered by waterwheels on the basis that water scoops would not have had the motive force to operate their blast furnace bellows. Evidence of Han vertical waterwheels can be seen in two contemporary funeral ware models depicting water powered trip hammers. The earliest texts to describe the device are the "Jijiupian" dictionary of 40 BC, Yang Xiong's text known as the "Fangyan" of 15 BC, as well as the "Xin Lun" written by Huan Tan about 20 AD. It was also during this time that the engineer Du Shi (c. AD 31) applied the power of waterwheels to piston-bellows in forging cast iron.

The power of a wave of water released from a tank was used for extraction of metal ores in a method known as hushing. The method was first used at the Dolaucothi Gold Mines in Wales from 75 AD onwards, but had been developed in Spain at such mines as Las Médulas. Hushing was also widely used in Britain in the Medieval and later periods to extract lead and tin ores. It later evolved into hydraulic mining when used during the California Gold Rush.

In the Muslim world during the Islamic Golden Age and Arab Agricultural Revolution (8th–13th centuries), engineers made wide use of hydropower as well as early uses of tidal power, and large hydraulic factory complexes. A variety of water-powered industrial mills were used in the Islamic world, including fulling mills, gristmills, paper mills, hullers, sawmills, ship mills, stamp mills, steel mills, sugar mills, and tide mills. By the 11th century, every province throughout the Islamic world had these industrial mills in operation, from Al-Andalus and North Africa to the Middle East and Central Asia. Muslim engineers also used water turbines, employed gears in watermills and water-raising machines, and pioneered the use of dams as a source of water power, used to provide additional power to watermills and water-raising machines.

Islamic mechanical engineer Al-Jazari (1136–1206) described designs for 50 devices, many of them water powered, in his book, "The Book of Knowledge of Ingenious Mechanical Devices", including clocks, a device to serve wine, and five devices to lift water from rivers or pools, though three are animal-powered and one can be powered by animal or water. These include an endless belt with jugs attached, a cow-powered shadoof, and a reciprocating device with hinged valves.

In 1753, French engineer Bernard Forest de Bélidor published "Architecture Hydraulique" which described vertical- and horizontal-axis hydraulic machines. The growing demand for the Industrial Revolution would drive development as well.

Hydraulic power networks used pipes to carry pressurized water and transmit mechanical power from the source to end users. The power source was normally a head of water, which could also be assisted by a pump. These were extensive in Victorian cities in the United Kingdom. A hydraulic power network was also developed in Geneva, Switzerland. The world-famous Jet d'Eau was originally designed as the over-pressure relief valve for the network.

At the beginning of the Industrial Revolution in Britain, water was the main source of power for new inventions such as Richard Arkwright's water frame. Although the use of water power gave way to steam power in many of the larger mills and factories, it was still used during the 18th and 19th centuries for many smaller operations, such as driving the bellows in small blast furnaces (e.g. the Dyfi Furnace) and gristmills, such as those built at Saint Anthony Falls, which uses the 50-foot (15 m) drop in the Mississippi River.

In the 1830s, at the early peak in the US canal-building, hydropower provided the energy to transport barge traffic up and down steep hills using inclined plane railroads. As railroads overtook canals for transportation, canal systems were modified and developed into hydropower systems; the history of Lowell, Massachusetts is a classic example of commercial development and industrialization, built upon the availability of water power.

Technological advances had moved the open water wheel into an enclosed turbine or water motor. In 1848 James B. Francis, while working as head engineer of Lowell's Locks and Canals company, improved on these designs to create a turbine with 90% efficiency. He applied scientific principles and testing methods to the problem of turbine design. His mathematical and graphical calculation methods allowed the confident design of high-efficiency turbines to exactly match a site's specific flow conditions. The Francis reaction turbine is still in wide use today. In the 1870s, deriving from uses in the California mining industry, Lester Allan Pelton developed the high efficiency Pelton wheel impulse turbine, which utilized hydropower from the high head streams characteristic of the mountainous California interior.

A hydropower resource can be evaluated by its available power. Power is a function of the hydraulic head and volumetric flow rate. The head is the energy per unit weight (or unit mass) of water. The static head is proportional to the difference in height through which the water falls. Dynamic head is related to the velocity of moving water. Each unit of water can do an amount of work equal to its weight times the head.

The power available from falling water can be calculated from the flow rate and density of water, the height of fall, and the local acceleration due to gravity:

To illustrate, the power output of a turbine that is 85% efficient, with a flow rate of 80 cubic metres per second (2800 cubic feet per second) and a head of 145 metres (480 feet), is 97 Megawatts:

Operators of hydroelectric stations will compare the total electrical energy produced with the theoretical potential energy of the water passing through the turbine to calculate efficiency. Procedures and definitions for calculation of efficiency are given in test codes such as ASME PTC 18 and IEC 60041. Field testing of turbines is used to validate the manufacturer's guaranteed efficiency. Detailed calculation of the efficiency of a hydropower turbine will account for the head lost due to flow friction in the power canal or penstock, rise in tail water level due to flow, the location of the station and effect of varying gravity, the temperature and barometric pressure of the air, the density of the water at ambient temperature, and the altitudes above sea level of the forebay and tailbay. For precise calculations, errors due to rounding and the number of significant digits of constants must be considered.

Some hydropower systems such as water wheels can draw power from the flow of a body of water without necessarily changing its height. In this case, the available power is the kinetic energy of the flowing water. Over-shot water wheels can efficiently capture both types of energy.
The water flow in a stream can vary widely from season to season. Development of a hydropower site requires analysis of flow records, sometimes spanning decades, to assess the reliable annual energy supply. Dams and reservoirs provide a more dependable source of power by smoothing seasonal changes in water flow. However reservoirs have significant environmental impact, as does alteration of naturally occurring stream flow. The design of dams must also account for the worst-case, "probable maximum flood" that can be expected at the site; a spillway is often included to bypass flood flows around the dam. A computer model of the hydraulic basin and rainfall and snowfall records are used to predict the maximum flood.

Large dams can ruin river ecosystems, cover large areas of land causing green house gas emissions from underwater rotting vegetation and displace thousands of people and affect their livelihood.

Where there is a plentiful head of water it can be made to generate compressed air directly without moving parts. In these designs, a falling column of water is purposely mixed with air bubbles generated through turbulence or a venturi pressure reducer at the high-level intake. This is allowed to fall down a shaft into a subterranean, high-roofed chamber where the now-compressed air separates from the water and becomes trapped. The height of the falling water column maintains compression of the air in the top of the chamber, while an outlet, submerged below the water level in the chamber allows water to flow back to the surface at a lower level than the intake. A separate outlet in the roof of the chamber supplies the compressed air. A facility on this principle was built on the Montreal River at Ragged Shutes near Cobalt, Ontario in 1910 and supplied 5,000 horsepower to nearby mines.

Hydroelectricity is the application of hydropower to generate electricity.
It is the primary use of hydropower today.
Hydroelectric power plants can include a reservoir (generally created by a dam) to exploit the energy of falling water, or can use the kinetic energy of water as in run-of-the-river hydroelectricity.
Hydroelectric plants can vary in size from small community sized plants (micro hydro) to very large plants supplying power to a whole country.
As of 2019, the five largest power stations in the world are conventional hydroelectric power stations with dams.

Hydroelectricity can also be used to store energy in the form of potential energy between two reservoirs at different heights with pumped-storage hydroelectricity.
Water is pumped uphill into reservoirs during periods of low demand to be released for generation when demand is high or system generation is low.

Other forms of electricity generation with hydropower include tidal stream generators using energy from tidal power generated from oceans, rivers, and human-made canal systems to generating electricity.



</doc>
<doc id="14076" url="https://en.wikipedia.org/wiki?curid=14076" title="Horse breed">
Horse breed

A horse breed is a selectively bred population of domesticated horses, often with pedigrees recorded in a breed registry. However, the term is sometimes used in a broader sense to define landrace animals of a common phenotype located within a limited geographic region, or even feral “breeds” that are naturally selected. Depending on definition, hundreds of "breeds" exist today, developed for many different uses. Horse breeds are loosely divided into three categories based on general temperament: spirited "hot bloods" with speed and endurance; "cold bloods," such as draft horses and some ponies, suitable for slow, heavy work; and "warmbloods," developed from crosses between hot bloods and cold bloods, often focusing on creating breeds for specific riding purposes, particularly in Europe. 

Horse breeds are groups of horses with distinctive characteristics that are transmitted consistently to their offspring, such as conformation, color, performance ability, or disposition. These inherited traits are usually the result of a combination of natural crosses and artificial selection methods aimed at producing horses for specific tasks. Certain breeds are known for certain talents. For example, Standardbreds are known for their speed in harness racing. Some breeds have been developed through centuries of crossings with other breeds, while others, such as the Morgan horse, originated via a single sire from which all current breed members descend. More than 300 horse breeds exist in the world today.

Modern horse breeds developed in response to a need for "form to function", the necessity to develop certain physical characteristics to perform a certain type of work. Thus, powerful but refined breeds such as the Andalusian or the Lusitano developed in the Iberian peninsula as riding horses that also had a great aptitude for dressage, while heavy draft horses such as the Clydesdale and the Shire developed out of a need to perform demanding farm work and pull heavy wagons. Ponies of all breeds originally developed mainly from the need for a working animal that could fulfill specific local draft and transportation needs while surviving in harsh environments. However, by the 20th century, many pony breeds had Arabian and other blood added to make a more refined pony suitable for riding. Other horse breeds developed specifically for light agricultural work, heavy and light carriage and road work, various equestrian disciplines, or simply as pets.

Horses have been selectively bred since their domestication. However, the concept of purebred bloodstock and a controlled, written breed registry only became of significant importance in modern times. Today, the standards for defining and registration of different breeds vary. Sometimes, purebred horses are called Thoroughbreds, which is incorrect; "Thoroughbred" is a specific breed of horse, while a "purebred" is a horse (or any other animal) with a defined pedigree recognized by a breed registry. 

An early example of people who practiced selective horse breeding were the Bedouin, who had a reputation for careful breeding practices, keeping extensive pedigrees of their Arabian horses and placing great value upon pure bloodlines. Though these pedigrees were originally transmitted by an oral tradition, written pedigrees of Arabian horses can be found that date to the 14th century. In the same period of the early Renaissance, the Carthusian monks of southern Spain bred horses and kept meticulous pedigrees of the best bloodstock; the lineage survives to this day in the Andalusian horse. One of the earliest formal registries was General Stud Book for Thoroughbreds, which began in 1791 and traced back to the Arabian stallions imported to England from the Middle East that became the foundation stallions for the breed.

Some breed registries have a closed stud book, where registration is based on pedigree, and no outside animals can gain admittance. For example, a registered Thoroughbred or Arabian must have two registered parents of the same breed. 

Other breeds have a partially closed stud book, but still allow certain infusions from other breeds. For example, the modern Appaloosa must have at least one Appaloosa parent, but may also have a Quarter Horse, Thoroughbred, or Arabian parent, so long as the offspring exhibits appropriate color characteristics. The Quarter Horse normally requires both parents to be registered Quarter Horses, but allows "Appendix" registration of horses with one Thoroughbred parent, and the horse may earn its way to full registration by completing certain performance requirements.
Open stud books exist for horse breeds that either have not yet developed a rigorously defined standard phenotype, or for breeds that register animals that conform to an ideal via the process of passing a studbook selection process. Most of the warmblood breeds used in sport horse disciplines have open stud books to varying degrees. While pedigree is considered, outside bloodlines are admitted to the registry if the horses meet the set standard for the registry. These registries usually require a [
selection process involving judging of an individual animal's quality, performance, and conformation before registration is finalized. A few "registries," particularly some color breed registries, are very open and will allow membership of all horses that meet limited criteria, such as coat color and species, regardless of pedigree or conformation.

Breed registries also differ as to their acceptance or rejection of breeding technology. For example, all Jockey Club Thoroughbred registries require that a registered Thoroughbred be a product of a natural mating, so-called "live cover". A foal born of two Thoroughbred parents, but by means of artificial insemination or embryo transfer, cannot be registered in the Thoroughbred studbook. However, since the advent of DNA testing to verify parentage, most breed registries now allow artificial insemination, embryo transfer, or both. The high value of stallions has helped with the acceptance of these techniques because they allow a stallion to breed more mares with each "collection" and greatly reduce the risk of injury during mating. Cloning of horses is highly controversial, and at the present time most mainstream breed registries will not accept cloned horses, though several cloned horses and mules have been produced. Such restrictions have led to legal challenges in the United States, sometime based on state law and sometimes based on antitrust laws.

Horses can crossbreed with other equine species to produce hybrids. These hybrid types are not breeds, but they resemble breeds in that crosses between certain horse breeds and other equine species produce characteristic offspring. The most common hybrid is the mule, a cross between a "jack" (male donkey) and a mare. A related hybrid, the hinny, is a cross between a stallion and a jenny (female donkey). Most other hybrids involve the zebra (see Zebroid). With rare exceptions, most equine hybrids are sterile and cannot reproduce. A notable exception is hybrid crosses between horses and "Equus ferus przewalskii", commonly known as Przewalski's horse.


</doc>
<doc id="14082" url="https://en.wikipedia.org/wiki?curid=14082" title="Horse breeding">
Horse breeding

Horse breeding is reproduction in horses, and particularly the human-directed process of selective breeding of animals, particularly purebred horses of a given breed. Planned matings can be used to produce specifically desired characteristics in domesticated horses. Furthermore, modern breeding management and technologies can increase the rate of conception, a healthy pregnancy, and successful foaling.

The male parent of a horse, a stallion, is commonly known as the "sire" and the female parent, the mare, is called the "dam". Both are genetically important, as each parent provides half of the genetic makeup of the ensuing offspring, called a foal. Contrary to popular misuse, "colt" refers to a young male horse only; "filly" is a young female. Though many horse owners may simply breed a family mare to a local stallion in order to produce a companion animal, most professional breeders use selective breeding to produce individuals of a given phenotype, or breed. Alternatively, a breeder could, using individuals of differing phenotypes, create a new breed with specific characteristics.

A horse is "bred" where it is foaled (born). Thus a colt conceived in England but foaled in the United States is regarded as being bred in the US. In some cases, most notably in the Thoroughbred breeding industry, American- and Canadian-bred horses may also be described by the state or province in which they are foaled. Some breeds denote the country, or state, where conception took place as the origin of the foal.

Similarly, the "breeder", is the person who owned or leased the mare at the time of foaling. That individual may not have had anything to do with the mating of the mare. It is important to review each breed registry's rules to determine which applies to any specific foal.

In the horse breeding industry, the term "half-brother" or "half-sister" only describes horses which have the same dam, but different sires. Horses with the same sire but different dams are simply said to be "by the same sire", and no sibling relationship is implied. "Full" (or "own") siblings have both the same dam and the same sire. The terms paternal half-sibling, and maternal half-sibling are also often used. Three-quarter siblings are horses out of the same dam, and are by sires that are either half-brothers (i.e. same dam) or who are by the same sire.

Thoroughbreds and Arabians are also classified through the "distaff" or direct female line, known as their "family" or "tail female" line, tracing back to their taproot foundation bloodstock or the beginning of their respective stud books. The female line of descent always appears at the bottom of a tabulated pedigree and is therefore often known as the "bottom line". In addition, the maternal grandfather of a horse has a special term: damsire.

"Linebreeding" technically is the duplication of fourth generation or more distant ancestors. However, the term is often used more loosely, describing horses with duplication of ancestors closer than the fourth generation. It also is sometimes used as a euphemism for the practice of inbreeding, a practice that is generally frowned upon by horse breeders, though used by some in an attempt to fix certain traits.

The estrous cycle (also spelled oestrous) controls when a mare is sexually receptive toward a stallion, and helps to physically prepare the mare for conception. It generally occurs during the spring and summer months, although some mares may be sexually receptive into the late fall, and is controlled by the photoperiod (length of the day), the cycle first triggered when the days begin to lengthen. The estrous cycle lasts about 19–22 days, with the average being 21 days. As the days shorten, the mare returns to a period when she is not sexually receptive, known as anestrus. Anestrus – occurring in the majority of, but not all, mares – prevents the mare from conceiving in the winter months, as that would result in her foaling during the harshest part of the year, a time when it would be most difficult for the foal to survive.

This cycle contains 2 phases:

Depending on breed, on average, 16% of mares have double ovulations, allowing them to twin, though this does not affect the length of time of estrus or diestrus.

Changes in hormone levels can have great effects on the physical characteristics of the reproductive organs of the mare, thereby preparing, or preventing, her from conceiving.

The cycle is controlled by several hormones which regulate the estrous cycle, the mare's behavior, and the reproductive system of the mare. The cycle begins when the increased day length causes the pineal gland to reduce the levels of melatonin, thereby allowing the hypothalamus to secrete GnRH.

While horses in the wild mate and foal in mid to late spring, in the case of horses domestically bred for competitive purposes, especially horse racing, it is desirable that they be born as close to January 1 in the northern hemisphere or August 1 in the southern hemisphere as possible, so as to be at an advantage in size and maturity when competing against other horses in the same age group. When an early foal is desired, barn managers will put the mare "under lights" by keeping the barn lights on in the winter to simulate a longer day, thus bringing the mare into estrus sooner than she would in nature. Mares signal estrus and ovulation by urination in the presence of a stallion, raising the tail and revealing the vulva. A stallion, approaching with a high head, will usually nicker, nip and nudge the mare, as well as sniff her urine to determine her readiness for mating.

Once fertilized, the oocyte (egg) remains in the oviduct for approximately 5.5 more days, and then descends into the uterus. The initial single cell combination is already dividing and by the time of entry into the uterus, the egg might have already reached the blastocyst stage.

The gestation period lasts for about eleven months, or about 340 days (normal average range 320–370 days). During the early days of pregnancy, the conceptus is mobile, moving about in the uterus until about day 16 when "fixation" occurs. Shortly after fixation, the embryo proper (so called up to about 35 days) will become visible on trans-rectal ultrasound (about day 21) and a heartbeat should be visible by about day 23. After the formation of the endometrial cups and early placentation is initiated (35–40 days of gestation) the terminology changes, and the embryo is referred to as a fetus. True implantation – invasion into the endometrium of any sort – does not occur until about day 35 of pregnancy with the formation of the endometrial cups, and true placentation (formation of the placenta) is not initiated until about day 40-45 and not completed until about 140 days of pregnancy. The fetus's sex can be determined by day 70 of the gestation using ultrasound. Halfway through gestation the fetus is the size of between a rabbit and a beagle. The most dramatic fetal development occurs in the last 3 months of pregnancy when 60% of fetal growth occurs.

Colts are carried on average about 4 days longer than fillies.

Domestic mares receive specific care and nutrition to ensure that they and their foals are healthy. Mares are given vaccinations against diseases such as the Rhinopneumonitis (EHV-1) virus (which can cause miscarriage) as well as vaccines for other conditions that may occur in a given region of the world. Pre-foaling vaccines are recommended 4–6 weeks prior to foaling to maximize the immunoglobulin content of the colostrum in the first milk. Mares are dewormed a few weeks prior to foaling, as the mare is the primary source of parasites for the foal.

Mares can be used for riding or driving during most of their pregnancy. Exercise is healthy, though should be moderated when a mare is heavily in foal. Exercise in excessively high temperatures has been suggested as being detrimental to pregnancy maintenance during the embryonic period; however ambient temperatures encountered during the research were in the region of 100 degrees F and the same results may not be encountered in regions with lower ambient temperatures.

During the first several months of pregnancy, the nutritional requirements do not increase significantly since the rate of growth of the fetus is very slow. However, during this time, the mare may be provided supplemental vitamins and minerals, particularly if forage quality is questionable. During the last 3–4 months of gestation, rapid growth of the fetus increases the mare's nutritional requirements. Energy requirements during these last few months, and during the first few months of lactation are similar to those of a horse in full training. Trace minerals such as copper are extremely important, particularly during the tenth month of pregnancy, for proper skeletal formation. Many feeds designed for pregnant and lactating mares provide the careful balance required of increased protein, increased calories through extra fat as well as vitamins and minerals. Overfeeding the pregnant mare, particularly during early gestation, should be avoided, as excess weight may contribute to difficulties foaling or fetal/foal related problems.

Mares due to foal are usually separated from other horses, both for the benefit of the mare and the safety of the soon-to-be-delivered foal. In addition, separation allows the mare to be monitored more closely by humans for any problems that may occur while giving birth. In the northern hemisphere, a special foaling stall that is large and clutter free is frequently used, particularly by major breeding farms. Originally, this was due in part to a need for protection from the harsh winter climate present when mares foal early in the year, but even in moderate climates, such as Florida, foaling stalls are still common because they allow closer monitoring of mares. Smaller breeders often use a small pen with a large shed for foaling, or they may remove a wall between two box stalls in a small barn to make a large stall. In the milder climates seen in much of the southern hemisphere, most mares foal outside, often in a paddock built specifically for foaling, especially on the larger stud farms. Many stud farms worldwide employ technology to alert human managers when the mare is about to foal, including webcams, closed-circuit television, or assorted types of devices that alert a handler via a remote alarm when a mare lies down in a position to foal.

On the other hand, some breeders, particularly those in remote areas or with extremely large numbers of horses, may allow mares to foal out in a field amongst a herd, but may also see higher rates of foal and mare mortality in doing so.

Most mares foal at night or early in the morning, and prefer to give birth alone when possible. Labor is rapid, often no more than 30 minutes, and from the time the feet of the foal appear to full delivery is often only about 15 to 20 minutes. Once the foal is born, the mare will lick the newborn foal to clean it and help blood circulation. In a very short time, the foal will attempt to stand and get milk from its mother. A foal should stand and nurse within the first hour of life.

To create a bond with her foal, the mare licks and nuzzles the foal, enabling her to distinguish the foal from others. Some mares are aggressive when protecting their foals, and may attack other horses or unfamiliar humans that come near their newborns.

After birth, a foal's navel is dipped in antiseptic to prevent infection. The foal is sometimes given an enema to help clear the meconium from its digestive tract. The newborn is monitored to ensure that it stands and nurses without difficulty. While most horse births happen without complications, many owners have first aid supplies prepared and a veterinarian on call in case of a birthing emergency. People who supervise foaling should also watch the mare to be sure that she passes the placenta in a timely fashion, and that it is complete with no fragments remaining in the uterus. Retained fetal membranes can cause a serious inflammatory condition (endometritis) and/or infection. If the placenta is not removed from the stall after it is passed, a mare will often eat it, an instinct from the wild, where blood would attract predators.

Foals develop rapidly, and within a few hours a wild foal can travel with the herd. In domestic breeding, the foal and dam are usually separated from the herd for a while, but within a few weeks are typically pastured with the other horses. A foal will begin to eat hay, grass and grain alongside the mare at about 4 weeks old; by 10–12 weeks the foal requires more nutrition than the mare's milk can supply. Foals are typically weaned at 4–8 months of age, although in the wild a foal may nurse for a year.

Beyond the appearance and conformation of a specific type of horse, breeders aspire to improve physical performance abilities. This concept, known as matching "form to function," has led to the development of not only different breeds, but also families or bloodlines within breeds that are specialists for excelling at specific tasks.

For example, the Arabian horse of the desert naturally developed speed and endurance to travel long distances and survive in a harsh environment, and domestication by humans added a trainable disposition to the animal's natural abilities. In the meantime, in northern Europe, the locally adapted heavy horse with a thick, warm coat was domesticated and put to work as a farm animal that could pull a plow or wagon. This animal was later adapted through selective breeding to create a strong but rideable animal suitable for the heavily armored knight in warfare.

Then, centuries later, when people in Europe wanted faster horses than could be produced from local horses through simple selective breeding, they imported Arabians and other oriental horses to breed as an outcross to the heavier, local animals. This led to the development of breeds such as the Thoroughbred, a horse taller than the Arabian and faster over the distances of a few miles required of a European race horse or light cavalry horse. Another cross between oriental and European horses produced the Andalusian, a horse developed in Spain that was powerfully built, but extremely nimble and capable of the quick bursts of speed over short distances necessary for certain types of combat as well as for tasks such as bullfighting.

Later, the people who settled America needed a hardy horse that was capable of working with cattle. Thus, Arabians and Thoroughbreds were crossed on Spanish horses, both domesticated animals descended from those brought over by the Conquistadors, and feral horses such as the Mustangs, descended from the Spanish horse, but adapted by natural selection to the ecology and climate of the west. These crosses ultimately produced new breeds such as the American Quarter Horse and the Criollo of Argentina. In Canada, the Canadian Horse descended from the French stock Louis XIV sent to Canada in the late 17th century.[6] The initial shipment, in 1665, consisted of two stallions and twenty mares from the Royal Stables in Normandy and Brittany, the centre of French horse breeding.[7] Only 12 of the 20 mares survived the trip. Two more shipments followed, one in 1667 of 14 horses (mostly mares, but with at least one stallion), and one in 1670 of 11 mares and a stallion. The shipments included a mix of draft horses and light horses, the latter of which included both pacing and trotting horses.[1] The exact origins of all the horses are unknown, although the shipments probably included Bretons, Normans, Arabians, Andalusians and Barbs. 

In modern times, these breeds themselves have since been selectively bred to further specialize at certain tasks. One example of this is the American Quarter Horse. Once a general-purpose working ranch horse, different bloodlines now specialize in different events. For example, larger, heavier animals with a very steady attitude are bred to give competitors an advantage in events such as team roping, where a horse has to start and stop quickly, but also must calmly hold a full-grown steer at the end of a rope. On the other hand, for an event known as cutting, where the horse must separate a cow from a herd and prevent it from rejoining the group, the best horses are smaller, quick, alert, athletic and highly trainable. They must learn quickly, have conformation that allows quick stops and fast, low turns, and the best competitors have a certain amount of independent mental ability to anticipate and counter the movement of a cow, popularly known as "cow sense."

Another example is the Thoroughbred. While most representatives of this breed are bred for horse racing, there are also specialized bloodlines suitable as show hunters or show jumpers. The hunter must have a tall, smooth build that allows it to trot and canter smoothly and efficiently. Instead of speed, value is placed on appearance and upon giving the equestrian a comfortable ride, with natural jumping ability that shows bascule and good form.

A show jumper, however, is bred less for overall form and more for power over tall fences, along with speed, scope, and agility. This favors a horse with a good galloping stride, powerful hindquarters that can change speed or direction easily, plus a good shoulder angle and length of neck. A jumper has a more powerful build than either the hunter or the racehorse.

The history of horse breeding goes back millennia. Though the precise date is in dispute, humans could have domesticated the horse as far back as approximately 4500 BCE. However, evidence of planned breeding has a more blurry history. It is well known, for example, that the Romans did breed horses and valued them in their armies, but little is known regarding their breeding and husbandry practices: all that remains are statues and artwork. Mankind has plenty of equestrian statues of Roman emperors, horses are mentioned in the Odyssey by Homer, and hieroglyphics and paintings left behind by Egyptians tell stories of pharaohs hunting elephants from chariots. Nearly nothing is known of what became of the horses they bred for hippodromes, for warfare, or even for farming.

One of the earliest people known to document the breedings of their horses were the Bedouin of the Middle East, the breeders of the Arabian horse. While it is difficult to determine how far back the Bedouin passed on pedigree information via an oral tradition, there were written pedigrees of Arabian horses by CE 1330. The Akhal-Teke of West-Central Asia is another breed with roots in ancient times that was also bred specifically for war and racing. The nomads of the Mongolian steppes bred horses for several thousand years as well, and the Caspian horse is believed to be a very close relative of Ottoman horses from the earliest origins of the Turks in Central Asia.

The types of horse bred varied with culture and with the times. The uses to which a horse was put also determined its qualities, including smooth amblers for riding, fast horses for carrying messengers, heavy horses for plowing and pulling heavy wagons, ponies for hauling cars of ore from mines, packhorses, carriage horses and many others.

Medieval Europe bred large horses specifically for war, called destriers. These horses were the ancestors of the great heavy horses of today, and their size was preferred not simply because of the weight of the armor, but also because a large horse provided more power for the knight's lance. Weighing almost twice as much as a normal riding horse, the destrier was a powerful weapon in battle meant to act like a giant battering ram that could quite literally run down men on an enemy line.

On the other hand, during this same time, lighter horses were bred in northern Africa and the Middle East, where a faster, more agile horse was preferred. The lighter horse suited the raids and battles of desert people, allowing them to outmaneuver rather than overpower the enemy. When Middle Eastern warriors and European knights collided in warfare, the heavy knights were frequently outmaneuvered. The Europeans, however, responded by crossing their native breeds with "oriental" type horses such as the Arabian, Barb, and Turkoman horse This cross-breeding led both to a nimbler war horse, such as today's Andalusian horse, but also created a type of horse known as a Courser, a predecessor to the Thoroughbred, which was used as a message horse.

During the Renaissance, horses were bred not only for war, but for haute ecole riding, derived from the most athletic movements required of a war horse, and popular among the elite nobility of the time. Breeds such as the Lipizzan and the now extinct Neapolitan horse were developed from Spanish-bred horses for this purpose, and also became the preferred mounts of cavalry officers, who were derived mostly from the ranks of the nobility. It was during this time that firearms were developed, and so the light cavalry horse, a faster and quicker war horse, was bred for "shoot and run" tactics rather than the shock action as in the Middle Ages. Fine horses usually had a well muscled, curved neck, slender body, and sweeping mane, as the nobility liked to show off their wealth and breeding in paintings of the era.

After Charles II retook the British throne in 1660, horse racing, which had been banned by Cromwell, was revived. The Thoroughbred was developed 40 years later, bred to be the ultimate racehorse, through the lines of three foundation Arabian stallions and one Turkish horse.

In the 18th century, James Burnett, Lord Monboddo noted the importance of selecting appropriate parentage to achieve desired outcomes of successive generations. Monboddo worked more broadly in the abstract thought of species relationships and evolution of species. The Thoroughbred breeding hub in Lexington, Kentucky was developed in the late 18th century, and became a mainstay in American racehorse breeding.

The 17th and 18th centuries saw more of a need for fine carriage horses in Europe, bringing in the dawn of the warmblood. The warmblood breeds have been exceptionally good at adapting to changing times, and from their carriage horse beginnings they easily transitioned during the 20th century into a sport horse type. Today's warmblood breeds, although still used for competitive driving, are more often seen competing in show jumping or dressage.

The Thoroughbred continues to dominate the horse racing world, although its lines have been more recently used to improve warmblood breeds and to develop sport horses. The French saddle horse is an excellent example as is the Irish Sport Horse, the latter being an unusual combination between a Thoroughbred and a draft breed.

The American Quarter Horse was developed early in the 18th century, mainly for quarter racing (racing ¼ of a mile). Colonists did not have racetracks or any of the trappings of Europe that the earliest Thoroughbreds had at their disposal, so instead the owners of Quarter Horses would run their horses on roads that lead through town as a form of local entertainment. As the USA expanded West, the breed went with settlers as a farm and ranch animal, and "cow sense" was particularly valued: their use for herding cattle increased on rough, dry terrain that often involved sitting in the saddle for long hours.

However, this did not mean that the original ¼-mile races that colonists held ever went out of fashion, so today there are three types: the stock horse type, the racer, and the more recently evolving sport type. The racing type most resembles the finer-boned ancestors of the first racing Quarter Horses, and the type is still used for ¼-mile races. The stock horse type, used in western events and as a farm and patrol animal is bred for a shorter stride, an ability to stop and turn quickly, and an unflappable attitude that remains calm and focused even in the face of an angry charging steer. The first two are still to this day bred to have a combination of explosive speed that exceeds the Thoroughbred on short distances clocked as high as 55 mph, but they still retain the gentle, calm, and kindly temperament of their ancestors that makes them easily handled.

The Canadian horse's origin corresponds to shipments of French horses, some of which came from Louis XIV's own stable and most likely were Baroque horses meant to be gentlemen's mounts. These were ill-suited to farm work and to the hardscrabble life of the New World, so like the Americans, early Canadians crossed their horses with natives escapees. In time they evolved along similar lines as the Quarter Horse to the South as both the US and Canada spread westward and needed a calm and tractable horse versatile enough to carry the farmer's son to school but still capable of running fast and running hard as a cavalry horse, a stockhorse, or a horse to pull a conestoga wagon.

Other horses from North America retained a hint of their mustang origins by being either derived from stock that Native Americans bred that came in a rainbow of color, like the Appaloosa and American Paint Horse. with those East of the Mississippi River increasingly bred to impress and mimic the trends of the upper classes of Europe: The Tennessee Walking Horse and Saddlebred were originally plantation horses bred for their gait and comfortable ride in the saddle as a plantation master would survey his vast lands like an English lord.

Horses were needed for heavy draft and carriage work until replaced by the automobile, truck, and tractor. After this time, draft and carriage horse numbers dropped significantly, though light riding horses remained popular for recreational pursuits. Draft horses today are used on a few small farms, but today are seen mainly for pulling and plowing competitions rather than farm work. Heavy harness horses are now used as an outcross with lighter breeds, such as the Thoroughbred, to produce the modern warmblood breeds popular in sport horse disciplines, particularly at the Olympic level.

Breeding a horse is an endeavor where the owner, particularly of the mare, will usually need to invest considerable time and money. For this reason, a horse owner needs to consider several factors, including:

There are value judgements involved in considering whether an animal is suitable breeding stock, hotly debated by breeders. Additional personal beliefs may come into play when considering a suitable level of care for the mare and ensuing foal, the potential market or use for the foal, and other tangible and intangible benefits to the owner.

If the breeding endeavor is intended to make a profit, there are additional market factors to consider, which may vary considerably from year to year, from breed to breed, and by region of the world. In many cases, the low end of the market is saturated with horses, and the law of supply and demand thus allows little or no profit to be made from breeding unregistered animals or animals of poor quality, even if registered.

The minimum cost of breeding for a mare owner includes the stud fee, and the cost of proper nutrition, management and veterinary care of the mare throughout gestation, parturition, and care of both mare and foal up to the time of weaning. Veterinary expenses may be higher if specialized reproductive technologies are used or health complications occur.

Making a profit in horse breeding is often difficult. While some owners of only a few horses may keep a foal for purely personal enjoyment, many individuals breed horses in hopes of making some money in the process.

A rule of thumb is that a foal intended for sale should be worth three times the cost of the stud fee if it were sold at the moment of birth. From birth forward, the costs of care and training are added to the value of the foal, with a sale price going up accordingly. If the foal wins awards in some form of competition, that may also enhance the price.

On the other hand, without careful thought, foals bred without a potential market for them may wind up being sold at a loss, and in a worst-case scenario, sold for "salvage" value—a euphemism for sale to slaughter as horsemeat.

Therefore, a mare owner must consider their reasons for breeding, asking hard questions of themselves as to whether their motivations are based on either emotion or profit and how realistic those motivations may be.

The stallion should be chosen to complement the mare, with the goal of producing a foal that has the best qualities of both animals, yet avoids having the weaker qualities of either parent. Generally, the stallion should have proven himself in the discipline or sport the mare owner wishes for the "career" of the ensuing foal. Mares should also have a competition record showing that they also have suitable traits, though this does not happen as often.

Some breeders consider the quality of the sire to be more important than the quality of the dam. However, other breeders maintain that the mare is the most important parent. Because stallions can produce far more offspring than mares, a single stallion can have a greater overall impact on a breed. However, the mare may have a greater influence on an individual foal because its physical characteristics influence the developing foal in the womb and the foal also learns habits from its dam when young. Foals may also learn the "language of intimidation and submission" from their dam, and this imprinting may affect the foal's status and rank within the herd. Many times, a mature horse will achieve status in a herd similar to that of its dam; the offspring of dominant mares become dominant themselves.
A purebred horse is usually worth more than a horse of mixed breeding, though this matters more in some disciplines than others. The breed of the horse is sometimes secondary when breeding for a sport horse, but some disciplines may prefer a certain breed or a specific phenotype of horse. Sometimes, purebred bloodlines are an absolute requirement: For example, most racehorses in the world must be recorded with a breed registry in order to race.

Bloodlines are often considered, as some bloodlines are known to cross well with others. If the parents have not yet proven themselves by competition or by producing quality offspring, the bloodlines of the horse are often a good indicator of quality and possible strengths and weaknesses. Some bloodlines are known not only for their athletic ability, but could also carry a conformational or genetic defect, poor temperament, or for a medical problem. Some bloodlines are also fashionable or otherwise marketable, which is an important consideration should the mare owner wish to sell the foal.

Horse breeders also consider conformation, size and temperament. All of these traits are heritable, and will determine if the foal will be a success in its chosen discipline. The offspring, or "get", of a stallion are often excellent indicators of his ability to pass on his characteristics, and the particular traits he actually passes on. Some stallions are fantastic performers but never produce offspring of comparable quality. Others sire fillies of great abilities but not colts. At times, a horse of mediocre ability sires foals of outstanding quality.

Mare owners also look into the question of if the stallion is fertile and has successfully "settled" (i.e. impregnated) mares. A stallion may not be able to breed naturally, or old age may decrease his performance. Mare care boarding fees and semen collection fees can be a major cost.

Breeding a horse can be an expensive endeavor, whether breeding a backyard competition horse or the next Olympic medalist. Costs may include:

Stud fees are determined by the quality of the stallion, his performance record, the performance record of his get (offspring), as well as the sport and general market that the animal is standing for.

The highest stud fees are generally for racing Thoroughbreds, which may charge from two to three thousand dollars for a breeding to a new or unproven stallion, to several hundred thousand dollars for a breeding to a proven producer of stakes winners. Stallions in other disciplines often have stud fees that begin in the range of $1,000 to $3,000, with top contenders who produce champions in certain disciplines able to command as much as $20,000 for one breeding. The lowest stud fees to breed to a grade horse or an animal of low-quality pedigree may only be $100–$200, but there are trade-offs: the horse will probably be unproven, and likely to produce lower-quality offspring than a horse with a stud fee that is in the typical range for quality breeding stock.

As a stallion's career, either performance or breeding, improves, his stud fee tends to increase in proportion. If one or two offspring are especially successful, winning several stakes races or an Olympic medal, the stud fee will generally greatly increase. Younger, unproven stallions will generally have a lower stud fee earlier on in their careers.

To help decrease the risk of financial loss should the mare die or abort the foal while pregnant, many studs have a live foal guarantee (LFG) – also known as "no foal, free return" or "NFFR" - allowing the owner to have a free breeding to their stallion the next year. However, this is not offered for every breeding.

There are two general ways to "cover" or breed the mare:

After the mare is bred or artificially inseminated, she is checked using ultrasound 14–16 days later to see if she "took", and is pregnant. A second check is usually performed at 28 days. If the mare is not pregnant, she may be bred again during her next cycle.

It is considered safe to breed a mare to a stallion of much larger size. Because of the mare's type of placenta and its attachment and blood supply, the foal will be limited in its growth within the uterus to the size of the mare's uterus, but will grow to its genetic potential after it is born. Test breedings have been done with draft horse stallions bred to small mares with no increase in the number of difficult births.

When breeding live cover, the mare is usually boarded at the stud. She may be "teased" several times with a stallion that will not breed to her, usually with the stallion being presented to the mare over a barrier. Her reaction to the teaser, whether hostile or passive, is noted. A mare that is in heat will generally tolerate a teaser (although this is not always the case), and may present herself to him, holding her tail to the side. A veterinarian may also determine if the mare is ready to be bred, by ultrasound or palpating daily to determine if ovulation has occurred. Live cover can also be done in liberty on a paddock or on pasture, although due to safety and efficacy concerns, it is not common at professional breeding farms.

When it has been determined that the mare is ready, both the mare and intended stud will be cleaned. The mare will then be presented to the stallion, usually with one handler controlling the mare and one or more handlers in charge of the stallion. Multiple handlers are preferred, as the mare and stallion can be easily separated should there be any trouble.

The Jockey Club, the organization that oversees the Thoroughbred industry in the United States, requires all registered foals to be bred through live cover. Artificial insemination, listed below, is not permitted. Similar rules apply in other countries.

By contrast, the U.S. standardbred industry allows registered foals to be bred by live cover, or by artificial insemination (AI) with fresh or frozen (not dried) semen. No other artificial fertility treatment is allowed. In addition, foals bred via AI of frozen semen may only be registered if the stallion's sperm was collected during his lifetime, and used no later than the calendar year of his death or castration.

Artificial insemination (AI) has several advantages over live cover, and has a very similar conception rate:

A stallion is usually trained to mount a phantom (or dummy) mare, although a live mare may be used, and he is most commonly collected using an artificial vagina (AV) which is heated to simulate the vagina of the mare. The AV has a filter and collection area at one end to capture the semen, which can then be processed in a lab. The semen may be chilled or frozen and shipped to the mare owner or used to breed mares "on-farm". When the mare is in heat, the person inseminating introduces the semen directly into her uterus using a syringe and pipette.

Often an owner does not want to take a valuable competition mare out of training to carry a foal. This presents a problem, as the mare will usually be quite old by the time she is retired from her competitive career, at which time it is more difficult to impregnate her. Other times, a mare may have physical problems that prevent or discourage breeding. However, there are now several options for breeding these mares. These options also allow a mare to produce multiple foals each breeding season, instead of the usual one. Therefore, mares may have an even greater value for breeding.

The world's first cloned horse, Prometea, was born in 2003. Other notable instances of horse cloning are:




</doc>
<doc id="14084" url="https://en.wikipedia.org/wiki?curid=14084" title="Heterosexuality">
Heterosexuality

Heterosexuality is romantic attraction, sexual attraction or sexual behavior between persons of the opposite sex or gender. As a sexual orientation, heterosexuality is "an enduring pattern of emotional, romantic, and/or sexual attractions" to persons of the opposite sex; it "also refers to a person's sense of identity based on those attractions, related behaviors, and membership in a community of others who share those attractions." Someone who is heterosexual is commonly referred to as "straight."

Along with bisexuality and homosexuality, heterosexuality is one of the three main categories of sexual orientation within the heterosexual–homosexual continuum. Across cultures, most people are heterosexual, and heterosexual activity is by far the most common type of sexual activity.

Scientists do not know the exact cause of sexual orientation, but they theorize that it is caused by a complex interplay of genetic, hormonal, and environmental influences, and do not view it as a choice. Although no single theory on the cause of sexual orientation has yet gained widespread support, scientists favor biologically-based theories. There is considerably more evidence supporting nonsocial, biological causes of sexual orientation than social ones, especially for males.

The term "heterosexual" or "heterosexuality" is usually applied to humans, but heterosexual behavior is observed in all other mammals and in other animals, as it is necessary for sexual reproduction.

"Hetero-" comes from the Greek word "ἕτερος" [héteros], meaning "other party" or "another", used in science as a prefix meaning "different"; and the Latin word for sex (that is, characteristic sex or sexual differentiation).

The current use of the term "heterosexual" has its roots in the broader 19th century tradition of personality taxonomy. The term "heterosexual" was coined alongside the word "homosexual" by Karl Maria Kertbeny in 1869. The terms were not in current use during the late nineteenth century, but were reintroduced by Richard von Krafft-Ebing and Albert Moll around 1890. The noun came into wider use from the early 1920s, but did not enter common use until the 1960s. The colloquial shortening "hetero" is attested from 1933. The abstract noun "heterosexuality" is first recorded in 1900. The word ""heterosexual"" was listed in Merriam-Webster's "New International Dictionary" in 1923 as a medical term for "morbid sexual passion for one of the opposite sex"; however, in 1934 in their "Second Edition Unabridged" it is defined as a "manifestation of sexual passion for one of the opposite sex; normal sexuality".

In LGBT slang, the term "breeder" has been used as a denigrating phrase to deride heterosexuals. Hyponyms of heterosexual include "heteroflexible".

The word can be informally shortened to "hetero". The term "straight" originated as a mid-20th century gay slang term for heterosexuals, ultimately coming from the phrase "to go straight" (as in "straight and narrow"), or stop engaging in homosexual sex. One of the first uses of the word in this way was in 1941 by author G. W. Henry. Henry's book concerned conversations with homosexual males and used this term in connection with people who are identified as ex-gays. It is now simply a colloquial term for "heterosexual", having changed in primary meaning over time. Some object to usage of the term "straight" because it implies that non-heteros are crooked.

In their 2016 literature review, Bailey "et al." stated that they "expect that in all cultures the vast majority of individuals are sexually predisposed exclusively to the other sex (i.e., heterosexual)" and that there is no persuasive evidence that the demographics of sexual orientation have varied much across time or place. Heterosexual activity between only one male and one female is by far the most common type of sociosexual activity.

According to several major studies, 89% to 98% of people have had only heterosexual contact within their lifetime; but this percentage falls to 79–84% when either or both same-sex attraction and behavior are reported.

A 1992 study reported that 93.9% of males in Britain have only had heterosexual experience, while in France the number was reported at 95.9%. According to a 2008 poll, 85% of Britons have only opposite-sex sexual contact while 94% of Britons identify themselves as heterosexual. Similarly, a survey by the UK Office for National Statistics (ONS) in 2010 found that 95% of Britons identified as heterosexual, 1.5% of Britons identified themselves as homosexual or bisexual, and the last 3.5% gave more vague answers such as "don't know", "other", or did not respond to the question. In the United States, according to a Williams Institute report in April 2011, 96% or approximately 250 million of the adult population are heterosexual.

An October 2012 Gallup poll provided unprecedented demographic information about those who identify as heterosexual, arriving at the conclusion that 96.6%, with a margin of error of ±1%, of all U.S. adults identify as heterosexual. The Gallup results show:

In a 2015 YouGov survey of 1,000 adults of the United States, 89% of the sample identified as heterosexual, 4% as homosexual (2% as homosexual male and 2% as homosexual female) and 4% as bisexual (of either sex).

Bailey "et al.", in their 2016 review, stated that in recent Western surveys, about 93% of men and 87% of women identify as completely heterosexual, and about 4% of men and 10% of women as mostly heterosexual.

No simple and singular determinant for sexual orientation has been conclusively demonstrated, but scientists believe that a combination of genetic, hormonal, and environmental factors determine sexual orientation. They favor biological theories for explaining the causes of sexual orientation, as there is considerably more evidence supporting nonsocial, biological causes than social ones, especially for males.

Factors related to the development of a heterosexual orientation include genes, prenatal hormones, and brain structure, and their interaction with the environment.

The neurobiology of the masculinization of the brain is fairly well understood. Estradiol and testosterone, which is catalyzed by the enzyme 5α-reductase into dihydrotestosterone, act upon androgen receptors in the brain to masculinize it. If there are few androgen receptors (people with androgen insensitivity syndrome) or too much androgen (females with congenital adrenal hyperplasia), there can be physical and psychological effects. It has been suggested that both male and female heterosexuality are the results of this process. In these studies heterosexuality in females is linked to a lower amount of masculinization than is found in lesbian females, though when dealing with male heterosexuality there are results supporting both higher and lower degrees of masculinization than homosexual males.

Sexual reproduction in the animal world is facilitated through opposite-sex sexual activity, although there are also animals that reproduce asexually, including protozoa and lower invertebrates.

Reproductive sex does not require a heterosexual orientation, since sexual orientation typically refers to a long-term enduring pattern of sexual and emotional attraction leading often to long-term social bonding, while reproduction requires as little as a single act of copulation to fertilize the ovum by sperm.

Often, sexual orientation and sexual orientation identity are not distinguished, which can impact accurately assessing sexual identity and whether or not sexual orientation is able to change; sexual orientation identity can change throughout an individual's life, and may or may not align with biological sex, sexual behavior or actual sexual orientation.<ref name="Concordance/discordance in SO"></ref> Sexual orientation is stable and unlikely to change for the vast majority of people, but some research indicates that some people may experience change in their sexual orientation, and this is more likely for women than for men. The American Psychological Association distinguishes between sexual orientation (an innate attraction) and sexual orientation identity (which may change at any point in a person's life).

A 2012 study found that 2% of a sample of 2,560 adult participants reported a change of sexual orientation identity after a 10-year period. For men, a change occurred in 0.78% of those who had identified as heterosexual, 9.52% of homosexuals, and 47% of bisexuals. For women, a change occurred in 1.36% of heterosexuals, 63.6% of lesbians, and 64.7% of bisexuals.

A 2-year study by Lisa M. Diamond on a sample of 80 non-heterosexual female adolescents (age 16-23) reported that half of the participants had changed sexual-minority identities more than once, one third of them during the 2-year follow-up. Diamond concluded that "although sexual attractions appear fairly stable, sexual identities and behaviors are more fluid."

Heteroflexibility is a form of sexual orientation or situational sexual behavior characterized by minimal homosexual activity in an otherwise primarily heterosexual orientation that is considered to distinguish it from bisexuality. It has been characterized as "mostly straight".

Sexual orientation change efforts are methods that aim to change sexual orientation, used to try to convert homosexual and bisexual people to heterosexuality. Scientists and mental health professionals generally do not believe that sexual orientation is a choice. There are no studies of adequate scientific rigor that conclude that sexual orientation change efforts are effective.

A heterosexual couple, a man and woman in an intimate relationship, form the core of a nuclear family.
Many societies throughout history have insisted that a marriage take place before the couple settle down, but enforcement of this rule or compliance with it has varied considerably.

Heterosexual symbolism dates back to the earliest artifacts of humanity, with gender symbols, ritual fertility carvings, and primitive art. This was later expressed in the symbolism of fertility rites and polytheistic worship, which often included images of human reproductive organs, such as lingam in Hinduism. Modern symbols of heterosexuality in societies derived from European traditions still reference symbols used in these ancient beliefs. One such image is a combination of the symbol for Mars, the Roman god of war, as the definitive male symbol of masculinity, and Venus, the Roman goddess of love and beauty, as the definitive female symbol of femininity. The unicode character for this combined symbol is ⚤ (U+26A4).

There was no need to coin a term such as "heterosexual" until there was something else to contrast and compare it with. Jonathan Ned Katz dates the definition of heterosexuality, as it is used today, to the late 19th century. According to Katz, in the Victorian era, sex was seen as a means to achieve reproduction, and relations between the sexes were not believed to be overtly sexual. The body was thought of as a tool for procreation, "human energy, though of as a closed and severely limited system, was to be used in producing children and in work, not wasted in libidinous pleasures."

Katz argues that modern ideas of sexuality and eroticism began to develop in America and Germany in the later 19th century. The changing economy and the "transformation of the family from producer to consumer" resulted in shifting values. The Victorian work ethic had changed, pleasure became more highly valued and this allowed ideas of human sexuality to change. Consumer culture had created a market for the erotic, pleasure became commoditized. At the same time medical doctors began to acquire more power and influence. They developed the medical model of "normal love," in which healthy men and women enjoyed sex as part of a "new ideal of male-female relationships that included.. an essential, necessary, normal eroticism." This model also had a counterpart, "the Victorian Sex Pervert," anyone who failed to meet the norm. The basic oppositeness of the sexes was the basis for normal, healthy sexual attraction. "The attention paid the sexual abnormal created a need to name the sexual normal, the better to distinguish the average him and her from the deviant it." The creation of the term "heterosexual" consolidated the social existence of the pre-existing heterosexual experience and created a sense of ensured and validated normalcy within it.

The Judeo-Christian tradition has several scriptures related to heterosexuality. The Book of Genesis states that God created man because "It is not good that the man should be alone; I will make him an help meet for him," (KJV), and that "Therefore shall a man leave his father and his mother, and shall cleave unto his wife: and they shall be one flesh," (KJV). In 1 Corinthians, Christians are advised:

For the most part, religious traditions in the world reserve marriage to heterosexual unions, but there are exceptions including certain Buddhist and Hindu traditions, Unitarian Universalists, Metropolitan Community Church, some Anglican dioceses, and some Quaker, United Church of Canada, and Reform and Conservative Jewish congregations.

Almost all religions believe that lawful sex between a man and a woman is allowed, but there are a few that believe that it is a sin, such as The Shakers, The Harmony Society, and The Ephrata Cloister. These religions tend to view all sexual relations as sinful, and promote celibacy. Some religions require celibacy for certain roles, such as Catholic priests; however, the Catholic Church also views heterosexual marriage as sacred and necessary.

Heteronormativity denotes or relates to a world view that promotes heterosexuality as the normal or preferred sexual orientation for people to have. It can assign strict gender roles to males and females. The term was popularized by Michael Warner in 1991. Many gender and sexuality scholars argue that compulsory heterosexuality, a continual and repeating reassertion of heterosexual norms, is a facet of heterosexism. Compulsory heterosexuality is the idea that female heterosexuality is both assumed and enforced by a patriarchal society. Heterosexuality is then viewed as the natural inclination or obligation by both sexes. Consequently, anyone who differs from the normalcy of heterosexuality is deemed deviant or abhorrent.

Heterosexism is a form of bias or discrimination in favor of opposite-sex sexuality and relationships. It may include an assumption that everyone is heterosexual and may involve various kinds of discrimination against gays, lesbians, bisexuals, asexuals, heteroflexible people, or transgender or non-binary individuals.

Straight pride is a slogan that arose in the late 1980s and early 1990s and has been used primarily by social conservative groups as a political stance and strategy. The term is described as a response to gay pride adopted by various LGBT groups in the early 1970s or to the accommodations provided to gay pride initiatives.





</doc>
<doc id="14086" url="https://en.wikipedia.org/wiki?curid=14086" title="Hopewell Centre (Hong Kong)">
Hopewell Centre (Hong Kong)

Hopewell Centre is a , 64-storey skyscraper at 183 Queen's Road East, in Wan Chai, Hong Kong Island in Hong Kong. The tower is the first circular skyscraper in Hong Kong. It is named after Hong Kong–listed property firm Hopewell Holdings Limited, which constructed the building. Hopewell Holdings Limited's headquarters are in the building and its Chief executive officer, Gordon Wu, has his office on the top floor.

Construction started in 1977 and was completed in 1980. Upon completion, Hopewell Centre surpassed Jardine House as Hong Kong's tallest building. It was also the second tallest building in Asia at the time. It kept its title in Hong Kong until 1989, when the Bank of China Tower was completed. The building is now the 20th tallest building in Hong Kong.

The building has a circular floor plan. Although the front entrance is on the 'ground floor', commuters are taken through a set of escalators to the 3rd floor lift lobby. Hopewell Centre stands on the slope of a hill so steep that the building has its back entrance on the 17th floor towards Kennedy Road. There is a circular private swimming pool on the roof of the building built for feng shui reasons.

A revolving restaurant located on the 62nd floor, called "Revolving 66", overlooks other tall buildings below and the harbour. It was originally called Revolving 62, but soon changed its name as locals kept calling it Revolving 66. It completes a 360-degree rotation each hour. Passengers take either office lifts (faster) or the scenic lifts (with a view) to the 56/F, where they transfer to smaller lifts up to the 62/F. The restaurant is now named The Grand Buffet.

The building comprises several groups of lifts. Lobbies are on the 3rd and 17th floor, and are connected to Queen's Road East and Kennedy Road respectively. A mini-skylobby is on the 56th floor and serves as a transfer floor for diners heading to the 60/F and 62/F restaurants. The building's white 'bumps' between the windows have built in window-washer guide rails.

This skyscraper was the filming location for R&B group Dru Hill's music video for "How Deep Is Your Love," directed by Brett Ratner, who also directed the movie Rush Hour, whose soundtrack features the song. The circular private swimming pool is well visible in this music video. This swimming pool has also featured in an Australian television advertisement by one of that country's major gaming companies, Tattersall's Limited, promoting a weekly lottery competition.


Hopewell shares shoot up 31 per cent after developer unveils HK$21.26 billion privatisation plan




</doc>
<doc id="14089" url="https://en.wikipedia.org/wiki?curid=14089" title="Harwich, Massachusetts">
Harwich, Massachusetts

Harwich ( ) is a New England town on Cape Cod, in Barnstable County in the state of Massachusetts in the United States. At the 2010 census it had a population of 12,243. The town is a popular vacation spot, located near the Cape Cod National Seashore. Harwich's beaches are on the Nantucket Sound side of Cape Cod. Harwich has three active harbors. Saquatucket, Wychmere and Allen Harbors are all in Harwich Port. The town of Harwich includes the villages of Pleasant Lake, West Harwich, East Harwich, Harwich Port, Harwich Center, North Harwich and South Harwich.

Harwich was first settled by Europeans in 1670 as part of Yarmouth. The town was officially incorporated in 1694, and originally included the lands of the current town of Brewster. Early industry involved fishing and farming. The town is considered by some to be the birthplace of the cranberry industry, with the first commercial operation opened in 1846. There are still many bogs in the town, although the economy is now more centered on tourism and as a residential community. The town is also the site of the start/finish line of the "Sail Around the Cape", which rounds the Cape counter-clockwise, returning via the Cape Cod Canal.

Since 1976, the town has hosted the annual Harwich Cranberry Festival, noted for its fireworks display, in September.

In the summer, the town is host to the Harwich Mariners of the Cape Cod Baseball League. The Mariners were the 2008 league champions. The team plays at Whitehouse Field.

The Patriot Square Shopping Center in neighboring South Dennis is convenient for residents of North Harwich and West Harwich. The plaza contains a Stop & Shop supermarket and other stores around it. Supermarkets in Harwich include a Shaw's Star Market on the Harwich Port/West Harwich border and another Stop & Shop in East Harwich.

According to the United States Census Bureau, the town has a total area of , of which is land and , or 36.97%, is water. The seven villages of Harwich are West Harwich, North Harwich, East Harwich, South Harwich, Harwich Center, Harwich Port and Pleasant Lake. These are also referred to as the Harwiches.

Harwich is on the southern side of Cape Cod, just west of the southeastern corner. It is bordered by Dennis to the west, Brewster to the north, Orleans to the northeast, Chatham to the east, and Nantucket Sound to the south. Harwich is approximately east of Barnstable, east of the Cape Cod Canal, south of Provincetown, and southeast of Boston.

The town shares the largest lake on the Cape, called Long Pond, with the town of Brewster. Long Pond serves as a private airport for planes with the ability to land on water. The village of Pleasant Lake is at the southwest corner of the lake. Numerous other smaller bodies of water dot the town. Sand Pond, a public beach and swimming area, is located off Great Western Road in North Harwich.

The shore is home to several harbors and rivers, including the Herring River, Allens Harbor, Wychmere Harbor, Saquatucket Harbor, and the Andrews River. The town is also the home to the Hawksnest State Park, as well as a marina and several beaches, including two on Long Pond. There are also many beaches in West Harwich and South Harwich.

According to the Köppen climate classification system, Harwich, Massachusetts has a warm-summer, wet all year, humid continental climate ("Dfb"). Dfb climates are characterized by at least one month having an average mean temperature ≤ 32.0 °F (≤ 0.0 °C), at least four months with an average mean temperature ≥ 50.0 °F (≥ 10.0 °C), all months with an average mean temperature ≤ 71.6 °F (≤ 22.0 °C), and no significant precipitation difference between seasons. The average seasonal (Nov-Apr) snowfall total is around 30 in (76 cm). The average snowiest month is February which corresponds with the annual peak in nor'easter activity. The plant hardiness zone is 7a with an average annual extreme minimum air temperature of 4.0 °F (-15.6 °C).

According to the A. W. Kuchler U.S. Potential natural vegetation Types, Harwich, Massachusetts would primarily contain a Northeastern Oak/Pine ("110") vegetation type with a Southern Mixed Forest ("26") vegetation form.

As of the census of 2000, there were 12,386 people, 5,471 households, and 3,545 families residing in the town. The population density was 588.6 people per square mile (227.3/km). There were 9,450 housing units at an average density of 449.1 per square mile (173.4/km). The racial makeup of the town was 95.41% White, 0.71% Black or African American, 0.19% Native American, 0.22% Asian, 0.05% Pacific Islander, 2.03% from other races, and 1.40% from two or more races. 0.96% of the population were Hispanic or Latino of any race.

There were 5,471 households, out of which 21.3% had children under the age of 18 living with them, 53.4% were married couples living together, 9.0% had a female householder with no husband present, and 35.2% were non-families. 29.8% of all households were made up of individuals, and 16.9% had someone living alone who was 65 years of age or older. The average household size was 2.20 and the average family size was 2.72.

In the town, the population was spread out, with 18.3% under the age of 18, 4.2% from 18 to 24, 22.1% from 25 to 44, 25.8% from 45 to 64, and 29.6% who were 65 years of age or older. The median age was 49 years. For every 100 females, there were 84.5 males. For every 100 females age 18 and over, there were 79.7 males.

The median income for a household in the town was $41,552, and the median income for a family was $51,070. Males had a median income of $38,948 versus $27,439 for females. The per capita income for the town was $23,063. About 2.9% of families and 15.5% of the population were below the poverty line, including 8.4% of those under age 18 and 8.1% of those age 65 or over.

The town of Harwich contains several smaller census-designated places (CDPs) for which the U.S. Census reports more focused geographic and demographic information. The CDPs in Harwich are Harwich Center, Harwich Port (including South Harwich), East Harwich and Northwest Harwich (including West Harwich, North Harwich, and Pleasant Lake).

Harwich is represented in the Massachusetts House of Representatives as a part of the Fourth Barnstable district, which includes (with the exception of Brewster) all the towns east and north of Harwich on the Cape. The town is represented in the Massachusetts Senate as a part of the Cape and Islands District, which includes all of Cape Cod, Martha's Vineyard and Nantucket except the towns of Bourne, Falmouth, Sandwich and a portion of Barnstable. The town is patrolled by the Second (Yarmouth) Barracks of Troop D of the Massachusetts State Police.

On the national level, Harwich is a part of Massachusetts's 9th congressional district, and is currently represented by William R. Keating. The state's senior member of the United States Senate is Elizabeth Warren, elected in 2012. The junior senator is Ed Markey, elected in 2013.

Harwich is governed by the open town meeting form of government, led by a town administrator and a board of selectmen.

There are three libraries in the town. The municipal library, the Brooks Free Library in Harwich Center, is the largest and is a member of the Cape Libraries Automated Materials Sharing (CLAMS) library network. There are two smaller non-municipal libraries – the Chase Library on Route 28 in West Harwich at the Dennis town line, and the Harwich Port Library on Lower Bank Street in Harwich Port.

Harwich is the site of the Long Pond Medical Center, which serves the southeastern Cape region.

Harwich has police and fire departments, with one fire and police station headquarters, and Station 2 in East Harwich.

There are post offices in Harwich Port, South Harwich, West Harwich, and East Harwich.

Harwich's schools are part of the Monomoy Regional School District. Harwich Elementary School serves students from pre-school through fourth grade, Monomoy Regional Middle School which serves both Harwich and its joining town, Chatham. This middle school serves grades 5–7, and Monomoy Regional High School serves grades 8–12 for both Harwich and Chatham. Monomoy's teams are known as the Sharks. Harwich is known for its excellent boys basketball, girls basketball, girls field hockey, softball and baseball teams.

The Lighthouse Charter School recently moved into where the Harwich Cinema building was located.

Harwich is the site of Cape Cod Regional Technical High School, a grades 9–12 high school which serves most of Cape Cod. The town is also home to Holy Trinity PreSchool, a Catholic pre-school which serves pre-kindergarten in West Harwich.

Two of Massachusetts major routes, U.S. Route 6 and Massachusetts Route 28, cross the town. The town has the southern termini of Routes 39 and 124, and a portion of Route 137 passes through the town. Route 39 leads east through East Harwich to Orleans. Route 28 passes through West Harwich and Harwich Port, connecting the towns of Dennis and Chatham. Route 124 leads from Harwich Center to Brewster, and Route 137 cuts through East Harwich leading from Chatham to Brewster.

A portion of the Cape Cod Rail Trail, as well as several other bicycle routes, are in town. There is no rail service in town, but the Cape Cod Rail Trail rotary is located in North Harwich near Main Street.

Other than the occasional sea plane landing on the pond, the nearest airport is in neighboring Chatham; the nearest regional service is at Barnstable Municipal Airport; and the nearest national and international air service is at Logan International Airport in Boston.

In recent years parts of Cape Cod have introduced bus service, especially during the summer to help cut down on traffic.





</doc>
<doc id="14090" url="https://en.wikipedia.org/wiki?curid=14090" title="Hull classification symbol">
Hull classification symbol

The United States Navy, United States Coast Guard, and United States National Oceanic and Atmospheric Administration (NOAA) use a hull classification symbol (sometimes called hull code or hull number) to identify their ships by type and by individual ship within a type. The system is analogous to the pennant number system that the Royal Navy and other European and Commonwealth navies use.

The U.S. Navy began to assign unique Naval Registry Identification Numbers to its ships in the 1890s. The system was a simple one in which each ship received a number which was appended to its ship type, fully spelled out, and added parenthetically after the ship's name when deemed necessary to avoid confusion between ships. Under this system, for example, the battleship "Indiana" was USS "Indiana" (Battleship No. 1,) the cruiser "Olympia" was USS "Olympia" (Cruiser No. 6,) and so on. Beginning in 1907, some ships also were referred to alternatively by single-letter or three-letter codes—for example, USS "Indiana" (Battleship No. 1) could be referred to as USS "Indiana" (B-1) and USS "Olympia" (Cruiser No. 6) could also be referred to as USS "Olympia" (C-6), while USS "Pennsylvania" (Armored Cruiser No. 4) could be referred to as USS "Pennsylvania" (ACR-4). However, rather than replacing it, these codes coexisted and were used interchangeably with the older system until the modern system was instituted on 17 July 1920.

During World War I, the U.S. Navy acquired large numbers of privately owned and commercial ships and craft for use as patrol vessels, mine warfare vessels, and various types of naval auxiliary ships, some of them with identical names. To keep track of them all, the Navy assigned unique identifying numbers to them. Those deemed appropriate for patrol work received section patrol numbers (SP), while those intended for other purposes received "identification numbers", generally abbreviated "Id. No." or "ID;" some ships and craft changed from an SP to an ID number or vice versa during their careers, without their unique numbers themselves changing, and some ships and craft assigned numbers in anticipation of naval service were never acquired by the Navy. The SP/ID numbering sequence was unified and continuous, with no SP number repeated in the ID series or vice versa so that there could not be, for example, both an "SP-435" and an "Id. No 435". The SP and ID numbers were used parenthetically after each boat's or ship's name to identify it; although this system pre-dated the modern hull classification system and its numbers were not referred to at the time as "hull codes" or "hull numbers," it was used in a similar manner to today's system and can be considered its precursor.

The United States Revenue Cutter Service, which merged with the United States Lifesaving Service in January 1915 to form the modern United States Coast Guard, began following the Navy's lead in the 1890s, with its cutters having parenthetical numbers called Naval Registry Identification Numbers following their names, such as (Cutter No. 1), etc. This persisted until the Navy's modern hull classification system's introduction in 1920, which included Coast Guard ships and craft.

Like the U.S. Navy, the United States Coast and Geodetic Survey – a uniformed seagoing service of the United States Government and a predecessor of the National Oceanic and Atmospheric Administration (NOAA) – adopted a hull number system for its fleet in the 20th century. Its largest vessels, "Category I" oceanographic survey ships, were classified as "ocean survey ships" and given the designation "OSS". Intermediate-sized "Category II" oceanographic survey ships received the designation "MSS" for "medium survey ship," and smaller "Category III" oceanographic survey ships were given the classification "CSS" for "coastal survey ship." A fourth designation, "ASV" for "auxiliary survey vessel," included even smaller vessels. In each case, a particular ship received a unique designation based on its classification and a unique hull number separated by a space rather than a hyphen; for example, the third Coast and Geodetic Survey ship named "Pioneer" was an ocean survey ship officially known as USC&GS "Pioneer" (OSS 31). The Coast and Geodetic Surveys system persisted after the creation of NOAA in 1970, when NOAA took control of the Surveys fleet, but NOAA later changed to its modern hull classification system.

The U.S. Navy instituted its modern hull classification system on 17 July 1920, doing away with section patrol numbers, "identification numbers", and the other numbering systems described above. In the new system, all hull classification symbols are at least two letters; for basic types the symbol is the first letter of the type name, doubled, except for aircraft carriers.

The combination of symbol and hull number identifies a modern Navy ship uniquely. A heavily modified or re-purposed ship may receive a new symbol, and either retain the hull number or receive a new one. For example, the heavy gun cruiser was converted to a gun/missile cruiser, changing the hull number to CAG-1. Also, the system of symbols has changed a number of times both since it was introduced in 1907 and since the modern system was instituted in 1920, so ships' symbols sometimes change without anything being done to the physical ship.

Hull numbers are assigned by classification. Duplication between, but not within, classifications is permitted. Hence, CV-1 was the aircraft carrier and BB-1 was the battleship .

Ship types and classifications have come and gone over the years, and many of the symbols listed below are not presently in use. The Naval Vessel Register maintains an online database of U.S. Navy ships showing which symbols are presently in use.

After World War II until 1975, the U.S. Navy defined a "frigate" as a type of surface warship larger than a destroyer and smaller than a cruiser. In other navies, such a ship generally was referred to as a "flotilla leader", or "destroyer leader". Hence the U.S. Navy's use of "DL" for "frigate" prior to 1975, while "frigates" in other navies were smaller than destroyers and more like what the U.S. Navy termed a "destroyer escort", "ocean escort", or "DE". The United States Navy 1975 ship reclassification of cruisers, frigates, and ocean escorts brought U.S. Navy classifications into line with other nations' classifications, at least cosmetically in terms of terminology, and eliminated the perceived "cruiser gap" with the Soviet Navy by redesignating the former "frigates" as "cruisers".

If a U.S. Navy ship's hull classification symbol begins with "T-", it is part of the Military Sealift Command, has a primarily civilian crew, and is a United States Naval Ship (USNS) in non-commissioned service – as opposed to a commissioned United States Ship (USS) with an all-military crew.

If a ship's hull classification symbol begins with "W", it is a commissioned cutter of the United States Coast Guard. Until 1965, the Coast Guard used U.S. Navy hull classification codes, prepending a "W" to their beginning. In 1965, it retired some of the less mission-appropriate Navy-based classifications and developed new ones of its own, most notably WHEC for "high endurance cutter" and WMEC for "medium endurance cutter".

The National Oceanic and Atmospheric Administration (NOAA), a component of the United States Department of Commerce, includes the National Oceanic and Atmospheric Administration Commissioned Officer Corps (or "NOAA Corps"), one of the eight uniformed services of the United States, and operates a fleet of seagoing research and survey ships. The NOAA fleet also uses a hull classification symbol system, which it also calls "hull numbers," for its ships.

After NOAA took over the former Coast and Geodetic Survey fleet in 1970 along with research vessels of other government agencies, it adopted a new system of ship classification. In its system, the NOAA fleet is divided into two broad categories, research ships and survey ships. The research ships, which include oceanographic and fisheries research vessels, are given hull numbers beginning with "R", while the survey ships, generally hydrographic survey vessels, receive hull numbers beginning with "S". The letter is followed by a three-digit number; the first digit indicates the NOAA "class" (i.e., size) of the vessel, which NOAA assigns based on the ship's gross tonnage and horsepower, while the next two digits combine with the first digit to create a unique three-digit identifying number for the ship.

Generally, each NOAA hull number is written with a space between the letter and the three-digit number, as in, for example, or .

Unlike the Navy, once an older NOAA ship leaves service, a newer one can be given the same hull number; for example, "S 222" was assigned to , then assigned to NOAAS "Thomas Jefferson" (S 222), which entered NOAA service after "Mount Mitchell" was stricken.

The U.S. Navy's system of alpha-numeric ship designators, and its associated hull numbers, have been for several decades a unique method of categorizing ships of all types: combatants, auxiliaries and district craft. Though considerably changed in detail and expanded over the years, this system remains essentially the same as when formally implemented in 1920. It is a very useful tool for organizing and keeping track of naval vessels, and also provides the basis for the identification numbers painted on the bows (and frequently the sterns) of most U.S. Navy ships.

The ship designator and hull number system's roots extend back to the late 1880s when ship type serial numbers were assigned to most of the new-construction warships of the emerging "Steel Navy". During the course of the next thirty years, these same numbers were combined with filing codes used by the Navy's clerks to create an informal version of the system that was put in place in 1920. Limited usage of ship numbers goes back even earlier, most notably to the "Jeffersonian Gunboats" of the early 1800s and the "Tinclad" river gunboats of the Civil War Mississippi Squadron.

It is important to understand that hull number-letter prefixes are not acronyms, and should not be carelessly treated as abbreviations of ship type classifications. Thus, "DD" does not stand for anything more than "Destroyer". "SS" simply means "Submarine". And "FF" is the post-1975 type code for "Frigate."

The hull classification codes for ships in active duty in the United States Navy are governed under Secretary of the Navy Instruction 5030.8B (SECNAVINST 5030.8B).

Warships are designed to participate in combat operations.

The origin of the two-letter code derives from the need to distinguish various cruiser subtypes.
Aircraft carriers are ships designed primarily for the purpose of conducting combat operations by aircraft which engage in attacks against airborne, surface, sub-surface and shore targets. Contrary to popular belief, the "CV" hull classification symbol does not stand for "carrier vessel". "CV" derives from the cruiser designation, with the v for French "voler", "to fly". Aircraft carriers are designated in two sequences: the first sequence runs from CV-1 USS "Langley" to the very latest ships, and the second sequence, "CVE" for escort carriers, ran from CVE-1 "Long Island" to CVE-127 "Okinawa" before being discontinued.

Surface combatants are ships which are designed primarily to engage enemy forces on the high seas. The primary surface combatants are battleships, cruisers and destroyers. Battleships are very heavily armed and armored; cruisers moderately so; destroyers and smaller warships, less so. Before 1920, ships were called "<type> no. X", with the type fully pronounced. The types were commonly abbreviated in ship lists to "B-X", "C-X", "D-X" et cetera—for example, before 1920, would have been called "USS "Minnesota", Battleship number 22" orally and "USS "Minnesota", B-22" in writing. After 1920, the ship's name would have been both written and pronounced "USS "Minnesota" (BB-22)". In generally decreasing size, the types are:

Submarines are all self-propelled submersible types (usually started with SS) regardless of whether employed as combatant, auxiliary, or research and development vehicles which have at least a residual combat capability. While some classes, including all diesel-electric submarines, are retired from USN service, non-U.S. navies continue to employ SS, SSA, SSAN, SSB, SSC, SSG, SSM, and SST types. With the advent of new Air Independent Propulsion/Power (AIP) systems, both SSI and SSP are used to distinguish the types within the USN, but SSP has been declared the preferred term. SSK, retired by the USN, continues to be used colloquially and interchangeably with SS for diesel-electric attack/patrol submarines within the USN, and, more formally, by the Royal Navy and British firms such as Jane's Information Group.


Patrol combatants are ships whose mission may extend beyond coastal duties and whose characteristics include adequate endurance and seakeeping, providing a capability for operations exceeding 48 hours on the high seas without support. This notably included Brown Water Navy/Riverine Forces during the Vietnam War. Few of these ships are in service today.

Amphibious warfare vessels include all ships having an organic capability for amphibious warfare and which have characteristics enabling long duration operations on the high seas. There are two classifications of craft: amphibious warfare ships which are built to cross oceans, and landing craft, which are designed to take troops from ship to shore in an invasion.

The US Navy hull classification symbol for a ship with a well deck depends on its facilities for aircraft:

Ships

Landing Craft

Operated by Military Sealift Command, have ship prefix "USNS", hull code begins with "T-".

Ships which have the capability to provide underway replenishment to fleet units.

Mine warfare ships are those ships whose primary function is mine warfare on the high seas.

Coastal defense ships are those whose primary function is coastal patrol and interdiction.

Mobile logistics ships have the capability to provide direct material support to other deployed units operating far from home ports.

An auxiliary ship is designed to operate in any number of roles supporting combatant ships and other naval operations.

Although technically an aircraft, pre-World War II rigid airships (e.g., zeppelins) were treated like commissioned surface warships and submarines, flew the U.S. ensign from their stern and carried a United States Ship (USS) designation. Non-rigid airships (e.g., blimps) continued to fly the U.S. ensign from their stern but were always considered to be primarily aircraft.

Support ships are not designed to participate in combat and are generally not armed. For ships with civilian crews (owned by and/or operated for Military Sealift Command and the Maritime Administration), the prefix T- is placed at the front of the hull classification.

Support ships are designed to operate in the open ocean in a variety of sea states to provide general support to either combatant forces or shore-based establishments. They include smaller auxiliaries which, by the nature of their duties, leave inshore waters.

Service craft are navy-subordinated craft (including non-self-propelled) designed to provide general support to either combatant forces or shore-based establishments. The suffix "N" refers to non-self-propelled variants.

Prior to 1965, U.S. Coast Guard cutters used the same designation as naval ships but preceded by a "W" to indicate Coast Guard commission. The U.S. Coast Guard considers any ship over 65 feet in length with a permanently assigned crew, a cutter.



United States Navy Designations (Temporary) are a form of U.S. Navy ship designation, intended for temporary identification use. Such designations usually occur during periods of sudden mobilization, such as that which occurred prior to, and during, World War II or the Korean War, when it was determined that a sudden temporary need arose for a ship for which there was no official Navy designation.

During World War II, for example, a number of commercial vessels were requisitioned, or acquired, by the U.S. Navy to meet the sudden requirements of war. A yacht acquired by the U.S. Navy during the start of World War II might seem desirable to the Navy whose use for the vessel might not be fully developed or explored at the time of acquisition.

On the other hand, a U.S. Navy vessel, such as the yacht in the example above, already in commission or service, might be desired, or found useful, for another need or purpose for which there is no official designation.

Numerous other U.S. Navy vessels were launched with a temporary, or nominal, designation, such as YMS or PC, since it could not be determined, at the time of construction, what they should be used for. Many of these were vessels in the 150 to 200 feet length class with powerful engines, whose function could be that of a minesweeper, patrol craft, submarine chaser, seaplane tender, tugboat, or other. Once their destiny, or capability, was found or determined, such vessels were reclassified with their actual designation.


The letter is paired with a three-digit number. The first digit of the number is determined by the ships "power tonnage," defined as the sum of its shaft horsepower and gross international tonnage, as follows:

The second and third digits are assigned to create a unique three-digit hull number.





</doc>
<doc id="14091" url="https://en.wikipedia.org/wiki?curid=14091" title="Habeas corpus">
Habeas corpus

Habeas corpus (; Medieval Latin meaning "[we, a Court, command] that you have the body [of the detainee brought before us]") is a recourse in law through which a person can report an unlawful detention or imprisonment to a court and request that the court order the custodian of the person, usually a prison official, to bring the prisoner to court, to determine whether the detention is lawful.

The writ of "habeas corpus" is known as the "great and efficacious writ in all manner of illegal confinement". It is a summons with the force of a court order; it is addressed to the custodian (a prison official, for example) and demands that a prisoner be brought before the court, and that the custodian present proof of authority, allowing the court to determine whether the custodian has lawful authority to detain the prisoner. If the custodian is acting beyond their authority, then the prisoner must be released. Any prisoner, or another person acting on their behalf, may petition the court, or a judge, for a writ of "habeas corpus". One reason for the writ to be sought by a person other than the prisoner is that the detainee might be held incommunicado. Most civil law jurisdictions provide a similar remedy for those unlawfully detained, but this is not always called "habeas corpus". For example, in some Spanish-speaking nations, the equivalent remedy for unlawful imprisonment is the "amparo de libertad" ("protection of freedom").

"Habeas corpus" has certain limitations. Though a writ of right, it is not a writ of course. It is technically only a procedural remedy; it is a guarantee against any detention that is forbidden by law, but it does not necessarily protect other rights, such as the entitlement to a fair trial. So if an imposition such as internment without trial is permitted by the law, then "habeas corpus" may not be a useful remedy. In some countries, the writ has been temporarily or permanently suspended under the pretext of a war or state of emergency, for example by Abraham Lincoln.

The right to petition for a writ of "habeas corpus" has nonetheless long been celebrated as the most efficient safeguard of the liberty of the subject. The jurist Albert Venn Dicey wrote that the British Habeas Corpus Acts "declare no principle and define no rights, but they are for practical purposes worth a hundred constitutional articles guaranteeing individual liberty".

The writ of "habeas corpus" is one of what are called the "extraordinary", "common law", or "prerogative writs", which were historically issued by the English courts in the name of the monarch to control inferior courts and public authorities within the kingdom. The most common of the other such prerogative writs are "quo warranto", "prohibito", "mandamus", "procedendo", and "certiorari". The due process for such petitions is not simply civil or criminal, because they incorporate the presumption of non-authority. The official who is the respondent must prove their authority to do or not do something. Failing this, the court must decide for the petitioner, who may be any person, not just an interested party. This differs from a motion in a civil process in which the movant must have standing, and bears the burden of proof.

The phrase is from the Latin "habeās", 2nd person singular present subjunctive active of "habēre", "to have", "to hold"; and "corpus", accusative singular of "corpus", "body". In reference to more than one person, the phrase is "habeas corpora".

Literally, the phrase means "[we command] that you should have the [detainee's] body [brought to court]". The complete phrase "habeas corpus [coram nobis] ad subjiciendum" means "that you have the person [before us] for the purpose of subjecting (the case to examination)". These are words of writs included in a 14th-century Anglo-French document requiring a person to be brought before a court or judge, especially to determine if that person is being legally detained.


The full name of the writ is often used to distinguish it from similar ancient writs, also named "habeas corpus". These include:

"Habeas corpus" originally stems from the Assize of Clarendon, a re-issuance of rights during the reign of Henry II of England in the 12th century. The foundations for "habeas corpus" are "wrongly thought" to have originated in Magna Carta. This charter declared that:
However the preceding article of Magna Carta, nr 38, declares:

Pursuant to that language, a person may not be subjected to any legal proceeding, such as arrest and imprisonment, without sufficient evidence having already been collected to show that there is a "prima facie" case to answer. This evidence must be collected beforehand, because it must be available to be exhibited in a public hearing within hours, or at the most days, after arrest, not months or longer as may happen in other jurisdictions that apply Napoleonic-inquisitorial criminal laws where evidence is commonly sought after a suspect's incarceration. Any charge leveled at the hearing thus must be based on evidence already collected, and an arrest and incarceration order is not lawful if not supported by sufficient evidence.

In contrast with the common law approach, consider the case of "Luciano Ferrari-Bravo v. Italy" the European Court of Human Rights ruled that "detention is intended to facilitate … the preliminary investigation". Ferrari-Bravo sought relief after nearly five years of preventive detention, and his application was rejected. The European Court of Human Rights deemed the five year detention to be "reasonable" under Article 6 of the European Convention on Human Rights, which provides that a prisoner has a right to a public hearing before an impartial tribunal within a "reasonable" time after arrest. After his eventual trial, the evidence against Ferrari-Bravo was deemed insufficient and he was found not guilty.

William Blackstone cites the first recorded usage of "habeas corpus ad subjiciendum" in 1305, during the reign of King Edward I. However, other writs were issued with the same effect as early as the reign of Henry II in the 12th century. Blackstone explained the basis of the writ, saying "[t]he king is at all times entitled to have an account, why the liberty of any of his subjects is restrained, wherever that restraint may be inflicted." The procedure for issuing a writ of "habeas corpus" was first codified by the Habeas Corpus Act 1679, following judicial rulings which had restricted the effectiveness of the writ. A previous law (the Habeas Corpus Act 1640) had been passed forty years earlier to overturn a ruling that the command of the King was a sufficient answer to a petition of "habeas corpus". The cornerstone purpose of the "writ of habeas corpus" was to limit the King's Chancery's ability to undermine the surety of law by allowing courts of justice decisions to be overturned in favor and application of "equity", a process managed by the Chancellor (a bishop) with the King's authority.

The 1679 codification of "habeas corpus" took place in the context of a sharp confrontation between King Charles II and the Parliament, which was dominated by the then sharply oppositional, nascent Whig Party. The Whig leaders had good reasons to fear the King moving against them through the courts (as indeed happened in 1681) and regarded "habeas corpus" as safeguarding their own persons. The short-lived Parliament which made this enactment came to be known as the "Habeas Corpus Parliament" – being dissolved by the King immediately afterwards.

Then, as now, the writ of "habeas corpus" was issued by a superior court in the name of the Sovereign, and commanded the addressee (a lower court, sheriff, or private subject) to produce the prisoner before the royal courts of law. A "habeas corpus" petition could be made by the prisoner him or herself or by a third party on his or her behalf and, as a result of the Habeas Corpus Acts, could be made regardless of whether the court was in session, by presenting the petition to a judge. Since the 18th century the writ has also been used in cases of unlawful detention by private individuals, most famously in "Somersett's Case" (1772), where the black slave, Somersett, was ordered to be freed. During that case, these famous words are said to have been uttered: "... that the air of England was too pure for slavery." (although it was the lawyers in argument who expressly used this phrase – referenced from a much earlier argument heard in The Star Chamber – and not Lord Mansfield himself). During the Seven Years' War and later conflicts, the Writ was used on behalf of soldiers and sailors pressed into military and naval service. The Habeas Corpus Act 1816 introduced some changes and expanded the territoriality of the legislation.

The privilege of "habeas corpus" has been suspended or restricted several times during English history, most recently during the 18th and 19th centuries. Although internment without trial has been authorised by statute since that time, for example during the two World Wars and the Troubles in Northern Ireland, the "habeas corpus" procedure has in modern times always technically remained available to such internees. However, as "habeas corpus" is only a procedural device to examine the lawfulness of a prisoner's detention, so long as the detention is in accordance with an Act of Parliament, the petition for "habeas corpus" is unsuccessful. Since the passage of the Human Rights Act 1998, the courts have been able to declare an Act of Parliament to be incompatible with the European Convention on Human Rights, but such a declaration of incompatibility has no legal effect unless and until it is acted upon by the government.

The wording of the writ of "habeas corpus" implies that the prisoner is brought to the court for the legality of the imprisonment to be examined. However, rather than issuing the writ immediately and waiting for the return of the writ by the custodian, modern practice in England is for the original application to be followed by a hearing with both parties present to decide the legality of the detention, without any writ being issued. If the detention is held to be unlawful, the prisoner can usually then be released or bailed by order of the court without having to be produced before it. With the development of modern public law, applications for habeas corpus have been to some extent discouraged, in favour of applications for judicial review. The writ, however, maintains its vigour, and was held by the UK Supreme Court to be available in respect of a prisoner captured by British forces in Afghanistan, albeit that the Secretary of State made a valid return to the writ justifying the detention of the claimant.

The writ of "habeas corpus" as a procedural remedy is part of Australia's English law inheritance. In 2005, the Australian parliament passed the Australian Anti-Terrorism Act 2005. Some legal experts questioned the constitutionality of the act, due in part to limitations it placed on "habeas corpus".

"Habeas corpus" rights are part of the British legal tradition inherited by Canada. The rights exist in the common law but have been enshrined in section 10(c) of the "Charter of Rights and Freedoms", which states that "[e]veryone has the right on arrest or detention … to have the validity of the detention determined by way of "habeas corpus" and to be released if the detention is not lawful". The test for "habeas corpus" in Canada was recently laid down by the Supreme Court of Canada in "Mission Institution v Khela", as follows:To be successful, an application for "habeas corpus" must satisfy the following criteria. First, the applicant [i.e., the person seeking "habeas corpus" review] must establish that he or she has been deprived of liberty. Once a deprivation of liberty is proven, the applicant must raise a legitimate ground upon which to question its legality. If the applicant has raised such a ground, the onus shifts to the respondent authorities [i.e., the person or institution detaining the applicant] to show that the deprivation of liberty was lawful.Suspension of the writ in Canadian history occurred famously during the October Crisis, during which the "War Measures Act" was invoked by the Governor General of Canada on the constitutional advice of Prime Minister Pierre Trudeau, who had received a request from the Quebec Cabinet. The Act was also used to justify German, Slavic, and Ukrainian Canadian internment during the First World War, and the internment of German-Canadians, Italian-Canadians and Japanese-Canadians during the Second World War. The writ was suspended for several years following the Battle of Fort Erie (1866) during the Fenian Rising, though the suspension was only ever applied to suspects in the Thomas D'Arcy McGee assassination.

The writ is available where there is no other adequate remedy. However, a superior court always has the discretion to grant the writ even in the face of an alternative remedy (see "May v Ferndale Institution"). Under the "Criminal Code" the writ is largely unavailable if a statutory right of appeal exists, whether or not this right has been exercised.

A fundamental human right in the "1789 Declaration of the Rights of Man" drafted by Lafayette in cooperation with Thomas Jefferson, the guarantees against arbitrary detention are enshrined in the French Constitution and regulated by the Penal Code. The safeguards are equivalent to those found under the Habeas-Corpus provisions found in Germany, the United States and several Commonwealth countries. The French system of accountability prescribes severe penalties for ministers, police officers and civil and judiciary authorities who either violate or fail to enforce the law.

"Article 7 of [1789] Declaration also provides that 'No individual may be accused, arrested, or detained except where the law so prescribes, and in accordance with the procedure it has laid down.' ... The Constitution further states that 'No one may be arbitrarily detained. The judicial authority, guardian of individual liberty, ensures the observance of this principle under the condition specified by law.' Its article 5 provides that everyone has the right to liberty and sets forth permissible circumstances under which people may be deprived of their liberty and procedural safeguards in case of detention. In particular, it states that 'anyone deprived of his liberty by arrest or detention shall be entitled to take proceedings by which the lawfulness of his detention shall be decided speedily by a court and his release ordered if the detention is not lawful'."

France and the United States played a synergistic role in the international team, led by Eleanor Roosevelt, which crafted the Universal Declaration of Human Rights. The French judge and Nobel Peace Laureate René Cassin produced the first draft and argued against arbitrary detentions. René Cassin and the French team subsequently championed the "habeas corpus" provisions enshrined in the European Convention for the Protection of Human Rights and Fundamental Freedoms.

Germany has constitutional guarantees against improper detention and these have been implemented in statutory law in a manner that can be considered as equivalent to writs of habeas corpus.

Article 104, paragraph 1 of the Basic Law for the Federal Republic of Germany provides that deprivations of liberty may be imposed only on the basis of a specific enabling statute that also must include procedural rules. Article 104, paragraph 2 requires that any arrested individual be brought before a judge by the end of the day following the day of the arrest. For those detained as criminal suspects, article 104, paragraph 3 specifically requires that the judge must grant a hearing to the suspect in order to rule on the detention.

Restrictions on the power of the authorities to arrest and detain individuals also emanate from article 2 paragraph 2 of the Basic Law which guarantees liberty and requires a statutory authorization for any deprivation of liberty. In addition, several other articles of the Basic Law have a bearing on the issue. The most important of these are article 19, which generally requires a statutory basis for any infringements of the fundamental rights guaranteed by the Basic Law while also guaranteeing judicial review; article 20, paragraph 3, which guarantees the rule of law; and article 3 which guarantees equality.

In particular, a constitutional obligation to grant remedies for improper detention is required by article 19, paragraph 4 of the Basic Law, which provides as follows: "Should any person's right be violated by public authority, he may have recourse to the courts. If no other jurisdiction has been established, recourse shall be to the ordinary courts."

The Indian judiciary, in a catena of cases, has effectively resorted to the writ of "habeas corpus" to secure release of a person from illegal detention. For example, in October 2009, the Karnataka High Court heard a "habeas corpus" petition filed by the parents of a girl who married a Muslim boy from Kannur district and was allegedly confined in a "madrasa" in Malapuram town. Usually, in most other jurisdictions, the writ is directed at police authorities. The extension to non-state authorities has its grounds in two cases: the 1898 Queen's Bench case of "Ex Parte Daisy Hopkins", wherein the Proctor of Cambridge University did detain and arrest Hopkins without his jurisdiction, and Hopkins was released, and that of "Somerset v Stewart", in which an African slave whose master had moved to London was freed by action of the writ.

The Indian judiciary has dispensed with the traditional doctrine of "locus standi", so that if a detained person is not in a position to file a petition, it can be moved on his behalf by any other person. The scope of "habeas" relief has expanded in recent times by actions of the Indian judiciary.

In 1976, the "habeas" writ was used in the Rajan case, a student victim of torture in local police custody during the nationwide Emergency in India. On 12 March 2014, Subrata Roy's counsel approached the Chief Justice moving a "habeas corpus" petition. It was also filed by the Panthers Party to protest the imprisonment of Anna Hazare, a social activist.

In the Republic of Ireland, the writ of "habeas corpus" is available at common law and under the Habeas Corpus Acts of 1782 and 1816. A remedy equivalent to "habeas corpus" is also guaranteed by Article 40 of the 1937 constitution.

The article guarantees that "no citizen shall be deprived of his personal liberty save in accordance with law" and outlines a specific procedure for the High Court to enquire into the lawfulness of any person's detention. It does not mention the Latin term, "habeas corpus", but includes the English phrase "produce the body".

Article 40.4.2° provides that a prisoner, or anyone acting on his behalf, may make a complaint to the High Court (or to any High Court judge) of unlawful detention. The court must then investigate the matter "forthwith" and may order that the defendant bring the prisoner before the court and give reasons for his detention. The court must immediately release the detainee unless it is satisfied that he is being held lawfully. The remedy is available not only to prisoners of the state, but also to persons unlawfully detained by any private party. However the constitution provides that the procedure is not binding on the Defence Forces during a state of war or armed rebellion.

The full text of Article 40.4.2° is as follows: 

The writ of "habeas corpus" continued as part of the Irish law when the state seceded from the United Kingdom in 1922. A remedy equivalent to "habeas corpus" was also guaranteed by Article 6 of the Constitution of the Irish Free State, enacted in 1922. That article used similar wording to Article 40.4 of the current constitution, which replaced it 1937.

The relationship between the Article 40 and the Habeas Corpus Acts of 1782 and 1816 is ambiguous, and Forde and Leonard write that "The extent if any to which Article 40.4 has replaced these Acts has yet to be determined". In "The State (Ahern) v. Cotter" (1982) Walsh J. opined that the ancient writ referred to in the Habeas Corpus Acts remains in existence in Irish law as a separate remedy from that provided for in Article 40.

In 1941, the Article 40 procedure was restricted by the Second Amendment. Prior to the amendment, a prisoner had the constitutional right to apply to any High Court judge for an enquiry into her detention, and to as many High Court judges as she wished. If the prisoner successfully challenged her detention before the High Court she was entitled to immediate, unconditional release.

The Second Amendment provided that a prisoner has only the right to apply to a single judge, and, once a writ has been issued, the President of the High Court has authority to choose the judge or panel of three judges who will decide the case. If the High Court finds that the prisoner's detention is unlawful due to the unconstitutionality of a law the judge must refer the matter to the Supreme Court, and until the Supreme's Court's decision is rendered the prisoner may be released only on bail.

The power of the state to detain persons prior to trial was extended by the Sixteenth Amendment, in 1996. In 1965, the Supreme Court ruled in the "O'Callaghan" case that the constitution required that an individual charged with a crime could be refused bail only if she was likely to flee or to interfere with witnesses or evidence. Since the Sixteenth Amendment, it has been possible for a court to take into account whether a person has committed serious crimes while on bail in the past.

The right to freedom from arbitrary detention is guaranteed by Article 13 of the Constitution of Italy, which states: 

This implies that within 48 hours every arrest made by a police force must be validated by a court.

Furthermore, if subject to a valid detention, an arrested can ask for a review of the detention to another court, called the Review Court ("Tribunale del Riesame", also known as the Freedom Court, "Tribunale della Libertà").

In Malaysia, the remedy of "habeas corpus" is guaranteed by the federal constitution, although not by name. Article 5(2) of the Constitution of Malaysia provides that "Where complaint is made to a High Court or any judge thereof that a person is being unlawfully detained the court shall inquire into the complaint and, unless satisfied that the detention is lawful, shall order him to be produced before the court and release him".

As there are several statutes, for example, the Internal Security Act 1960, that still permit detention without trial, the procedure is usually effective in such cases only if it can be shown that there was a procedural error in the way that the detention was ordered.

In New Zealand, "habeas corpus" may be invoked against the government or private individuals. In 2006, a child was allegedly kidnapped by his maternal grandfather after a custody dispute. The father began "habeas corpus" proceedings against the mother, the grandfather, the grandmother, the great grandmother, and another person alleged to have assisted in the kidnap of the child. The mother did not present the child to the court and so was imprisoned for contempt of court. She was released when the grandfather came forward with the child in late January 2007.

Issuance of a writ is an exercise of an extraordinary jurisdiction of the superior courts in Pakistan. A writ of habeas corpus may be issued by any High Court of a province in Pakistan. Article 199 of the 1973 Constitution of the Islamic Republic of Pakistan, specifically provides for the issuance of a writ of habeas corpus, empowering the courts to exercise this prerogative. Subject to the Article 199 of the Constitution, "A High Court may, if it is satisfied that no other adequate remedy is provided by law, on the application of any person, make an order that a person in custody within the territorial jurisdiction of the Court be brought before it so that the Court may satisfy itself that he is not being held in custody without a lawful authority or in an unlawful manner". The hallmark of extraordinary constitutional jurisdiction is to keep various functionaries of State within the ambit of their authority. Once a High Court has assumed jurisdiction to adjudicate the matter before it, justiciability of the issue raised before it is beyond question. The Supreme Court of Pakistan has stated clearly that the use of words "in an unlawful manner" implies that the court may examine, if a statute has allowed such detention, whether it was a colorable exercise of the power of authority. Thus, the court can examine the malafides of the action taken.

In the Bill of Rights of the Philippine constitution, "habeas corpus" is guaranteed in terms almost identically to those used in the U.S. Constitution. Article 3, Section 15 of the Constitution of the Philippines states that "The privilege of the writ of "habeas corpus" shall not be suspended except in cases of invasion or rebellion when the public safety requires it".

In 1971, after the Plaza Miranda bombing, the Marcos administration, under Ferdinand Marcos, suspended "habeas corpus" in an effort to stifle the oncoming insurgency, having blamed the Filipino Communist Party for the events of August 21. Many considered this to be a prelude to martial law. After widespread protests, however, the Arroyo administration decided to reintroduce the writ. In December 2009, "habeas corpus" was suspended in Maguindanao as the province was placed under martial law. This occurred in response to the Maguindanao massacre.

In 2016, President Rodrigo Duterte said he was planning on suspending the habeas corpus.

At 10 pm on 23 May 2017 Philippine time, President Rodrigo Duterte declared martial law in the whole island of Mindanao including Sulu and Tawi-tawi for the period of 60 days due to the series of attacks mounted by the Maute group, an ISIS-linked terrorist organization. The declaration suspends the writ.

The Parliament of Scotland passed a law to have the same effect as "habeas corpus" in the 18th century. This is now known as the Criminal Procedure Act 1701 c.6. It was originally called "the Act for preventing wrongful imprisonment and against undue delays in trials". It is still in force although certain parts have been repealed.

The present Constitution of Spain states that "A "habeas corpus" procedure shall be provided for by law to ensure the immediate handing over to the judicial authorities of any person illegally arrested". The statute which regulates the procedure is the "Law of Habeas Corpus of 24 May 1984", which provides that a person imprisoned may, on her or his own or through a third person, allege that she or he is imprisoned unlawfully and request to appear before a judge. The request must specify the grounds on which the detention is considered to be unlawful, which can be, for example, that the custodian holding the prisoner does not have the legal authority, that the prisoner's constitutional rights have been violated, or that he has been subjected to mistreatment. The judge may then request additional information if needed, and may issue a "habeas corpus" order, at which point the custodian has 24 hours to bring the prisoner before the judge.

Historically, many of the territories of Spain had remedies equivalent to the "habeas corpus", such as the privilege of "manifestación" in the Crown of Aragon or the right of the Tree in Biscay.

The United States inherited "habeas corpus" from the English common law. In England, the writ was issued in the name of the monarch. When the original thirteen American colonies declared independence, and became a republic based on popular sovereignty, any person, in the name of the people, acquired authority to initiate such writs. The U.S. Constitution specifically includes the "habeas" procedure in the Suspension Clause (Clause 2), located in Article One, Section 9. This states that "The privilege of the writ of "habeas corpus" shall not be suspended, unless when in cases of rebellion or invasion the public safety may require it".

The writ of "habeas corpus ad subjiciendum" is a civil, not criminal, "ex parte" proceeding in which a court inquires as to the legitimacy of a prisoner's custody. Typically, "habeas corpus" proceedings are to determine whether the court that imposed sentence on the defendant had jurisdiction and authority to do so, or whether the defendant's sentence has expired. "Habeas corpus" is also used as a legal avenue to challenge other types of custody such as pretrial detention or detention by the United States Bureau of Immigration and Customs Enforcement pursuant to a deportation proceeding.

Presidents Abraham Lincoln and Ulysses Grant suspended "habeas corpus" during the Civil War and Reconstruction for some places or types of cases. During World War II, President Franklin D. Roosevelt suspended habeas corpus. Following the September 11 attacks, President George W. Bush attempted to place Guantanamo Bay detainees outside of the jurisdiction of "habeas corpus", but the Supreme Court of the United States overturned this action in "Boumediene v. Bush".

In 1526, the "Fuero Nuevo of the Señorío de Vizcaya" ("New Charter of the Lordship of Biscay") established a form of "habeas corpus" in the territory of the "Señorío de Vizcaya", nowadays part of Spain. This revised version of the "Fuero Viejo" (Old Charter) of 1451 codified the medieval custom whereby no person could be arbitrarily detained without being summoned first to the Oak of Gernika, an ancestral oak tree located in the outskirts of Gernika under which all laws of the Lordship of Biscay were passed.

The New Charter formalised that no one could be detained without a court order (Law 26 of Chapter 9) nor due to debts (Law 3 of Chapter 16). It also established due process and a form of habeas corpus: no one could be arrested without previously having been summoned to the Oak of Gernika and given 30 days to answer the said summons. Upon appearing under the Tree, they had to be provided with accusations and all evidence held against them so that they could defend themselves (Law 7 of Chapter 9). No one could be sent to prison or deprived of their freedom until being formally trialed, and no one could be accused of a different crime until their current court trial was over (Law 5 of Chapter 5). Those fearing they were being arrested illegally could appeal to the "Regimiento General" that their rights could be upheld. The "Regimiento" (the executive arm of the Juntas Generales of Biscay) would demand the prisoner be handed over to them, and thereafter the prisoner would be released and placed under the protection of the Regimiento while awaiting for trial.

The Crown of Aragon also had a remedy equivalent to the "habeas corpus" called the "manifestación de personas" (literally, "demonstration of persons"). According to the right of "manifestación", the Justicia de Aragon (lit. "Justice of Aragon", an Aragonese judiciary figure similar to an ombudsman, but with far reaching executive powers) could require a judge, a court of justice, or any other official that they handed over to the "Justicia" (i.e., that they be "demonstrated" to the Justicia) anyone being prosecuted so as to guarantee that this person's rights were upheld, and that no violence would befall this person prior to their being sentenced. Furthermore, the "Justicia" retained the right to examine the judgement passed, and decide whether it satisfied the conditions of a fair trial. If the "Justicia" was not satisfied, he could refuse to hand over the accused back to the authorities. The right of "manifestación" acted like an habeas corpus: knowing that the appeal to the "Justicia" would immediately follow any unlawful detention, these were effectively illegal. Equally, torture (which had been banned since 1325 in Aragon) would never take place. In some cases, people exerting their right of "manifestación" were kept under the Justicia's watch in "manifestación" prisons (famous for their mild and easy conditions) or under house arrest. More generally however, the person was released from confinement and placed under the "Justicia's protection", awaiting for trial. The "Justicia" always granted the right of "manifestación" by default, but they only really had to act in extreme cases, as for instance famously happened in 1590 when Antonio Pérez, the disgraced secretary to Philip II of Spain, fled from Castile to Aragon and used his Aragonese ascendency to appeal to the "Justicia" for manifestación right, thereby preventing his arrest at the King's behest.

The right of "manifestación" was codified in 1325 in the Declaratio Privilegii generalis passed by the Aragonese Corts under king James II of Aragon. It had been practised since the inception of the kingdom of Aragon in the 11th century, and therefore predates the English "habeas corpus" itself.

In 1430, King Władysław II Jagiełło of Poland granted the Privilege of Jedlnia, which proclaimed, "Neminem captivabimus nisi iure victum" ("We will not imprison anyone except if convicted by law"). This revolutionary innovation in civil libertarianism gave Polish citizens due process-style rights that did not exist in any other European country for another 250 years. Originally, the Privilege of Jedlnia was restricted to the nobility (the szlachta), but it was extended to cover townsmen in the 1791 Constitution. Importantly, social classifications in the Polish–Lithuanian Commonwealth were not as rigid as in other European countries; townspeople and Jews were sometimes ennobled. The Privilege of Jedlnia provided broader coverage than many subsequently enacted habeas corpus laws, because Poland's nobility constituted an unusually large percentage of the country's total population, which was Europe's largest. As a result, by the 16th century, it was protecting the liberty of between five hundred thousand and a million Poles.

In South Africa and other countries whose legal systems are based on Roman-Dutch law, the "interdictum de homine libero exhibendo" is the equivalent of the writ of "habeas corpus". In South Africa, it has been entrenched in the Bill of Rights, which provides in section 35(2)(d) that every detained person has the right to challenge the lawfulness of the detention in person before a court and, if the detention is unlawful, to be released.

In the 1950s, American lawyer Luis Kutner began advocating an international writ of "habeas corpus" to protect individual human rights. In 1952, he filed a petition for a "United Nations Writ of Habeas Corpus" on behalf of William N. Oatis, an American journalist jailed the previous year by the Communist government of Czechoslovakia. Alleging that Czechoslovakia had violated Oatis's rights under the United Nations Charter and the Universal Declaration of Human Rights and that the United Nations General Assembly had "inherent power" to fashion remedies for human rights violations, the petition was filed with the United Nations Commission on Human Rights. The Commission forwarded the petition to Czechoslovakia, but no other United Nations action was taken. Oatis was released in 1953. Kutner went on to publish numerous articles and books advocating the creation of an "International Court of Habeas Corpus".

Article 3 of the Universal Declaration of Human Rights provides that "everyone has the right to life, liberty and security of person". Article 5 of the European Convention on Human Rights goes further and calls for persons detained to have the right to challenge their detention, providing at article 5.4: 





</doc>
<doc id="14092" url="https://en.wikipedia.org/wiki?curid=14092" title="Prince Henry the Navigator">
Prince Henry the Navigator

Infante Dom Henrique of Portugal, Duke of Viseu (4 March 1394 – 13 November 1460), better known as Prince Henry the Navigator (), was a central figure in the early days of the Portuguese Empire and in the 15th-century European maritime discoveries and maritime expansion. Through his administrative direction, he is regarded as the main initiator of what would be known as the Age of Discovery. Henry was the fourth child of the Portuguese king John I, who founded the House of Aviz.

Henry was responsible for the early development of Portuguese exploration and maritime trade with other continents through the systematic exploration of Western Africa, the islands of the Atlantic Ocean, and the search for new routes. He encouraged his father to conquer Ceuta (1415), the Muslim port on the North African coast across the Straits of Gibraltar from the Iberian Peninsula. He learned of the opportunities offered by the Saharan trade routes that terminated there, and became fascinated with Africa in general; he was most intrigued by the Christian legend of Prester John and the expansion of Portuguese trade. He is regarded as the patron of Portuguese exploration.

Henry was the third surviving son of King John I and his wife Philippa, sister of King Henry IV of England. He was baptized in Porto, and may have been born there, probably when the royal couple was living in the city's old mint, now called Casa do Infante (Prince's House), or in the region nearby. Another possibility is that he was born at the Monastery of Leça do Bailio, in Leça de Palmeira, during the same period of the royal couple's residence in the city of Porto.

Henry was 21 when he and his father and brothers captured the Moorish port of Ceuta in northern Morocco. Ceuta had long been a base for Barbary pirates who raided the Portuguese coast, depopulating villages by capturing their inhabitants to be sold in the African slave trade. Following this success, Henry began to explore the coast of Africa, most of which was unknown to Europeans. His objectives included finding the source of the West African gold trade and the legendary Christian kingdom of Prester John, and stopping the pirate attacks on the Portuguese coast.

At that time, the ships of the Mediterranean were too slow and too heavy to make these voyages. Under his direction, a new and much lighter ship was developed, the caravel, which could sail further and faster, and, above all, was highly maneuverable and could sail much nearer the wind, or "into the wind". This made the caravel largely independent of the prevailing winds.
With the caravel, Portuguese mariners explored rivers and shallow waters as well as the open ocean with wide autonomy. In fact, the invention of the caravel was what made Portugal poised to take the lead in transoceanic exploration.

In 1419, Henry's father appointed him governor of the province of the Algarve.

On 25 May 1420, Henry gained appointment as the Grand Master of the Military Order of Christ, the Portuguese successor to the Knights Templar, which had its headquarters at Tomar, in central Portugal. Henry held this position for the remainder of his life, and the Order was an important source of funds for Henry's ambitious plans, especially his persistent attempts to conquer the Canary Islands, which the Portuguese had claimed to have discovered before the year 1346.

In 1425, his second brother the Infante Peter, Duke of Coimbra, made a tour of Europe. While largely a diplomatic mission, among his goals was to seek out geographic material for his brother Henry. Peter returned from Venice with a current world map drafted by a Venetian cartographer.

In 1431, he donated houses for the "Estudo Geral" to reunite all the sciences—grammar, logic, rhetoric, arithmetic, music, and astronomy—into what would later become the University of Lisbon. For other subjects like medicine or philosophy, he ordered that each room should be decorated according to each subject that was being taught.

Henry also had other resources. When John I died in 1433, Henry's eldest brother Edward of Portugal became king. He granted Henry all profits from trading within the areas he discovered as well as the sole right to authorize expeditions beyond Cape Bojador. Henry also held a monopoly on tuna fishing in the Algarve. When Edward died eight years later, Henry supported his brother Peter, Duke of Coimbra for the regency during the minority of Edward's son Afonso V, and in return received a confirmation of this levy.

Henry functioned as a primary organizer of the disastrous expedition to Tangier in 1437. Henry's younger brother Ferdinand was given as a hostage to guarantee that the Portuguese would fulfill the terms of the peace agreement that had been made with Çala Ben Çala. The Portuguese Cortes refused to approve the return of Ceuta in exchange for the Infante Ferdinand who remained in captivity until his death six years later.

Prince Regent Peter had an important role and responsibility in the Portuguese maritime expansion in the Atlantic Ocean and Africa during his administration. Henry promoted the colonization of the Azores during Peter's regency (1439–1448).

For most of the latter part of his life, Henry concentrated on his maritime activities, or on Portuguese court politics.

According to João de Barros, in the Algarve he repopulated a village that he called Terçanabal (from "terça nabal" or "tercena nabal"). This village was situated in a strategic position for his maritime enterprises and was later called Vila do Infante ("Estate or Town of the Prince").

It is traditionally suggested that Henry gathered at his villa on the Sagres peninsula a school of navigators and map-makers. However modern historians hold this to be a misconception. He did employ some cartographers to chart the coast of Mauritania after the voyages he sent there, but there was no center of navigation science or observatory in the modern sense of the word, nor was there an organized navigational center.

Referring to Sagres, sixteenth-century Portuguese mathematician and cosmographer Pedro Nunes remarked, "from it our sailors went out well taught and provided with instruments and rules which all map makers and navigators should know."

The view that Henry's court rapidly grew into the technological base for exploration, with a naval arsenal and an observatory, etc., although repeated in popular culture, has never been established. Henry did possess geographical curiosity, and employed cartographers. Jehuda Cresques, a noted cartographer, has been said to have accepted an invitation to come to Portugal to make maps for the infante. Prestage makes the argument that the presence of the latter at the Prince's court "probably accounts for the legend of the School of Sagres, which is now discredited."

The first contacts with the African slave market were made by expeditions to ransom Portuguese subjects enslaved by pirate attacks on Portuguese ships or villages. As Sir Peter Russell remarks in his biography, "In Henryspeak, conversion and enslavement were interchangeable terms."

Henry sponsored voyages, collecting a 20% tax ("o quinto") on the profits made by naval expeditions, which was the usual practice in the Iberian states of that time. The nearby port of Lagos provided a convenient harbor from which these expeditions left. The voyages were made in very small ships, mostly the caravel, a light and maneuverable vessel. The caravel used the lateen sail, the prevailing rig in Christian Mediterranean navigation since late antiquity. Most of the voyages sent out by Henry consisted of one or two ships that navigated by following the coast, stopping at night to tie up along some shore.

During Prince Henry's time and after, the Portuguese navigators discovered and perfected the North Atlantic "Volta do Mar" (the "turn of the sea" or "return from the sea"): the dependable pattern of trade winds blowing largely from the east near the equator and the returning westerlies in the mid-Atlantic. This was a major step in the history of navigation, when an understanding of oceanic wind patterns was crucial to Atlantic navigation, from Africa and the open ocean to Europe, and enabled the main route between the New World and Europe in the North Atlantic in future voyages of discovery. Although the lateen sail allowed sailing upwind to some extent, it was worth even major extensions of course to have a faster and calmer following wind for most of a journey. Portuguese mariners who sailed south and southwest towards the Canary Islands and West Africa would afterwards sail far to the northwest—that is, away from continental Portugal, and seemingly in the wrong direction—before turning northeast near the Azores islands and finally east to Europe in order to have largely following winds for their full journey. Christopher Columbus used this on his transatlantic voyages.

The first explorations followed not long after the capture of Ceuta in 1415. Henry was interested in locating the source of the caravans that brought gold to the city. During the reign of his father, John I, João Gonçalves Zarco and Tristão Vaz Teixeira were sent to explore along the African coast. Zarco, a knight in service to Prince Henry, had commanded the caravels guarding the coast of Algarve from the incursions of the Moors. He had also been at Ceuta.

In 1418, Zarco and Teixeira were blown off-course by a storm while making the "volta do mar" westward swing to return to Portugal. They found shelter at an island they named Porto Santo. Henry directed that Porto Santo be colonized. The move to claim the Madeiran islands was probably a response to Castile's efforts to claim the Canary Islands. In 1420, settlers then moved to the nearby island of Madeira.

A chart drawn by the Catalan cartographer, Gabriel de Vallseca of Mallorca, has been interpreted to indicate that the Azores were first discovered by Diogo de Silves in 1427. In 1431, Gonçalo Velho was dispatched with orders to determine the location of "islands" first identified by de Silves. Velho apparently got as far as the Formigas, in the eastern archipelago, before having to return to Sagres, probably due to bad weather.

By this time the Portuguese navigators had also reached the Sargasso Sea (western North Atlantic region), naming it after the "Sargassum" seaweed growing there ("sargaço" / "sargasso" in Portuguese).

Until Henry's time, Cape Bojador remained the most southerly point known to Europeans on the desert coast of Africa. Superstitious seafarers held that beyond the cape lay sea monsters and the edge of the world. In 1434, Gil Eanes, the commander of one of Henry's expeditions, became the first European known to pass Cape Bojador.

Using the new ship type, the expeditions then pushed onwards. Nuno Tristão and Antão Gonçalves reached Cape Blanco in 1441. The Portuguese sighted the Bay of Arguin in 1443 and built an important fort there around the year 1448. Dinis Dias soon came across the Senegal River and rounded the peninsula of Cap-Vert in 1444. By this stage the explorers had passed the southern boundary of the desert, and from then on Henry had one of his wishes fulfilled: the Portuguese had circumvented the Muslim land-based trade routes across the western Sahara Desert, and slaves and gold began arriving in Portugal. This rerouting of trade devastated Algiers and Tunis, but made Portugal rich. By 1452, the influx of gold permitted the minting of Portugal's first gold "cruzado" coins. A cruzado was equal to 400 reis at the time. From 1444 to 1446, as many as forty vessels sailed from Lagos on Henry's behalf, and the first private mercantile expeditions began.

Alvise Cadamosto explored the Atlantic coast of Africa and discovered several islands of the Cape Verde archipelago between 1455 and 1456. In his first voyage, which started on 22 March 1455, he visited the Madeira Islands and the Canary Islands. On the second voyage, in 1456, Cadamosto became the first European to reach the Cape Verde Islands. António Noli later claimed the credit. By 1462, the Portuguese had explored the coast of Africa as far as present-day Sierra Leone. Twenty-eight years later, Bartolomeu Dias proved that Africa could be circumnavigated when he reached the southern tip of the continent, now known as the Cape of Good Hope. In 1498, Vasco da Gama became the first European sailor to reach India by sea.

No one used the nickname "Navigator" to refer to prince Henry during his lifetime or in the following three centuries. The term was coined by two nineteenth-century German historians: Heinrich Schaefer and Gustave de Veer. Later on it was made popular by two British authors who included it in the titles of their biographies of the prince: Henry Major in 1868 and Raymond Beazley in 1895. In Portuguese, even in modern times, it is uncommon to call him by this epithet; the preferred use is "Infante D. Henrique".





</doc>
<doc id="14094" url="https://en.wikipedia.org/wiki?curid=14094" title="Human cloning">
Human cloning

Human cloning is the creation of a genetically identical copy (or clone) of a human. The term is generally used to refer to artificial human cloning, which is the reproduction of human cells and tissue. It does not refer to the natural conception and delivery of identical twins. The possibility of person cloning has raised controversies. These ethical concerns have prompted several nations to pass laws regarding human cloning and its legality.

Two commonly discussed types of theoretical human cloning are "therapeutic cloning" and "reproductive cloning". Therapeutic cloning would involve cloning cells from a human for use in medicine and transplants, and is an active area of research, but is not in medical practice anywhere in the world, . Two common methods of therapeutic cloning that are being researched are somatic-cell nuclear transfer and, more recently, pluripotent stem cell induction. Reproductive cloning would involve making an entire cloned human, instead of just specific cells or tissues.

Although the possibility of cloning humans had been the subject of speculation for much of the 20th century, scientists and policymakers began to take the prospect seriously in 1969. J. B. S. Haldane was the first to introduce the idea of human cloning, for which he used the terms "clone" and "cloning", which had been used in agriculture since the early 20th century. In his speech on "Biological Possibilities for the Human Species of the Next Ten Thousand Years" at the "Ciba Foundation Symposium on Man and his Future" in 1963, he said:
Nobel Prize-winning geneticist Joshua Lederberg advocated cloning and genetic engineering in an article in "The American Naturalist" in 1966 and again, the following year, in "The Washington Post". He sparked a debate with conservative bioethicist Leon Kass, who wrote at the time that "the programmed reproduction of man will, in fact, dehumanize him." Another Nobel Laureate, James D. Watson, publicized the potential and the perils of cloning in his "Atlantic Monthly" essay, "Moving Toward the Clonal Man", in 1971.

With the cloning of a sheep known as Dolly in 1996 by somatic cell nuclear transfer (SCNT), the idea of human cloning became a hot debate topic. Many nations outlawed it, while a few scientists promised to make a clone within the next few years. The first hybrid human clone was created in November 1998, by Advanced Cell Technology. It was created using SCNT; a nucleus was taken from a man's leg cell and inserted into a cow's egg from which the nucleus had been removed, and the hybrid cell was cultured and developed into an embryo. The embryo was destroyed after 12 days.

In 2004 and 2005, Hwang Woo-suk, a professor at Seoul National University, published two separate articles in the journal " Science" claiming to have successfully harvested pluripotent, embryonic stem cells from a cloned human blastocyst using SCNT techniques. Hwang claimed to have created eleven different patient-specific stem cell lines. This would have been the first major breakthrough in human cloning. However, in 2006 "Science" retracted both of his articles on clear evidence that much of his data from the experiments was fabricated.

In January 2008, Dr. Andrew French and Samuel Wood of the biotechnology company Stemagen announced that they successfully created the first five mature human embryos using SCNT. In this case, each embryo was created by taking a nucleus from a skin cell (donated by Wood and a colleague) and inserting it into a human egg from which the nucleus had been removed. The embryos were developed only to the blastocyst stage, at which point they were studied in processes that destroyed them. Members of the lab said that their next set of experiments would aim to generate embryonic stem cell lines; these are the "holy grail" that would be useful for therapeutic or reproductive cloning.

In 2011, scientists at the New York Stem Cell Foundation announced that they had succeeded in generating embryonic stem cell lines, but their process involved leaving the oocyte's nucleus in place, resulting in triploid cells, which would not be useful for cloning.

In 2013, a group of scientists led by Shoukhrat Mitalipov published the first report of embryonic stem cells created using SCNT. In this experiment, the researchers developed a protocol for using SCNT in human cells, which differs slightly from the one used in other organisms. Four embryonic stem cell lines from human fetal somatic cells were derived from those blastocysts. All four lines were derived using oocytes from the same donor, ensuring that all mitochondrial DNA inherited was identical. A year later, a team led by Robert Lanza at Advanced Cell Technology reported that they had replicated Mitalipov's results and further demonstrated the effectiveness by cloning adult cells using SCNT.

In 2018, the first successful cloning of primates using SCNT was reported with the birth of two live female clones, crab-eating macaques named Zhong Zhong and Hua Hua.

In somatic cell nuclear transfer ("SCNT"), the nucleus of a somatic cell is taken from a donor and transplanted into a host egg cell, which had its own genetic material removed previously, making it an enucleated egg. After the donor somatic cell genetic material is transferred into the host oocyte with a micropipette, the somatic cell genetic material is fused with the egg using an electric current. Once the two cells have fused, the new cell can be permitted to grow in a surrogate or artificially. This is the process that was used to successfully clone Dolly the sheep (see section on History in this article). The technique, now refined, has indicated that it was possible to replicate cells and reestablish pluripotency-"the potential of an embryonic cell to grow into any one of the numerous different types of mature body cells that make up a complete organism"

Creating induced pluripotent stem cells ("iPSCs") is a long and inefficient process. Pluripotency refers to a stem cell that has the potential to differentiate into any of the three germ layers: endoderm (interior stomach lining, gastrointestinal tract, the lungs), mesoderm (muscle, bone, blood, urogenital), or ectoderm (epidermal tissues and nervous tissue). A specific set of genes, often called "reprogramming factors", are introduced into a specific adult cell type. These factors send signals in the mature cell that cause the cell to become a pluripotent stem cell. This process is highly studied and new techniques are being discovered frequently on how to better this induction process.

Depending on the method used, reprogramming of adult cells into iPSCs for implantation could have severe limitations in humans. If a virus is used as a reprogramming factor for the cell, cancer-causing genes called oncogenes may be activated. These cells would appear as rapidly dividing cancer cells that do not respond to the body's natural cell signaling process. However, in 2008 scientists discovered a technique that could remove the presence of these oncogenes after pluripotency induction, thereby increasing the potential use of iPSC in humans.

Both the processes of SCNT and iPSCs have benefits and deficiencies. Historically, reprogramming methods were better studied than SCNT derived embryonic stem cells (ESCs). However, more recent studies have put more emphasis on developing new procedures for SCNT-ESCs. The major advantage of SCNT over iPSCs at this time is the speed with which cells can be produced. iPSCs derivation takes several months while SCNT would take a much shorter time, which could be important for medical applications. New studies are working to improve the process of iPSC in terms of both speed and efficiency with the discovery of new reprogramming factors in oocytes. Another advantage SCNT could have over iPSCs is its potential to treat mitochondrial disease, as it utilizes a donor oocyte. No other advantages are known at this time in using stem cells derived from one method over stem cells derived from the other.

Work on cloning techniques has advanced our basic understanding of developmental biology in humans. Observing human pluripotent stem cells grown in culture provides great insight into human embryo development, which otherwise cannot be seen. Scientists are now able to better define steps of early human development. Studying signal transduction along with genetic manipulation within the early human embryo has the potential to provide answers to many developmental diseases and defects. Many human-specific signaling pathways have been discovered by studying human embryonic stem cells. Studying developmental pathways in humans has given developmental biologists more evidence toward the hypothesis that developmental pathways are conserved throughout species.

iPSCs and cells created by SCNT are useful for research into the causes of disease, and as model systems used in drug discovery.

Cells produced with SCNT, or iPSCs could eventually be used in stem cell therapy, or to create organs to be used in transplantation, known as regenerative medicine. Stem cell therapy is the use of stem cells to treat or prevent a disease or condition. Bone marrow transplantation is a widely used form of stem cell therapy. No other forms of stem cell therapy are in clinical use at this time. Research is underway to potentially use stem cell therapy to treat heart disease, diabetes, and spinal cord injuries. Regenerative medicine is not in clinical practice, but is heavily researched for its potential uses. This type of medicine would allow for autologous transplantation, thus removing the risk of organ transplant rejection by the recipient. For instance, a person with liver disease could potentially have a new liver grown using their same genetic material and transplanted to remove the damaged liver. In current research, human pluripotent stem cells have been promised as a reliable source for generating human neurons, showing the potential for regenerative medicine in brain and neural injuries.

In bioethics, the ethics of cloning refers to a variety of ethical positions regarding the practice and possibilities of cloning, especially human cloning. While many of these views are religious in origin, the questions raised by cloning are faced by secular perspectives as well. Human therapeutic and reproductive cloning are not commercially used; animals are currently cloned in laboratories and in livestock production.

Advocates support development of therapeutic cloning in order to generate tissues and whole organs to treat patients who otherwise cannot obtain transplants, to avoid the need for immunosuppressive drugs, and to stave off the effects of aging. Advocates for reproductive cloning believe that parents who cannot otherwise procreate should have access to the technology.

Opposition to therapeutic cloning mainly centers around the status of embryonic stem cells, which has connections with the abortion debate.

Some opponents of reproductive cloning have concerns that technology is not yet developed enough to be safe – for example, the position of the American Association for the Advancement of Science , while others emphasize that reproductive cloning could be prone to abuse (leading to the generation of humans whose organs and tissues would be harvested), and have concerns about how cloned individuals could integrate with families and with society at large. Some opponents will raise questions on whether clones have rights. "Cloning's Future" raises series question on whether the embryo's have any rights or if the donor outweigh's the right of the embryo to live.

Religious groups are divided, with some opposing the technology as usurping God's role in creation and, to the extent embryos are used, destroying a human life; others support therapeutic cloning's potential life-saving benefits.

In 2018 it was reported that about 70 countries had banned human cloning.

Human cloning is banned by the Presidential Decree 200/97 of 7 March 1997.

Australia has prohibited human cloning, though , a bill legalizing therapeutic cloning and the creation of human embryos for stem cell research passed the House of Representatives. Within certain regulatory limits, and subject to the effect of state legislation, therapeutic cloning is now legal in some parts of Australia.

Canadian law prohibits the following: cloning humans, cloning stem cells, growing human embryos for research purposes, and buying or selling of embryos, sperm, eggs or other human reproductive material. It also bans making changes to human DNA that would pass from one generation to the next, including use of animal DNA in humans. Surrogate mothers are legally allowed, as is donation of sperm or eggs for reproductive purposes. Human embryos and stem cells are also permitted to be donated for research.

There have been consistent calls in Canada to ban human reproductive cloning since the 1993 Report of the Royal Commission on New Reproductive Technologies. Polls have indicated that an overwhelming majority of Canadians oppose human reproductive cloning, though the regulation of human cloning continues to be a significant national and international policy issue. The notion of "human dignity" is commonly used to justify cloning laws. The basis for this justification is that reproductive human cloning necessarily infringes notions of human dignity.

Human cloning is prohibited in Article 133 of the Colombian Penal Code.

The European Convention on Human Rights and Biomedicine prohibits human cloning in one of its additional protocols, this protocol has been ratified by 25 states.

The Charter of Fundamental Rights of the European Union explicitly prohibits reproductive human cloning. The charter is legally binding for the institutions of the European Union under the Treaty of Lisbon and for some member countries of the Union implementing EU regulations.

India does not have specific law regarding cloning but has guidelines prohibiting whole human cloning or reproductive cloning. India allows therapeutic cloning and the use of embryonic stem cells for research purposes.

Council of Islamic Ideology of Pakistan has declared human cloning as un-Islamic act. According to Council of Islamic Ideology, research and thinking is not banned in Islam and new innovations are allowed but within the limits of the religion.

Human cloning forbidden by article 87 of Act of 25 June 2015.

The Federal Assembly of Russia introduced the Federal Law N 54-FZ "On the temporary ban on human cloning" on April 19, 2002. On May 20, 2002 President Vladimir Putin signed this moratorium on the implementation of human cloning. On March 29, 2010 The Federal Assembly introduced second revision of this law without time limit.

Human cloning is explicitly prohibited in Article 24, "Right to Life" of the 2006 Constitution of Serbia.

In terms of section 39A of the Human Tissue Act 65 of 1983, genetic manipulation of gametes or zygotes outside the human body is absolutely prohibited. A zygote is the cell resulting from the fusion of two gametes; thus the fertilised ovum. Section 39A thus prohibits human cloning.

Section 5 of the Human Cloning and Other Prohibited Practices Act 2004 prohibits placing human embryo clone in body of human or animal.

On January 14, 2001 the British government passed The Human Fertilisation and Embryology (Research Purposes) Regulations 2001 to amend the Human Fertilisation and Embryology Act 1990 by extending allowable reasons for embryo research to permit research around stem cells and cell nuclear replacement, thus allowing therapeutic cloning. However, on November 15, 2001, a pro-life group won a High Court legal challenge, which struck down the regulation and effectively left all forms of cloning unregulated in the UK. Their hope was that Parliament would fill this gap by passing prohibitive legislation. Parliament was quick to pass the Human Reproductive Cloning Act 2001 which explicitly prohibited reproductive cloning. The remaining gap with regard to therapeutic cloning was closed when the appeals courts reversed the previous decision of the High Court.

The first license was granted on August 11, 2004 to researchers at the University of Newcastle to allow them to investigate treatments for diabetes, Parkinson's disease and Alzheimer's disease. The Human Fertilisation and Embryology Act 2008, a major review of fertility legislation, repealed the 2001 Cloning Act by making amendments of similar effect to the 1990 Act. The 2008 Act also allows experiments on hybrid human-animal embryos.

On December 13, 2001, the United Nations General Assembly began elaborating an international convention against the reproductive cloning of humans. A broad coalition of states, including Spain, Italy, the Philippines, the United States, Costa Rica, and the Holy See sought to extend the debate to ban all forms of human cloning, noting that, in their view, therapeutic human cloning violates human dignity. Costa Rica proposed the adoption of an international convention to ban all forms of human cloning. Unable to reach a consensus on a binding convention, in March 2005 a non-binding United Nations Declaration on Human Cloning, calling for the ban of all forms of human cloning contrary to human dignity, was adopted.

The Patients First Act of 2017 (HR 2918, 115th Congress) aims to promote stem cell research, using cells that are “ethically obtained”, that could contribute to a better understanding of diseases and therapies, and promote the “derivation of pluripotent stem cell lines without the creation of human embryos…”

In 1998, 2001, 2004, 2005, 2007 and 2009, the US Congress voted whether to ban all human cloning, both reproductive and therapeutic (see Stem Cell Research Enhancement Act). "President Clinton in 1993 lifts the ban on taxpayer-funded fetal tissue research. [Throughout the following years in February 1997 Dolly was cloned; Clinton launches review of US policy and In May 1997 Federal funding for human cloning [was] banned" divisions in the Senate, or an eventual veto from the sitting President (President George W. Bush in 2005 and 2007), over therapeutic cloning prevented either competing proposal (a ban on both forms or on reproductive cloning only) from being passed into law. On March 10, 2010 a bill (HR 4808) was introduced with a section banning federal funding for human cloning. Such a law, if passed, would not have prevented research from occurring in private institutions (such as universities) that have both private and federal funding. However, the 2010 law was not passed.

There are currently no federal laws in the United States which ban cloning completely. Fifteen American states (Arkansas, California, Connecticut, Iowa, Indiana, Massachusetts, Maryland, Michigan, North Dakota, New Jersey, Rhode Island, South Dakota, Florida, Georgia, and Virginia) ban reproductive cloning and three states (Arizona, Maryland, and Missouri) prohibit use of public funds for such activities.

10 states, California, Connecticut, Illinois, Iowa, Maryland, Massachusetts, Missouri, Montana, New Jersey, and Rhode Island, have "clone and kill" laws that prevent cloned embryo implantation for childbirth, but allow embryos to be destroyed.

Science fiction has used cloning, most commonly and specifically human cloning, due to the fact that it brings up controversial questions of identity. Humorous fiction, such as "Multiplicity" (1996) and the Maxwell Smart feature "The Nude Bomb" (1980), have featured human cloning. A recurring sub-theme of cloning fiction is the use of clones as a supply of organs for transplantation. Robin Cook's 1997 novel "Chromosome 6" and Michael Bay's "The Island" are examples of this; "Chromosome 6" also features genetic manipulation and xenotransplantation. The series "Orphan Black" follows human clones' stories and experiences as they deal with issues and react to being the property of a chain of scientific institutions. In the 2019 horror film "Us", the entirety of the United States' population is secretly cloned. Years later, these clones (known as The Tethered) reveal themselves to the world by successfully pulling off a mass genocide of their counterparts.




</doc>
<doc id="14097" url="https://en.wikipedia.org/wiki?curid=14097" title="History of Asia">
History of Asia

The history of Asia can be seen as the collective history of several distinct peripheral coastal regions such as East Asia, South Asia, Southeast Asia and the Middle East linked by the interior mass of the Eurasian steppe.

The coastal periphery was the home to some of the world's earliest known civilizations and religions, with each of the three regions developing early civilizations around fertile river valleys. These valleys were fertile because the soil there was rich and could bear many root crops. The civilizations in Mesopotamia, India, and China shared many similarities and likely exchanged technologies and ideas such as mathematics and the wheel. Other notions such as that of writing likely developed individually in each area. Cities, states and then empires developed in these lowlands.

The steppe region had long been inhabited by mounted nomads, and from the central steppes they could reach all areas of the Asian continent. The northern part of the continent, covering much of Siberia was also inaccessible to the steppe nomads due to the dense forests and the tundra. These areas in Siberia were very sparsely populated.

The centre and periphery were kept separate by mountains and deserts. The Caucasus, Himalaya, Karakum Desert, and Gobi Desert formed barriers that the steppe horsemen could only cross with difficulty. While technologically and culturally the city dwellers were more advanced, they could do little militarily to defend against the mounted hordes of the steppe. However, the lowlands did not have enough open grasslands to support a large horsebound force. Thus the nomads who conquered states in the Middle East were soon forced to adapt to the local societies.

The spread of Islam waved the Islamic Golden Age and the Timurid Renaissance, which later influenced the age of Islamic gunpowder empires.

Asia's history features major developments seen in other parts of the world, as well as events that have affected those other regions. These include the trade of the Silk Road, which spread cultures, languages, religions, and diseases throughout Afro-Eurasian trade. Another major advancement was the innovation of gunpowder in medieval China, later developed by the Gunpowder empires, mainly by the Mughals and Safavids, which led to advanced warfare through the use of guns.

A report by archaeologist Rakesh Tewari on Lahuradewa, India shows new C14 datings that range between 9000 and 8000 BCE associated with rice, making Lahuradewa the earliest Neolithic site in entire South Asia.

The prehistoric Beifudi site near Yixian in Hebei Province, China, contains relics of a culture contemporaneous with the Cishan and Xinglongwa cultures of about 8000–7000 BCE, neolithic cultures east of the Taihang Mountains, filling in an archaeological gap between the two Northern Chinese cultures. The total excavated area is more than 1,200 square meters and the collection of neolithic findings at the site consists of two phases.

Around 5500 BCE the Halafian culture appeared in Lebanon, Israel, Syria, Anatolia, and northern Mesopotamia, based upon dryland agriculture.

In southern Mesopotamia were the alluvial plains of Sumer and Elam. Since there was little rainfall, irrigation systems were necessary. The Ubaid culture flourished from 5500 BCE.

The Chalcolithic period (or Copper Age) began about 4500 BCE, then the Bronze Age began about 3500 BCE, replacing the Neolithic cultures.

The Indus Valley Civilization (IVC) was a Bronze Age civilization (3300–1300 BCE; mature period 2600–1900 BCE) which was centered mostly in the western part of the Indian Subcontinent; it is considered that an early form of Hinduism was performed during this civilization. Some of the great cities of this civilization include Harappa and Mohenjo-daro, which had a high level of town planning and arts. The cause of the destruction of these regions around 1700 BCE is debatable, although evidence suggests it was caused by natural disasters (especially flooding). This era marks Vedic period in India, which lasted from roughly 1500 to 500 BCE. During this period, the Sanskrit language developed and the Vedas were written, epic hymns that told tales of gods and wars. This was the basis for the Vedic religion, which would eventually sophisticate and develop into Hinduism.

China and Vietnam were also centres of metalworking. Dating back to the Neolithic Age, the first bronze drums, called the Dong Son drums have been uncovered in and around the Red River Delta regions of Vietnam and Southern China. These relate to the prehistoric Dong Son Culture of Vietnam.
Song Da bronze drum's surface, Dong Son culture, Vietnam

In Ban Chiang, Thailand (Southeast Asia), bronze artifacts have been discovered dating to 2100 BCE.

In Nyaunggan, Burma bronze tools have been excavated along with ceramics and stone artifacts. Dating is still currently broad (3500–500 BCE).

The Iron Age saw the widespread use of iron tools, weaponry, and armor throughout the major civilizations of Asia.

The Achaemenid dynasty of the Persian Empire, founded by Cyrus the Great, ruled an area from Greece and Turkey to the Indus River and Central Asia during the 6th to 4th centuries BCE. Persian politics included a tolerance for other cultures, a highly centralized government, and significant infrastructure developments. Later, in Darius the Great's rule, the territories were integrated, a bureaucracy was developed, nobility were assigned military positions, tax collection was carefully organized, and spies were used to ensure the loyalty of regional officials. The primary religion of Persia at this time was Zoroastrianism, developed by the philosopher Zoroaster. It introduced an early form of monotheism to the area. The religion banned animal sacrifice and the use of intoxicants in rituals; and introduced the concept of spiritual salvation through personal moral action, an end time, and both general and Particular judgment with a heaven or hell. These concepts would heavily influence later emperors and the masses. More importantly, Zoroastrianism would be an important precursor for the Abrahamic religions such as Christianity, Islam, or Judaism. The Persian Empire was successful in establishing peace and stability throughout the Middle East and were a major influence in art, politics (affecting Hellenistic leaders), and religion.

Alexander the Great conquered this dynasty in the 4th century BCE, creating the brief Hellenistic period. He was unable to establish stability and after his death, Persia broke into small, weak dynasties including the Seleucid Empire, followed by the Parthian Empire. By the end of the Classical age, Persia had been reconsolidated into the Sassanid Empire, also known as the second Persian Empire.

The Roman Empire would later control parts of Western Asia. The Seleucid, Parthian and Sassanid dynasties of Persia dominated Western Asia for centuries.

The Maurya and Gupta empires are called the Golden Age of India and were marked by extensive inventions and discoveries in science, technology, art, religion, and philosophy that crystallized the elements of what is generally known as Indian culture. The religions of Hinduism and Buddhism, which began in Indian sub-continent, were an important influence on South, East and Southeast Asia.
By 600 BCE, India had been divided into 17 regional states that would occasionally feud amongst themselves. In 327 BCE, Alexander the Great came to India with a vision of conquering the whole world. He crossed northwestern India and created the province Bactria but could not move further because his army wanted to go back to their family. Shortly prior, the soldier Chandragupta Maurya began to take control of the Ganges river and soon established the Maurya Empire. The Maurya Empire (Sanskrit: मौर्य राजवंश, Maurya Rājavaṃśa) was the geographically extensive and powerful empire in ancient India, ruled by the Mauryan dynasty from 321 to 185 BCE. It was one of the world's largest empires in its time, stretching to the Himalayas in the north, what is now Assam in the east, probably beyond modern Pakistan in the west, and annexing Balochistan and much of what is now Afghanistan, at its greatest extent. South of Mauryan empire was the Tamilakam an independent country dominated by three dynasties, the Pandyans, Cholas and Cheras. The government established by Chandragupta was led by an autocratic king, who primarily relied on the military to assert his power. It also applied the use of a bureaucracy and even sponsored a postal service. Chandragupta's grandson, Ashoka, greatly extended the empire by conquering most of modern-day India (save for the southern tip). He eventually converted to Buddhism, though, and began a peaceful life where he promoted the religion as well as humane methods throughout India. The Maurya Empire would disintegrate soon after Ashoka's death and was conquered by the Kushan invaders from the northwest, establishing the Kushan Empire. Their conversion to Buddhism caused the religion to be associated with foreigners and therefore a decline in its popularity occurred.

The Kushan Empire would fall apart by 220 CE, creating more political turmoil in India. Then in 320, the Gupta Empire (Sanskrit: गुप्त राजवंश, Gupta Rājavanśha) was established and covered much of the Indian Subcontinent. Founded by Maharaja Sri-Gupta, the dynasty was the model of a classical civilization. Gupta kings united the area primarily through negotiation of local leaders and families as well as strategical intermarriage. Their rule covered less land than the Maurya Empire, but established the greatest stability. In 535, the empire ended when India was overrun by the Hunas.

Since 1029 BCE, the Zhou dynasty ( ), had existed in China and it would continue to until 258 BCE. The Zhou dynasty had been using a feudal system by giving power to local nobility and relying on their loyalty in order to control its large territory. As a result, the Chinese government at this time tended to be very decentralized and weak, and there was often little the emperor could do to resolve national issues. Nonetheless, the government was able to retain its position with the creation of the Mandate of Heaven, which could establish an emperor as divinely chosen to rule. The Zhou additionally discouraged the human sacrifice of the preceding eras and unified the Chinese language. Finally, the Zhou government encouraged settlers to move into the Yangtze River valley, thus creating the Chinese Middle Kingdom.

But by 500 BCE, its political stability began to decline due to repeated nomadic incursions and internal conflict derived from the fighting princes and families. This was lessened by the many philosophical movements, starting with the life of Confucius. His philosophical writings (called Confucianism) concerning the respect of elders and of the state would later be popularly used in the Han dynasty. Additionally, Laozi's concepts of Taoism, including yin and yang and the innate duality and balance of nature and the universe, became popular throughout this period. Nevertheless, the Zhou Dynasty eventually disintegrated as the local nobles began to gain more power and their conflict devolved into the Warring States period, from 402 to 201 BCE.

One leader eventually came on top, Qin Shi Huang (, "Shǐ Huángdì"), who overthrew the last Zhou emperor and established the Qin dynasty. The Qin dynasty (Chinese: 秦朝; pinyin: Qín Cháo) was the first ruling dynasty of Imperial China, lasting from 221 to 207 BCE. The new Emperor abolished the feudal system and directly appointed a bureaucracy that would rely on him for power. Huang's imperial forces crushed any regional resistance, and they furthered the Chinese empire by expanding down to the South China Sea and northern Vietnam. Greater organization brought a uniform tax system, a national census, regulated road building (and cart width), standard measurements, standard coinage, and an official written and spoken language. Further reforms included new irrigation projects, the encouragement of silk manufacturing, and (most famously) the beginning of the construction of the Great Wall of China—designed to keep out the nomadic raiders who'd constantly badger the Chinese people. However, Shi Huang was infamous for his tyranny, forcing laborers to build the Wall, ordering heavy taxes, and severely punishing all who opposed him. He oppressed Confucians and promoted Legalism, the idea that people were inherently evil, and that a strong, forceful government was needed to control them. Legalism was infused with realistic, logical views and rejected the pleasures of educated conversation as frivolous. All of this made Shi Huang extremely unpopular with the people. As the Qin began to weaken, various factions began to fight for control of China.

The Han dynasty (simplified Chinese: 汉朝; traditional Chinese: 漢朝; pinyin: Hàn Cháo; 206 BCE – 220 CE) was the second imperial dynasty of China, preceded by the Qin Dynasty and succeeded by the Three Kingdoms (220–265 CE). Spanning over four centuries, the period of the Han Dynasty is considered a golden age in Chinese history. One of the Han dynasty's greatest emperors, Emperor Wu of Han, established a peace throughout China comparable to the Pax Romana seen in the Mediterranean a hundred years later. To this day, China's majority ethnic group refers to itself as the "Han people". The Han Dynasty was established when two peasants succeeded in rising up against Shi Huang's significantly weaker successor-son. The new Han government retained the centralization and bureaucracy of the Qin, but greatly reduced the repression seen before. They expanded their territory into Korea, Vietnam, and Central Asia, creating an even larger empire than the Qin.

The Han developed contacts with the Persian Empire in the Middle East and the Romans, through the Silk Road, with which they were able to trade many commodities—primarily silk. Many ancient civilizations were influenced by the Silk Road, which connected China, India, the Middle East and Europe. Han emperors like Wu also promoted Confucianism as the national "religion" (although it is debated by theologians as to whether it is defined as such or as a philosophy). Shrines devoted to Confucius were built and Confucian philosophy was taught to all scholars who entered the Chinese bureaucracy. The bureaucracy was further improved with the introduction of an examination system that selected scholars of high merit. These bureaucrats were often upper-class people educated in special schools, but whose power was often checked by the lower-class brought into the bureaucracy through their skill. The Chinese imperial bureaucracy was very effective and highly respected by all in the realm and would last over 2,000 years. The Han government was highly organized and it commanded the military, judicial law (which used a system of courts and strict laws), agricultural production, the economy, and the general lives of its people. The government also promoted intellectual philosophy, scientific research, and detailed historical records.

However, despite all of this impressive stability, central power began to lose control by the turn of the Common Era. As the Han Dynasty declined, many factors continued to pummel it into submission until China was left in a state of chaos. By 100 CE, philosophical activity slowed, and corruption ran rampant in the bureaucracy. Local landlords began to take control as the scholars neglected their duties, and this resulted in heavy taxation of the peasantry. Taoists began to gain significant ground and protested the decline. They started to proclaim magical powers and promised to save China with them; the Taoist Yellow Turban Rebellion in 184 (led by rebels in yellow scarves) failed but was able to weaken the government. The aforementioned Huns combined with diseases killed up to half of the population and officially ended the Han dynasty by 220. The ensuing period of chaos was so terrible it lasted for three centuries, where many weak regional rulers and dynasties failed to establish order in China. This period of chaos and attempts at order is commonly known as that of the Six Dynasties. The first part of this included the Three Kingdoms which started in 220 and describes the brief and weak successor "dynasties" that followed the Han. In 265, the Jin dynasty of China was started and this soon split into two different empires in control of northwestern and southeastern China. In 420, the conquest and abdication of those two dynasties resulted in the first of the Southern and Northern Dynasties. The Northern and Southern Dynasties passed through until finally, by 557, the Northern Zhou dynasty ruled the north and the Chen dynasty ruled the south.

During this period, the Eastern world empires continued to expand through trade, migration and conquests of neighboring areas. Gunpowder was widely used as early as the 11th century and they were using moveable type printing five hundred years before Gutenberg created his press. Buddhism, Taoism, Confucianism were the dominant philosophies of the Far East during the Middle Ages. Marco Polo was not the first Westerner to travel to the Orient and return with amazing stories of this different culture, but his accounts published in the late 13th and early 14th centuries were the first to be widely read throughout Europe.

The Arabian peninsula and the surrounding Middle East and Near East regions saw dramatic change during the Medieval era caused primarily by the spread of Islam and the establishment of the Arabian Empires.

In the 5th century, the Middle East was separated into small, weak states; the two most prominent were the Sassanian Empire of the Persians in what is now Iran and Iraq, and the Byzantine Empire in Anatolia (modern-day Turkey). The Byzantines and Sassanians fought with each other continually, a reflection of the rivalry between the Roman Empire and the Persian Empire seen during the previous five hundred years. The fighting weakened both states, leaving the stage open to a new power. Meanwhile, the nomadic Bedouin tribes who dominated the Arabian desert saw a period of tribal stability, greater trade networking and a familiarity with Abrahamic religions or monotheism.

While the Byzantine Roman and Sassanid Persian empires were both weakened by the Byzantine–Sasanian War of 602–628, a new power in the form of Islam grew in the Middle East under Muhammad in Medina. In a series of rapid Muslim conquests, the Rashidun army, led by the Caliphs and skilled military commanders such as Khalid ibn al-Walid, swept through most of the Middle East, taking more than half of Byzantine territory in the Arab–Byzantine wars and completely engulfing Persia in the Muslim conquest of Persia. It would be the Arab Caliphates of the Middle Ages that would first unify the entire Middle East as a distinct region and create the dominant ethnic identity that persists today. These Caliphates included the Rashidun Caliphate, Umayyad Caliphate, Abbasid Caliphate, and later the Seljuq Empire.
After Muhammad introduced Islam, it jump-started Middle Eastern culture into an Islamic Golden Age, inspiring achievements in architecture, the revival of old advances in science and technology, and the formation of a distinct way of life. Muslims saved and spread Greek advances in medicine, algebra, geometry, astronomy, anatomy, and ethics that would later finds it way back to Western Europe.

The dominance of the Arabs came to a sudden end in the mid-11th century with the arrival of the Seljuq Turks, migrating south from the Turkic homelands in Central Asia. They conquered Persia, Iraq (capturing Baghdad in 1055), Syria, Palestine, and the Hejaz. This was followed by a series of Christian Western Europe invasions. The fragmentation of the Middle East allowed joined forces, mainly from England, France, and the emerging Holy Roman Empire, to enter the region. In 1099 the knights of the First Crusade captured Jerusalem and founded the Kingdom of Jerusalem, which survived until 1187, when Saladin retook the city. Smaller crusader fiefdoms survived until 1291. In the early 13th century, a new wave of invaders, the armies of the Mongol Empire, swept through the region, sacking Baghdad in the Siege of Baghdad (1258) and advancing as far south as the border of Egypt in what became known as the Mongol conquests. The Mongols eventually retreated in 1335, but the chaos that ensued throughout the empire deposed the Seljuq Turks. In 1401, the region was further plagued by the Turko-Mongol, Timur, and his ferocious raids. By then, another group of Turks had arisen as well, the Ottomans.

The Mongol Empire conquered a large part of Asia in the 13th century, an area extending from China to Europe. Medieval Asia was the kingdom of the Khans. Never before had any person controlled as much land as Genghis Khan. He built his power unifying separate Mongol tribes before expanding his kingdom south and west. He and his grandson, Kublai Khan, controlled lands in China, Burma, Central Asia, Russia, Iran, the Middle East, and Eastern Europe. Estimates are that the Mongol armies reduced the population of China by nearly a third. Genghis Khan was a pagan who tolerated nearly every religion, and their culture often suffered the harshest treatment from Mongol armies. The Khan armies pushed as far west as Jerusalem before being defeated in 1260.

The Indian early medieval age, 600 to 1200, is defined by regional kingdoms and cultural diversity. When Harsha of Kannauj, who ruled much of the Indo-Gangetic Plain from 606 to 647, attempted to expand southwards, he was defeated by the Chalukya ruler of the Deccan. When his successor attempted to expand eastwards, he was defeated by the Pala king of Bengal. When the Chalukyas attempted to expand southwards, they were defeated by the Pallavas from farther south, who in turn were opposed by the Pandyas and the Cholas from still farther south. The Cholas could under the rule of Raja Raja Chola defeat their rivals and rise to a regional power. Cholas expanded northward and defeated Eastern Chalukya, Kalinga and the Pala. Under Rajendra Chola the Cholas created the first notable navy of Indian subcontinent. The Chola navy extended the influence of Chola empire to southeast asia. During this time, pastoral peoples whose land had been cleared to make way for the growing agricultural economy were accommodated within caste society, as were new non-traditional ruling classes.

The Muslim conquest in the Indian subcontinent mainly took place from the 12th century onwards, though earlier Muslim conquests include the limited inroads into modern Afghanistan and Pakistan and the Umayyad campaigns in India, during the time of the Rajput kingdoms in the 8th century.

Major economic and military powers like the Delhi Sultanate and Bengal Sultanate, were seen to be established. The search of their wealth led the Voyages of Christopher Columbus.

China saw the rise and fall of the Sui, Tang, Song, and Yuan dynasties and therefore improvements in its bureaucracy, the spread of Buddhism, and the advent of Neo-Confucianism. It was an unsurpassed era for Chinese ceramics and painting. Medieval architectural masterpieces the Great South Gate in Todaiji, Japan, and the Tien-ning Temple in Peking, China are some of the surviving constructs from this era.

A new powerful dynasty began to rise in the 580s, amongst the divided factions of China. This was started when an aristocrat named Yang Jian married his daughter into the Northern Zhou dynasty. He proclaimed himself Emperor Wen of Sui and appeased the nomadic military by abandoning the Confucian scholar-gentry. Emperor Wen soon led the conquest of the southern Chen Dynasty and united China once more under the Sui dynasty. The emperor lowered taxes and constructed granaries that he used to prevent famine and control the market. Later Wen's son would murder him for the throne and declare himself Emperor Yang of Sui. Emperor Yang revived the Confucian scholars and the bureaucracy, much to anger of the aristocrats and nomadic military leaders. Yang became an excessive leader who overused China's resources for personal luxury and perpetuated exhaustive attempts to conquer Goguryeo. His military failures and neglect of the empire forced his own ministers to assassinate him in 618, ending the Sui Dynasty.

Fortunately, one of Yang's most respectable advisors, Li Yuan, was able to claim the throne quickly, preventing a chaotic collapse. He proclaimed himself Emperor Gaozu, and established the Tang dynasty in 623. The Tang saw expansion of China through conquest to Tibet in the west, Vietnam in the south, and Manchuria in the north. Tang emperors also improved the education of scholars in the Chinese bureaucracy. A Ministry of Rites was established and the examination system was improved to better qualify scholars for their jobs. In addition, Buddhism became popular in China with two different strains between the peasantry and the elite, the Pure Land and Zen strains, respectively. Greatly supporting the spread of Buddhism was Empress Wu, who additionally claimed an unofficial "Zhou dynasty" and displayed China's tolerance of a woman ruler, which was rare at the time. However, Buddhism would also experience some backlash, especially from Confucianists and Taoists. This would usually involve criticism about how it was costing the state money, since the government was unable to tax Buddhist monasteries, and additionally sent many grants and gifts to them.

The Tang dynasty began to decline under the rule of Emperor Xuanzong, who began to neglect the economy and military and caused unrest amongst the court officials due to the excessive influence of his concubine, Yang Guifei, and her family. This eventually sparked a revolt in 755. Although the revolt failed, subduing it required involvement with the unruly nomadic tribes outside of China and distributing more power to local leaders—leaving the government and economy in a degraded state. The Tang dynasty officially ended in 907 and various factions led by the aforementioned nomadic tribes and local leaders would fight for control of China in the Five Dynasties and Ten Kingdoms period.

By 960, most of China proper had been reunited under the Song dynasty, although it lost territories in the north and could not defeat one of the nomadic tribes there—the Liao dynasty of the highly sinicized Khitan people. From then on, the Song would have to pay tribute to avoid invasion and thus set the precedent for other nomadic kingdoms to oppress them. The Song also saw the revival of Confucianism in the form of Neo-Confucianism. This had the effect of putting the Confucian scholars at a higher status than aristocrats or Buddhists and also intensified the reduction of power in women. The infamous practice of foot binding developed in this period as a result. Eventually the Liao dynasty in the north was overthrown by the Jin dynasty of the Manchu-related Jurchen people. The new Jin kingdom invaded northern China, leaving the Song to flee farther south and creating the Southern Song dynasty in 1126. There, cultural life flourished.

By 1227, the Mongols had conquered the Western Xia kingdom northwest of China. Soon the Mongols incurred upon the Jin empire of the Jurchens. Chinese cities were soon besieged by the Mongol hordes that showed little mercy for those who resisted and the Southern Song Chinese were quickly losing territory. In 1271 the current great khan, Kublai Khan, claimed himself Emperor of China and officially established the Yuan Dynasty. By 1290, all of China was under control of the Mongols, marking the first time they were ever completely conquered by a foreign invader; the new capital was established at Khanbaliq (modern-day Beijing). Kublai Khan segregated Mongol culture from Chinese culture by discouraging interactions between the two peoples, separating living spaces and places of worship, and reserving top administrative positions to Mongols, thus preventing Confucian scholars to continue the bureaucratic system. Nevertheless, Kublai remained fascinated with Chinese thinking, surrounding himself with Chinese Buddhist, Taoist, or Confucian advisors.

Mongol women displayed a contrasting independent nature compared to the Chinese women who continued to be suppressed. Mongol women often rode out on hunts or even to war. Kublai's wife, Chabi, was a perfect example of this; Chabi advised her husband on several political and diplomatic matters; she convinced him that the Chinese were to be respected and well-treated in order to make them easier to rule. However this was not enough to affect Chinese women's position, and the increasingly Neo-Confucian successors of Kublai further repressed Chinese and even Mongol women.

The Black Death, which would later ravage Western Europe, had its beginnings in Asia, where it wiped out large populations in China in 1331.

The three Kingdoms of Korea involves Goguryeo in north, Baekje in southwest, and Silla in southeast Korean peninsula. These three kingdoms were like a bridge of cultures between China and Japan. Thanks to them, Japan was able to accept Chinese splendid cultures. Prince Shōtoku of Japan had been taught by two teachers. One was from Baekje, the other was from Goguryeo. Once Japan invaded Silla, Goguryeo helped Silla to defeat Japan. Baekje met the earliest heyday of them. Its heyday was the 5th century AD. Its capital was Seoul. During its heyday, the kingdom made colonies overseas. Liaodong, China and Kyushu, Japan were the colonies of Baekje during its short heyday. Goguryeo was the strongest kingdom of all. They sometimes called themselves as an Empire. Its heyday was 6th century. King Gwanggaeto widened its territory to north. So Goguryeo dominated from Korean peninsula to Manchuria. And his son, King Jangsu widened its territory to south. He occupied Seoul, and moved its capital to Pyeongyang. Goguryeo almost occupied three quarters of South Korean peninsula thanks to king Jangsu who widened the kingdom's territory to south. Silla met the latest heyday. King Jinheung went north and occupiedSeoul. But it was short. Baekje became stronger and attacked Silla. Baekje occupied more than 40 cities of Silla. So Silla could hardly survive.
China's Sui dynasty invaded Goguryeo and Goguryeo–Sui War occurs between Korea and China. Goguryeo won against China and Sui dynasty fell. After then, Tang dynasty reinvaded Goguryeo and helped Silla to unify the peninsula. Because Silla was the weakest of all, so it asked Tang for help. Goguryeo, Baekje, and Japan helped each other against Tang-Silla alliance, but Baekje and Goguryeo fell. Unfortunately, Tang dynasty betrayed Silla in order to occupy the whole Korean peninsula. During the Silla-Tang war, people of fallen Baekje and Goguryeo helped Silla against Chinese invasion, so Silla could beat China and unified the peninsula. This war helped Korean people to unite mentally.

The rest of Goguryeo people established Balhae and won the war against Tang in later 7th century AD. Balhae is the north state, and Later Silla was the south state. Balhae was a quite strong kingdom as their ancestor Goguryeo did. Finally, the Emperor of Tang dynasty admits Balhae as 'A strong country in the East'. They liked to trade with Japan, China, and Silla. Balhae and Later Silla sent a lot of international students to China. And Arabian merchants came into Korean peninsula, so Korea became known as 'Silla' in the western countries. Silla improved Korean writing system called Idu letters. Idu affected Katakana of Japan. Liao dynasty invaded Balhae in early 10th century, so Balhae fell.

The unified Korean kingdom, Later Silla divided into three kingdoms again because of the corrupt central government. It involves Later Goguryeo(also as known as "Taebong"), Later Baekje, and Later Silla. The general of Later Goguryeo, Wang Geon took the thrown and changed its name into Goryeo, which was derived by the old strong kingdom, Goguryeo, and Goryeo reunified the peninsula.

Goryeo reunited the Korean peninsula during the later three kingdoms period and named itself as 'Empire'. But nowadays, Goryeo is known as a kingdom. The name 'Goryeo' was derived from Goguryeo, and the name Korea was derived from Goryeo. Goryeo adopted people from fallen Balhae. They also widened their territory to north by defending Liao dynasty and attacking the Jurchen people. Goryeo developed a splendid culture. The first metal type printed book Jikji was also from Korea. The Goryeo ware is one of the most famous legacies of this kingdom. Goryeo imported Chinese government system and developed into their own ways.

During this period, laws were codified and a civil service system was introduced. Buddhism flourished and spread throughout the peninsula. The Tripitaka Koreana is 81,258 books total. It was made to keep Korea safe against the Mongolian invasion. It is now a UNESCO world heritage. Goryeo won the battle against Liao dynasty. Then, the Mongolian Empire invaded Goryeo. Goryeo did not disappear but it had to obey Mongolians. After 80 years, in 14th century, the Mongolian dynasty Yuan lose power, King Gongmin tried to free themselves against Mongol although his wife was also Mongolian. At the 14th century, Ming dynasty want Goryeo to obey China. But Goryeo didn't. They decided to invade China. Going to China, the general of Goryeo, Lee Sung-Gae came back and destroyed Goryeo and established new dynasty, Joseon. And he became Taejo of Joseon, which means the first king of Joseon.

Japan's medieval history began with the Asuka period, from around 600 to 710. The time was characterized by the Taika Reform and imperial centralization, both of which were a direct result of growing Chinese contact and influences. In 603, Prince Shōtoku of the Yamato dynasty began significant political and cultural changes. He issued the Seventeen-article constitution in 604, centralizing power towards the emperor (under the title "tenno", or heavenly sovereign) and removing the power to levy taxes from provincial lords. Shōtoku was also a patron of Buddhism and he encouraged building temples competitively.

Shōtoku's reforms transitioned Japan to the Nara period (c. 710 to c. 794), with the moving of the Japanese capital to Nara in Honshu. This period saw the culmination of Chinese-style writing, etiquette, and architecture in Japan along with Confucian ideals to supplement the already present Buddhism. Peasants revered both Confucian scholars and Buddhist monks. However, in the wake of the 735–737 Japanese smallpox epidemic, Buddhism gained the status of state religion and the government ordered the construction of numerous Buddhist temples, monasteries, and statues. The lavish spending combined with the fact that many aristocrats did not pay taxes, put a heavy burden on peasantry that caused poverty and famine. Eventually the Buddhist position got out of control, threatening to seize imperial power and causing Emperor Kanmu to move the capital to Heian-kyō to avoid a Buddhist takeover. This marked the beginning of the Heian period and the end of Taika reform.

With the Heian period (from 794 to 1185) came a decline of imperial power. Chinese influence also declined, as a result of its correlation with imperial centralization and the heavenly mandate, which came to be regarded as ineffective. By 838, the Japanese court discontinued its embassies in China; only traders and Buddhist monks continued to travel to China. Buddhism itself came to be considered more Japanese than Chinese, and persisted to be popular in Japan. Buddhists monks and monasteries continued their attempts to gather personal power in courts, along with aristocrats. One particular noble family that dominated influence in the imperial bureaucracy was the Fujiwara clan. During this time cultural life in the imperial court flourished. There was a focus on beauty and social interaction and writing and literature was considered refined. Noblewomen were cultured the same as noblemen, dabbling in creative works and politics. A prime example of both Japanese literature and women's role in high-class culture at this time was "The Tale of Genji", written by the lady-in-waiting Murasaki Shikibu. Popularization of wooden palaces and shōji sliding doors amongst the nobility also occurred.

Loss of imperial power also led to the rise of provincial warrior elites. Small lords began to function independently. They administered laws, supervised public works projects, and collected revenue for themselves instead of the imperial court. Regional lords also began to build their own armies. These warriors were loyal only their local lords and not the emperor, although the imperial government increasingly called them in to protect the capital. The regional warrior class developed into the samurai, which created its own culture: including specialized weapons such as the katana and a form of chivalry, bushido. The imperial government's loss of control in the second half of the Heian period allowed banditry to grow, requiring both feudal lords and Buddhist monasteries to procure warriors for protection. As imperial control over Japan declined, feudal lords also became more independent and seceded from the empire. These feudal states squandered the peasants living in them, reducing the farmers to an almost serfdom status. Peasants were also rigidly restricted from rising to the samurai class, being physically set off by dress and weapon restrictions. As a result of their oppression, many peasants turned to Buddhism as a hope for reward in the afterlife for upright behavior.

With the increase of feudalism, families in the imperial court began to depend on alliances with regional lords. The Fujiwara clan declined from power, replaced by a rivalry between the Taira clan and the Minamoto clan. This rivalry grew into the Genpei War in the early 1180s. This war saw the use of both samurai and peasant soldiers. For the samurai, battle was ritual and they often easily cut down the poorly trained peasantry. The Minamoto clan proved successful due to their rural alliances. Once the Taira was destroyed, the Minamoto established a military government called the shogunate (or bakufu), centered in Kamakura.

The end of the Genpei War and the establishment of the Kamakura shogunate marked the end of the Heian period and the beginning of the Kamakura period in 1185, solidifying feudal Japan.

In 802, Jayavarman II consolidated his rule over neighboring peoples and declared himself chakravartin, or "universal ruler". The Khmer Empire effectively dominated all Mainland Southeast Asia from the early 9th until the 15th century, during which time they developed a sophisticated monumental architecture of most exquisite expression and mastery of composition at Angkor.

The Russian Empire began to expand into Asia from the 17th century, and would eventually take control of all of Siberia and most of Central Asia by the end of the 19th century. The Ottoman Empire controlled Anatolia, the Middle East, North Africa and the Balkans from the 16th century onwards. In the 17th century, the Manchu conquered China and established the Qing Dynasty. In the 16th century, the Mughal Empire controlled much of India and initiated the second golden age for India. China was the largest economy in the world for much of the time, followed by India until the 18th century.

By 1368, Zhu Yuanzhang had claimed himself Hongwu Emperor and established the Ming dynasty of China. Immediately, the new emperor and his followers drove the Mongols and their culture out of China and beyond the Great Wall. The new emperor was somewhat suspicious of the scholars that dominated China's bureaucracy, for he had been born a peasant and was uneducated. Nevertheless, Confucian scholars were necessary to China's bureaucracy and were reestablished as well as reforms that would improve the exam systems and make them more important in entering the bureaucracy than ever before. The exams became more rigorous, cut down harshly on cheating, and those who excelled were more highly appraised. Finally, Hongwu also directed more power towards the role of emperor so as to end the corrupt influences of the bureaucrats.

The Hongwu emperor, perhaps for his sympathy of the common-folk, had built many irrigation systems and other public projects that provided help for the peasant farmers. They were also allowed to cultivate and claim unoccupied land without having to pay any taxes and labor demands were lowered. However, none of this was able to stop the rising landlord class that gained many privileges from the government and slowly gained control of the peasantry. Moneylenders foreclosed on peasant debt in exchange for mortgages and bought up farmer land, forcing them to become the landlords' tenants or to wander elsewhere for work. Also during this time, Neo-Confucianism intensified even more than the previous two dynasties (the Song and Yuan). Focus on the superiority of elders over youth, men over women, and teachers over students resulted in minor discrimination of the "inferior" classes. The fine arts grew in the Ming era, with improved techniques in brush painting that depicted scenes of court, city or country life; people such as scholars or travelers; or the beauty of mountains, lakes, or marshes. The Chinese novel fully developed in this era, with such classics written such as "Water Margin", "Journey to the West", and "Jin Ping Mei".

Economics grew rapidly in the Ming Dynasty as well. The introduction of American crops such as maize, sweet potatoes, and peanuts allowed for cultivation of crops in infertile land and helped prevent famine. The population boom that began in the Song dynasty accelerated until China's population went from 80 or 90 million to 150 million in three centuries, culminating in 1600. This paralleled the market economy that was growing both internally and externally. Silk, tea, ceramics, and lacquer-ware were produced by artisans that traded them in Asia and to Europeans. Westerners began to trade (with some Chinese-assigned limits), primarily in the port-towns of Macau and Canton. Although merchants benefited greatly from this, land remained the primary symbol of wealth in China and traders' riches were often put into acquiring more land. Therefore, little of these riches were used in private enterprises that could've allowed for China to develop the market economy that often accompanied the highly-successful Western countries.

In the interest of national glory, the Chinese began sending impressive junk ships across the South China Sea and the Indian Ocean. From 1403 to 1433, the Yongle Emperor commissioned expeditions led by the admiral Zheng He, a Muslim eunuch from China. Chinese junks carrying hundreds of soldiers, goods, and animals for zoos, traveled to Southeast Asia, Persia, southern Arabia, and east Africa to show off Chinese power. Their prowess exceeded that of current Europeans at the time, and had these expeditions not ended, the world economy may be different from today. In 1433, the Chinese government decided that the cost of a navy was an unnecessary expense. The Chinese navy was slowly dismantled and focus on interior reform and military defense began. It was China's longstanding priority that they protect themselves from nomads and they have accordingly returned to it. The growing limits on the Chinese navy would leave them vulnerable to foreign invasion by sea later on.

As was inevitable, Westerners arrived on the Chinese east coast, primarily Jesuit missionaries which reached the mainland in 1582. They attempted to convert the Chinese people to Christianity by first converting the top of the social hierarchy and allowing the lower classes to subsequently convert. To further gain support, many Jesuits adopted Chinese dress, customs, and language. Some Chinese scholars were interested in certain Western teachings and especially in Western technology. By the 1580s, Jesuit scholars like Matteo Ricci and Adam Schall amazed the Chinese elite with technological advances such as European clocks, improved calendars and cannons, and the accurate prediction of eclipses. Although some the scholar-gentry converted, many were suspicious of the Westerners whom they called "barbarians" and even resented them for the embarrassment they received at the hand of Western correction. Nevertheless, a small group of Jesuit scholars remained at the court to impress the emperor and his advisors.

Near the end of the 1500s, the extremely centralized government that gave so much power to the emperor had begun to fail as more incompetent rulers took the mantle. Along with these weak rulers came increasingly corrupt officials who took advantage of the decline. Once more the public projects fell into disrepair due to neglect by the bureaucracy and resulted in floods, drought, and famine that rocked the peasantry. The famine soon became so terrible that some peasants resorted to selling their children to slavery to save them from starvation, or to eating bark, the feces of geese, or other people. Many landlords abused the situation by building large estates where desperate farmers would work and be exploited. In turn, many of these farmers resorted to flight, banditry, and open rebellion.

All of this corresponded with the usual dynastic decline of China seen before, as well as the growing foreign threats. In the mid-16th century, Japanese and ethnic Chinese pirates began to raid the southern coast, and neither the bureaucracy nor the military were able to stop them. The threat of the northern Manchu people also grew. The Manchu were an already large state north of China, when in the early 17th century a local leader named Nurhaci suddenly united them under the Eight Banners—armies that the opposing families were organized into. The Manchus adopted many Chinese customs, specifically taking after their bureaucracy. Nevertheless, the Manchus still remained a Chinese vassal. In 1644 Chinese administration became so weak, the 16th and last emperor, the Chongzhen Emperor, did not respond to the severity of an ensuing rebellion by local dissenters until the enemy had invaded the Forbidden City (his personal estate). He soon hanged himself in the imperial gardens. For a brief amount of time, the Shun dynasty was claimed, until a loyalist Ming official called support from the Manchus to put down the new dynasty. The Shun Dynasty ended within a year and the Manchu were now within the Great Wall. Taking advantage of the situation, the Manchus marched on the Chinese capital of Beijing. Within two decades all of China belonged to the Manchu and the Qing dynasty was established.
In early-modern Korea, the 500-year-old kingdom, Goryeo fell and new dynasty Joseon rose in August 5th, 1392. Taejo of Joseon changed the country's name from Goryeo to Joseon. The fourth king, Sejong the Great created Hangul, the Korean alphabets by himself in 1443. He also improved science technology as Koreans invent Sun Clocks, Water Clocks, Rain-Measuring system, Star Map, Korean map, and detail records of Korean small villages. He even widened the territory to the north. So nowadays' Korean territory formed at that age. He even attacked Japanese pirates in Tsushima Island, who had been attacking Korea a lot. So he is considered the best king ever in history of Korea. The ninth king, Seongjong accomplished the first complete Korean law code in 1485. So the culture and people's lives were improved again. 

In 1592, Japan under Toyotomi Hideyoshi invaded Korea. That war is Imjin war. At that war, Joseon was in a long peace like PAX ROMANA. So Joseon was not ready for the war. Joseon lose again and again. Japanese army conquered Seoul. The whole Korean peninsula was in danger. But Yi Sun-sin, the most renowned general of Korea defeated Japanese fleet in southern Korea coast even 13 ships VS 133 ships. This is the legendary battle called "Battle of Myeongnyang". After then, Ming dynasty helped Joseon and Japan lose the battle. So Toyotomi Hideyoshi's campaign in Korea failed, and the Tokugawa Shogunate later began. Korea hurt a lot at Imjin war. Not long after, Manchurian people invaded Joseon again. It is called Qing invasion of Joseon. First invasion was for sake. Because Qing was at war between Ming, so Ming's alliance Joseon was threatening. And the second invasion was for Joseon to obey Qing. After then, Qing defeated Ming and took the whole Chinese territories. Joseon also had to obey Qing because Joseon lose the second war against Qing.

After Qing invasion, the princes of Joseon dynasty lived their childhood in China. The son of king Injo met Adam Schall in Beijing. So he wanted to introduce western technologies to Korean people when he becomes a king. Unfortunately, he died before he takes the thrown. After then, the alternative prince became the 17th king of Joseon dynasty, Hyojong, trying to revenge for his kingdom and fallen Ming dynasty to Qing. Later kings such as Yeongjo and Jeongjo tried to improve their people's lives and stop the governors' unreasonable competition. From 17th century to 18th century, Joseon sent diplomats and artists to Japan more than 10 times. This group was called 'Tongshinsa'. They were sent to Japan to teach Japan about advanced Korean cultures. Japanese people liked to receive poems from Korean nobles. At that time, Korea was more powerful than Japan. But that relationship between Joseon and Japan was reversed after 19th century. Because Japan became more powerful than Korea and China, either. So Joseon sent diplomats called 'Sooshinsa' to learn Japanese advanced technologies. After king Jeongjo's death, some noble families controlled the whole kingdom in early 19th century. At the end of that period, Western people invaded Joseon. In 1876, Joseon was set free from Qing so they did not have to obey Qing. But Japanese Empire was happy because Joseon became a perfect independent kingdom. So Japan could intervene the kingdom more. After this, Joseon traded with the US and sent 'Sooshinsa' to Japan, 'Youngshinsa' to Qing, and 'Bobingsa' to the US and Europe. These groups took many modern things to Korean peninsula.

In early-modern Japan following the Sengoku period of "warring states", central government had been largely reestablished by Oda Nobunaga and Toyotomi Hideyoshi during the Azuchi–Momoyama period. After the Battle of Sekigahara in 1600, central authority fell to Tokugawa Ieyasu who completed this process and received the title of "shōgun" in 1603.

Society in the Japanese "Tokugawa period" (see Edo society), unlike the shogunates before it, was based on the strict class hierarchy originally established by Toyotomi Hideyoshi. The "daimyōs" (feudal lords) were at the top, followed by the warrior-caste of samurai, with the farmers, artisans, and merchants ranking below. The country was strictly closed to foreigners with few exceptions with the "Sakoku" policy. Literacy rose in the two centuries of isolation.

In some parts of the country, particularly smaller regions, "daimyōs" and samurai were more or less identical, since "daimyōs" might be trained as samurai, and samurai might act as local lords. Otherwise, the largely inflexible nature of this social stratification system unleashed disruptive forces over time. Taxes on the peasantry were set at fixed amounts which did not account for inflation or other changes in monetary value. As a result, the tax revenues collected by the samurai landowners were worth less and less over time. This often led to numerous confrontations between noble but impoverished samurai and well-to-do peasants. None, however, proved compelling enough to seriously challenge the established order until the arrival of foreign powers.

In the Indian subcontinent, the Mughal Empire ruled most of India in the early 18th century. During emperor Shah Jahan and his son Aurangzeb's Islamic sharia reigns, the empire reached its architectural and economic zenith, and became the world's largest economy, worth over 25% of world GDP and signaled the proto-industrialization.

Following major events such as the Nader Shah's invasion of the Mughal Empire, Battle of Plassey, Battle of Buxar and the long Anglo-Mysore Wars, most of South Asia was colonised and governed by the British Empire, thus establishing the British Raj. The "classic period" ended with the death of Mughal Emperor Aurangzeb, although the dynasty continued for another 150 years. During this period, the Empire was marked by a highly centralized administration connecting the different regions. All the significant monuments of the Mughals, their most visible legacy, date to this period which was characterised by the expansion of Persian cultural influence in the Indian subcontinent, with brilliant literary, artistic, and architectural results. The Maratha Empire was located in the south west of present-day India and expanded greatly under the rule of the Peshwas, the prime ministers of the Maratha empire. In 1761, the Maratha army lost the Third Battle of Panipat against Shah Ahmad Durrani which halted imperial expansion and the empire was then divided into a confederacy of Maratha states.

The European economic and naval powers pushed into Asia, first to do trading, and then to take over major colonies. The Dutch led the way followed by the British. Portugal had arrived first, but was too weak to maintain its small holdings and was largely pushed out, retaining only Goa and Macau. The British set up a private organization, the East India Company, which handled both trade and Imperial control of much of India.

The commercial colonization of India commenced in 1757, after the Battle of Plassey, when the Nawab of Bengal surrendered his dominions to the British East India Company, in 1765, when the Company was granted the "diwani", or the right to collect revenue, in Bengal and Bihar, or in 1772, when the Company established a capital in Calcutta, appointed its first Governor-General, Warren Hastings, and became directly involved in governance.

The Maratha states, following the Anglo-Maratha wars, eventually lost to the British East India Company in 1818 with the Third Anglo-Maratha War. The rule lasted until 1858, when, after the Indian rebellion of 1857 and consequent of the Government of India Act 1858, the British government assumed the task of directly administering India in the new British Raj. In 1819 Stamford Raffles established Singapore as a key trading post for Britain in their rivalry with the Dutch. However, their rivalry cooled in 1824 when an Anglo-Dutch treaty demarcated their respective interests in Southeast Asia. From the 1850s onwards, the pace of colonization shifted to a significantly higher gear.

The Dutch East India Company (1800) and British East India Company (1858) were dissolved by their respective governments, who took over the direct administration of the colonies. Only Thailand was spared the experience of foreign rule, although, Thailand itself was also greatly affected by the power politics of the Western powers. Colonial rule had a profound effect on Southeast Asia. While the colonial powers profited much from the region's vast resources and large market, colonial rule did develop the region to a varying extent.

The Great Game was a political and diplomatic confrontation between Great Britain and Russia over Afghanistan and neighbouring territories in Central and South Asia. It lasted from 1828 to 1907. There was no war, but there were many threats. Russia was fearful of British commercial and military inroads into Central Asia, and Britain was fearful of Russia threatening its largest and most important possession, India. This resulted in an atmosphere of distrust and the constant threat of war between the two empires. Britain made it a high priority to protect all the approaches to India, and the "great game" is primarily how the British did this in terms of a possible Russian threat. Historians with access to the archives have concluded that Russia had no plans involving India, as the Russians repeatedly stated.

The Great Game began in 1838 when Britain decided to gain control over the Emirate of Afghanistan and make it a protectorate, and to use the Ottoman Empire, the Persian Empire, the Khanate of Khiva, and the Emirate of Bukhara as buffer states between both empires. This would protect India and also key British sea trade routes by stopping Russia from gaining a port on the Persian Gulf or the Indian Ocean. Russia proposed Afghanistan as the neutral zone, and the final result was diving up Afghanistan with a neutral zone in the middle between Russian areas in the north and British in the South. Important episodes included the failed First Anglo-Afghan War of 1838, the First Anglo-Sikh War of 1845, the Second Anglo-Sikh War of 1848, the Second Anglo-Afghan War of 1878, and the annexation of Kokand by Russia. The 1901 novel "Kim" by Rudyard Kipling made the term popular and introduced the new implication of great power rivalry. It became even more popular after the 1979 advent of the Soviet–Afghan War.

By 1644, the northern Manchu people had conquered China and established a foreign dynasty—the Qing Dynasty—once more. The Manchu Qing emperors, especially Confucian scholar Kangxi, remained largely conservative—retaining the bureaucracy and the scholars within it, as well as the Confucian ideals present in Chinese society. However, changes in the economy and new attempts at resolving certain issues occurred too. These included increased trade with Western countries that brought large amounts of silver into the Chinese economy in exchange for tea, porcelain, and silk textiles. This allowed for a new merchant-class, the compradors, to develop. In addition, repairs were done on existing dikes, canals, roadways, and irrigation works. This, combined with the lowering of taxes and government-assigned labor, was supposed to calm peasant unrest. However, the Qing failed to control the growing landlord class which had begun to exploit the peasantry and abuse their position.

By the late 18th century, both internal and external issues began to arise in Qing China's politics, society, and economy. The exam system with which scholars were assigned into the bureaucracy became increasingly corrupt; bribes and other forms of cheating allowed for inexperienced and inept scholars to enter the bureaucracy and this eventually caused rampant neglect of the peasantry, military, and the previously mentioned infrastructure projects. Poverty and banditry steadily rose, especially in rural areas, and mass migrations looking for work throughout China occurred. The perpetually conservative government refused to make reforms that could resolve these issues.

China saw its status reduced by what it perceived as parasitic trade with Westerners. Originally, European traders were at a disadvantage because the Chinese cared little for their goods, while European demand for Chinese commodities such as tea and porcelain only grew. In order to tip the trade imbalance in their favor, British merchants began to sell Indian opium to the Chinese. Not only did this sap Chinese bullion reserves, it also led to widespread drug addiction amongst the bureaucracy and society in general. A ban was placed on opium as early as 1729 by the Yongzheng Emperor, but little was done to enforce it. By the early 19th century, under the new Daoguang Emperor, the government began serious efforts to eradicate opium from Chinese society. Leading this endeavour were respected scholar-officials including Imperial Commissioner Lin Zexu.

After Lin destroyed more than 20,000 chests of opium in the summer of 1839, Europeans demanded compensation for what they saw as unwarranted Chinese interference in their affairs. When it was not paid, the British declared war later the same year, starting what became known as the First Opium War. The outdated Chinese junks were no match for the advanced British gunboats, and soon the Yangzi River region came under threat of British bombardment and invasion. The emperor had no choice but to sue for peace, resulting in the exile of Lin and the making of the Treaty of Nanking, which ceded the British control of Hong Kong and opened up trade and diplomacy with other European countries, including Germany, France, and the USA.

Northeast China came under influence of Russia with the building of the Chinese Eastern Railway through Harbin to Vladivostok. The Empire of Japan replaced Russian influence in the region as a result of the Russo-Japanese War in 1904–1905, and Japan laid the South Manchurian Railway in 1906 to Port Arthur. During the Warlord Era in China, Zhang Zuolin established himself in Northeast China, but was murdered by the Japanese for being too independent. The former Chinese emperor, Puyi, was then placed on the throne to lead a Japanese puppet state of Manchukuo. In August 1945, the Soviet Union invaded the region. From 1945 to 1948, Northeast China was a base area for Mao Zedong's People's Liberation Army in the Chinese Civil War. With the encouragement of the Kremlin, the area was used as a staging ground during the Civil War for the Chinese Communists, who were victorious in 1949 and have controlled ever since.
When it became the 19th century, the king of Joseon was powerless. Because the noble family of the king's wife got the power and ruled the country by their way. The 26th king of Joseon dynasty, Gojong's father, Heungseon Daewongun wanted the king be powerful again. Even he wasn't the king. As the father of young king, he destroyed noble families and corrupt organizations. So the royal family got the power again. But he wanted to rebuild Gyeongbokgung palace in order to show the royal power to people. So he was criticized by people because he spent enormous money and inflation occurred because of that. So his son, the real king Gojong got power.

The 26th king of Joseon, Gojong changed the nation's name to "Daehan Jeguk". It means the Korean Empire. And he also promoted himself as an emperor. The new empire accepted more western technology and strengthened military power. And Korean Empire was going to become a Neutral Nation. Unfortunately, in the Russo-Japanese war, Japan ignored this, and eventually Japan won against Russian Empire, and started to invade Korea. Japan first stole the right of diplomacy from Korean Empire illegally. But every western country ignored this invasion because they knew Japan became a strong country as they defeated Russian Empire. So emperor Gojong sent diplomats to a Dutch city known as The Hague to let everyone know that Japan stole the Empire's right illegally. But it was failed. Because the diplomats couldn't go into the conference room. Japan kicked Gojong off on the grounds that this reason. 3 years after, In 1910, Korean Empire became a part of Empire of Japan. It was the first time ever after invasion of Han dynasty in 108 BC.

The European powers had control of other parts of Asia by the early 20th century, such as British India, French Indochina, Spanish East Indies, and Portuguese Macau and Goa. The Great Game between Russia and Britain was the struggle for power in the Central Asian region in the nineteenth century. The Trans-Siberian Railway, crossing Asia by train, was complete by 1916. Parts of Asia remained free from European control, although not influence, such as Persia, Thailand and most of China. In the twentieth century, Imperial Japan expanded into China and Southeast Asia during the World War II. After the war, many Asian countries became independent from European powers. During the Cold War, the northern parts of Asia were communist controlled with the Soviet Union and People's Republic of China, while western allies formed pacts such as CENTO and SEATO. Conflicts such as the Korean War, Vietnam War and Soviet invasion of Afghanistan were fought between communists and anti-communists. In the decades after the Second World War, a massive restructuring plan drove Japan to become the world's second-largest economy, a phenomenon known as the Japanese post-war economic miracle. The Arab–Israeli conflict has dominated much of the recent history of the Middle East. After the Soviet Union's collapse in 1991, there were many new independent nations in Central Asia.

Prior to World War II, China faced a civil war between Mao Zedong's Communist party and Chiang Kai-shek's nationalist party; the nationalists appeared to be in the lead. However, once the Japanese invaded in 1937, the two parties were forced to form a temporary cease-fire in order to defend China. The nationalists faced many military failures that caused them to lose territory and subsequently, respect from the Chinese masses. In contrast, the communists' use of guerilla warfare (led by Lin Biao) proved effective against the Japanese's conventional methods and put the Communist Party on top by 1945. They also gained popularity for the reforms they were already applying in controlled areas, including land redistribution, education reforms, and widespread health care. For the next four years, the nationalists would be forced to retreat to the small island east of China, known as Taiwan (formerly known as Formosa), where they remain today. In mainland China, People's Republic of China was established by the Communist Party, with Mao Zedong as its state chairman.

The communist government in China was defined by the party cadres. These hard-line officers controlled the People's Liberation Army, which itself controlled large amounts of the bureaucracy. This system was further controlled by the Central Committee, which additionally supported the state chairman who was considered the head of the government. The People's Republic's foreign policies included the repressing of secession attempts in Mongolia and Tibet and supporting of North Korea and North Vietnam in the Korean War and Vietnam War, respectively. Additionally, by 1960 China began to cut off its connections with the Soviet Union due to border disputes and an increasing Chinese sense of superiority, especially the personal feeling of Mao over the Russian premier, Nikita Khrushchev.

Today China plays important roles in world economics and politics. China today is the world's second largest economy and the second fastest growing economy.

During the period when the Korean War occurred, Korea divided into North and South. Syngman Rhee became the first president of South Korea, and Kim Il-sung became the supreme leader of North Korea. After the war, the president of South Korea, Syngman Rhee tries to become a dictator. So the April Revolution occurred, eventually Syngman Rhee was exiled from his country. 
In 1963, Park Chung-hee was empowered with a military coup d'état. He dispatched Republic of Korea Army to Vietnam War. And during this age, the economy of South Korea outran that of North Korea.

Although Park Chung-hee improved the nation's economy, he was a dictator, so people didn't like him. Eventually, he is murdered by Kim Jae-gyu. In 1979, Chun Doo-hwan was empowered by another coup d’état by military. He oppressed the resistances in the city of Gwangju. That event is called 'Gwangju Uprising'. Despite the Gwangju Uprising, Chun Doo-hwan became the president. But the people resisted again in 1987. This movement is called 'June Struggle'. As a result of Gwangju Uprising and June Struggle, South Korea finally became a democratic republic in 1987. 

Roh Tae-woo (1988–93), Kim Young-sam (1993–98), Kim Dae-jung (1998–2003), Roh Moo-hyun (2003–2008), Lee Myung-bak (2008–2013), Park Geun-hye (2013–2017), Moon Jae-in (2017–) were elected as a president in order after 1987. In 1960, North Korea was far more wealthier than South Korea. But in 1970, South Korea begins to outrun the North Korean economy. In 2018, South Korea is ranked #10 in world GDP ranking.






</doc>
<doc id="14098" url="https://en.wikipedia.org/wiki?curid=14098" title="History of the Americas">
History of the Americas

The prehistory of the Americas (North, South, and Central America, and the Caribbean) begins with people migrating to these areas from Asia during the height of an Ice Age. These groups are generally believed to have been isolated from the people of the "Old World" until the coming of Europeans in the 10th century from Iceland led by Leif Erikson and with the voyages of Christopher Columbus in 1492.

The ancestors of today's American Indigenous peoples were the Paleo-Indians; they were hunter-gatherers who migrated into North America. The most popular theory asserts that migrants came to the Americas via Beringia, the land mass now covered by the ocean waters of the Bering Strait. Small lithic stage peoples followed megafauna like bison, mammoth (now extinct), and caribou, thus gaining the modern nickname "big-game hunters." Groups of people may also have traveled into North America on shelf or sheet ice along the northern Pacific coast.

Cultural traits brought by the first immigrants later evolved and spawned such cultures as Iroquois on North America and Pirahã of South America. These cultures later developed into civilizations. In many cases, these cultures expanded at a later date than their Old World counterparts. Cultures that may be considered advanced or civilized include Norte Chico, Cahokia, Zapotec, Toltec, Olmec, Maya, Aztec, Chimor, Mixtec, Moche, Mississippian, Puebloan, Totonac, Teotihuacan, Huastec people, Purépecha, Izapa, Mazatec, Muisca, and the Inca.

After the voyages of Christopher Columbus in 1492, Spanish and later Portuguese, English, French and Dutch colonial expeditions arrived in the New World, conquering and settling the discovered lands, which led to a transformation of the cultural and physical landscape in the Americas. Spain colonized most of the Americas from present-day Southwestern United States, Florida and the Caribbean to the southern tip of South America. Portugal settled in what is mostly present-day Brazil while England established colonies on the Eastern coast of the United States, as well as the North Pacific coast and in most of Canada. France settled in Quebec and other parts of Eastern Canada and claimed an area in what is today the central United States. The Netherlands settled New Netherland (administrative centre New Amsterdam - now New York), some Caribbean islands and parts of Northern South America.

European colonization of the Americas led to the rise of new cultures, civilizations and eventually states, which resulted from the fusion of Native American and European traditions, peoples and institutions. The transformation of American cultures through colonization is evident in architecture, religion, gastronomy, the arts and particularly languages, the most widespread being Spanish (376 million speakers), English (348 million) and Portuguese (201 million). The colonial period lasted approximately three centuries, from the early 16th to the early 19th centuries, when Brazil and the larger Hispanic American nations declared independence. The United States obtained independence from Great Britain much earlier, in 1776, while Canada formed a federal dominion in 1867 and received legal independence in 1931. Others remained attached to their European parent state until the end of the 19th century, such as Cuba and Puerto Rico which were linked to Spain until 1898. Smaller territories such as Guyana obtained independence in the mid-20th century, while certain Caribbean islands and French Guiana remain part of a European power to this day.

 
The specifics of Paleo-Indian migration to and throughout the Americas, including the exact dates and routes traveled, are subject to ongoing research and discussion. The traditional theory has been that these early migrants moved into the Beringia land bridge between eastern Siberia and present-day Alaska around 40,000 – 17,000 years ago, when sea levels were significantly lowered due to the Quaternary glaciation. These people are believed to have followed herds of now-extinct Pleistocene megafauna along "ice-free corridors" that stretched between the Laurentide and Cordilleran ice sheets. Another route proposed is that, either on foot or using primitive boats, they migrated down the Pacific Northwest coast to South America. Evidence of the latter would since have been covered by a sea level rise of a hundred meters following the last ice age.

Archaeologists contend that the Paleo-Indian migration out of Beringia (eastern Alaska), ranges from 40,000 to around 16,500 years ago. This time range is a hot source of debate. The few agreements achieved to date are the origin from Central Asia, with widespread habitation of the Americas during the end of the last glacial period, or more specifically what is known as the late glacial maximum, around 16,000 – 13,000 years before present.

The American Journal of Human Genetics released an article in 2007 stating "Here we show, by using 86 complete mitochondrial genomes, that all Indigenous American haplogroups, including Haplogroup X (mtDNA), were part of a single founding population." Amerindian groups in the Bering Strait region exhibit perhaps the strongest DNA or mitochondrial DNA relations to Siberian peoples. The genetic diversity of Amerindian indigenous groups increase with distance from the assumed entry point into the Americas. Certain genetic diversity patterns from West to East suggest, particularly in South America, that migration proceeded first down the west coast, and then proceeded eastward. Geneticists have variously estimated that peoples of Asia and the Americas were part of the same population from 42,000 to 21,000 years ago.

New studies shed light on the founding population of indigenous Americans, suggesting that their ancestry traced to both east Asian and western Eurasians who migrated to North America directly from Siberia. A 2013 study in the journal Nature reported that DNA found in the 24,000-year-old remains of a young boy in Mal’ta Siberia suggest that up to one-third of the indigenous Americans may have ancestry that can be traced back to western Eurasians, who may have "had a more north-easterly distribution 24,000 years ago than commonly thought" Professor Kelly Graf said that "Our findings are significant at two levels. First, it shows that Upper Paleolithic Siberians came from a cosmopolitan population of early modern humans that spread out of Africa to Europe and Central and South Asia. Second, Paleoindian skeletons with phenotypic traits atypical of modern-day Native Americans can be explained as having a direct historical connection to Upper Paleolithic Siberia." A route through Beringia is seen as more likely than the Solutrean hypothesis.

On October 3, 2014, the Oregon cave where the oldest DNA evidence of human habitation in North America was found was added to the National Register of Historic Places. The DNA, radiocarbon dated to 14,300 years ago, was found in fossilized human coprolites uncovered in the Paisley Five Mile Point Caves in south central Oregon.

The Lithic stage or "Paleo-Indian period", is the earliest classification term referring to the first stage of human habitation in the Americas, covering the Late Pleistocene epoch. The time period derives its name from the appearance of "Lithic flaked" stone tools. Stone tools, particularly projectile points and scrapers, are the primary evidence of the earliest well known human activity in the Americas. Lithic reduction stone tools are used by archaeologists and anthropologists to classify cultural periods.

Several thousand years after the first migrations, the first complex civilizations arose as hunter-gatherers settled into semi-agricultural communities. Identifiable sedentary settlements began to emerge in the so-called Middle Archaic period around 6000 BCE. Particular archaeological cultures can be identified and easily classified throughout the Archaic period.

In the late Archaic, on the north-central coastal region of Peru, a complex civilization arose which has been termed the Norte Chico civilization, also known as Caral-Supe. It is the oldest known civilization in the Americas and one of the five sites where civilization originated independently and indigenously in the ancient world, flourishing between the 30th and 18th centuries BC. It pre-dated the Mesoamerican Olmec civilization by nearly two millennia. It was contemporaneous with the Egypt following the unification of its kingdom under Narmer and the emergence of the first Egyptian hieroglyphics.

Monumental architecture, including earthwork platform mounds and sunken plazas have been identified as part of the civilization. Archaeological evidence points to the use of textile technology and the worship of common god symbols. Government, possibly in the form of theocracy, is assumed to have been required to manage the region. However, numerous questions remain about its organization. In archaeological nomenclature, the culture was pre-ceramic culture of the pre-Columbian Late Archaic period. It appears to have lacked ceramics and art.

Ongoing scholarly debate persists over the extent to which the flourishing of Norte Chico resulted from its abundant maritime food resources, and the relationship that these resources would suggest between coastal and inland sites.

The role of seafood in the Norte Chico diet has been a subject of scholarly debate. In 1973, examining the Aspero region of Norte Chico, Michael E. Moseley contended that a maritime subsistence (seafood) economy had been the basis of society and its early flourishing. This theory, later termed "maritime foundation of Andean Civilization" was at odds with the general scholarly consensus that civilization arose as a result of intensive grain-based agriculture, as had been the case in the emergence of civilizations in northeast Africa (Egypt) and southwest Asia (Mesopotamia).

While earlier research pointed to edible domestic plants such as squash, beans, lucuma, guava, pacay, and camote at Caral, publications by Haas and colleagues have added avocado, achira, and maize (Zea Mays) to the list of foods consumed in the region. In 2013, Haas and colleagues reported that maize was a primary component of the diet throughout the period of 3000 to 1800 BC.

Cotton was another widespread crop in Norte Chico, essential to the production of fishing nets and textiles. Jonathan Haas noted a mutual dependency, whereby "The prehistoric residents of the Norte Chico needed the fish resources for their protein and the fishermen needed the cotton to make the nets to catch the fish."

In the 2005 book "", journalist Charles C. Mann surveyed the literature at the time, reporting a date "sometime before 3200 BC, and possibly before 3500 BC" as the beginning date for the formation of Norte Chico. He notes that the earliest date securely associated with a city is 3500 BC, at Huaricanga in the (inland) Fortaleza area.

The Norte Chico civilization began to decline around 1800 BC as more powerful centers appeared to the south and north along its coast, and to the east within the Andes Mountains.

After the decline of the Norte Chico civilization, several large, centralized civilizations developed in the Western Hemisphere: Chavin, Nazca, Moche, Huari, Quitus, Cañaris, Chimu, Pachacamac, Tiahuanaco, Aymara and Inca in the Central Andes (Ecuador, Peru and Bolivia); Muisca in Colombia ; Taínos in Dominican Republic (Hispaniola, Española) and part of Caribbean; and the Olmecs, Maya, Toltecs, Mixtecs, Zapotecs, Aztecs and Purepecha in southern North America (Mexico, Guatemala).

The Olmec civilization was the first Mesoamerican civilization, beginning around 1600-1400 BC and ending around 400 BC. Mesoamerica is considered one of the six sites around the globe in which civilization developed independently and indigenously. This civilization is considered the mother culture of the Mesoamerican civilizations. The Mesoamerican calendar, numeral system, writing, and much of the Mesoamerican pantheon seem to have begun with the Olmec.

Some elements of agriculture seem to have been practiced in Mesoamerica quite early. The domestication of maize is thought to have begun around 7,500 to 12,000 years ago. The earliest record of lowland maize cultivation dates to around 5100 BC. Agriculture continued to be mixed with a hunting-gathering-fishing lifestyle until quite late compared to other regions, but by 2700 BC, Mesoamericans were relying on maize, and living mostly in villages. Temple mounds and classes started to appear. By 1300/ 1200 BC, small centres coalesced into the Olmec civilization, which seems to have been a set of city-states, united in religious and commercial concerns. The Olmec cities had ceremonial complexes with earth/clay pyramids, palaces, stone monuments, aqueducts and walled plazas. The first of these centers was at San Lorenzo (until 900 bc). La Venta was the last great Olmec centre. Olmec artisans sculpted jade and clay figurines of Jaguars and humans. Their iconic giant heads - believed to be of Olmec rulers - stood in every major city.

The Olmec civilization ended in 400 BC, with the defacing and destruction of San Lorenzo and La Venta, two of the major cities. It nevertheless spawned many other states, most notably the Mayan civilization, whose first cities began appearing around 700-600 BC. Olmec influences continued to appear in many later Mesoamerican civilizations.

Cities of the Aztecs, Mayas, and Incas were as large and organized as the largest in the Old World, with an estimated population of 200,000 to 350,000 in Tenochtitlan, the capital of the Aztec Empire. The market established in the city was said to have been the largest ever seen by the conquistadors when they arrived. The capital of the Cahokians, Cahokia, located near modern East St. Louis, Illinois, may have reached a population of over 20,000. At its peak, between the 12th and 13th centuries, Cahokia may have been the most populous city in North America. Monk's Mound, the major ceremonial center of Cahokia, remains the largest earthen construction of the prehistoric New World.

These civilizations developed agriculture as well, breeding maize (corn) from having ears 2–5 cm in length to perhaps 10–15 cm in length. Potatoes, tomatoes, beans (greens), pumpkins, avocados, and chocolate are now the most popular of the pre-Columbian agricultural products. The civilizations did not develop extensive livestock as there were few suitable species, although alpacas and llamas were domesticated for use as beasts of burden and sources of wool and meat in the Andes. By the 15th century, maize was being farmed in the Mississippi River Valley after introduction from Mexico. The course of further agricultural development was greatly altered by the arrival of Europeans.


Cahokia was a major regional chiefdom, with trade and tributary chiefdoms located in a range of areas from bordering the Great Lakes to the Gulf of Mexico.


The Iroquois League of Nations or "People of the Long House", based in present-day upstate and western New York, had a confederacy model from the mid-15th century. It has been suggested that their culture contributed to political thinking during the development of the later United States government. Their system of affiliation was a kind of federation, different from the strong, centralized European monarchies.

Leadership was restricted to a group of 50 sachem chiefs, each representing one clan within a tribe; the Oneida and Mohawk people had nine seats each; the Onondagas held fourteen; the Cayuga had ten seats; and the Seneca had eight. Representation was not based on population numbers, as the Seneca tribe greatly outnumbered the others. When a sachem chief died, his successor was chosen by the senior woman of his tribe in consultation with other female members of the clan; property and hereditary leadership were passed matrilineally. Decisions were not made through voting but through consensus decision making, with each sachem chief holding theoretical veto power. The Onondaga were the "firekeepers", responsible for raising topics to be discussed. They occupied one side of a three-sided fire (the Mohawk and Seneca sat on one side of the fire, the Oneida and Cayuga sat on the third side.)

Elizabeth Tooker, an anthropologist, has said that it was unlikely the US founding fathers were inspired by the confederacy, as it bears little resemblance to the system of governance adopted in the United States. For example, it is based on inherited rather than elected leadership, selected by female members of the tribes, consensus decision-making regardless of population size of the tribes, and a single group capable of bringing matters before the legislative body.

Long-distance trading did not prevent warfare and displacement among the indigenous peoples, and their oral histories tell of numerous migrations to the historic territories where Europeans encountered them. The Iroquois invaded and attacked tribes in the Ohio River area of present-day Kentucky and claimed the hunting grounds. Historians have placed these events as occurring as early as the 13th century, or in the 17th century Beaver Wars.

Through warfare, the Iroquois drove several tribes to migrate west to what became known as their historically traditional lands west of the Mississippi River. Tribes originating in the Ohio Valley who moved west included the Osage, Kaw, Ponca and Omaha people. By the mid-17th century, they had resettled in their historical lands in present-day Kansas, Nebraska, Arkansas and Oklahoma. The Osage warred with Caddo-speaking Native Americans, displacing them in turn by the mid-18th century and dominating their new historical territories.


The Pueblo people of what is now occupied by the Southwestern United States and northern Mexico, living conditions were that of large stone apartment like adobe structures. They live in Arizona, New Mexico, Utah, Colorado, and possibly surrounding areas.

Chichimeca was the name that the Mexica (Aztecs) generically applied to a wide range of semi-nomadic peoples who inhabited the north of modern-day Mexico, and carried the same sense as the European term "barbarian". The name was adopted with a pejorative tone by the Spaniards when referring especially to the semi-nomadic hunter-gatherer peoples of northern Mexico.

The Olmec civilization emerged around 1200 BCE in Mesoamerica and ended around 400 BCE. Olmec art and concepts influenced surrounding cultures after their downfall. This civilization was thought to be the first in America to develop a writing system. After the Olmecs abandoned their cities for unknown reasons, the Maya, Zapotec and Teotihuacan arose.

The Purepecha civilization emerged around 1000 CE in Mesoamerica . They flourished from 1100 CE to 1530 CE. They continue to live on in the state of Michoacán. Fierce warriors, they were never conquered and in their glory years, successfully sealed off huge areas from Aztec domination.


Maya history spans 3,000 years. The Classic Maya may have collapsed due to changing climate in the end of the 10th century.

The Toltec were a nomadic people, dating from the 10th - 12th century, whose language was also spoken by the Aztecs.

Teotihuacan (4th century BCE - 7/8th century CE) was both a city, and an empire of the same name, which, at its zenith between 150 and the 5th century, covered most of Mesoamerica.

The Aztec having started to build their empire around 14th century found their civilization abruptly ended by the Spanish conquistadors. They lived in Mesoamerica, and surrounding lands. Their capital city Tenochtitlan was one of the largest cities of all time.

The oldest known civilization of the Americas was established in the Norte Chico region of modern Peru. Complex society emerged in the group of coastal valleys, between 3000 and 1800 BCE. The Quipu, a distinctive recording device among Andean civilizations, apparently dates from the era of Norte Chico's prominence.

The Chavín established a trade network and developed agriculture by as early as (or late compared to the Old World) 900 BCE according to some estimates and archaeological finds. Artifacts were found at a site called Chavín in modern Peru at an elevation of 3,177 meters. Chavín civilization spanned from 900 BCE to 300 BCE.

Holding their capital at the great city of Cusco, the Inca civilization dominated the Andes region from 1438 to 1533.
Known as "Tahuantinsuyu", or "the land of the four regions", in Quechua, the Inca culture was highly distinct and developed. Cities were built with precise, unmatched stonework, constructed over many levels of mountain terrain. Terrace farming was a useful form of agriculture. There is evidence of excellent metalwork and even successful trepanation of the skull in Inca civilization.

Around 1000, the Vikings established a short-lived settlement in Newfoundland, now known as L'Anse aux Meadows. Speculations exist about other Old World discoveries of the New World, but none of these are generally or completely accepted by most scholars.

Spain sponsored a major exploration led by Italian explorer Christopher Columbus in 1492; it quickly led to extensive European colonization of the Americas. The Europeans brought Old World diseases which are thought to have caused catastrophic epidemics and a huge decrease of the native population. Columbus came at a time in which many technical developments in sailing techniques and communication made it possible to report his voyages easily and to spread word of them throughout Europe. It was also a time of growing religious, imperial and economic rivalries that led to a competition for the establishment of colonies.

15th to 19th century colonies in the New World:

The formation of sovereign states in the New World began with the United States Declaration of Independence of 1776. The American Revolutionary War lasted through the period of the Siege of Yorktown — its last major campaign — in the early autumn of 1781, with peace being achieved in 1783.

The Spanish colonies won their independence in the first quarter of the 19th century, in the Spanish American wars of independence. Simón Bolívar and José de San Martín, among others, led their independence struggle. Although Bolivar attempted to keep the Spanish-speaking parts of Latin America politically allied, they rapidly became independent of one another as well, and several further wars were fought, such as the Paraguayan War and the War of the Pacific. (See Latin American integration.) In the Portuguese colony Dom Pedro I (also Pedro IV of Portugal), son of the Portuguese king Dom João VI, proclaimed the country's independence in 1822 and became Brazil's first Emperor. This was peacefully accepted by the crown in Portugal, upon compensation.

Slavery has had a significant role in the economic development of the New World after the colonization of the Americas by the Europeans. The cotton, tobacco, and sugar cane harvested by slaves became important exports for the United States and the Caribbean countries.

As a part of the British Empire, Canada immediately entered World War I when it broke out in 1914. Canada bore the brunt of several major battles during the early stages of the war, including the use of poison gas attacks at Ypres. Losses became grave, and the government eventually brought in conscription, despite the fact this was against the wishes of the majority of French Canadians. In the ensuing Conscription Crisis of 1917, riots broke out on the streets of Montreal. In neighboring Newfoundland, the new dominion suffered a devastating loss on July 1, 1916, the First day on the Somme.

The United States stayed out of the conflict until 1917, when it joined the Entente powers. The United States was then able to play a crucial role at the Paris Peace Conference of 1919 that shaped interwar Europe. Mexico was not part of the war, as the country was embroiled in the Mexican Revolution at the time.

The 1920s brought an age of great prosperity in the United States, and to a lesser degree Canada. But the Wall Street Crash of 1929 combined with drought ushered in a period of economic hardship in the United States and Canada. From 1936 to 1949, there was a popular uprising against the anti-Catholic Mexican government of the time, set off specifically by the anti-clerical provisions of the Mexican Constitution of 1917.

Once again, Canada found itself at war before its neighbors, with numerically modest but significant contributions overseas such as the Battle of Hong Kong and the Battle of Britain. The entry of the United States into the war helped to tip the balance in favour of the allies. Two Mexican tankers, transporting oil to the United States, were attacked and sunk by the Germans in the Gulf of Mexico waters, in 1942. The incident happened in spite of Mexico's neutrality at that time. This led Mexico to enter the conflict with a declaration of war on the Axis nations. The destruction of Europe wrought by the war vaulted all North American countries to more important roles in world affairs, especially the United States, which emerged as a "superpower".

The early Cold War era saw the United States as the most powerful nation in a Western coalition of which Mexico and Canada were also a part. In Canada, Quebec was transformed by the Quiet Revolution and the emergence of Quebec nationalism. Mexico experienced an era of huge economic growth after World War II, a heavy industrialization process and a growth of its middle class, a period known in Mexican history as ""El Milagro Mexicano"" (the Mexican miracle). The Caribbean saw the beginnings of decolonization, while on the largest island the Cuban Revolution introduced Cold War rivalries into Latin America.

The civil rights movement in the U.S. ended Jim Crow and empowered black voters in the 1960s, which allowed black citizens to move into high government offices for the first time since Reconstruction. However, the dominant New Deal coalition collapsed in the mid 1960s in disputes over race and the Vietnam War, and the conservative movement began its rise to power, as the once dominant liberalism weakened and collapsed. Canada during this era was dominated by the leadership of Pierre Elliot Trudeau. In 1982, at the end of his tenure, Canada enshrined a new constitution.

Canada's Brian Mulroney not only ran on a similar platform but also favored closer trade ties with the United States. This led to the Canada-United States Free Trade Agreement in January 1989. Mexican presidents Miguel de la Madrid, in the early 1980s and Carlos Salinas de Gortari in the late 1980s, started implementing liberal economic strategies that were seen as a good move. However, Mexico experienced a strong economic recession in 1982 and the Mexican peso suffered a devaluation. In the United States president Ronald Reagan attempted to move the United States back towards a hard anti-communist line in foreign affairs, in what his supporters saw as an attempt to assert moral leadership (compared to the Soviet Union) in the world community. Domestically, Reagan attempted to bring in a package of privatization and regulation to stimulate the economy.

The end of the Cold War and the beginning of the era of sustained economic expansion coincided during the 1990s. On January 1, 1994, Canada, Mexico and the United States signed the North American Free Trade Agreement, creating the world's largest free trade area. In 2000, Vicente Fox became the first non-PRI candidate to win the Mexican presidency in over 70 years. The optimism of the 1990s was shattered by the 9/11 attacks of 2001 on the United States, which prompted military intervention in Afghanistan, which also involved Canada. Canada did not support the United States' later move to invade Iraq, however.

In the U.S. the Reagan Era of conservative national policies, deregulation and tax cuts took control with the election of Ronald Reagan in 1980. By 2010, political scientists were debating whether the election of Barack Obama in 2008 represented an end of the Reagan Era, or was only a reaction against the bubble economy of the 2000s (decade), which burst in 2008 and became the Late-2000s recession with prolonged unemployment.

Despite the failure of a lasting political union, the concept of Central American reunification, though lacking enthusiasm from the leaders of the individual countries, rises from time to time. In 1856–1857 the region successfully established a military coalition to repel an invasion by United States adventurer William Walker. Today, all five nations fly flags that retain the old federal motif of two outer blue bands bounding an inner white stripe. (Costa Rica, traditionally the least committed of the five to regional integration, modified its flag significantly in 1848 by darkening the blue and adding a double-wide inner red band, in honor of the French tricolor).

In 1907, a Central American Court of Justice was created. On December 13, 1960, Guatemala, El Salvador, Honduras, and Nicaragua established the Central American Common Market ("CACM"). Costa Rica, because of its relative economic prosperity and political stability, chose not to participate in the CACM. The goals for the CACM were to create greater political unification and success of import substitution industrialization policies. The project was an immediate economic success, but was abandoned after the 1969 "Football War" between El Salvador and Honduras. A Central American Parliament has operated, as a purely advisory body, since 1991. Costa Rica has repeatedly declined invitations to join the regional parliament, which seats deputies from the four other former members of the Union, as well as from Panama and the Dominican Republic.

In the 1960s and 1970s, the governments of Argentina, Brazil, Chile, and Uruguay were overthrown or displaced by U.S.-aligned military dictatorships. These dictatorships detained tens of thousands of political prisoners, many of whom were tortured and/or killed (on inter-state collaboration, see Operation Condor). Economically, they began a transition to neoliberal economic policies. They placed their own actions within the United States Cold War doctrine of "National Security" against internal subversion. Throughout the 1980s and 1990s, Peru suffered from an internal conflict (see Túpac Amaru Revolutionary Movement and Shining Path). Revolutionary movements and right-wing military dictatorships have been common, but starting in the 1980s a wave of democratization came through the continent, and democratic rule is widespread now. Allegations of corruption remain common, and several nations have seen crises which have forced the resignation of their presidents, although normal civilian succession has continued.

International indebtedness became a notable problem, as most recently illustrated by Argentina's default in the early 21st century. In recent years, South American governments have drifted to the left, with socialist leaders being elected in Chile, Bolivia, Brazil, Venezuela, and a leftist president in Argentina and Uruguay. Despite the move to the left, South America is still largely capitalist. With the founding of the Union of South American Nations, South America has started down the road of economic integration, with plans for political integration in the European Union style.



</doc>
<doc id="14099" url="https://en.wikipedia.org/wiki?curid=14099" title="History of Africa">
History of Africa

The history of Africa begins with the emergence of hominids, archaic humans and—at least 200,000 years ago—anatomically modern humans ("Homo sapiens"), in East Africa, and continues unbroken into the present as a patchwork of diverse and politically developing nation states.The earliest known recorded history arose in Ancient Egypt, and later in Nubia, the Sahel, the Maghreb and the Horn of Africa.

Following the desertification of the Sahara, North African history became entwined with the Middle East and Southern Europe while the Bantu expansion swept from modern day Cameroon (Central Africa) across much of the sub-Saharan continent in waves between around 1000 BC and 1 AD, creating a linguistic commonality across much of the central and Southern continent.

During the Middle Ages, Islam spread west from Arabia to Egypt, crossing the Maghreb and the Sahel. Some notable pre-colonial states and societies in Africa include the Ajuran Empire, D'mt, Adal Sultanate, Alodia, Warsangali Sultanate, Kingdom of Nri, Nok culture, Mali Empire, Bono State, Songhai Empire, Benin Empire, Oyo Empire, Kingdom of Lunda (Punu-yaka), Ashanti Empire, Ghana Empire, Mossi Kingdoms, Mutapa Empire, Kingdom of Mapungubwe, Kingdom of Sine, Kingdom of Sennar, Kingdom of Saloum, Kingdom of Baol, Kingdom of Cayor, Kingdom of Zimbabwe, Kingdom of Kongo, Empire of Kaabu, Kingdom of Ile Ife, Ancient Carthage, Numidia, Mauretania, and the Aksumite Empire. At its peak, prior to European colonialism, it is estimated that Africa had up to 10,000 different states and autonomous groups with distinct languages and customs.<ref name="http://newswatch.nationalgeographic.com/2013/10/31/getting-to-know-africa-50-facts/">Africa Information</ref>

From the mid-7th century, the Arab slave trade saw Arabs enslave Africans. Following an armistice between the Rashidun Caliphate and the Kingdom of Makuria after the Second Battle of Dongola in 652 AD, they were transported, along with Asians and Europeans, across the Red Sea, Indian Ocean, and Sahara Desert.

From the late 15th century, Europeans joined the slave trade. One could say the Portuguese led in partnership with other Europeans. That includes the triangular trade, with the Portuguese initially acquiring slaves through trade and later by force as part of the Atlantic slave trade. They transported enslaved West, Central, and Southern Africans overseas. Subsequently, European colonization of Africa developed rapidly from around 10% (1870) to over 90% (1914) in the Scramble for Africa (1881–1914). However following struggles for independence in many parts of the continent, as well as a weakened Europe after the Second World War , decolonization took place across the continent, culminating in the 1960 Year of Africa.

Disciplines such as recording of oral history, historical linguistics, archaeology and genetics have been vital in rediscovering the great African civilizations of antiquity.

The first known hominids evolved in Africa. According to paleontology, the early hominids' skull anatomy was similar to that of the gorilla and the chimpanzee, great apes that also evolved in Africa, but the hominids had adopted a bipedal locomotion which freed their hands. This gave them a crucial advantage, enabling them to live in both forested areas and on the open savanna at a time when Africa was drying up and the savanna was encroaching on forested areas. This would have occurred 10 to 5 million years ago, but these claims are controversial because biologists and genetics have humans appearing around the last 70 thousand to 200 thousand years.

By 4 million years ago, several australopithecine hominid species had developed throughout Southern, Eastern and Central Africa. They were tool users, and makers of tools. They scavenged for meat and were omnivores.

By approximately 3.3 million years ago, primitive stone tools were first used to scavenge kills made by other predators and to harvest carrion and marrow from their bones. In hunting, "Homo habilis" was probably not capable of competing with large predators and was still more prey than hunter. "H. habilis" probably did steal eggs from nests and may have been able to catch small game and weakened larger prey (cubs and older animals). The tools were classed as Oldowan.

Around 1.8 million years ago, "Homo ergaster" first appeared in the fossil record in Africa. From "Homo ergaster", "Homo erectus" evolved 1.5 million years ago. Some of the earlier representatives of this species were still fairly small-brained and used primitive stone tools, much like "H. habilis". The brain later grew in size, and "H. erectus" eventually developed a more complex stone tool technology called the Acheulean. Possibly the first hunters, "H. erectus" mastered the art of making fire and was the first hominid to leave Africa, colonizing most of Afro-Eurasia and perhaps later giving rise to "Homo floresiensis". Although some recent writers have suggested that "Homo georgicus" was the first and primary hominid ever to live outside Africa, many scientists consider "H. georgicus" to be an early and primitive member of the "H. erectus" species.
The fossil record shows "Homo sapiens" (also known as "modern humans" or "anatomically modern humans") living in Africa by about 350,000-260,000 years ago. The earliest known "Homo sapiens" fossils include the Jebel Irhoud remains from Morocco (ca. 315,000 years ago), the Florisbad Skull from South Africa (ca. 259,000 years ago), and the Omo remains from Ethiopia (ca. 195,000 years ago). Scientists have suggested that "Homo sapiens" may have arisen between 350,000 and 260,000 years ago through a merging of populations in East Africa and South Africa.

Evidence of a variety behaviors indicative of Behavioral modernity date to the African Middle Stone Age, associated with early "Homo sapiens" and their emergence. Abstract imagery, widened subsistence strategies, and other "modern" behaviors have been discovered from that period in Africa, especially South, North, and East Africa. The Blombos Cave site in South Africa, for example, is famous for rectangular slabs of ochre engraved with geometric designs. Using multiple dating techniques, the site was confirmed to be around 77,000 and 100–75,000 years old. Ostrich egg shell containers engraved with geometric designs dating to 60,000 years ago were found at Diepkloof, South Africa. Beads and other personal ornamentation have been found from Morocco which might be as much as 130,000 years old; as well, the Cave of Hearths in South Africa has yielded a number of beads dating from significantly prior to 50,000 years ago., and shell beads dating to about 75,000 years ago have been found at Blombos Cave, South Africa.

Specialized projectile weapons as well have been found at various sites in Middle Stone Age Africa, including bone and stone arrowheads at South African sites such as Sibudu Cave (along with an early bone needle also found at Sibudu) dating approximately 60,000-70,000 years ago, and bone harpoons at the Central African site of Katanda dating to about 90,000 years ago. Evidence also exists for the systematic heat treating of silcrete stone to increased its flake-ability for the purpose of toolmaking, beginning approximately 164,000 years ago at the South African site of Pinnacle Point and becoming common there for the creation of microlithic tools at about 72,000 years ago. Early stone-tipped projectile weapons (a characteristic tool of "Homo sapiens"), the stone tips of javelins or throwing spears, were discovered in 2013 at the Ethiopian site of Gademotta, and date to around 279,000 years ago.

In 2008, an ochre processing workshop likely for the production of paints was uncovered dating to ca. 100,000 years ago at Blombos Cave, South Africa. Analysis shows that a liquefied pigment-rich mixture was produced and stored in the two abalone shells, and that ochre, bone, charcoal, grindstones and hammer-stones also formed a composite part of the toolkits. Evidence for the complexity of the task includes procuring and combining raw materials from various sources (implying they had a mental template of the process they would follow), possibly using pyrotechnology to facilitate fat extraction from bone, using a probable recipe to produce the compound, and the use of shell containers for mixing and storage for later use.<ref name="Washington Post-2011/10/12/gIQApyHrhL"></ref>
Modern behaviors, such as the making of shell beads, bone tools and arrows, and the use of ochre pigment, are evident at a Kenyan site by 78,000-67,000 years ago.

Expanding subsistence strategies beyond big-game hunting and the consequential diversity in tool types has been noted as signs of behavioral modernity. A number of South African sites have shown an early reliance on aquatic resources from fish to shellfish. Pinnacle Point, in particular, shows exploitation of marine resources as early as 120,000 years ago, perhaps in response to more arid conditions inland. Establishing a reliance on predictable shellfish deposits, for example, could reduce mobility and facilitate complex social systems and symbolic behavior. Blombos Cave and Site 440 in Sudan both show evidence of fishing as well. Taphonomic change in fish skeletons from Blombos Cave have been interpreted as capture of live fish, clearly an intentional human behavior. Humans in North Africa (Nazlet Sabaha, Egypt) are known to have dabbled in chert mining, as early as ≈100,000 years ago, for the construction of stone tools.

Evidence was found in 2018, dating to about 320,000 years ago, at the Kenyan site of Olorgesailie, of the early emergence of modern behaviors including: long-distance trade networks (involving goods such as obsidian), the use of pigments, and the possible making of projectile points. It is observed by the authors of three 2018 studies on the site, that the evidence of these behaviors is approximately contemporary to the earliest known "Homo sapiens" fossil remains from Africa (such as at Jebel Irhoud and Florisbad), and they suggest that complex and modern behaviors began in Africa around the time of the emergence of "Homo sapiens". In 2019, further evidence of early complex projectile weapons in Africa was found at Adouma, Ethiopia dated 80,000-100,000 years ago, in the form of points considered likely to belong to darts delivered by spear throwers.

Around 65–50,000 years ago, the species' expansion out of Africa launched the colonization of the planet by modern human beings. By 10,000 BC, "Homo sapiens" had spread to most corners of Afro-Eurasia. Their disperals are traced by linguistic, cultural and genetic evidence. The earliest physical evidence of astronomical activity appears to be a lunar calendar found on the Ishango bone dated to between 23,000 and 18,000 BC from in what is now the Democratic Republic of the Congo.

Scholars have argued that warfare was absent throughout much of humanity's prehistoric past, and that it emerged from more complex political systems as a result of sedentism, agricultural farming, etc. However, the findings at the site of Nataruk in Turkana County, Kenya, where the remains of 27 individuals who died as the result of an intentional attack by another group 10,000 years ago, suggest that inter-human conflict has a much longer history.

Around 16,000 BC, from the Red Sea Hills to the northern Ethiopian Highlands, nuts, grasses and tubers were being collected for food. By 13,000 to 11,000 BC, people began collecting wild grains. This spread to Western Asia, which domesticated its wild grains, wheat and barley. Between 10,000 and 8000 BC, Northeast Africa was cultivating wheat and barley and raising sheep and cattle from Southwest Asia. A wet climatic phase in Africa turned the Ethiopian Highlands into a mountain forest. Omotic speakers domesticated enset around 6500–5500 BC. Around 7000 BC, the settlers of the Ethiopian highlands domesticated donkeys, and by 4000 BC domesticated donkeys had spread to Southwest Asia. Cushitic speakers, partially turning away from cattle herding, domesticated teff and finger millet between 5500 and 3500 BC.

During the 10th millennium BP, pottery was independently invented in Africa, with the earliest pottery there dating to about 9,400 BC from central Mali.. It soon spread throughout the southern Sahara and Sahel. In the steppes and savannahs of the Sahara and Sahel in Northern West Africa, the Nilo-Saharan speakers and Mandé peoples started to collect and domesticate wild millet, African rice and sorghum between 8000 and 6000 BC. Later, gourds, watermelons, castor beans, and cotton were also collected and domesticated. The people started capturing wild cattle and holding them in circular thorn hedges, resulting in domestication. They also started making pottery and built stone settlements (e.g., Tichitt, Oualata). Fishing, using bone-tipped harpoons, became a major activity in the numerous streams and lakes formed from the increased rains. Mande peoples have been credited with the independent development of agriculture about 3000–4000 BC.

In West Africa, the wet phase ushered in an expanding rainforest and wooded savanna from Senegal to Cameroon. Between 9000 and 5000 BC, Niger–Congo speakers domesticated the oil palm and raffia palm. Two seed plants, black-eyed peas and voandzeia (African groundnuts), were domesticated, followed by okra and kola nuts. Since most of the plants grew in the forest, the Niger–Congo speakers invented polished stone axes for clearing forest.

Most of Southern Africa was occupied by pygmy peoples and Khoisan who engaged in hunting and gathering. Some of the oldest rock art was produced by them.

For several hundred thousand years the Sahara has alternated between desert and savanna grassland in a 41,000 year cycle caused by changes ("precession") in the Earth's axis as it rotates around the sun which change the location of the North African Monsoon. When the North African monsoon is at its strongest annual precipitation and subsequent vegetation in the Sahara region increase, resulting in conditions commonly referred to as the "green Sahara". For a relatively weak North African monsoon, the opposite is true, with decreased annual precipitation and less vegetation resulting in a phase of the Sahara climate cycle known as the "desert Sahara". The Sahara has been a desert for several thousand years, and is expected to become green again in about 15,000 years time (17,000 AD).

Just prior to Saharan desertification, the communities that developed south of Egypt, in what is now Sudan, were full participants in the Neolithic revolution and lived a settled to semi-nomadic lifestyle, with domesticated plants and animals. It has been suggested that megaliths found at Nabta Playa are examples of the world's first known archaeoastronomical devices, predating Stonehenge by some 1,000 years. The sociocultural complexity observed at Nabta Playa and expressed by different levels of authority within the society there has been suggested as forming the basis for the structure of both the Neolithic society at Nabta and the Old Kingdom of Egypt.
By 5000 BC, Africa entered a dry phase, and the climate of the Sahara region gradually became drier. The population trekked out of the Sahara region in all directions, including towards the Nile Valley below the Second Cataract, where they made permanent or semipermanent settlements. A major climatic recession occurred, lessening the heavy and persistent rains in Central and Eastern Africa.

Archaeological findings in Central Africa have been discovered dating back to over 100,000 years. Extensive walled sites and settlements have recently been found in Zilum, Chad approximately southwest of Lake Chad dating to the first millennium BC.

Trade and improved agricultural techniques supported more sophisticated societies, leading to the early civilizations of Sao, Kanem, Bornu, Shilluk, Baguirmi, and Wadai.

Around 1,000 BC, Bantu migrants had reached the Great Lakes Region in Central Africa. Halfway through the first millennium BC, the Bantu had also settled as far south as what is now Angola.

The first metals to be smelted in Africa were lead, copper, and bronze in the fourth millennium BC.

Copper was smelted in Egypt during the predynastic period, and bronze came into use after 3,000 BC at the latest in Egypt and Nubia. Nubia was a major source of copper as well as gold. The use of gold and silver in Egypt dates back to the predynastic period.

In the Aïr Mountains, present-day Niger, copper was smelted independently of developments in the Nile valley between 3,000 and 2,500 BC. The process used was unique to the region, indicating that it was not brought from outside the region; it became more mature by about 1,500 BC.

By the 1st millennium BC, iron working had been introduced in Northwestern Africa, Egypt, and Nubia. According to Zangato an Holl, there is evidence of iron-smelting in the Central African Republic and Cameroon that may date back to 3,000 to 2,500 BC. In 670 BC, Nubians were pushed out of Egypt by Assyrians using iron weapons, after which the use of iron in the Nile valley became widespread.

The theory of iron spreading to Sub-Saharan Africa via the Nubian city of Meroe is no longer widely accepted. Metalworking in West Africa has been dated as early as 2,500 BC at Egaro west of the Termit in Niger, and iron working was practiced there by 1,500 BC. Iron smelting has been dated to 2,000 BC in southeast Nigeria. In Central Africa, there is evidence that iron working may have been practiced as early as the 3rd millennium BC. Iron smelting was developed in the area between Lake Chad and the African Great Lakes between 1,000 and 600 BC, and in West Africa around 2,000 BC, long before it reached Egypt. Before 500 BC, the Nok culture in the Jos Plateau was already smelting iron. Archaeological sites containing iron smelting furnaces and slag have been excavated at sites in the Nsukka region of southeast Nigeria in what is now Igboland: dating to 2,000 BC at the site of Lejja (Eze-Uzomaka 2009) and to 750 BC and at the site of Opi (Holl 2009). The site of Gbabiri (in the Central African Republic) has also yielded evidence of iron metallurgy, from a reduction furnace and blacksmith workshop; with earliest dates of 896-773 BC and 907-796 BC respectively.

The ancient history of North Africa is inextricably linked to that of the Ancient Near East. This is particularly true of Ancient Egypt and Nubia. In the Horn of Africa the Kingdom of Aksum ruled modern-day Eritrea, northern Ethiopia and the coastal area of the western part of the Arabian Peninsula. The Ancient Egyptians established ties with the Land of Punt in 2,350 BC. Punt was a trade partner of Ancient Egypt and it is believed that it was located in modern-day Somalia, Djibouti or Eritrea. Phoenician cities such as Carthage were part of the Mediterranean Iron Age and classical antiquity. Sub-Saharan Africa developed more or less independently in those times. 

After the desertification of the Sahara, settlement became concentrated in the Nile Valley, where numerous sacral chiefdoms appeared. The regions with the largest population pressure were in the Nile Delta region of Lower Egypt, in Upper Egypt, and also along the second and third cataracts of the Dongola Reach of the Nile in Nubia. This population pressure and growth was brought about by the cultivation of southwest Asian crops, including wheat and barley, and the raising of sheep, goats, and cattle. Population growth led to competition for farm land and the need to regulate farming. Regulation was established by the formation of bureaucracies among sacral chiefdoms. The first and most powerful of the chiefdoms was Ta-Seti, founded around 3,500 BC. The idea of sacral chiefdom spread throughout Upper and Lower Egypt.
Later consolidation of the chiefdoms into broader political entities began to occur in Upper and Lower Egypt, culminating into the unification of Egypt into one political entity by Narmer (Menes) in 3,100 BC. Instead of being viewed as a sacral chief, he became a divine king. The henotheism, or worship of a single god within a polytheistic system, practiced in the sacral chiefdoms along Upper and Lower Egypt, became the polytheistic Ancient Egyptian religion. Bureaucracies became more centralized under the pharaohs, run by viziers, governors, tax collectors, generals, artists, and technicians. They engaged in tax collecting, organizing of labor for major public works, and building irrigation systems, pyramids, temples, and canals. During the Fourth Dynasty (2,620–2,480 BC), long distance trade was developed, with the Levant for timber, with Nubia for gold and skins, with Punt for frankincense, and also with the western Libyan territories. For most of the Old Kingdom, Egypt developed her fundamental systems, institutions and culture, always through the central bureaucracy and by the divinity of the Pharaoh.

After the fourth millennium BC, Egypt started to extend direct military and political control over her southern and western neighbors. By 2,200 BC, the Old Kingdom's stability was undermined by rivalry among the governors of the nomes who challenged the power of pharaohs and by invasions of Asiatics into the Nile Delta. The First Intermediate Period had begun, a time of political division and uncertainty.

Middle Kingdom of Egypt arose when Mentuhotep II of Eleventh Dynasty unified Egypt once again between 2041 and 2016 BC beginning with his conquering of Tenth Dynasty in 2041 BC. Pyramid building resumed, long-distance trade re-emerged, and the center of power moved from Memphis to Thebes. Connections with the southern regions of Kush, Wawat and Irthet at the second cataract were made stronger. Then came the Second Intermediate Period, with the invasion of the Hyksos on horse-drawn chariots and utilizing bronze weapons, a technology heretofore unseen in Egypt. Horse-drawn chariots soon spread to the west in the inhabitable Sahara and North Africa. The Hyksos failed to hold on to their Egyptian territories and were absorbed by Egyptian society. This eventually led to one of Egypt's most powerful phases, the New Kingdom (1,580–1,080 BC), with the Eighteenth Dynasty. Egypt became a superpower controlling Nubia and Judea while exerting political influence on the Libyans to the West and on the Mediterranean.

As before, the New Kingdom ended with invasion from the west by Libyan princes, leading to the Third Intermediate Period. Beginning with Shoshenq I, the Twenty-second Dynasty was established. It ruled for two centuries.

To the south, Nubian independence and strength was being reasserted. This reassertion led to the conquest of Egypt by Nubia, begun by Kashta and completed by Piye (Pianhky, 751–730 BC) and Shabaka (716–695 BC). This was the birth of the Twenty-fifth Dynasty of Egypt. The Nubians tried to re-establish Egyptian traditions and customs. They ruled Egypt for a hundred years. This was ended by an Assyrian invasion, with Taharqa experiencing the full might of Assyrian iron weapons. The Nubian pharaoh Tantamani was the last of the Twenty-fifth dynasty.

When the Assyrians and Nubians left, a new Twenty-sixth Dynasty emerged from Sais. It lasted until 525 BC, when Egypt was invaded by the Persians. Unlike the Assyrians, the Persians stayed. In 332, Egypt was conquered by Alexander the Great. This was the beginning of the Ptolemaic dynasty, which ended with Roman conquest in 30 BC. Pharaonic Egypt had come to an end.

Around 3,500 BC, one of the first sacral kingdoms to arise in the Nile was Ta-Seti, located in northern Nubia. Ta-Seti was a powerful sacral kingdom in the Nile Valley at the 1st and 2nd cataracts that exerted an influence over nearby chiefdoms based on pictorial representation ruling over Upper Egypt. Ta-Seti traded as far as Syro-Palestine, as well as with Egypt. Ta-Seti exported gold, copper, ostrich feathers, ebony and ivory to the Old Kingdom. By the 32nd century BC, Ta-Seti was in decline. After the unification of Egypt by Narmer in 3,100 BC, Ta-Seti was invaded by the Pharaoh Hor-Aha of the First Dynasty, destroying the final remnants of the kingdom. Ta-Seti is affiliated with the A-Group Culture known to archaeology. 
Small sacral kingdoms continued to dot the Nubian portion of the Nile for centuries after 3,000 BC. Around the latter part of the third millennium, there was further consolidation of the sacral kingdoms. Two kingdoms in particular emerged: the Sai kingdom, immediately south of Egypt, and the Kingdom of Kerma at the third cataract. Sometime around the 18th century BC, the Kingdom of Kerma conquered the Kingdom of Sai, becoming a serious rival to Egypt. Kerma occupied a territory from the first cataract to the confluence of the Blue Nile, White Nile, and Atbarah River. About 1,575 to 1,550 BC, during the latter part of the Seventeenth Dynasty, the Kingdom of Kerma invaded Egypt. The Kingdom of Kerma allied itself with the Hyksos invasion of Egypt.

Egypt eventually re-energized under the Eighteenth Dynasty and conquered the Kingdom of Kerma or Kush, ruling it for almost 500 years. The Kushites were Egyptianized during this period. By 1100 BC, the Egyptians had withdrawn from Kush. The region regained independence and reasserted its culture. Kush built a new religion around Amun and made Napata its spiritual center. In 730 BC, the Kingdom of Kush invaded Egypt, taking over Thebes and beginning the Nubian Empire. The empire extended from Palestine to the confluence of the Blue Nile, the White Nile, and River Atbara.

In 760 BC, the Kushites were expelled from Egypt by iron-wielding Assyrians. Later, the administrative capital was moved from Napata to Meröe, developing a new Nubian culture. Initially, Meroites were highly Egyptianized, but they subsequently began to take on distinctive features. Nubia became a center of iron-making and cotton cloth manufacturing. Egyptian writing was replaced by the Meroitic alphabet. The lion god Apedemak was added to the Egyptian pantheon of gods. Trade links to the Red Sea increased, linking Nubia with Mediterranean Greece. Its architecture and art diversified, with pictures of lions, ostriches, giraffes, and elephants. Eventually, with the rise of Aksum, Nubia's trade links were broken and it suffered environmental degradation from the tree cutting required for iron production. In 350 AD, the Aksumite king Ezana brought Meröe to an end.

The Egyptians referred to the people west of the Nile, ancestral to the Berbers, as Libyans. The Libyans were agriculturalists like the Mauri of Morocco and the Numidians of central and eastern Algeria and Tunis. They were also nomadic, having the horse, and occupied the arid pastures and desert, like the Gaetuli. Berber desert nomads were typically in conflict with Berber coastal agriculturalists.

The Phoenicians were Mediterranean seamen in constant search for valuable metals such as copper, gold, tin, and lead. They began to populate the North African coast with settlementstrading and mixing with the native Berber population. In 814 BC, Phoenicians from Tyre established the city of Carthage. By 600 BC, Carthage had become a major trading entity and power in the Mediterranean, largely through trade with tropical Africa. Carthage's prosperity fostered the growth of the Berber kingdoms, Numidia and Mauretania. Around 500 BC, Carthage provided a strong impetus for trade with Sub-Saharan Africa. Berber middlemen, who had maintained contacts with Sub-Saharan Africa since the desert had desiccated, utilized pack animals to transfer products from oasis to oasis. Danger lurked from the Garamantes of Fez, who raided caravans. Salt and metal goods were traded for gold, slaves, beads, and ivory.
The Carthaginians were rivals to the Greeks and Romans. Carthage fought the Punic Wars, three wars with Rome: the First Punic War (264 to 241 BC), over Sicily; the Second Punic War (218 to 201 BC), in which Hannibal invaded Europe; and the Third Punic War (149 to 146 BC). Carthage lost the first two wars, and in the third it was destroyed, becoming the Roman province of Africa, with the Berber Kingdom of Numidia assisting Rome. The Roman province of Africa became a major agricultural supplier of wheat, olives, and olive oil to imperial Rome via exorbitant taxation. Two centuries later, Rome brought the Berber kingdoms of Numidia and Mauretania under its authority. In the 420's AD, Vandals invaded North Africa and Rome lost her territories. The Berber kingdoms subsequently regained their independence.

Christianity gained a foothold in Africa at Alexandria in the 1st century AD and spread to Northwest Africa. By 313 AD, with the Edict of Milan, all of Roman North Africa was Christian. Egyptians adopted Monophysite Christianity and formed the independent Coptic Church. Berbers adopted Donatist Christianity. Both groups refused to accept the authority of the Roman Catholic Church. 

As Carthaginian power grew, its impact on the indigenous population increased dramatically. Berber civilization was already at a stage in which agriculture, manufacturing, trade, and political organization supported several states. Trade links between Carthage and the Berbers in the interior grew, but territorial expansion also resulted in the enslavement or military recruitment of some Berbers and in the extraction of tribute from others. By the early 4th century BC, Berbers formed one of the largest element, with Gauls, of the Carthaginian army.

In the Mercenary War (241-238 BC), a rebellion was instigated by mercenary soldiers of Carthage and African allies. Berber soldiers participated after being unpaid following the defeat of Carthage in the First Punic War. Berbers succeeded in obtaining control of much of Carthage's North African territory, and they minted coins bearing the name Libyan, used in Greek to describe natives of North Africa. The Carthaginian state declined because of successive defeats by the Romans in the Punic Wars; in 146 BC the city of Carthage was destroyed. As Carthaginian power waned, the influence of Berber leaders in the hinterland grew. By the 2nd century BC, several large but loosely administered Berber kingdoms had emerged. Two of them were established in Numidia, behind the coastal areas controlled by Carthage. West of Numidia lay Mauretania, which extended across the Moulouya River in Morocco to the Atlantic Ocean. The high point of Berber civilization, unequaled until the coming of the Almohads and Almoravid dynasty more than a millennium later, was reached during the reign of Masinissa in the 2nd century BC. After Masinissa's death in 148 BC, the Berber kingdoms were divided and reunited several times. Masinissa's line survived until 24 AD, when the remaining Berber territory was annexed to the Roman Empire.

The ancestors of the Somali people were an important link in the Horn of Africa connecting the region's commerce with the rest of the ancient world. Somali sailors and merchants were the main suppliers of frankincense, myrrh and spices, all of which were valuable luxuries to the Ancient Egyptians, Phoenicians, Mycenaeans and Babylonians.

In the classical era, several flourishing Somali city-states such as Opone, Mosylon, Cape Guardafui, and Malao competed with the Sabaeans, Parthians and Axumites for the rich Indo–Greco-Roman trade.

"Increases in urbanization and in the area under cultivation during Roman rule caused wholesale dislocations of the Berber society, forcing nomad tribes to settle or to move from their traditional rangelands. Sedentary tribes lost their autonomy and connection with the land. Berber opposition to the Roman presence was nearly constant. The Roman emperor Trajan established a frontier in the south by encircling the Aurès and Nemencha mountains and building a line of forts from Vescera (modern Biskra) to Ad Majores (Hennchir Besseriani, southeast of Biskra). The defensive line extended at least as far as Castellum Dimmidi (modern Messaâd, southwest of Biskra), Roman Algeria's southernmost fort. Romans settled and developed the area around Sitifis (modern Sétif) in the 2nd century, but farther west the influence of Rome did not extend beyond the coast and principal military roads until much later."
The Roman military presence of North Africa remained relatively small, consisting of about 28,000 troops and auxiliaries in Numidia and the two Mauretanian provinces. Starting in the 2nd century AD, these garrisons were manned mostly by local inhabitants.

Aside from Carthage, urbanization in North Africa came in part with the establishment of settlements of veterans under the Roman emperors Claudius (reigned 41–54), Nerva (96–98), and Trajan (98–117). In Algeria such settlements included Tipasa, Cuicul or Curculum (modern Djemila, northeast of Sétif), Thamugadi (modern Timgad, southeast of Sétif), and Sitifis (modern Sétif). The prosperity of most towns depended on agriculture. Called the "granary of the empire", North Africa became one of the largest exporters of grain in the empire, shipping to the provinces which did not produce cereals, like Italy and Greece. Other crops included fruit, figs, grapes, and beans. By the 2nd century AD, olive oil rivaled cereals as an export item.

The beginnings of the Roman imperial decline seemed less serious in North Africa than elsewhere. However, uprisings did take place. In 238 AD, landowners rebelled unsuccessfully against imperial fiscal policies. Sporadic tribal revolts in the Mauretanian mountains followed from 253 to 288, during the Crisis of the Third Century. The towns also suffered economic difficulties, and building activity almost ceased.

The towns of Roman North Africa had a substantial Jewish population. Some Jews had been deported from Judea or Palestine in the 1st and 2nd centuries AD for rebelling against Roman rule; others had come earlier with Punic settlers. In addition, a number of Berber tribes had converted to Judaism.
Christianity arrived in the 2nd century and soon gained converts in the towns and among slaves. More than eighty bishops, some from distant frontier regions of Numidia, attended the Council of Carthage (256) in 256. By the end of the 4th century, the settled areas had become Christianized, and some Berber tribes had converted "en masse".

A division in the church that came to be known as the Donatist heresy began in 313 among Christians in North Africa. The Donatists stressed the holiness of the church and refused to accept the authority to administer the sacraments of those who had surrendered the scriptures when they were forbidden under the Emperor Diocletian (reigned 284–305). The Donatists also opposed the involvement of Constantine the Great (reigned 306–337) in church affairs in contrast to the majority of Christians who welcomed official imperial recognition.

The occasionally violent Donatist controversy has been characterized as a struggle between opponents and supporters of the Roman system. The most articulate North African critic of the Donatist position, which came to be called a heresy, was Augustine, bishop of Hippo Regius. Augustine maintained that the unworthiness of a minister did not affect the validity of the sacraments because their true minister was Jesus Christ. In his sermons and books Augustine, who is considered a leading exponent of Christian dogma, evolved a theory of the right of orthodox Christian rulers to use force against schismatics and heretics. Although the dispute was resolved by a decision of an imperial commission in Carthage in 411, Donatist communities continued to exist as late as the 6th century.

A decline in trade weakened Roman control. Independent kingdoms emerged in mountainous and desert areas, towns were overrun, and Berbers, who had previously been pushed to the edges of the Roman Empire, returned.

During the Vandalic War, Belisarius, general of the Byzantine emperor Justinian I based in Constantinople, landed in North Africa in 533 with 16,000 men and within a year destroyed the Vandal Kingdom. Local opposition delayed full Byzantine control of the region for twelve years, however, and when imperial control came, it was but a shadow of the control exercised by Rome. Although an impressive series of fortifications were built, Byzantine rule was compromised by official corruption, incompetence, military weakness, and lack of concern in Constantinople for African affairs, which made it an easy target for the Arabs during the Early Muslim conquests . As a result, many rural areas reverted to Berber rule. 

The earliest state in Eritrea and northern Ethiopia, Dʿmt, dates from around the 8th and 7th centuries BC. D'mt traded through the Red Sea with Egypt and the Mediterranean, providing frankincense. By the 5th and 3rd centuries, D'mt had declined, and several successor states took its place. Later there was greater trade with South Arabia, mainly with the port of Saba. Adulis became an important commercial center in the Ethiopian Highlands. The interaction of the peoples in the two regions, the southern Arabia Sabaeans and the northern Ethiopians, resulted in the Ge'ez culture and language and eventual development of the Ge'ez script. Trade links increased and expanded from the Red Sea to the Mediterranean, with Egypt, Israel, Phoenicia, Greece, and Rome, to the Black Sea, and to Persia, India, and China. Aksum was known throughout those lands. By the 5th century BC, the region was very prosperous, exporting ivory, hippopotamus hides, gold dust, spices, and live elephants. It imported silver, gold, olive oil, and wine. Aksum manufactured glass crystal, brass, and copper for export. A powerful Aksum emerged, unifying parts of eastern Sudan, northern Ethiopia (Tigre), and Eritrea. Its kings built stone palatial buildings and were buried under megalithic monuments. By 300 AD, Aksum was minting its own coins in silver and gold.

In 331 AD, King Ezana (320–350 AD) was converted to Miaphysite Christianity which believes in one united divine-human nature of Christ, supposedly by Frumentius and Aedesius, who became stranded on the Red Sea coast. Some scholars believed the process was more complex and gradual than a simple conversion. Around 350, the time Ezana sacked Meroe, the Syrian monastic tradition took root within the Ethiopian church.

In the 6th century Aksum was powerful enough to add Saba on the Arabian peninsula to her empire. At the end of the 6th century, the Sasanian Empire pushed Aksum out of the peninsula. With the spread of Islam through Western Asia and Northern Africa, Aksum's trading networks in the Mediterranean faltered. The Red Sea trade diminished as it was diverted to the Persian Gulf and dominated by Arabs, causing Aksum to decline. By 800 AD, the capital was moved south into the interior highlands, and Aksum was much diminished.

In the western Sahel the rise of settled communities occurred largely as a result of the domestication of millet and of sorghum. Archaeology points to sizable urban populations in West Africa beginning in the 2nd millennium BC. Symbiotic trade relations developed before the trans-Saharan trade, in response to the opportunities afforded by north–south diversity in ecosystems across deserts, grasslands, and forests. The agriculturists received salt from the desert nomads. The desert nomads acquired meat and other foods from pastoralists and farmers of the grasslands and from fishermen on the Niger River. The forest-dwellers provided furs and meat.
Dhar Tichitt and Oualata in present-day Mauritania figure prominently among the early urban centers, dated to 2,000 BC. About 500 stone settlements litter the region in the former savannah of the Sahara. Its inhabitants fished and grew millet. It has been found Augustin Holl that the Soninke of the Mandé peoples were likely responsible for constructing such settlements. Around 300 BC the region became more desiccated and the settlements began to decline, most likely relocating to Koumbi Saleh. Architectural evidence and the comparison of pottery styles suggest that Dhar Tichitt was related to the subsequent Ghana Empire. Djenné-Djenno (in present-day Mali) was settled around 300 BC, and the town grew to house a sizable Iron Age population, as evidenced by crowded cemeteries. Living structures were made of sun-dried mud. By 250 BC Djenné-Djenno had become a large, thriving market town.

Farther south, in central Nigeria, around 1,500 BC, the Nok culture developed in Jos Plateau. It was a highly centralized community. The Nok people produced lifelike representations in terracotta, including human heads and human figures, elephants, and other animals. By 500 BC they were smelting iron. By 200 AD the Nok culture had vanished. Based on stylistic similarities with the Nok terracottas, the bronze figurines of the Yoruba kingdom of Ife and those of the Bini kingdom of Benin are now believed to be continuations of the traditions of the earlier Nok culture.

The Bantu expansion involved a significant movement of people in African history and in the settling of the continent. People speaking Bantu languages (a branch of the Niger–Congo family) began in the second millennium BC to spread from Cameroon eastward to the Great Lakes region. In the first millennium BC, Bantu languages spread from the Great Lakes to southern and east Africa. One early movement headed south to the upper Zambezi valley in the 2nd century BC. Then Bantu-speakers pushed westward to the savannahs of present-day Angola and eastward into Malawi, Zambia, and Zimbabwe in the 1st century AD. The second thrust from the Great Lakes was eastward, 2,000 years ago, expanding to the Indian Ocean coast, Kenya and Tanzania. The eastern group eventually met the southern migrants from the Great Lakes in Malawi, Zambia, and Zimbabwe. Both groups continued southward, with eastern groups continuing to Mozambique and reaching Maputo in the 2nd century AD, and expanding as far as Durban.

By the later first millennium AD, the expansion had reached the Great Kei River in present-day South Africa. Sorghum, a major Bantu crop, could not thrive under the winter rainfall of Namibia and the western Cape. Khoisan people inhabited the remaining parts of southern Africa.

The Sao civilization flourished from about the sixth century BC to as late as the 16th century AD in Central Africa. The Sao lived by the Chari River south of Lake Chad in territory that later became part of present-day Cameroon and Chad. They are the earliest people to have left clear traces of their presence in the territory of modern Cameroon. Today, several ethnic groups of northern Cameroon and southern Chad - but particularly the Sara people - claim descent from the civilization of the Sao. Sao artifacts show that they were skilled workers in bronze, copper, and iron.
Finds include bronze sculptures and terracotta statues of human and animal figures, coins, funerary urns, household utensils, jewelry, highly decorated pottery, and spears. The largest Sao archaeological finds have occurred south of Lake Chad.

The Kanem Empire was centered in the Chad Basin. It was known as the Kanem Empire from the 9th century AD onward and lasted as the independent kingdom of Bornu until 1893. At its height it encompassed an area covering not only much of Chad, but also parts of modern southern Libya, eastern Niger, northeastern Nigeria, northern Cameroon, parts of South Sudan and the Central African Republic. The history of the Empire is mainly known from the Royal Chronicle or "Girgam" discovered in 1851 by the German traveller Heinrich Barth. Kanem rose in the 8th century in the region to the north and east of Lake Chad. The Kanem empire went into decline, shrank, and in the 14th century was defeated by Bilala invaders from the Lake Fitri region.

Around the 9th century AD, the central Sudanic Empire of Kanem, with its capital at Njimi, was founded by the Kanuri-speaking nomads. Kanem arose by engaging in the trans-Saharan trade. It exchanged slaves captured by raiding the south for horses from North Africa, which in turn aided in the acquisition of slaves. By the late 11th century, the Islamic Sayfawa (Saifawa) dynasty was founded by Humai (Hummay) ibn Salamna. The Sayfawa Dynasty ruled for 771 years, making it one of the longest-lasting dynasties in human history. In addition to trade, taxation of local farms around Kanem became a source of state income. Kanem reached its peak under "Mai" (king) Dunama Dibalemi ibn Salma (1210–1248). The empire reportedly was able to field 40,000 cavalry, and it extended from Fezzan in the north to the Sao state in the south. Islam became firmly entrenched in the empire. Pilgrimages to Mecca were common; Cairo had hostels set aside specifically for pilgrims from Kanem.

The Kanuri people led by the Sayfuwa migrated to the west and south of the lake, where they established the Bornu Empire. By the late 16th century the Bornu empire had expanded and recaptured the parts of Kanem that had been conquered by the Bulala. Satellite states of Bornu included the Damagaram in the west and Baguirmi to the southeast of Lake Chad.
Around 1400, the Sayfawa Dynasty moved its capital to Bornu, a tributary state southwest of Lake Chad with a new capital Birni Ngarzagamu. Overgrazing had caused the pastures of Kanem to become too dry. In addition, political rivalry from the Bilala clan was becoming intense. Moving to Bornu better situated the empire to exploit the trans-Saharan trade and to widen its network in that trade. Links to the Hausa states were also established, providing horses and salt from Bilma for Bonoman gold. Mai Ali Gazi ibn Dunama (c. 1475–1503) defeated the Bilala, reestablishing complete control of Kanem.
During the early 16th century, the Sayfawa Dynasty solidified its hold on the Bornu population after much rebellion. In the latter half of the 16th century, "Mai" Idris Alooma modernized its military, in contrast to the Songhai Empire. Turkish mercenaries were used to train the military. The Sayfawa Dynasty were the first monarchs south of the Sahara to import firearms. The empire controlled all of the Sahel from the borders of Darfur in the east to Hausaland to the west. Friendly relationship was established with the Ottoman Empire via Tripoli. The "Mai" exchanged gifts with the Ottoman sultan.
During the 17th and 18th centuries, not much is known about Bornu. During the 18th century, it became a center of Islamic learning. However, Bornu's army became outdated by not importing new arms, and Kamembu had also begun its decline. The power of the "mai" was undermined by droughts and famine that were becoming more intense, internal rebellion in the pastoralist north, growing Hausa power, and the importation of firearms which made warfare more bloody. By 1841, the last "mai" was deposed, bringing to an end the long-lived Sayfawa Dynasty. In its place, the al-Kanemi dynasty of the "shehu" rose to power.

The Shilluk Kingdom was centered in South Sudan from the 15th century from along a strip of land along the western bank of the White Nile, from Lake No to about 12° north latitude. The capital and royal residence was in the town of Fashoda. The kingdom was founded during the mid-15th century AD by its first ruler, Nyikang. During the 19th century, the Shilluk Kingdom faced decline following military assaults from the Ottoman Empire and later British and Sudanese colonization in Anglo-Egyptian Sudan.

The Kingdom of Baguirmi existed as an independent state during the 16th and 17th centuries southeast of Lake Chad in what is now the country of Chad. Baguirmi emerged to the southeast of the Kanem-Bornu Empire. The kingdom's first ruler was Mbang Birni Besse. Later in his reign, the Bornu Empire conquered and made the state a tributary.

The Wadai Empire was centered on Chad and the Central African Republic from the 17th century. The Tunjur people founded the Wadai Kingdom to the east of Bornu in the 16th century. In the 17th century there was a revolt of the Maba people who established a Muslim dynasty.

At first Wadai paid tribute to Bornu and Durfur, but by the 18th century Wadai was fully independent and had become an aggressor against its neighbors.To the west of Bornu, by the 15th century the Kingdom of Kano had become the most powerful of the Hausa Kingdoms, in an unstable truce with the Kingdom of Katsina to the north. Both were absorbed into the Sokoto Caliphate during the Fulani Jihad of 1805, which threatened Bornu itself.

Sometime between 1300 and 1400 AD, Kongolo Mwamba (Nkongolo) from the Balopwe clan unified the various Luba peoples, near Lake Kisale. He founded the Kongolo Dynasty, which was later ousted by Kalala Ilunga. Kalala expanded the kingdom west of Lake Kisale. A new centralized political system of spiritual kings ("balopwe") with a court council of head governors and sub-heads all the way to village heads. The "balopwe" was the direct communicator with the ancestral spirits and chosen by them. Conquered states were integrated into the system and represented in the court, with their titles. The authority of the "balopwe" resided in his spiritual power rather than his military authority. The army was relatively small. The Luba was able to control regional trade and collect tribute for redistribution. Numerous offshoot states were formed with founders claiming descent from the Luba. The Luba political system spread throughout Central Africa, southern Uganda, Rwanda, Burundi, Malawi, Zambia, Zimbabwe, and the western Congo. Two major empires claiming Luba descent were the Lunda Empire and Maravi Empire. The Bemba people and Basimba people of northern Zambia were descended from Luba migrants who arrived in Zambia during the 17th century.

In the 1450s, a Luba from the royal family Ilunga Tshibinda married Lunda queen Rweej and united all Lunda peoples. Their son "mulopwe" Luseeng expanded the kingdom. His son Naweej expanded the empire further and is known as the first Lunda emperor, with the title "mwato yamvo" ("mwaant yaav", "mwant yav"), the Lord of Vipers. The Luba political system was retained, and conquered peoples were integrated into the system. The "mwato yamvo" assigned a "cilool" or "kilolo" (royal adviser) and tax collector to each state conquered.

Numerous states claimed descent from the Lunda. The Imbangala of inland Angola claimed descent from a founder, Kinguri, brother of Queen Rweej, who could not tolerate the rule of "mulopwe" Tshibunda. "Kinguri" became the title of kings of states founded by Queen Rweej's brother. The Luena (Lwena) and Lozi (Luyani) in Zambia also claim descent from Kinguri. During the 17th century, a Lunda chief and warrior called Mwata Kazembe set up an Eastern Lunda kingdom in the valley of the Luapula River. The Lunda's western expansion also saw claims of descent by the Yaka and the Pende. The Lunda linked Central Africa with the western coast trade. The kingdom of Lunda came to an end in the 19th century when it was invaded by the Chokwe, who were armed with guns.

By the 15th century AD, the farming Bakongo people ("ba" being the plural prefix) were unified as the Kingdom of Kongo under a ruler called the "manikongo", residing in the fertile Pool Malebo area on the lower Congo River. The capital was M'banza-Kongo. With superior organization, they were able to conquer their neighbors and extract tribute. They were experts in metalwork, pottery, and weaving raffia cloth. They stimulated interregional trade via a tribute system controlled by the "manikongo". Later, maize (corn) and cassava (manioc) would be introduced to the region via trade with the Portuguese at their ports at Luanda and Benguela. The maize and cassava would result in population growth in the region and other parts of Africa, replacing millet as a main staple.

By the 16th century, the "manikongo" held authority from the Atlantic in the west to the Kwango River in the east. Each territory was assigned a "mani-mpembe" (provincial governor) by the "manikongo". In 1506, Afonso I (1506–1542), a Christian, took over the throne. Slave trading increased with Afonso's wars of conquest. About 1568 to 1569, the Jaga invaded Kongo, laying waste to the kingdom and forcing the "manikongo" into exile. In 1574, Manikongo Álvaro I was reinstated with the help of Portuguese mercenaries. During the latter part of the 1660s, the Portuguese tried to gain control of Kongo. Manikongo António I (1661–1665), with a Kongolese army of 5,000, was destroyed by an army of Afro-Portuguese at the Battle of Mbwila. The empire dissolved into petty polities, fighting among each other for war captives to sell into slavery.

Kongo gained captives from the Kingdom of Ndongo in wars of conquest. Ndongo was ruled by the "ngola". Ndongo would also engage in slave trading with the Portuguese, with São Tomé being a transit point to Brazil. The kingdom was not as welcoming as Kongo; it viewed the Portuguese with great suspicion and as an enemy. The Portuguese in the latter part of the 16th century tried to gain control of Ndongo but were defeated by the Mbundu. Ndongo experienced depopulation from slave raiding. The leaders established another state at Matamba, affiliated with Queen Nzinga, who put up a strong resistance to the Portuguese until coming to terms with them. The Portuguese settled along the coast as trade dealers, not venturing on conquest of the interior. Slavery wreaked havoc in the interior, with states initiating wars of conquest for captives. The Imbangala formed the slave-raiding state of Kasanje, a major source of slaves during the 17th and 18th centuries.

The birth of Islam opposite Somalia's Red Sea coast meant that Somali merchants and sailors living on the Arabian Peninsula gradually came under the influence of the new religion through their converted Arab Muslim trading partners. With the migration of Muslim families from the Islamic world to Somalia in the early centuries of Islam, and the peaceful conversion of the Somali population by Somali Muslim scholars in the following centuries, the ancient city-states eventually transformed into Islamic Mogadishu, Berbera, Zeila, Barawa and Merka, which were part of the "Berber" (the medieval Arab term for the ancestors of the modern Somalis) civilization. The city of Mogadishu came to be known as the "City of Islam" and controlled the East African gold trade for several centuries.
During this period, sultanates such as the Ajuran Empire and the Sultanate of Mogadishu, and republics like Barawa, Merca and Hobyo and their respective ports flourished and had a lucrative foreign commerce with ships sailing to and coming from Arabia, India, Venice, Persia, Egypt, Portugal and as far away as China. Vasco da Gama, who passed by Mogadishu in the 15th century, noted that it was a large city with houses four or five stories high and big palaces in its centre, in addition to many mosques with cylindrical minarets.

In the 16th century, Duarte Barbosa noted that many ships from the Kingdom of Cambaya in modern-day India sailed to Mogadishu with cloth and spices, for which they in return received gold, wax, and ivory. Barbosa also highlighted the abundance of meat, wheat, barley, horses, and fruit in the coastal markets, which generated enormous wealth for the merchants. Mogadishu, the center of a thriving weaving industry known as "toob benadir" (specialized for the markets in Egypt and Syria), together with Merca and Barawa, served as a transit stop for Swahili merchants from Mombasa and Malindi and for the gold trade from Kilwa. Jewish merchants from the Strait of Hormuz brought their Indian textiles and fruit to the Somali coast to exchange for grain and wood.

Trading relations were established with Malacca in the 15th century, with cloth, ambergris, and porcelain being the main commodities of the trade. Giraffes, zebras, and incense were exported to the Ming Empire of China, which established Somali merchants as leaders in the commerce between the Asia and Africa and influenced the Chinese language with borrowings from the Somali language in the process. Hindu merchants from Surat and southeast African merchants from Pate, seeking to bypass both the Portuguese blockade and Omani meddling, used the Somali ports of Merca and Barawa (which were out of the two powers' jurisdiction) to conduct their trade in safety and without any problems.

The Zagwe dynasty ruled many parts of modern Ethiopia and Eritrea from approximately 1137 to 1270. The name of the dynasty comes from the Cushitic speaking Agaw of northern Ethiopia. From 1270 AD and on for many centuries, the Solomonic dynasty ruled the Ethiopian Empire. 

In the early 15th century Ethiopia sought to make diplomatic contact with European kingdoms for the first time since Aksumite times. A letter from King Henry IV of England to the Emperor of Abyssinia survives. In 1428, the Emperor Yeshaq I sent two emissaries to Alfonso V of Aragon, who sent return emissaries who failed to complete the return trip.

The first continuous relations with a European country began in 1508 with the Kingdom of Portugal under Emperor Lebna Dengel, who had just inherited the throne from his father. This proved to be an important development, for when the empire was subjected to the attacks of the Adal general and imam, Ahmad ibn Ibrahim al-Ghazi (called ""Grañ"", or "the Left-handed"), Portugal assisted the Ethiopian emperor by sending weapons and four hundred men, who helped his son Gelawdewos defeat Ahmad and re-establish his rule. This Abyssinian–Adal War was also one of the first proxy wars in the region as the Ottoman Empire, and Portugal took sides in the conflict.

When Emperor Susenyos converted to Roman Catholicism in 1624, years of revolt and civil unrest followed resulting in thousands of deaths. The Jesuit missionaries had offended the Orthodox faith of the local Ethiopians, and on June 25, 1632, Susenyos's son, Emperor Fasilides, declared the state religion to again be Ethiopian Orthodox Christianity and expelled the Jesuit missionaries and other Europeans.

By 711 AD, the Umayyad Caliphate had conquered all of North Africa. By the 10th century, the majority of the population of North Africa was Muslim.

By the 9th century AD, the unity brought about by the Islamic conquest of North Africa and the expansion of Islamic culture came to an end. Conflict arose as to who should be the successor of the prophet. The Umayyads had initially taken control of the Caliphate, with their capital at Damascus. Later, the Abbasids had taken control, moving the capital to Baghdad. The Berber people, being independent in spirit and hostile to outside interference in their affairs and to Arab exclusivity in orthodox Islam, adopted Shi'ite and Kharijite Islam, both considered unorthodox and hostile to the authority of the Abbasid Caliphate. Numerous Kharijite kingdoms came and fell during the 8th and 9th centuries, asserting their independence from Baghdad. In the early 10th century, Shi'ite groups from Syria, claiming descent from Muhammad's daughter Fatimah, founded the Fatimid Dynasty in the Maghreb. By 950, they had conquered all of the Maghreb and by 969 all of Egypt. They had immediately broken away from Baghdad.

In an attempt to bring about a purer form of Islam among the Sanhaja Berbers, Abdallah ibn Yasin founded the Almoravid movement in present-day Mauritania and Western Sahara. The Sanhaja Berbers, like the Soninke, practiced an indigenous religion alongside Islam. Abdallah ibn Yasin found ready converts in the Lamtuna Sanhaja, who were dominated by the Soninke in the south and the Zenata Berbers in the north. By the 1040s, all of the Lamtuna was converted to the Almoravid movement. With the help of Yahya ibn Umar and his brother Abu Bakr ibn Umar, the sons of the Lamtuna chief, the Almoravids created an empire extending from the Sahel to the Mediterranean. After the death of Abdallah ibn Yassin and Yahya ibn Umar, Abu Bakr split the empire in half, between himself and Yusuf ibn Tashfin, because it was too big to be ruled by one individual. Abu Bakr took the south to continue fighting the Soninke, and Yusuf ibn Tashfin took the north, expanding it to southern Spain. The death of Abu Bakr in 1087 saw a breakdown of unity and increase military dissension in the south. This caused a re-expansion of the Soninke. The Almoravids were once held responsible for bringing down the Ghana Empire in 1076, but this view is no longer credited.

During the 10th through 13th centuries, there was a large-scale movement of bedouins out of the Arabian Peninsula. About 1050, a quarter of a million Arab nomads from Egypt moved into the Maghreb. Those following the northern coast were referred to as Banu Hilal. Those going south of the Atlas Mountains were the Banu Sulaym. This movement spread the use of the Arabic language and hastened the decline of the Berber language and the Arabisation of North Africa. Later an Arabised Berber group, the Hawwara, went south to Nubia via Egypt.
In the 1140s, Abd al-Mu'min declared jihad on the Almoravids, charging them with decadence and corruption. He united the northern Berbers against the Almoravids, overthrowing them and forming the Almohad Empire. During this period, the Maghreb became thoroughly Islamised and saw the spread of literacy, the development of algebra, and the use of the number zero and decimals. By the 13th century, the Almohad states had split into three rival states. Muslim states were largely extinguished in the Iberian Peninsula by the Christian kingdoms of Castile, Aragon, and Portugal. Around 1415, Portugal engaged in a "reconquista" of North Africa by capturing Ceuta, and in later centuries Spain and Portugal acquired other ports on the North African coast. In 1492, at the end of the Granada War, Spain defeated Muslims in the Emirate of Granada, effectively ending eight centuries of Muslim domination in southern Iberia.
Portugal and Spain took the ports of Tangiers, Algiers, Tripoli, and Tunis. This put them in direct competition with the Ottoman Empire, which re-took the ports using Turkish corsairs (pirates and privateers). The Turkish corsairs would use the ports for raiding Christian ships, a major source of booty for the towns. Technically, North Africa was under the control of the Ottoman Empire, but only the coastal towns were fully under Istanbul's control. Tripoli benefited from trade with Borno. The pashas of Tripoli traded horses, firearms, and armor via Fez with the sultans of the Bornu Empire for slaves.

In the 16th century, an Arab nomad tribe that claimed descent from Muhammad's daughter, the Saadis, conquered and united Morocco. They prevented the Ottoman Empire from reaching to the Atlantic and expelled Portugal from Morocco's western coast. Ahmad al-Mansur brought the state to the height of its power. He invaded Songhay in 1591, to control the gold trade, which had been diverted to the western coast of Africa for European ships and to the east, to Tunis. Morocco's hold on Songhay diminished in the 17th century. In 1603, after Ahmad's death, the kingdom split into the two sultanates of Fes and Marrakesh. Later it was reunited by Moulay al-Rashid, founder of the Alaouite Dynasty (1672–1727). His brother and successor, Ismail ibn Sharif (1672–1727), strengthened the unity of the country by importing slaves from the Sudan to build up the military.

In 642 AD, the Rashidun Caliphate conquered Byzantine Egypt.

Egypt under the Fatimid Caliphate was prosperous. Dams and canals were repaired, and wheat, barley, flax, and cotton production increased. Egypt became a major producer of linen and cotton cloth. Its Mediterranean and Red Sea trade increased. Egypt also minted a gold currency called the Fatimid dinar, which was used for international trade. The bulk of revenues came from taxing the fellahin (peasant farmers), and taxes were high. Tax collecting was leased to Berber overlords, who were soldiers who had taken part in the Fatimid conquest in 969 AD. The overlords paid a share to the caliphs and retained what was left. Eventually, they became landlords and constituted a settled land aristocracy.

To fill the military ranks, Mamluk Turkish slave cavalry and Sudanese slave infantry were used. Berber freemen were also recruited. In the 1150s, tax revenues from farms diminished. The soldiers revolted and wreaked havoc in the countryside, slowed trade, and diminished the power and authority of the Fatimid caliphs.

During the 1160s, Fatimid Egypt came under threat from European crusaders. Out of this threat, a Kurdish general named Ṣalāḥ ad-Dīn Yūsuf ibn Ayyūb (Saladin), with a small band of professional soldiers, emerged as an outstanding Muslim defender. Saladin defeated the Christian crusaders at Egypt's borders and recaptured Jerusalem in 1187. On the death of Al-Adid, the last Fatimid caliph, in 1171, Saladin became the ruler of Egypt, ushering in the Ayyubid Dynasty. Under his rule, Egypt returned to Sunni Islam, Cairo became an important center of Arab Islamic learning, and Mamluk slaves were increasingly recruited from Turkey and southern Russia for military service. Support for the military was tied to the "iqta", a form of land taxation in which soldiers were given ownership in return for military service.

Over time, Mamluk slave soldiers became a very powerful landed aristocracy, to the point of getting rid of the Ayyubid dynasty in 1250 and establishing a Mamluk dynasty. The more powerful Mamluks were referred to as "amirs". For 250 years, Mamluks controlled all of Egypt under a military dictatorship. Egypt extended her territories to Syria and Palestine, thwarted the crusaders, and halted a Mongol invasion in 1260 at the Battle of Ain Jalut. Mamluk Egypt came to be viewed as a protector of Islam, and of Medina and Mecca. Eventually the "iqta" system declined and proved unreliable for providing an adequate military. The Mamluks started viewing their "iqta" as hereditary and became attuned to urban living. Farm production declined, and dams and canals lapsed into disrepair. Mamluk military skill and technology did not keep pace with new technology of handguns and cannons.

With the rise of the Ottoman Empire, Egypt was easily defeated. In 1517, at the end of an Ottoman–Mamluk War, Egypt became part of the Ottoman Empire. The Istanbul government revived the "iqta" system. Trade was reestablished in the Red Sea, but it could not completely connect with the Indian Ocean trade because of growing Portuguese presence. During the 17th and 18th centuries, hereditary Mamluks regained power. The leading Mamluks were referred to as "beys". Pashas, or viceroys, represented the Istanbul government in name only, operating independently. During the 18th century, dynasties of pashas became established. The government was weak and corrupt.

In 1798, Napoleon invaded Egypt. The local forces had little ability to resist the French conquest. However, the British Empire and the Ottoman Empire were able to remove French occupation in 1801. These events marked the beginning of a 19th-century Anglo-Franco rivalry over Egypt.

After Ezana of Aksum sacked Meroe, people associated with the site of Ballana moved into Nubia from the southwest and founded three kingdoms: Makuria, Nobatia, and Alodia. They would rule for 200 years. Makuria was above the third cataract, along the Dongola Reach with its capital at Dongola. Nobadia was to the north with its capital at Faras, and Alodia was to the south with its capital at Soba. Makuria eventually absorbed Nobadia. The people of the region converted to Monophysite Christianity around 500 to 600 CE. The church initially started writing in Coptic, then in Greek, and finally in Old Nubian, a Nilo-Saharan language. The church was aligned with the Egyptian Coptic Church.

By 641, Egypt was conquered by the Rashidun Caliphate. This effectively blocked Christian Nubia and Aksum from Mediterranean Christendom. In 651–652, Arabs from Egypt invaded Christian Nubia. Nubian archers soundly defeated the invaders. The Baqt (or Bakt) Treaty was drawn, recognizing Christian Nubia and regulating trade. The treaty controlled relations between Christian Nubia and Islamic Egypt for almost six hundred years.

By the 13th century, Christian Nubia began its decline. The authority of the monarchy was diminished by the church and nobility. Arab bedouin tribes began to infiltrate Nubia, causing further havoc. "Fakirs" (holy men) practicing Sufism introduced Islam into Nubia. By 1366, Nubia had become divided into petty fiefdoms when it was invaded by Mamluks. During the 15th century, Nubia was open to Arab immigration. Arab nomads intermingled with the population and introduced the Arab culture and the Arabic language. By the 16th century, Makuria and Nobadia had been Islamized. During the 16th century, Abdallah Jamma headed an Arab confederation that destroyed Soba, capital of Alodia, the last holdout of Christian Nubia. Later Alodia would fall under the Funj Sultanate.

During the 15th century, Funj herders migrated north to Alodia and occupied it. Between 1504 and 1505, the kingdom expanded, reaching its peak and establishing its capital at Sennar under Badi II Abu Daqn (c. 1644–1680). By the end of the 16th century, the Funj had converted to Islam. They pushed their empire westward to Kordofan. They expanded eastward, but were halted by Ethiopia. They controlled Nubia down to the 3rd Cataract. The economy depended on captured enemies to fill the army and on merchants travelling through Sennar. Under Badi IV (1724–1762), the army turned on the king, making him nothing but a figurehead. In 1821, the Funj were conquered by Muhammad Ali (1805–1849), Pasha of Egypt.

Settlements of Bantu-speaking peoples who were iron-using agriculturists and herdsmen were long already well established south of the Limpopo River by the 4th century CE, displacing and absorbing the original Khoisan speakers. They slowly moved south, and the earliest ironworks in modern-day KwaZulu-Natal Province are believed to date from around 1050. The southernmost group was the Xhosa people, whose language incorporates certain linguistic traits from the earlier Khoi-San people, reaching the Great Fish River in today's Eastern Cape Province.

The Kingdom of Mapungubwe was the first state in Southern Africa, with its capital at Mapungubwe. The state arose in the 12th century CE. Its wealth came from controlling the trade in ivory from the Limpopo Valley, copper from the mountains of northern Transvaal, and gold from the Zimbabwe Plateau between the Limpopo and Zambezi rivers, with the Swahili merchants at Chibuene. By the mid-13th century, Mapungubwe was abandoned.

After the decline of Mapungubwe, Great Zimbabwe rose on the Zimbabwe Plateau. "Zimbabwe" means stone building. Great Zimbabwe was the first city in Southern Africa and was the center of an empire, consolidating lesser Shona polities. Stone building was inherited from Mapungubwe. These building techniques were enhanced and came into maturity at Great Zimbabwe, represented by the wall of the Great Enclosure. The dry-stack stone masonry technology was also used to build smaller compounds in the area. Great Zimbabwe flourished by trading with Swahili Kilwa and Sofala. The rise of Great Zimbabwe parallels the rise of Kilwa. Great Zimbabwe was a major source of gold. Its royal court lived in luxury, wore Indian cotton, surrounded themselves with copper and gold ornaments, and ate on plates from as far away as Persia and China. Around the 1420s and 1430s, Great Zimbabwe was on decline. The city was abandoned by 1450. Some have attributed the decline to the rise of the trading town Ingombe Ilede.

A new chapter of Shona history ensued. Nyatsimba Mutota, a northern Shona king of the Karanga, engaged in conquest. He and his son Mutope conquered the Zimbabwe Plateau, going through Mozambique to the east coast, linking the empire to the coastal trade. They called their empire "Wilayatu 'l Mu'anamutapah" or "mwanamutapa" (Lord of the Plundered Lands), or the Kingdom of Mutapa. "Monomotapa" was the Portuguese corruption. They did not build stone structures; the northern Shonas had no traditions of building in stone. After the death of Matope in 1480, the empire split into two small empires: Torwa in the south and Mutapa in the north. The split occurred over rivalry from two Shona lords, Changa and Togwa, with the "mwanamutapa" line. Changa was able to acquire the south, forming the Kingdom of Butua with its capital at Khami.

The Mutapa Empire continued in the north under the "mwenemutapa" line. During the 16th century the Portuguese were able to establish permanent markets up the Zambezi River in an attempt to gain political and military control of Mutapa. They were partially successful. In 1628, a decisive battle allowed them to put a puppet "mwanamutapa" named Mavura, who signed treaties that gave favorable mineral export rights to the Portuguese. The Portuguese were successful in destroying the "mwanamutapa" system of government and undermining trade. By 1667, Mutapa was in decay. Chiefs would not allow digging for gold because of fear of Portuguese theft, and the population declined.

The Kingdom of Butua was ruled by a "changamire", a title derived from the founder, Changa. Later it became the Rozwi Empire. The Portuguese tried to gain a foothold but were thrown out of the region in 1693, by Changamire Dombo. The 17th century was a period of peace and prosperity. The Rozwi Empire fell into ruins in the 1830s from invading Nguni from Natal.

By 1500 AD, most of southern Africa had established states. In northwestern Namibia, the Ovambo engaged in farming and the Herero engaged in herding. As cattle numbers increased, the Herero moved southward to central Namibia for grazing land. A related group, the Ovambanderu, expanded to Ghanzi in northwestern Botswana. The Nama, a Khoi-speaking, sheep-raising group, moved northward and came into contact with the Herero; this would set the stage for much conflict between the two groups. The expanding Lozi states pushed the Mbukushu, Subiya, and Yei to Botei, Okavango, and Chobe in northern Botswana.

The development of Sotho–Tswana states based on the highveld, south of the Limpopo River, began around 1000 CE. The chief's power rested on cattle and his connection to the ancestor. This can be seen in the Toutswemogala Hill settlements with stone foundations and stone walls, north of the highveld and south of the Vaal River. Northwest of the Vaal River developed early Tswana states centered on towns of thousands of people. When disagreements or rivalry arose, different groups moved to form their own states.

Southeast of the Drakensberg mountains lived Nguni-speaking peoples (Zulu, Xhosa, Swazi, and Ndebele). They too engaged in state building, with new states developing from rivalry, disagreements, and population pressure causing movement into new regions. This 19th-century process of warfare, state building and migration later became known as the Mfecane (Nguni) or Difaqane (Sotho). Its major catalyst was the consolidation of the Zulu Kingdom. They were metalworkers, cultivators of millet, and cattle herders.

The Khoisan lived in the southwestern Cape Province, where winter rainfall is plentiful. Earlier Khoisan populations were absorbed by Bantu peoples, such as the Sotho and Nguni, but the Bantu expansion stopped at the region with winter rainfall. Some Bantu languages have incorporated the click consonant of the Khoisan languages. The Khoisan traded with their Bantu neighbors, providing cattle, sheep, and hunted items. In return, their Bantu speaking neighbors traded copper, iron, and tobacco.

By the 16th century, the Dutch East India Company established a replenishing station at Table Bay for restocking water and purchasing meat from the Khoikhoi. The Khoikhoi received copper, iron, tobacco, and beads in exchange. In order to control the price of meat and stock and make service more consistent, the Dutch established a permanent settlement at Table Bay in 1652. They grew fresh fruit and vegetables and established a hospital for sick sailors. To increase produce, the Dutch decided to increase the number of farms at Table Bay by encouraging freeburgher "boers" (farmers) on lands worked initially by slaves from West Africa. The land was taken from Khoikhoi grazing land, triggering the first Khoikhoi-Dutch war in 1659. No victors emerged, but the Dutch assumed a "right of conquest" by which they claimed all of the cape. In a series of wars pitting the Khoikhoi against each other, the Boers assumed all Khoikhoi land and claimed all their cattle. The second Khoikoi-Dutch war (1673–1677) was a cattle raid. The Khoikhoi also died in thousands from European diseases.

By the 18th century, the cape colony had grown, with slaves coming from Madagascar, Mozambique, and Indonesia. The settlement also started to expand northward, but Khoikhoi resistance, raids, and guerrilla warfare slowed the expansion during the 18th century. Boers who started to practice pastoralism were known as "trekboers". A common source of "trekboer" labor was orphan children who were captured during raids and whose parents had been killed.

According to the theory of recent African origin of modern humans, the mainstream position held within the scientific community, all humans originate from either Southeast Africa or the Horn of Africa. During the first millennium CE, Nilotic and Bantu-speaking peoples moved into the region.

Following the Bantu Migration, on the coastal section of Southeast Africa, a mixed Bantu community developed through contact with Muslim Arab and Persian traders, leading to the development of the mixed Arab, Persian and African Swahili City States. The Swahili culture that emerged from these exchanges evinces many Arab and Islamic influences not seen in traditional Bantu culture, as do the many Afro-Arab members of the Bantu Swahili people. With its original speech community centered on the coastal parts of Tanzania (particularly Zanzibar) and Kenya—a seaboard referred to as the Swahili Coast—the Bantu Swahili language contains many Arabic language loan-words as a consequence of these interactions.

The earliest Bantu inhabitants of the Southeast coast of Kenya and Tanzania encountered by these later Arab and Persian settlers have been variously identified with the trading settlements of Rhapta, Azania and Menouthias referenced in early Greek and Chinese writings from 50 AD to 500 AD, ultimately giving rise to the name for Tanzania. These early writings perhaps document the first wave of Bantu settlers to reach Southeast Africa during their migration.

Historically, the Swahili people could be found as far north as northern Kenya and as far south as the Ruvuma River in Mozambique. Arab geographers referred to the Swahili coast as the land of the "zanj" (blacks).

Although once believed to be the descendants of Persian colonists, the ancient Swahili are now recognized by most historians, historical linguists, and archaeologists as a Bantu people who had sustained important interactions with Muslim merchants, beginning in the late 7th and early 8th centuries AD.
Medieval Swahili kingdoms are known to have had island trade ports, described by Greek historians as "metropolises", and to have established regular trade routes with the Islamic world and Asia. Ports such as Mombasa, Zanzibar, and Kilwa were known to Chinese sailors under Zheng He and medieval Islamic geographers such as the Berber traveller Abu Abdullah ibn Battuta. The main Swahili exports were ivory, slaves, and gold. They traded with Arabia, India, Persia, and China.

The Portuguese arrived in 1498. On a mission to economically control and Christianize the Swahili coast, the Portuguese attacked Kilwa first in 1505 and other cities later. Because of Swahili resistance, the Portuguese attempt at establishing commercial control was never successful. By the late 17th century, Portuguese authority on the Swahili coast began to diminish. With the help of Omani Arabs, by 1729 the Portuguese presence had been removed. The Swahili coast eventually became part of the Sultanate of Oman. Trade recovered, but it did not regain the levels of the past.

The Urewe culture developed and spread in and around the Lake Victoria region of Africa during the African Iron Age. The culture's earliest dated artifacts are located in the Kagera Region of Tanzania, and it extended as far west as the Kivu region of the Democratic Republic of the Congo, as far east as the Nyanza and Western provinces of Kenya, and north into Uganda, Rwanda and Burundi. Sites from the Urewe culture date from the Early Iron Age, from the 5th century BC to the 6th century AD.

The origins of the Urewe culture are ultimately in the Bantu expansion originating in Cameroon. Research into early Iron Age civilizations in Sub-Saharan Africa has been undertaken concurrently with studies on African linguistics on Bantu expansion. The Urewe culture may correspond to the Eastern subfamily of Bantu languages, spoken by the descendants of the first wave of Bantu peoples to settle East Africa. At first sight, Urewe seems to be a fully developed civilization recognizable through its distinctive, stylish earthenware and highly technical and sophisticated iron working techniques. Given our current level of knowledge, neither seems to have developed or altered for nearly 2,000 years. However, minor local variations in the ceramic ware can be observed.

Urewe is the name of the site in Kenya brought to prominence through the publication in 1948 of Mary Leakey's archaeological findings. She described the early Iron Age period in the Great Lakes region in Central East Africa around Lake Victoria.

Madagascar was apparently first settled by Austronesian speakers from Southeast Asia before the 6th century AD and subsequently by Bantu speakers from the east African mainland in the 6th or 7th century, according to archaeological and linguistic data. The Austronesians introduced banana and rice cultivation, and the Bantu speakers introduced cattle and other farming practices. About the year 1000, Arab and Indian trade settlement were started in northern Madagascar to exploit the Indian Ocean trade. By the 14th century, Islam was introduced on the island by traders. Madagascar functioned in the East African medieval period as a contact port for the other Swahili seaport city-states such as Sofala, Kilwa, Mombasa, and Zanzibar.

Several kingdoms emerged after the 15th century: the Sakalava Kingdom (16th century) on the west coast, Tsitambala Kingdom (17th century) on the east coast, and Merina (15th century) in the central highlands. By the 19th century, Merina controlled the whole island. In 1500, the Portuguese were the first Europeans on the island, raiding the trading settlements.

The British and later the French arrived. During the latter part of the 17th century, Madagascar was a popular transit point for pirates. Radama I (1810–1828) invited Christian missionaries in the early 19th century. Queen Ranavalona I "the Cruel" (1828–1861) banned the practice of Christianity in the kingdom, and an estimated 150,000 Christians perished. Under Radama II (1861–1863), Madagascar took a French orientation, with great commercial concession given to the French. In 1895, in the second Franco-Hova War, the French invaded Madagascar, taking over Antsiranana (Diego Suarez) and declaring Madagascar a protectorate.

Between the 14th and 15th centuries, large Southeast African kingdoms and states emerged, such as the Buganda and Karagwe Kingdoms of Uganda and Tanzania.

By 1000 AD, numerous states had arisen on the Lake Plateau among the Great Lakes of East Africa. Cattle herding, cereal growing, and banana cultivation were the economic mainstays of these states. The Ntusi and Bigo earthworks are representative of one of the first states, the Bunyoro kingdom, which oral tradition stipulates was part of the Empire of Kitara that dominated the whole Lakes region. A Luo ethnic elite, from the Bito clan, ruled over the Bantu-speaking Nyoro people. The society was essentially Nyoro in its culture, based on the evidence from pottery, settlement patterns, and economic specialization.

The Bito clan claimed legitimacy by being descended from the Bachwezi clan, who were said to have ruled the Empire of Kitara. However, very little is known about Kitara; some scholars even question its historical existence. Most founding leaders of the various polities in the lake region seem to have claimed descent from the Bachwezi. There are now 13 million Tara who are part of the second African loss,(Nafi and Uma are two losses).

The Buganda kingdom was founded by Kato Kimera around the 14th century AD. Kato Kintu may have migrated to the northwest of Lake Victoria as early as 1000 BC. Buganda was ruled by the "kabaka" with a "bataka" composed of the clan heads. Over time, the "kabakas" diluted the authority of the "bataka", with Buganda becoming a centralized monarchy. By the 16th century, Buganda was engaged in expansion but had a serious rival in Bunyoro. By the 1870s, Buganda was a wealthy nation-state. The "kabaka" ruled with his "Lukiko" (council of ministers). Buganda had a naval fleet of a hundred vessels, each manned by thirty men. Buganda supplanted Bunyoro as the most important state in the region. However, by the early 20th century, Buganda became a province of the British Uganda Protectorate.

Southeast of Bunyoro, near Lake Kivu at the bottom of the western rift, the Kingdom of Rwanda was founded, perhaps during the 17th century. Tutsi (BaTutsi) pastoralists formed the elite, with a king called the "mwami". The Hutu (BaHutu) were farmers. Both groups spoke the same language, but there were strict social norms against marrying each other and interaction. According to oral tradition, the Kingdom of Rwanda was founded by Mwami Ruganzu II (Ruganzu Ndori) (c. 1600–1624), with his capital near Kigali. It took 200 years to attain a truly centralized kingdom under Mwami Kigeli IV (Kigeri Rwabugiri) (1840–1895). Subjugation of the Hutu proved more difficult than subduing the Tutsi. The last Tutsi chief gave up to Mwami Mutara II (Mutara Rwogera) (1802–1853) in 1852, but the last Hutu holdout was conquered in the 1920s by Mwami Yuhi V (Yuli Musinga) (1896–1931).

South of the Kingdom of Rwanda was the Kingdom of Burundi. It was founded by the Tutsi chief Ntare Rushatsi (c. 1657–1705). Like Rwanda, Burundi was built on cattle raised by Tutsi pastoralists, crops from Hutu farmers, conquest, and political innovations. Under Mwami Ntare Rugaamba (c. 1795–1852), Burundi pursued an aggressive expansionist policy, one based more on diplomacy than force.

The Maravi claimed descent from Karonga ("kalonga"), who took that title as king. The Maravi connected Central Africa to the east coastal trade, with Swahili Kilwa. By the 17th century, the Maravi Empire encompassed all the area between Lake Malawi and the mouth of the Zambezi River. The "karonga" was Mzura, who did much to extend the empire. Mzura made a pact with the Portuguese to establish a 4,000-man army to attack the Shona in return for aid in defeating his rival Lundi, a chief of the Zimba. In 1623, he turned on the Portuguese and assisted the Shona. In 1640, he welcomed back the Portuguese for trade. The Maravi Empire did not long survive the death of Mzura. By the 18th century, it had broken into its previous polities.

The Ghana Empire may have been an established kingdom as early as the 8th century AD, founded among the Soninke by Dinge Cisse. Ghana was first mentioned by Arab geographer Al-Farazi in the late 8th century. Ghana was inhabited by urban dwellers and rural farmers. The urban dwellers were the administrators of the empire, who were Muslims, and the "Ghana" (king), who practiced traditional religion. Two towns existed, one where the Muslim administrators and Berber-Arabs lived, which was connected by a stone-paved road to the king's residence. The rural dwellers lived in villages, which joined together into broader polities that pledged loyalty to the "Ghana." The "Ghana" was viewed as divine, and his physical well-being reflected on the whole society. Ghana converted to Islam around 1050, after conquering Aoudaghost.

The Ghana Empire grew wealthy by taxing the trans-Saharan trade that linked Tiaret and Sijilmasa to Aoudaghost. Ghana controlled access to the goldfields of Bambouk, southeast of Koumbi Saleh. A percentage of salt and gold going through its territory was taken. The empire was not involved in production.

By the 11th century, Ghana was in decline. It was once thought that the sacking of Koumbi Saleh by Berbers under the Almoravid dynasty in 1076 was the cause. This is no longer accepted. Several alternative explanations are cited. One important reason is the transfer of the gold trade east to the Niger River and the Taghaza Trail, and Ghana's consequent economic decline. Another reason cited is political instability through rivalry among the different hereditary polities.
The empire came to an end in 1230, when Takrur in northern Senegal took over the capital.

The Mali Empire began in the 13th century AD, when a Mande (Mandingo) leader, Sundiata (Lord Lion) of the Keita clan, defeated Soumaoro Kanté, king of the Sosso or southern Soninke, at the Battle of Kirina in c. 1235. Sundiata continued his conquest from the fertile forests and Niger Valley, east to the Niger Bend, north into the Sahara, and west to the Atlantic Ocean, absorbing the remains of the Ghana Empire. Sundiata took on the title of "mansa". He established the capital of his empire at Niani.

Although the salt and gold trade continued to be important to the Mali Empire, agriculture and pastoralism was also critical. The growing of sorghum, millet, and rice was a vital function. On the northern borders of the Sahel, grazing cattle, sheep, goats, and camels were major activities. Mande society was organize around the village and land. A cluster of villages was called a "kafu", ruled by a "farma". The "farma" paid tribute to the "mansa". A dedicated army of elite cavalry and infantry maintained order, commanded by the royal court. A formidable force could be raised from tributary regions, if necessary.

Conversion to Islam was a gradual process. The power of the "mansa" depended on upholding traditional beliefs and a spiritual foundation of power. Sundiata initially kept Islam at bay. Later "mansas" were devout Muslims but still acknowledged traditional deities and took part in traditional rituals and festivals, which were important to the Mande. Islam became a court religion under Sundiata's son Uli I (1225–1270). "Mansa" Uli made a pilgrimage to Mecca, becoming recognized within the Muslim world. The court was staffed with literate Muslims as secretaries and accountants. Muslim traveller Ibn Battuta left vivid descriptions of the empire.

Mali reached the peak of its power and extent in the 14th century, when "Mansa" Musa (1312–1337) made his famous "hajj" to Mecca with 500 slaves, each holding a bar of gold worth 500 mitqals. "Mansa" Musa's "hajj" devalued gold in Mamluk Egypt for a decade. He made a great impression on the minds of the Muslim and European world. He invited scholars and architects like Ishal al-Tuedjin (al-Sahili) to further integrate Mali into the Islamic world.

The Mali Empire saw an expansion of learning and literacy. In 1285, Sakura, a freed slave, usurped the throne. This "mansa" drove the Tuareg out of Timbuktu and established it as a center of learning and commerce. The book trade increased, and book copying became a very respectable and profitable profession. Timbuktu and Djenné became important centers of learning within the Islamic world.

After the reign of Mansa Suleyman (1341–1360), Mali began its spiral downward. Mossi cavalry raided the exposed southern border. Tuareg harassed the northern border in order to retake Timbuktu. Fulani (Fulbe) eroded Mali's authority in the west by establishing the independent Imamate of Futa Toro, a successor to the kingdom of Takrur. Serer and Wolof alliances were broken. In 1545 to 1546, the Songhai Empire took Niani. After 1599, the empire lost the Bambouk goldfields and disintegrated into petty polities.

The Songhai people are descended from fishermen on the Middle Niger River. They established their capital at Kukiya in the 9th century AD and at Gao in the 12th century. The Songhai speak a Nilo-Saharan language.

Sonni Ali, a Songhai, began his conquest by capturing Timbuktu in 1468 from the Tuareg. He extended the empire to the north, deep into the desert, pushed the Mossi further south of the Niger, and expanded southwest to Djenne. His army consisted of cavalry and a fleet of canoes. Sonni Ali was not a Muslim, and he was portrayed negatively by Berber-Arab scholars, especially for attacking Muslim Timbuktu. After his death in 1492, his heirs were deposed by General Muhammad Ture, a Muslim of Soninke origins

Muhammad Ture (1493–1528) founded the Askiya Dynasty, "askiya" being the title of the king. He consolidated the conquests of Sonni Ali. Islam was used to extend his authority by declaring jihad on the Mossi, reviving the trans-Saharan trade, and having the Abbasid "shadow" caliph in Cairo declare him as caliph of Sudan. He established Timbuktu as a great center of Islamic learning. Muhammad Ture expanded the empire by pushing the Tuareg north, capturing Aïr in the east, and capturing salt-producing Taghaza. He brought the Hausa states into the Songhay trading network. He further centralized the administration of the empire by selecting administrators from loyal servants and families and assigning them to conquered territories. They were responsible for raising local militias. Centralization made Songhay very stable, even during dynastic disputes. Leo Africanus left vivid descriptions of the empire under Askiya Muhammad. Askiya Muhammad was deposed by his son in 1528. After much rivalry, Muhammad Ture's last son Askiya Daoud (1529–1582) assumed the throne.

In 1591, Morocco invaded the Songhai Empire under Ahmad al-Mansur of the Saadi Dynasty in order to secure the goldfields of the Sahel. At the Battle of Tondibi, the Songhai army was defeated. The Moroccans captured Djenne, Gao, and Timbuktu, but they were unable to secure the whole region. Askiya Nuhu and the Songhay army regrouped at Dendi in the heart of Songhai territory where a spirited guerrilla resistance sapped the resources of the Moroccans, who were dependent upon constant resupply from Morocco. Songhai split into several states during the 17th century.

Morocco found its venture unprofitable. The gold trade had been diverted to Europeans on the coast. Most of the trans-Saharan trade was now diverted east to Bornu. Expensive equipment purchased with gold had to be sent across the Sahara, an unsustainable scenario. The Moroccans who remained married into the population and were referred to as "Arma" or "Ruma". They established themselves at Timbuktu as a military caste with various fiefs, independent from Morocco. Amid the chaos, other groups began to assert themselves, including the Fulani of Futa Tooro who encroached from the west. The Bambara Empire, one of the states that broke from Songhai, sacked Gao. In 1737, the Tuareg massacred the "Arma".

The Fulani were migratory people. They moved from Mauritania and settled in Futa Tooro, Futa Djallon, and subsequently throughout the rest of West Africa. By the 14th century CE, they had converted to Islam. During the 16th century, they established themselves at Macina in southern Mali. During the 1670s, they declared jihads on non-Muslims. Several states were formed from these jihadist wars, at Futa Toro, Futa Djallon, Macina, Oualia, and Bundu. The most important of these states was the Sokoto Caliphate or Fulani Empire.

In the city of Gobir, Usman dan Fodio (1754–1817) accused the Hausa leadership of practicing an impure version of Islam and of being morally corrupt. In 1804, he launched the Fulani War as a jihad among a population that was restless about high taxes and discontented with its leaders. Jihad fever swept northern Nigeria, with strong support among both the Fulani and the Hausa. Usman created an empire that included parts of northern Nigeria, Benin, and Cameroon, with Sokoto as its capital. He retired to teach and write and handed the empire to his son Muhammed Bello. The Sokoto Caliphate lasted until 1903 when the British conquered northern Nigeria.

The Akan speak a Kwa language. The speakers of Kwa languages are believed to have come from East/Central Africa, before settling in the Sahel. By the 12th century, the Akan Kingdom of Bonoman (Bono State) was established. During the 13th century, when the gold mines in modern-day Mali started to dry up, Bonoman and later other Akan states began to rise to prominence as the major players in the Gold trade. It was Bonoman and other Akan kingdoms like Denkyira, Akyem, Akwamu which were the predecessors, and later the emergence of the Empire of Ashanti. When and how the Ashante got to their present location is debatable. What is known is that by the 17th century an Akan people were identified as living in a state called Kwaaman. The location of the state was north of Lake Bosomtwe. The state's revenue was mainly derived from trading in gold and kola nuts and clearing forest to plant yams. They built towns between the Pra and Ofin rivers. They formed alliances for defense and paid tribute to Denkyira one of the more powerful Akan states at that time along with Adansi and Akwamu. During the 16th century, Ashante society experienced sudden changes, including population growth because of cultivation of New World plants such as cassava and maize and an increase in the gold trade between the coast and the north.

By the 17th century, Osei Kofi Tutu I (c. 1695–1717), with help of Okomfo Anokye, unified what became the Ashante into a confederation with the Golden Stool as a symbol of their unity and spirit. Osei Tutu engaged in a massive territorial expansion. He built up the Ashante army based on the Akan state of Akwamu, introducing new organization and turning a disciplined militia into an effective fighting machine. In 1701, the Ashante conquered Denkyira, giving them access to the coastal trade with Europeans, especially the Dutch. Opoku Ware I (1720–1745) engaged in further expansion, adding other southern Akan states to the growing empire. He turned north adding Techiman, Banda, Gyaaman, and Gonja, states on the Black Volta. Between 1744 and 1745, "Asantehene" Opoku attacked the powerful northern state of Dagomba, gaining control of the important middle Niger trade routes. Kusi Obodom (1750–1764) succeeded Opoku. He solidified all the newly won territories. Osei Kwadwo (1777–1803) imposed administrative reforms that allowed the empire to be governed effectively and to continue its military expansion. Osei Kwame Panyin (1777–1803), Osei Tutu Kwame (1804–1807), and Osei Bonsu (1807–1824) continued territorial consolidation and expansion. The Ashante Empire included all of present-day Ghana and large parts of the Ivory Coast.

The "Ashantehene" inherited his position from his mother. He was assisted at the capital, Kumasi, by a civil service of men talented in trade, diplomacy, and the military, with a head called the "Gyaasehene". Men from Arabia, Sudan, and Europe were employed in the civil service, all of them appointed by the "Ashantehene". At the capital and in other towns, the "ankobia" or special police were used as bodyguards to the "Ashantehene", as sources of intelligence, and to suppress rebellion. Communication throughout the empire was maintained via a network of well-kept roads from the coast to the middle Niger and linking together other trade cities.

For most of the 19th century, the Ashante Empire remained powerful. It was later destroyed in 1900 by British superior weaponry and organization following the four Anglo-Ashanti wars.

The Dahomey Kingdom was founded in the early 17th century when the Aja people of the Allada kingdom moved northward and settled among the Fon. They began to assert their power a few years later. In so doing they established the Kingdom of Dahomey, with its capital at Agbome. King Houegbadja (c. 1645–1685) organized Dahomey into a powerful centralized state. He declared all lands to be owned of the king and subject to taxation. Primogeniture in the kingship was established, neutralizing all input from village chiefs. A "cult of kingship" was established. A captive slave would be sacrificed annually to honor the royal ancestors. During the 1720s, the slave-trading states of Whydah and Allada were taken, giving Dahomey direct access to the slave coast and trade with Europeans. King Agadja (1708–1740) attempted to end the slave trade by keeping the slaves on plantations producing palm oil, but the European profits on slaves and Dahomey's dependency on firearms were too great. In 1730, under king Agaja, Dahomey was conquered by the Oyo Empire, and Dahomey had to pay tribute. Taxes on slaves were mostly paid in cowrie shells. During the 19th century, palm oil was the main trading commodity. France conquered Dahomey during the Second Franco-Dahomean War (1892–1894) and established a colonial government there. Most of the troops who fought against Dahomey were native Africans.

Traditionally, the Yoruba people viewed themselves as the inhabitants of a united empire, in contrast to the situation today, in which "Yoruba" is the cultural-linguistic designation for speakers of a language in the Niger–Congo family. The name comes from a Hausa word to refer to the Oyo Empire. The first Yoruba state was Ile-Ife, said to have been founded around 1000 AD by a supernatural figure, the first "oni" Oduduwa. Oduduwa's sons would be the founders of the different city-states of the Yoruba, and his daughters would become the mothers of the various Yoruba "obas", or kings. Yoruba city-states were usually governed by an "oba" and an "iwarefa", a council of chiefs who advised the "oba." by the 18th century, the Yoruba city-states formed a loose confederation, with the "Oni" of Ife as the head and Ife as the capital. As time went on, the individual city-states became more powerful with their "obas" assuming more powerful spiritual positions and diluting the authority of the "Oni" of Ife. Rivalry became intense among the city-states.

The Oyo Empire rose in the 16th century. The Oyo state had been conquered in 1550 by the kingdom of Nupe, which was in possession of cavalry, an important tactical advantage. The "alafin" (king) of Oyo was sent into exile. After returning, "Alafin" Orompoto (c. 1560–1580) built up an army based on heavily armed cavalry and long-service troops. This made them invincible in combat on the northern grasslands and in the thinly wooded forests. By the end of the 16th century, Oyo had added the western region of the Niger to the hills of Togo, the Yoruba of Ketu, Dahomey, and the Fon nation.

A governing council served the empire, with clear executive divisions. Each acquired region was assigned a local administrator. Families served in king-making capacities. Oyo, as a northern Yoruba kingdom, served as middle-man in the north–south trade and connecting the eastern forest of Guinea with the western and central Sudan, the Sahara, and North Africa. The Yoruba manufactured cloth, ironware, and pottery, which were exchanged for salt, leather, and most importantly horses from the Sudan to maintain the cavalry. Oyo remained strong for two hundred years. It became a protectorate of Great Britain in 1888, before further fragmenting into warring factions. The Oyo state ceased to exist as any sort of power in 1896.

The Kwa Niger–Congo speaking Edo people had established the Benin Empire by the middle of the 15th century. It was engaged in political expansion and consolidation from its very beginning. Under "Oba" (king) Ewuare (c. 1450–1480 AD), the state was organized for conquest. He solidified central authority and initiated 30 years of war with his neighbors. At his death, the Benin Empire extended to Dahomey in the west, to the Niger Delta in the east, along the west African coast, and to the Yoruba towns in the north.

Ewuare's grandson "Oba" Esigie (1504–1550) eroded the power of the "uzama" (state council) and increased contact and trade with Europeans, especially with the Portuguese who provided a new source of copper for court art.
The "oba" ruled with the advice of the "uzama", a council consisting of chiefs of powerful families and town chiefs of different guilds. Later its authority was diminished by the establishment of administrative dignitaries. Women wielded power. The queen mother who produced the future "oba" wielded immense influence.

Benin was never a significant exporter of slaves, as Alan Ryder's book Benin and the Europeans showed. By the early 18th century, it was wrecked with dynastic disputes and civil wars. However, it regained much of its former power in the reigns of Oba Eresoyen and Oba Akengbuda. After the 16th century, Benin mainly exported pepper, ivory, gum, and cotton cloth to the Portuguese and Dutch who resold it to other African societies on the coast. In 1897, the British sacked the city.

The Niger Delta comprised numerous city-states with numerous forms of government. These city-states were protected by the waterways and thick vegetation of the delta. The region was transformed by trade in the 17th century. The delta's city-states were comparable to those of the Swahili people in East Africa. Some, like Bonny, Kalabari, and Warri, had kings. Others, like Brass, were republics with small senates, and those at Cross River and Old Calabar were ruled by merchants of the "ekpe" society. The "ekpe" society regulated trade and made rules for members known as house systems. Some of these houses, like the Pepples of Bonny, were well known in the Americas and Europe.

The Igbo lived east of the delta (but with the Anioma on the west of the Niger River). The Kingdom of Nri rose in the 9th century, with the "Eze" Nri being its leader. It was a political entity composed of villages, and each village was autonomous and independent with its own territory and name, each recognized by its neighbors. Villages were democratic with all males and sometimes females a part of the decision-making process. Graves at Igbo-Ukwu (800 AD) contained brass artifacts of local manufacture and glass beads from Egypt or India, indicative of extraregional trade.

By the 1850s, British and German missionaries and traders had penetrated present-day Namibia. Herero and Nama competed for guns and ammunition, providing cattle, ivory, and ostrich feathers. The Germans were more firmly established than the British in the region. By 1884, the Germans declared the coastal region from the Orange River to the Kunene River a German protectorate, part of German South West Africa. They pursued an aggressive policy of land expansion for white settlements. They exploited rivalry between the Nama and Herero.

The Herero entered into an alliance with the Germans, thinking they could get an upper hand on the Nama. The Germans set up a garrison at the Herero capital and started allocating Herero land for white settlements, including the best grazing land in the central plateau, and made tax and labor demands. The Herero and Ovambanderu rebelled, but the rebellion was crushed and leaders were executed. Between 1896 and 1897, rinderpest crippled the economic backbone of the Herero and Nama economy and slowed white expansion. The Germans continued the policy of making Namibia a white settlement by seizing land and cattle, and even trying to export Herero labor to South Africa.

In 1904, the Herero rebelled. German General Lothar von Trotha implemented an extermination policy at the Battle of Waterberg, which drove the Herero west of the Kalahari Desert. At the end of 1905, only 16,000 Herero were alive, out of a previous population of 80,000. Nama resistance was crushed in 1907. All Nama and Herero cattle and land were confiscated from the very diminished population, with remaining Nama and Herero assuming a subordinate position. Labor had to be imported from among the Ovambo.

A moment of great disorder in southern Africa was the "Mfecane", "the crushing." It was started by the northern Nguni kingdoms of Mthethwa, Ndwandwe, and Swaziland over scarce resource and famine. When Dingiswayo of Mthethwa died, Shaka of the Zulu people took over. He established the Zulu Kingdom, asserting authority over the Ndwandwe and pushing the Swazi north. The scattering Ndwandwe and Swazi caused the Mfecane to spread. During the 1820s, Shaka expanded the empire all along the Drakensberg foothills, with tribute being paid as far south as the Tugela and Umzimkulu rivers. He replaced the chiefs of conquered polities with "indunas", responsible to him. He introduced a centralized, dedicated, and disciplined military force not seen in the region, with a new weapon in the short stabbing-spear.

In 1828, Shaka was assassinated by his half brother Dingane, who lacked the military genius and leadership skills of Shaka. Voortrekkers tried to occupy Zulu land in 1838. In the early months they were defeated, but the survivors regrouped at the Ncome River and soundly defeated the Zulu. However, the Voortrekkers dared not settle Zulu land. Dingane was killed in 1840 during a civil war. His brother Mpande took over and strengthened Zulu territories to the north. In 1879 the Zulu Kingdom was invaded by Britain in a quest to control all of South Africa. The Zulu Kingdom was victorious at the Battle of Isandlwana but was defeated at the Battle of Ulundi.

One of the major states to emerge from the Mfecane was the Sotho Kingdom founded at Thaba Bosiu by Moshoeshoe I around 1821 to 1822. It was a confederation of different polities that accepted the absolute authority of Moshoeshoe. During the 1830s, the kingdom invited missionaries as a strategic means of acquiring guns and horses from the Cape. The Orange Free State slowly diminished the kingdom but never completely defeated it. In 1868, Moshoeshoe asked that the Sotho Kingdom be annexed by Britain, to save the remnant. It became the British protectorate of Basutoland.

The arrival of the ancestors of the Tswana-speakers who came to control the region (from the Vaal River to Botswana) has yet to be dated precisely although AD 600 seems to be a consensus estimate. This massive cattle-raising complex prospered until 1300 AD or so. All these various peoples were connected to trade routes that ran via the Limpopo River to the Indian Ocean, and trade goods from Asia such as beads made their way to Botswana most likely in exchange for ivory, gold, and rhinoceros horn.
The first written records relating to modern-day Botswana appear in 1824. What these records show is that the Bangwaketse had become the predominant power in the region. Under the rule of Makaba II, the Bangwaketse kept vast herds of cattle in well-protected desert areas, and used their military prowess to raid their neighbours. Other chiefdoms in the area, by this time, had capitals of 10,000 or so and were fairly prosperous. This equilibrium came to end during the Mfecane period, 1823–1843, when a succession of invading peoples from South Africa entered the country. Although the Bangwaketse were able to defeat the invading Bakololo in 1826, over time all the major chiefdoms in Botswana were attacked, weakened, and impoverished. The Bakololo and Amandebele raided repeatedly, and took large numbers of cattle, women, and children from the Batswana—most of whom were driven into the desert or sanctuary areas such as hilltops and caves. Only after 1843, when the Amandebele moved into western Zimbabwe, did this threat subside.
During the 1840s and 1850s trade with Cape Colony-based merchants opened up and enabled the Batswana chiefdoms to rebuild. The Bakwena, Bangwaketse, Bangwato and Batawana cooperated to control the lucrative ivory trade, and then used the proceeds to import horses and guns, which in turn enabled them to establish control over what is now Botswana. This process was largely complete by 1880, and thus the Bushmen, the Bakalanga, the Bakgalagadi, the Batswapong and other current minorities were subjugated by the Batswana. Following the Great Trek, Afrikaners from the Cape Colony established themselves on the borders of Botswana in the Transvaal. In 1852 a coalition of Tswana chiefdoms led by Sechele I resisted Afrikaner incursions, and after about eight years of intermittent tensions and hostilities, eventually came to a peace agreement in Potchefstroom in 1860. From that point on, the modern-day border between South Africa and Botswana was agreed on, and the Afrikaners and Batswana traded and worked together peacefully.
In the 1820s, refugees from the Zulu expansion under Shaka came into contact with the Basotho people residing on the highveld.
In 1823, those pressures caused one group of Basotho, the Kololo, to migrate north, past the Okavango Swamp and across the Zambezi into Barotseland, now part of Zambia. In 1845, the Kololo conquered Barotseland.

At about the same time, the Boers began to encroach upon Basotho territory. After the Cape Colony had been ceded to Britain at the conclusion of the Napoleonic Wars, the "voortrekkers" ("pioneers") were farmers who opted to leave the former Dutch colony and moved inland where they eventually established independent polities.

At the time of these developments, Moshoeshoe I gained control of the Basotho kingdoms of the southern Highveld. Universally praised as a skilled diplomat and strategist, he was able to wield the disparate refugee groups escaping the Difaqane into a cohesive nation.
His inspired leadership helped his small nation to survive the dangers and pitfalls (the Zulu hegemony, the inward expansion of the voortrekkers and the designs of imperial Britain) that destroyed other indigenous South African kingdoms during the 19th century

In 1822, Moshoeshoe established his capital at Butha-Buthe, an easily defensible mountain in the northern Drakensberg mountains, laying the foundations of the eventual Kingdom of Lesotho. His capital was later moved to Thaba Bosiu

To deal with the encroaching voortrekker groups, Moshoeshoe encouraged French missionary activity in his kingdom. Missionaries sent by the Paris Evangelical Missionary Society provided the King with foreign affairs counsel and helped to facilitate the purchase of modern weapons.

Aside from acting as state ministers, missionaries (primarily Casalis and Arbousset) played a vital role in delineating Sesotho orthography and printing Sesotho language materials between 1837 and 1855. The first Sesotho translation of the Bible appeared in 1878.

In 1868, after losing the western lowlands to the Boers during the Free State–Basotho Wars; Moshoeshoe successfully appealed to Queen Victoria to proclaim Lesotho (then known as Basutoland) a protectorate of Britain and the British administration was placed in Maseru, the site of Lesotho's current capital. Local chieftains retained power over internal affairs while Britain was responsible for foreign affairs and the defence of the protectorate. In 1869, the British sponsored a process by which the borders of Basutoland were finally demarcated.
While many clans had territory within Basutoland, large numbers of Sesotho speakers resided in areas allocated to the Orange Free State, the sovereign voortrekker republic that bordered the Basotho kingdom.

By the 19th century, most Khoikhoi territory was under Boer control. The Khoikhoi had lost economic and political independence and had been absorbed into Boer society. The Boers spoke Afrikaans, a language or dialect derived from Dutch, and no longer called themselves Boers but Afrikaners. Some Khoikhoi were used as commandos in raids against other Khoikhoi and later Xhosa. A mixed Khoi, slave, and European population called the Cape Coloureds, who were outcasts within colonial society, also arose. Khoikhoi who lived far on the frontier included the Kora, Oorlams, and Griqua. In 1795, the British took over the cape colony from the Dutch.

In the 1830s, Boers embarked on a journey of expansion, east of the Great Fish River into the Zuurveld. They were referred to as "Voortrekkers". They founded republics of the Transvaal and Orange Free State, mostly in areas of sparse population that had been diminished by the "Mfecane/Difaqane". Unlike the Khoisan, the Bantu states were not conquered by the Afrikaners, because of population density and greater unity. Additionally, they began to arm themselves with guns acquired through trade at the cape. In some cases, as in the Xhosa/Boer Wars, Boers were removed from Xhosa lands. It required a dedicated imperial military force to subdue the Bantu-speaking states. In 1901, the Boer republics were defeated by Britain in the Second Boer War. The defeat however consummated many Afrikaners' ambition: South Africa would be under white rule. The British placed all power—legislative, executive, administrative—in English and Afrikaner hands.

Between 1878 and 1898, European states partitioned and conquered most of Africa. For 400 years, European nations had mainly limited their involvement to trading stations on the African coast. Few dared venture inland from the coast; those that did, like the Portuguese, often met defeats and had to retreat to the coast. Several technological innovations helped to overcome this 400-year pattern. One was the development of repeating rifles, which were easier and quicker to load than muskets. Artillery was being used increasingly. In 1885, Hiram S. Maxim developed the maxim gun, the model of the modern-day machine gun. European states kept these weapons largely among themselves by refusing to sell these weapons to African leaders.

African germs took numerous European lives and deterred permanent settlements. Diseases such as yellow fever, sleeping sickness, yaws, and leprosy made Africa a very inhospitable place for Europeans. The deadliest disease was malaria, endemic throughout Tropical Africa. In 1854, the discovery of quinine and other medical innovations helped to make conquest and colonization in Africa possible.

Strong motives for conquest of Africa were at play. Raw materials were needed for European factories. Europe in the early part of the 19th century was undergoing its Industrial Revolution. Nationalist rivalries and prestige were at play. Acquiring African colonies would show rivals that a nation was powerful and significant. These factors culminated in the Scramble for Africa.

Knowledge of Africa increased. Numerous European explorers began to explore the continent. Mungo Park traversed the Niger River. James Bruce travelled through Ethiopia and located the source of the Blue Nile. Richard Francis Burton was the first European at Lake Tanganyika. Samuel White Baker explored the Upper Nile. John Hanning Speke located a source of the Nile at Lake Victoria. Other significant European explorers included Heinrich Barth, Henry Morton Stanley (coiner of the term "Dark Continent" for Africa in an 1878 book), Silva Porto, Alexandre de Serpa Pinto, Rene Caille, Friedrich Gerhard Rohlfs, Gustav Nachtigal, George Schweinfurth, and Joseph Thomson. The most famous of the explorers was David Livingstone, who explored southern Africa and traversed the continent from the Atlantic at Luanda to the Indian Ocean at Quelimane. European explorers made use of African guides and servants, and established long-distance trading routes.

Missionaries attempting to spread Christianity also increased European knowledge of Africa. Between 1884 and 1885, European nations met at the Berlin West Africa Conference to discuss the partitioning of Africa. It was agreed that European claims to parts of Africa would only be recognised if Europeans provided effective occupation. In a series of treaties in 1890–1891, colonial boundaries were completely drawn. All of Sub-Saharan Africa was claimed by European powers, except for Ethiopia (Abyssinia) and Liberia.

The European powers set up a variety of different administrations in Africa, reflecting different ambitions and degrees of power. In some areas, such as parts of British West Africa, colonial control was tenuous and intended for simple economic extraction, strategic power, or as part of a long-term development plan. In other areas, Europeans were encouraged to settle, creating settler states in which a European minority dominated. Settlers only came to a few colonies in sufficient numbers to have a strong impact. British settler colonies included British East Africa (now Kenya), Northern and Southern Rhodesia, (Zambia and Zimbabwe, respectively), and South Africa, which already had a significant population of European settlers, the Boers. France planned to settle Algeria and eventually incorporate it into the French state on an equal basis with the European provinces. Algeria's proximity across the Mediterranean allowed plans of this scale.

In most areas colonial administrations did not have the manpower or resources to fully administer the territory and had to rely on local power structures to help them. Various factions and groups within the societies exploited this European requirement for their own purposes, attempting to gain positions of power within their own communities by cooperating with Europeans. One aspect of this struggle included what Terence Ranger has termed the "invention of tradition." In order to legitimize their own claims to power in the eyes of both the colonial administrators and their own people, native elites would essentially manufacture "traditional" claims to power, or ceremonies. As a result, many societies were thrown into disarray by the new order.

Following the Scramble for Africa, an early but secondary focus for most colonial regimes was the suppression of slavery and the slave trade. By the end of the colonial period they were mostly successful in this aim, though slavery is still very active in Africa.

As a part of the Scramble for Africa, France had the establishment of a continuous west–east axis of the continent as an objective, in contrast with the British north–south axis. Tensions between Britain and France reached tinder stage in Africa. At several points war was possible, but never happened. The most serious episode was the Fashoda Incident of 1898. French troops tried to claim an area in the Southern Sudan, and a much more powerful British force purporting to be acting in the interests of the Khedive of Egypt arrived to confront them. Under heavy pressure the French withdrew securing British control over the area. The status quo was recognised by an agreement between the two states acknowledging British control over Egypt, while France became the dominant power in Morocco, but France suffered a humiliating defeat overall.

Belgium

France

Germany
Italy

Portugal

Spain
United Kingdom
Independent states

In the 1880s the European powers had divided up almost all of Africa (only Ethiopia and Liberia were independent). They ruled until after World War II when forces of nationalism grew much stronger. In the 1950s and 1960s the colonial holdings became independent states. The process was usually peaceful but there were several long bitter bloody civil wars, as in Algeria, Kenya and elsewhere. Across Africa the powerful new force of nationalism drew upon the organizational skills that natives learned in the British and French and other armies in the world wars. It led to organizations that were not controlled by or endorsed by either the colonial powers not the traditional local power structures that were collaborating with the colonial powers. Nationalistic organizations began to challenge both the traditional and the new colonial structures and finally displaced them. Leaders of nationalist movements took control when the European authorities exited; many ruled for decades or until they died off. These structures included political, educational, religious, and other social organizations. In recent decades, many African countries have undergone the triumph and defeat of nationalistic fervor, changing in the process the loci of the centralizing state power and patrimonial state.

With the vast majority of the continent under the colonial control of European governments, the World Wars were significant events in the geopolitical history of Africa. Africa was a theater of war and saw fighting in both wars. More important in most regions, the total war footing of colonial powers impacted the governance of African colonies, through resource allocation, conscription, and taxation. In World War I there were several campaigns in Africa, including the Togoland Campaign, the Kamerun Campaign, the South West Africa campaign, and the East African campaign. In each, Allied forces, primarily British, but also French, Belgian, South African, and Portuguese, sought to force the Germans out of their African colonies. In each, German forces were badly outnumbered and, due to Allied naval superiority, were cut off from reinforcement or resupply. The Allies eventually conquered all German colonies; German forces in East Africa managed to avoid surrender throughout the war, though they could not hold any territory after 1917. After World War I, former German colonies in Africa were taken over by France, Belgium, and the British Empire.

After World War I, colonial powers continued to consolidate their control over their African territories. In some areas, particularly in Southern and East Africa, large settler populations were successful in pressing for additional devolution of administration, so-called "home rule" by the white settlers. In many cases, settler regimes were harsher on African populations, tending to see them more as a threat to political power, as opposed to colonial regimes which had generally endeavored to co-opt local populations into economic production. The Great Depression strongly affected Africa's non-subsistence economy, much of which was based on commodity production for Western markets. As demand increased in the late 1930s, Africa's economy rebounded as well.

Africa was the site of one of the first instances of fascist territorial expansions in the 1930s. Italy had attempted to conquer Ethiopia in the 1890s but had been rebuffed in the First Italo-Ethiopian War. Ethiopia lay between two Italian colonies, Italian Somaliland and Eritrea and was invaded in October 1935. With an overwhelming advantage in armor and aircraft, by May 1936, Italian forces had occupied the capital of Addis Ababa and effectively declared victory. Ethiopia and their other colonies were consolidated into Italian East Africa.

Africa was a large continent whose geography gave it strategic importance during the war. North Africa was the scene of major British and American campaigns against Italy and Germany; East Africa was the scene of a major British campaign against Italy. The vast geography provided major transportation routes linking the United States to the Middle East and Mediterranean regions. The sea route around South Africa was heavily used even though it added 40 days to voyages that had to avoid the dangerous Suez region. Lend Lease supplies to Russia often came this way. Internally, long-distance road and railroad connections facilitated the British war effort. The Union of Africa had dominion status and was largely self-governing, the other British possessions were ruled by the colonial office, usually with close ties to local chiefs and kings. Italian holdings were the target of successful British military campaigns. The Belgian Congo, and two other Belgian colonies, were major exporters. In terms of numbers and wealth, the British -controlled the richest portions of Africa, and made extensive use not only of the geography, but the manpower, and the natural resources. Civilian colonial officials made a special effort to upgrade the African infrastructure, promote agriculture, integrate colonial Africa with the world economy, and recruit over a half million soldiers.

Before the war, Britain had made few plans for the utilization of Africa, but it quickly set up command structures. The Army set up the West Africa Command, which recruited 200,000 soldiers. The East Africa Command was created in September 1941 to support the overstretched Middle East Command. It provided the largest number of men, over 320,000, chiefly from Kenya, Tanganyika, and Uganda. The Southern Command was the domain of South Africa. The Royal Navy set up the South Atlantic Command based in Sierra Leone, that became one of the main convoy assembly points. The RAF Coastal Command had major submarine-hunting operations based in West Africa, while a smaller RAF command Dealt with submarines in the Indian Ocean. Ferrying aircraft from North America and Britain was the major mission of the Western Desert Air Force. In addition smaller more localized commands were set up throughout the war.

Before 1939, the military establishments were very small throughout British Africa, and largely consisted of whites, who comprised under two percent of the population outside South Africa. As soon as the war began, newly created African units were set up, primarily by the Army. The new recruits were almost always volunteers, usually provided in close cooperation with local tribal leaders. During the war, military pay scales far exceeded what civilians natives could earn, especially when food, housing and clothing allowances are included. The largest numbers were in construction units, called Pioneer units, with over 82,000 soldiers.. The RAF and Navy also did some recruiting. The volunteers did some fighting, a great deal of guard duty, and construction work. 80,000 served in the Middle East. A special effort was made not to challenge white supremacy, certainly before the war, and to a large extent during the war itself. Nevertheless, the soldiers were drilled and train to European standards, given strong doses of propaganda, and learn leadership and organizational skills that proved essential to the formation of nationalistic and independence movements after 1945. There were minor episodes of discontent, but nothing serious, among the natives. Afrikaner nationalism was a factor in South Africa, But the proto-German Afrikaner prime minister was replaced in 1939 by Jan Smuts, an Afrikaner who was an enthusiastic supporter of the British Empire. His government closely cooperated with London and raised 340,000 volunteers (190,000 were white, or about one-third of the eligible white men).

As early as 1857, the French established volunteer units of black soldiers in sub- Sahara Africa, termed the "tirailleurs senegalais." They served in military operations throughout the Empire, including 171,000 soldiers in World War I and 160,000 in World War II. About 90,000 became POWs in Germany. The veterans played a central role in the postwar independence movement in French Africa.

authorities in West Africa declared allegiance to the Vichy regime, as did the colony of French Gabon Vichy forces defeated a Free French Forces invasion of French West Africa in the two battles of Dakar in July and September 1940. Gabon fell to Free France after the Battle of Gabon in November 1940, but West Africa remained under Vichy control until November 1942. Vichy forces tried to resist the overwhelming Allied landings in North Africa (operation "Torch") in November 1942. Vichy Admiral François Darlan suddenly switched sides and the fighting ended. The Allies gave Darlan control of North African French forces in exchange for support from both French North Africa as well as French West Africa. Vichy was now eliminated as a factor in Africa. Darlan was assassinated in December, and the two factions of Free French, led by Charles de Gaulle and Henri Giraud, jockeyed for power. De Gaulle finally won out.

Since Germany had lost its African colonies following World War I, World War II did not reach Africa until Italy joined the war on June 10, 1940, controlling Libya and Italian East Africa. With the fall of France on June 25, most of France's colonies in North and West Africa were controlled by the Vichy government, though much of Central Africa fell under Free French control after some fighting between Vichy and Free French forces at the Battle of Dakar and the Battle of Gabon. After the fall of France, Africa was the only active theater for ground combat until the Italian invasion of Greece in October. In the Western Desert campaign Italian forces from Libya sought to overrun Egypt, controlled by the British. Simultaneously, in the East African campaign, Italian East African forces overran British Somaliland and some British outposts in Kenya and Anglo-Egyptian Sudan. When Italy's efforts to conquer Egypt (including the crucial Suez Canal) and Sudan fell short, they were unable to reestablish supply to Italian East Africa. Without the ability to reinforce or resupply and surrounded by Allied possessions, Italian East Africa was conquered by mainly British and South African forces in 1941. In North Africa, the Italians soon requested help from the Germans who sent a substantial force under General Rommel. With German help, the Axis forces regained the upper hand but were unable to break through British defenses in two tries at El Alamein. In late 1942, Allied forces, mainly Americans and Canadians, invaded French North Africa in Operation Torch, where Vichy French forces initially surprised them with their resistance but were convinced to stop fighting after three days. The second front relieved pressure on the British in Egypt who began pushing west to meet up with the Torch forces, eventually pinning German and Italian forces in Tunisia, which was conquered by May 1943 in the Tunisia campaign, ending the war in Africa. The only other significant operations occurred in the French colony of Madagascar, which was invaded by the British in May 1942 to deny its ports to the Axis (potentially the Japanese who had reached the eastern Indian Ocean). The French garrisons in Madagascar surrendered in November 1942.

The decolonization of Africa started with Libya in 1951, although Liberia, South Africa, Egypt and Ethiopia were already independent. Many countries followed in the 1950s and 1960s, with a peak in 1960 with the Year of Africa, which saw 17 African nations declare independence, including a large part of French West Africa. Most of the remaining countries gained independence throughout the 1960s, although some colonizers (Portugal in particular) were reluctant to relinquish sovereignty, resulting in bitter wars of independence which lasted for a decade or more. The last African countries to gain formal independence were Guinea-Bissau (1974), Mozambique (1975) and Angola (1975) from Portugal; Djibouti from France in 1977; Zimbabwe from the United Kingdom in 1980; and Namibia from South Africa in 1990. Eritrea later split off from Ethiopia in 1993.

The Mau Mau Uprising took place in Kenya from 1952 until 1956 but was put down by British and local forces. A state of emergency remained in place until 1960. Kenya became independent in 1963, and Jomo Kenyatta served as its first president.

The early 1960s also signaled the start of major clashes between the Hutus and the Tutsis in Rwanda and Burundi. In 1994 this culminated in the Rwandan genocide, a conflict in which over 800,000 people were murdered.

Moroccan nationalism developed during the 1930s; the Istiqlal Party was formed, pushing for independence. In 1953 sultan Mohammed V of Morocco called for independence. On March 2, 1956, Morocco became independent of France. Mohammed V became ruler of independent Morocco.

In 1954, Algeria formed the National Liberation Front (FLN) as it split from France. This resulted in the Algerian War, which lasted until independence negotiations in 1962. Muhammad Ahmed Ben Bella was elected President of Algeria. Over a million French nationals, predominantly Pied-Noirs, left the country, crippling the economy.

In 1934, the "Neo Destour" (New Constitution) party was founded by Habib Bourguiba pushing for independence in Tunisia. Tunisia became independent in 1955. Its "bey" was deposed and Habib Bourguiba elected as President of Tunisia.

In 1954, Gamal Abdel Nasser deposed the monarchy of Egypt in the Egyptian Revolution of 1952 and came to power as Prime Minister of Egypt. Muammar Gaddafi led the 1969 Libyan coup d'état which deposed Idris of Libya. Gaddafi remained in power until his death in the Libyan Civil War of 2011.

Egypt was involved in several wars against Israel and was allied with other Arab countries. The first was the 1948 Arab–Israeli War, right after the state of Israel was founded. Egypt went to war again in the Six-Day War of 1967 and lost the Sinai Peninsula to Israel. They went to war yet again in the Yom Kippur War of 1973. In 1979, President of Egypt Anwar Sadat and Prime Minister of Israel Menachem Begin signed the Camp David Accords, which gave back the Sinai Peninsula to Egypt in exchange for the recognition of Israel. The accords are still in effect today. In 1981, Sadat was assassinated by members of the Egyptian Islamic Jihad under Khalid Islambouli. The assassins were Islamists who targeted Sadat for his signing of the Accords.

In 1948 the apartheid laws were started in South Africa by the dominant National Party. These were largely a continuation of existing policies; the difference was the policy of "separate development" (Apartheid). Where previous policies had only been disparate efforts to economically exploit the African majority, Apartheid represented an entire philosophy of separate racial goals, leading to both the divisive laws of 'petty apartheid,' and the grander scheme of African homelands.

In 1994, Apartheid ended, and Nelson Mandela of the African National Congress was elected president after the South African general election, 1994, the country's first non-racial election.

The central regions of Africa were traditionally regarded to be the regions between Kilwa and the mouth of the Zambesi river. Due to its isolated position from the coasts, this area has received minimal attention from historian pertaining to Africa. It also had one of the most varied sources of European colonial imperialists including Germany in Cameroon, Britain in Northern Cameroons, Belgium in Congo, and France in CAF. Due to its territory, among the main trope s regarding Central Africa is traversing its lands and the nature of its tropicals. Since 1982, one of the main protracted issues within central Africa has been the ongoing secession movement of the secessionist entity of Ambazonia. The impasse between Cameroon and Ambazonia gained steam in 1992 when Fon Gorji-Dinka filed an international lawsuit against Cameroon claiming that Ambazonian territories are held illegally by the latter and describing Cameroonian claims on Ambazonian territories as illegal. Fifteen years later, this stalemate would escalate when Abmazonia formally declared itself as the Federal Republic of Ambazonia.

Following World War II, nationalist movements arose across West Africa, most notably in Ghana under Kwame Nkrumah. In 1957, Ghana became the first sub-Saharan colony to achieve its independence, followed the next year by France's colonies; by 1974, West Africa's nations were entirely autonomous. Since independence, many West African nations have been plagued by corruption and instability, with notable civil wars in Nigeria, Sierra Leone, Liberia, and Ivory Coast, and a succession of military coups in Ghana and Burkina Faso. Many states have failed to develop their economies despite enviable natural resources, and political instability is often accompanied by undemocratic government.

See also 2014 Ebola virus epidemic in Sierra Leone, 2014 Ebola virus epidemic in Guinea, and 2014 Ebola virus epidemic in Liberia

The first historical studies in English appeared in the 1890s, and followed one of four approaches. 1) The territorial narrative was typically written by a veteran soldier or civil servant who gave heavy emphasis to what he had seen. 2) The "apologia" were essays designed to justify British policies. 3) Popularizers tried to reach a large audience. 4) Compendia appeared designed to combine academic and official credentials. Professional scholarship appeared around 1900, and began with the study of business operations, typically using government documents and unpublished archives.

The economic approach was widely practiced in the 1930s, primarily to provide descriptions of the changes underway in the previous half-century. In 1935, American historian William L. Langer published "The Diplomacy of Imperialism: 1890–1902", a book that is still widely cited. In 1939, Oxford professor Reginald Coupland published "The Exploitation of East Africa, 1856–1890: The Slave Trade and the Scramble", another popular treatment.

World War II diverted most scholars to wartime projects and accounted for a pause in scholarship during the 1940s.

By the 1950s many African students were studying in British universities, and they produced a demand for new scholarship, and started themselves to supply it as well. Oxford University became the main center for African studies, with activity as well at Cambridge University and the London School of Economics. The perspective of British government policymakers or international business operations slowly gave way to a new interest in the activities of the natives, especially nationalistic movements and the growing demand for independence. The major breakthrough came from Ronald Robinson and John Andrew Gallagher, especially with their studies of the impact of free trade on Africa. In 1985 "The Oxford History of South Africa" (2 vols.) was published, attempting to synthesize the available materials. In 2013, "The Oxford Handbook of Modern African History" was published, bringing the scholarship up to date.








</doc>
<doc id="14104" url="https://en.wikipedia.org/wiki?curid=14104" title="History of Oceania">
History of Oceania

The History of Oceania includes the history of Australia, New Zealand, Hawaii, Papua New Guinea, Fiji and other Pacific island nations.

The prehistory of Oceania is divided into the prehistory of each of its major areas: Polynesia, Micronesia, Melanesia, and Australasia, and these vary greatly as to when they were first inhabited by humans—from 70,000 years ago (Australasia) to 3,000 years ago (Polynesia).

The Polynesian people are considered to be by linguistic, archaeological and human genetic ancestry a subset of the sea-migrating Austronesian people and tracing Polynesian languages places their prehistoric origins in the Malay Archipelago, and ultimately, in Taiwan. Between about 3000 and 1000 BCE speakers of Austronesian languages began spreading from Taiwan into Island South-East Asia, as tribes whose natives were thought to have arrived through South China about 8,000 years ago to the edges of western Micronesia and on into Melanesia, although they are different from the Han Chinese who now form the majority of people in China and Taiwan. There are three theories regarding the spread of humans across the Pacific to Polynesia. These are outlined well by Kayser "et al." (2000) and are as follows:

In the archaeological record there are well-defined traces of this expansion which allow the path it took to be followed and dated with some certainty. It is thought that by roughly 1400 BCE, "Lapita Peoples", so-named after their pottery tradition, appeared in the Bismarck Archipelago of north-west Melanesia. This culture is seen as having adapted and evolved through time and space since its emergence "Out of Taiwan". They had given up rice production, for instance, after encountering and adapting to breadfruit in the Bird's Head area of New Guinea. In the end, the most eastern site for Lapita archaeological remains recovered so far has been through work on the archaeology in Samoa. The site is at Mulifanua on Upolu. The Mulifanua site, where 4,288 pottery shards have been found and studied, has a "true" age of c. 1000 BCE based on C14 dating. A 2010 study places the beginning of the human archaeological sequences of Polynesia in Tonga at 900 BCE, the small differences in dates with Samoa being due to differences in radiocarbon dating technologies between 1989 and 2010, the Tongan site apparently predating the Samoan site by some few decades in real time.

Within a mere three or four centuries between about 1300 and 900 BCE, the Lapita archaeological culture spread 6,000 kilometres further to the east from the Bismarck Archipelago, until it reached as far as Fiji, Tonga, and Samoa. The area of Tonga, Fiji, and Samoa served as a gateway into the rest of the Pacific region known as Polynesia. Ancient Tongan mythologies recorded by early European explorers report the islands of 'Ata and Tongatapu as the first islands being hauled to the surface from the deep ocean by Maui.

The "Tuʻi Tonga Empire" or "Tongan Empire" in Oceania are descriptions sometimes given to Tongan expansionism and projected hegemony dating back to 950 CE, but at its peak during the period 1200–1500. While modern researchers and cultural experts attest to widespread Tongan influence and evidences of transoceanic trade and exchange of material and non-material cultural artifacts, empirical evidence of a true political empire ruled for any length of time by successive rulers is lacking.

Modern archeology, anthropology and linguistic studies confirm widespread Tongan cultural influence ranging widely through East 'Uvea, Rotuma, Futuna, Samoa and Niue, parts of Micronesia (Kiribati, Pohnpei), Vanuatu, and New Caledonia and the Loyalty Islands, and while some academics prefer the term "maritime chiefdom", others argue that, while very different from examples elsewhere, "..."empire" is probably the most convenient term."

Pottery art from Fijian towns shows that Fiji was settled before or around 3500 to 1000 BC, although the question of Pacific migration still lingers. It is believed that the Lapita people or the ancestors of the Polynesians settled the islands first but not much is known of what became of them after the Melanesians arrived; they may have had some influence on the new culture, and archaeological evidence shows that they would have then moved on to Tonga, Samoa and even Hawai'i.

The first settlements in Fiji were started by voyaging traders and settlers from the west about 5000 years ago. Lapita pottery shards have been found at numerous excavations around the country. Aspects of Fijian culture are similar to the Melanesian culture of the western Pacific but have a stronger connection to the older Polynesian cultures. Across from east to west, Fiji has been a nation of many languages. Fiji's history was one of settlement but also of mobility.

Over the centuries, a unique Fijian culture developed. Constant warfare and cannibalism between warring tribes were quite rampant and very much part of everyday life. In later centuries, the ferocity of the cannibal lifestyle deterred European sailors from going near Fijian waters, giving Fiji the name "Cannibal Isles"; as a result, Fiji remained unknown to the rest of the world.

Early European visitors to Easter Island recorded the local oral traditions about the original settlers. In these traditions, Easter Islanders claimed that a chief Hotu Matu'a arrived on the island in one or two large canoes with his wife and extended family. They are believed to have been Polynesian. There is considerable uncertainty about the accuracy of this legend as well as the date of settlement. Published literature suggests the island was settled around 300–400 CE, or at about the time of the arrival of the earliest settlers in Hawaii.

Some scientists say that Easter Island was not inhabited until 700–800 CE. This date range is based on glottochronological calculations and on three radiocarbon dates from charcoal that appears to have been produced during forest clearance activities.

Moreover, a recent study which included radiocarbon dates from what is thought to be very early material suggests that the island was settled as recently as 1200 CE. This seems to be supported by a 2006 study of the island's deforestation, which could have started around the same time. A large now extinct palm, "Paschalococos disperta", related to the Chilean wine palm "(Jubaea chilensis)", was one of the dominant trees as attested by fossil evidence; this species, whose sole occurrence was Easter Island, became extinct due to deforestation by the early settlers.

Micronesia began to be settled several millennia ago, although there are competing theories about the origin and arrival of the first settlers. There are numerous difficulties with conducting archaeological excavations in the islands, due to their size, settlement patterns and storm damage. As a result, much evidence is based on linguistic analysis. The earliest archaeological traces of civilization have been found on the island of Saipan, dated to 1500 BCE or slightly before.

The ancestors of the Micronesians settled there over 4,000 years ago. A decentralized chieftain-based system eventually evolved into a more centralized economic and religious culture centered on Yap and Pohnpei. The prehistory of many Micronesian islands such as Yap are not known very well.

On Pohnpei, pre-colonial history is divided into three eras: "Mwehin Kawa" or "Mwehin Aramas" (Period of Building, or Period of Peopling, before c. 1100); "Mwehin Sau Deleur" (Period of the Lord of Deleur, c. 1100 to c. 1628); and "Mwehin Nahnmwarki" (Period of the Nahnmwarki, c. 1628 to c. 1885). Pohnpeian legend recounts that the Saudeleur rulers, the first to bring government to Pohnpei, were of foreign origin. The Saudeleur centralized form of absolute rule is characterized in Pohnpeian legend as becoming increasingly oppressive over several generations. Arbitrary and onerous demands, as well as a reputation for offending Pohnpeian deities, sowed resentment among Pohnpeians. The Saudeleur Dynasty ended with the invasion of Isokelekel, another semi-mythical foreigner, who replaced the Saudeleur rule with the more decentralized "nahnmwarki" system in existence today. Isokelekel is regarded as the creator of the modern Pohnpeian "nahnmwarki" social system and the father of the Pompeian people.

Construction of Nan Madol, a megalithic complex made from basalt lava logs in Pohnpei began as early as 1200 CE. Nan Madol is offshore of Temwen Island near Pohnpei, consists of a series of small artificial islands linked by a network of canals, and is often called the "Venice of the Pacific". It is located near the island of Pohnpei and was the ceremonial and political seat of the Saudeleur Dynasty that united Pohnpei's estimated 25,000 people until its centralized system collapsed amid the invasion of Isokelekel. Isokelekel and his descendants initially occupied the stone city, but later abandoned it.

The first people of the Northern Mariana Islands navigated to the islands at some period between 4000 BCE to 2000 BCE from South-East Asia. They became known as the Chamorros, and spoke an Austronesian language called Chamorro. The ancient Chamorro left a number of megalithic ruins, including Latte stone. The Refaluwasch, or Carolinian, people came to the Marianas in the 1800s from the Caroline Islands. Micronesian colonists gradually settled the Marshall Islands during the 2nd millennium BCE, with inter-island navigation made possible using traditional stick charts.

The first settlers of Australia, New Guinea, and the large islands just to the east arrived between 50,000 and 30,000 years ago, when Neanderthals still roamed Europe. The original inhabitants of the group of islands now named Melanesia were likely the ancestors of the present-day Papuan-speaking people. Migrating from South-East Asia, they appear to have occupied these islands as far east as the main islands in the Solomon Islands archipelago, including Makira and possibly the smaller islands farther to the east.

Particularly along the north coast of New Guinea and in the islands north and east of New Guinea, the Austronesian people, who had migrated into the area somewhat more than 3,000 years ago, came into contact with these pre-existing populations of Papuan-speaking peoples. In the late 20th century, some scholars theorized a long period of interaction, which resulted in many complex changes in genetics, languages, and culture among the peoples. Kayser, et al. proposed that, from this area, a very small group of people (speaking an Austronesian language) departed to the east to become the forebears of the Polynesian people.

However, the theory is contradicted by the findings of a genetic study published by Temple University in 2008; based on genome scans and evaluation of more than 800 genetic markers among a wide variety of Pacific peoples, it found that neither Polynesians nor Micronesians have much genetic relation to Melanesians. Both groups are strongly related genetically to East Asians, particularly Taiwanese aborigines. It appeared that, having developed their sailing outrigger canoes, the Polynesian ancestors migrated from East Asia, moved through the Melanesian area quickly on their way, and kept going to eastern areas, where they settled. They left little genetic evidence in Melanesia.

The study found a high rate of genetic differentiation and diversity among the groups living within the Melanesian islands, with the peoples distinguished by island, language, topography, and geography among the islands. Such diversity developed over their tens of thousands of years of settlement before the Polynesian ancestors ever arrived at the islands. For instance, populations developed differently in coastal areas, as opposed to those in more isolated mountainous valleys.

Additional DNA analysis has taken research into new directions, as more human species have been discovered since the late 20th century. Based on his genetic studies of the Denisova hominin, an ancient human species discovered in 2010, Svante Pääbo claims that ancient human ancestors of the Melanesians interbred in Asia with these humans. He has found that people of New Guinea share 4–6% of their genome with the Denisovans, indicating this exchange. The Denisovans are considered cousin to the Neanderthals; both groups are now understood to have migrated out of Africa, with the Neanderthals going into Europe, and the Denisovans heading east about 400,000 years ago. This is based on genetic evidence from a fossil found in Siberia. The evidence from Melanesia suggests their territory extended into south Asia, where ancestors of the Melanesians developed.

Melanesians of some islands are one of the few non-European peoples, and the only dark-skinned group of people outside Australia, known to have blond hair.

Indigenous Australians are the original inhabitants of the Australian continent and nearby islands. Indigenous Australians migrated from Africa to Asia around 70,000 years ago and arrived in Australia around 50,000 years ago. The Torres Strait Islanders are indigenous to the Torres Strait Islands, which are at the northernmost tip of Queensland near Papua New Guinea. The term "Aboriginal" is traditionally applied to only the indigenous inhabitants of mainland Australia and Tasmania, along with some of the adjacent islands, i.e.: the "first peoples". "Indigenous Australians" is an inclusive term used when referring to both Aboriginal and Torres Strait islanders.

The earliest definite human remains found to date are that of Mungo Man, which have been dated at about 40,000 years old, but the time of arrival of the ancestors of Indigenous Australians is a matter of debate among researchers, with estimates dating back as far as 125,000 years ago. There is great diversity among different Indigenous communities and societies in Australia, each with its own unique mixture of cultures, customs and languages. In present-day Australia these groups are further divided into local communities.

Oceania was first explored by Europeans from the 16th century onwards. Portuguese navigators, between 1512 and 1526, reached the Moluccas (by António de Abreu and Francisco Serrão in 1512), Timor, the Aru Islands (Martim A. Melo Coutinho), the Tanimbar Islands, some of the Caroline Islands (by Gomes de Sequeira in 1525), and west Papua New Guinea (by Jorge de Menezes in 1526). In 1519 a Castilian ('Spanish') expedition led by Ferdinand Magellan sailed down the east coast of South America, found and sailed through the strait that bears his name and on 28 November 1520 entered the ocean which he named "Pacific". The three remaining ships, led by Magellan and his captains Duarte Barbosa and João Serrão, then sailed north and caught the trade winds which carried them across the Pacific to the Philippines where Magellan was killed. One surviving ship led by Juan Sebastián Elcano returned west across the Indian Ocean and the other went north in the hope of finding the westerlies and reaching Mexico. Unable to find the right winds, it was forced to return to the East Indies. The Magellan-Elcano expedition achieved the first circumnavigation of the world and reached the Philippines, the Mariana Islands and other islands of Oceania.

From 1527 to 1595 a number of other large Spanish expeditions crossed the Pacific Ocean, leading to the discovery of the Marshall Islands and Palau in the North Pacific, as well as Tuvalu, the Marquesas, the Solomon Islands archipelago, the Cook Islands and the Admiralty Islands in the South Pacific.

In 1565, Spanish navigator Andrés de Urdaneta found a wind system that would allow ships to sail eastward from Asia, back to the Americas. From then until 1815 the annual Manila Galleons crossed the Pacific from Mexico to the Philippines and back, in the first transpacific trade route in history. Combined with the Spanish Atlantic or West Indies Fleet, the Manila Galleons formed one of the first global maritime exchange in human history, linking Seville in Spain with Manila in the Philippines, via Mexico.

Later, in the quest for Terra Australis, Spanish explorers in the 17th century discovered the Pitcairn and Vanuatu archipelagos, and sailed the Torres Strait between Australia and New Guinea, named after navigator Luís Vaz de Torres. In 1668 the Spanish founded a colony on Guam as a resting place for west-bound galleons. For a long time this was the only non-coastal European settlement in the Pacific.

The Dutch were the first non-natives to undisputedly explore and chart coastlines of Australia, Tasmania, New Zealand, Tonga, Fiji, Samoa, and Easter Island. Verenigde Oostindische Compagnie (or VOC) was a major force behind the (c. 1590s–1720s) and Netherlandish cartography (c. 1570s–1670s). In the 17th century, the VOC's navigators and explorers charted almost three-quarters of the Australian coastline, except the east coast.

Abel Tasman was the first known European explorer to reach the islands of Van Diemen's Land (now Tasmania) and New Zealand, and to sight the Fiji islands. His navigator François Visscher, and his merchant Isaack Gilsemans, mapped substantial portions of Australia, New Zealand, Tonga and the Fijian islands.
On 24 November 1642 Abel Tasman sighted the west coast of Tasmania, north of Macquarie Harbour. He named his discovery Van Diemen's Land after Antonio van Diemen, Governor-General of the Dutch East Indies. then claimed formal possession of the land on 3 December 1642.
After some exploration, Tasman had intended to proceed in a northerly direction but as the wind was unfavourable he steered east. On 13 December they sighted land on the north-west coast of the South Island, New Zealand, becoming the first Europeans to do so. Tasman named it "Staten Landt" on the assumption that it was connected to an island (Staten Island, Argentina) at the south of the tip of South America. Proceeding north and then east, he stopped to gather water, but one of his boats was attacked by Māori in a double hulled waka (canoes) and four of his men were attacked and killed by mere. As Tasman sailed out of the bay he was again attacked, this time by 11 waka . The waka approached the Zeehan which fired and hit one Māori who fell down. Canister shot hit the side of a waka.

Archeological research has shown the Dutch had tried to land at a major agricultural area, which the Māori may have been trying to protect. Tasman named the bay "Murderers' Bay" (now known as Golden Bay) and sailed north, but mistook Cook Strait for a bight (naming it "Zeehaen's Bight"). Two names he gave to New Zealand landmarks still endure, Cape Maria van Diemen and Three Kings Islands, but "Kaap Pieter Boreels" was renamed by Cook 125 years later to Cape Egmont.

En route back to Batavia, Tasman came across the Tongan archipelago on 20 January 1643. While passing the Fiji Islands Tasman's ships came close to being wrecked on the dangerous reefs of the north-eastern part of the Fiji group. He charted the eastern tip of Vanua Levu and Cikobia before making his way back into the open sea. He eventually turned north-west to New Guinea, and arrived at Batavia on 15 June 1643. For over a century after Tasman's voyages, until the era of James Cook, Tasmania and New Zealand were not visited by Europeans—mainland Australia was visited, but usually only by accident.

In 1766 the Royal Society engaged James Cook to travel to the Pacific Ocean to observe and record the transit of Venus across the Sun. The expedition sailed from England on 26 August 1768, rounded Cape Horn and continued westward across the Pacific to arrive at Tahiti on 13 April 1769, where the observations of the Venus Transit were made. Once the observations were completed, Cook opened the sealed orders which were additional instructions from the Admiralty for the second part of his voyage: to search the south Pacific for signs of the postulated rich southern continent of "Terra Australis".

With the help of a Tahitian named Tupaia, who had extensive knowledge of Pacific geography, Cook managed to reach New Zealand on 6 October 1769, leading only the second group of Europeans to do so (after Abel Tasman over a century earlier, in 1642). Cook mapped the complete New Zealand coastline, making only some minor errors (such as calling Banks Peninsula an island, and thinking Stewart Island/Rakiura was a peninsula of the South Island). He also identified Cook Strait, which separates the North Island from the South Island, and which Tasman had not seen.

Cook then voyaged west, reaching the south-eastern coast of Australia on 19 April 1770, and in doing so his expedition became the first recorded Europeans to have encountered its eastern coastline. On 23 April he made his first recorded direct observation of indigenous Australians at Brush Island near Bawley Point, noting in his journal: ""…and were so near the Shore as to distinguish several people upon the Sea beach they appear'd to be of a very dark or black Colour but whether this was the real colour of their skins or the C[l]othes they might have on I know not"." On 29 April Cook and crew made their first landfall on the mainland of the continent at a place now known as the Kurnell Peninsula. It is here that James Cook made first contact with an aboriginal tribe known as the Gweagal.

After his departure from Botany Bay he continued northwards. After a grounding mishap on the Great Barrier Reef, the voyage continued, sailing through Torres Strait before returning to England via Batavia, the Cape of Good Hope, and Saint Helena.

In 1772 the Royal Society commissioned Cook to search for the hypothetical Terra Australis again. On his first voyage, Cook had demonstrated by circumnavigating New Zealand that it was not attached to a larger landmass to the south. Although he charted almost the entire eastern coastline of Australia, showing it to be continental in size, the Terra Australis was believed by the Royal Society to lie further south.

Cook commanded on this voyage, while Tobias Furneaux commanded its companion ship, . Cook's expedition circumnavigated the globe at an extreme southern latitude, becoming one of the first to cross the Antarctic Circle (17 January 1773). In the Antarctic fog, "Resolution" and "Adventure" became separated. Furneaux made his way to New Zealand, where he lost some of his men during an encounter with Māori, and eventually sailed back to Britain, while Cook continued to explore the Antarctic, reaching 71°10'S on 31 January 1774.

Cook almost encountered the mainland of Antarctica, but turned towards Tahiti to resupply his ship. He then resumed his southward course in a second fruitless attempt to find the supposed continent. On this leg of the voyage he brought a young Tahitian named Omai, who proved to be somewhat less knowledgeable about the Pacific than Tupaia had been on the first voyage. On his return voyage to New Zealand in 1774, Cook landed at the Friendly Islands, Easter Island, Norfolk Island, New Caledonia, and Vanuatu.

Before returning to England, Cook made a final sweep across the South Atlantic from Cape Horn. He then turned north to South Africa, and from there continued back to England. His reports upon his return home put to rest the popular myth of Terra Australis.

On his last voyage, Cook again commanded HMS "Resolution", while Captain Charles Clerke commanded . The voyage was ostensibly planned to return the Pacific Islander, Omai to Tahiti, or so the public were led to believe. The trip's principal goal was to locate a North-West Passage around the American continent. After dropping Omai at Tahiti, Cook travelled north and in 1778 became the first European to visit the Hawaiian Islands. After his initial landfall in January 1778 at Waimea harbour, Kauai, Cook named the archipelago the "Sandwich Islands" after the fourth Earl of Sandwich—the acting First Lord of the Admiralty.

From the Sandwich Islands Cook sailed north and then north-east to explore the west coast of North America north of the Spanish settlements in Alta California. Cook explored and mapped the coast all the way to the Bering Strait, on the way identifying what came to be known as Cook Inlet in Alaska. In a single visit, Cook charted the majority of the North American north-west coastline on world maps for the first time, determined the extent of Alaska, and closed the gaps in Russian (from the West) and Spanish (from the South) exploratory probes of the Northern limits of the Pacific.

Cook returned to Hawaii in 1779. After sailing around the archipelago for some eight weeks, he made landfall at Kealakekua Bay, on 'Hawaii Island', largest island in the Hawaiian Archipelago. Cook's arrival coincided with the "Makahiki", a Hawaiian harvest festival of worship for the Polynesian god Lono. Coincidentally the form of Cook's ship, HMS "Resolution", or more particularly the mast formation, sails and rigging, resembled certain significant artefacts that formed part of the season of worship. Similarly, Cook's clockwise route around the island of Hawaii before making landfall resembled the processions that took place in a clockwise direction around the island during the Lono festivals. It has been argued (most extensively by Marshall Sahlins) that such coincidences were the reasons for Cook's (and to a limited extent, his crew's) initial deification by some Hawaiians who treated Cook as an incarnation of Lono. Though this view was first suggested by members of Cook's expedition, the idea that any Hawaiians understood Cook to be Lono, and the evidence presented in support of it, were challenged in 1992.

After a month's stay, Cook resumed his exploration of the Northern Pacific. Shortly after leaving Hawaii Island, however, the "Resolution" foremast broke, so the ships returned to Kealakekua Bay for repairs. Tensions rose, and a number of quarrels broke out between the Europeans and Hawaiians. On 14 February 1779, at Kealakekua Bay, some Hawaiians took one of Cook's small boats. As thefts were quite common in Tahiti and the other islands, Cook would have taken hostages until the stolen articles were returned. He attempted to take as hostage the King of Hawaiʻi, Kalaniʻōpuʻu. The Hawaiians prevented this, and Cook's men had to retreat to the beach. As Cook turned his back to help launch the boats, he was struck on the head by the villagers and then stabbed to death as he fell on his face in the surf. Hawaiian tradition says that he was killed by a chief named Kalaimanokahoʻowaha or Kanaʻina. The Hawaiians dragged his body away. Four of Cook's men were also killed and two others were wounded in the confrontation.

The esteem which the islanders nevertheless held for Cook caused them to retain his body. Following their practice of the time, they prepared his body with funerary rituals usually reserved for the chiefs and highest elders of the society. The body was disembowelled, baked to facilitate removal of the flesh, and the bones were carefully cleaned for preservation as religious icons in a fashion somewhat reminiscent of the treatment of European saints in the Middle Ages. Some of Cook's remains, thus preserved, were eventually returned to his crew for a formal burial at sea.

Clerke assumed leadership of the expedition. Following the death of Clerke, "Resolution" and "Discovery" returned home in October 1780 commanded by John Gore, a veteran of Cook's first voyage, and Captain James King. After their arrival in England, King completed Cook's account of the voyage.

In 1789 the Mutiny on the Bounty against William Bligh led to several of the mutineers escaping the Royal Navy and settling on Pitcairn Islands, which later became a British colony. Britain also established colonies in Australia in 1788, New Zealand in 1840 and Fiji in 1872, with much of Oceania becoming part of the British Empire.

The Gilbert Islands (now known as Kiribati) and the Ellice Islands (now known as Tuvalu) came under Britain's sphere of influence in the late 19th century. The Ellice Islands were administered as British protectorate by a Resident Commissioner from 1892 to 1916 as part of the British Western Pacific Territories (BWPT), and later as part of the Gilbert and Ellice Islands colony from 1916 to 1974.

Among the last islands in Oceania to be colonised was Niue (1900). In 1887, King Fata-a-iki, who reigned Niue from 1887 to 1896, offered to cede sovereignty to the British Empire, fearing the consequences of annexation by a less benevolent colonial power. The offer was not accepted until 1900. Niue was a British protectorate, but the UK's direct involvement ended in 1901 when New Zealand annexed the island.

French Catholic missionaries arrived on Tahiti in 1834; their expulsion in 1836 caused France to send a gunboat in 1838. In 1842, Tahiti and Tahuata were declared a French protectorate, to allow Catholic missionaries to work undisturbed. The capital of Papeetē was founded in 1843. In 1880, France annexed Tahiti, changing the status from that of a protectorate to that of a colony.

On 24 September 1853, under orders from Napoleon III, Admiral Febvrier Despointes took formal possession of New Caledonia and Port-de-France (Nouméa) was founded 25 June 1854. A few dozen free settlers settled on the west coast in the following years. New Caledonia became a penal colony, and from the 1860s until the end of the transportations in 1897, about 22,000 criminals and political prisoners were sent to New Caledonia, among them many Communards, including Henri de Rochefort and Louise Michel. Between 1873 and 1876, 4,200 political prisoners were "relegated" in New Caledonia. Only forty of them settled in the colony, the rest returned to France after being granted amnesty in 1879 and 1880.

In the 1880s, France claimed the Tuamotu Archipelago, which formerly belonged to the Pōmare Dynasty, without formally annexing it. Having declared a protectorate over Tahuata in 1842, the French regarded the entire Marquesas Islands as French. In 1885, France appointed a governor and established a general council, thus giving it the proper administration for a colony. The islands of Rimatara and Rūrutu unsuccessfully lobbied for British protection in 1888, so in 1889 they were annexed by France. Postage stamps were first issued in the colony in 1892. The first official name for the colony was "Établissements de l'Océanie" (Settlements in Oceania); in 1903 the general council was changed to an advisory council and the colony's name was changed to "Établissements Français de l'Océanie" (French Settlements in Oceania).

The Spanish explorer Alonso de Salazar landed in the Marshall Islands in 1529. They were later named by Krusenstern, after English explorer John Marshall, who visited them together with Thomas Gilbert in 1788, en route from Botany Bay to Canton (two ships of the First Fleet). The Marshall Islands were claimed by Spain in 1874.

In November 1770, Felipe González de Ahedo commanded an expedition from the Viceroyalty of Peru that searched for Davis Land and Madre de Dios Island and looked for foreign naval activities.
This expedition landed on "Isla de San Carlos" (Easter Island) and signed a treaty of annexion with the Rapa Nui chiefs.

In 1606 Luís Vaz de Torres explored the southern coast of New Guinea from Milne Bay to the Gulf of Papua including Orangerie Bay which he named "Bahía de San Lorenzo". His expedition also discovered Basilaki Island naming it "Tierra de San Buenaventura", which he claimed for Spain in July 1606. On 18 October his expedition reached the western part of the island in present-day Indonesia, and also claimed the territory for the King of Spain.

A successive European claim occurred in 1828, when the Netherlands formally claimed the western half of the island as Netherlands New Guinea. In 1883, following a short-lived French annexation of New Ireland, the British colony of Queensland annexed south-eastern New Guinea. However, the Queensland government's superiors in the United Kingdom revoked the claim, and (formally) assumed direct responsibility in 1884, when Germany claimed north-eastern New Guinea as the protectorate of German New Guinea (also called Kaiser-Wilhelmsland).

The first Dutch government posts were established in 1898 and in 1902: Manokwari on the north coast, Fak-Fak in the west and Merauke in the south at the border with British New Guinea. The German, Dutch and British colonial administrators each attempted to suppress the still-widespread practices of inter-village warfare and headhunting within their respective territories.

In 1905 the British government transferred some administrative responsibility over south-east New Guinea to Australia (which renamed the area "Territory of Papua"); and in 1906, transferred all remaining responsibility to Australia. During World War I, Australian forces seized German New Guinea, which in 1920 became the Territory of New Guinea,
to be administered by Australia under a League of Nations mandate. The territories under Australian administration became collectively known as The Territories of Papua and New Guinea (until February 1942).

Germany established colonies in New Guinea in 1884, and Samoa in 1900.

Following papal mediation and German compensation of $4.5 million, Spain recognized a German claim in 1885. Germany established a protectorate and set up trading stations on the islands of Jaluit and Ebon to carry out the flourishing copra (dried coconut meat) trade. Marshallese Iroij (high chiefs) continued to rule under indirect colonial German administration.

The United States also expanded into the Pacific, beginning with Baker Island and Howland Island in 1857, and with Hawaii becoming a U.S. territory in 1898. Disagreements between the US, Germany and UK over Samoa led to the Tripartite Convention of 1899.

Samoa aligned its interests with the United States in a Deed of Succession, signed by the "Tui Manúʻa" (supreme chief of Manúʻa) on 16 July 1904 at the Crown residence of the Tuimanuʻa called the "Faleula" in the place called Lalopua (from Official documents of the Tuimanuʻa government, 1893; Office of the Governor, 2004).

Cession followed the Tripartite Convention of 1899 that partitioned the eastern islands of Samoa (including Tutuila and the Manúʻa Group) from the western islands of Samoa (including ʻUpolu and Savaiʻi).

At the beginning of World War I, Japan assumed control of the Marshall Islands. The Japanese headquarters was established at the German center of administration, Jaluit. On 31 January 1944, during World War II, American forces landed on Kwajalein atoll and U.S. Marines and Army troops later took control of the islands from the Japanese on 3 February, following intense fighting on Kwajalein and Enewetak atolls. In 1947, the United States, as the occupying power, entered into an agreement with the UN Security Council to administer much of Micronesia, including the Marshall Islands, as the Trust Territory of the Pacific Islands.

During World War II, Japan colonized many Oceanic colonies by wresting control from western powers.

The Samoan Crisis was a confrontation standoff between the United States, Imperial Germany and Great Britain from 1887–1889 over control of the Samoan Islands during the Samoan Civil War.

The prime minister of the kingdom of Hawaii Walter M. Gibson had long aimed to establishing an empire in the Pacific.
In 1887 his government sent the "homemade battleship" Kaimiloa to Samoa looking for an alliance against colonial powers.
It ended in suspicions from the German Navy and embarrassment for the conduct of the crew.

The 1889 incident involved three American warships, USS , and and three German warships, SMS "Adler", SMS "Olga", and SMS "Eber", keeping each other at bay over several months in Apia harbor, which was monitored by the British warship .

The standoff ended on 15 and 16 March when a cyclone wrecked all six warships in the harbor. "Calliope" was able to escape the harbor and survived the storm. Robert Louis Stevenson witnessed the storm and its aftermath at Apia and later wrote about what he saw. The Samoan Civil War continued, involving Germany, United States and Britain, eventually resulting, via the Tripartite Convention of 1899, in the partition of the Samoan Islands into American Samoa and German Samoa.

The Asian and Pacific Theatre of World War I was a conquest of German colonial possession in the Pacific Ocean and China. The most significant military action was the Siege of Tsingtao in what is now China, but smaller actions were also fought at Battle of Bita Paka and Siege of Toma in German New Guinea.

All other German and Austrian possessions in Asia and the Pacific fell without bloodshed. Naval warfare was common; all of the colonial powers had naval squadrons stationed in the Indian or Pacific Oceans. These fleets operated by supporting the invasions of German held territories and by destroying the East Asia Squadron.

One of the first land offensives in the Pacific theatre was the Occupation of German Samoa in August 1914 by New Zealand forces. The campaign to take Samoa ended without bloodshed after over 1,000 New Zealanders landed on the German colony, supported by an Australian and French naval squadron.

Australian forces attacked German New Guinea in September 1914: 500 Australians encountered 300 Germans and native policemen at the Battle of Bita Paka; the Allies won the day and the Germans retreated to Toma. A company of Australians and a British warship besieged the Germans and their colonial subjects, ending with a German surrender.

After the fall of Toma, only minor German forces were left in New Guinea and these generally capitulated once met by Australian forces. In December 1914, one German officer near Angorum attempted resist the occupation with thirty native police but his force deserted him after they fired on an Australian scouting party and he was subsequently captured.

German Micronesia, the Marianas, the Carolines and the Marshall Islands also fell to Allied forces during the war.

The Pacific front saw major action during the Second World War, mainly between the belligerents Japan and the United States.

The attack on Pearl Harbor was a surprise military strike conducted by the Imperial Japanese Navy against the United States naval base at Pearl Harbor, Hawaii, on the morning of 7 December 1941 (8 December in Japan). The attack led to the United States' entry into World War II.

The attack was intended as a preventive action in order to keep the U.S. Pacific Fleet from interfering with military actions the Empire of Japan was planning in South-East Asia against overseas territories of the United Kingdom, the Netherlands, and the United States. There were simultaneous Japanese attacks on the U.S.-held Philippines and on the British Empire in Malaya, Singapore, and Hong Kong.

The Japanese subsequently invaded New Guinea, the Solomon Islands and other Pacific islands. The Japanese were turned back at the Battle of the Coral Sea and the Kokoda Track campaign before they were finally defeated in 1945.

Some of the most prominent Oceanic battlegrounds were the Solomon Islands campaign, the Air raids on Darwin, the Kokada Track, and the Borneo campaign.

In 1940 the administration of French Polynesia recognised the Free French Forces and many Polynesians served in World War II. Unknown at the time to French and Polynesians, the Konoe Cabinet in Imperial Japan on 16 September 1940 included French Polynesia among the many territories which were to become Japanese possessions in the post-war world—though in the course of the war in the Pacific the Japanese were not able to launch an actual invasion of the French islands.

Some of the most intense fighting of the Second World War occurred in the Solomons. The most significant of the Allied Forces' operations against the Japanese Imperial Forces was launched on 7 August 1942, with simultaneous naval bombardments and amphibious landings on the Florida Islands at Tulagi and Red Beach on Guadalcanal.

The Guadalcanal Campaign became an important and bloody campaign fought in the Pacific War as the Allies began to repulse Japanese expansion. Of strategic importance during the war were the coastwatchers operating in remote locations, often on Japanese held islands, providing early warning and intelligence of Japanese naval, army and aircraft movements during the campaign.

"The Slot" was a name for New Georgia Sound, when it was used by the Tokyo Express to supply the Japanese garrison on Guadalcanal. Of more than 36,000 Japanese on Guadalcanal, about 26,000 were killed or missing, 9,000 died of disease, and 1,000 were captured.

The Kokoda Track campaign was a campaign consisting of a series of battles fought between July and November 1942 between Japanese and Allied—primarily Australian—forces in what was then the Australian territory of Papua. Following a landing near Gona, on the north coast of New Guinea, Japanese forces attempted to advance south overland through the mountains of the Owen Stanley Range to seize Port Moresby as part of a strategy of isolating Australia from the United States. Initially only limited Australian forces were available to oppose them, and after making rapid progress the Japanese South Seas Force clashed with under strength Australian forces at Awala, forcing them back to Kokoda. A number of Japanese attacks were subsequently fought off by the Australian Militia, yet they began to withdraw over the Owen Stanley Range, down the Kokoda Track.

In sight of Port Moresby itself, the Japanese began to run out of momentum against the Australians who began to receive further reinforcements. Having outrun their supply lines and following the reverses suffered by the Japanese at Guadalcanal, the Japanese were now on the defensive, marking the limit of the Japanese advance southwards. The Japanese subsequently withdrew to establish a defensive position on the north coast, but they were followed by the Australians who recaptured Kokoda on 2 November. Further fighting continued into November and December as the Australian and United States forces assaulted the Japanese beachheads, in what later became known as the Battle of Buna–Gona.

Due to its low population, Oceania was a popular location for atmospheric and underground nuclear tests. Tests were conducted in various locations by the United Kingdom (Operation Grapple and Operation Antler), the United States (Bikini atoll and the Marshall Islands) and France (Moruroa), often with devastating consequences for the inhabitants.

From 1946 to 1958, the Marshall Islands served as the Pacific Proving Grounds for the United States, and was the site of 67 nuclear tests on various atolls. The world's first hydrogen bomb, codenamed "Mike", was tested at the Enewetak atoll in the Marshall Islands on 1 November (local date) in 1952, by the United States.

In 1954, fallout from the American Castle Bravo hydrogen bomb test in the Marshall Islands was such that the inhabitants of the Rongelap Atoll were forced to abandon their island. Three years later the islanders were allowed to return, but suffered abnormally high levels of cancer. They were evacuated again in 1985 and in 1996 given $45 million in compensation.

A series of British tests were also conducted in the 1950s at Maralinga in South Australia, forcing the removal of the Pitjantjatjara and Yankunytjatjara peoples from their ancestral homelands.

In 1962, France's early nuclear testing ground of Algeria became independent and the atoll of Moruroa in the Tuamotu Archipelago was selected as the new testing site. Moruroa atoll became notorious as a site of French nuclear testing, primarily because tests were carried out there after most Pacific testing had ceased. These tests were opposed by most other nations in Oceania. The last atmospheric test was conducted in 1974, and the last underground test in 1996.

French nuclear testing in the Pacific was controversial in the 1980s, in 1985 French agents caused the Sinking of the Rainbow Warrior in Auckland to prevent it from arriving at the test site in Moruroa. In September 1995, France stirred up widespread protests by resuming nuclear testing at Fangataufa atoll after a three-year moratorium. The last test was on 27 January 1996. On 29 January 1996, France announced that it would accede to the Comprehensive Test Ban Treaty, and no longer test nuclear weapons.

Fiji has suffered several coups d'état: military in 1987 and 2006 and civilian in 2000. All were ultimately due to ethnic tension between indigenous Fijians and Indo-Fijians, who originally came to the islands as indentured labour in the late nineteenth and early twentieth century. The 1987 coup followed the election of a multi-ethnic coalition, which Lieutenant Colonel Sitiveni Rabuka overthrew, claiming racial discrimination against ethnic Fijians. The coup was denounced by the United Nations and Fiji was expelled from the Commonwealth of Nations.

The 2000 coup was essentially a repeat of the 1987 affair, although it was led by civilian George Speight, apparently with military support. Commodore Frank Bainimarama, who was opposed to Speight, then took over and appointed a new Prime Minister. Speight was later tried and convicted for treason. Many indigenous Fijians were unhappy at the treatment of Speight and his supporters, feeling that the coup had been legitimate. In 2006 the Fijian parliament attempted to introduce a series of bills which would have, amongst other things, pardoned those involved in the 2000 coup. Bainimarama, concerned that the legal and racial injustices of the previous coups would be perpetuated, staged his own coup. It was internationally condemned, and Fiji again suspended from the Commonwealth.

In 2006 the then Australia Defence Minister, Brendan Nelson, warned Fijian officials of an Australian Naval fleet within proximity of Fiji that would respond to any attacks against its citizens.

The Australian government estimated that anywhere between 15,000 and 20,000 people could have died in the Bougainville Civil War. More conservative estimates put the number of combat deaths as 1–2,000.

From 1975, there were attempts by the Bougainville Province to secede from Papua New Guinea. These were resisted by Papua New Guinea primarily because of the presence in Bougainville of the Panguna mine, which was vital to Papua New Guinea's economy. The Bougainville Revolutionary Army began attacking the mine in 1988, forcing its closure the following year. Further BRA activity led to the declaration of a state of emergency and the conflict continued until about 2005, when successionist leader and self-proclaimed King of Bougainville Francis Ona died of malaria. Peacekeeping troops led by Australia have been in the region since the late 1990s, and a referendum on independence will be held in the 2010s.

In 1946, French Polynesians were granted French citizenship and the islands' status was changed to an overseas territory; the islands' name was changed in 1957 to Polynésie Française (French Polynesia).

Australia and New Zealand became dominions in the 20th century, adopting the Statute of Westminster Act in 1942 and 1947 respectively, marking their legislative independence from the United Kingdom. Hawaii became a U.S. state in 1959.

Samoa became the first pacific nation to gain independence in 1962, Fiji and Tonga became independent in 1970, with many other nations following in the 1970s and 1980s. The South Pacific Forum was founded in 1971, which became the Pacific Islands Forum in 2000. Bougainville Island, geographically part of the Solomon Islands archipelago but politically part of Papua New Guinea, tried unsuccessfully to become independent in 1975, and a civil war followed in the early 1990s, with it later being granted autonomy.

On 1 May 1979, in recognition of the evolving political status of the Marshall Islands, the United States recognized the constitution of the Marshall Islands and the establishment of the Government of the Republic of the Marshall Islands. The constitution incorporates both American and British constitutional concepts.

In 1852, French Polynesia was granted partial internal autonomy; in 1984, the autonomy was extended. French Polynesia became a full overseas collectivity of France in 2004.

Between 2001 and 2007 Australia's Pacific Solution policy transferred asylum seekers to several Pacific nations, including the Nauru detention centre. Australia, New Zealand and other nations took part in the Regional Assistance Mission to Solomon Islands from 2003 after a request for aid.




</doc>
<doc id="14105" url="https://en.wikipedia.org/wiki?curid=14105" title="Hanseatic League">
Hanseatic League

The Hanseatic League (; , , ; ; ; ; ) was a commercial and defensive confederation of merchant guilds and market towns in Northwestern and Central Europe. Growing from a few North German towns in the late 1100s, the league came to dominate Baltic maritime trade for three centuries along the coasts of Northern Europe. Hansa territories stretched from the Baltic to the North Sea and inland during the Late Middle Ages, and diminished slowly after 1450.

Merchant circles established the league to protect the guilds' economic interests and diplomatic privileges in their affiliated cities and countries, as well as along the trade routes which the merchants used. The Hanseatic cities had their own legal system and operated their own armies for mutual protection and aid. Despite this, the organization was not a state, nor could it be called a confederation of city-states; only a very small number of the cities within the league enjoyed autonomy and liberties comparable to those of a free imperial city.

Exploratory trading adventures, raids, and piracy occurred early throughout the Baltic region; the sailors of Gotland sailed up rivers as far away as Novgorod. Scandinavians led international trade in the Baltic area before the Hanseatic League, establishing major trading hubs at Birka, Haithabu, and Schleswig by the 9th century CE. The later Hanseatic ports between Mecklenburg and Königsberg, (present-day Kaliningrad) originally formed part of the Scandinavian-led Baltic trade-system.

Historians generally trace the origins of the Hanseatic League to the rebuilding of the north German town of Lübeck in 1159 by the powerful Henry the Lion, Duke of Saxony and Bavaria, after he had captured the area from Adolf II, Count of Schauenburg and Holstein. More recent scholarship has deemphasized the focus on Lübeck due to it having been designed as one of several regional trading centers.

German cities achieved domination of trade in the Baltic with striking speed during the 13th century, and Lübeck became a central node in the seaborne trade that linked the areas around the North and Baltic seas. The hegemony of Lübeck peaked during the 15th century.

Lübeck became a base for merchants from Saxony and Westphalia trading eastward and northward. Well before the term "Hanse" appeared in a document in 1267,
merchants in different cities began to form guilds, or "Hansa", with the intention of trading with towns overseas, especially in the economically less-developed eastern Baltic. This area could supply timber, wax, amber, resins, and furs, along with rye and wheat brought down on barges from the hinterland to port markets. The towns raised their own armies, with each guild required to provide levies when needed. The Hanseatic cities came to the aid of one another, and commercial ships often had to be used to carry soldiers and their arms.

Visby (on the island of Gotland) functioned as the leading centre in the Baltic before the Hansa. Sailing east, Visby merchants established a trading post at Novgorod called "Gutagard" (also known as "Gotenhof") in 1080. Merchants from northern Germany also stayed there in the early period of the Gotlander settlement. Later they established their own trading station in Novgorod, known as , which was further up-river, in the first half of the 13th century. In 1229 German merchants at Novgorod were granted certain privileges that made their positions more secure.

Hansa societies worked to remove restrictions on trade for their members. The earliest extant documentary mention, although without a name, of a specific German commercial federation dates from 1157 in London. That year, the merchants of the Hansa in Cologne convinced King Henry II of England to exempt them from all tolls in London
and allow them to trade at fairs throughout England. The "Queen of the Hansa", Lübeck, where traders were required to trans-ship goods between the North Sea and the Baltic, gained imperial privileges to become a free imperial city in 1226, as had Hamburg in 1189.

In 1241 Lübeck, which had access to the Baltic and North seas' fishing grounds, formed an alliance—a precursor to the League—with Hamburg, another trading city, which controlled access to salt-trade routes from Lüneburg. The allied cities gained control over most of the salt-fish trade, especially the Scania Market; Cologne joined them in the Diet of 1260. In 1266 King Henry III of England granted the Lübeck and Hamburg Hansa a charter for operations in England, and the Cologne Hansa joined them in 1282 to form the most powerful Hanseatic colony in London. Much of the drive for this co-operation came from the fragmented nature of existing territorial governments, which failed to provide security for trade. Over the next 50 years, the Hansa solidified with formal agreements for confederation and co-operation covering the west and east trade routes. The principal city and linchpin remained Lübeck; with the first general diet of the Hansa held there in 1356, the Hanseatic League acquired an official structure.

Lübeck's location on the Baltic provided access for trade with Scandinavia and Kievan Rus' (with its sea-trade center, Veliky Novgorod), putting it in direct competition with the Scandinavians who had previously controlled most of the Baltic trade-routes. A treaty with the Visby Hansa put an end to this competition: through this treaty the Lübeck merchants gained access to the inland Russian port of Novgorod, where they built a trading post or "Kontor" (literally: "office"). Although such alliances formed throughout the Holy Roman Empire, the league never became a closely managed formal organisation. Assemblies of the Hanseatic towns met irregularly in Lübeck for a "Hansetag" (Hanseatic day) from 1356 onwards, but many towns chose not to attend nor to send representatives, and decisions were not binding on individual cities. Over the period, a network of alliances grew to include a flexible roster of 70 to 170 cities.

The league succeeded in establishing additional "Kontors" in Bruges (Flanders), Bergen (Norway), and London (England). These trading posts became significant enclaves. The London "Kontor", established in 1320, stood west of London Bridge near Upper Thames Street, on the site now occupied by Cannon Street station. It grew into a significant walled community with its own warehouses, weighhouse, church, offices and houses, reflecting the importance and scale of trading activity on the premises. The first reference to it as the Steelyard ("der Stahlhof") occurs in 1422.

Starting with trade in coarse woollen fabrics, the Hanseatic League had the effect of bringing both commerce and industry to northern Germany. As trade increased, newer and finer woollen and linen fabrics, and even silks, were manufactured in northern Germany. The same refinement of products out of cottage industry occurred in other fields, e.g. etching, wood carving, armour production, engraving of metals, and wood-turning. The century-long monopolization of sea navigation and trade by the Hanseatic League ensured that the Renaissance arrived in northern Germany long before it did in the rest of Europe.

In addition to the major "Kontors", individual Hanseatic ports had a representative merchant and warehouse. In England this happened in Boston, Bristol, Bishop's Lynn (now King's Lynn, which features the sole remaining Hanseatic warehouse in England), Hull, Ipswich, Norwich, Yarmouth (now Great Yarmouth), and York.

The league primarily traded timber, furs, resin (or tar), flax, honey, wheat, and rye from the east to Flanders and England with cloth (and, increasingly, manufactured goods) going in the other direction. Metal ore (principally copper and iron) and herring came southwards from Sweden.

German colonists in the 12th and 13th centuries settled in numerous cities on and near the east Baltic coast, such as Elbing (Elbląg), Thorn (Toruń), Reval (Tallinn), Riga, and Dorpat (Tartu), which became members of the Hanseatic League, and some of which still retain many Hansa buildings and bear the style of their Hanseatic days. Most were granted Lübeck law ("Lübisches Recht"), after the league's most prominent town. The law provided that they had to appeal in all legal matters to Lübeck's city council. The Livonian Confederation of 1435 to incorporated modern-day Estonia and parts of Latvia and had its own Hanseatic parliament (diet); all of its major towns became members of the Hanseatic League. The dominant language of trade was Middle Low German, a dialect with significant impact for countries involved in the trade, particularly the larger Scandinavian languages, Estonian, and Latvian.

The league had a fluid structure, but its members shared some characteristics; most of the Hansa cities either started as independent cities or gained independence through the collective bargaining power of the league, though such independence remained limited. The Hanseatic free cities owed allegiance directly to the Holy Roman Emperor, without any intermediate family tie of obligation to the local nobility.

Another similarity involved the cities' strategic locations along trade routes. At the height of their power in the late-14th century, the merchants of the Hanseatic League succeeded in using their economic power and, sometimes, their military might—trade routes required protection and the league's ships sailed well-armed—to influence imperial policy.

The league also wielded power abroad. Between 1361 and 1370 it waged war against Denmark. Initially unsuccessful, Hanseatic towns in 1368 allied in the Confederation of Cologne, sacked Copenhagen and Helsingborg, and forced Valdemar IV, King of Denmark, and his son-in-law Haakon VI, King of Norway, to grant the league 15% of the profits from Danish trade in the subsequent peace treaty of Stralsund in 1370, thus gaining an effective trade and economic monopoly in Scandinavia. This favourable treaty marked the height of Hanseatic power. After the Danish-Hanseatic War (1426–1435) and the Bombardment of Copenhagen (1428), the Treaty of Vordingborg renewed the commercial privileges in 1435.

The Hansa also waged a vigorous campaign against pirates. Between 1392 and 1440 maritime trade of the league faced danger from raids of the Victual Brothers and their descendants, privateers hired in 1392 by Albert of Mecklenburg, King of Sweden, against Margaret I, Queen of Denmark. In the Dutch–Hanseatic War (1438–1441), the merchants of Amsterdam sought and eventually won free access to the Baltic and broke the Hanseatic monopoly. As an essential part of protecting their investment in ships and their cargoes, the League trained pilots and erected lighthouses.

Most foreign cities confined the Hanseatic traders to certain trading areas and to their own trading posts. They seldom interacted with the local inhabitants, except when doing business. Many locals, merchant and noble alike, envied the power of the League and tried to diminish it. For example, in London, the local merchants exerted continuing pressure for the revocation of privileges. The refusal of the Hansa to offer reciprocal arrangements to their English counterparts exacerbated the tension. King Edward IV of England reconfirmed the league's privileges in the Treaty of Utrecht (1474) despite the latent hostility, in part thanks to the significant financial contribution the League made to the Yorkist side during the Wars of the Roses of 1455–1487. In 1597 Queen Elizabeth of England expelled the League from London, and the Steelyard closed the following year. Tsar Ivan III of Russia closed the Hanseatic "Kontor" at Novgorod in 1494. The very existence of the League and its privileges and monopolies created economic and social tensions that often crept over into rivalries between League members.

The economic crises of the late 15th century did not spare the Hansa. Nevertheless, its eventual rivals emerged in the form of the territorial states, whether new or revived, and not just in the west: Ivan III, Grand Prince of Moscow, ended the entrepreneurial independence of Hansa's Novgorod "Kontor" in 1478—it closed completely and finally in 1494. New vehicles of credit were imported from Italy, where double-entry book-keeping was invented in 1492, and outpaced the Hansa economy, in which silver coins changed hands rather than bills of exchange.

In the 15th century, tensions between the Prussian region and the "Wendish" cities (Lübeck and its eastern neighbours) increased. Lübeck was dependent on its role as centre of the Hansa, being on the shore of the sea without a major river. It was on the entrance of the land route to Hamburg, but this land route could be bypassed by sea travel around Denmark and through the Kattegat. Prussia's main interest, on the other hand, was the export of bulk products like grain and timber, which were very important for England, the Low Countries, and, later on, also for Spain and Italy.

In 1454, the year of the marriage of Elisabeth of Austria to the Jagiellonian king, the towns of the Prussian Confederation rose up against the dominance of the Teutonic Order and asked Casimir IV, King of Poland, for help. Gdańsk (Danzig), Thorn and Elbing became part of the Kingdom of Poland, (from 1466 to 1569 referred to as Royal Prussia, region of Poland) by the Second Peace of Thorn (1466). Poland in turn was heavily supported by the Holy Roman Empire through family connections and by military assistance under the Habsburgs. Kraków, then the capital of Poland, had a loose association with the Hansa. The lack of customs borders on the River Vistula after 1466 helped to gradually increase Polish grain exports, transported to the sea down the Vistula, from per year, in the late 15th century, to over in the 17th century. The Hansa-dominated maritime grain trade made Poland one of the main areas of its activity, helping Danzig to become the Hansa's largest city.

The member cities took responsibility for their own protection. In 1567, a Hanseatic League agreement reconfirmed previous obligations and rights of league members, such as common protection and defense against enemies. The Prussian Quartier cities of Thorn, Elbing, Königsberg and Riga and Dorpat also signed. When pressed by the King of Poland–Lithuania, Danzig remained neutral and would not allow ships running for Poland into its territory. They had to anchor somewhere else, such as at Pautzke (Puck).

A major economic advantage for the Hansa was its control of the shipbuilding market, mainly in Lübeck and in Danzig. The Hansa sold ships everywhere in Europe, including Italy. They drove out the Dutch, because Holland wanted to favour Bruges as a huge staple market at the end of a trade route. When the Dutch started to become competitors of the Hansa in shipbuilding, the Hansa tried to stop the flow of shipbuilding technology from Hanseatic towns to Holland. Danzig, a trading partner of Amsterdam, attempted to forestall the decision. Dutch ships sailed to Danzig to take grain from the city directly, to the dismay of Lübeck. Hollanders also circumvented the Hanseatic towns by trading directly with north German princes in non-Hanseatic towns. Dutch freight costs were much lower than those of the Hansa, and the Hansa were excluded as middlemen.

When Bruges, Antwerp and Holland all became part of the Duchy of Burgundy they actively tried to take over the monopoly of trade from the Hansa, and the staples market from Bruges was transferred to Amsterdam. The Dutch merchants aggressively challenged the Hansa and met with much success. Hanseatic cities in Prussia, Livonia, supported the Dutch against the core cities of the Hansa in northern Germany. After several naval wars between Burgundy and the Hanseatic fleets, Amsterdam gained the position of leading port for Polish and Baltic grain from the late 15th century onwards. The Dutch regarded Amsterdam's grain trade as the mother of all trades ("Moedernegotie").

Nuremberg in Franconia developed an overland route to sell formerly Hansa-monopolised products from Frankfurt via Nuremberg and Leipzig to Poland and Russia, trading Flemish cloth and French wine in exchange for grain and furs from the east. The Hansa profited from the Nuremberg trade by allowing Nurembergers to settle in Hanseatic towns, which the Franconians exploited by taking over trade with Sweden as well. The Nuremberger merchant Albrecht Moldenhauer was influential in developing the trade with Sweden and Norway, and his sons Wolf Moldenhauer and Burghard Moldenhauer established themselves in Bergen and Stockholm, becoming leaders of the local Hanseatic activities.

At the start of the 16th century, the league found itself in a weaker position than it had known for many years. The rising Swedish Empire had taken control of much of the Baltic Sea. Denmark had regained control over its own trade, the "Kontor" in Novgorod had closed, and the "Kontor" in Bruges had become effectively moribund. The individual cities making up the league had also started to put self-interest before their common Hanseatic interests. Finally, the political authority of the German princes had started to grow, constraining the independence of the merchants and Hanseatic towns.

The league attempted to deal with some of these issues: it created the post of Syndic in 1556 and elected Heinrich Sudermann as a permanent official with legal training, who worked to protect and extend the diplomatic agreements of the member towns. In 1557 and 1579 revised agreements spelled out the duties of towns and some progress was made. The Bruges "Kontor" moved to Antwerp and the Hansa attempted to pioneer new routes. However the league proved unable to prevent the growing mercantile competition, and so a long decline commenced. The Antwerp "Kontor" closed in 1593, followed by the London "Kontor" in 1598. The Bergen "Kontor" continued until 1754; of all the "Kontore", only its buildings, the "Bryggen", survive.

The gigantic warship "Adler von Lübeck" was constructed for military use against Sweden during the Northern Seven Years' War (1563–70) but was never put to military use, epitomizing the vain attempts of Lübeck to uphold its long-privileged commercial position in a changing economic and political climate.

By the late 17th century, the league had imploded and could no longer deal with its own internal struggles. The social and political changes that accompanied the Protestant Reformation included the rise of Dutch and English merchants and the pressure of the Ottoman Empire upon the Holy Roman Empire and its trade routes. Only nine members attended the last formal meeting in 1669 and only three (Lübeck, Hamburg and Bremen) remained as members until its demise in 1862, in the wake of the creation of the German Empire under Kaiser Wilhelm I. Hence, only Lübeck, Hamburg, and Bremen retain the words "Hanseatic City" in their official German titles.

The members of the Hanseatic League were Low German merchants, whose towns were, with the exception of Dinant, where these merchants held citizenship. Not all towns with Low German merchant communities were members of the league (e.g., Emden, Memel (today Klaipėda), Viborg (today Vyborg) and Narva never joined). However, Hanseatic merchants could also come from settlements without German town law—the premise for league membership was birth to German parents, subjection to German law, and a commercial education. The league served to advance and defend the common interests of its heterogeneous members: commercial ambitions such as enhancement of trade, and political ambitions such as ensuring maximum independence from the noble territorial rulers.The Hanseatic League was by no means a monolithic organization or a 'state within a state' but rather a complex and loose-jointed confederation of protagonists pursuing their own interests, which coincided in a shared program of economic domination in the Baltic region. 

Decisions and actions of the Hanseatic League were the consequence of a consensus-based procedure. If an issue arose, the league's members were invited to participate in a central meeting, the "Tagfahrt" ("meeting ride", sometimes also referred to as "Hansetag", since 1358). The member communities then chose envoys ("Ratssendeboten") to represent their local consensus on the issue at the "Tagfahrt". Not every community sent an envoy; delegates were often entitled to represent a set of communities. Consensus-building on local and "Tagfahrt" levels followed the Low Saxon tradition of "Einung", where consensus was defined as absence of protest: after a discussion, the proposals which gained sufficient support were dictated aloud to the scribe and passed as binding "Rezess" if the attendees did not object; those favouring alternative proposals unlikely to get sufficient support were obliged to remain silent during this procedure. If consensus could not be established on a certain issue, it was found instead in the appointment of a number of league members who were then empowered to work out a compromise.

The Hanseatic "Kontore", which operated like an early stock exchange,
each had their own treasury, court and seal. Like the guilds, the "Kontore" were led by "Ältermänner" ("eldermen", or English aldermen). The Stalhof "Kontor", as a special case, had a Hanseatic and an English "Ältermann". In 1347 the "Kontor" of Brussels modified its statute to ensure an equal representation of the league's members. To that end, member communities from different regions were pooled into three circles ("Drittel" ("third [part]"): the Wendish and Saxon Drittel, the Westphalian and Prussian Drittel as well as the Gothlandian, Livonian and Swedish Drittel. The merchants from their respective "Drittel" would then each choose two "Ältermänner" and six members of the Eighteen Men's Council ("Achtzehnmännerrat") to administer the "Kontor" for a set period of time. In 1356, during a Hanseatic meeting in preparation of the first "Tagfahrt", the league confirmed this statute. The league in general gradually adopted and institutionalized the division into "Drittel" (see table).

The "Tagfahrt" or "Hansetag" was the only central institution of the Hanseatic League. However, with the division into "Drittel", the members of the respective subdivisions frequently held a "Dritteltage" (""Drittel" meeting") to work out common positions which could then be presented at a "Tagfahrt". On a more local level, league members also met, and while such regional meetings were never formalized into a Hanseatic institution, they gradually gained importance in the process of preparing and implementing "Tagfahrt" decisions.

From 1554, the division into "Drittel" was modified to reduce the circles' heterogeneity, to enhance the collaboration of the members on a local level and thus to make the league's decision-making process more efficient. The number of circles rose to four, so they were called "Quartiere" (quarters):

This division was however not adopted by the "Kontore", who, for their purposes (like "Ältermänner" elections), grouped the league members in different ways (e.g., the division adopted by the Stahlhof in London in 1554 grouped the league members into "Dritteln", whereby Lübeck merchants represented the Wendish, Pomeranian Saxon and several Westphalian towns, Cologne merchants represented the Cleves, Mark, Berg and Dutch towns, while Danzig merchants represented the Prussian and Livonian towns).

The names of the Quarters have been abbreviated in the following table:

Kontor: The Kontore were foreign trading posts of the League, not cities that were Hanseatic members, and are set apart in a separate table below.

The remaining column headings are as follows:
 

 


Despite its collapse, several cities still maintained the link to the Hanseatic League. Dutch cities including Groningen, Deventer, Kampen, Zutphen and Zwolle, and a number of German cities including Bremen, Demmin, Greifswald, Hamburg, Lübeck, Lüneburg, Rostock, Stade, Stralsund and Wismar still call themselves "Hanse" cities (their car license plates are prefixed "H", e.g. –"HB"– for "Hansestadt Bremen"). Hamburg and Bremen continue to style themselves officially as "free Hanseatic cities", with Lübeck named "Hanseatic City" (Rostock's football team is named F.C. Hansa Rostock in memory of the city's trading past). For Lübeck in particular, this anachronistic tie to a glorious past remained especially important in the 20th century. In 1937, the Nazi Party removed this privilege through the Greater Hamburg Act possibly because the "Senat" of Lübeck did not permit Adolf Hitler to speak in Lübeck during his 1932 election campaign. He held the speech in Bad Schwartau, a small village on the outskirts of Lübeck. Subsequently, he referred to Lübeck as "the small city close to Bad Schwartau."

After the EU enlargement to the East in May 2004 there were some experts who wrote about the resurrection of the Baltic Hansa.

The legacy of the Hansa is remembered today in several names: the German airline Lufthansa (i.e., "Air Hansa"); F.C. Hansa Rostock; Hanze University of Applied Sciences, Groningen, Netherlands; Hanze oil production platform, Netherlands; the Hansa Brewery in Bergen and the Hanse Sail in Rostock. DDG Hansa was a major German shipping company from 1881 until its bankruptcy in 1980. Hansabank in the Baltic states, which has been rebranded into Swedbank. Hansa-Park, one of the biggest theme parks in Germany.

There are two museums in Europe dedicated specifically to the history of the Hanseatic League: the European Hansemuseum in Lübeck and the Hanseatic Museum and Schøtstuene in Bergen.

In 1980, former Hanseatic League members established a "new Hanse" in Zwolle. This league is open to all former Hanseatic League members and cities that share a Hanseatic Heritage. In 2012 the New Hanseatic league had 187 members. This includes twelve Russian cities, most notably Novgorod, which was a major Russian trade partner of the Hansa in the Middle Ages. The "new Hanse" fosters and develops business links, tourism and cultural exchange.

The headquarters of the New Hansa is in Lübeck, Germany. The current President of the Hanseatic League of New Time is Bernd Saxe, Mayor of Lübeck.

Each year one of the member cities of the New Hansa hosts the Hanseatic Days of New Time international festival.

In 2006 King's Lynn became the first English member of the newly formed new Hanseatic League. It was joined by Hull in 2012 and Boston in 2016.

The "New Hanseatic League" was established in February 2018 by finance ministers from Denmark, Estonia, Finland, Ireland, Latvia, Lithuania, the Netherlands and Sweden through the signing of a two-page foundational document which set out the countries' "shared views and values in the discussion on the architecture of the EMU."






</doc>
<doc id="14107" url="https://en.wikipedia.org/wiki?curid=14107" title="Harvard (disambiguation)">
Harvard (disambiguation)

Harvard University is a university in Cambridge, Massachusetts, USA.

Harvard may also refer to:






</doc>
<doc id="14108" url="https://en.wikipedia.org/wiki?curid=14108" title="Historical African place names">
Historical African place names

This is a list of historical African place names. The names on the left are linked to the corresponding subregion(s) from History of Africa.

See also: List of extinct countries, empires, etc.


</doc>
<doc id="14109" url="https://en.wikipedia.org/wiki?curid=14109" title="Horror fiction">
Horror fiction

Horror is a genre of speculative fiction which is intended to frighten, scare, or disgust. Literary historian J. A. Cuddon defined the horror story as "a piece of fiction in prose of variable length... which shocks, or even frightens the reader, or perhaps induces a feeling of repulsion or loathing". It creates an eerie and frightening atmosphere. Horror is frequently supernatural, though it might also be non-supernatural. Often the central menace of a work of horror fiction can be interpreted as a metaphor for the larger fears of a society.

The horror genre has ancient origins with roots in folklore and religious traditions, focusing on death, the afterlife, evil, the demonic and the principle of the thing embodied in the person. These were manifested in stories of beings such as demons, witches, vampires, werewolves and ghosts. European horror fiction became established through works of the Ancient Greeks and Ancient Romans. The well-known 19th-century novel about Frankenstein was greatly influenced by the story of Hippolytus, where Asclepius revives him from death. Euripides wrote plays based on the story, "Hippolytos Kalyptomenos" and "Hippolytus". In Plutarch's "The Lives of the Noble Grecians and Romans" focused on Cimon, the author describes the spirit of a murderer, Damon, who himself was murdered in a bathhouse in Chaeronea.

Pliny the Younger tells the tale of Athenodorus Cananites who bought a haunted house in Athens. Athenodorus was cautious since the house was inexpensive. While writing a book on philosophy, he was visited by a ghostly appearing figure bound in chains. The figure disappeared in the courtyard; the following day, the magistrates dug it up to find an unmarked grave.

Werewolf stories were popular in medieval French literature. One of Marie de France's twelve lais is a werewolf story titled "Bisclavret". 

The Countess Yolande commissioned a werewolf story titled "Guillaume de Palerme". Anonymous writers penned two werewolf stories, "Biclarel" and "Melion".

Much horror fiction derives from the cruellest personages of the 15th century. Dracula can be traced to the Prince of Wallachia Vlad III, whose alleged war crimes were published in German pamphlets. A 1499 pamphlet was published by Markus Ayrer, which is most notable for its woodcut imagery. The alleged serial-killer sprees of Gilles de Rais have been seen as the inspiration for "Bluebeard". The motif of the vampiress is most notably derived from the real-life noblewoman and murderess, Elizabeth Bathory, and helped usher in the emergence of horror fiction in the 18th century, such as through László Turóczi's 1729 book "Tragica Historia".

The 18th century saw the gradual development of Romanticism and the Gothic horror genre. It drew on the written and material heritage of the Late Middle Ages, finding its form with Horace Walpole's seminal and controversial 1764 novel, "The Castle of Otranto". In fact, the first edition was published disguised as an actual medieval romance from Italy, discovered and republished by a fictitious translator. Once revealed as modern, many found it anachronistic, reactionary, or simply in poor taste but it proved immediately popular. "Otranto" inspired "Vathek" (1786) by William Beckford, "A Sicilian Romance" (1790), "The Mysteries of Udolpho" (1794) and "The Italian" (1796) by Ann Radcliffe and "The Monk" (1797) by Matthew Lewis. A significant amount of horror fiction of this era was written by women and marketed towards a female audience, a typical scenario of the novels being a resourceful female menaced in a gloomy castle.

The Gothic tradition blossomed into the genre that modern readers today call horror literature in the 19th century. Influential works and characters that continue resonating in fiction and film today saw their genesis in the Brothers Grimm's "Hänsel und Gretel" (1812), Mary Shelley's "Frankenstein" (1818), John Polodori's "The Vampyre" (1819), Charles Maturin's "Melmoth the Wanderer" (1820), Washington Irving's "The Legend of Sleepy Hollow" (1820), Jane C. Loudon's "" (1827), Victor Hugo's "The Hunchback of Notre Dame" (1831), Thomas Peckett Prest's "Varney the Vampire" (1847), the works of Edgar Allan Poe, the works of Sheridan Le Fanu, Robert Louis Stevenson's "Strange Case of Dr Jekyll and Mr Hyde" (1886), Oscar Wilde's "The Picture of Dorian Gray" (1890), H. G. Wells' "The Invisible Man" (1897), and Bram Stoker's "Dracula" (1897). Each of these works created an enduring icon of horror seen in later re-imaginings on the page, stage and screen.

A proliferation of cheap periodicals around turn of the century led to a boom in horror writing. For example, Gaston Leroux serialized his "Le Fantôme de l'Opéra" before it became a novel in 1910. One writer who specialized in horror fiction for mainstream pulps, such as "All-Story Magazine," was Tod Robbins, whose fiction deals with themes of madness and cruelty. Later, specialist publications emerged to give horror writers an outlet, prominent among them was "Weird Tales" and "Unknown" "Worlds".

Influential horror writers of the early 20th century made inroads in these mediums. Particularly, the venerated horror author H.P. Lovecraft, and his enduring Cthulhu Mythos transformed and popularized the genre of cosmic horror, and M.R. James is credited with redefining the ghost story in that era.

The serial murderer became a recurring theme. Yellow journalism and sensationalism of various murderers, such as Jack the Ripper, and lesser so, Carl Panzram, Fritz Haarman, and Albert Fish, all perpetuated this phenomenon. The trend continued in the postwar era, partly renewed after the murders committed by Ed Gein. In 1959, Robert Bloch, inspired by the murders, wrote "Psycho". The crimes committed in 1969 by the Manson family influenced the slasher theme in horror fiction of the 1970s. In 1981, Thomas Harris wrote "Red Dragon", introducing Dr. Hannibal Lecter. In 1988, the sequel to that novel, "The Silence of the Lambs", was published.

Early cinema was inspired by many aspects of horror literature, and started a strong tradition of horror films and subgenres that continues to this day. Up until the graphic depictions of violence and gore on the screen commonly associated with 1960s and 1970s slasher films and splatter films, comic books such as those published by EC Comics (most notably "Tales From The Crypt") in the 1950s satisfied readers' quests for horror imagery that the silver screen could not provide. This imagery made these comics controversial, and as a consequence, they were frequently censored.

The modern zombie tale dealing with the motif of the living dead harks back to works including H.P. Lovecraft's stories "Cool Air" (1925), "In The Vault" (1926), and "The Outsider" (1926), and Dennis Wheatley's "Strange Conflict" (1941). Richard Matheson's novel "I Am Legend" (1954) influenced an entire genre of apocalyptic zombie fiction emblematized by the films of George A. Romero.

In the late 1960s and early 1970s, the enormous commercial success of three books - "Rosemary's Baby" (1967) by Ira Levin, "The Exorcist" by William Peter Blatty, and "The Other" by Thomas Tryon - encouraged publishers to begin releasing numerous other horror novels, thus creating a "horror boom".
One of the best-known late-20th century horror writers is Stephen King, known for "Carrie", "The Shining", "It", "Misery" and several dozen other novels and about 200 short stories. Beginning in the 1970s, King's stories have attracted a large audience, for which he was awarded by the U.S. National Book Foundation in 2003. Other popular horror authors of the period included Anne Rice, Brian Lumley, Graham Masterton, James Herbert, Dean Koontz, Clive Barker, Ramsey Campbell, and Peter Straub.

Best-selling book series of contemporary times exist in genres related to horror fiction, such as the werewolf fiction urban fantasy Kitty Norville books by Carrie Vaughn (2005 onward). Horror elements continue to expand outside the genre. The alternate history of more traditional historical horror in Dan Simmons's 2007 novel "The Terror" sits on bookstore shelves next to genre mash ups such as "Pride and Prejudice and Zombies" (2009), and historical fantasy and horror comics such as "Hellblazer" (1993 onward) and Mike Mignola's Hellboy (1993 onward). Horror also serves as one of the central genres in more complex modern works such as Mark Z. Danielewski's "House of Leaves" (2000), a finalist for the National Book Award. There are many horror novels for teens, such as "The Monstrumologist" by Rick Yancey (2009). Additionally, many movies, particularly animated ones, use a horror aesthetic. These are what can be collectively referred to as "children's horror". Although it's unknown for sure why children enjoy these movies (as it seems counter-intuitive), it is theorized that it is the grotesque monsters that fascinate kids. Tangential to this, the internalized impact of horror television programs and films on children is rather under-researched, especially when compared to the research done on the similar subject of violence in TV and film's impact on the young mind. What little research there is tends to be inconclusive on the impact that viewing such media has.

One defining trait of the horror genre is that it provokes an emotional, psychological, or physical response within readers that causes them to react with fear. One of H.P. Lovecraft's most famous quotes about the genre is that: "The oldest and strongest emotion of mankind is fear, and the oldest and strongest kind of fear is fear of the unknown." the first sentence from his seminal essay, "Supernatural Horror in Literature". Science fiction historian Darrell Schweitzer has stated, "In the simplest sense, a horror story is one that scares us" and "the true horror story requires a sense of evil, not in necessarily in a theological sense; but the menaces must be truly menacing, life-destroying, and antithetical to happiness." 

In her essay "Elements of Aversion", Elizabeth Barrette articulates the need by some for horror tales in a modern world:
In a sense similar to the reason a person seeks out the controlled thrill of a roller coaster, readers in the modern era seek out feelings of horror and terror to feel a sense of excitement. However, Barrette adds that horror fiction is one of the few mediums where readers seek out a form of art that forces themselves to confront ideas and images they "might rather ignore to challenge preconceptions of all kinds."

One can see the confrontation of ideas that readers and characters would "rather ignore" throughout literature in famous moments such as Hamlet's musings about the skull of Yorick, its implications of the mortality of humanity, and the gruesome end that bodies inevitably come to. In horror fiction, the confrontation with the gruesome is often a metaphor for the problems facing the current generation of the author.

There are many theories as to why people enjoy being scared. For example, "people who like horror films are more likely to score highly for openness to experience, a personality trait linked to intellect and imagination."

It is a now commonly accepted viewpoint that the horror elements of Dracula's portrayal of vampirism are metaphors for sexuality in a repressed Victorian era. But this is merely one of many interpretations of the metaphor of Dracula. Judith Halberstam postulates many of these in her essay "Technologies of Monstrosity: Bram Stoker's Dracula". She writes:

Halberstram articulates a view of Dracula as manifesting the growing perception of the aristocracy as an evil and outdated notion to be defeated. The depiction of a multinational band of protagonists using the latest technologies (such as a telegraph) to quickly share, collate, and act upon new information is what leads to the destruction of the vampire. This is one of many interpretations of the metaphor of only one central figure of the canon of horror fiction, as over a dozen possible metaphors are referenced in the analysis, from the religious to the anti-semitic.

Noël Carroll's "Philosophy of Horror" postulates that a modern piece of horror fiction's "monster", villain, or a more inclusive menace must exhibit the following two traits:

In addition to those essays and articles shown above, scholarship on horror fiction is almost as old as horror fiction itself. In 1826, the gothic novelist Ann Radcliffe published an essay distinguishing two elements of horror fiction, "terror" and "horror." Whereas terror is a feeling of dread that takes place before an event happens, horror is a feeling of revulsion or disgust after an event has happened. Radcliffe describes terror as that which "expands the soul and awakens the faculties to a high degree of life," whereas horror is described as that which "freezes and nearly annihilates them."

Modern scholarship on horror fiction draws upon a range of sources. In their historical studies of the gothic novel, both Devandra Varma and S.L. Varnado make reference to the theologian Rudolf Otto, whose concept of the "numinous" was originally used to describe religious experience.

A recent survey reports how often horror media is consumed:To assess frequency of horror consumption, we asked respondents the following question: “In the past year, about how often have you used horror media (e.g., horror literature, film, and video games) for entertainment?” 11.3% said “Never,” 7.5% “Once,” 28.9% “Several times,” 14.1% “Once a month,” 20.8% “Several times a month,” 7.3% “Once a week,” and 10.2% “Several times a week.” Evidently, then, most respondents (81.3%) claimed to use horror media several times a year or more often. Unsurprisingly, there is a strong correlation between liking and frequency of use (r=.79, p<.0001). 

Achievements in horror fiction are recognized by numerous awards. The Horror Writer's Association presents the Bram Stoker Awards for Superior Achievement, named in honor of Bram Stoker, author of the seminal horror novel "Dracula". The Australian Horror Writers Association presents annual Australian Shadows Awards. The International Horror Guild Award was presented annually to works of horror and dark fantasy from 1995 to 2008. The Shirley Jackson Awards are literary awards for outstanding achievement in the literature of psychological suspense, horror, and the dark fantastic works. Other important awards for horror literature are included as subcategories within general awards for fantasy and science fiction in such awards as the Aurealis Award.

Some writers of fiction normally classified as "horror" tend to dislike the term, considering it too lurid. They instead use the terms dark fantasy or Gothic fantasy for supernatural horror, or "psychological thriller" for non-supernatural horror.





</doc>
<doc id="14110" url="https://en.wikipedia.org/wiki?curid=14110" title="Holomorphic function">
Holomorphic function

In mathematics, a holomorphic function is a complex-valued function of one or more complex variables that is, at every point of its domain, complex differentiable in a neighborhood of the point. The existence of a complex derivative in a neighbourhood is a very strong condition, for it implies that any holomorphic function is actually infinitely differentiable and equal, locally, to its own Taylor series ("analytic"). Holomorphic functions are the central objects of study in complex analysis.

Though the term "analytic function" is often used interchangeably with "holomorphic function", the word "analytic" is defined in a broader sense to denote any function (real, complex, or of more general type) that can be written as a convergent power series in a neighbourhood of each point in its domain. The fact that all holomorphic functions are complex analytic functions, and vice versa, is a major theorem in complex analysis.

Holomorphic functions are also sometimes referred to as "regular functions". A holomorphic function whose domain is the whole complex plane is called an entire function. The phrase "holomorphic at a point "z"" means not just differentiable at "z", but differentiable everywhere within some neighbourhood of "z" in the complex plane.

Given a complex-valued function "f" of a single complex variable, the derivative of "f" at a point "z" in its domain is defined by the limit

This is the same as the definition of the derivative for real functions, except that all of the quantities are complex. In particular, the limit is taken as the complex number "z" approaches "z", and must have the same value for any sequence of complex values for "z" that approach "z" on the complex plane. If the limit exists, we say that "f" is complex-differentiable at the point "z". This concept of complex differentiability shares several properties with real differentiability: it is linear and obeys the product rule, quotient rule, and chain rule.

If "f" is "complex differentiable" at "every" point "z" in an open set "U", we say that "f" is holomorphic on "U". We say that "f" is holomorphic at the point "z" if "f" is complex differentiable on some neighbourhood of "z". We say that "f" is holomorphic on some non-open set "A" if it is holomorphic in an open set containing "A". As a pathological non-example, the function given by "f"("z") = |"z"| is complex differentiable at exactly one point ("z" = 0), and for this reason, it is "not" holomorphic at 0 because there is no open set around 0 on which "f" is complex differentiable. 

The relationship between real differentiability and complex differentiability is the following. If a complex function is holomorphic, then "u" and "v" have first partial derivatives with respect to "x" and "y", and satisfy the Cauchy–Riemann equations:

or, equivalently, the Wirtinger derivative of "f" with respect to the complex conjugate of "z" is zero:

which is to say that, roughly, "f" is functionally independent from the complex conjugate of "z".

If continuity is not given, the converse is not necessarily true. A simple converse is that if "u" and "v" have "continuous" first partial derivatives and satisfy the Cauchy–Riemann equations, then "f" is holomorphic. A more satisfying converse, which is much harder to prove, is the Looman–Menchoff theorem: if "f" is continuous, "u" and "v" have first partial derivatives (but not necessarily continuous), and they satisfy the Cauchy–Riemann equations, then "f" is holomorphic.

The word "holomorphic" was introduced by two of Cauchy's students, Briot (1817–1882) and Bouquet (1819–1895), and derives from the Greek ὅλος ("holos") meaning "entire", and μορφή ("morphē") meaning "form" or "appearance".

Today, the term "holomorphic function" is sometimes preferred to "analytic function", as the latter is a more general concept. This is also because an important result in complex analysis is that every holomorphic function is complex analytic, a fact that does not follow obviously from the definitions. The term "analytic" is however also in wide use.

Because complex differentiation is linear and obeys the product, quotient, and chain rules; the sums, products and compositions of holomorphic functions are holomorphic, and the quotient of two holomorphic functions is holomorphic wherever the denominator is not zero.

If one identifies C with R, then the holomorphic functions coincide with those functions of two real variables with continuous first derivatives which solve the Cauchy–Riemann equations, a set of two partial differential equations.

Every holomorphic function can be separated into its real and imaginary parts, and each of these is a solution of Laplace's equation on R. In other words, if we express a holomorphic function "f"("z") as both "u" and "v" are harmonic functions, where v is the harmonic conjugate of u.

Cauchy's integral theorem implies that the contour integral of every holomorphic function along a loop vanishes:

Here "γ" is a rectifiable path in a simply connected open subset "U" of the complex plane C whose start point is equal to its end point, and is a holomorphic function.

Cauchy's integral formula states that every function holomorphic inside a disk is completely determined by its values on the disk's boundary. Furthermore: Suppose "U" is an open subset of C, is a holomorphic function and the closed disk is completely contained in "U". Let γ be the circle forming the boundary of "D". Then for every "a" in the interior of "D":

where the contour integral is taken counter-clockwise.

The derivative "f"′("a") can be written as a contour integral using Cauchy's differentiation formula:

for any simple loop positively winding once around "a", and

for infinitesimal positive loops γ around "a".

In regions where the first derivative is not zero, holomorphic functions are conformal in the sense that they preserve angles and the shape (but not size) of small figures.

Every holomorphic function is analytic. That is, a holomorphic function "f" has derivatives of every order at each point "a" in its domain, and it coincides with its own Taylor series at "a" in a neighbourhood of "a". In fact, "f" coincides with its Taylor series at "a" in any disk centred at that point and lying within the domain of the function.

From an algebraic point of view, the set of holomorphic functions on an open set is a commutative ring and a complex vector space. Additionally, the set of holomorphic functions in an open set U is an integral domain if and only if the open set U is connected. In fact, it is a locally convex topological vector space, with the seminorms being the suprema on compact subsets.

From a geometric perspective, a function "f" is holomorphic at "z" if and only if its exterior derivative "df" in a neighbourhood "U" of "z" is equal to "f"′("z") "dz" for some continuous function "f"′. It follows from

that "df"′ is also proportional to "dz", implying that the derivative "f"′ is itself holomorphic and thus that "f" is infinitely differentiable. Similarly, the fact that implies that any function "f" that is holomorphic on the simply connected region "U" is also integrable on "U". (For a path γ from "z" to "z" lying entirely in "U", define

in light of the Jordan curve theorem and the generalized Stokes' theorem, "F"("z") is independent of the particular choice of path γ, and thus "F"("z") is a well-defined function on "U" having and .)

All polynomial functions in "z" with complex coefficients are holomorphic on C, and so are sine, cosine and the exponential function. (The trigonometric functions are in fact closely related to and can be defined via the exponential function using Euler's formula). The principal branch of the complex logarithm function is holomorphic on the set The square root function can be defined as

and is therefore holomorphic wherever the logarithm log("z") is. The function 1/"z" is holomorphic on 

As a consequence of the Cauchy–Riemann equations, a real-valued holomorphic function must be constant. Therefore, the absolute value of "z", the argument of "z", the real part of "z" and the imaginary part of "z" are not holomorphic. Another typical example of a continuous function which is not holomorphic is the complex conjugate formed by complex conjugation.

The definition of a holomorphic function generalizes to several complex variables in a straightforward way. Let "D" denote an open subset of C, and let . The function "f" is analytic at a point "p" in "D" if there exists an open neighbourhood of "p" in which "f" is equal to a convergent power series in "n" complex variables. Define "f" to be holomorphic if it is analytic at each point in its domain. Osgood's lemma shows (using the multivariate Cauchy integral formula) that, for a continuous function "f", this is equivalent to "f" being holomorphic in each variable separately (meaning that if any coordinates are fixed, then the restriction of "f" is a holomorphic function of the remaining coordinate). The much deeper Hartogs' theorem proves that the continuity hypothesis is unnecessary: "f" is holomorphic if and only if it is holomorphic in each variable separately.

More generally, a function of several complex variables that is square integrable over every compact subset of its domain is analytic if and only if it satisfies the Cauchy–Riemann equations in the sense of distributions.

Functions of several complex variables are in some basic ways more complicated than functions of a single complex variable. For example, the region of convergence of a power series is not necessarily an open ball; these regions are Reinhardt domains, the simplest example of which is a polydisk. However, they also come with some fundamental restrictions. Unlike functions of a single complex variable, the possible domains on which there are holomorphic functions that cannot be extended to larger domains are highly limited. Such a set is called a domain of holomorphy.

A complex differential ("p",0)-form α is holomorphic if and only if its antiholomorphic Dolbeault derivative is zero, formula_11.

The concept of a holomorphic function can be extended to the infinite-dimensional spaces of functional analysis. For instance, the Fréchet or Gateaux derivative can be used to define a notion of a holomorphic function on a Banach space over the field of complex numbers.



</doc>
<doc id="14113" url="https://en.wikipedia.org/wiki?curid=14113" title="History of Algeria">
History of Algeria

Much of the history of Algeria has taken place 
on the fertile coastal plain of North Africa, which is often called the Maghreb (or Maghrib). North Africa served as a transit region for people moving towards Europe or the Middle East, thus, the region's inhabitants have been influenced by populations from other areas, including the Carthaginians, Romans, and Vandals. The region was conquered by the Muslims in the early 8th century AD, but broke off from the Umayyad Caliphate after the Berber Revolt of 740. Later, various Berbers, Arabs, Persian Muslim states, Sunni, Shia or Ibadi communities were established that ruled parts of modern-day of Algeria: including the Rustamids, Ifranids, Fatimids, Maghrawas, Zirids, Hammadids, Almoravid, Almohads, Hafsids, and Ziyyanids. During the Ottoman period, Algiers was the center of the Barbary slave trade which led to many naval conflicts. The last significant events in the country's recent history have been the Algerian War and Algerian Civil War.

Evidence of the early human occupation of Algeria is demonstrated by the discovery of 1.8 million year old Oldowan stone tools found at Ain Hanech in 1992. In 1954 fossilised "Homo erectus" bones were discovered by C. Arambourg at Ternefine that are 700,000 years old. Neolithic civilization (marked by animal domestication and subsistence agriculture) developed in the Saharan and Mediterranean Maghrib between 6000 and 2000 BC. This type of economy, richly depicted in the Tassili n'Ajjer cave paintings in southeastern Algeria, predominated in the Maghrib until the classical period. The amalgam of peoples of North Africa coalesced eventually into a distinct native population, the Berbers lacked a written language and hence tended to be overlooked or marginalized in historical accounts.

Since 4000 BC, the indigenous peoples of northern Africa successfully resisted Phoenician, Roman, Vandal, Byzantine, Turkish, and French invaders but accepted Islam between the 7th to 9th century, and Arabic is now the language spoken by a majority in the country.

Phoenician traders arrived on the North African coast around 900 BC and established Carthage (in present-day Tunisia) around 800 BC. During the classical period, Berber civilization was already at a stage in which agriculture, manufacturing, trade, and political organization supported several states. Trade links between Carthage and the Berbers in the interior grew, but territorial expansion also resulted in the enslavement or military recruitment of some Berbers and in the extraction of tribute from others.

The Carthaginian state declined because of successive defeats by the Romans in the Punic Wars, and in 146 BC, the city of Carthage was destroyed. As Carthaginian power waned, the influence of Berber leaders in the hinterland grew.

By the 2nd century BC, several large but loosely administered Berber kingdoms had emerged. After that, king Masinissa managed to unify Numidia under his rule.

Madghacen was a king of independent kingdoms of the Numidians, between 12 and 3 BC.

Christianity arrived in the 2nd century. By the end of the 4th century, the settled areas had become Christianized, and some Berber tribes had converted en masse.

After the fall of the Western Roman Empire, Algeria came under the control of the Vandal Kingdom. Later, the Eastern Roman Empire (also known as the Byzantine Empire) conquered Algeria from the Vandals, incorporating it into the Praetorian prefecture of Africa and later the Exarchate of Africa.

From the 8th century Umayyad conquest of North Africa led by Musa bin Nusayr, Arab colonization started. The 11th century invasion of migrants from the Arabian peninsula brought oriental tribal customs. The introduction of Islam and Arabic had a profound impact on North Africa. The new religion and language introduced changes in social and economic relations, and established links with the Arab world through acculturation and assimilation.

According to historians of the Middle Ages, the Berbers are divided into two branches, both are from their ancestor Mazigh. The two branches Botr and Barnès are divided into tribes, and each Maghreb region is made up of several tribes. The large Berber tribes or peoples are Sanhaja, Houara, Zenata, Masmuda, Kutama, Awarba, Barghawata ... etc. Each tribe is divided into sub tribes. All these tribes have independence and territorial decisions.

Several Berber dynasties emerged during the Middle Ages: - In North Africa, Sudan, in Andalusia, Italy, in Mali, Niger, Senegal and Egypt. Ibn Khaldoun made a table of Berber dynasties: Zirid, Banu Ifran, Maghrawa, Almoravid, Hammadid, Almohad Caliphate, Marinid, Zayyanid, Wattasid, Meknes, Hafsid dynasty.

The invasion of the Banu Hilal Arab tribes in 11th century, sacked Kairouan, and the area under Zirid control was reduced to the coastal region, and the Arab conquests fragmented into petty Bedouin emirates.

The second Arab military expeditions into the Maghreb, between 642 and 669, resulted in the spread of Islam. The Umayyads (a Muslim dynasty based in Damascus from 661 to 750) recognised that the strategic necessity of dominating the Mediterranean dictated a concerted military effort on the North African front. By 711 Umayyad forces helped by Berber converts to Islam had conquered all of North Africa. In 750 the Abbasids succeeded the Umayyads as Muslim rulers and moved the caliphate to Baghdad. Under the Abbasids, Berber Kharijites Sufri Banu Ifran were opposed to Umayyad and Abbasids. After, the Rustumids (761–909) actually ruled most of the central Maghrib from Tahirt, southwest of Algiers. The imams gained a reputation for honesty, piety, and justice, and the court of Tahirt was noted for its support of scholarship. The Rustumid imams failed, however, to organise a reliable standing army, which opened the way for Tahirt's demise under the assault of the Fatimid dynasty.

With their interest focused primarily on Egypt and Muslim lands beyond, the Fatimids left the rule of most of Algeria to the Zirids and Hammadid (972–1148), a Berber dynasty that centered significant local power in Algeria for the first time, but who were still at war with Banu Ifran (kingdom of Tlemcen) and Maghraoua (942-1068). This period was marked by constant conflict, political instability, and economic decline. Following a large incursion of Arab Bedouin from Egypt beginning in the first half of the 11th century, the use of Arabic spread to the countryside, and sedentary Berbers were gradually Arabised.

The Almoravid ("those who have made a religious retreat") movement developed early in the 11th century among the Sanhaja Berbers of southern Morocco. The movement's initial impetus was religious, an attempt by a tribal leader to impose moral discipline and strict adherence to Islamic principles on followers. But the Almoravid movement shifted to engaging in military conquest after 1054. By 1106, the Almoravids had conquered the Maghreb as far east as Algiers and Morocco, and Spain up to the Ebro River.

Like the Almoravids, the Almohads ("unitarians") found their inspiration in Islamic reform. The Almohads took control of Morocco by 1146, captured Algiers around 1151, and by 1160 had completed the conquest of the central Maghrib. The zenith of Almohad power occurred between 1163 and 1199. For the first time, the Maghrib was united under a local regime, but the continuing wars in Spain overtaxed the resources of the Almohads, and in the Maghrib their position was compromised by factional strife and a renewal of tribal warfare.

In the central Maghrib, the Abdalwadid founded a dynasty that ruled the Kingdom of Tlemcen in Algeria. For more than 300 years, until the region came under Ottoman suzerainty in the 16th century, the Zayanids kept a tenuous hold in the central Maghrib. Many coastal cities asserted their autonomy as municipal republics governed by merchant oligarchies, tribal chieftains from the surrounding countryside, or the privateers who operated out of their ports. Nonetheless, Tlemcen, the "pearl of the Maghrib," prospered as a commercial center.

The final triumph of the 700-year Christian reconquest of Spain was marked by the fall of Granada in 1492. Christian Spain imposed its influence on the Maghrib coast by constructing fortified outposts and collecting tribute. But Spain never sought to extend its North African conquests much beyond a few modest enclaves. Privateering was an age-old practice in the Mediterranean, and North African rulers engaged in it increasingly in the late 16th and early 17th centuries because it was so lucrative. Until the 17th century the Barbary pirates used galleys, but a Dutch renegade of the name of Zymen Danseker taught them the advantage of using sailing ships.

Algeria became the privateering city-state par excellence, and two privateer brothers were instrumental in extending Ottoman influence in Algeria. At about the time Spain was establishing its presidios in the Maghrib, the Muslim privateer brothers Aruj and Khair ad Din—the latter known to Europeans as Barbarossa, or Red Beard—were operating successfully off Tunisia. In 1516 Aruj moved his base of operations to Algiers but was killed in 1518. Khair ad Din succeeded him as military commander of Algiers, and the Ottoman sultan gave him the title of beylerbey (provincial governor).

The Spanish expansionist policy in North Africa begun with the Catholic Monarchs and the regent Cisneros, once the "Reconquista" in the Iberian Peninsula was finished. That way, several towns and outposts in the Algerian coast were conquered and occupied: Mers El Kébir (1505), Oran (1509), Algiers (1510) and Bugia (1510). The Spanish conquest of Oran was won with much bloodshed: 4,000 Algerians were massacred, and up to 8,000 were taken prisoner. For about 200 years, Oran's inhabitants were virtually held captive in their fortress walls, ravaged by famine and plague; Spanish soldiers, too, were irregularly fed and paid.

The Spaniards left Algiers in 1529, Bujia in 1554, Mers El Kébir and Oran in 1708. The Spanish returned in 1732 when the armada of the Duke of Montemar was victorious in the Battle of Aïn-el-Turk and retook Oran and Mers El Kébir; the Spanish massacred many Muslim soldiers. In 1751, a Spanish adventurer, named John Gascon, obtained permission, and vessels and fireworks, to go against Algiers, and set fire, at night, to the Algerian fleet. The plan, however, miscarried. In 1775, Charles III of Spain sent a large force to attack Algiers, under the command of Alejandro O'Reilly (who had led Spanish forces in crushing French rebellion in Louisiana), resulting in a disastrous defeat. The Algerians suffered 5,000 casualties. The Spanish navy bombarded Algiers in 1784; over 20,000 cannonballs were fired, much of the city and its fortifications were destroyed and most of the Algerian fleet was sunk.

Oran and Mers El Kébir were held until 1792, when they were sold by the king Charles IV to the Bey of Algiers.

Under Khair ad Din's regency, Algiers became the center of Ottoman authority in the Maghrib. For 300 years, Algeria was a province of the Ottoman Empire under a regency that had Algiers as its capital (see Dey). Subsequently, with the institution of a regular Ottoman administration, governors with the title of pasha ruled. Turkish was the official language, and Arabs and Berbers were excluded from government posts. In 1671 a new leader took power, adopting the title of dey. In 1710 the dey persuaded the sultan to recognize him and his successors as regent, replacing the pasha in that role.

Although Algiers remained a part of the Ottoman Empire, the Ottoman government ceased to have effective influence there. European maritime powers paid the tribute demanded by the rulers of the privateering states of North Africa (Algiers, Tunis, Tripoli, and Morocco) to prevent attacks on their shipping. The Napoleonic wars of the early 19th century diverted the attention of the maritime powers from suppressing piracy. But when peace was restored to Europe in 1815, Algiers found itself at war with Spain, the Netherlands, Prussia, Denmark, Russia, and Naples. Algeria and surrounding areas, collectively known as the Barbary States, were responsible for piracy in the Mediterranean Sea, as well as the enslaving of Christians, actions which brought them into the First and Second Barbary War with the United States of America.

North African boundaries have shifted during various stages of the conquests. The borders of modern Algeria were created by the French, whose colonization began in 1830 (French invasion began on July 5). To benefit French colonists (many of whom were not in fact of French origin but Italian, Maltese, and Spanish) and nearly the entirety of whom lived in urban areas, northern Algeria was eventually organized into overseas departments of France, with representatives in the French National Assembly. France controlled the entire country, but the traditional Muslim population in the rural areas remained separated from the modern economic infrastructure of the European community.

As a result of what the French considered an insult to the French consul in Algiers by the Day in 1827, France blockaded Algiers for three years. In 1830, France invaded and occupied the coastal areas of Algeria, citing a diplomatic incident as casus belli. Hussein Dey went into exile. French colonization then gradually penetrated southwards, and came to have a profound impact on the area and its populations. The European conquest, initially accepted in the Algiers region, was soon met by a rebellion, led by Abdel Kadir, which took roughly a decade for the French troops to put down.
By 1848 nearly all of northern Algeria was under French control, and the new government of the French Second Republic declared the occupied lands an integral part of France. Three "civil territories"—Algiers, Oran, and Constantine—were organized as French départements (local administrative units) under a civilian government.

In addition to enduring the affront of being ruled by a foreign, non-Muslim power, many Algerians lost their lands to the new government or to colonists. Traditional leaders were eliminated, coopted, or made irrelevant, and the traditional educational system was largely dismantled; social structures were stressed to the breaking point. From 1856, native Muslims and Jews were viewed as French subjects not citizens.

However, in 1865, Napoleon III allowed them to apply for full French citizenship, a measure that few took, since it involved renouncing the right to be governed by "sharia" law in personal matters, and was considered a kind of apostasy; in 1870, the Crémieux Decree made French citizenship automatic for Jewish natives, a move which largely angered many Muslims, which resulted in the Jews being seen as the accomplices of the colonial power by anti-colonial Algerians. Nonetheless, this period saw progress in health, some infrastructures, and the overall expansion of the economy of Algeria, as well as the formation of new social classes, which, after exposure to ideas of equality and political liberty, would help propel the country to independence.

A new generation of Islamic leadership emerged in Algeria at the time of World War I and grew to maturity during the 1920s and 1930s. Various groups were formed in opposition to French rule, most notable the National Liberation Front (FLN) and the National Algerian Movement.

"Colons" (colonists), or, more popularly, "pieds noirs" (literally, black feet) dominated the government and controlled the bulk of Algeria's wealth. Throughout the colonial era, they continued to block or delay all attempts to implement even the most modest reforms. But from 1933 to 1936, mounting social, political, and economic crises in Algeria induced the indigenous population to engage in numerous acts of political protest. The government responded with more restrictive laws governing public order and security. Algerian Muslims rallied to the French side at the start of World War II as they had done in World War I. But the colons were generally sympathetic to the collaborationist Vichy regime established following France's defeat by Nazi Germany. After the fall of the Vichy regime in Algeria (November 11, 1942) as a result of Operation Torch, the Free French commander in chief in North Africa slowly rescinded repressive Vichy laws, despite opposition by colon extremists.

In March 1943, Muslim leader Ferhat Abbas presented the French administration with the Manifesto of the Algerian People, signed by 56 Algerian nationalist and international leaders. The manifesto demanded an Algerian constitution that would guarantee immediate and effective political participation and legal equality for Muslims. Instead, the French administration in 1944 instituted a reform package, based on the 1936 Viollette Plan, that granted full French citizenship only to certain categories of "meritorious" Algerian Muslims, who numbered about 60,000. In April 1945 the French had arrested the Algerian nationalist leader Messali Hadj. On May 1 the followers of his Parti du Peuple Algérien (PPA) participated in demonstrations which were violently put down by the police. Several Algerians were killed. The tensions between the Muslim and colon communities exploded on May 8, 1945, V-E Day. When a Muslim march was met with violence, marchers rampaged. The army and police responded by conducting a prolonged and systematic ratissage (literally, raking over) of suspected centers of dissidence. According to official French figures, 1,500 Muslims died as a result of these countermeasures. Other estimates vary from 6,000 to as high as 45,000 killed. Many nationalists drew the conclusion that independence could not be won by peaceful means, and so started organizing for violent rebellion.

In August 1947, the French National Assembly approved the government-proposed Organic Statute of Algeria. This law called for the creation of an Algerian Assembly with one house representing Europeans and "meritorious" Muslims and the other representing the remaining 8 million or more Muslims. Muslim and colon deputies alike abstained or voted against the statute but for diametrically opposed reasons: the Muslims because it fell short of their expectations and the colons because it went too far.

The Algerian War of Independence (1954–1962), brutal and long, was the most recent major turning point in the country's history. Although often fratricidal, it ultimately united Algerians and seared the value of independence and the philosophy of anticolonialism into the national consciousness. Abusive tactics of the French Army remains a controversial subject in France to this day.

In the early morning hours of November 1, 1954, the National Liberation Front (Front de Libération Nationale—FLN) launched attacks throughout Algeria in the opening salvo of a war of independence. An important watershed in this war was the massacre of civilians by the FLN near the town of Philippeville in August 1955. The government claimed it killed 1,273 guerrillas in retaliation; according to the FLN, 12,000 Muslims perished in an orgy of bloodletting by the armed forces and police, as well as colon gangs. After Philippeville, all-out war began in Algeria. The FLN fought largely using guerrilla tactics whilst the French counter-insurgency tactics often included severe reprisals and repression.

Eventually, protracted negotiations led to a cease-fire signed by France and the FLN on March 18, 1962, at Evian, France. The Evian accords also provided for continuing economic, financial, technical, and cultural relations, along with interim administrative arrangements until a referendum on self-determination could be held. The Evian accords guaranteed the religious and property rights of French settlers, but the perception that they would not be respected led to the exodus of one million "pieds-noirs" and "harkis".

Between 350,000 and 1 million Algerians are estimated to have died during the war, and more than 2 million, out of a total Muslim population of 9 or 10 million, were made into refugees or forcibly relocated into government-controlled camps. Much of the countryside and agriculture was devastated, along with the modern economy, which had been dominated by urban European settlers (the "pied-noirs"). French sources estimated that at least 70,000 Muslim civilians were killed or abducted and presumed killed, by the FLN during the Algerian War. Nearly one million people of mostly French, Spanish and Italian descent were forced to flee the country at independence due to the unbridgeable rifts opened by the civil war and threats from units of the victorious FLN; along with them fled most Algerians of Jewish descent and those Muslim Algerians who had supported a French Algeria ("harkis"). 30–150,000 pro-French Muslims were also killed in Algeria by FLN in post-war reprisals.

The referendum was held in Algeria on 1 July 1962, and France declared Algeria independent on 3 July. On 8 September 1963, a constitution was adopted by referendum, and later that month, Ahmed Ben Bella was formally elected the first president, after receiving support from the military, led by Houari Boumediène. The war for independence and its aftermath had severely disrupted Algeria's society and economy. In addition to the physical destruction, the exodus of the "colons" deprived the country of most of its managers, civil servants, engineers, teachers, physicians, and skilled workers. The homeless and displaced numbered in the hundreds of thousands, many suffering from illness, and some 70 percent of the workforce was unemployed.

The months immediately following independence witnessed the pell-mell rush of Algerians, their government, and its officials to claim the property and jobs left behind by the Europeans. In the 1963 March Decrees, Ben Bella declared that all agricultural, industrial, and commercial properties previously owned and operated by Europeans were vacant, thereby legalizing confiscation by the state. A new constitution drawn up under close FLN supervision was approved by nationwide referendum in September 1963, and Ben Bella was confirmed as the party's choice to lead the country for a five-year term.

The military played an important role in Ben Bella's administration. Since Ben Bella recognized the role that the military played in bringing him to power, Ben Bella appointed senior officers as ministers and other important positions within the new state, including naming Boumediène as the defence minister. They played a core role into implementing the country's security and foreign policy.

Under the new constitution, Ben Bella as president combined the functions of chief of state and head of government with those of supreme commander of the armed forces. He formed his government without needing legislative approval and was responsible for the definition and direction of its policies. There was no effective institutional check on its powers. Opposition leader Hocine Aït-Ahmed quit the National Assembly in 1963 to protest the increasingly dictatorial tendencies of the regime and formed a clandestine resistance movement, the Front of Socialist Forces (Front des Forces Socialistes—FFS) dedicated to overthrowing the Ben Bella regime by force.

Late summer 1963 saw sporadic incidents attributed to the FFS. More serious fighting broke out a year later. The army moved quickly and in force to crush the rebellion. As minister of defense, Houari Boumédienne had no qualms about sending the army to put down regional uprisings because he felt they posed a threat to the state. Ben Bella also attempted to co-opt allies from among some of those regionalists, in order to undermine the ability of military commanders to influence foreign and security policy. Tensions increased between Houari Boumédienne and Ahmed Ben Bella. In 1965 the military toppled Ahmed Ben Bella, and Houari Boumedienne became head of state.

On 19 June 1965, Houari Boumédienne deposed Ahmed Ben Bella in a military coup d'état that was both swift and bloodless. Ben Bella "disappeared", and would not be seen again until he was released from house arrest in 1980 by Boumédienne's successor, Colonel Chadli Bendjedid. Boumédienne immediately dissolved the National Assembly and suspended the 1963 constitution. Political power resided in the Council of the Revolution, a predominantly military body intended to foster cooperation among various factions in the army and the party.

Houari Boumédienne's position as head of government and of state was initially not secure partly because of his lack of a significant power base outside the armed forces; he relied strongly on a network of former associates known as the Oujda group (after his posting as ALN leader in the Moroccan border town of Oujda during the war years), but he could not fully dominate the fractious regime. This situation may have accounted for his deference to collegial rule.

Over Boumédienne's 11-year reign as the Chairman of the Revolutionary Council, it introduced two formal mechanisms, such as the People's Municipal Assembly ("Assemblée Populaires Communales") and the People's Provincial Assembly ("Assemblée Populaires de Wilaya") for popular participation in politics. Under his rule, leftist and socialist concepts are merged into Islam.

Boumédienne also used Islam opportunistically to consolidate his power. On one side, he made token concessions and cosmetic changes, such as appointing Ahmed Taleb Ibrahimi in charge of national education in 1965, or adopting policies like criminalizing gambling, establishing Friday as the national holiday and dropping plans to introduce birth control to paint an Islamic image of the new government. But on the other hand, the government also repressed Islamic groups progressively, such as ordering the dissolution of Al Qiyam.

Following attempted coups—most notably that of chief-of-staff Col. Tahar Zbiri in December 1967—and a failed assassination attempt in (April 25, 1968), Boumédienne consolidated power and forced military and political factions to submit. He took a systematic, authoritarian approach to state building, arguing that Algeria needed stability and an economic base before any political institutions.

Eleven years after Houari Boumédienne took power, after much public debate, a long-promised new constitution was promulgated in November 1976. The Constitution restored the National Popular Assembly and it was given legislative, consent and oversight functions. Boumédienne was later elected president with 95 percent of the cast votes.

Boumédienne's death on December 27, 1978 set off a struggle within the FLN to choose a successor. To break a deadlock between two candidates, Colonel Chadli Bendjedid, a moderate who had collaborated with Boumédienne in deposing Ahmed Ben Bella, was sworn in on February 9, 1979. He was re-elected in 1984 and 1988. After the violent 1988 October Riots, a new constitution was adopted in 1989 that allowed the formation of political associations other than the FLN. It also removed the armed forces, which had run the government since the days of Boumédienne, from a role in the operation of the government.

Among the scores of parties that sprang up under the new constitution, the militant Islamic Salvation Front (FIS) was the most successful, winning more than 50% of all votes cast in municipal elections in June 1990 as well as in first stage of national legislative elections held in December 1991.

The surprising first round of success for the fundamentalist FIS party in the December 1991 balloting caused the army to discuss options to intervene, as officers feared that an Islamist government would interfere their positions and core interests in economic, national security and foreign policy, since the FIS has promised to make a fundamental re-haul of the social, political and economical structure to achieve their radical Islamist agenda. Senior military figures, such as Defence Minister Khaled Nezzar, Chief of the General Staff Abdelmalek Guenaizia and other leaders of the navy, Gendarmerie and security services, all agreed that the FIS should be stopped from gaining power from the polling box. They also agreed that Bendjedid would need to be removed from office because he was the biggest obstacle to achieving the plan, due to his determination to hold the second round of ballots. If Bendjedid resigns, not only it will remove the obstacle to the military's plan, it would also suspend the second ballot.

On 11 January 1992, Bendjedid announced his resignation on national television, saying it was necessary to "protect the unity of the people and the security of the country". Later on the same day, the High Council of State ("Haut Comité d'Etat", HCE), which was composed of five people (including Nezzar, Tedjini Haddam, Ali Kafi, Mohamed Boudiaf and Ali Haroun), was appointed to carry out duties of the President.

The new government, led by Sid Ahmed Ghozali also banned all political activity at mosques and begin turning away people from attending prayers at the popular mosques. The FIS was legally dissolved by Interior Minister Larbi Belkheir on 9 February, for attempting "insurrections against the state". A state of emergency was also declared and extraordinary powers, such as curtailing the right to associate, were installed on the regime.

Between January and March, a growing number of FIS militants were arrested by the military, including Abdelkader Hachani and his successors to the FIS leadership, Othman Aissani and Rabah Kebir were also detained. Following the announcement to dissolve the FIS and implementing a state of emergency on 9 February, security forces used their new powers to conduct large scale arrests of FIS members and housed them in 5 "detention centers" in the Sahara. Between 5000 (official number) to 30,000 (FIS number) people were detained.

The fundamentalist response has resulted in a continuous low-grade, conflict, the Algerian Civil War, with the secular state apparatus, which nonetheless has allowed elections featuring pro-government and moderate religious-based parties. This civil war lasted until 2002.

After Chadli Bendjedid was removed from the presidency in the coup of 1992, a series of figureheads were selected by the military to assume the presidency, as officers were reluctant to assume public political power, even though they have manifested control over the country. It was because the military's senior leaders felt a need to give a civilian face to the new political regime they had hastily constructed in the aftermath of the ousting of Chadli and the termination of elections, and therefore their preference for a civilian face to front the regime.

The first of such was Mohamed Boudiaf, who was appointed president of the HCE in February 1992 after a 27-year exile in Morocco. However, Boudiaf quickly came to odds with the military, as attempts by the Boudiaf to appoint his own staff or forming a political party were viewed with suspicion by officers. Boudiaf also launch initiatives, such as a rigorous anti-corruption campaign in April 1992 and sacking Khaled Nezzar from his post as Defence Minister, which was seen by the military as an attempt to remove their influence, because a genuine campaign could implicate many senior figures who benefited massively and illegally from the system for many years. He was assassinated in June 1992 by one of his bodyguards with Islamist sympathies.

Ali Kafi temporary assumed the HCE presidency after Boudiaf's death, before Liamine Zéroual was appointed to be a long-term replacement in 1994. However, Zéroual only remained in office for four years before he announced his retirement, as he became embroiled with a clan warfare within the upper classes of the military and fell out with groups of more senior generals. Abdelaziz Bouteflika, Boumedienne's foreign minister succeeded as the president.

After the civil war ended, presidential elections were held again in April 1999. Although seven candidates qualified for election, all but Abdelaziz Bouteflika, who has the support of the military as well as the FLN, withdrew on the eve of the election amid charges of electoral fraud and interference from the military. Bouteflika went on to win with 70 percent of the cast votes.

But the civilian government in immediate post 1999 only acts as 'hijab' to run day-to-day businesses, while the military still runs the country behind the scenes, as ministerial mandates to individuals were only granted with the military's approval, while different factions of the military invested in various political parties and press to use them as pawns to gain influence.

The military's influence over politics decreased gradually, leaving Bouteflika with more authority on deciding policy. One reason for such was the senior commanders who dominated the political scene during the 1960s and 1970s started to retire. But Bouteflika's former experiences as Boumedienne's foreign minister earned him connections that rejuvenated Algeria's international reputation that was tarnished in the early 1990s. On the domestic front, Bouteflika's policy of 'national reconciliation' to bring a close to violence earned him a popular mandate that helped him to win further terms in 2004, 2009 and 2014.

In 2019, after 30 years in office, Bouteflika announced in February he would seek a fifth term of office. This sparked widespread discontent around Algeria and protests in Algiers for the first time since the Civil War. Despite later attempts at saying he would resign after his term finished in late April, Bouteflika resigned on 2 April, after the chief of the army, Ahmed Gaid Salah, called for a declaration that he was "unfit for office".

Despite Gaid Salah being a loyalist to Bouteflika, many in the military share an identity with civilians, as nearly 70 percent of the army are conscripts who are required to serve for 18 months. Since demonstrators demand a change to the whole system, army officers aligned themselves with demonstrators in the hopes of surviving the revolution and save their own positions.


1. The indigenous peoples of northern Africa were identified by the Romans as "Berbers", a word derived from the word "Barbare" or "Barbarian", but they prefer being called "Imazighen".
2. On the Banu Hilal invasion, see Ibn Khaldoun (v.1).




</doc>
<doc id="14114" url="https://en.wikipedia.org/wiki?curid=14114" title="History of Zimbabwe">
History of Zimbabwe

Following the Lancaster House Agreement of 1979 there was a transition to internationally recognized majority rule in 1980; the United Kingdom ceremonially granted Zimbabwe independence on 18 April that year. In the 2000s Zimbabwe's economy began to deteriorate due to various factors, including, the imposition of economic sanctions by western countries led by the United Kingdom, and also due to wide spread corruption in government. Economic instability caused a lot of Zimbabweans to move overseas or to neighboring countries. Prior to its recognized independence as Zimbabwe in 1980, the nation had been known by several names: Rhodesia, Southern Rhodesia and Zimbabwe Rhodesia.

Prior to the arrival of Bantu speakers in present-day Zimbabwe the region was populated by ancestors of the San people. The first Bantu-speaking farmers arrived during the Bantu expansion around 2000 years ago.

These Bantu speakers were the makers of early Iron Age pottery belonging to the Silver Leaves or Matola tradition, third to fifth centuries A.D., found in southeast Zimbabwe. This tradition was part of the eastern stream of Bantu expansion (sometimes called Kwale) which originated west of the Great Lakes, spreading to the coastal regions of southeastern Kenya and north eastern Tanzania, and then southwards to Mozambique, south eastern Zimbabwe and Natal. More substantial in numbers in Zimbabwe were the makers of the Ziwa and Gokomere ceramic wares, of the fourth century A.D. Their early Iron Age ceramic tradition belonged to the highlands facies of the eastern stream, which moved inland to Malawi and Zimbabwe. Imports of beads have been found at Gokomere and Ziwa sites, possibly in return for gold exported to the coast.

A later phase of the Gokomere culture was the Zhizo in southern Zimbabwe. Zhizo communities settled in the Shashe-Limpopo area in the tenth century. Their capital there was Schroda (just across the Limpopo River from Zimbabwe). Many fragments of ceramic figurines have been recovered from there, figures of animals and birds, and also fertility dolls. The inhabitants produced ivory bracelets and other ivory goods. Imported beads found there and at other Zhizo sites, are evidence of trade, probably of ivory and skins, with traders on the Indian Ocean coast.

Pottery belonging to a western stream of Bantu expansion (sometimes called Kalundu) has been found at sites in northeastern Zimbabwe, dated from the seventh century. (The western stream originated in the same area as the eastern stream: both belong to the same style system, called by Phillipson the Chifumbadze system, which has general acceptance by archaeologists.) The terms eastern and western streams represent the expansion of the Bantu speaking peoples in terms of their culture. Another question is the branches of the Bantu languages which they spoke. It seems that the makers of the Ziwa/Gokomere wares were not the ancestral speakers of the Shona languages of today's Zimbabwe, who did not arrive in there until around the tenth century, from south of the Limpopo river, and whose ceramic culture belonged to the western stream. The linguist and historian Ehret believes that in view of the similarity of the Ziwa/Gokomere pottery to the Nkope of the ancestral Nyasa language speakers, the Ziwa/Gokomere people spoke a language closely related to the Nyasa group. Their language, whatever it was, was superseded by the ancestral Shona languages, although Ehret says that a set of Nyasa words occur in central Shona dialects today.

The evidence that the ancestral Shona speakers came from South Africa is that the ceramic styles associated with Shona speakers in Zimbabwe from the thirteenth to the seventeenth centuries can be traced back to western stream (Kalunndu) pottery styles in South Africa. The Ziwa /Gokomere and Zhizo traditions were superseded by Leopards Kopje and Gumanye wares of the Kalundu tradition from the tenth century.

Although the western stream Kalundu tradition was ancestral to Shona ceramic wares, the closest relationships of the ancestral Shona language according to many linguists were with a southern division of eastern Bantu – such languages as the southeastern languages (Nguni, Sotho-Tswana, Tsonga), Nyasa and Makwa. While it may well be the case that the people of the western stream spoke a language belonging to a wider Eastern Bantu division, it is a puzzle which remains to be resolved that they spoke a language most closely related to the languages just mentioned, all of which are today spoken in southeastern Africa.

After the Shona speaking people moved into the present day Zimbabwe many different dialects developed over time in the different parts of the country. Among these was Kalanga.
It is believed that Kalanga speaking societies first emerged in the middle Limpopo valley in the 9th century before moving on to the Zimbabwean highlands. The Zimbabwean plateau eventually became the centre of subsequent Kalanga states. The Kingdom of Mapungubwe was the first in a series of sophisticated trade states developed in Zimbabwe by the time of the first European explorers from Portugal. They traded in gold, ivory and copper for cloth and glass. From about 1250 until 1450, Mapungubwe was eclipsed by the Kingdom of Zimbabwe. This Kalanga state further refined and expanded upon Mapungubwe's stone architecture, which survives to this day at the ruins of the kingdom's capital of Great Zimbabwe. From circa 1450–1760, Zimbabwe gave way to the Kingdom of Mutapa. This Kalanga state ruled much of the area that is known as Zimbabwe today, and parts of central Mozambique. It is known by many names including the Mutapa Empire, also known as Mwenemutapa was known for its gold trade routes with Arabs and the Portuguese. However, Portuguese settlers destroyed the trade and began a series of wars which left the empire in near collapse in the early 17th century. As a direct response to Portuguese aggression in the interior, a new Kalanga state emerged called the Rozwi Empire. Relying on centuries of military, political and religious development, the Rozwi (which means "destroyers") removed the Portuguese from the Zimbabwe plateau by force of arms. The Rozwi continued the stone building traditions of the Zimbabwe and Mapungubwe kingdoms while adding guns to its arsenal and developing a professional army to protect its trade routes and conquests. Around 1821, the Zulu general Mzilikazi of the Khumalo clan successfully rebelled from King Shaka and created his own clan, the Ndebele. The Ndebele fought their way northwards into the Transvaal, leaving a trail of destruction in their wake and beginning an era of widespread devastation known as the Mfecane. When Dutch trekboers converged on the Transvaal in 1836, they drove the tribe even further northward. By 1838, the Rozwi Empire, along with the other Shona states had been unconquered by the Ndebele.

After losing their remaining South African lands in 1840, Mzilikazi and his tribe permanently settled the southwest of present-day Zimbabwe in what became known as Matabeleland, establishing Bulawayo as their capital. Mzilikazi then organised his society into a military system with regimental kraals, similar to those of Shaka, which was stable enough to repel further Boer incursions. During the pre-colonial period, the Ndebele social structure was stratified. It was composed of mainly three social groups, Zansi, Enhla and Amahole. The Zansi comprised the ruling class the original Khumalo people who migrated from south of Limpopo with Mzilikazi. The Enhla and Amahole groups were made up of other tribes and ethnics who had been incorporated into the empire during the migration. However, with the passage of time, this stratification has slowly disappeared The Ndebele people have for long ascribed to the worship of Unkunkulu as their supreme being. Their religious life in general, rituals, ceremonies, practices, devotion and loyalty revolves around the worship of this Supreme Being. However, with the popularisation of Christianity and other religions, Ndebele traditional religion is now uncommon

Mzilikazi died in 1868 and, following a violent power struggle, was succeeded by his son, Lobengula. King Mzilikazi had established the Ndebele Kingdom, with Shona subjects paying tribute to him. This Kingdom (Ndebele Kingdom) under King Lobengula faced the threat of colonialism and King Lobengula signed the treaties of occupation with the European imperialists that saw the occupation of the present Zimbabwe state.

In the 1880s, the British arrived with Cecil Rhodes' British South Africa Company. In 1898, the name Southern Rhodesia was adopted. In 1888, British colonialist Cecil Rhodes obtained a concession for mining rights from King Lobengula of the Ndebele peoples. Cecil Rhodes presented this concession to persuade the government of the United Kingdom to grant a royal charter to his British South Africa Company (BSAC) over Matabeleland, and its subject states such as Mashonaland. Rhodes sought permission to negotiate similar concessions covering all territory between the Limpopo River and Lake Tanganyika, then known as 'Zambesia'. In accordance with the terms of aforementioned concessions and treaties, Cecil Rhodes promoted the colonisation of the region's land, with British control over labour as well as precious metals and other mineral resources. In 1895 the BSAC adopted the name 'Rhodesia' for the territory of Zambesia, in honour of Cecil Rhodes. In 1898, 'Southern Rhodesia' became the official denotation for the region south of the Zambezi, which later became Zimbabwe. The region to the north was administered separately by the BSAC and later named Northern Rhodesia (now Zambia).

The Shona staged unsuccessful revolts (known as Chimurenga) against encroachment upon their lands by clients of BSAC and Cecil Rhodes in 1896 and 1897. Following the failed insurrections of 1896–97 the Ndebele and Shona groups became subject to Rhodes's administration thus precipitating European settlement en masse which led to land distribution disproportionately favouring Europeans, displacing the Shona, Ndebele, and other indigenous peoples.

The colony's first formal constitution was drafted in 1899, and copied various pieces of legislation directly from that of the Union of South Africa; Rhodesia was meant to be, in many ways, a shadow colony of the Cape. Many within the administrative framework of the BSAC assumed that Southern Rhodesia, when its "development" was "suitably advanced", would "take its rightful place as a member of" the Union of South Africa after the Anglo-Boer War of 1902, when the four South African colonies joined under the auspices of one flag and began to work towards the creation of a unified administrative structure. The territory was made open to white settlement, and these settlers were then in turn given considerable administrative powers, including a franchise that, while on the surface non-racial, ensured "a predominantly European electorate" which "operated to preclude Great Britain from modifying her policy in Southern Rhodesia and subsequently treating it as a territory inhabited mainly by Africans whose interests should be paramount and to whom British power should be transferred".
Southern Rhodesia became a self-governing British colony in October 1923, subsequent to a referendum held the previous year. The British government took full command of the British South Africa Company's holdings, including both Northern and Southern Rhodesia. Northern Rhodesia retained its status as a colonial protectorate; Southern Rhodesia was given responsible self-government – with limitations and still annexed to the crown as a colony. Many studies of the country see it as a state that operated independently within the Commonwealth; nominally under the rule of the British crown, but technically able to do as it pleased. And in theory, Southern Rhodesia was able to govern itself, draft its own legislation, and elect its own parliamentary leaders. But in reality, this was self-government subject to supervision. Until the white minority settler government's declaration of unilateral independence in 1965, London remained in control of the colony's external affairs, and all legislation was subject to approval from the United Kingdom Government and the Queen.

In 1930, the Land Apportionment Act divided rural land along racial lines, creating four types of land: white-owned land that could not be acquired by Africans; purchase areas for those Africans who could afford to purchase land; Tribal Trust Lands designated as the African reserves; and Crown lands owned by the state, reserved for future use and public parks. Fifty one percent of the land was given to approximately 50,000 white inhabitants, with 29.8 per cent left for over a million Africans.

Many Rhodesians served on behalf of the United Kingdom during World War II, mainly in the East African Campaign against Axis forces in Italian East Africa.

In 1953, in the face of African opposition, Britain consolidated the two colonies of Rhodesia with Nyasaland (now Malawi) in the ill-fated Federation of Rhodesia and Nyasaland which was dominated by Southern Rhodesia. Growing African nationalism and general dissent, particularly in Nyasaland, persuaded the UK to dissolve the Union in 1963, forming three colonies. As colonial rule was ending throughout the continent and as African-majority governments assumed control in neighbouring Northern Rhodesia and in Nyasaland, the white-minority Rhodesian government led by Ian Smith made a Unilateral Declaration of Independence (UDI) from the United Kingdom on 11 November 1965. The United Kingdom deemed this an act of rebellion, but did not re-establish control by force. The white minority government declared itself a republic in 1970. A civil war ensued, with Joshua Nkomo's ZAPU and Robert Mugabe's ZANU using assistance from the governments of Zambia and Mozambique. Although Smith's declaration was not recognised by the United Kingdom nor any other foreign power, Southern Rhodesia dropped the designation "Southern", and claimed nation status as the Republic of Rhodesia in 1970 although this was not recognised internationally.

The country gained official independence as Zimbabwe on 18 April 1980. The government held independence celebrations in Rufaro stadium in Salisbury, the capital. Lord Christopher Soames, the last Governor of Southern Rhodesia, watched as Charles, Prince of Wales, gave a farewell salute and the Rhodesian Signal Corps played "God Save the Queen". Many foreign dignitaries also attended, including Prime Minister Indira Gandhi of India, President Shehu Shagari of Nigeria, President Kenneth Kaunda of Zambia, President Seretse Khama of Botswana, and Prime Minister Malcolm Fraser of Australia, representing the Commonwealth of Nations. Bob Marley sang 'Zimbabwe', a song he wrote, at the government's invitation in a concert at the country's independence festivities.

President Shagari pledged $15 million at the celebration to train Zimbabweans in Zimbabwe and expatriates in Nigeria. Mugabe's government used part of the money to buy newspaper companies owned by South Africans, increasing the government's control over the media. The rest went to training students in Nigerian universities, government workers in the Administrative Staff College of Nigeria in Badagry, and soldiers in the Nigerian Defence Academy in Kaduna. Later that year Mugabe commissioned a report by the BBC on press freedom in Zimbabwe. The BBC issued its report on 26 June, recommending the privatisation of the Zimbabwe Broadcasting Corporation and its independence from political interests. "See also:" Foreign relations of Zimbabwe

Mugabe's government changed the capital's name from Salisbury to Harare on 18 April 1982 in celebration of the second anniversary of independence. The government renamed the main street in the capital, Jameson Avenue, in honour of Samora Machel, President of Mozambique.

In 1992, a World Bank study indicated that more than 500 health centres had been built since 1980. The percentage of children vaccinated increased from 25% in 1980 to 67% in 1988 and life expectancy increased from 55 to 59 years. Enrolment increased by 232 per cent one year after primary education was made free and secondary school enrolment increased by 33 per cent in two years. These social policies lead to an increase in the debt ratio.Several laws were passed in the 1980s in an attempt to reduce wage gaps. However, the gaps remained considerable. In 1988, the law gave women, at least in theory, the same rights as men. Previously, they could only take a few personal initiatives without the consent of their father or husband.

The new Constitution provided for an executive President as Head of State with a Prime Minister as Head of Government. Reverend Canaan Banana served as the first President. In government amended the Constitution in 1987 to provide for an Executive President and abolished the office of Prime Minister. The constitutional changes came into effect on 1 January 1988 with Robert Mugabe as President. The bicameral Parliament of Zimbabwe had a directly elected House of Assembly and an indirectly elected Senate, partly made up of tribal chiefs. The Constitution established two separate voters rolls, one for the black majority, who had 80% of the seats in Parliament, and the other for whites and other ethnic minorities, such as Coloureds, people of mixed race, and Asians, who held 20%. The government amended the Constitution in 1986, eliminating the voter rolls and replacing the white seats with seats filled by nominated members. Many white MPs joined ZANU which then reappointed them. In 1990 the government abolished the Senate and increased the House of Assembly's membership to include members nominated by the President.

Prime Minister Mugabe kept Peter Walls, the head of the army, in his government and put him in charge of integrating the Zimbabwe People's Revolutionary Army (ZIPRA), Zimbabwe African National Liberation Army (ZANLA), and the Rhodesian Army. While Western media outlets praised Mugabe's efforts at reconciliation with the white minority, tension soon developed. On 17 March 1980, after several unsuccessful assassination attempts Mugabe asked Walls, "Why are your men trying to kill me?" Walls replied, "If they were my men you would be dead." BBC news interviewed Walls on 11 August 1980. He told the BBC that he had asked British Prime Minister Margaret Thatcher to annul the 1980 election prior to the official announcement of the result on the grounds that Mugabe used intimidation to win the election. Walls said Thatcher had not replied to his request. On 12 August British government officials denied that they had not responded, saying Antony Duff, Deputy Governor of Salisbury, told Walls on 3 March that Thatcher would not annul the election.

Minister of Information Nathan Shamuyarira said the government would not be "held ransom by racial misfits" and told "all those Europeans who do not accept the new order to pack their bags." He also said the government continued to consider taking "legal or administrative action" against Walls. Mugabe, returning from a visit with United States President Jimmy Carter in New York City, said, "One thing is quite clear—we are not going to have disloyal characters in our society." Walls returned to Zimbabwe after the interview, telling Peter Hawthorne of "Time" magazine, "To stay away at this time would have appeared like an admission of guilt." Mugabe drafted legislation that would exile Walls from Zimbabwe for life and Walls moved to South Africa.

Ethnic divisions soon came back to the forefront of national politics. Tension between ZAPU and ZANU erupted with guerrilla activity starting again in Matabeleland in south-western Zimbabwe. Nkomo (ZAPU) left for exile in Britain and did not return until Mugabe guaranteed his safety. In 1982 government security officials discovered large caches of arms and ammunition on properties owned by ZAPU, accusing Nkomo and his followers of plotting to overthrow the government. Mugabe fired Nkomo and his closest aides from the cabinet. Seven MPs, members of the Rhodesian Front, left Smith's party to sit as "independents" on 4 March 1982, signifying their dissatisfaction with his policies. As a result of what they saw as persecution of Nkomo and his party, PF-ZAPU supporters, army deserters began a campaign of dissidence against the government. Centring primarily in Matabeleland, home of the Ndebeles who were at the time PF-ZAPU's main followers, this dissidence continued through 1987. It involved attacks on government personnel and installations, armed banditry aimed at disrupting security and economic life in the rural areas, and harassment of ZANU-PF members.

Because of the unsettled security situation immediately after independence and democratic sentiments, the government kept in force a "state of emergency". This gave the government widespread powers under the "Law and Order Maintenance Act," including the right to detain persons without charge which it used quite widely. In 1983 to 1984 the government declared a curfew in areas of Matabeleland and sent in the army in an attempt to suppress members of the Ndebele tribe. The pacification campaign, known as the Gukuruhundi, or strong wind, resulted in at least 20,000 civilian deaths perpetrated by an elite, North Korean-trained brigade, known in Zimbabwe as the Gukurahundi.

ZANU-PF increased its majority in the 1985 elections, winning 67 of the 100 seats. The majority gave Mugabe the opportunity to start making changes to the constitution, including those with regard to land restoration. Fighting did not cease until Mugabe and Nkomo reached an agreement in December 1987 whereby ZAPU became part of ZANU-PF and the government changed the constitution to make Mugabe the country's first executive president and Nkomo one of two vice-presidents.

Elections in March 1990 resulted in another overwhelming victory for Mugabe and his party, which won 117 of the 120 election seats. Election observers estimated voter turnout at only 54% and found the campaign neither free nor fair, though balloting met international standards. Unsatisfied with a "de facto" one-party state, Mugabe called on the ZANU-PF Central Committee to support the creation of a "de jure" one-party state in September 1990 and lost. The government began further amending the constitution. The judiciary and human rights advocates fiercely criticised the first amendments enacted in April 1991 because they restored corporal and capital punishment and denied recourse to the courts in cases of compulsory purchase of land by the government. The general health of the civilian population also began to significantly flounder and by 1997 25% of the population of Zimbabwe had been infected by HIV, the AIDS virus.

During the 1990s students, trade unionists, and workers often demonstrated to express their discontent with the government. Students protested in 1990 against proposals for an increase in government control of universities and again in 1991 and 1992 when they clashed with police. Trade unionists and workers also criticised the government during this time. In 1992 police prevented trade unionists from holding anti-government demonstrations. In 1994 widespread industrial unrest weakened the economy. In 1996 civil servants, nurses, and junior doctors went on strike over salary issues.

On 9 December 1997 a national strike paralysed the country. Mugabe was panicked by demonstrations by Zanla ex-combatants, war veterans, who had been the heart of incursions 20 years earlier in the Bush War. He agreed to pay them large gratuities and pensions, which proved to be a wholly unproductive and unbudgeted financial commitment. The discontent with the government spawned draconian government crackdowns which in turn started to destroy both the fabric of the state and of society. This in turn brought with it further discontent within the population. Thus a vicious downward spiral commenced.

Although many whites had left Zimbabwe after independence, mainly for neighbouring South Africa, those who remained continued to wield disproportionate control of some sectors of the economy, especially agriculture. In the late-1990s whites accounted for less than 1% of the population but owned 70% of arable land. Mugabe raised this issue of land ownership by white farmers. In a calculated move, he began forcible land redistribution, which brought the government into headlong conflict with the International Monetary Fund. Amid a severe drought in the region, the police and military were instructed not to stop the invasion of white-owned farms by the so-called 'war veterans' and youth militia. This has led to a mass migration of White Zimbabweans out of Zimbabwe. At present almost no arable land is in the possession of white farmers.

The economy was run along corporatist lines with strict governmental controls on all aspects of the economy. Controls were placed on wages, prices and massive increases in government spending resulting in significant budget deficits. This experiment met with very mixed results and Zimbabwe fell further behind the first world and unemployment. Some market reforms in the 1990s were attempted. A 40 per cent devaluation of the Zimbabwean dollar was allowed to occur and price and wage controls were removed. These policies also failed at that time. Growth, employment, wages, and social service spending contracted sharply, inflation did not improve, the deficit remained well above target, and many industrial firms, notably in textiles and footwear, closed in response to increased competition and high real interest rates. The incidence of poverty in the country increased during this time.

However, Zimbabwe began experiencing a period of considerable political and economic upheaval in 1999. Opposition to President Mugabe and the ZANU-PF government grew considerably after the mid-1990s in part due to worsening economic and human rights conditions brought about by crippling economic sanctions imposed by western countries led by Britain in response to land seizures from the White minority farmers. The Movement for Democratic Change (MDC) was established in September 1999 as an opposition party founded by trade unionist Morgan Tsvangirai.

The MDC's first opportunity to test opposition to the Mugabe government came in February 2000, when a referendum was held on a draft constitution proposed by the government. Among its elements, the new constitution would have permitted President Mugabe to seek two additional terms in office, granted government officials immunity from prosecution, and authorised government seizure of white-owned land. The referendum was handily defeated. Shortly thereafter, the government, through a loosely organised group of war veterans, some of the so-called war veterans judging from their age were not war veterans as they were too young to have fought in the chimurenga, sanctioned an aggressive land redistribution program often characterised by forced expulsion of white farmers and violence against both farmers and farm employees.

Parliamentary elections held in June 2000 were marred by localised violence, and claims of electoral irregularities and government intimidation of opposition supporters. Nonetheless, the MDC succeeded in capturing 57 of 120 seats in the National Assembly.

Presidential elections were held in March 2002. In the months leading up to the poll, ZANU-PF, with the support of the army, security services, and especially the so-called 'war veterans', – very few of whom actually fought in the Second Chimurenga against the Smith regime in the 1970s – set about wholesale intimidation and suppression of the MDC-led opposition. Despite strong international criticism, these measures, together with organised subversion of the electoral process, ensured a Mugabe victory . The government's behaviour drew strong criticism from the EU and the US, which imposed limited sanctions against the leading members of the Mugabe regime. Since the 2002 election, Zimbabwe has suffered further economic difficulty and growing political chaos.

Divisions within the opposition MDC had begun to fester early in the decade, after Morgan Tsvangirai (the president of the MDC) was lured into a government sting operation that videotaped him talking of Mr. Mugabe's removal from power. He was subsequently arrested and put on trial on treason charges. This crippled his control of party affairs and raised questions about his competence. It also catalysed a major split within the party. In 2004 he was acquitted, but not until after suffering serious abuse and mistreatment in prison. The opposing faction was led by Welshman Ncube who was the general secretary of the party. In mid-2004, vigilantes loyal to Mr. Tsvangirai began attacking members who were mostly loyal to Ncube, climaxing in a September raid on the party's Harare headquarters in which the security director was nearly thrown to his death.

An internal party inquiry later established that aides to Tsvangirai had tolerated, if not endorsed, the violence. Divisive as the violence was, it was a debate over the rule of law that set off the party's final break-up in November 2005. These division severely weakened the opposition. In addition the government employed its own operatives to both spy on each side and to undermine each side via acts of espionage. Zimbabwean parliamentary election, 2005 were held in March 2005 in which ZANU-PF won a two-thirds majority, were again criticised by international observers as being flawed. Mugabe's political operatives were thus able to weaken the opposition internally and the security apparatus of the state was able to destabilise it externally by using violence in anti-Mugabe strongholds to prevent citizens from voting. Some voters were 'turned away' from polling station despite having proper identification, further guaranteeing that the government could control the results. Additionally Mugabe had started to appoint judges sympathetic to the government, making any judicial appeal futile. Mugabe was also able to appoint 30 of the members of parliament.

As Senate elections approached further opposition splits occurred. Ncube's supporters argued that the M.D.C. should field a slate of candidates; Tsvangirai's argued for a boycott. When party leaders voted on the issue, Ncube's side narrowly won, but Mr. Tsvangirai declared that as president of the party he was not bound by the majority's decision. Again the opposition was weakened. As a result, the elections for a new Senate in November 2005 were largely boycotted by the opposition. Mugabe's party won 24 of the 31 constituencies where elections were held amid low voter turnout. Again, evidence surfaced of voter intimidation and fraud. 

In May 2005 the government began Operation Murambatsvina. It was officially billed to rid urban areas of illegal structures, illegal business enterprises, and criminal activities. In practice its purpose was to punish political opponents. The UN estimates 700,000 people have been left without jobs or homes as a result. Families and traders, especially at the beginning of the operation, were often given no notice before police destroyed their homes and businesses. Others were able to salvage some possessions and building materials but often had nowhere to go, despite the government's statement that people should be returning to their rural homes. Thousands of families were left unprotected in the open in the middle of Zimbabwe's winter., . The government interfered with non-governmental organisation (NGO) efforts to provide emergency assistance to the displaced in many instances. Some families were removed to transit camps, where they had no shelter or cooking facilities and minimal food, supplies, and sanitary facilities. The operation continued into July 2005, when the government began a program to provide housing for the newly displaced.

Human Rights Watch said the evictions had disrupted treatment for people with HIV/AIDS in a country where 3,000 die from the disease each week and about 1.3 million children have been orphaned. The operation was "the latest manifestation of a massive human rights problem that has been going on for years", said Amnesty International. As of September 2006, housing construction fell far short of demand, and there were reports that beneficiaries were mostly civil servants and ruling party loyalists, not those displaced. The government campaign of forced evictions continued in 2006, albeit on a lesser scale.

In September 2005 Mugabe signed constitutional amendments that reinstituted a national senate (abolished in 1987) and that nationalised all land. This converted all ownership rights into leases. The amendments also ended the right of landowners to challenge government expropriation of land in the courts and marked the end of any hope of returning any land that had been hitherto grabbed by armed land invasions. Elections for the senate in November resulted in a victory for the government. The MDC split over whether to field candidates and partially boycotted the vote. In addition to low turnout there was widespread government intimidation. The split in the MDC hardened into factions, each of which claimed control of the party. The early months of 2006 were marked by food shortages and mass hunger. The sheer extremity of the siltation was revealed by the fact that in the courts, state witnesses said they were too weak from hunger to testify.

In August 2006 runaway inflation forced the government to replace its existing currency with a revalued one. In December 2006, ZANU-PF proposed the "harmonisation" of the parliamentary and presidential election schedules in 2010; the move was seen by the opposition as an excuse to extend Mugabe's term as president until 2010.

Morgan Tsvangirai was badly beaten on 12 March 2007 after being arrested and held at Machipisa Police Station in the Highfield suburb of Harare. The event garnered an international outcry and was considered particularly brutal and extreme, even considering the reputation of Mugabe's government. Kolawole Olaniyan, Director of Amnesty International's Africa Programme said "We are very concerned by reports of continuing brutal attacks on opposition activists in Zimbabwe and call on the government to stop all acts of violence and intimidation against opposition activists".

The economy has shrunk by 50% from 2000 to 2007. In September 2007 the inflation rate was put at almost 8,000%, the world's highest. There are frequent power and water outages. Harare's drinking water became unreliable in 2006 and as a consequence dysentery and cholera swept the city in December 2006 and January 2007. Unemployment in formal jobs is running at a record 80%. There was widespread hunger, manipulated by the government so that opposition strongholds suffer the most. Availability of bread was severely constrained after a poor wheat harvest and the closure of all bakeries.

The country, which used to be one of Africa's richest, became one of its poorest. Many observers now view the country as a 'failed state'. The settlement of the Second Congo War brought back Zimbabwe's substantial military commitment, although some troops remain to secure the mining assets under their control. The government lacks the resources or machinery to deal with the ravages of the HIV/AIDS pandemic, which affects 25% of the population. With all this and the forced and violent removal of white farmers in a brutal land redistribution program, Mugabe has earned himself widespread scorn from the international arena.

The regime has managed to cling to power by creating wealthy enclaves for government ministers, and senior party members. For example, Borrowdale Brook, a suburb of Harare is an oasis of wealth and privilege. It features mansions, manicured lawns, full shops with fully stocked shelves containing an abundance of fruit and vegetables, big cars and a golf club give is the home to President Mugabe's out-of-town retreat.

Zimbabwe's bakeries shut down in October 2007 and supermarkets warned that they would have no bread for the foreseeable future due to collapse in wheat production after the seizure of white-owned farms. The ministry of agriculture has also blamed power shortages for the wheat shortfall, saying that electricity cuts have affected irrigation and halved crop yields per acre. The power shortages are because Zimbabwe relies on Mozambique for some of its electricity and that due to an unpaid bill of $35 million Mozambique had reduced the amount of electrical power it supplies. On 4 December 2007, The United States imposed travel sanctions against 38 people with ties to President Mugabe because they "played a central role in the regime's escalated human rights abuses."

On 8 December 2007, Mugabe attended a meeting of EU and African leaders in Lisbon, prompting UK Prime Minister Gordon Brown to decline to attend. While German chancellor Angela Merkel criticised Mugabe with her public comments, the leaders of other African countries offered him statements of support.

The educational system in Zimbabwe which was once regarded as among the best in Africa, went into crisis in 2007 because of the country's economic meltdown. One foreign reporter witnessed hundreds of children at Hatcliffe Extension Primary School in Epworth, west of Harare, writing in the dust on the floor because they had no exercise books or pencils. The high school exam system unravelled in 2007. Examiners refused to mark examination papers when they were offered just Z$79 a paper, enough to buy three small candies. Corruption has crept into the system and may explain why in January 2007 thousands of pupils received no marks for subjects they had entered, while others were deemed "excellent" in subjects they had not sat. However, as of late the education system has recovered and is still considered the best in Southern Africa.

Zimbabwe held a presidential election along with a 2008 parliamentary election of 29 March. The three major candidates were incumbent President Robert Mugabe of the Zimbabwe African National Union – Patriotic Front (ZANU-PF), Morgan Tsvangirai of the Movement for Democratic Change – Tsvangirai (MDC-T), and Simba Makoni, an independent. As no candidate received an outright majority in the first round, a second round was held on 27 June 2008 between Tsvangirai (with 47.9% of the first round vote) and Mugabe (43.2%). Tsvangirai withdrew from the second round a week before it was scheduled to take place, citing violence against his party's supporters. The second round went ahead, despite widespread criticism, and led to victory for Mugabe.

Because of Zimbabwe's dire economic situation the election was expected to provide President Mugabe with his toughest electoral challenge to date. Mugabe's opponents were critical of the handling of the electoral process, and the government was accused of planning to rig the election; Human Rights Watch said that the election was likely to be "deeply flawed". After the first round, but before the counting was completed, Jose Marcos Barrica, the head of the Southern African Development Community observer mission, described the election as "a peaceful and credible expression of the will of the people of Zimbabwe."

No official results were announced for more than a month after the first round. The failure to release results was strongly criticised by the MDC, which unsuccessfully sought an order from the High Court to force their release. An independent projection placed Tsvangirai in the lead, but without the majority needed to avoid a second round. The MDC declared that Tsvangirai won a narrow majority in the first round and initially refused to participate in any second round. ZANU-PF has said that Mugabe will participate in a second round; the party alleged that some electoral officials, in connection with the MDC, fraudulently reduced Mugabe's score, and as a result a recount was conducted.

After the recount and the verification of the results, the Zimbabwe Electoral Commission (ZEC) announced on 2 May that Tsvangirai won 47.9% and Mugabe won 43.2%, thereby necessitating a run-off, which was to be held on 27 June 2008. Despite Tsvangirai's continuing claims to have won a first round majority, he refused to participate in the second round. The period following the first round was marked by serious political violence caused by ZANU-PF. ZANU-PF blamed the MDC supporters for perpetrating this violence; Western governments and prominent Western organisations have blamed ZANU-PF for the violence which seems very likely to be true. On 22 June 2008, Tsvangirai announced that he was withdrawing from the run-off, describing it as a "violent sham" and saying that his supporters risked being killed if they voted for him. The second round nevertheless went ahead as planned with Mugabe as the only actively participating candidate, although Tsvangirai's name remained on the ballot. Mugabe won the second round by an overwhelming margin and was sworn in for another term as President on 29 June.

The international reaction to the second round have varied. The United States and states of the European Union have called for increased sanctions. On 11 July, the United Nations Security Council voted to impose sanctions on the Zimbabwe; Russia and China vetoed. The African Union has called for a "government of national unity."

Preliminary talks to set up conditions for official negotiations began between leading negotiators from both parties on 10 July, and on 22 July, the three party leaders met for the first time in Harare to express their support for a negotiated settlement of disputes arising out of the presidential and parliamentary elections. Negotiations between the parties officially began on 25 July and are currently proceeding with very few details released from the negotiation teams in Pretoria, as coverage by the media is barred from the premises where the negotiations are taking place. The talks were mediated by South African President Thabo Mbeki.

On 15 September 2008, the leaders of the 14-member Southern African Development Community witnessed the signing of the power-sharing agreement, brokered by South African leader Thabo Mbeki. With symbolic handshake and warm smiles at the Rainbow Towers hotel, in Harare, Mugabe and Tsvangirai signed the deal to end the violent political crisis. As provided, Robert Mugabe will remain president, Morgan Tsvangirai will become prime minister, ZANU-PF and the MDC will share control of the police, Mugabe's Zanu (PF) will command the Army, and Arthur Mutambara becomes deputy prime minister.

In November 2008 the Air Force of Zimbabwe was sent, after some police officers began refusing orders to shoot the illegal miners at Marange diamond fields. Up to 150 of the estimated 30,000 illegal miners were shot from helicopter gunships. In 2008 some Zimbabwean lawyers and opposition politicians from Mutare claimed that Shiri was the prime mover behind the military assaults on illegal diggers in the diamond mines in the east of Zimbabwe. Estimates of the death toll by mid-December range from 83 reported by the Mutare City Council, based on a request for burial ground, to 140 estimated by the (then) opposition Movement for Democratic Change - Tsvangirai party.

In January 2009, Morgan Tsvangirai announced that he would do as the leaders across Africa had insisted and join a coalition government as prime minister with his nemesis, President Robert Mugabe . On 11 February 2009 Tsvangirai was sworn in as the Prime Minister of Zimbabwe. By 2009 inflation had peaked at 500 billion % per year under the Mugabe government and the Zimbabwe currency was worthless. The opposition shared power with the Mugabe regime between 2009 and 2013, Zimbabwe switched to using the US dollar as currency and the economy improved reaching a growth rate of 10% per year.

In 2013 the Mugabe government won an election which The Economist described as "rigged," doubled the size of the civil service and embarked on "...misrule and dazzling corruption." However, the United Nations, African Union and SADC endorsed the elections as free and fair.

By 2016 the economy had collapsed, nationwide protests took place throughout the country and the finance minister admitted "Right now we literally have nothing."
There was the introduction of bond notes to literally fight the biting cash crisis and liquidity crunch. Cash became scarce on the market in the year 2017.

On Wednesday 15 November 2017 the military placed President Mugabe under house arrest and removed him from power. The military stated that the president was safe. The military placed tanks around government buildings in Harare and blocked the main road to the airport. Public opinion in the capital favored the dictators removal although they were uncertain about his replacement with another dictatorship. The Times reported that Emmerson Mnangagwa helped to orchestrate the coup. He had recently been sacked by Mr Mugabe so that the path could be smoothed for Grace Mugabe to replace her husband. A Zimbabwean army officer, Major General Sibusiso Moyo, went on television to say the military was targeting "criminals" around President Mugabe but not actively removing the president from power. However the head of the African Union described it as such.

Ugandan writer Charles Onyango-Obbo stated on Twitter "If it looks like a coup, walks like a coup and quacks like a coup, then it's a coup". Naunihal Singh, an assistant professor at the U.S. Naval War College and author of a book on military coups, described the situation in Zimbabwe as a coup. He tweeted that "'The President is safe' is a classic coup catch-phrase" of such an event.

Robert Mugabe resigned 21 November 2017. Second Vice-President Phelekezela Mphoko became the Acting President. Emmerson Mnangagwa was sworn in as President on 24 November 2017.

General elections were held on 30 July 2018 to elect the president and members of both houses of parliament. Ruling party ZANU-PF won the majority of seats in parliament, incumbent President Emmerson Mnangagwa was declared the winner after receiving 50.8% of votes. The opposition accused the government of rigging the vote. In subsequent riots by MDC supporters, the army opened fire and killed three people, while three others died of their injuries the following day.
In January 2019 following a 130% increase in the price of fuel thousands of Zimbabweans protested and the government responded with a coordinated crackdown that resulted in hundreds of arrests and multiple deaths.





</doc>
<doc id="14115" url="https://en.wikipedia.org/wiki?curid=14115" title="History of Russia">
History of Russia

The history of Russia begins with the histories of the East Slavs. The traditional start-date of specifically Russian history is the establishment of the Rus' state in the north in 862 ruled by Vikings. Staraya Ladoga and Novgorod became the first major cities of the new union of immigrants from Scandinavia with the Slavs and Finno-Ugrians. In 882 Prince Oleg of Novgorod seized Kiev, thereby uniting the northern and southern lands of the Eastern Slavs under one authority. The state adopted Christianity from the Byzantine Empire in 988, beginning the synthesis of Byzantine and Slavic cultures that defined Orthodox Slavic culture for the next millennium. Kievan Rus' ultimately disintegrated as a state due to the Mongol invasions in 1237–1240 along with the resulting deaths of about half the population of Rus'.

After the 13th century, Moscow became a cultural center. The territories of the Grand Duchy of Moscow became the Tsardom of Russia in 1547. In 1721 Tsar Peter the Great renamed his state as the Russian Empire, hoping to associate it with historical and cultural achievements of ancient Rus' – in contrast to his policies oriented towards Western Europe. The state now extended from the eastern borders of the Polish–Lithuanian Commonwealth to the Pacific Ocean. Peasant revolts were common, and all were fiercely suppressed. The Emperor Alexander II abolished Russian serfdom in 1861, but the peasants fared poorly and revolutionary pressures grew. In the following decades, reform efforts such as the Stolypin reforms of 1906–1914, the constitution of 1906, and the State Duma (1906–1917) attempted to open and liberalize the economy and political system, but the Emperors refused to relinquish autocratic rule and resisted sharing their power.

A combination of economic breakdown, war-weariness, and discontent with the autocratic system of government triggered revolution in Russia in 1917. The overthrow of the monarchy initially brought into office a coalition of liberals and moderate socialists, but their failed policies led to seizure of power by the communist Bolsheviks on 25 October 1917 (7 November New Style). Between 1922 and 1991 the history of Russia became essentially the history of the Soviet Union, effectively an ideologically-based state roughly conterminous with the Russian Empire before the 1918 Treaty of Brest-Litovsk. The approach to the building of socialism, however, varied over different periods in Soviet history: from the mixed economy and diverse society and culture of the 1920s through the command economy and repressions of the Joseph Stalin era to the "era of stagnation" from the 1960s to the 1980s. From its first years, government in the Soviet Union-based itself on the one-party rule of the Communists, as the Bolsheviks called themselves, beginning in March 1918.

By the mid-1980s, with the weaknesses of Soviet economic and political structures becoming acute, Mikhail Gorbachev embarked on major reforms, which eventually led to the overthrow of the communist party and the breakup of the USSR, leaving Russia again on its own and marking the start of the history of post-Soviet Russia. The Russian Federation came into being in January 1992 as the legal successor to the USSR. Russia retained its nuclear arsenal but lost its superpower status. Scrapping the socialist central planning and state-ownership of property of the socialist era, new leaders, led by President Vladimir Putin (who first became President in 2000), took political and economic power after 2000 and engaged in an energetic foreign policy. Russia's 2014 annexation of the Crimean peninsula has led to economic sanctions imposed by the United States and the European Union.

In 2006, 1.5-million-year-old Oldowan flint tools were discovered in the Dagestan Akusha region of the north Caucasus, demonstrating the presence of early humans in Russia from a very early time. The discovery of some of the earliest evidence for the presence of anatomically modern humans found anywhere in Europe was reported in 2007 from the deepest levels of the Kostenki archaeological site near the Don River in Russia, which has been dated to at least 40,000 years ago. Arctic Russia was reached by 40,000 years ago. That Russia was also home to some of the last surviving Neanderthals was revealed by the discovery of the partial skeleton of a Neanderthal infant in Mezmaiskaya cave in Adygea, which was carbon dated to only 29,000 years ago. In 2008, Russian archaeologists from the Institute of Archaeology and Ethnology of Novosibirsk, working at the site of Denisova Cave in the Altai Mountains of Siberia, uncovered a 40,000-year-old small bone fragment from the fifth finger of a juvenile hominin, which DNA analysis revealed to be a previously unknown species of human, which was named the Denisova hominin.

During the prehistoric eras the vast steppes of Southern Russia were home to tribes of nomadic pastoralists. In classical antiquity, the Pontic Steppe was known as Scythia. Remnants of these long gone steppe cultures were discovered in the course of the 20th century in such places as Ipatovo, Sintashta, Arkaim, and Pazyryk.

In the later part of the 8th century BCE, Greek merchants brought classical civilization to the trade emporiums in Tanais and Phanagoria. Gelonus was described by Herodotus as a huge (Europe's biggest) earth- and wood-fortified grad inhabited around 500 BC by Heloni and Budini. The Bosporan Kingdom was incorporated as part of the Roman province of Moesia Inferior from 63 to 68 AD, under Emperor Nero. At about the 2nd century AD Goths migrated to the Black Sea, and in the 3rd and 4th centuries AD, a semi-legendary Gothic kingdom of Oium existed in Southern Russia until it was overrun by Huns. Between the 3rd and 6th centuries AD, the Bosporan Kingdom, a Hellenistic polity which succeeded the Greek colonies, was also overwhelmed by successive waves of nomadic invasions, led by warlike tribes which would often move on to Europe, as was the case with the Huns and Turkish Avars.

A Turkic people, the Khazars, ruled the lower Volga basin steppes between the Caspian and Black Seas through to the 8th century. Noted for their laws, tolerance, and cosmopolitanism, the Khazars were the main commercial link between the Baltic and the Muslim Abbasid empire centered in Baghdad. They were important allies of the Byzantine Empire, and waged a series of successful wars against the Arab Caliphates. In the 8th century, the Khazars embraced Judaism.

Some of the ancestors of the modern Russians were the Slavic tribes, whose original home is thought by some scholars to have been the wooded areas of the Pripet Marshes. The Early East Slavs gradually settled Western Russia in two waves: one moving from Kiev towards present-day Suzdal and Murom and another from Polotsk towards Novgorod and Rostov.

From the 7th century onwards, East Slavs constituted the bulk of the population in Western Russia and slowly but peacefully assimilated the native Finno-Ugric tribes, such as the Merya, the Muromians, and the Meshchera.

Scandinavian Norsemen, known as Vikings in Western Europe and Varangians in the East, combined piracy and trade throughout Northern Europe. In the mid-9th century, they began to venture along the waterways from the eastern Baltic to the Black and Caspian Seas. According to the earliest Russian chronicle, a Varangian named Rurik was elected ruler ("knyaz") of Novgorod in about 860, before his successors moved south and extended their authority to Kiev, which had been previously dominated by the Khazars. Oleg, Rurik's son Igor and Igor's son Sviatoslav subsequently subdued all local East Slavic tribes to Kievan rule, destroyed the Khazar khaganate and launched several military expeditions to Byzantium and Persia.

Thus, the first East Slavic state, Rus', emerged in the 9th century along the Dnieper River valley. A coordinated group of princely states with a common interest in maintaining trade along the river routes, Kievan Rus' controlled the trade route for furs, wax, and slaves between Scandinavia and the Byzantine Empire along the Volkhov and Dnieper Rivers.

By the end of the 10th century, the minority Norse military aristocracy had merged with the native Slavic population, which also absorbed Greek Christian influences in the course of the multiple campaigns to loot Tsargrad, or Constantinople. One such campaign claimed the life of the foremost Slavic druzhina leader, Svyatoslav I, who was renowned for having crushed the power of the Khazars on the Volga. At the time, the Byzantine Empire was experiencing a major military and cultural revival; despite its later decline, its culture would have a continuous influence on the development of Russia in its formative centuries.
Kievan Rus' is important for its introduction of a Slavic variant of the Eastern Orthodox religion, dramatically deepening a synthesis of Byzantine and Slavic cultures that defined Russian culture for the next thousand years. The region adopted Christianity in 988 by the official act of public baptism of Kiev inhabitants by Prince Vladimir I, who followed the private conversion of his grandmother. Some years later the first code of laws, Russkaya Pravda, was introduced by Yaroslav the Wise. From the onset, the Kievan princes followed the Byzantine example and kept the Church dependent on them, even for its revenues, so that the Russian Church and state were always closely linked.

By the 11th century, particularly during the reign of Yaroslav the Wise, Kievan Rus' displayed an economy and achievements in architecture and literature superior to those that then existed in the western part of the continent. Compared with the languages of European Christendom, the Russian language was little influenced by the Greek and Latin of early Christian writings. This was because Church Slavonic was used directly in liturgy instead.

A nomadic Turkic people, the Kipchaks (also known as the Cumans), replaced the earlier Pechenegs as the dominant force in the south steppe regions neighbouring to Rus' at the end of the 11th century and founded a nomadic state in the steppes along the Black Sea (Desht-e-Kipchak). Repelling their regular attacks, especially in Kiev, which was just one day's ride from the steppe, was a heavy burden for the southern areas of Rus'. The nomadic incursions caused a massive influx of Slavs to the safer, heavily forested regions of the north, particularly to the area known as Zalesye.

Kievan Rus' ultimately disintegrated as a state because of in-fighting between members of the princely family that ruled it collectively. Kiev's dominance waned, to the benefit of Vladimir-Suzdal in the north-east, Novgorod in the north, and Halych-Volhynia in the south-west. Conquest by the Mongol Golden Horde in the 13th century was the final blow. Kiev was destroyed. Halych-Volhynia would eventually be absorbed into the Polish–Lithuanian Commonwealth, while the Mongol-dominated Vladimir-Suzdal and independent Novgorod Republic, two regions on the periphery of Kiev, would establish the basis for the modern Russian nation.

The invading Mongols accelerated the fragmentation of the Rus'. In 1223, the disunited southern princes faced a Mongol raiding party at the Kalka River and were soundly defeated. In 1237–1238 the Mongols burnt down the city of Vladimir (4 February 1238) and other major cities of northeast Russia, routed the Russians at the Sit' River, and then moved west into Poland and Hungary. By then they had conquered most of the Russian principalities. Only the Novgorod Republic escaped occupation and continued to flourish in the orbit of the Hanseatic League.

The impact of the Mongol invasion on the territories of Kievan Rus' was uneven. The advanced city culture was almost completely destroyed. As older centers such as Kiev and Vladimir never recovered from the devastation of the initial attack, the new cities of Moscow, Tver and Nizhny Novgorod began to compete for hegemony in the Mongol-dominated Russia. Although a Russian army defeated the Golden Horde at Kulikovo in 1380, Mongol domination of the Russian-inhabited territories, along with demands of tribute from Russian princes, continued until about 1480.

The Mongols held Russia and Volga Bulgaria in sway from their western capital at Sarai, one of the largest cities of the medieval world. The princes of southern and eastern Russia had to pay tribute to the Mongols of the Golden Horde, commonly called Tatars; but in return they received charters authorizing them to act as deputies to the khans. In general, the princes were allowed considerable freedom to rule as they wished, while the Russian Orthodox Church even experienced a spiritual revival under the guidance of Metropolitan Alexis and Sergius of Radonezh.

The Mongols left their impact on the Russians in such areas as military tactics and transportation. Under Mongol occupation, Russia also developed its postal road network, census, fiscal system, and military organization.

At the same time, Prince of Novgorod, Alexander Nevsky, managed to repel the offensive of the Northern Crusades against Russia from the West. Despite this, becoming the Grand prince, Alexander declared himself a vassal to the Golden Horde, not having the strength to resist its power.

Daniil Aleksandrovich, the youngest son of Alexander Nevsky, founded the principality of Moscow (known as Muscovy in English), which first cooperated with and ultimately expelled the Tatars from Russia. Well-situated in the central river system of Russia and surrounded by protective forests and marshes, Moscow was at first only a vassal of Vladimir, but soon it absorbed its parent state.

A major factor in the ascendancy of Moscow was the cooperation of its rulers with the Mongol overlords, who granted them the title of Grand Prince of Moscow and made them agents for collecting the Tatar tribute from the Russian principalities. The principality's prestige was further enhanced when it became the center of the Russian Orthodox Church. Its head, the Metropolitan, fled from Kiev to Vladimir in 1299 and a few years later established the permanent headquarters of the Church in Moscow under the original title of Kiev Metropolitan.

By the middle of the 14th century, the power of the Mongols was declining, and the Grand Princes felt able to openly oppose the Mongol yoke. In 1380, at Battle of Kulikovo on the Don River, the Mongols were defeated, and although this hard-fought victory did not end Tatar rule of Russia, it did bring great fame to the Grand Prince Dmitry Donskoy. Moscow's leadership in Russia was now firmly based and by the middle of the 14th century its territory had greatly expanded through purchase, war, and marriage.

In the 15th century, the grand princes of Moscow continued to consolidate Russian land to increase their population and wealth. The most successful practitioner of this process was Ivan III, who laid the foundations for a Russian national state. Ivan competed with his powerful northwestern rival, the Grand Duchy of Lithuania, for control over some of the semi-independent Upper Principalities in the upper Dnieper and Oka River basins.

Through the defections of some princes, border skirmishes, and a long war with the Novgorod Republic, Ivan III was able to annex Novgorod and Tver. As a result, the Grand Duchy of Moscow tripled in size under his rule. During his conflict with Pskov, a monk named Filofei (Philotheus of Pskov) composed a letter to Ivan III, with the prophecy that the latter's kingdom would be the Third Rome. The Fall of Constantinople and the death of the last Greek Orthodox Christian emperor contributed to this new idea of Moscow as "New Rome" and the seat of Orthodox Christianity, as did Ivan's 1472 marriage to Byzantine Princess Sophia Palaiologina.

Under Ivan III, the first central government bodies were created in Russia - Prikaz. The Sudebnik was adopted, the first set of laws since the 11th century. The double-headed eagle was adopted as the coat of arms of Russia, as a symbol of the continuity of the power of Byzantium by Russia.

A contemporary of the Tudors and other "new monarchs" in Western Europe, Ivan proclaimed his absolute sovereignty over all Russian princes and nobles. Refusing further tribute to the Tatars, Ivan initiated a series of attacks that opened the way for the complete defeat of the declining Golden Horde, now divided into several Khanates and hordes. Ivan and his successors sought to protect the southern boundaries of their domain against attacks of the Crimean Tatars and other hordes. To achieve this aim, they sponsored the construction of the Great Abatis Belt and granted manors to nobles, who were obliged to serve in the military. The manor system provided a basis for an emerging cavalry based army.

In this way, internal consolidation accompanied outward expansion of the state. By the 16th century, the rulers of Moscow considered the entire Russian territory their collective property. Various semi-independent princes still claimed specific territories, but Ivan III forced the lesser princes to acknowledge the grand prince of Moscow and his descendants as unquestioned rulers with control over military, judicial, and foreign affairs. Gradually, the Russian ruler emerged as a powerful, autocratic ruler, a tsar. The first Russian ruler to officially crown himself "Tsar" was Ivan IV.

Ivan III tripled the territory of his state, ended the dominance of the Golden Horde over the Rus', renovated the Moscow Kremlin, and laid the foundations of the Russian state. Biographer Fennell concludes that his reign was "militarily glorious and economically sound," and especially points to his territorial annexations and his centralized control over local rulers. However, Fennell, the leading British specialist on Ivan III, argues that his reign was also "a period of cultural depression and spiritual barrenness. Freedom was stamped out within the Russian lands. By his bigoted anti-Catholicism Ivan brought down the curtain between Russia and the west. For the sake of territorial aggrandizement he deprived his country of the fruits of Western learning and civilization."

The development of the Tsar's autocratic powers reached a peak during the reign of Ivan IV (1547–1584), known as "Ivan the Terrible". He strengthened the position of the monarch to an unprecedented degree, as he ruthlessly subordinated the nobles to his will, exiling or executing many on the slightest provocation. Nevertheless, Ivan is often seen as a farsighted statesman who reformed Russia as he promulgated a new code of laws (Sudebnik of 1550), established the first Russian feudal representative body (Zemsky Sobor), curbed the influence of the clergy, and introduced local self-management in rural regions. Tsar also created the first regular army in Russia - Streltsy.

Although his long Livonian War for control of the Baltic coast and access to the sea trade ultimately proved a costly failure, Ivan managed to annex the Khanates of Kazan, Astrakhan, and Siberia. These conquests complicated the migration of aggressive nomadic hordes from Asia to Europe via the Volga and Urals. Through these conquests, Russia acquired a significant Muslim Tatar population and emerged as a multiethnic and multiconfessional state. Also around this period, the mercantile Stroganov family established a firm foothold in the Urals and recruited Russian Cossacks to colonise Siberia.

In the later part of his reign, Ivan divided his realm in two. In the zone known as the "oprichnina", Ivan's followers carried out a series of bloody purges of the feudal aristocracy (whom he suspected of treachery after the betrayal of prince Kurbsky), culminating in the Massacre of Novgorod in 1570. This combined with the military losses, epidemics, and poor harvests so weakened Russia that the Crimean Tatars were able to sack central Russian regions and burn down Moscow in 1571 . Despite this, the next year the Russians defeated the Crimean Tatar army at the Battle of Molodi. In 1572 Ivan abandoned the "oprichnina".

At the end of Ivan IV's reign the Polish–Lithuanian and Swedish armies carried out a powerful intervention in Russia, devastating its northern and northwest regions.

The death of Ivan's childless son Feodor was followed by a period of civil wars and foreign intervention known as the "Time of Troubles" (1606–13). Extremely cold summers (1601–1603) wrecked crops, which led to the Russian famine of 1601–1603 and increased the social disorganization. Boris Godunov's (Борис Годунов) reign ended in chaos, civil war combined with foreign intrusion, devastation of many cities and depopulation of the rural regions. The country rocked by internal chaos also attracted several waves of interventions by the Polish–Lithuanian Commonwealth.

During the Polish–Muscovite War (1605–1618), Polish–Lithuanian forces reached Moscow and installed the impostor False Dmitriy I in 1605, then supported False Dmitry II in 1607. The decisive moment came when a combined Russian-Swedish army was routed by the Polish forces under hetman Stanisław Żółkiewski at the Battle of Klushino on . As the result of the battle, the Seven Boyars, a group of Russian nobles, deposed the tsar Vasily Shuysky on , and recognized the Polish prince Władysław IV Vasa as the Tsar of Russia on . The Poles entered Moscow on . Moscow revolted but riots there were brutally suppressed and the city was set on fire.

The crisis provoked a patriotic national uprising against the invasion, both in 1611 and 1612. Finally, a volunteer army, led by the merchant Kuzma Minin and prince Dmitry Pozharsky, expelled the foreign forces from the capital on .

The Russian statehood survived the "Time of Troubles" and the rule of weak or corrupt Tsars because of the strength of the government's central bureaucracy. Government functionaries continued to serve, regardless of the ruler's legitimacy or the faction controlling the throne. However, the "Time of Troubles" provoked by the dynastic crisis resulted in the loss of much territory to the Polish–Lithuanian Commonwealth in the Russo-Polish war, as well as to the Swedish Empire in the Ingrian War.

In February 1613, with the chaos ended and the Poles expelled from Moscow, a national assembly, composed of representatives from fifty cities and even some peasants, elected Michael Romanov, the young son of Patriarch Filaret, to the throne. The Romanov dynasty ruled Russia until 1917.

The immediate task of the new dynasty was to restore peace. Fortunately for Moscow, its major enemies, the Polish–Lithuanian Commonwealth and Sweden, were engaged in a bitter conflict with each other, which provided Russia the opportunity to make peace with Sweden in 1617 and to sign a truce with the Polish–Lithuanian Commonwealth in 1619.

Recovery of lost territories began in the mid-17th century, when the Khmelnitsky Uprising (1648–57) in Ukraine against Polish rule brought about the Treaty of Pereyaslav, concluded between Russia and the Ukrainian Cossacks. According to the treaty, Russia granted protection to the Cossacks state in Left-bank Ukraine, formerly under Polish control. This triggered a prolonged Russo-Polish War (1654-1667), which ended with the Treaty of Andrusovo, where Poland accepted the loss of Left-bank Ukraine, Kiev and Smolensk.

The Russian conquest of Siberia, begun at the end of the 16th century, continued in the 17th century. By the end of the 1640s, the Russians reached the Pacific Ocean, the Russian explorer Semyon Dezhnev, opened the strait between Asia and America. Russian expansion in the Far East faced resistance from Qing China. After the war between Russia and China, the Treaty of Nerchinsk was signed, delimiting the territories in the Amur region.

Rather than risk their estates in more civil war, the boyars cooperated with the first Romanovs, enabling them to finish the work of bureaucratic centralization. Thus, the state required service from both the old and the new nobility, primarily in the military. In return, the tsars allowed the boyars to complete the process of enserfing the peasants.

In the preceding century, the state had gradually curtailed peasants' rights to move from one landlord to another. With the state now fully sanctioning serfdom, runaway peasants became state fugitives, and the power of the landlords over the peasants "attached" to their land had become almost complete. Together the state and the nobles placed an overwhelming burden of taxation on the peasants, whose rate was 100 times greater in the mid-17th century than it had been a century earlier. In addition, middle-class urban tradesmen and craftsmen were assessed taxes, and, like the serfs, they were forbidden to change residence. All segments of the population were subject to military levy and to special taxes.

Riots amongst peasants and citizens of Moscow at this time were endemic, and included the Salt Riot (1648), Copper Riot (1662), and the Moscow Uprising (1682). By far the greatest peasant uprising in 17th-century Europe erupted in 1667. As the free settlers of South Russia, the Cossacks, reacted against the growing centralization of the state, serfs escaped from their landlords and joined the rebels. The Cossack leader Stenka Razin led his followers up the Volga River, inciting peasant uprisings and replacing local governments with Cossack rule. The tsar's army finally crushed his forces in 1670; a year later Stenka was captured and beheaded. Yet, less than half a century later, the strains of military expeditions produced another revolt in Astrakhan, ultimately subdued.

Much of Russia's expansion occurred in the 17th century, culminating in the first Russian colonisation of the Pacific in the mid-17th century, the Russo-Polish War (1654–67) that incorporated left-bank Ukraine, and the Russian conquest of Siberia. Poland was divided in the 1790–1815 era, with much of the land and population going to Russia. Most of the 19th century growth came from adding territory in Asia, south of Siberia.

Peter the Great (1672–1725) brought centralized autocracy into Russia and played a major role in bringing his country into the European state system. Russia had now become the largest country in the world, stretching from the Baltic Sea to the Pacific Ocean. The vast majority of the land was unoccupied, and travel was slow. Much of its expansion had taken place in the 17th century, culminating in the first Russian settlement of the Pacific in the mid-17th century, the reconquest of Kiev, and the pacification of the Siberian tribes. However, a population of only 14 million was stretched across this vast landscape. With a short growing season grain yields trailed behind those in the West and potato farming was not yet widespread. As a result, the great majority of the population workforce was occupied with agriculture. Russia remained isolated from the sea trade and its internal trade, communication and manufacturing were seasonally dependent.
Peter reformed the Russian army and created the Russian navy. Peter's first military efforts were directed against the Ottoman Turks. His aim was to establish a Russian foothold on the Black Sea by taking the town of Azov. His attention then turned to the north. Peter still lacked a secure northern seaport except at Archangel on the White Sea, whose harbor was frozen nine months a year. Access to the Baltic was blocked by Sweden, whose territory enclosed it on three sides. Peter's ambitions for a "window to the sea" led him in 1699 to make a secret alliance with the Polish–Lithuanian Commonwealth and Denmark against Sweden resulting in the Great Northern War.

The war ended in 1721 when an exhausted Sweden sued for peace with Russia. Peter acquired four provinces situated south and east of the Gulf of Finland, thus securing his coveted access to the sea. There, in 1703, he had already founded the city that was to become Russia's new capital, Saint Petersburg, as a "window opened upon Europe" to replace Moscow, long Russia's cultural center. Russian intervention in the Commonwealth marked, with the Silent Sejm, the beginning of a 200-year domination of that region by the Russian Empire. In celebration of his conquests, Peter assumed the title of emperor, and the Russian Tsardom officially became the Russian Empire in 1721.

Peter reorganized his government based on the latest Western models, molding Russia into an absolutist state. He replaced the old "boyar" Duma (council of nobles) with a nine-member senate, in effect a supreme council of state. The countryside was also divided into new provinces and districts. Peter told the senate that its mission was to collect tax revenues. In turn tax revenues tripled over the course of his reign.

Administrative Collegia (ministries) were established in St. Petersburg, to replace the old governmental departments. In 1722 Peter promulgated his famous Table of ranks. As part of the government reform, the Orthodox Church was partially incorporated into the country's administrative structure, in effect making it a tool of the state. Peter abolished the patriarchate and replaced it with a collective body, the Holy Synod, led by a lay government official. Peter continued and intensified his predecessors' requirement of state service for all nobles.

By this same time, the once powerful Persian Safavid Empire to the south was heavily declining. Taking advantage of the profitable situation, Peter launched the Russo-Persian War (1722-1723), known as "The Persian Expedition of Peter the Great" by Russian histographers, in order to be the first Russian emperor to establish Russian influence in the Caucasus and Caspian Sea region. After considerable success and the capture of many provinces and cities in the Caucasus and northern mainland Persia, the Safavids were forced to hand over the territories to Russia. However, by twelve years later, all the territories were ceded back to Persia, which was now led by the charismatic military genius Nader Shah, as part of the Treaty of Resht and Treaty of Ganja and the Russo-Persian alliance against the Ottoman Empire, the common neighbouring rivalling enemy.

Peter the Great died in 1725, leaving an unsettled succession, but Russia had become a great power by the end of his reign. Peter I was succeeded by his second wife, Catherine I (1725–1727), who was merely a figurehead for a powerful group of high officials, then by his minor grandson, Peter II (1727–1730), then by his niece, Anna (1730–1740), daughter of Tsar Ivan V. The heir to Anna was soon deposed in a coup and Elizabeth, daughter of Peter I, ruled from 1741 to 1762. During her reign, Russia took part in the Seven Years' War.

Nearly forty years were to pass before a comparably ambitious ruler appeared on the Russian throne. Catherine II, "the Great" (r. 1762–1796), was a German princess who married the German heir to the Russian crown. He took weak positions, and Catherine overthrew him in a coup in 1762, becoming queen regnant. Catherine enthusiastically supported the ideals of The Enlightenment, thus earning the status of an enlightened despot She patronized the arts, science and learning. She contributed to the resurgence of the Russian nobility that began after the death of Peter the Great. Catherine promulgated the Charter to the Gentry reaffirming rights and freedoms of the Russian nobility and abolishing mandatory state service. She seized control of all the church lands, drastically reduced the size of the monasteries, and put the surviving clergy on a tight budget.

Catherine spent heavily to promote an expansive foreign policy. She extended Russian political control over the Polish–Lithuanian Commonwealth with actions, including the support of the Targowica Confederation. The cost of her campaigns, on top of the oppressive social system that required serfs to spend almost all of their time laboring on the land of their lords, provoked a major peasant uprising in 1773. Inspired by a Cossack named Pugachev, with the emphatic cry of "Hang all the landlords!", the rebels threatened to take Moscow until Catherine crushed the rebellion. Like the other enlightened despots of Europe, Catherine made certain of her own power and formed an alliance with the nobility.

Catherine successfully waged two wars (1768-74, 1787-92) against the decaying Ottoman Empire and advanced Russia's southern boundary to the Black Sea. Russia annexed Crimea in 1783, and created the Black Sea fleet. Then, by allying with the rulers of Austria and Prussia, she incorporated the territories of the Polish–Lithuanian Commonwealth, where after a century of Russian rule non-Catholic, mainly Orthodox population prevailed during the Partitions of Poland, pushing the Russian frontier westward into Central Europe.

In accordance to the treaty Russia had signed with the Georgians to protect them against any new invasion of their Persian suzerains and further political aspirations, Catherine waged a new war against Persia in 1796 after they had again invaded Georgia and established rule over it about a year prior, and had expelled the newly established Russian garrisons in the Caucasus.

In 1798-99, Russian troops participated in the anti-French coalition, the troops under the command of Alexander Suvorov defeated the French in Northern Italy.

Russian emperors of the 18th century professed the ideas of Enlightened absolutism. Innovative tsars such as Peter the Great and Catherine the Great brought in Western experts, scientists, philosophers, and engineers. However, Westernization and modernization affected only the upper classes of Russian society, while the bulk of the population, consisting of peasants, remained in a state of serfdom. Powerful Russians resented their privileged positions and alien ideas. The backlash was especially severe after the Napoleonic wars. It produced a powerful anti-western campaign that "led to a wholesale purge of Western specialists and their Russian followers in universities, schools, and government service."

The middle of the 18th century was marked by the emergence of higher education in Russia, The first two major universities Saint Petersburg State University and Moscow State University were opened in both capitals. Russian exploration of Siberia and the Far East continued. Great Northern Expedition laid the foundation for the development of Alaska by the Russians. By the end of the 18th century, Alaska became a Russian colony (Russian America). In the early 19th century, Alaska was used as a base for the First Russian circumnavigation. In 1819-21, Russian sailors discovered Antarctica during an Antarctic expedition.

Russia was in a continuous state of financial crisis. While revenue rose from 9 million rubles in 1724 to 40 million in 1794, expenses grew more rapidly, reaching 49 million in 1794. The budget was allocated 46 percent to the military, 20 percent to government economic activities, 12 percent to administration, and nine percent for the Imperial Court in St. Petersburg. The deficit required borrowing, primarily from Amsterdam; five percent of the budget was allocated to debt payments. Paper money was issued to pay for expensive wars, thus causing inflation. For its spending, Russia obtained a large and glorious army, a very large and complex bureaucracy, and a splendid court that rivaled Paris and London. However, the government was living far beyond its means, and 18th-century Russia remained "a poor, backward, overwhelmingly agricultural, and illiterate country."

By the time of her death in 1796, Catherine's expansionist policy had made Russia into a major European power. Alexander I continued this policy, wresting Finland from the weakened kingdom of Sweden in 1809 and Bessarabia from the Ottomans in 1812.

After Russian armies liberated allied Georgia from Persian occupation in 1802, they clashed with Persia over control and consolidation over Georgia, as well as the Iranian territories that comprise modern-day Azerbaijan and Dagestan. They also became involved in the Caucasian War against the Caucasian Imamate. In 1813, the war with Persia concluded with a Russian victory, forcing Qajar Iran to cede swaths of its territories in the Caucasus to Russia, which drastically increased its territory in the region. To the south-west, Russia attempted to expand at the expense of the Ottoman Empire, using Georgia at its base for the Caucasus and Anatolian front.

In European policy, Alexander I switched Russia back and forth four times in 1804–1812 from neutral peacemaker to anti-Napoleon to an ally of Napoleon, winding up in 1812 as Napoleon's enemy. In 1805, he joined Britain in the War of the Third Coalition against Napoleon, but after the massive defeat at the Battle of Austerlitz he switched and formed an alliance with Napoleon by the Treaty of Tilsit (1807) and joined Napoleon's Continental System. He fought a small-scale naval war against Britain, 1807–12. He and Napoleon could never agree, especially about Poland, and the alliance collapsed by 1810.

Furthermore, Russia's economy had been hurt by Napoleon's Continental System, which cut off trade with Britain. As Esdaile notes, "Implicit in the idea of a Russian Poland was, of course, a war against Napoleon." Schroeder says Poland was the root cause of the conflict but Russia's refusal to support the Continental System was also a factor.

The invasion of Russia was a catastrophe for Napoleon and his 450,000 invasion troops. One major battle was fought at Borodino; casualties were very high but it was indecisive and Napoleon was unable to engage and defeat the Russian armies. He attempted to force the Tsar to terms by capturing Moscow at the onset of winter, even though the French Army had already lost most of its men. The expectation proved futile. The Russians retreated, burning crops and food supplies in a scorched earth policy that multiplied Napoleon's logistic problems. Unprepared for winter warfare, 85%–90% of Napoleon's soldiers died from disease, cold, starvation or by ambush by peasant guerrilla fighters. As Napoleon's forces retreated, Russian troops pursued them into Central and Western Europe and finally captured Paris. Out of a total population of around 43 million people, Russia lost about 1.5 million in the year 1812; of these about 250,000 to 300,000 were soldiers and the rest peasants and serfs.

After the final defeat of Napoleon in 1815, Alexander became known as the 'savior of Europe.' He presided over the redrawing of the map of Europe at the Congress of Vienna (1814–15), which made him the king of Congress Poland. He formed the Holy Alliance with Austria and Prussia, to suppress revolutionary movements in Europe that he saw as immoral threats to legitimate Christian monarchs. He helped Austria's Klemens von Metternich in suppressing all national and liberal movements.

Although the Russian Empire would play a leading political role as late as 1848, its retention of serfdom precluded economic progress of any significant degree. As West European economic growth accelerated during the Industrial Revolution, sea trade and colonialism which had begun in the second half of the 18th century, Russia began to lag ever farther behind, undermining its ability to field strong armies.

Russia's great power status obscured the inefficiency of its government, the isolation of its people, and its economic backwardness. Following the defeat of Napoleon, Alexander I was willing to discuss constitutional reforms, and though a few were introduced, no thoroughgoing changes were attempted.

The tsar was succeeded by his younger brother, Nicholas I (1825–1855), who at the onset of his reign was confronted with an uprising. The background of this revolt lay in the Napoleonic Wars, when a number of well-educated Russian officers traveled in Europe in the course of the military campaigns, where their exposure to the liberalism of Western Europe encouraged them to seek change on their return to autocratic Russia. The result was the Decembrist Revolt (December 1825), the work of a small circle of liberal nobles and army officers who wanted to install Nicholas' brother as a constitutional monarch. But the revolt was easily crushed, leading Nicholas to turn away from liberal reforms and champion the reactionary doctrine "Orthodoxy, Autocracy, and Nationality".

In 1826–1828 Russia fought another war against Persia. Russia lost almost all of its recently consolidated territories during the first year but gained them back and won the war on highly favourable terms. At the 1828 Treaty of Turkmenchay, Russia gained Armenia, Nakhchivan, Nagorno-Karabakh, Azerbaijan, and Iğdır. In the 1828–1829 Russo-Turkish War Russia invaded northeastern Anatolia and occupied the strategic Ottoman towns of Erzurum and Gumushane and, posing as protector and saviour of the Greek Orthodox population, received extensive support from the region's Pontic Greeks. Following a brief occupation, the Russian imperial army withdrew back into Georgia. By the 1830s, Russia had conquered all Persian territories and major Ottoman territories in the Caucasus.

In 1831 Nicholas crushed the November Uprising in Poland. The Russian autocracy gave Polish artisans and gentry reason to rebel in 1863 by assailing the national core values of language, religion, and culture. The resulting January Uprising was a massive Polish revolt, which also was crushed. France, Britain and Austria tried to intervene in the crisis but were unable to do so. The Russian patriotic press used the Polish uprising to unify the Russian nation, claiming it was Russia's God-given mission to save Poland and the world. Poland was punished by losing its distinctive political and judicial rights, with Russianization imposed on its schools and courts.

Tsar Nicholas I (reigned 1825–1855) lavished attention on his very large army; with a population of 60–70 million people, the army included a million men. They had outdated equipment and tactics, but the tsar, who dressed like a soldier and surrounded himself with officers, gloried in the victory over Napoleon in 1812 and took enormous pride in its smartness on parade. The cavalry horses, for example, were only trained in parade formations, and did poorly in battle. The glitter and braid masked profound weaknesses that he did not see. He put generals in charge of most of his civilian agencies regardless of their qualifications. An agnostic who won fame in cavalry charges was made supervisor of Church affairs. The Army became the vehicle of upward social mobility for noble youths from non-Russian areas, such as Poland, the Baltic, Finland and Georgia. On the other hand, many miscreants, petty criminals and undesirables were punished by local officials by enlisting them for life in the Army. The conscription system was highly unpopular with people, as was the practice of forcing peasants to house the soldiers for six months of the year. Curtiss finds that "The pedantry of Nicholas' military system, which stressed unthinking obedience and parade ground evolutions rather than combat training, produced ineffective commanders in time of war." His commanders in the Crimean War were old and incompetent, and indeed so were his muskets as the colonels sold the best equipment and the best food.
Finally the Crimean War at the end of his reign demonstrated to the world what no one had previously realized: Russia was militarily weak, technologically backward, and administratively incompetent. Despite his grand ambitions toward the south and Ottoman Empire, Russia had not built its railroad network in that direction, and communications were bad. The bureaucracy was riddled with graft, corruption and inefficiency and was unprepared for war. The Navy was weak and technologically backward; the Army, although very large, was good only for parades, suffered from colonels who pocketed their men's pay, poor morale, and was even more out of touch with the latest technology as developed by Britain and France. As Fuller notes, "Russia had been beaten on the Crimean peninsula, and the military feared that it would inevitably be beaten again unless steps were taken to surmount its military weakness."

The 1st quarter of the 19th century is the time when Russian literature becomes an independent and very striking phenomenon; this is the time when the very laws of the Russian literary language are formed. The reasons for such a rapid development of Russian literature during this period lie both in the intra-literary processes and in the socio-political life of Russian society.

As Western Europe modernized, after 1840 the issue for Russia became one of direction. Westernizers favored imitating Western Europe while others renounced the West and called for a return of the traditions of the past. The latter path was championed by Slavophiles, who heaped scorn on the "decadent" West. The Slavophiles were opponents of bureaucracy and preferred the collectivism of the medieval Russian "mir", or village community, to the individualism of the West.

Westernizers formed an intellectual movement that deplored the backwardness of Russian culture, and looked to western Europe for intellectual leadership. They were opposed by Slavophiles who denounced the West as too materialistic and instead promoted the spiritual depth of Russian traditionalism. A forerunner of the movement was Pyotr Chaadayev (1794–1856). He exposed the cultural isolation of Russia, from the perspective of Western Europe, in his "Philosophical Letters" of 1831. He cast doubt on the greatness of the Russian past, and ridiculed Orthodoxy for failing to provide a sound spiritual basis for the Russian mind. He called on Russia to emulate Western Europe, especially in rational and logical thought, its progressive spirit, its leadership in science, and indeed its leadership on the path to freedom. Vissarion Belinsky (1811–1848), and Alexander Herzen (1812–1870) were prominent Westernizers.

Since the war against Napoleon, Russia had become deeply involved in the affairs of Europe, as part of the "Holy Alliance." The Holy Alliance was formed to serve as the "policeman of Europe." However, to be the policeman of Europe and maintain the alliance required large armies. Prussia, Austria, Britain and France (the other members of the alliance) lacked large armies and needed Russia to supply the required numbers, which fit the philosophy of Nicholas I. When the Revolutions of 1848 swept Europe, however, Russia was quiet. The Tsar sent his army into Hungary in 1849 at the request of the Austrian Empire and broke the revolt there, while preventing its spread to Russian Poland. The Tsar cracked down on any signs of internal unrest.

Russia expected that in exchange for supplying the troops to be the policeman of Europe, it should have a free hand in dealing with the decaying Ottoman Empire—the "sick man of Europe." In 1853 Russia invaded Ottoman-controlled areas leading to the Crimean War. Britain and France came to the rescue of the Ottomans. After a gruelling war fought largely in Crimea, with very high death rates from disease, the allies won.

Historian Orlando Figes points to the long-term damage Russia suffered: 
As Fuller notes, "Russia had been beaten on the Crimean peninsula, and the military feared that it would inevitably be beaten again unless steps were taken to surmount its military weakness."

Tsar Nicholas died with his philosophy in dispute. One year earlier, Russia had become involved in the Crimean War, a conflict fought primarily in the Crimean peninsula. Since playing a major role in the defeat of Napoleon, Russia had been regarded as militarily invincible, but, once pitted against a coalition of the great powers of Europe, the reverses it suffered on land and sea exposed the weakness of Tsar Nicholas' regime.

When Alexander II came to the throne in 1855, desire for reform was widespread. The most pressing problem confronting the Government was serfdom. In 1859, there were 23 million serfs (out of a total population of 67.1 Million). In anticipation of civil unrest that could ultimately foment a revolution, Alexander II chose to preemptively abolish serfdom with the emancipation reform in 1861. Emancipation brought a supply of free labor to the cities, stimulated industry, and the middle class grew in number and influence. The freed peasants had to buy land, allotted to them, from the landowners with the state assistance. The Government issued special bonds to the landowners for the land that they had lost, and collected a special tax from the peasants, called redemption payments, at a rate of 5% of the total cost of allotted land yearly. All the land turned over to the peasants was owned collectively by the "mir", the village community, which divided the land among the peasants and supervised the various holdings.

Alexander was the most successful Russian reformer since Peter the Great, and was responsible for numerous reforms besides abolishing serfdom. He reorganized the judicial system, setting up elected local judges, abolishing capital punishment, promoting local self-government through the zemstvo system, imposing universal military service, ending some of the privileges of the nobility, and promoting the universities. In foreign policy, he sold Alaska to the United States in 1867, fearing the remote colony would fall into British hands if there was another war. He modernized the military command system. He sought peace, and moved away from bellicose France when Napoleon III fell. He joined with Germany and Austria in the League of the Three Emperors that stabilized the European situation. The Russian Empire expanded in Siberia and in the Caucasus and made gains at the expense of China. Faced with an uprising in Poland in 1863, he stripped that land of its separate Constitution and incorporated it directly into Russia. To counter the rise of a revolutionary and anarchistic movements, he sent thousands of dissidents into exile in Siberia and was proposing additional parliamentary reforms when he was assassinated in 1881.

In the late 1870s Russia and the Ottoman Empire again clashed in the Balkans. The Russo-Turkish War was popular among the Russian people, who supported the independence of their fellow Orthodox Slavs, the Serbs and the Bulgarians. Russia's victory in this war allowed a number of Balkan states to gain independence: Romania, Serbia, Montenegro. In addition, Bulgaria de facto also became independent after 500 years of Turks yoke. However, the war increased tension with Austria-Hungary, which also had ambitions in the region. The tsar was disappointed by the results of the Congress of Berlin in 1878, but abided by the agreement. 

During this period Russia expanded its empire into Central Asia, which was rich in raw materials, conquering the khanates of Kokand, Bukhara, and Khiva, as well as the Trans-Caspian region. Russia's advance in Asia led to a confrontation with British Empire for dominance in the region. This conflict is called the Great Game. The confrontation ended only at the beginning of the 20th century, when two empires divided Asia into zones of influence.

In the 1860s a movement known as Nihilism developed in Russia. A term originally coined by Ivan Turgenev in his 1862 novel "Fathers and Sons", Nihilists favoured the destruction of human institutions and laws, based on the assumption that such institutions and laws are artificial and corrupt. At its core, Russian nihilism was characterized by the belief that the world lacks comprehensible meaning, objective truth, or value. For some time many Russian liberals had been dissatisfied by what they regarded as the empty discussions of the intelligentsia. The Nihilists questioned all old values and shocked the Russian establishment. They moved beyond being purely philosophical to becoming major political forces after becoming involved in the cause of reform. Their path was facilitated by the previous actions of the Decembrists, who revolted in 1825, and the financial and political hardship caused by the Crimean War, which caused many Russians to lose faith in political institutions. Russian nihilists created the «Catechism of a Revolutionary». The activities of one of the leaders of Russian nihilists, Sergei Nechaev, became the basis for Dostoevsky's novel "Demons".

The Nihilists first attempted to convert the aristocracy to the cause of reform. Failing there, they turned to the peasants. Their campaign, which targeted the people instead of the aristocracy or the landed gentry, became known as the Populist movement. It was based upon the belief that the common people possessed the wisdom and peaceful ability to lead the nation.

While the Narodnik movement was gaining momentum, the government quickly moved to extirpate it. In response to the growing reaction of the government, a radical branch of the Narodniks advocated and practiced terrorism. One after another, prominent officials were shot or killed by bombs. This represented the ascendancy of anarchism in Russia as a powerful revolutionary force. Finally, after several attempts, Alexander II was assassinated by anarchists in 1881, on the very day he had approved a proposal to call a representative assembly to consider new reforms in addition to the abolition of serfdom designed to ameliorate revolutionary demands.

The end of the 19th century - the beginning of the 20th century is known as the Silver Age of Russian culture. The Silver Age was dominated by the artistic movements of Russian Symbolism, Acmeism, and Russian Futurism, many poetic schools flourished, including the Mystical Anarchism tendency within the Symbolist movement. The Russian avant-garde was a large, influential wave of modern art that flourished in Russian Empire and Soviet Union, approximately from 1890 to 1930—although some have placed its beginning as early as 1850 and its end as late as 1960. The term covers many separate art movements of the era in painting, literature, music and architecture.

Unlike his father, the new tsar Alexander III (1881–1894) was throughout his reign a staunch reactionary who revived the maxim of "Orthodoxy, Autocracy, and National Character". A committed Slavophile, Alexander III believed that Russia could be saved from chaos only by shutting itself off from the subversive influences of Western Europe. In his reign Russia concluded the union with republican France to contain the growing power of Germany, completed the conquest of Central Asia, and exacted important territorial and commercial concessions from China.

The tsar's most influential adviser was Konstantin Pobedonostsev, tutor to Alexander III and his son Nicholas, and procurator of the Holy Synod from 1880 to 1895. He taught his royal pupils to fear freedom of speech and press and to hate democracy, constitutions, and the parliamentary system. Under Pobedonostsev, revolutionaries were hunted down and a policy of Russification was carried out throughout the empire.

Alexander was succeeded by his son Nicholas II (1894–1917). The Industrial Revolution, which began to exert a significant influence in Russia, was meanwhile creating forces that would finally overthrow the tsar. Politically, these opposition forces organized into three competing parties: The liberal elements among the industrial capitalists and nobility, who believed in peaceful social reform and a constitutional monarchy, founded the Constitutional Democratic party or "Kadets" in 1905. Followers of the Narodnik tradition established the Socialist-Revolutionary Party or "Esers" in 1901, advocating the distribution of land among those who actually worked it—the peasants. A third radical group founded the Russian Social Democratic Labour Party or "RSDLP" in 1898; this party was the primary exponent of Marxism in Russia. Gathering their support from the radical intellectuals and the urban working class, they advocated complete social, economic and political revolution.

In 1903 the RSDLP split into two wings: the radical Bolsheviks, led by Vladimir Lenin, and the relatively moderate Mensheviks, led by Yuli Martov. The Mensheviks believed that Russian socialism would grow gradually and peacefully and that the tsar's regime should be succeeded by a democratic republic in which the socialists would cooperate with the liberal bourgeois parties. The Bolsheviks advocated the formation of a small elite of professional revolutionists, subject to strong party discipline, to act as the vanguard of the proletariat in order to seize power by force.

At the beginning of the 20th century, Russia continued its expansion in the Far East; Chinese Manchuria was in the zone of Russian interests. Russia took an active part in the intervention of the great powers in China to suppress the Boxer rebellion. During this war, Russia occupied Manchuria, which caused a clash of interests with Japan. In 1904, the Russo-Japanese War began, which ended extremely unsuccessfully for Russia.

The disastrous performance of the Russian armed forces in the Russo-Japanese War was a major blow to the Russian State and increased the potential for unrest.

In January 1905, an incident known as "Bloody Sunday" occurred when Father Gapon led an enormous crowd to the Winter Palace in Saint Petersburg to present a petition to the tsar. When the procession reached the palace, Cossacks opened fire on the crowd, killing hundreds. The Russian masses were so aroused over the massacre that a general strike was declared demanding a democratic republic. This marked the beginning of the Russian Revolution of 1905. Soviets (councils of workers) appeared in most cities to direct revolutionary activity.

In October 1905, Nicholas reluctantly issued the October Manifesto, which conceded the creation of a national Duma (legislature) to be called without delay. The right to vote was extended, and no law was to go into force without confirmation by the Duma. The moderate groups were satisfied; but the socialists rejected the concessions as insufficient and tried to organize new strikes. By the end of 1905, there was disunity among the reformers, and the tsar's position was strengthened for the time being.

The Archduke Franz Ferdinand of Austro-Hungary was assassinated by Bosnian Serbs on 28 June 1914. An ultimatum followed to Serbia, which was considered a Russian client-state, by Austro-Hungary on 23 July. Russia had no treaty obligation to Serbia, and in long-term perspective, Russia was militarily gaining on Germany and Austro-Hungary, and thus had an incentive to wait. Most Russian leaders wanted to avoid a war. However, in the present crisis they had the support of France, and they feared that the failure to support Serbia would lead to a loss of Russian credibility and a major political defeat to Russia's goals for a leadership role in the Balkans. Tsar Nicholas II mobilised Russian forces on 30 July 1914 to defend Serbia from Austria-Hungary. Christopher Clark states: "The Russian general mobilisation [of 30 July] was one of the most momentous decisions of the July crisis. This was the first of the general mobilisations. It came at the moment when the German government had not yet even declared the State of Impending War". Germany responded with her own mobilisation and declaration of War on 1 August 1914. At the opening of hostilities, the Russians took the offensive against both Germany and Austria-Hungary.

The very large but poorly equipped Russian army fought tenaciously and desperately at times despite its lack of organization and very weak logistics. Casualties were enormous. In the 1914 campaign, Russian forces defeated Austro-Hungarian forces in the Battle of Galicia. The success of the Russian army forced the German army to withdraw troops from the western front to the Russian front. However, the shell famine led to the defeat of the Russian forces in Poland by the central powers in the 1915 campaign, which led to a major retreat of the Russian army. In 1916, the Russians again dealt a powerful blow to the Austrians during the Brusilov offensive.

By 1915, many soldiers were sent to the front unarmed, and told to pick up whatever weapons they could from the battlefield. Nevertheless, the Russian army fought on, and tied down large numbers of Germans and Austrians. When civilians showed a surge of patriotism, the tsar and his entourage failed to exploit it for military benefit. Instead, they relied on slow-moving bureaucracies. In areas where they did advance against the Austrians, they failed to rally the ethnic and religious minorities that were hostile to Austria, such as Poles. The tsar refused to cooperate with the national legislature, the Duma, and listened less to experts than to his wife, who was in thrall to her chief advisor, the so-called holy man Grigori Rasputin. More than two million refugees fled.

Repeated military failures and bureaucratic ineptitude soon turned large segments of the population against the government. The German and Ottoman fleets prevented Russia from importing supplies and exporting goods through the Baltic and Black seas.

By the middle of 1915 the impact of the war was demoralizing. Food and fuel were in short supply, casualties kept occurring, and inflation was mounting. Strikes increased among low-paid factory workers, and the peasants, who wanted land reforms, were restless. Meanwhile, elite distrust of the regime was deepened by reports that Rasputin was gaining influence; his assassination in late 1916 ended the scandal but did not restore the autocracy's lost prestige.

The Tsarist system was completely overthrown in February 1917. Rabinowitch argues:
The February 1917 revolution...grew out of prewar political and economic instability, technological backwardness, and fundamental social divisions, coupled with gross mismanagement of the war effort, continuing military defeats, domestic economic dislocation, and outrageous scandals surrounding the monarchy.
In late February (3 March 1917), a strike occurred in a factory in the capital Petrograd (the new name for Saint Petersburg). On 23 February (8 March) 1917, thousands of female textile workers walked out of their factories protesting the lack of food and calling on other workers to join them. Within days, nearly all the workers in the city were idle, and street fighting broke out. The tsar ordered the Duma to disband, ordered strikers to return to work, and ordered troops to shoot at demonstrators in the streets. His orders triggered the February Revolution, especially when soldiers openly sided with the strikers. The tsar and the aristocracy fell on 2 March, as Nicholas II abdicated.

To fill the vacuum of authority, the Duma declared a Provisional Government, headed by Prince Lvov, which was collectively known as the Russian Republic. Meanwhile, the socialists in Petrograd organized elections among workers and soldiers to form a soviet (council) of workers' and soldiers' deputies, as an organ of popular power that could pressure the "bourgeois" Provisional Government.
In July, following a series of crises that undermined their authority with the public, the head of the Provisional Government resigned and was succeeded by Alexander Kerensky, who was more progressive than his predecessor but not radical enough for the Bolsheviks or many Russians discontented with the deepening economic crisis and the continuation of the war. While Kerensky's government marked time, the socialist-led soviet in Petrograd joined with soviets that formed throughout the country to create a national movement.

The German government provided over 40 million gold marks to subsidize Bolshevik publications and activities subversive of the tsarist government, especially focusing on disgruntled soldiers and workers. In April 1917 Germany provided a special sealed train to carry Vladimir Lenin back to Russia from his exile in Switzerland. After many behind-the-scenes maneuvers, the soviets seized control of the government in November 1917 and drove Kerensky and his moderate provisional government into exile, in the events that would become known as the October Revolution.

When the national Constituent Assembly (elected in December 1917) refused to become a rubber stamp of the Bolsheviks, it was dissolved by Lenin's troops and all vestiges of democracy were removed. With the handicap of the moderate opposition removed, Lenin was able to free his regime from the war problem by the harsh Treaty of Brest-Litovsk (1918) with Germany. Russia lost much of her western borderlands. However, when Germany was defeated the Soviet government repudiated the Treaty.

The Bolshevik grip on power was by no means secure, and a lengthy struggle broke out between the new regime and its opponents, which included the Socialist Revolutionaries, right-wing White movement, and large numbers of peasants. At the same time the Allied powers sent several expeditionary armies to support the anti-Communist forces in an attempt to force Russia to rejoin the world war. The Bolsheviks fought against both these forces and national independence movements in the former Russian Empire. By 1921, they had defeated their internal enemies and brought most of the newly independent states under their control, with the exception of Finland, the Baltic States, the Moldavian Democratic Republic (which joined Romania), and Poland (with whom they had fought the Polish–Soviet War). Finland also annexed the region Pechenga of the Russian Kola peninsula; Soviet Russia and allied Soviet republics conceded the parts of its territory to Estonia (Petseri County and Estonian Ingria), Latvia (Pytalovo), and Turkey (Kars). Poland incorporated the contested territories of Western Belarus and Western Ukraine, the former parts of the Russian Empire (except Galicia) east to Curzon Line.

Both sides regularly committed brutal atrocities against civilians. During the civil war era White Terror (Russia) for example, Petlyura and Denikin's forces massacred 100,000 to 150,000 Jews in Ukraine and southern Russia. Hundreds of thousands of Jews were left homeless and tens of thousands became victims of serious illness.

Estimates for the total number of people killed during the Red Terror carried out by the Bolsheviks vary widely. One source asserts that the total number of victims of repression and pacification campaigns could be 1.3 million, whereas others give estimates ranging from 10,000 in the initial period of repression to 50,000 to 140,000 and an estimate of 28,000 executions per year from December 1917 to February 1922. The most reliable estimations for the total number of killings put the number at about 100,000, whereas others suggest a figure of 200,000.

The Russian economy was devastated by the war, with factories and bridges destroyed, cattle and raw materials pillaged, mines flooded and machines damaged. The droughts of 1920 and 1921, as well as the 1921 famine, worsened the disaster still further. Disease had reached pandemic proportions, with 3,000,000 dying of typhus alone in 1920. Millions more also died of widespread starvation. By 1922 there were at least 7,000,000 street children in Russia as a result of nearly ten years of devastation from the Great War and the civil war. Another one to two million people, known as the White émigrés, fled Russia, many were evacuated from Crimea in the 1920, some through the Far East, others west into the newly independent Baltic countries. These émigrés included a large percentage of the educated and skilled population of Russia.

The history of Russia between 1922 and 1991 is essentially the history of the Union of Soviet Socialist Republics, or Soviet Union. This ideologically based union, established in December 1922 by the leaders of the Russian Communist Party, was roughly coterminous with Russia before the Treaty of Brest-Litovsk. At that time, the new nation included four constituent republics: the Russian SFSR, the Ukrainian SSR, the Belarusian SSR, and the Transcaucasian SFSR.

The constitution, adopted in 1924, established a federal system of government based on a succession of soviets set up in villages, factories, and cities in larger regions. This pyramid of soviets in each constituent republic culminated in the All-Union Congress of Soviets. However, while it appeared that the congress exercised sovereign power, this body was actually governed by the Communist Party, which in turn was controlled by the Politburo from Moscow, the capital of the Soviet Union, just as it had been under the tsars before Peter the Great.

The period from the consolidation of the Bolshevik Revolution in 1917 until 1921 is known as the period of war communism. Land, all industry, and small businesses were nationalized, and the money economy was restricted. Strong opposition soon developed. The peasants wanted cash payments for their products and resented having to surrender their surplus grain to the government as a part of its civil war policies. Confronted with peasant opposition, Lenin began a strategic retreat from war communism known as the New Economic Policy (NEP). The peasants were freed from wholesale levies of grain and allowed to sell their surplus produce in the open market. Commerce was stimulated by permitting private retail trading. The state continued to be responsible for banking, transportation, heavy industry, and public utilities.

Although the left opposition among the Communists criticized the rich peasants, or kulaks, who benefited from the NEP, the program proved highly beneficial and the economy revived. The NEP would later come under increasing opposition from within the party following Lenin's death in early 1924.

As the Russian Empire included during this period not only the region of Russia, but also today's territories of Ukraine, Belarus, Poland, Lithuania, Estonia, Latvia, Moldavia and the Caucasian and Central Asian countries, it is possible to examine the firm formation process in all those regions. One of the main determinants of firm creation for given regions of Russian Empire might be urban demand of goods and supply of industrial and organizational skill.

While the Russian economy was being transformed, the social life of the people underwent equally drastic changes. The Family Code of 1918 granted women equal status to men, and permitted a couple to take either the husband or wife’s name. Divorce no longer required court procedure,
and to make women completely free of the responsibilities of childbearing, abortion was made legal as early as 1920. As a side effect, the emancipation of women increased the labor market. Girls were encouraged to secure an education and pursue a career in the factory or the office. Communal nurseries were set up for the care of small children, and efforts were made to shift the center of people's social life from the home to educational and recreational groups, the soviet clubs.

The Soviet government pursued a policy of eliminating illiteracy Likbez. After industrialization, massive urbanization began in the USSR. In the field of national policy in the 1920s, the Korenizatsiya was carried out. However, from the mid-30s, the Stalinist government returned to the tsarist policy of Russification of the outskirts. In particular, the languages of all the nations of the USSR were translated into the Cyrillic alphabet Cyrillization.

The years from 1929 to 1939 comprised a tumultuous decade in Soviet history—a period of massive industrialization and internal struggles as Joseph Stalin established near total control over Soviet society, wielding virtually unrestrained power. Following Lenin's death Stalin wrestled to gain control of the Soviet Union with rival factions in the Politburo, especially Leon Trotsky's. By 1928, with the Trotskyists either exiled or rendered powerless, Stalin was ready to put a radical programme of industrialisation into action.

In 1929 Stalin proposed the first five-year plan. Abolishing the NEP, it was the first of a number of plans aimed at swift accumulation of capital resources through the buildup of heavy industry, the collectivization of agriculture, and the restricted manufacture of consumer goods. For the first time in history a government controlled all economic activity. The rapid growth of production capacity and the volume of production of heavy industry (4 times) was of great importance for ensuring economic independence from western countries and strengthening the country's defense capability. At this time, the Soviet Union made the transition from an agrarian country to an industrial one.

As a part of the plan, the government took control of agriculture through the state and collective farms ("kolkhozes"). By a decree of February 1930, about one million individual peasants ("kulaks") were forced off their land. Many peasants strongly opposed regimentation by the state, often slaughtering their herds when faced with the loss of their land. In some sections they revolted, and countless peasants deemed "kulaks" by the authorities were executed. The combination of bad weather, deficiencies of the hastily established collective farms, and massive confiscation of grain precipitated a serious famine, and several million peasants died of starvation, mostly in Ukraine, Kazakhstan and parts of southwestern Russia. The deteriorating conditions in the countryside drove millions of desperate peasants to the rapidly growing cities, fueling industrialization, and vastly increasing Russia's urban population in the space of just a few years.

The plans received remarkable results in areas aside from agriculture. Russia, in many measures the poorest nation in Europe at the time of the Bolshevik Revolution, now industrialized at a phenomenal rate, far surpassing Germany's pace of industrialization in the 19th century and Japan's earlier in the 20th century.

While the Five-Year Plans were forging ahead, Stalin was establishing his personal power. 

The NKVD gathered in tens of thousands of Soviet citizens to face arrest, deportation, or execution. Of the six original members of the 1920 Politburo who survived Lenin, all were purged by Stalin. Old Bolsheviks who had been loyal comrades of Lenin, high officers in the Red Army, and directors of industry were liquidated in the Great Purges. Purges in other Soviet republics also helped centralize control in the USSR.

Stalin destroyed the opposition in the party consisting of the old Bolsheviks during the Moscow trials. The NKVD under the leadership of Stalin's commissar Nikolai Yezhov carried out a series of massive repressive operations against the kulaks and various national minorities in the USSR. During the Great Purges of 1937-38, about 700 000 people were executed.

Stalin's repressions led to the creation of a vast system of internal exile, of considerably greater dimensions than those set up in the past by the tsars. Draconian penalties were introduced and many citizens were prosecuted for fictitious crimes of sabotage and espionage. The labor provided by convicts working in the labor camps of the Gulag system became an important component of the industrialization effort, especially in Siberia. An estimated 18 million people passed through the Gulag system, and perhaps another 15 million had experience of some other form of forced labor.

After the partition of Poland in 1939, the NKVD executed 20,000 captured Polish officers in Katyn massacre. In the late 30s - first half of the 40s, the Stalinist government carried out massive deportations of various nationalities. A number of ethnic groups were deported from their settlement to Central Asia.

The Soviet Union viewed the 1933 accession of fervently anti-Communist Hitler's government to power in Germany with great alarm from the onset, especially since Hitler proclaimed the Drang nach Osten as one of the major objectives in his vision of the German strategy of Lebensraum. The Soviets supported the republicans of Spain who struggled against fascist German and Italian troops in the Spanish Civil War. In 1938–1939, immediately prior to WWII, the Soviet Union successfully fought against Imperial Japan in the Soviet–Japanese border conflicts in the Russian Far East, which led to Soviet-Japanese neutrality and the tense border peace that lasted until August 1945.

In 1938 Germany annexed Austria and, together with major Western European powers, signed the Munich Agreement following which Germany, Hungary and Poland divided parts of Czechoslovakia between themselves. German plans for further eastward expansion, as well as the lack of resolve from Western powers to oppose it, became more apparent. Despite the Soviet Union strongly opposing the Munich deal and repeatedly reaffirming its readiness to militarily back commitments given earlier to Czechoslovakia, the Western Betrayal led to the end of Czechoslovakia and further increased fears in the Soviet Union of a coming German attack. This led the Soviet Union to rush the modernization of its military industry and to carry out its own diplomatic maneuvers. In 1939 the Soviet Union signed the Molotov–Ribbentrop Pact: a non-aggression pact with Nazi Germany dividing Eastern Europe into two separate spheres of influence. Following the pact, the USSR normalized relations with Nazi Germany and resumed Soviet–German trade.

On 17 September 1939, sixteen days after the start of World War II and with the victorious Germans having advanced deep into Polish territory, the Red Army invaded eastern Poland, stating as justification the "need to protect Ukrainians and Belarusians" there, after the "cessation of existence" of the Polish state. As a result, the Belarusian and Ukrainian Soviet republics' western borders were moved westward, and the new Soviet western border was drawn close to the original Curzon line. In the meantime negotiations with Finland over a Soviet-proposed land swap that would redraw the Soviet-Finnish border further away from Leningrad failed, and in December 1939 the USSR invaded Finland, beginning a campaign known as the Winter War (1939–40). The war took a heavy death toll on the Red Army but forced Finland to sign a Moscow Peace Treaty and cede the Karelian Isthmus and Ladoga Karelia. In summer 1940 the USSR issued an ultimatum to Romania forcing it to cede the territories of Bessarabia and Northern Bukovina. At the same time, the Soviet Union also occupied the three formerly independent Baltic states (Estonia, Latvia and Lithuania).
The peace with Germany was tense, as both sides were preparing for the military conflict, and abruptly ended when the Axis forces led by Germany swept across the Soviet border on 22 June 1941. By the autumn the German army had seized Ukraine, laid a siege of Leningrad, and threatened to capture the capital, Moscow, itself. Despite the fact that in December 1941 the Red Army threw off the German forces from Moscow in a successful counterattack, the Germans retained the strategic initiative for approximately another year and held a deep offensive in the south-eastern direction, reaching the Volga and the Caucasus. However, two major German defeats in Stalingrad and Kursk proved decisive and reversed the course of the entire World War as the Germans never regained the strength to sustain their offensive operations and the Soviet Union recaptured the initiative for the rest of the conflict. By the end of 1943, the Red Army had broken through the German siege of Leningrad and liberated much of Ukraine, much of Western Russia and moved into Belarus. During the 1944 campaign, the Red Army defeated German forces in a series of offensive campaigns known as Stalin's ten blows. By the end of 1944, the front had moved beyond the 1939 Soviet frontiers into eastern Europe. Soviet forces drove into eastern Germany, capturing Berlin in May 1945. The war with Germany thus ended triumphantly for the Soviet Union.
As agreed at the Yalta Conference, three months after the Victory Day in Europe the USSR launched the Soviet invasion of Manchuria, defeating the Japanese troops in neighboring Manchuria, the last Soviet battle of World War II.
Although the Soviet Union was victorious in World War II, the war resulted in around 26–27 million Soviet deaths (estimates vary) and had devastated the Soviet economy in the struggle. Some 1,710 towns and 70,000 settlements were destroyed. The occupied territories suffered from the ravages of German occupation and deportations of slave labor by Germany. Thirteen million Soviet citizens became victims of the repressive policies of Germany and its allies in occupied territories, where people died because of mass murders, famine, absence of elementary medical aid and slave labor. The Nazi Genocide of the Jews, carried out by German "Einsatzgruppen" along with local collaborators, resulted in almost complete annihilation of the Jewish population over the entire territory temporarily occupied by Germany and its allies. During the occupation, the Leningrad region lost around a quarter of its population, Soviet Belarus lost from a quarter to a third of its population, and 3.6 million Soviet prisoners of war (of 5.5 million) died in German camps.

Collaboration among the major Allies had won the war and was supposed to serve as the basis for postwar reconstruction and security. USSR became one of the founders of the UN and a permanent member of the UN Security Council. However, the conflict between Soviet and U.S. national interests, known as the Cold War, came to dominate the international stage in the postwar period.

The Cold War emerged from a conflict between Stalin and U.S. President Harry Truman over the future of Eastern Europe during the Potsdam Conference in the summer of 1945. Russia had suffered three devastating Western onslaughts in the previous 150 years during the Napoleonic Wars, the First World War, and the Second World War, and Stalin's goal was to establish a buffer zone of states between Germany and the Soviet Union. Truman charged that Stalin had betrayed the Yalta agreement. With Eastern Europe under Red Army occupation, Stalin was also biding his time, as his own atomic bomb project was steadily and secretly progressing.

In April 1949 the United States sponsored the North Atlantic Treaty Organization (NATO), a mutual defense pact in which most Western nations pledged to treat an armed attack against one nation as an assault on all. The Soviet Union established an Eastern counterpart to NATO in 1955, dubbed the Warsaw Pact. The division of Europe into Western and Soviet blocks later took on a more global character, especially after 1949, when the U.S. nuclear monopoly ended with the testing of a Soviet bomb and the Communist takeover in China.

The foremost objectives of Soviet foreign policy were the maintenance and enhancement of national security and the maintenance of hegemony over Eastern Europe. The Soviet Union maintained its dominance over the Warsaw Pact through crushing the Hungarian Revolution of 1956, suppressing the Prague Spring in Czechoslovakia in 1968, and supporting the suppression of the Solidarity movement in Poland in the early 1980s. The Soviet Union opposed the United States in a number of proxy conflicts all over the world, including the Korean War and Vietnam War.

As the Soviet Union continued to maintain tight control over its sphere of influence in Eastern Europe, the Cold War gave way to "Détente" and a more complicated pattern of international relations in the 1970s in which the world was no longer clearly split into two clearly opposed blocs. The nuclear race continued, the number of nuclear weapons in the hands of the USSR and the United States reached a menacing scale, allowing them to destroy the planet many times. Less powerful countries had more room to assert their independence, and the two superpowers were partially able to recognize their common interest in trying to check the further spread and proliferation of nuclear weapons in treaties such as SALT I, SALT II, and the Anti-Ballistic Missile Treaty.

U.S.–Soviet relations deteriorated following the beginning of the nine-year Soviet–Afghan War in 1979 and the 1980 election of Ronald Reagan, a staunch anti-communist, but improved as the communist bloc started to unravel in the late 1980s. With the collapse of the Soviet Union in 1991, Russia lost the superpower status that it had won in the Second World War.

In the power struggle that erupted after Stalin's death in 1953, his closest followers lost out. Nikita Khrushchev solidified his position in a speech before the Twentieth Congress of the Communist Party in 1956 detailing Stalin's atrocities.

In 1964 Khrushchev was impeached by the Communist Party's Central Committee, charging him with a host of errors that included Soviet setbacks such as the Cuban Missile Crisis. After a period of collective leadership led by Leonid Brezhnev, Alexei Kosygin and Nikolai Podgorny, a veteran bureaucrat, Brezhnev, took Khrushchev's place as Soviet leader. Brezhnev emphasized heavy industry, instituted the Soviet economic reform of 1965, and also attempted to ease relationships with the United States. In the 1960s the USSR became a leading producer and exporter of petroleum and natural gas. Soviet science and industry peaked in the Khrushchev and Brezhnev years. The world's first nuclear power plant was established in 1954 in Obninsk, and the Baikal Amur Mainline was built. In addition, in 1980 Moscow hosted the Summer Olympic Games.

While all modernized economies were rapidly moving to computerization after 1965, the USSR fell further and further behind. Moscow's decision to copy the IBM 360 of 1965 proved a decisive mistake for it locked scientists into an antiquated system they were unable to improve. They had enormous difficulties in manufacturing the necessary chips reliably and in quantity, in programming workable and efficient programs, in coordinating entirely separate operations, and in providing support to computer users.

One of the greatest strengths of Soviet economy was its vast supplies of oil and gas; world oil prices quadrupled in the 1973–74, and rose again in 1979–1981, making the energy sector the chief driver of the Soviet economy, and was used to cover multiple weaknesses. At one point, Soviet Premier Alexei Kosygin told the head of oil and gas production, "things are bad with bread. Give me 3 million tons [of oil] over the plan." Former prime minister Yegor Gaidar, an economist looking back three decades, in 2007 wrote:

The Soviet space program, founded by Sergey Korolev, was especially successful. On 4 October 1957 Soviet Union launched the first space satellite Sputnik. On 12 April 1961 Yuri Gagarin became the first human to travel into space in the Soviet spaceship Vostok 1. Other achievements of Russian space program include: the first photo of the far side of the Moon; exploration of Venus; the first spacewalk by Alexei Leonov; first female spaceflight by Valentina Tereshkova. In 1970 and 1973, the world's first planetary rovers were sent to the moon and successfully worked there: Lunokhod 1 and Lunokhod 2. More recently, the Soviet Union produced the world's first space station, Salyut which in 1986 was replaced by Mir, the first consistently inhabited long-term space station, that served from 1986 to 2001.

Two developments dominated the decade that followed: the increasingly apparent crumbling of the Soviet Union's economic and political structures, and the patchwork attempts at reforms to reverse that process. After the rapid succession of former KGB Chief Yuri Andropov and Konstantin Chernenko, transitional figures with deep roots in Brezhnevite tradition, Mikhail Gorbachev implemented perestroika in an attempt to modernize Soviet communism, and made significant changes in the party leadership. However, Gorbachev's social reforms led to unintended consequences. His policy of "glasnost" facilitated public access to information after decades of government repression, and social problems received wider public attention, undermining the Communist Party's authority. "Glasnost" allowed ethnic and nationalist disaffection to reach the surface, and many constituent republics, especially the Baltic republics, Georgian SSR and Moldavian SSR, sought greater autonomy, which Moscow was unwilling to provide. In the revolutions of 1989 the USSR lost its allies in Eastern Europe. Gorbachev's attempts at economic reform were not sufficient, and the Soviet government left intact most of the fundamental elements of communist economy. Suffering from low pricing of petroleum and natural gas, the ongoing war in Afghanistan, and outdated industry and pervasive corruption, the Soviet planned economy proved to be ineffective, and by 1990 the Soviet government had lost control over economic conditions. Due to price control, there were shortages of almost all products, reaching their peak in the end of 1991, when people had to stand in long lines and were lucky to buy even the essentials. Control over the constituent republics was also relaxed, and they began to assert their national sovereignty over Moscow.
The tension between Soviet Union and Russian SFSR authorities came to be personified in the bitter power struggle between Gorbachev and Boris Yeltsin. Squeezed out of Union politics by Gorbachev in 1987, Yeltsin, who represented himself as a committed democrat, presented a significant opposition to Gorbachev's authority. In a remarkable reversal of fortunes, he gained election as chairman of the Russian republic's new Supreme Soviet in May 1990. The following month, he secured legislation giving Russian laws priority over Soviet laws and withholding two-thirds of the budget. In the first Russian presidential election in 1991 Yeltsin became president of the Russian SFSR.
At last Gorbachev attempted to restructure the Soviet Union into a less centralized state. However, on 19 August 1991, a coup against Gorbachev, conspired by senior Soviet officials, was attempted. The coup faced wide popular opposition and collapsed in three days, but disintegration of the Union became imminent. The Russian government took over most of the Soviet Union government institutions on its territory. Because of the dominant position of Russians in the Soviet Union, most gave little thought to any distinction between Russia and the Soviet Union before the late 1980s. In the Soviet Union, only Russian SFSR lacked even the paltry instruments of statehood that the other republics possessed, such as its own republic-level Communist Party branch, trade union councils, Academy of Sciences, and the like. The Communist Party of the Soviet Union was banned in Russia in 1991–1992, although no lustration has ever taken place, and many of its members became top Russian officials. However, as the Soviet government was still opposed to market reforms, the economic situation continued to deteriorate. By December 1991, the shortages had resulted in the introduction of food rationing in Moscow and Saint Petersburg for the first time since World War II. Russia received humanitarian food aid from abroad. After the Belavezha Accords, the Supreme Soviet of Russia withdrew Russia from the Soviet Union on 12 December. The Soviet Union officially ended on 25 December 1991, and the Russian Federation (formerly the Russian Soviet Federative Socialist Republic) took power on 26 December. The Russian government lifted price control on January 1992. Prices rose dramatically, but shortages disappeared.

Although Yeltsin came to power on a wave of optimism, he never recovered his popularity after endorsing Yegor Gaidar's "shock therapy" of ending Soviet-era price controls, drastic cuts in state spending, and an open foreign trade regime in early 1992 ("see" Russian economic reform in the 1990s). The reforms immediately devastated the living standards of much of the population. In the 1990s Russia suffered an economic downturn that was, in some ways, more severe than the United States or Germany had undergone six decades earlier in the Great Depression. Hyperinflation hit the ruble, due to monetary overhang from the days of the planned economy.

Meanwhile, the profusion of small parties and their aversion to coherent alliances left the legislature chaotic. During 1993, Yeltsin's rift with the parliamentary leadership led to the September–October 1993 constitutional crisis. The crisis climaxed on 3 October, when Yeltsin chose a radical solution to settle his dispute with parliament: he called up tanks to shell the Russian White House, blasting out his opponents. As Yeltsin was taking the unconstitutional step of dissolving the legislature, Russia came close to a serious civil conflict. Yeltsin was then free to impose the current Russian constitution with strong presidential powers, which was approved by referendum in December 1993. The cohesion of the Russian Federation was also threatened when the republic of Chechnya attempted to break away, leading to the First and Second Chechen Wars.

Economic reforms also consolidated a semi-criminal oligarchy with roots in the old Soviet system. Advised by Western governments, the World Bank, and the International Monetary Fund, Russia embarked on the largest and fastest privatization that the world had ever seen in order to reform the fully nationalized Soviet economy. By mid-decade, retail, trade, services, and small industry was in private hands. Most big enterprises were acquired by their old managers, engendering a new rich (Russian tycoons) in league with criminal mafias or Western investors. Corporate raiders such as Andrei Volgin engaged in hostile takeovers of corrupt corporations by the mid-1990s.

By the mid-1990s Russia had a system of multiparty electoral politics. But it was harder to establish a representative government because of two structural problems—the struggle between president and parliament and the anarchic party system.

Meanwhile, the central government had lost control of the localities, bureaucracy, and economic fiefdoms, and tax revenues had collapsed. Still in a deep depression, Russia's economy was hit further by the financial crash of 1998. After the crisis, Yeltsin was at the end of his political career. Just hours before the first day of 2000, Yeltsin made a surprise announcement of his resignation, leaving the government in the hands of the little-known Prime Minister Vladimir Putin, a former KGB official and head of the FSB, the KGB's post-Soviet successor agency. 

In 2000, the new acting president defeated his opponents in the presidential election on 26 March, and won in a landslide four years later. The Second Chechen war ended with the victory of Russia, at the same time, after the September 11 terrorist attacks, there was a rapprochement between Russia and the United States. Putin has created a system of guided democracy in Russia by subjugating parliament, suppressing independent media and placing major oil and gas companies under state control.
International observers were alarmed by moves in late 2004 to further tighten the presidency's control over parliament, civil society, and regional officeholders. In 2008 Dmitri Medvedev, a former Gazprom chairman and Putin's head of staff, was elected new President of Russia. In 2012 Putin and Medvedev switched places, Putin became president again, prompting massive protests in Moscow in 2011-12.

Russia's long-term problems include a shrinking workforce, rampant corruption, and underinvestment in infrastructure. Nevertheless, reversion to a socialist command economy seemed almost impossible. The economic problems are aggravated by massive capital outflows, as well as extremely difficult conditions for doing business, due to pressure from the security forces "Siloviki" and government agencies.

Due to high oil prices, from 2000 to 2008, Russia's GDP at PPP doubled. Although high oil prices and a relatively cheap ruble initially drove this growth, since 2003 consumer demand and, more recently, investment have played a significant role. Russia is well ahead of most other resource-rich countries in its economic development, with a long tradition of education, science, and industry. The economic recovery of the 2000s allowed Russia to obtain the right to host the 2014 Winter Olympic Games in Sochi.

In 2014, following a referendum, in which separation was favored by a large majority of voters, the Russian leadership announced the accession of Crimea into the Russian Federation. Following Russia's annexation of Crimea and alleged Russian interference in the war in eastern Ukraine, Western sanctions were imposed on Russia.

Since 2015, Russia has been conducting military intervention in Syria in support of the Bashar al-Assad regime, against ISIS and the Syrian opposition.

In 2018, the FIFA World Cup was held in Russia. Vladimir Putin was re-elected for a fourth presidential term.









</doc>
<doc id="14117" url="https://en.wikipedia.org/wiki?curid=14117" title="History of Christianity">
History of Christianity

The history of Christianity concerns the Christian religion, Christian countries, and the Church with its various denominations, from the 1st century to the present.

Christianity originated with the ministry of Jesus in the 1st century Roman province of Judea. According to the Gospels, Jesus was a Jewish teacher and healer who proclaimed the imminent kingdom of God and was crucified . His followers believed that he was then raised from the dead and exalted by God, and would return soon at the inception of God's kingdom.

The earliest followers of Jesus were apocalyptic Jewish Christians. The inclusion of gentiles in the developing early Christian Church caused a schism between Judaism and Jewish Christianity during the first two centuries of the Christian Era. In 313, Emperor Constantine I issued the Edict of Milan legalizing Christian worship. In 380, with the Edict of Thessalonica put forth under Theodosius I, the Roman Empire officially adopted Trinitarian Christianity as its state religion, and Christianity established itself as a predominantly Roman religion in the state church of the Roman Empire. Christological debates about the human and divine nature of Jesus consumed the Christian Church for two centuries, and seven ecumenical councils were called to resolve these debates. Arianism was condemned at the First Council of Nicea (325), which supported the Trinitarian doctrine as expounded in the Nicene Creed.

In the early Middle Ages, missionary activities spread Christianity towards the west among German peoples. During the High Middle Ages, eastern and western Christianity grew apart, leading to the East–West Schism of 1054. Growing criticism of the Roman Catholic ecclesiological structure and its behaviour led to the Protestant movement of the 16th century and the split of western Christianity. Since the Renaissance era, with colonialism inspired by the Church, Christianity has expanded throughout the world. Today there are more than two billion Christians worldwide, and Christianity has become the world's largest religion. Within the last century, as the influence of Christianity has waned in the West, it has rapidly grown in the East and the Global South in China, South Korea and much of sub-Saharan Africa.

The religious climate of 1st century Judea was diverse, with numerous Judaic sects. The ancient historian Josephus describes four prominent groups in the Judaism of the time: Pharisees, Sadducees, Essenes and Zealots. This led to unrest, and the 1st century BC and 1st century AD had numerous charismatic religious leaders, contributing to what would become the Mishnah of rabbinic Judaism, including Yohanan ben Zakkai and Hanina ben Dosa. Jewish messianism, and the Jewish messiah concept, has its roots in the apocalyptic literature of the 2nd century BC to 1st century BC, promising a future "anointed" leader (messiah or king) from the Davidic line to resurrect the Israelite Kingdom of God, in place of the foreign rulers of the time.

The main sources of information regarding Jesus' life and teachings are the four canonical gospels, and to a lesser extent the Acts of the Apostles and the Pauline epistles. According to the Gospels, Jesus was a Jewish teacher and healer who was crucified c.30–33 AD. His followers believe that he is the son of God, and lived, died, and was raised from death for the forgiveness of sin.

Early Christianity is generally reckoned by church historians to begin with the ministry of Jesus ( 27-30) and end with the First Council of Nicaea (325). It is typically divided into two periods: the "Apostolic Age" ( 30–100, when the first apostles were still alive) and the "Ante-Nicene Period" ( 100–325).

The Apostolic Age is named after the Apostles and their missionary activities. It holds special significance in Christian tradition as the age of the direct apostles of Jesus. A primary source for the Apostolic Age is the Acts of the Apostles, but its historical accuracy is questionable and its coverage is partial, focusing especially from onwards on the ministry of Paul, and ending around 62 AD with Paul preaching in Rome under house arrest.

The earliest followers of Jesus were apocalyptic Jewish Christians. The early Christian groups were strictly Jewish, such as the Ebionites and the early Christian community in Jerusalem, led by James, the brother of Jesus. According to , they described themselves as "disciples of the Lord" and [followers] "of the Way", and according to a settled community of disciples at Antioch were the first to be called "Christians". Some of the early Christian communities attracted gentile God-fearers, who already visited Jewish synagogues. The inclusion of gentiles posed a problem, as they could not fully observe the Halakha. Saul of Tarsus, commonly known as Paul the Apostle, persecuted the early Jewish Christians, then converted and started his mission among the gentiles. The main concern of Paul's letters is the inclusion of gentiles into God's New Covenant, deeming faith in Christ sufficient for righteousness. Because of this inclusion of gentiles, early Christianity changed its character and gradually grew apart from Judaism and Jewish Christianity during the first two centuries of the Christian Era.

The Gospels and New Testament epistles contain early creeds and hymns, as well as accounts of the Passion, the empty tomb, and Resurrection appearances. Early Christianity slowly spread to pockets of believers among Aramaic-speaking peoples along the Mediterranean coast and also to the inland parts of the Roman Empire and beyond, into the Parthian Empire and the later Sasanian Empire, including Mesopotamia, which was dominated at different times and to varying extent by these empires.

The ante-Nicene period (literally meaning "before Nicaea") was the period following the Apostolic Age down to the First Council of Nicaea in 325. By the beginning of the Nicene period, the Christian faith had spread throughout Western Europe and the Mediterranean Basin, and to North Africa and the East. A more formal Church structure grew out of the early communities, and variant Christian doctrines developed. Christianity grew apart from Judaism, creating its own identity by an increasingly harsh rejection of Judaism and of Jewish practices.

The number of Christians grew by approximately 40% per decade during the first and second centuries. In the post-Apostolic church a hierarchy of clergy gradually emerged as overseers of urban Christian populations took on the form of "episkopoi" (overseers, the origin of the terms bishop and episcopal) and "presbyters" (elders; the origin of the term priest) and then "deacons" (servants). But this emerged slowly and at different times in different locations. Clement, a 1st-century bishop of Rome, refers to the leaders of the Corinthian church in his epistle to Corinthians as bishops and presbyters interchangeably. The New Testament writers also use the terms overseer and elders interchangeably and as synonyms.

The Ante-Nicene period saw the rise of a great number of Christian sects, cults and movements with strong unifying characteristics lacking in the apostolic period. They had different interpretations of Scripture, particularly the divinity of Jesus and the nature of the Trinity. Many variations in this time defy neat categorizations, as various forms of Christianity interacted in a complex fashion to form the dynamic character of Christianity in this era. The Post-Apostolic period was diverse both in terms of beliefs and practices. In addition to the broad spectrum of general branches of Christianity, there was constant change and diversity that variably resulted in both internecine conflicts and syncretic adoption.

The Pauline epistles were circulating in collected form by the end of the 1st century. By the early 3rd century, there existed a set of Christian writings similar to the current New Testament, though there were still disputes over the canonicity of Hebrews, James, II Peter, II and III John, and Revelation. By the 4th century, there existed unanimity in the West concerning the New Testament canon, and by the 5th century the East, with a few exceptions, had come to accept the Book of Revelation and thus had come into harmony on the matter of the canon.

As Christianity spread, it acquired certain members from well-educated circles of the Hellenistic world; they sometimes became bishops. They produced two sorts of works, theological and apologetic, the latter being works aimed at defending the faith by using reason to refute arguments against the veracity of Christianity. These authors are known as the Church Fathers, and study of them is called patristics. Notable early fathers include Ignatius of Antioch, Polycarp, Justin Martyr, Irenaeus, Tertullian, Clement of Alexandria, and Origen.

Christian art emerged relatively late and the first known Christian images emerge from about 200 AD, though there is some literary evidence that small domestic images were used earlier. The oldest known Christian paintings are from the Roman catacombs, dated to about 200, and the oldest Christian sculptures are from sarcophagi, dating to the beginning of the 3rd century.

Although many Hellenistic Jews seem to have had images of religious figures, as at the Dura-Europos synagogue, the traditional Mosaic prohibition of "graven images" no doubt retained some effect, although never proclaimed by theologians. This early rejection of images, and the necessity to hide Christian practise from persecution, leaves few archaeological records regarding early Christianity and its evolution.

There was no empire-wide persecution of Christians until the reign of Decius in the third century. The last and most severe persecution organised by the imperial authorities was the Diocletianic Persecution, 303–311. The Edict of Serdica was issued in 311 by the Roman Emperor Galerius, officially ending the persecution in the East. With the passage in 313 AD of the Edict of Milan, in which the Roman Emperors Constantine the Great and Licinius legalised the Christian religion, persecution of Christians by the Roman state ceased.

How much Christianity Constantine adopted at this point is difficult to discern, but his accession was a turning point for the Christian Church. He supported the Church financially, built various basilicas, granted privileges (e.g., exemption from certain taxes) to clergy, promoted Christians to some high offices, and returned confiscated property. Constantine played an active role in the leadership of the Church. In 316, he acted as a judge in a North African dispute concerning the Donatist controversy. More significantly, in 325 he summoned the Council of Nicaea, the first ecumenical council. He thus established a precedent for the emperor as responsible to God for the spiritual health of his subjects, and thus with a duty to maintain orthodoxy. He was to enforce doctrine, root out heresy, and uphold ecclesiastical unity.

Constantine's son's successor, his nephew Julian, under the influence of his adviser Mardonius, renounced Christianity and embraced a Neo-platonic and mystical form of paganism, shocking the Christian establishment. He began reopening pagan temples, modifying them to resemble Christian traditions such as the episcopal structure and public charity (previously unknown in Roman paganism). Julian's short reign ended when he died in battle with the Persians.

Arianism was a popular doctrine of the 4th century, which was the denial of the divinity of Christ as propounded by Arius. Although this doctrine was condemned as heresy and eventually eliminated by the Roman Church, it remained popular underground for some time. In the late 4th century, Ulfilas, a Roman bishop and an Arian, was appointed as the first bishop to the Goths, the Germanic peoples in much of Europe at the borders of and within the Empire. Ulfilas spread Arian Christianity among the Goths, firmly establishing the faith among many of the Germanic tribes, thus helping to keep them culturally distinct.

During this age, the first ecumenical councils were convened. They were mostly concerned with Christological disputes. The First Council of Nicaea (325) and the First Council of Constantinople (381) resulted in condemnation of Arian teachings as heresy and produced the Nicene Creed.

On 27 February 380, with the Edict of Thessalonica put forth under Theodosius I, Gratian, and Valentinian II, the Roman Empire officially adopted Trinitarian Christianity as its state religion. Prior to this date, Constantius II and Valens had personally favoured Arian or Semi-Arian forms of Christianity, but Valens' successor Theodosius I supported the Trinitarian doctrine as expounded in the Nicene Creed.

After its establishment, the Church adopted the same organisational boundaries as the Empire: geographical provinces, called dioceses, corresponding to imperial government territorial divisions. The bishops, who were located in major urban centres as in pre-legalisation tradition, thus oversaw each diocese. The bishop's location was his "seat", or "see". Among the sees, five came to hold special eminence: Rome, Constantinople, Jerusalem, Antioch, and Alexandria. The prestige of most of these sees depended in part on their apostolic founders, from whom the bishops were therefore the spiritual successors. Though the bishop of Rome was still held to be the First among equals, Constantinople was second in precedence as the new capital of the empire.

Theodosius I decreed that others not believing in the preserved "faithful tradition", such as the Trinity, were to be considered to be practitioners of illegal heresy, and in 385, this resulted in the first case of the state, not Church, infliction of capital punishment on a heretic, namely Priscillian.

During the early 5th century, the School of Edessa had taught a Christological perspective stating that Christ's divine and human nature were distinct persons. A particular consequence of this perspective was that Mary could not be properly called the mother of God but could only be considered the mother of Christ. The most widely known proponent of this viewpoint was the Patriarch of Constantinople Nestorius. Since referring to Mary as the mother of God had become popular in many parts of the Church this became a divisive issue.

The Roman Emperor Theodosius II called for the Council of Ephesus (431), with the intention of settling the issue. The council ultimately rejected Nestorius' view. Many churches who followed the Nestorian viewpoint broke away from the Roman Church, causing a major schism. The Nestorian churches were persecuted, and many followers fled to the Sasanian Empire where they were accepted. The Sasanian (Persian) Empire had many Christian converts early in its history tied closely to the Syriac branch of Christianity. The Empire was officially Zoroastrian and maintained a strict adherence to this faith in part to distinguish itself from the religion of the Roman Empire (originally the pagan Roman religion and then Christianity). Christianity became tolerated in the Sasanian Empire, and as the Roman Empire increasingly exiled heretics during the 4th and 6th centuries, the Sasanian Christian community grew rapidly. By the end of the 5th century, the Persian Church was firmly established and had become independent of the Roman Church. This church evolved into what is today known as the Church of the East.

In 451, the Council of Chalcedon was held to further clarify the Christological issues surrounding Nestorianism. The council ultimately stated that Christ's divine and human nature were separate but both part of a single entity, a viewpoint rejected by many churches who called themselves miaphysites. The resulting schism created a communion of churches, including the Armenian, Syrian, and Egyptian churches. Though efforts were made at reconciliation in the next few centuries, the schism remained permanent, resulting in what is today known as Oriental Orthodoxy.

Monasticism is a form of asceticism whereby one renounces worldly pursuits and goes off alone as a hermit or joins a tightly organized community. It began early in the Church as a family of similar traditions, modelled upon Scriptural examples and ideals, and with roots in certain strands of Judaism. John the Baptist is seen as an archetypical monk, and monasticism was inspired by the organisation of the Apostolic community as recorded in Acts 2:42-47.

Eremetic monks, or hermits, live in solitude, whereas cenobitics live in communities, generally in a monastery, under a rule (or code of practice) and are governed by an abbot. Originally, all Christian monks were hermits, following the example of Anthony the Great. However, the need for some form of organised spiritual guidance lead Pachomius in 318 to organise his many followers in what was to become the first monastery. Soon, similar institutions were established throughout the Egyptian desert as well as the rest of the eastern half of the Roman Empire. Women were especially attracted to the movement. Central figures in the development of monasticism were Basil the Great in the East and, in the West, Benedict, who created the famous Rule of Saint Benedict, which would become the most common rule throughout the Middle Ages and the starting point for other monastic rules.

The transition into the Middle Ages was a gradual and localised process. Rural areas rose as power centres whilst urban areas declined. Although a greater number of Christians remained in the East (Greek areas), important developments were underway in the West (Latin areas) and each took on distinctive shapes. The bishops of Rome, the popes, were forced to adapt to drastically changing circumstances. Maintaining only nominal allegiance to the emperor, they were forced to negotiate balances with the "barbarian rulers" of the former Roman provinces. In the East, the Church maintained its structure and character and evolved more slowly.

The stepwise loss of Western Roman Empire dominance, replaced with foederati and Germanic kingdoms, coincided with early missionary efforts into areas not controlled by the collapsing empire. As early as in the 5th century, missionary activities from Roman Britain into the Celtic areas (Scotland, Ireland and Wales) produced competing early traditions of Celtic Christianity, that was later reintegrated under the Church in Rome. Prominent missionaries were Saints Patrick, Columba and Columbanus. The Anglo-Saxon tribes that invaded southern Britain some time after the Roman abandonment were initially pagan but were converted to Christianity by Augustine of Canterbury on the mission of Pope Gregory the Great. Soon becoming a missionary centre, missionaries such as Wilfrid, Willibrord, Lullus and Boniface converted their Saxon relatives in Germania.

The largely Christian Gallo-Roman inhabitants of Gaul (modern France) were overrun by the Franks in the early 5th century. The native inhabitants were persecuted until the Frankish King Clovis I converted from paganism to Roman Catholicism in 496. Clovis insisted that his fellow nobles follow suit, strengthening his newly established kingdom by uniting the faith of the rulers with that of the ruled. After the rise of the Frankish Kingdom and the stabilizing political conditions, the Western part of the Church increased the missionary activities, supported by the Merovingian kingdom as a means to pacify troublesome neighbour peoples. After the foundation of a church in Utrecht by Willibrord, backlashes occurred when the pagan Frisian King Radbod destroyed many Christian centres between 716 and 719. In 717, the English missionary Boniface was sent to aid Willibrord, re-establishing churches in Frisia and continuing missions in Germany.

Following a series of heavy military reverses against the Muslims, Iconoclasm emerged in the early 8th century. In the 720s, the Byzantine Emperor Leo III the Isaurian banned the pictorial representation of Christ, saints, and biblical scenes. In the West, Pope Gregory III held two synods at Rome and condemned Leo's actions. The Byzantine Iconoclast Council, held at Hieria in 754, ruled that holy portraits were heretical. The movement destroyed much of the Christian church's early artistic history. The iconoclastic movement was later defined as heretical in 787 under the Second Council of Nicaea (the seventh ecumenical council) but had a brief resurgence between 815 and 842.

The Carolingian Renaissance was a period of intellectual and cultural revival of literature, arts, and scriptural studies during the late 8th and 9th centuries, mostly during the reigns of Charlemagne and Louis the Pious, Frankish rulers. To address the problems of illiteracy among clergy and court scribes, Charlemagne founded schools and attracted the most learned men from all of Europe to his court.

Tensions in Christian unity started to become evident in the 4th century. Two basic problems were involved: the nature of the primacy of the bishop of Rome and the theological implications of adding a clause to the Nicene Creed, known as the "filioque" clause. These doctrinal issues were first openly discussed in Photius's patriarchate. The Eastern churches viewed Rome's understanding of the nature of episcopal power as being in direct opposition to the Church's essentially conciliar structure and thus saw the two ecclesiologies as mutually antithetical.

Another issue developed into a major irritant to Eastern Christendom, the gradual introduction into the Nicene Creed in the West of the "Filioque" clause – meaning "and the Son" – as in "the Holy Spirit ... proceeds from the Father "and the Son"", where the original Creed, sanctioned by the councils and still used today by the Eastern Orthodox, simply states "the Holy Spirit, ... proceeds from the Father." The Eastern Church argued that the phrase had been added unilaterally and therefore illegitimately, since the East had never been consulted. In addition to this ecclesiological issue, the Eastern Church also considered the" Filioque" clause unacceptable on dogmatic grounds.

In the 9th century, a controversy arose between Eastern (Byzantine, Greek Orthodox) and Western (Latin, Roman Catholic) Christianity that was precipitated by the opposition of the Roman Pope John VII to the appointment by the Byzantine Emperor Michael III of Photios I to the position of patriarch of Constantinople. Photios was refused an apology by the pope for previous points of dispute between the East and West. Photios refused to accept the supremacy of the pope in Eastern matters or accept the "Filioque" clause. The Latin delegation at the council of his consecration pressed him to accept the clause in order to secure their support. The controversy also involved Eastern and Western ecclesiastical jurisdictional rights in the Bulgarian church. Photios did provide concession on the issue of jurisdictional rights concerning Bulgaria, and the papal legates made do with his return of Bulgaria to Rome. This concession, however, was purely nominal, as Bulgaria's return to the Byzantine rite in 870 had already secured for it an autocephalous church. Without the consent of Boris I of Bulgaria, the papacy was unable to enforce any of its claims.

The East–West Schism, or Great Schism, separated the Church into Western (Latin) and Eastern (Greek) branches, i.e., Western Catholicism and Eastern Orthodoxy. It was the first major division since certain groups in the East rejected the decrees of the Council of Chalcedon (see Oriental Orthodoxy) and was far more significant. Though normally dated to 1054, the East–West Schism was actually the result of an extended period of estrangement between Latin and Greek Christendom over the nature of papal primacy and certain doctrinal matters like the "Filioque", but intensified from cultural and linguistic differences.

From the 6th century onward, most of the monasteries in the West were of the Benedictine Order. Owing to the stricter adherence to a reformed Benedictine rule, the abbey of Cluny became the acknowledged leader of western monasticism from the later 10th century. Cluny created a large, federated order in which the administrators of subsidiary houses served as deputies of the abbot of Cluny and answered to him. The Cluniac spirit was a revitalising influence on the Norman church, at its height from the second half of the 10th century through the early 12th century.
The next wave of monastic reform came with the Cistercian Movement. The first Cistercian abbey was founded in 1098, at Cîteaux Abbey. The keynote of Cistercian life was a return to a literal observance of the Benedictine rule, rejecting the developments of the Benedictines. The most striking feature in the reform was the return to manual labour, and especially to field-work. Inspired by Bernard of Clairvaux, the primary builder of the Cistercians, they became the main force of technological diffusion in medieval Europe. By the end of the 12th century, the Cistercian houses numbered 500, and at its height in the 15th century the order claimed to have close to 750 houses. Most of these were built in wilderness areas, and played a major part in bringing such isolated parts of Europe into economic cultivation.

A third level of monastic reform was provided by the establishment of the Mendicant orders. Commonly known as friars, mendicants live under a monastic rule with traditional vows of poverty, chastity, and obedience but they emphasise preaching, missionary activity, and education, in a secluded monastery. Beginning in the 12th century, the Franciscan order was instituted by the followers of Francis of Assisi, and thereafter the Dominican order was begun by St. Dominic.

The Investiture Controversy, or Lay Investiture Controversy, was the most significant conflict between secular and religious powers in medieval Europe. It began as a dispute in the 11th century between the Holy Roman Emperor Henry IV and Pope Gregory VII concerning who would appoint bishops (investiture). The end of lay investiture threatened to undercut the power of the Empire and the ambitions of noblemen. Bishoprics being merely lifetime appointments, a king could better control their powers and revenues than those of hereditary noblemen. Even better, he could leave the post vacant and collect the revenues, theoretically in trust for the new bishop, or give a bishopric to pay a helpful noble. The Church wanted to end lay investiture to end this and other abuses, to reform the episcopate and provide better pastoral care. Pope Gregory VII issued the "Dictatus Papae", which declared that the pope alone could appoint bishops. Henry IV's rejection of the decree led to his excommunication and a ducal revolt. Eventually Henry received absolution after dramatic public penance, though the Great Saxon Revolt and conflict of investiture continued.

A similar controversy occurred in England between King Henry I and St. Anselm, Archbishop of Canterbury, over investiture and episcopal vacancy. The English dispute was resolved by the Concordat of London, 1107, where the king renounced his claim to invest bishops but continued to require an oath of fealty. This was a partial model for the Concordat of Worms ("Pactum Calixtinum"), which resolved the Imperial investiture controversy with a compromise that allowed secular authorities some measure of control but granted the selection of bishops to their cathedral canons. As a symbol of the compromise, both ecclesiastical and lay authorities invested bishops with respectively, the staff and the ring.

Generally, the Crusades refer to the campaigns in the Holy Land sponsored by the papacy against Muslim forces. There were other crusades against Islamic forces in southern Spain, southern Italy, and Sicily, as well as the campaigns of Teutonic Knights against pagan strongholds in north-eastern Europe. A few crusades were waged within Christendom against groups that were considered heretical and schismatic.

The Holy Land had been part of the Roman Empire, and thus Byzantine Empire, until the Islamic conquests of the 7th and 8th centuries. Thereafter, Christians had generally been permitted to visit the sacred places in the Holy Land until 1071, when the Seljuk Turks closed Christian pilgrimages and assailed the Byzantines, defeating them at the Battle of Manzikert. Emperor Alexius I asked for aid from Pope Urban II against Islamic aggression. He probably expected money from the pope for the hiring of mercenaries. Instead, Urban II called upon the knights of Christendom in a speech made at the Council of Clermont on 27 November 1095, combining the idea of pilgrimage to the Holy Land with that of waging a holy war against infidels.

The First Crusade captured Antioch in 1099 and then Jerusalem. The Second Crusade occurred in 1145 when Edessa was retaken by Islamic forces. Jerusalem was held until 1187 and the Third Crusade, famous for the battles between Richard the Lionheart and Saladin. The Fourth Crusade, begun by Innocent III in 1202, intended to retake the Holy Land but was soon subverted by Venetians who used the forces to sack the Christian city of Zara. When the crusaders arrived in Constantinople, they sacked the city and other parts of Asia Minor and established the Latin Empire of Constantinople in Greece and Asia Minor. This was effectively the last crusade sponsored by the papacy, with later crusades being sponsored by individuals.

Jerusalem was held by the crusaders for nearly a century, while other strongholds in the Near East remained in Christian possession much longer. The crusades in the Holy Land ultimately failed to establish permanent Christian kingdoms. Islamic expansion into Europe remained a threat for centuries, culminating in the campaigns of Suleiman the Magnificent in the 16th century. Crusades in southern Spain, southern Italy, and Sicily eventually lead to the demise of Islamic power in Europe. Teutonic Knights expanded Christian domains in Eastern Europe, and the much less frequent crusades within Christendom, such as the Albigensian Crusade, achieved their goal of maintaining doctrinal unity.

The Medieval Inquisition was a series of inquisitions (Roman Catholic Church bodies charged with suppressing heresy) from around 1184, including the Episcopal Inquisition (1184–1230s) and later the Papal Inquisition (1230s). It was in response to movements within Europe considered apostate or heretical to Western Catholicism, in particular the Cathars and the Waldensians in southern France and northern Italy. These were the first inquisition movements of many that would follow. The inquisitions in combination with the Albigensian Crusade were fairly successful in ending heresy. Historian Thomas F. Madden has written about popular myths regarding the inquisition.

Early evangelisation in Scandinavia was begun by Ansgar, Archbishop of Bremen, "Apostle of the North". Ansgar, a native of Amiens, was sent with a group of monks to Jutland in around 820 at the time of the pro-Christian King Harald Klak. The mission was only partially successful, and Ansgar returned two years later to Germany, after Harald had been driven out of his kingdom. In 829, Ansgar went to Birka on Lake Mälaren, Sweden, with his aide friar Witmar, and a small congregation was formed in 831 which included the king's steward Hergeir. Conversion was slow, however, and most Scandinavian lands were only completely Christianised at the time of rulers such as Saint Canute IV of Denmark and Olaf I of Norway in the years following AD 1000.

The Christianisation of the Slavs was initiated by one of Byzantium's most learned churchmen – the patriarch Photios I of Constantinople. The Byzantine Emperor Michael III chose Cyril and Methodius in response to a request from King Rastislav of Moravia, who wanted missionaries that could minister to the Moravians in their own language. The two brothers spoke the local Slavonic vernacular and translated the Bible and many of the prayer books. As the translations prepared by them were copied by speakers of other dialects, the hybrid literary language Old Church Slavonic was created, which later evolved into Church Slavonic and is the common liturgical language still used by the Russian Orthodox Church and other Slavic Orthodox Christians. Methodius went on to convert the Serbs. 
Bulgaria was a pagan country since its establishment in 681 until 864 when Boris I converted to Christianity. The reasons for that decision were complex; the most important factors were that Bulgaria was situated between two powerful Christian empires, Byzantium and East Francia; Christian doctrine particularly favoured the position of the monarch as God's representative on Earth, while Boris also saw it as a way to overcome the differences between Bulgars and Slavs. Bulgaria was officially recognised as a patriarchate by Constantinople in 927, Serbia in 1346, and Russia in 1589. All of these nations, however, had been converted long before these dates. 

The Avignon Papacy, sometimes referred to as the Babylonian Captivity, was a period from 1309 to 1378 during which seven popes resided in Avignon, in modern-day France. In 1309, Pope Clement V moved to Avignon in southern France. Confusion and political animosity waxed, as the prestige and influence of Rome waned without a resident pontiff. Troubles reached their peak in 1378 when Gregory XI died while visiting Rome. A papal conclave met in Rome and elected Urban VI, an Italian. Urban soon alienated the French cardinals, and they held a second conclave electing Robert of Geneva to succeed Gregory XI, beginning the Western Schism.

John Wycliffe, an English scholar and alleged heretic best known for denouncing the corruptions of the Church, was a precursor of the Protestant Reformation. He emphasized the supremacy of the Bible and called for a direct relationship between God and the human person, without interference by priests and bishops. His followers played a role in the English Reformation. Jan Hus, a Czech theologian in Prague, was influenced by Wycliffe and spoke out against the corruptions he saw in the Church. He was a forerunner of the Protestant Reformation, and his legacy has become a powerful symbol of Czech culture in Bohemia.

The Renaissance was a period of great cultural change and achievement, marked in Italy by a classical orientation and an increase of wealth through mercantile trade. The city of Rome, the papacy, and the papal states were all affected by the Renaissance. On the one hand, it was a time of great artistic patronage and architectural magnificence, where the Church commissioned such artists as Michelangelo, Brunelleschi, Bramante, Raphael, Fra Angelico, Donatello, and da Vinci. On the other hand, wealthy Italian families often secured episcopal offices, including the papacy, for their own members, some of whom were known for immorality, such as Alexander VI and Sixtus IV.

In addition to being the head of the Church, the pope became one of Italy's most important secular rulers, and pontiffs such as Julius II often waged campaigns to protect and expand their temporal domains. Furthermore, the popes, in a spirit of refined competition with other Italian lords, spent lavishly both on private luxuries but also on public works, repairing or building churches, bridges, and a magnificent system of aqueducts in Rome that still function today.

In 1453, Constantinople fell to the Ottoman Empire. Eastern Christians fleeing Constantinople, and the Greek manuscripts they carried with them, is one of the factors that prompted the literary renaissance in the West at about this time. The Ottoman government followed Islamic law when dealing with the conquered Christian population. Christians were officially tolerated as people of the Book. As such, the Church's canonical and hierarchical organisation were not significantly disrupted, and its administration continued to function. One of the first things that Mehmet the Conqueror did was to allow the Church to elect a new patriarch, Gennadius Scholarius. However, these rights and privileges, including freedom of worship and religious organisation, were often established in principle but seldom corresponded to reality. Christians were viewed as second-class citizens, and the legal protections they depended upon were subject to the whims of the sultan and the sublime porte. The Hagia Sophia and the Parthenon, which had been Christian churches for nearly a millennium, were converted into mosques. Violent persecutions of Christians were common and reached their climax in the Armenian, Assyrian, and Greek genocides.

In the early 16th century, attempts were made by the theologians Martin Luther and Huldrych Zwingli, along with many others, to reform the Church. They considered the root of corruptions to be doctrinal, rather than simply a matter of moral weakness or lack of ecclesiastical discipline, and thus advocated for God's autonomy in redemption, and against voluntaristic notions that salvation could be earned by people. The Reformation is usually considered to have started with the publication of the "Ninety-five Theses" by Luther in 1517, although there was no schism until the 1521 Diet of Worms. The edicts of the Diet condemned Luther and officially banned citizens of the Holy Roman Empire from defending or propagating his ideas.

The word "Protestant" is derived from the Latin "protestatio", meaning "declaration", which refers to the letter of protestation by Lutheran princes against the decision of the Diet of Speyer in 1529, which reaffirmed the edict of the Diet of Worms ordering the seizure of all property owned by persons guilty of advocating Lutheranism. The term "Protestant" was not originally used by Reformation era leaders; instead, they called themselves "evangelical", emphasising the "return to the true gospel (Greek: "euangelion")."

Early protest was against corruptions such as simony, the holding of multiple church offices by one person at the same time, episcopal vacancies, and the sale of indulgences. The Protestant position also included "sola scriptura, sola fide", the priesthood of all believers, Law and Gospel, and the two kingdoms doctrine. The three most important traditions to emerge directly from the Reformation were the Lutheran, Reformed, and Anglican traditions, though the latter group identifies as both "Reformed" and "Catholic", and some subgroups reject the classification as "Protestant".

Unlike other reform movements, the English Reformation began by royal influence. Henry VIII considered himself a thoroughly Catholic king, and in 1521 he defended the papacy against Luther in a book he commissioned entitled, "The Defence of the Seven Sacraments", for which Pope Leo X awarded him the title "Fidei Defensor" (Defender of the Faith). However, the king came into conflict with the papacy when he wished to annul his marriage with Catherine of Aragon, for which he needed papal sanction. Catherine, among many other noble relations, was the aunt of Emperor Charles V, the papacy's most significant secular supporter. The ensuing dispute eventually lead to a break from Rome and the declaration of the King of England as head of the English Church, which saw itself as a Protestant Church navigating a middle way between Lutheranism and Reformed Christianity, but leaning more towards the latter. Consequently, England experienced periods of reform and also Counter-Reformation. Monarchs such as Edward VI, Lady Jane Grey, Mary I, Elizabeth I, and Archbishops of Canterbury such as Thomas Cranmer and William Laud pushed the Church of England in different directions over the course of only a few generations. What emerged was the Elizabethan Religious Settlement and a state church that considered itself both "Reformed" and "Catholic" but not "Roman", and other unofficial more radical movements such as the Puritans. In terms of politics, the English Reformation included heresy trials, the exiling of Roman Catholic populations to Spain and other Roman Catholic lands, and censorship and prohibition of books.

The Counter-Reformation was the response of the Catholic Church to the Protestant Reformation. In terms of meetings and documents, it consisted of the "Confutatio Augustana", the Council of Trent, the "Roman Catechism", and the "Defensio Tridentinæ fidei". In terms of politics, the Counter-Reformation included heresy trials, the exiling of Protestant populations from Catholic lands, the seizure of children from their Protestant parents for institutionalized Catholic upbringing, a series of wars, the "Index Librorum Prohibitorum" (the list of prohibited books), and the Spanish Inquisition. 

Although Protestants were excommunicated in an attempt to reduce their influence within the Catholic Church, at the same time they were persecuted during the Counter-Reformation, prompting some to live as crypto-Protestants (also termed Nicodemites, against the urging of John Calvin who urged them to live their faith openly. Crypto-Protestants were documented as late as the 19th century in Latin America.
The Council of Trent (1545–1563) initiated by Pope Paul III addressed issues of certain ecclesiastical corruptions such as simony, absenteeism, nepotism, the holding of multiple church offices by one person, and other abuses. It also reasserted traditional practices and doctrines of the Church, such as the episcopal structure, clerical celibacy, the seven Sacraments, transubstantiation (the belief that during mass the consecrated bread and wine truly become the body and blood of Christ), the veneration of relics, icons, and saints (especially the Blessed Virgin Mary), the necessity of both faith and good works for salvation, the existence of purgatory and the issuance (but not the sale) of indulgences. In other words, all Protestant doctrinal objections and changes were uncompromisingly rejected. The Council also fostered an interest in education for parish priests to increase pastoral care. Milan's Archbishop Saint Charles Borromeo set an example by visiting the remotest parishes and instilling high standards.

Simultaneous to the Counter-Reformation, the Catholic Reformation consisted of improvements in art and culture, anti-corruption measures, the founding of the Jesuits, the establishment of seminaries, a reassertion of traditional doctrines and the emergence of new religious orders aimed at both moral reform and new missionary activity. Also part of this was the development of new yet orthodox forms of spirituality, such as that of the Spanish mystics and the French school of spirituality.

The papacy of St. Pius V was known not only for its focus on halting heresy and worldly abuses within the Church, but also for its focus on improving popular piety in a determined effort to stem the appeal of Protestantism. Pius began his pontificate by giving large alms to the poor, charity, and hospitals, and the pontiff was known for consoling the poor and sick as well as supporting missionaries. These activities coincided with a rediscovery of the ancient Christian catacombs in Rome. As Diarmaid MacCulloch states, "Just as these ancient martyrs were revealed once more, Catholics were beginning to be martyred afresh, both in mission fields overseas and in the struggle to win back Protestant northern Europe: the catacombs proved to be an inspiration for many to action and to heroism."

Catholic missions were carried to new places beginning with the new Age of Discovery, and the Roman Catholic Church established missions in the Americas.

The Galileo affair, in which Galileo Galilei came into conflict with the Roman Catholic Church over his support of Copernican astronomy, is often considered a defining moment in the history of the relationship between religion and science. In 1610, Galileo published his "Sidereus Nuncius (Starry Messenger)", describing the surprising observations that he had made with the new telescope. These and other discoveries exposed major difficulties with the understanding of the heavens that had been held since antiquity, and raised new interest in radical teachings such as the heliocentric theory of Copernicus. In reaction, many scholars maintained that the motion of the earth and immobility of the sun were heretical, as they contradicted some accounts given in the Bible as understood at that time. Galileo's part in the controversies over theology, astronomy and philosophy culminated in his trial and sentencing in 1633, on a grave suspicion of heresy.

The most famous colonisation by Protestants in the New World was that of English Puritans in North America. Unlike the Spanish or French, the English colonists made surprisingly little effort to evangelise the native peoples. The Puritans, or Pilgrims, left England so that they could live in an area with Puritanism established as the exclusive civic religion. Though they had left England because of the suppression of their religious practice, most Puritans had thereafter originally settled in the Low Countries but found the licentiousness there, where the state hesitated from enforcing religious practice, as unacceptable, and thus they set out for the New World and the hopes of a Puritan utopia.

Revivalism refers to the Calvinist and Wesleyan revival, called the Great Awakening in North America, which saw the development of evangelical Congregationalist, Presbyterian, Baptist, and new Methodist churches.

The First Great Awakening was a wave of religious enthusiasm among Protestants in the American colonies c. 1730–1740, emphasising the traditional Reformed virtues of Godly preaching, rudimentary liturgy, and a deep sense of personal guilt and redemption by Christ Jesus. Historian Sydney E. Ahlstrom saw it as part of a "great international Protestant upheaval" that also created pietism in Germany, the Evangelical Revival, and Methodism in England. It centred on reviving the spirituality of established congregations and mostly affected Congregational, Presbyterian, Dutch Reformed, German Reformed, Baptist, and Methodist churches, while also spreading within the slave population. The Second Great Awakening (1800–1830s), unlike the first, focused on the unchurched and sought to instill in them a deep sense of personal salvation as experienced in revival meetings. It also sparked the beginnings of groups such as the Mormons, the Restoration Movement and the Holiness movement. The Third Great Awakening began from 1857 and was most notable for taking the movement throughout the world, especially in English speaking countries. The final group to emerge from the "great awakenings" in North America was Pentecostalism, which had its roots in the Methodist, Wesleyan, and Holiness movements, and began in 1906 on Azusa Street in Los Angeles. Pentecostalism would later lead to the Charismatic movement.

Restorationism refers to the belief that a purer form of Christianity should be restored using the early church as a model. In many cases, restorationist groups believed that contemporary Christianity, in all its forms, had deviated from the true, original Christianity, which they then attempted to "reconstruct", often using the Book of Acts as a "guidebook" of sorts. Restorationists do not usually describe themselves as "reforming" a Christian church continuously existing from the time of Jesus, but as "restoring" the Church that they believe was lost at some point. "Restorationism" is often used to describe the Stone-Campbell Restoration Movement. 

The term "restorationist" is also used to describe the Jehovah's Witness movement, founded in the late 1870s by Charles Taze Russell. The term can also be used to describe the Latter Day Saint movement, including The Church of Jesus Christ of Latter-day Saints (LDS Church), the Community of Christ and numerous other Latter Day Saints sects. Latter Day Saints, also known as Mormons, believe that Joseph Smith was chosen to restore the original organization established by Jesus, now "in its fullness", rather than to reform the church.

The Russian Orthodox Church held a privileged position in the Russian Empire, expressed in the motto of the late empire from 1833: Orthodoxy, Autocracy, and Populism. Nevertheless, the Church reform of Peter I in the early 18th century had placed the Orthodox authorities under the control of the tsar. An ober-procurator appointed by the tsar ran the committee which governed the Church between 1721 and 1918: the Most Holy Synod.
The Church became involved in the various campaigns of russification, and was accused of involvement in Russian anti-semitism, despite the lack of an official position on Judaism as such.

The Bolsheviks and other Russian revolutionaries saw the Church, like the tsarist state, as an enemy of the people. Criticism of atheism was strictly forbidden and sometimes lead to imprisonment. Some actions against Orthodox priests and believers included torture, being sent to prison camps, labour camps or mental hospitals, as well as execution. 

In the first five years after the Bolshevik revolution, 28 bishops and 1,200 priests were executed. This included people like the Grand Duchess Elizabeth Fyodorovna who was at this point a monastic. Executed along with her were: Grand Duke Sergei Mikhailovich Romanov; the Princes Ioann Konstantinvich, Konstantin Konstantinovich, Igor Konstantinovich and Vladimir Pavlovich Paley; Grand Duke Sergei's secretary, Fyodor Remez; and Varvara Yakovleva, a sister from the Grand Duchess Elizabeth's convent.

Liberal Christianity, sometimes called liberal theology, is an umbrella term covering diverse, philosophically informed religious movements and moods within late 18th, 19th and 20th-century Christianity. The word "liberal" in liberal Christianity does not refer to a leftist "political" agenda or set of beliefs, but rather to the freedom of dialectic process associated with continental philosophy and other philosophical and religious paradigms developed during the Age of Enlightenment.

Fundamentalist Christianity is a movement that arose mainly within British and American Protestantism in the late 19th century and early 20th century in reaction to modernism and certain liberal Protestant groups that denied doctrines considered fundamental to Christianity yet still called themselves "Christian." Thus, fundamentalism sought to re-establish tenets that could not be denied without relinquishing a Christian identity, the "fundamentals": inerrancy of the Bible, "Sola Scriptura", the Virgin Birth of Jesus, the doctrine of substitutionary atonement, the bodily Resurrection of Jesus, and the imminent return of Jesus Christ.

Under the state atheism of countries in the Eastern Bloc, Christians of many denominations experienced persecution, with many churches and monasteries being destroyed, as well as clergy being executed.

The position of Christians affected by Nazism is highly complex. Pope Pius XI declared – "Mit brennender Sorge" – that Fascist governments had hidden "pagan intentions" and expressed the irreconcilability of the Catholic position and totalitarian fascist state worship, which placed the nation above God, fundamental human rights, and dignity. His declaration that "Spiritually, [Christians] are all Semites" prompted the Nazis to give him the title "Chief Rabbi of the Christian World."

Catholic priests were executed in concentration camps alongside Jews; for example, 2,600 Catholic priests were imprisoned in Dachau, and 2,000 of them were executed (cf. "Priesterblock"). A further 2,700 Polish priests were executed (a quarter of all Polish priests), and 5,350 Polish nuns were either displaced, imprisoned, or executed. Many Catholic laymen and clergy played notable roles in sheltering Jews during the Holocaust, including Pope Pius XII. The head rabbi of Rome became a Catholic in 1945 and, in honour of the actions the pope undertook to save Jewish lives, he took the name Eugenio (the pope's first name). A former Israeli consul in Italy claimed: "The Catholic Church saved more Jewish lives during the war than all the other churches, religious institutions, and rescue organisations put together."

The relationship between Nazism and Protestantism, especially the German Lutheran Church, was complex. Though many Protestant church leaders in Germany supported the Nazis' growing anti-Jewish activities, some such as Dietrich Bonhoeffer (a Lutheran pastor) of the Confessing Church, a movement within Protestantism that strongly opposed Nazism, were strongly opposed to the Third Reich. Bonhoeffer was later found guilty in the conspiracy to assassinate Hitler and executed.

On 11 October 1962, Pope John XXIII opened the Second Vatican Council, the 21st ecumenical council of the Catholic Church. The council was "pastoral" in nature, interpreting dogma in terms of its scriptural roots, revising liturgical practices, and providing guidance for articulating traditional Church teachings in contemporary times. The council is perhaps best known for its instructions that the Mass may be celebrated in the vernacular as well as in Latin.

Ecumenism broadly refers to movements between Christian groups to establish a degree of unity through dialogue. Ecumenism is derived from Greek ("oikoumene"), which means "the inhabited world", but more figuratively something like "universal oneness." The movement can be distinguished into Catholic and Protestant movements, with the latter characterised by a redefined ecclesiology of "denominationalism" (which the Catholic Church, among others, rejects).

Over the last century, moves have been made to reconcile the schism between the Catholic Church and the Eastern Orthodox churches. Although progress has been made, concerns over papal primacy and the independence of the smaller Orthodox churches has blocked a final resolution of the schism. On 30 November 1894, Pope Leo XIII published "Orientalium Dignitas". On 7 December 1965, a Joint Catholic-Orthodox Declaration of Pope Paul VI and the Ecumenical Patriarch Athenagoras I was issued lifting the mutual excommunications of 1054.

Some of the most difficult questions in relations with the ancient Eastern Churches concern some doctrine (i.e. "Filioque", scholasticism, functional purposes of asceticism, the essence of God, Hesychasm, Fourth Crusade, establishment of the Latin Empire, Uniatism to note but a few) as well as practical matters such as the concrete exercise of the claim to papal primacy and how to ensure that ecclesiastical union would not mean mere absorption of the smaller Churches by the Latin component of the much larger Catholic Church (the most numerous single religious denomination in the world) and the stifling or abandonment of their own rich theological, liturgical and cultural heritage.

With respect to Catholic relations with Protestant communities, certain commissions were established to foster dialogue and documents have been produced aimed at identifying points of doctrinal unity, such as the Joint Declaration on the Doctrine of Justification produced with the Lutheran World Federation in 1999.

Ecumenical movements within Protestantism have focused on determining a list of doctrines and practices essential to being Christian and thus extending to all groups which fulfill these basic criteria a (more or less) co-equal status, with perhaps one's own group still retaining a "first among equal" standing. This process involved a redefinition of the idea of "the Church" from traditional theology. This ecclesiology, known as denominationalism, contends that each group (which fulfills the essential criteria of "being Christian") is a sub-group of a greater "Christian Church", itself a purely abstract concept with no direct representation, i.e., no group, or "denomination", claims to be "the Church." This ecclesiology is at variance with other groups that indeed consider themselves to be "the Church." The "essential criteria" generally consist of belief in the Trinity, belief that Jesus Christ is the only way to bring forgiveness and eternal life, and that Jesus died and rose again bodily.

In reaction to these developments, Christian fundamentalism was a movement to reject the radical influences of philosophical humanism as this was affecting the Christian religion. Especially targeting critical approaches to the interpretation of the Bible, and trying to blockade the inroads made into their churches by atheistic scientific assumptions, the fundamentalists began to appear in various denominations as numerous independent movements of resistance to the drift away from historic Christianity. Over time, the Fundamentalist Evangelical movement has divided into two main wings, with the label Fundamentalist following one branch, while Evangelical has become the preferred banner of the more moderate movement. Although both movements primarily originated in the English-speaking world, the majority of Evangelicals now live elsewhere in the world.



The following links give an overview of the history of Christianity:

The following links provide quantitative data related to Christianity and other major religions, including rates of adherence at different points in time:


</doc>
<doc id="14121" url="https://en.wikipedia.org/wiki?curid=14121" title="Hertz">
Hertz

The hertz (symbol: Hz) is the derived unit of frequency in the International System of Units (SI) and is defined as one cycle per second. It is named after Heinrich Rudolf Hertz, the first person to provide conclusive proof of the existence of electromagnetic waves. Hertz are commonly expressed in multiples: kilohertz (10 Hz, kHz), megahertz (10 Hz, MHz), gigahertz (10 Hz, GHz), terahertz (10 Hz, THz), petahertz (10 Hz, PHz), exahertz (10 Hz, EHz),
and zettahertz (10 Hz, ZHz).

Some of the unit's most common uses are in the description of sine waves and musical tones, particularly those used in radio- and audio-related applications. It is also used to describe the clock speeds at which computers and other electronics are driven. The units are sometimes also used as a representation of energy, via the photon energy equation ("E"="h"ν), with one hertz equivalent to "h" joules.

The hertz is defined as one cycle per second. The International Committee for Weights and Measures defined the second as "the duration of 9 192 631 770 periods of the radiation corresponding to the transition between the two hyperfine levels of the ground state of the caesium-133 atom" and then adds: "It follows that the hyperfine splitting in the ground state of the caesium 133 atom is exactly 9 192 631 770 hertz, ν(hfs Cs) = 9 192 631 770 Hz." The dimension of the unit hertz is 1/time (1/T). Expressed in base SI units it is 1/second (1/s). Problems can arise because the units of angular measure (cycle or radian) are omitted in SI. 

In English, "hertz" is also used as the plural form. As an SI unit, Hz can be prefixed; commonly used multiples are kHz (kilohertz, 10 Hz), MHz (megahertz, 10 Hz), GHz (gigahertz, 10 Hz) and THz (terahertz, 10 Hz). One hertz simply means "one cycle per second" (typically that which is being counted is a complete cycle); 100 Hz means "one hundred cycles per second", and so on. The unit may be applied to any periodic event—for example, a clock might be said to tick at 1 Hz, or a human heart might be said to beat at 1.2 Hz. 

The occurrence rate of aperiodic or stochastic events is expressed in reciprocal second or inverse second (1/s or s) in general or, in the specific case of radioactive decay, in becquerels. Whereas 1 Hz is 1 cycle per second, 1 Bq is 1 aperiodic radionuclide event per second.

Even though angular velocity, angular frequency and the unit hertz all have the dimension 1/s, angular velocity and angular frequency are not expressed in hertz, but rather in an appropriate angular unit such as radians per second. Thus a disc rotating at 60 revolutions per minute (rpm) is said to be rotating at either 2 rad/s "or" 1 Hz, where the former measures the angular velocity and the latter reflects the number of "complete" revolutions per second. The conversion between a frequency "f" measured in hertz and an angular velocity "ω" measured in radians per second is

The hertz is named after the German physicist Heinrich Hertz (1857–1894), who made important scientific contributions to the study of electromagnetism. The name was established by the International Electrotechnical Commission (IEC) in 1930. It was adopted by the General Conference on Weights and Measures (CGPM) ("Conférence générale des poids et mesures") in 1960, replacing the previous name for the unit, "cycles per second" (cps), along with its related multiples, primarily "kilocycles per second" (kc/s) and "megacycles per second" (Mc/s), and occasionally "kilomegacycles per second" (kMc/s). The term "cycles per second" was largely replaced by "hertz" by the 1970s. One hobby magazine, "Electronics Illustrated", declared their intention to stick with the traditional kc., Mc., etc. units.

Sound is a traveling longitudinal wave which is an oscillation of pressure. Humans perceive frequency of sound waves as pitch. Each musical note corresponds to a particular frequency which can be measured in hertz. An infant's ear is able to perceive frequencies ranging from 20 Hz to 20,000 Hz; the average adult human can hear sounds between 20 Hz and 16,000 Hz. The range of ultrasound, infrasound and other physical vibrations such as molecular and atomic vibrations extends from a few femtohertz into the terahertz range and beyond.

Electromagnetic radiation is often described by its frequency—the number of oscillations of the perpendicular electric and magnetic fields per second—expressed in hertz.

Radio frequency radiation is usually measured in kilohertz (kHz), megahertz (MHz), or gigahertz (GHz). Light is electromagnetic radiation that is even higher in frequency, and has frequencies in the range of tens (infrared) to thousands (ultraviolet) of terahertz. Electromagnetic radiation with frequencies in the low terahertz range (intermediate between those of the highest normally usable radio frequencies and long-wave infrared light) is often called terahertz radiation. Even higher frequencies exist, such as that of gamma rays, which can be measured in exahertz (EHz). (For historical reasons, the frequencies of light and higher frequency electromagnetic radiation are more commonly specified in terms of their wavelengths or photon energies: for a more detailed treatment of this and the above frequency ranges, see electromagnetic spectrum.)

In computers, most central processing units (CPU) are labeled in terms of their clock rate expressed in megahertz (10 Hz) or gigahertz (10 Hz). This specification refers to the frequency of the CPU's master clock signal. This signal is a square wave, which is an electrical voltage that switches between low and high logic values at regular intervals. As the hertz has become the primary unit of measurement accepted by the general populace to determine the performance of a CPU, many experts have criticized this approach, which they claim is an easily manipulable benchmark. Some processors use multiple clock periods to perform a single operation, while others can perform multiple operations in a single cycle. For personal computers, CPU clock speeds have ranged from approximately 1 MHz in the late 1970s (Atari, Commodore, Apple computers) to up to 6 GHz in IBM POWER microprocessors.

Various computer buses, such as the front-side bus connecting the CPU and northbridge, also operate at various frequencies in the megahertz range.

Higher frequencies than the International System of Units provides prefixes for are believed to occur naturally in the frequencies of the quantum-mechanical vibrations of high-energy, or, equivalently, massive particles, although these are not directly observable and must be inferred from their interactions with other phenomena. By convention, these are typically not expressed in hertz, but in terms of the equivalent quantum energy, which is proportional to the frequency by the factor of Planck's constant.




</doc>
<doc id="14123" url="https://en.wikipedia.org/wiki?curid=14123" title="Heroic couplet">
Heroic couplet

A heroic couplet is a traditional form for English poetry, commonly used in epic and narrative poetry, and consisting of a rhyming pair of lines in iambic pentameter. Use of the heroic couplet was pioneered by Geoffrey Chaucer in the "Legend of Good Women" and the "Canterbury Tales", and generally considered to have been perfected by John Dryden and Alexander Pope in the Restoration Age and early 18th century respectively. 

A frequently-cited example illustrating the use of heroic couplets is this passage from "Cooper's Hill" by John Denham, part of his description of the Thames:

The term "heroic couplet" is sometimes reserved for couplets that are largely "closed" and self-contained, as opposed to the enjambed couplets of poets like John Donne. The heroic couplet is often identified with the English Baroque works of John Dryden and Alexander Pope, who used the form for their translations of the epics of Virgil and Homer, respectively. Major poems in the closed couplet, apart from the works of Dryden and Pope, are Samuel Johnson's "The Vanity of Human Wishes", Oliver Goldsmith's "The Deserted Village", and John Keats's "Lamia". The form was immensely popular in the 18th century. The looser type of couplet, with occasional enjambment, was one of the standard verse forms in medieval narrative poetry, largely because of the influence of the Canterbury Tales.

English heroic couplets, especially in Dryden and his followers, are sometimes varied by the use of the occasional alexandrine, or hexameter line, and triplet. Often these two variations are used together to heighten a climax. The breaking of the regular pattern of rhyming pentameter pairs brings about a sense of poetic closure. Here are two examples from Book IV of Dryden's translation of the "Aeneid".
Twentieth-century authors have occasionally made use of the heroic couplet, often as an allusion to the works of poets of previous centuries. An example of this is Vladimir Nabokov's novel "Pale Fire", the second section of which is a 999-line, 4-canto poem largely written in loose heroic couplets with frequent enjambment. Here is an example from the first canto:


</doc>
<doc id="14127" url="https://en.wikipedia.org/wiki?curid=14127" title="Höðr">
Höðr

Höðr ( ; often anglicized as Hod, Hoder, or Hodur) is a blind god and a son of Odin and Frigg in Norse mythology. Tricked and guided by Loki, he shot the mistletoe arrow which was to slay the otherwise invulnerable Baldr.

According to the "Prose Edda" and the "Poetic Edda", the goddess Frigg, Baldr's mother, made everything in existence swear never to harm Baldr, except for the mistletoe, which she found too unimportant to ask (alternatively, which she found too young to demand an oath from). The gods amused themselves by trying weapons on Baldr and seeing them fail to do any harm. Loki, the mischief-maker, upon finding out about Baldr's one weakness, made a spear from mistletoe, and helped Höðr shoot it at Baldr. In reaction to this, Odin and the giantess Rindr gave birth to Váli, who grew to adulthood within a day and slew Höðr.

The Danish historian Saxo Grammaticus recorded an alternative version of this myth in his "Gesta Danorum". In this version, the mortal hero Høtherus and the demi-god "Balderus" compete for the hand of Nanna. Ultimately, Høtherus slays Balderus.

The name "Hǫðr", meaning 'warrior', is comparable with the Old English "heaðu-deór" ('brave, stout in war'). Like the Old Norse noun "hǫð" ('war, slaughter'), it stems from Proto-Germanic "*haþuz" ('battle'; compare with Old English "heaðo-", Old High German "hadu"-, Old Saxon "hathu"-).

In the "Gylfaginning" part of Snorri Sturluson's Prose Edda Höðr is introduced in an ominous way.

Höðr is not mentioned again until the prelude to Baldr's death is described. All things except the mistletoe (believed to be harmless) have sworn an oath not to harm Baldr, so the Æsir throw missiles at him for sport.

The "Gylfaginning" does not say what happens to Höðr after this. In fact it specifically states that Baldr cannot be avenged, at least not immediately.

It does seem, however, that Höðr ends up in Hel one way or another for the last mention of him in "Gylfaginning" is in the description of the post-Ragnarök world.

Snorri's source of this knowledge is clearly "Völuspá" as quoted below.

In the "Skáldskaparmál" section of the Prose Edda several kennings for Höðr are related.

None of those kennings, however, are actually found in surviving skaldic poetry. Neither are Snorri's kennings for Váli, which are also of interest in this context.

It is clear from this that Snorri was familiar with the role of Váli as Höðr's slayer, even though he does not relate that myth in the "Gylfaginning" prose. Some scholars have speculated that he found it distasteful, since Höðr is essentially innocent in his version of the story.

Höðr is referred to several times in the Poetic Edda, always in the context of Baldr's death. The following strophes are from "Völuspá".

This account seems to fit well with the information in the Prose Edda, but here the role of Baldr's avenging brother is emphasized.

Baldr and Höðr are also mentioned in "Völuspá"'s description of the world after Ragnarök.

The poem "Vafþrúðnismál" informs us that the gods who survive Ragnarök are Viðarr, Váli, Móði and Magni with no mention of Höðr and Baldr.

The myth of Baldr's death is also referred to in another Eddic poem, "Baldrs draumar".

Höðr is not mentioned again by name in the Eddas. He is, however, referred to in "Völuspá in skamma".

The name of Höðr occurs several times in skaldic poetry as a part of warrior-kennings. Thus "Höðr brynju", "Höðr of byrnie", is a warrior and so is "Höðr víga", "Höðr of battle". Some scholars have found the fact that the poets should want to compare warriors with Höðr to be incongruous with Snorri's description of him as a blind god, unable to harm anyone without assistance. It is possible that this indicates that some of the poets were familiar with other myths about Höðr than the one related in "Gylfaginning" - perhaps some where Höðr has a more active role. On the other hand, the names of many gods occur in kennings and the poets might not have been particular in using any god name as a part of a kenning.

In "Gesta Danorum" Hotherus is a human hero of the Danish and Swedish royal lines. He is gifted in swimming, archery, fighting and music and Nanna, daughter of King Gevarus falls in love with him. But at the same time Balderus, son of Othinus, has caught sight of Nanna bathing and fallen violently in love with her. He resolves to slay Hotherus, his rival.

Out hunting, Hotherus is led astray by a mist and meets wood-maidens who control the fortunes of war. They warn him that Balderus has designs on Nanna but also tell him that he shouldn't attack him in battle since he is a demigod. Hotherus goes to consult with King Gevarus and asks him for his daughter. The king replies that he would gladly favour him but that Balderus has already made a like request and he does not want to incur his wrath.

Gevarus tells Hotherus that Balderus is invincible but that he knows of one weapon which can defeat him, a sword kept by Mimingus, the satyr of the woods. Mimingus also has another magical artifact, a bracelet that increases the wealth of its owner. Riding through a region of extraordinary cold in a carriage drawn by reindeer, Hotherus captures the satyr with a clever ruse and forces him to yield his artifacts.

Hearing about Hotherus's artifacts, Gelderus, king of Saxony, equips a fleet to attack him. Gevarus warns Hotherus of this and tells him where to meet Gelderus in battle. When the battle is joined, Hotherus and his men save their missiles while defending themselves against those of the enemy with a testudo formation. With his missiles exhausted, Gelderus is forced to sue for peace. He is treated mercifully by Hotherus and becomes his ally. Hotherus then gains another ally with his eloquent oratory by helping King Helgo of Hålogaland win a bride.

Meanwhile, Balderus enters the country of king Gevarus armed and sues for Nanna. Gevarus tells him to learn Nanna's own mind. Balderus addresses her with cajoling words but is refused. Nanna tells him that because of the great difference in their nature and stature, since he is a demigod, they are not suitable for marriage.

As news of Balderus's efforts reaches Hotherus, he and his allies resolve to attack Balderus. A great naval battle ensues where the gods fight on the side of Balderus. Thoro in particular shatters all opposition with his mighty club. When the battle seems lost, Hotherus manages to hew Thoro's club off at the haft and the gods are forced to retreat. Gelderus perishes in the battle and Hotherus arranges a funeral pyre of vessels for him. After this battle Hotherus finally marries Nanna.

Balderus is not completely defeated and shortly afterwards returns to defeat Hotherus in the field. But Balderus's victory is without fruit for he is still without Nanna. Lovesick, he is harassed by phantoms in Nanna's likeness and his health deteriorates so that he cannot walk but has himself drawn around in a cart.

After a while Hotherus and Balderus have their third battle and again Hotherus is forced to retreat. Weary of life because of his misfortunes, he plans to retire and wanders into the wilderness. In a cave he comes upon the same maidens he had met at the start of his career. Now they tell him that he can defeat Balderus if he gets a taste of some extraordinary food which had been devised to increase the strength of Balderus.

Encouraged by this, Hotherus returns from exile and once again meets Balderus in the field. After a day of inconclusive fighting, he goes out during the night to spy on the enemy. He finds where Balderus's magical food is prepared and plays the lyre for the maidens preparing it. While they don't want to give him the food, they bestow on him a belt and a girdle which secure victory.

Heading back to his camp, Hotherus meets Balderus and plunges his sword into his side. After three days, Balderus dies from his wound. Many years later, Bous, the son of Othinus and Rinda, avenges his brother by killing Hotherus in a duel.

There are also two lesser-known DanishLatin chronicles, the "Chronicon Lethrense" and the "Annales Lundenses", of which the latter is included in the former. These two sources provide a second euhemerized account of Höðr's slaying of Balder.

It relates that Hother was the king of the Saxons, son of Hothbrod, the daughter of Hadding. Hother first slew Othen's (i.e., Odin's) son Balder in battle and then chased Othen and Thor. Finally, Othen's son Both killed Hother. Hother, Balder, Othen, and Thor were incorrectly considered to be gods.

According to the Swedish mythologist and romantic poet Viktor Rydberg, the story of Baldr's death was taken from Húsdrápa, a poem composed by Ulfr Uggason around 990 AD at a feast thrown by the Icelandic Chief Óláfr Höskuldsson to celebrate the finished construction of his new home, Hjarðarholt, the walls of which were filled with symbolic representations of the Baldr myth among others. Rydberg suggested that Höðr was depicted with eyes closed and Loki guiding his aim to indicate that Loki was the true cause of Baldr's death and Höðr was only his "blind tool." Rydberg theorized that the author of the "Gylfaginning" then mistook the description of the symbolic artwork in the Húsdrápa as the actual tale of Baldr's death.




</doc>
<doc id="14128" url="https://en.wikipedia.org/wiki?curid=14128" title="Herat">
Herat

Herāt (; Persian/Pashto: ) is the third-largest city of Afghanistan. It has a population of about 436,300, and serves as the capital of Herat Province, situated in the fertile valley of the Hari River in the western part of the country. It is linked with Kandahar, Kabul, and Mazar-i-Sharif via Highway 1 or the ring road. It is further linked to the city of Mashhad in neighboring Iran through the border town of Islam Qala, and to Mary in Turkmenistan to the north through the border town of Torghundi.

Herat dates back to Avestan times and was traditionally known for its wine. The city has a number of historic sites, including the Herat Citadel and the Musalla Complex. During the Middle Ages Herat became one of the important cities of Khorasan, as it was known as the "Pearl of Khorasan". After the conquest of Tamerlane, the city became an important center of intellectual and artistic life in the Islamic world. Under the rule of Shah Rukh the city served as the focal point of the Timurid Renaissance, whose glory matched Florence of the Italian Renaissance as the center of a cultural rebirth. After the fall of the Timurid Empire, Herat has been governed by various Afghan rulers since the early 18th century. In 1717, the city was invaded by the Hotaki forces until they were expelled by the Afsharids in 1729. After Nader Shah's death and Ahmad Shah Durrani's rise to power in 1747, Herat became part of Afghanistan. It witnessed some political disturbances and military invasions during the early half of the 19th century but the 1857 Treaty of Paris ended hostilities of the Anglo-Persian War.

Herat lies on the ancient trade routes of the Middle East, Central and South Asia, and today is a regional hub in western Afghanistan. The roads from Herat to Iran, Turkmenistan, and other parts of Afghanistan are still strategically important. As the gateway to Iran, it collects high amount of customs revenue for Afghanistan. It also has an international airport. The city has high residential density clustered around the core of the city. However, vacant plots account for a higher percentage of the city (21%) than residential land use (18%) and agricultural is the largest percentage of total land use (36%). Today the city is considered to be relatively safe.

Herat dates back to ancient times (its exact age remains unknown). During the period of the Achaemenid Empire (ca. 550-330 BC), the surrounding district was known as 𐏃𐎼𐎡𐎺 "Haraiva" (in Old Persian), and in classical sources the region was correspondingly known as Aria (Areia). In the Zoroastrian Avesta, the district is mentioned as "Haroiva". The name of the district and its main town is derived from that of the chief river of the region, the Herey River (Old Dari "Hereyrud", "Silken Water"), which traverses the district and passes some south of modern Herāt. Herey is mentioned in Sanskrit as yellow or golden color equivalent to Persian "Zard" meaning Gold (yellow). The naming of a region and its principal town after the main river is a common feature in this part of the world—compare the adjoining districts/rivers/towns of Arachosia and Bactria.

The district "Aria" of the Achaemenid Empire is mentioned in the provincial lists that are included in various royal inscriptions, for instance, in the Behistun inscription of Darius I (ca. 520 BC). Representatives from the district are depicted in reliefs, e.g., at the royal Achaemenid tombs of Naqsh-e Rustam and Persepolis. They are wearing Scythian-style dress (with a tunic and trousers tucked into high boots) and a twisted Bashlyk that covers their head, chin and neck.

Hamdallah Mustawfi, composer of the 14th century work "The Geographical Part of the Nuzhat-al-Qulub" writes that:

Herodotus described Herat as "the bread-basket of Central Asia". At the time of Alexander the Great in 330 BC, Aria was obviously an important district. It was administered by a satrap called Satibarzanes, who was one of the three main Persian officials in the East of the Empire, together with the satrap Bessus of Bactria and Barsaentes of Arachosia. In late 330 BC, Alexander captured the Arian capital that was called Artacoana. The town was rebuilt and the citadel was constructed. Afghanistan became part of the Seleucid Empire.
However, most sources suggest that Herat was predominantly Zoroastrian. It became part of the Parthian Empire in 167 BC. In the Sasanian period (226-652), 𐭧𐭥𐭩𐭥‎ "Harēv" is listed in an inscription on the Ka'ba-i Zartosht at Naqsh-e Rustam; and "Hariy" is mentioned in the Pahlavi catalogue of the provincial capitals of the empire. In around 430, the town is also listed as having a Christian community, with a Nestorian bishop.

In the last two centuries of Sasanian rule, Aria (Herat) had great strategic importance in the endless wars between the Sasanians, the Chionites and the Hephthalites who had been settled in the northern section of Afghanistan since the late 4th century.

At the time of the Arab invasion in the middle of the 7th century, the Sasanian central power seemed already largely nominal in the province in contrast with the role of the Hephthalites tribal lords, who were settled in the Herat region and in the neighboring districts, mainly in pastoral Bādghis and in Qohestān. It must be underlined, however, that Herat remained one of the three Sasanian mint centers in the east, the other two being Balkh and Marv. The Hephthalites from Herat and some unidentified Turks opposed the Arab forces in a battle of Qohestān in 651-52 AD, trying to block their advance on Nishāpur, but they were defeated

When the Arab armies appeared in Khorāsān in the 650s AD, Herāt was counted among the twelve capital towns of the Sasanian Empire. The Arab army under the general command of Ahnaf ibn Qais in its conquest of Khorāsān in 652 seems to have avoided Herāt, but it can be assumed that the city eventually submitted to the Arabs, since shortly afterwards an Arab governor is mentioned there. A treaty was drawn in which the regions of Bādghis and Bushanj were included. As did many other places in Khorāsān, Herāt rebelled and had to be re-conquered several times.

Another power that was active in the area in the 650s was Tang dynasty China which had embarked on a campaign that culminated in the Conquest of the Western Turks. By 659–661, the Tang claimed a tenuous suzerainty over Herat, the westernmost point of Chinese power in its long history. This hold however would be ephemeral with local Turkish tribes rising in rebellion in 665 and driving out the Tang.

In 702 AD Yazid ibn al-Muhallab defeated certain Arab rebels, followers of Ibn al-Ash'ath, and forced them out of Herat. The city was the scene of conflicts between different groups of Muslims and Arab tribes in the disorders leading to the establishment of the Abbasid Caliphate. Herat was also a centre of the followers of Ustadh Sis.

In 870 AD, Yaqub ibn Layth Saffari, a local ruler of the Saffarid dynasty conquered Herat and the rest of the nearby regions in the name of Islam.

The region of Herāt was under the rule of King Nuh III, the seventh of the Samanid line—at the time of Sebük Tigin and his older son, Mahmud of Ghazni. The governor of Herāt was a noble by the name of "Faik", who was appointed by Nuh III. It is said that Faik was a powerful, but insubordinate governor of Nuh III; and had been punished by Nuh III. Faik made overtures to Bogra Khan and Ughar Khan of Khorasan. Bogra Khan answered Faik's call, came to Herāt and became its ruler. The Samanids fled, betrayed at the hands of Faik to whom the defence of Herāt had been entrusted by Nuh III. In 994, Nuh III invited Alp Tigin to come to his aid. Alp Tigin, along with Mahmud of Ghazni, defeated Faik and annexed Herāt, Nishapur and Tous.

Herat was a great trading centre strategically located on trade routes from Mediterranean to India or to China. The city was noted for its textiles during the Abbasid Caliphate, according to many references by geographers. Herāt also had many learned sons such as Ansārī. The city is described by Estakhri and Ibn Hawqal in the 10th century as a prosperous town surrounded by strong walls with plenty of water sources, extensive suburbs, an inner citadel, a congregational mosque, and four gates, each gate opening to a thriving market place. The government building was outside the city at a distance of about a mile in a place called Khorāsānābād. A church was still visible in the countryside northeast of the town on the road to Balkh, and farther away on a hilltop stood a flourishing fire temple, called Sereshk, or Arshak according to Mustawfi.

Herat was a part of the Taherid dominion in Khorāsān until the rise of the Saffarids in Sistān under Ya'qub-i Laith in 861, who, in 862, started launching raids on Herat before besieging and capturing it on 16 August 867, and again in 872. The Saffarids succeeded in expelling the Taherids from Khorasan in 873.

The Sāmānid dynasty was established in Transoxiana by three brothers, Nuh, Yahyā, and Ahmad. Ahmad Sāmāni opened the way for the Samanid dynasty to the conquest of Khorāsān, including Herāt, which they were to rule for one century. The centralized Samanid administration served as a model for later dynasties. The Samanid power was destroyed in 999 by the Qarakhanids, who were advancing on Transoxiana from the northeast, and by the Ghaznavids, former Samanid retainers, attacking from the southeast.

Sultan Maḥmud of Ghazni officially took control of Khorāsān in 998. Herat was one of the six Ghaznavid mints in the region. In 1040, Herat was captured by the Seljuk Empire. Yet, in 1175, it was captured by the Ghurids of Ghor and then came under the Khawarazm Empire in 1214. According to the account of Mustawfi, Herat flourished especially under the Ghurid dynasty in the 12th century. Mustawfi reported that there were "359 colleges in Herat, 12,000 shops all fully occupied, 6,000 bath-houses; besides caravanserais and mills, also a darwish convent and a fire temple". There were about 444,000 houses occupied by a settled population. The men were described as "warlike and carry arms", and they were Sunni Muslims. The great mosque of Herāt was built by Ghiyas ad-Din Ghori in 1201. In this period Herāt became an important center for the production of metal goods, especially in bronze, often decorated with elaborate inlays in precious metals.

Herat was invaded and destroyed by Genghis Khan's Mongol army in 1221. The city was destroyed a second time and remained in ruins from 1222 to about 1236. In 1244 a local prince Shams al-Din Kart was named ruler of Herāt by the Mongol governor of Khorāsān and in 1255 he was confirmed in his rule by the founder of the Il-Khan dynasty Hulagu. Shams al-Din founded a new dynasty and his successors, especially Fakhr-al-Din and Ghiyath al-Din, built many mosques and other buildings. The members of this dynasty were great patrons of literature and the arts. By this time Herāt became known as the "pearl of Khorasan".

Timur took Herat in 1380 and he brought the Kartid dynasty to an end a few years later. The city reached its greatest glory under the Timurid princes, especially Sultan Husayn Bayqara who ruled Herat from 1469 until May 4, 1506. His chief minister, the poet and author in Persian and Turkish, Mir Ali-Shir Nava'i was a great builder and patron of the arts. Under the Timurids, Herat assumed the role of the main capital of an empire that extended in the West as far as central Persia. As the capital of the Timurid empire, it boasted many fine religious buildings and was famous for its sumptuous court life and musical performance and its tradition of miniature paintings. On the whole, the period was one of relative stability, prosperity, and development of economy and cultural activities. It began with the nomination of Shahrokh, the youngest son of Timur, as governor of Herat in 1397. The reign of Shahrokh in Herat was marked by intense royal patronage, building activities, and promotion of manufacturing and trade, especially through the restoration and enlargement of the Herat's bāzār. The present Musallah Complex, and many buildings such as the madrasa of Gawhar Shad, Ali Shir mahāl, many gardens, and others, date from this time. The village of Gazar Gah, over two km northeast of Herat, contained a shrine which was enlarged and embellished under the Timurids. The tomb of the poet and mystic Khwājah Abdullāh Ansārī (d. 1088), was first rebuilt by Shahrokh about 1425, and other famous men were buried in the shrine area. Herat was shortly captured by Kara Koyunlu between 1458–1459.

In 1507 Herat was occupied by the Uzbeks but after much fighting the city was taken by Shah Isma'il, the founder of the Safavid dynasty, in 1510 and the Shamlu Qizilbash assumed the governorship of the area. Under the Safavids, Herat was again relegated to the position of a provincial capital, albeit one of a particular importance. At the death of Shah Isma'il the Uzbeks again took Herat and held it until Shah Tahmasp retook it in 1528. The Persian king, Abbas was born in Herat, and in Safavid texts, Herat is referred to as "a'zam-i bilād-i īrān", meaning "the greatest of the cities of Iran". In the 16th century, all future Safavid rulers, from Tahmasp I to Abbas I, were governors of Herat in their youth.

By the early 18th century Herat was governed by various Hotaki and Abdali Afghans. After Nader Shah's death in 1747, Ahmad Shah Durrani took possession of the city and became part of the Durrani Empire.
In 1824, Herat became independent for several years when the Afghan Empire was split between the Durranis and Barakzais. The Persians sieged the city in 1837, but the British helped the Afghans in repelling them. In 1856, they invaded again, and briefly managed to retake the city; it led directly to the Anglo-Persian War. In 1857 hostilities between the Persians and the British ended after the Treaty of Paris was signed, and the Persian troops withdrew from Herat. Afghanistan reconquered Herat in 1863 under Dost Muhammad Khan, two weeks before his death.

One of the greatest tragedies for the Afghans and Muslims was the British invasion of, and subsequent destruction of the Islamic Musallah complex in Herat in 1885. The officially stated reason was to get a good line of sight for their artillery against Russian invaders who never came. This was but one small sidetrack in the Great Game, a century-long conflict between the British Empire and the Russian Empire in the 19th century.

In the 1960s, engineers from the United States built Herat Airport, which was used by the Soviet forces during the Democratic Republic of Afghanistan in the 1980s. Even before the Soviet invasion at the end of 1979, there was a substantial presence of Soviet advisors in the city with their families.

Between March 10 and March 20, 1979, the Afghan Army in Herāt under the control of commander Ismail Khan mutinied. Thousands of protesters took to the streets against the Khalq communist regime's oppression led by Nur Mohammad Taraki. The new rebels led by Khan managed to oust the communists and take control of the city for 3 days, with some protesters murdering any Soviet advisers. This shocked the government, who blamed the new administration of Iran following the Iranian Revolution for influencing the uprising. Reprisals by the government followed, and between 3,000 and 24,000 people (according to different sources) were killed, in what is called the 1979 Herat uprising, or in Persian as the "Qiam-e Herat". The city itself was recaptured with tanks and airborne forces, but at the cost of thousands of civilians killed. This massacre was the first of its kind since the country's independence in 1919, and was the bloodiest event preceding the Soviet–Afghan War.

Herat received damage during the Soviet–Afghan War in the 1980s, especially its western side. The province as a whole was one of the worst-hit. In April 1983, a series of Soviet bombings damaged half of the city and killed around 3,000 civilians, described as "extremely heavy, brutal and prolonged". Ismail Khan was the leading mujahideen commander in Herāt fighting against the Soviet-backed government.

After the communist government's collapse in 1992, Khan joined the new government and he became governor of Herat Province. The city was relatively safe and it was recovering and rebuilding from the damage caused in the Soviet–Afghan War. However, on September 5, 1995, the city was captured by the Taliban without much resistance, forcing Khan to flee. Herat became the first Persian-speaking city to be captured by the Taliban. The Taliban's strict enforcement of laws confining women at home and closing girls' schools alienated Heratis who are traditionally more liberal and educated, like the Kabulis, than other urban populations in the country. Two days of anti-Taliban protests occurred in December 1996 which was violently dispersed and led to the imposition of a curfew. In May 1999, a rebellion Herat was crushed by the Taliban, who blamed Iran for causing it.

After the U.S. invasion of Afghanistan, on November 12, 2001, it was captured from the Taliban by forces loyal to the Northern Alliance and Ismail Khan returned to power (see Battle of Herat). In 2004, Mirwais Sadiq, Aviation Minister of Afghanistan and the son of Ismail Khan, was ambushed and killed in Herāt by a local rival group. More than 200 people were arrested under suspicion of involvement.

In 2005, the International Security Assistance Force (ISAF) began establishing bases in and around the city. Its main mission was to train the Afghan National Security Forces (ANSF) and help with the rebuilding process of the country. Regional Command West, led by Italy, assisted the Afghan National Army (ANA) 207th Corps. Herat was one of the first seven areas that transitioned security responsibility from NATO to Afghanistan. In July 2011, the Afghan security forces assumed security responsibility from NATO.

Due to their close relations, Iran began investing in the development of Herat's power, economy and education sectors. In the meantime, the United States built a consulate in Herat to help further strengthen its relations with Afghanistan. In addition to the usual services, the consulate works with the local officials on development projects and with security issues in the region.

Herat has a cold semi-arid climate (Köppen climate classification "BSk"). Precipitation is very low, and mostly falls in winter. Although Herāt is approximately lower than Kandahar, the summer climate is more temperate, and the climate throughout the year is far from disagreeable, although winter temperatures are comparably lower. From May to September, the wind blows from the northwest with great force. The winter is tolerably mild; snow melts as it falls, and even on the mountains does not lie long. Three years out of four it does not freeze hard enough for the people to store ice. The eastern reaches of the Hari River, including the rapids, are frozen hard in the winter, and people travel on it as on a road.

India, Iran and Pakistan operate their consulate here for trade, military and political links.


Of the more than dozen minarets that once stood in Herāt, many have been toppled from war and neglect over the past century. Recently, however, everyday traffic threatens many of the remaining unique towers by shaking the very foundations they stand on. Cars and trucks that drive on a road encircling the ancient city rumble the ground every time they pass these historic structures. UNESCO personnel and Afghan authorities have been working to stabilize the Fifth Minaret.


The population of Herat numbered approximately 436,300 in 2013. The city houses a multi-ethnic society and speakers of the Persian language are in the majority. There is no current data on the precise ethnic composition of the city's population, but according to a 2003 map found in the National Geographic Magazine, Persian-speaking Tajik and Farsiwan peoples form the overwhelming majority of the city, comprising around 65% of the population. The remaining population comprises Pashtuns (30%), Hazaras (2%), Uzbeks (2%) and Turkmens (1%).

Persian is the native language of Herat and the local dialect – known by natives as "Herātī" – belongs to the "Khorāsānī" cluster within Persian. It is akin to the Persian dialects of eastern Iran, notably those of Mashhad and Khorasan Province, which borders Herat. This Persian dialect serves as the lingua franca of the city. The second language that is understood by many is Pashto, which is the native language of the Pashtuns. The local Pashto dialect spoken in Herat is a variant of western Pashto, which is also spoken in Kandahar and southern and western Afghanistan. Religiously, Sunni Islam is practiced by the majority, while Shias make up the minority.

The city once had a Jewish community. About 280 families lived in Herat as of 1948, but most of them moved to Israel that year, and the community disappeared by 1992. There are four former synagogues in the city's old quarter, which were neglected for decades and fell into disrepair. In the late 2000s, the buildings of the synagogues were renovated by the Aga Khan Trust for Culture, and at this time, three of them were turned into schools and nurseries, the Jewish community having vanished. The Jewish cemetery is being taken care of by Jalil Ahmed Abdelaziz.


Herat International Airport was built by engineers from the United States in the 1960s and was used by the Soviet Armed Forces during the Soviet–Afghan War in the 1980s. It was bombed in late 2001 during Operation Enduring Freedom but had been rebuilt within the next decade. The runway of the airport has been extended and upgraded and as of August 2014 there were regularly scheduled direct flights to Delhi, Dubai, Mashad, and various airports in Afghanistan. At least five airlines operated regularly scheduled direct flights to Kabul.

Rail connections to and from Herat were proposed many times, during "The Great Game" of the 19th century and again in the 1970s and 1980s, but nothing came to life. In February 2002, Iran and the Asian Development Bank announced funding for a railway connecting Torbat-e Heydarieh in Iran to Herat. This was later changed to begin in Khaf in Iran, a railway for both cargo and passengers, with work on the Iranian side of the border starting in 2006. Construction is underway in the Afghan side and it was estimated to be completed by March 2018. There is also the prospect of an extension across Afghanistan to Sher Khan Bandar.

The AH76 highway connects Herat to Maymana and the north. The AH77 connects it east towards Chaghcharan and north towards Mary in Turkmenistan. Highway 1 (part of Asian highway AH1) links it to Mashhad in Iran to the northwest, and south via the Kandahar–Herat Highway to Delaram.



 


</doc>
<doc id="14130" url="https://en.wikipedia.org/wiki?curid=14130" title="Hedeby">
Hedeby

Hedeby (, Old Norse "Heiðabýr", German "Haithabu") was an important Danish Viking Age (8th to the 11th centuries) trading settlement near the southern end of the Jutland Peninsula, now in the Schleswig-Flensburg district of Schleswig-Holstein, Germany. It is the most important archaeological site in Schleswig-Holstein. Around 965, chronicler Abraham ben Jacob visited Hedeby and described it as, "a very large city at the very end of the world's ocean."

The settlement developed as a trading centre at the head of a narrow, navigable inlet known as the Schlei, which connects to the Baltic Sea. The location was favorable because there is a short portage of less than 15 km to the Treene River, which flows into the Eider with its North Sea estuary, making it a convenient place where goods and ships could be pulled on a corduroy road overland for an almost uninterrupted seaway between the Baltic and the North Sea and avoid a dangerous and time-consuming circumnavigation of Jutland, providing Hedeby with a role similar to later Lübeck.
Hedeby was the second largest Nordic town during the Viking Age, after Uppåkra in present-day southern Sweden,
The city of Schleswig was later founded on the other side of the Schlei. Hedeby was abandoned after its destruction in 1066.

Hedeby was rediscovered in the late 19th century and excavations began in 1900. The Hedeby Museum was opened next to the site in 1985.

The Old Norse name "Heiða-býr" simply translates to "heath-settlement" ("heiðr" "heath" and "býr" = "yard; settlement, village, town"). The name is recorded in numerous spelling variants.


Sources from the 9th and 10th century AD also attest to the names "Sliesthorp" and "Sliaswich" (cf. "-thorp" vs. "-wich"), and the town of Schleswig still exists 3 km north of Hedeby. However, Æthelweard claimed in his Latin translation of the Anglo-Saxon Chronicle that the Saxons used "Slesuuic" and the Danes "Haithaby" to refer to the same town.

Hedeby is first mentioned in the Frankish chronicles of Einhard (804) who was in the service of Charlemagne,
but was probably founded around 770. In 808 the Danish king Godfred (Lat. Godofredus) destroyed a competing Slav trade centre named Reric, and it is recorded in the Frankish chronicles that he moved the merchants from there to Hedeby. This may have provided the initial impetus for the town to develop. The same sources record that Godfred strengthened the Danevirke, an earthen wall that stretched across the south of the Jutland peninsula. The Danevirke joined the defensive walls of Hedeby to form an east–west barrier across the peninsula, from the marshes in the west to the Schlei inlet leading into the Baltic in the east.

The town itself was surrounded on its three landward sides (north, west, and south) by earthworks. At the end of the 9th century the northern and southern parts of the town were abandoned for the central section. Later a 9-metre (29-ft) high semi-circular wall was erected to guard the western approaches to the town. On the eastern side, the town was bordered by the innermost part of the Schlei inlet and the bay of Haddebyer Noor.

Hedeby became a principal marketplace because of its geographical location on the major trade routes between the Frankish Empire and Scandinavia (north-south), and between the Baltic and the North Sea (east-west). Between 800 and 1000 the growing economic power of the Vikings led to its dramatic expansion as a major trading centre. Along with Birka and Schleswig, Hedeby's prominence as a major international trading hub served as a foundation of the Hanseatic League that would emerge by the 12th century.

The following indicate the importance achieved by the town:

A Swedish dynasty founded by Olof the Brash is said to have ruled Hedeby during the last decades of the 9th century and the first part of the 10th century. This was told to Adam of Bremen by the Danish king Sweyn Estridsson, and it is supported by three runestones found in Denmark. Two of them were raised by the mother of Olof's grandson Sigtrygg Gnupasson. The third runestone, discovered in 1796, is from Hedeby, the "Stone of Eric" (). It is inscribed with Norwegian-Swedish runes. It is, however, possible that Danes also occasionally wrote with this version of the younger futhark.

Life was short and crowded in Hedeby. The small houses were clustered tightly together in a grid, with the east–west streets leading down to jetties in the harbour. People rarely lived beyond 30 or 40, and archaeological research shows that their later years were often painful due to crippling diseases such as tuberculosis. Yet make-up for men and rights for women provide surprises to the modern understanding.

Al-Tartushi, a late 10th-century traveller from al-Andalus, provides one of the most colourful and often quoted descriptions of life in Hedeby. Al-Tartushi was from Cordoba in Spain, which had a significantly more wealthy and comfortable lifestyle than Hedeby. While Hedeby may have been significant by Scandinavian standards, Al-Tartushi was unimpressed:

The town was sacked in 1050 by King Harald Hardrada of Norway during a conflict with King Sweyn II of Denmark. He set the town on fire by sending several burning ships into the harbour, the charred remains of which were found at the bottom of the Schlei during recent excavations. A Norwegian "skald", quoted by Snorri Sturluson, describes the sack as follows:

In 1066 the town was sacked and burned by West Slavs. Following the destruction, Hedeby was slowly abandoned. People moved across the Schlei inlet, which separates the two peninsulas of Angeln and Schwansen, and founded the town of Schleswig.

After the settlement was abandoned, rising waters contributed to the complete disappearance of all visible structures on the site. It was even forgotten where the settlement had been. This proved to be fortunate for later archaeological work at the site.

Archaeological work began at the site in 1900 after the rediscovery of the settlement. Excavations were conducted for the next 15 years. Further excavations were carried out between 1930 and 1939. Archaeological work on the site was productive for two main reasons: that the site had never been built on since its destruction some 840 years earlier, and that the permanently waterlogged ground had preserved wood and other perishable materials. After the Second World War, in 1959, archaeological work was started again and has continued intermittently ever since. The embankments surrounding the settlement were excavated, and the harbour was partially dredged, during which the wreck of a Viking ship was discovered. Despite all this work, only 5% of the settlement (and only 1% of the harbour) has as yet been investigated.

The most important finds resulting from the excavations are now on display in the adjoining Haithabu Museum.

In 2005 an ambitious archaeological reconstruction program was initiated on the original site. Based on the results of archaeological analyses, exact copies of some of the original Viking houses have been rebuilt.





</doc>
<doc id="14131" url="https://en.wikipedia.org/wiki?curid=14131" title="Hazaras">
Hazaras

</ref>

The Hazaras (; ) are a Persian-speaking ethnic group native to, and primarily residing in, the mountainous region of Hazarajat, in central Afghanistan. They speak the Hazaragi dialect of Persian.

They are the third-largest ethnic group in Afghanistan, and are also a significant minority group in neighboring Pakistan, where there is a population of between 650,000 and 900,000, mostly in Quetta. Hazaras are considered to be one of the most oppressed groups in Afghanistan, and their persecution dates back decades.

Babur, founder of the Mughal Empire in the early 16th century, records the name "Hazara" in his autobiography. He referred to the populace of a region called "Hazaristan", located west of the Kabulistan region, east of Ghor, and north of Ghazni.

The conventional theory is that the name "Hazara" derives from the Persian word for "thousand" ( ). It may be the translation of the Mongol word (or ), a military unit of 1,000 soldiers at the time of Genghis Khan. With time, the term "Hazar" could have been substituted for the Mongol word and now stands for the group of people, while the Hazaras in their native language call themselves ( ) and ( ).

The origins of the Hazara have not been fully reconstructed. Significant Inner Asian descent—in historical context, Turkic and Mongol—is probable because their physical attributes, facial bone structures and parts of their culture and language resemble those of Mongols and Central Asian Turks. Genetic analysis of the Hazara indicate partial Mongol ancestry. Invading Mongols and Turco-Mongols mixed with the local Iranian population, forming a distinct group. For example, Nikudari Mongols settled in what is now Afghanistan and mixed with the native populations. A second wave of mostly Chagatai Mongols came from Central Asia and were followed by other Mongolic groups, associated with the Ilkhanate and the Timurids, all of whom settled in Hazarajat and mixed with the local population, forming a distinct group.

The Hazara identity in Afghanistan is believed by many to have originated in the aftermath of the 1221 Siege of Bamyan. The first mention of Hazara are made by Babur in the early 16th century and later by the court historians of Shah Abbas of the Safavid dynasty. It is reported that they embraced Shia Islam between the end of the 16th and the beginning of the 17th century, during the Safavid period.

Hazara men along with tribes of other ethnic groups had been recruited and added to the army of Ahmad Shah Durrani in the 18th century. Some claim that in the mid‑18th century Hazara were forced out of Helmand and the Arghandab District of Kandahar Province.

During the second reign of Dost Mohammad Khan in the 19th century, Hazara from Hazarajat began to be taxed for the first time. However, for the most part they still managed to keep their regional autonomy until the subjugation of Abdur Rahman Khan began in the late 19th century.

When the Treaty of Gandomak was signed and the Second Anglo-Afghan War ended in 1880, Abdur Rahman Khan set out a goal to bring Hazarajat and Kafiristan under his control. He launched several campaigns in Hazarajat due to resistance from the Hazara in which his forces committed atrocities. The southern part of Hazarajat was spared as they accepted his rule, while the other parts of Hazarajat rejected Abdur Rahman and instead supported his uncle, Sher Ali Khan. In response to this Abdur Rahman waged a war against tribal leaders who rejected his policies and rule. This is known as the Hazara Uprisings. Abdur Rahman arrested Syed Jafar, chief of the Sheikh Ali Hazaras, and jailed him in Mazar-i-Sharif.

These campaigns had a catastrophic impact on the demographics of Hazaras causing 60% of them to perish and become displaced.

In 1901, Habibullah Khan, Abdur Rahman's successor, granted amnesty to all people who were exiled by his predecessor. However, the division between the Afghan government and the Hazara people was already made too deep under Abdur Rahman. Hazara continued to face severe social, economic and political discrimination through most of the 20th century. In 1933 King Mohammed Nadir Khan was assassinated by Abdul Khaliq Hazara. The Afghan government captured and executed him later, along with several of his innocent family members.

Mistrust of the central government by the Hazaras and local uprisings continued. In particular, from 1945–1946, during Zahir Shah's rule, a revolt took place against new taxes that were exclusively imposed on the Hazara. The Kuchi nomads meanwhile not only were exempted from taxes, but also received allowances from the Afghan government. The angry rebels began capturing and killing government officials. In response, the central government sent a force to subdue the region and later removed the taxes.

During the Soviet–Afghan War, the Hazarajat region did not see as much heavy fighting as other regions of Afghanistan. However, rival Hazara political factions fought. The division was between the "Tanzáim-i nasl-i naw-i Hazara", a party based in Quetta, of Hazara nationalists and secular intellectuals, and the Islamist parties in Hazarajat. By 1979, the Hazara-Islamist groups liberated Hazarajat from the central Soviet-backed Afghan government and later took entire control of Hazarajat away from the secularists. By 1984, after severe fighting, the secularist groups lost all their power to the Islamists.

As the Soviets withdrew in 1989, the Islamist groups felt the need to broaden their political appeal and turned their focus to Hazara ethnic nationalism. This led to establishment of the Hizb-i-Wahdat, an alliance of all the Hazara resistance groups (except the "Harakat-i Islami"). In 1992 with the fall of Kabul, the "Harakat-i Islami" took sides with Burhanuddin Rabbani's government while the Hizb-i-Wahdat took sides with the opposition. The Hizb-i-Wahdat was eventually forced out of Kabul in 1995 when the Taliban movement captured and killed their leader Abdul Ali Mazari. With the Taliban's capture of Kabul in 1996, all the Hazara groups united with the new Northern Alliance against the common new enemy. However, it was too late and despite the fierce resistance Hazarajat fell to the Taliban by 1998. The Taliban had Hazarajat totally isolated from the rest of the world going as far as not allowing the United Nations to deliver food to the provinces of Bamyan, Ghor, Maidan Wardak, and Daykundi.
Hazaras have also been a significant role in the creation of Pakistan. One such Hazara was Qazi Muhammad Essa of the Sheikh Ali tribe, who had been close friends with Muhammad Ali Jinnah, having had met each other for the first time whilst they were studying in London. He had been the first from his native province of Balochistan to obtain a Bar-at-Law degree and had helped set up the All-India Muslim League in Balochistan. 

Though Hazara played a role in the anti-Soviet movement, other Hazara participated in the new communist government, which actively courted Afghan minorities. Sultan Ali Kishtmand, a Hazara, served as prime minister of Afghanistan from 1981–1990 (with one brief interruption in 1988). The Ismaili Hazara of Baghlan Province likewise supported the communists, and their "pir" (religious leader) Jaffar Naderi led a pro-Communist militia in the region.

During the years that followed, Hazara suffered severe oppression and many ethnic massacres, genocides and pogroms were carried out by the predominantly ethnic Pashtun Taliban and are documented by such groups the Human Rights Watch. These human rights abuses not only occurred in Hazarajat, but across all districts controlled by the Taliban. Particularly after their capture of Mazar-i-Sharif in 1998, where after a massive killing of some 8,000 civilians, the Taliban openly declared that the Hazara would be targeted.

Following the 11 September 2001 attacks in the United States, British and American forces invaded Afghanistan. Many Hazara have become leaders in today's newly emerging Afghanistan. Hazara have also pursued higher education, enrolled in the army, and many have top government positions. For example, Mohammad Mohaqiq, a Hazara from the Hizb-i-Wahdat party, ran in the 2004 presidential election in Afghanistan, and Karim Khalili became the Vice President of Afghanistan. A number of ministers and governors are Hazara, including Sima Samar, Habiba Sarabi, Sarwar Danish, Sayed Hussein Anwari, Abdul Haq Shafaq, Sayed Anwar Rahmati, Qurban Ali Oruzgani. The mayor of Nili in Daykundi Province is Azra Jafari, who became the first female mayor in Afghanistan. Some other notable Hazara include: Sultan Ali Keshtmand, Abdul Wahed Sarābi, Ghulam Ali Wahdat, Sayed Mustafa Kazemi, Muhammad Arif Shah Jahan, Ghulam Husain Naseri, Abbas Noyan, Abbas Ibrahim Zada, Ramazan Bashardost, Ahmad Shah Ramazan, Ahmad Behzad, Nasrullah Sadiqi Zada Nili. The Parliament of Afghanistan is 25% made up of ethnic Hazara, which represents 61 members.

Although Afghanistan has been historically one of the poorest countries in the world, the Hazarajat region has been kept even more poor from development by past governments. Since ousting the Taliban in late 2001, billions of dollars have poured into Afghanistan for reconstruction and several large-scale reconstruction projects took place in Afghanistan from August 2012. For example, there have been more than 5000 kilometers of road pavement completed across Afghanistan, of which little was done in central Afghanistan Hazarajat. On the other hand, the Band-e Amir in the Bamyan Province became the first national park of Afghanistan. The road from Kabul to Bamyan was also built, along with new police stations, government institutions, hospitals, and schools in the Bamyan Province, Daykundi Province, and the others. The first ski resort of Afghanistan was also established in Bamyan Province.

An indication of discrimination is that Kuchis (Pashtun nomads who have historically been migrating from region to region depending on the season) are allowed to use Hazarajat pastures during the summer season. It is believed that allowing the Kuchis to use some of the grazing land in Hazarajat began during the rule of Abdur Rahman Khan. Living in mountainous Hazarajat, where little farm land exists, Hazara people rely on these pasture lands for their livelihood during the long and harsh winters. In 2007 some Kuchi nomads entered into parts of Hazarajat to graze their livestock, and when the local Hazara resisted, a clash took place and several people on both sides died using assault rifles. Such events continue to occur, even after the central government was forced to intervene, including President Hamid Karzai. In late July 2012, a Hazara police commander in Uruzgan province reportedly rounded up and killed 9 Pashtun civilians in revenge for the death of two local Hazara. The matter is being investigated by the Afghan government.

The drive by President Hamid Karzai after the Peace Jirga to strike a deal with Taliban leaders caused deep unease in Afghanistan's minority communities, who fought the Taliban the longest and suffered the most during their rule. The leaders of the Tajik, Uzbek and Hazara communities, vowed to resist any return of the Taliban to power, referring to the large-scale massacres of Hazara civilians during the Taliban period.

Genetically, the Hazara are a mixture of western Eurasian and eastern Eurasian components, i.e. racially Eurasian. Genetic research suggests that the Hazaras of Afghanistan cluster closely with the Uzbek population of the country, while both groups are at a notable distance from Afghanistan's Tajik and Pashtun populations. There is evidence of both a patrimonial and maternal relation to Turkic Peoples and Mongols amongst some Hazaras.

East Eurasian male and female ancestry is supported by studies in genetic genealogy as well. East Asian maternal haplogroups (mtDNA) make up about 35%, suggesting that the male descendants of Turkic and Mongolic peoples were accompanied by women of East Asian ancestry, though the Hazaras as a whole have mostly west Eurasian mtDNA. Women of Non-East Asian mtDNA in Hazaras are at about 65%, most which are West Eurasians and some South Asian.

The most frequent paternal haplogroups found amongst the Pakistani Hazara were haplogroup C-M217 at 40%(10/25) and Haplogroup R1b at 32% (8/25).

One study about paternal DNA haplogroups of the Afghanistan shows that the Y-DNA haplogroups R1a and C-M217 are the most common haplogroups, followed by J2-M172 and L-M20. Some Hazaras also have the haplogroup R1a1a-M17, E1b1b1-M35, L-M20 and H-M69, which are common in Tajiks, Pashtuns as well as Indian populations. In one study, a small minority had the haplogroup B-M60, normally found in East Africa, and in one mtDNA study of Hazara, mtDNA Haplogroup L (which is of African origin) was detected at a frequency of 7.5%.

A recent study shows that the Uyghurs are closely related to the Hazaras. The study also suggests a small but notable East Asian ancestry in other populations of Pakistan and India.

The vast majority of Hazaras live in Hazarajat, and many others live in the cities, including in neighboring countries or abroad. The latest World Factbook estimates show that Hazara make up nine percent of the total Afghanistan population but some sources claim that they are about 20 percent.

Alessandro Monsutti argues, in his recent anthropological book, that migration is the traditional way of life of the Hazara people, referring to the seasonal and historical migrations which have never ceased and do not seem to be dictated only by emergency situations such as war. Due to the decades of war in Afghanistan and the sectarian violence in Pakistan, many Hazaras left their communities and have settled in Australia, New Zealand, Canada, the United States, the United Kingdom and particularly the Northern European countries such as Sweden and Denmark. Some go to these countries as exchange students while others through human smuggling, which sometimes costs them their lives. Since 2001, about 1,000 people have died in the ocean while trying to reach Australia by boats from Indonesia. Many of these were Hazaras, including women and small children who could not swim. The notable case was the Tampa affair in which a shipload of refugees, mostly Hazara, was rescued by the Norwegian freighter MV "Tampa" and subsequently sent to Nauru. New Zealand agreed to take some of the refugees and all but one of those were granted stay.

During the British expansion in the 19th century, Hazaras worked during the winter months in coal mines, road construction and in other menial labor jobs in some cities of what is now Pakistan. The earliest record of Hazara in the areas of Pakistan is found in Broadfoot's Sappers company from 1835 in Quetta. This company had also participated in the First Anglo-Afghan War. Some Hazara also worked in the agriculture farms in Sindh and construction of Sukkur barrage. Haider Ali Karmal Jaghori was a prominent political thinker of the Hazara people in Pakistan, writing about the political history of Hazara people. His work "Hazaraha wa Hazarajat Bastan Dar Aiyna-i-Tarikh" was published in Quetta in 1992, and another work by Aziz Tughyan Hazara "Tarikh Milli Hazara" was published in 1984 in Quetta.

Most Pakistani Hazaras today live in the city of Quetta, in Balochistan, Pakistan. Localities in the city of Quetta with prominent Hazara populations include Hazara Town and Mehr Abad and Hazara tribes such as the "Sardar" are exclusively Pakistani. Literacy level among the Hazara community in Pakistan is relatively high compare to the Hazaras of Afghanistan, and they have integrated well into the social dynamics of the local society. Saira Batool, a Hazara woman, was one of the first female pilots in Pakistan Air Force. Other notable Hazara include Qazi Mohammad Esa, General Musa Khan Hazara, who served as Commander in Chief of the Pakistani Army from 1958 to 1968, Air Marshal Sharbat Ali Changezi, Hussain Ali Yousafi, the slain chairman of the Hazara Democratic Party, Syed Nasir Ali Shah, MNA from Quetta and his father Haji Sayed Hussain Hazara who was a senator and member of Majlis-e-Shura during the Zia-ul-Haq era.

Despite all of this, Hazaras are often targeted by militant groups such as the Lashkar-e-Jhangvi and others. "Activists say at least 800-1,000 Hazaras have been killed since 1999 and the pace is quickening. More than one hundred have been murdered in and around Quetta since January, according to Human Rights Watch." The political representation of the community is served by Hazara Democratic Party, a secular liberal democratic party, headed by Abdul Khaliq Hazara.

Hazaras in Iran are also referred to as Khawaris or Barbaris.
Over the many years as a result of political unrest in Afghanistan some Hazaras have migrated to Iran. The local Hazara population has been estimated at 500,000 people of which at least one third have spent more than half their life in Iran.

They have complained of discrimination in Iran. In March 2011, "Eurasia Daily Monitor" reported that representatives of Hazara community in Iran have asked Mongolia to intervene in supporting their case with Iranian government and prevent Iranian forced repatriation to Afghanistan.

The Hazara, outside of Hazarajat, have adopted the cultures of the cities where they dwell, resembling customs and traditions of the Afghan Tajiks and Pashtuns. Traditionally the Hazara are highland farmers and although sedentary, in the Hazarajat, they have retained many of their own customs and traditions, some of which are more closely related to those of Central Asia than to those of the Afghan Tajiks. For instance, many Hazara musicians are widely hailed as being skilled in playing the dambura, a native, regional lute instrument similarly found in other Central Asian nations such as Tajikistan, Uzbekistan and Kazakhstan. The Hazara live in houses rather than tents; Aimaqs and Aimaq Hazaras in tents rather than houses.

Hazara people living in Hazarajat (Hazaristan) areas speak the Hazaragi language of Afghanistan, which is infused with a significant number of Altaic loan words including Mongolic and Turkic. The primary differences between Dari and Hazaragi are the accent and Hazaragi's greater array of some Altaic loanwords. Despite these differences, Hazaragi is mutually intelligible with Dari, one of the official languages of Afghanistan.

Many of the urban Hazara in the larger cities such as Kabul and Mazar-i-Sharif no longer speak Hazaragi but speak standard literary Dari (usually the "Kābolī" dialect) or other regional varieties of Dari (for example the "Khorāsānī" dialect in the western region of Herat).

Hazara are predominantly Shi'a Muslims, mostly of the Twelver sect and some Ismaili. Since the majority of Afghans practice Sunni Islam, this may have contributed to the discrimination against the Hazara. Hazara probably converted to Shi'ism during the first part of the 16th century, in the early days of the Safavid Dynasty. Nonetheless, some small numbers of Hazara are Sunni, such as the Aimaq Hazaras and Taimuris. Sunni Hazara have been attached to non-Hazara tribes (such as Taimuris), while the Ismaili Hazara have always been kept separate from the rest of the Hazara on account of religious beliefs and political purposes.

The Hazara people have been organized by various tribes. They include Sheikh Ali, Jaghori, Ghaznichi, Muhammad Khwaja, Behsudi, Uruzgani, Daikundi, Daizangi, Turkmani, Dai Mirdadi and others. The different tribes come from regions such as Parwan, Bamyan, Ghazni, Ghor, Urozgan, Daykundi and Maidan Wardak and have spread outwards from Hazarajat (Main City) into Kabul and other parts of Afghanistan.

Many Hazaras engaged varieties of sports, including football, volleyball, wrestling, martial arts, boxing, karate, taekwondo, judo, wushu and more. Pahlawan Ebrahim Khedri, 62 kg wrestler, was the national champion for two decades in Afghanistan.
Rohullah Nikpai, won a bronze medal in Taekwondo in the Beijing Olympics 2008, beating world champion Juan Antonio Ramos of Spain 4–1 in a play-off final. It was Afghanistan's first-ever Olympic medal. He then won a second Olympic medal for Afghanistan in the London 2012 games. Afghanistan's first female Olympic athlete Friba Razayee, competed in judo at the 2004 Athens Olympics, but was eliminated in the first round of competition.

Other famous Hazara athlete Syed Abdul Jalil Waiz, was the first ever badminton player representing Afghanistan in Asian Junior Championships in 2005 where he produced the first win for his country against Iraq, with 15–13, 15–1. He participated in several international championships since 2005 and achieved victories against Australia, Philippines and Mongolia. Hamid Rahimi is a new boxer from Afghanistan and lives in Germany. Hazara famous football players are Zohib Islam Amiri, Ali Hazara, Moshtagh Yaghoubi, Mustafa Amini and Rahmat Akbari. Zohib Islam Amiri, is currently playing for the Afghanistan national football team.

A Pakistani Hazara named Abrar Hussain, a former Olympic boxer, served as deputy director general of the Pakistan Sports Board. He represented Pakistan three times at the Olympics and won a gold medal at the 1990 Asian Games in Beijing. Some Hazara from Pakistan have also excelled in sports and have received numerous awards particularly in boxing, football and in field hockey. Qayum Changezi, a legendary Pakistani football player, was a Hazara.
New Hazara youngsters are seen to appear in many sports in Pakistan mostly from Quetta. Rajab Ali Hazara, who is leading under 16 Pakistan Football team as captain.





</doc>
<doc id="14132" url="https://en.wikipedia.org/wiki?curid=14132" title="Hawala">
Hawala

Hawala or hewala ( , meaning "transfer" or sometimes "trust"), also known as ' in Persian, and ' or in Somali, is a popular and informal value transfer system based not on the movement of cash, or on telegraph or computer network wire transfers between banks, but instead on the performance and honour of a huge network of money brokers (known as "hawaladars"). While hawaladars are spread throughout the world, they are primarily located in the Middle East, North Africa, the Horn of Africa, and the Indian subcontinent, operating outside of, or parallel to, traditional banking, financial channels, and remittance systems. Hawala follows Islamic traditions but its use is not limited to Muslims.

The hawala system originated in India. It has existed since the 8th century between Indian, Arabic and Muslim traders alongside the Silk Road and beyond as a protection against theft. It is believed to have arisen in the financing of long-distance trade around the emerging capital trade centers in the early medieval period. In South Asia, it appears to have developed into a fully-fledged money market instrument, which was only gradually replaced by the instruments of the formal banking system in the first half of the 20th century.

"Hawala" itself influenced the development of the agency in common law and in civil laws, such as the "aval" in French law and the "avallo" in Italian law. The words "aval" and "avallo" were themselves derived from "hawala". The transfer of debt, which was "not permissible under Roman law but became widely practiced in medieval Europe, especially in commercial transactions", was due to the large extent of the "trade conducted by the Italian cities with the Muslim world in the Middle Ages". The agency was also "an institution unknown to Roman law" as no "individual could conclude a binding contract on behalf of another as his agent". In Roman law, the "contractor himself was considered the party to the contract and it took a second contract between the person who acted on behalf of a principal and the latter in order to transfer the rights and the obligations deriving from the contract to him". On the other hand, Islamic law and the later common law "had no difficulty in accepting agency as one of its institutions in the field of contracts and of obligations in general".

Today, hawala is probably used mostly for migrant workers' remittances to their countries of origin. 

In the most basic variant of the hawala system, money is transferred via a network of hawala brokers, or "hawaladars". It is the transfer of money without actually moving it. In fact, a successful definition of the hawala system that is used is "money transfer without money movement". According to author Sam Vaknin, while there are large hawaladar operators with networks of middlemen in cities across many countries, most hawaladars are small businesses who work at hawala as a sideline or moonlighting operation.
The figure shows how hawala works: (1) a customer ("A", left-hand side) approaches a hawala broker ("X") in one city and gives a sum of money (red arrow) that is to be transferred to a recipient ("B", right-hand side) in another, usually foreign, city. Along with the money, he usually specifies something like a password that will lead to the money being paid out (blue arrows). (2b) The hawa calls another hawala broker "M" in the recipient's city, and informs "M" about the agreed password, or gives other disposition of the funds. Then, the intended recipient ("B"), who also has been informed by "A" about the password (2a), now approaches "M" and tells him the agreed password (3a). If the password is correct, then "M" releases the transferred sum to "B" (3b), usually minus a small commission. "X" now basically owes "M" the money that "M" had paid out to "B"; thus "M" has to trust "X"s promise to settle the debt at a later date.

The unique feature of the system is that no promissory instruments are exchanged between the hawala brokers; the transaction takes place entirely on the honour system. As the system does not depend on the legal enforceability of claims, it can operate even in the absence of a legal and juridical environment. Trust and extensive use of connections are the components that distinguish it from other remittance systems. Hawaladar networks are often based on membership in the same family, village, clan, or ethnic group, and cheating is punished by effective ex-communication and "loss of honour"—leading to severe economic hardship.

Informal records are produced of individual transactions, and a running tally of the amount owed by one broker to another is kept. Settlements of debts between hawala brokers can take a variety of forms (such as goods, services, properties, transfers of employees, etc.), and need not take the form of direct cash transactions.

In addition to commissions, hawala brokers often earn their profits through bypassing official exchange rates. Generally, the funds enter the system in the source country's currency and leave the system in the recipient country's currency. As settlements often take place without any foreign exchange transactions, they can be made at other than official exchange rates.

Hawala is attractive to customers because it provides a fast and convenient transfer of funds, usually with a far lower commission than that charged by banks. Its advantages are most pronounced when the receiving country applies unprofitable exchange rate regulations or when the banking system in the receiving country is less complex (e.g., due to differences in legal environment in places such as Afghanistan, Yemen, Somalia). Moreover, in some parts of the world it is the only option for legitimate fund transfers, and has even been used by aid organizations in areas where it is the best-functioning institution.

Dubai has been prominent for decades as a welcoming hub for hawala transactions worldwide.

The "hundi" is a financial instrument that developed on the Indian sub-continent for use in trade and credit transactions. Hundis are used as a form of remittance instrument to transfer money from place to place, as a form of credit instrument or IOU to borrow money and as a bill of exchange in trade transactions. The Reserve Bank of India describes the Hundi as "an unconditional order in writing made by a person directing another to pay a certain sum of money to a person named in the order."

The word "angadia" means courier in Hindi, but also designates those who act as hawaladars within India. These people mostly act as a parallel banking system for businessmen. They charge a commission of around 0.2–0.5% per transaction from transferring money from one city to another.

According to the CIA, with the dissolution of Somalia's formal banking system, many informal money transfer operators arose to fill the void. It estimates that such "hawaladars", "xawilaad" or "xawala" brokers are now responsible for the transfer of up to $1.6 billion per year in remittances to the country, most coming from working Somalis outside Somalia. Such funds have in turn had a stimulating effect on local business activity.

The 2012 Tuareg rebellion left Northern Mali without an official money transfer service for months. The coping mechanisms that appeared were patterned on the hawala system.

Some government officials assert that hawala can be used to facilitate money laundering, avoid taxation, and move wealth anonymously. As a result, it is illegal in some U.S. states, India, Pakistan, and some other countries.

After the September 11 terrorist attacks, the American government suspected that some hawala brokers may have helped terrorist organizations transfer money to fund their activities, and the 9/11 Commission Report stated that "Al Qaeda frequently moved the money it raised by hawala". As a result of intense pressure from the U.S. authorities to introduce systematic anti-money laundering initiatives on a global scale, a number of hawala networks were closed down and a number of hawaladars were successfully prosecuted for money laundering. However, there is little evidence that these actions brought the authorities any closer to identifying and arresting a significant number of terrorists or drug smugglers. Experts emphasized that the overwhelming majority of those who used these informal networks were doing so for legitimate purposes, and simply chose to use a transaction medium other than state-supported banking systems. Today, the hawala system in Afghanistan is instrumental in providing financial services for the delivery of emergency relief and humanitarian and developmental aid for the majority of international and domestic NGOs, donor organizations, and development aid agencies.

In November 2001, the Bush administration froze the assets of Al-Barakat, a Somali remittance hawala company used primarily by a large number of Somali immigrants. Many of its agents in several countries were initially arrested, though later freed after no concrete evidence against them was found. In August 2006 the last Al-Barakat representatives were taken off the U.S. terror list, though some assets remain frozen. The mass media has speculated that pirates from Somalia use the hawala system to move funds internationally, for example into neighboring Kenya, where these transactions are neither taxed nor recorded.

In January 2010, the Kabul office of New Ansari Exchange, Afghanistan's largest hawala money transfer business, was closed following a raid by the Sensitive Investigative Unit, the country's national anti-political corruption unit, allegedly because this company was involved in laundering profits from the illicit opium trade and the moving of cash earned by government allied warlords through extortion and drug trafficking. Thousands of records were seized, from which links were found between money transfers by this company and political and business figures and NGOs in the country, including relatives of President Hamid Karzai. In August 2010, Karzai took control of the task force that staged the raid, and the US-advised anti-corruption group, the Major Crimes Task Force. He ordered a commission to review scores of past and current anti-corruption inquests.





</doc>
<doc id="14133" url="https://en.wikipedia.org/wiki?curid=14133" title="Hydroponics">
Hydroponics

Hydroponics is a type of Horticulture and a subset of hydroculture, which is a method of growing plants, usually crops, without soil, by using mineral nutrient solutions in an aqueous solvent. Terrestrial plants may be grown with only their roots exposed to the nutritious liquid, or, in addition, the roots may be physically supported by an inert medium such as perlite, gravel, or other substrates. Despite inert media, roots can cause changes of the rhizosphere pH and root exudates can affect the rhizosphere biology.

The nutrients used in hydroponic systems can come from many of different sources, including (but not limited to) fish excrement, duck manure, purchased chemical fertilisers, or artificial nutrient solutions.

Plants commonly grown hydroponically, on inert media, include tomatoes, peppers, cucumbers, lettuces, marijuana, and model plants like "Arabidopsis thaliana".

Hydroponics offers many advantages, one of them being a decrease in water usage for agriculture. To grow of tomatoes using intensive farming methods requires of water; using hydroponics, ; and only using aeroponics. Since it takes much less water to grow produce, it could be possible in the future for providers in harsh environments with little accessible water to grow their own food.

The earliest published work on growing terrestrial plants without soil was the 1627 book "Sylva Sylvarum" or 'A Natural History' by Francis Bacon, printed a year after his death. Water culture became a popular research technique after that. In 1699 John Woodward published his water culture experiments with spearmint. He found that plants in less-pure water sources grew better than plants in distilled water. By 1842, a list of nine elements believed to be essential for plant growth had been compiled, and the discoveries of German botanists Julius von Sachs and Wilhelm Knop, in the years 1859–1875, resulted in a development of the technique of soilless cultivation. Growth of terrestrial plants without soil in mineral nutrient solutions was called solution culture. It quickly became a standard research and teaching technique and is still widely used. Solution culture is now considered a type of hydroponics where there is an inert medium.

Around the 1930s plant scientists investigated diseases of certain plants, and thereby, observed symptoms related to existing soil conditions. In this context, water culture experiments were undertaken with the hope of delivering similar symptoms under controlled conditions. This approach forced by Dennis Robert Hoagland led to model systems playing an increasingly important role in plant research. In 1929, William Frederick Gericke of the University of California at Berkeley began publicly promoting that solution culture be used for agricultural crop production. He first termed it aquaculture but later found that aquaculture was already applied to culture of aquatic organisms. Gericke created a sensation by growing tomato vines high in his back yard in mineral nutrient solutions rather than soil. He introduced the term hydroponics, water culture, in 1937, proposed to him by , a phycologist with an extensive education in the classics. Hydroponics is derived from neologism υδρωπονικά (derived from Greek ύδωρ=water and πονέω=cultivate), constructed in analogy to γεωπονικά (derived from Greek γαία=earth and πονέω=cultivate), geoponica, that which concerns agriculture, replacing, γεω-, earth, with ὑδρο-, water.

Unfortunately, Gericke underestimated that the time was not yet ripe for the general technical application of hydroponics. Reports of Gericke's work and his claims that hydroponics would revolutionize plant agriculture prompted a huge number of requests for further information. Gericke had been denied use of the university's greenhouses for his experiments due to the administration's skepticism, and when the university tried to compel him to release his preliminary nutrient recipes developed at home he requested greenhouse space and time to improve them using appropriate research facilities. While he was eventually provided greenhouse space, the university assigned Hoagland and Arnon to re-evaluate Gericke's claims and show his formula held no benefit over soil grown plant yields, a view held by Hoagland. In 1940, Gericke published the book, "Complete Guide to Soilless Gardening," after leaving his academic position in 1937 in a climate that was politically unfavorable. Therein, for the first time, he published his basic formula involving the macro- and micronutrient salts for hydroponically-grown plants. 

As a result of research of Gericke's claims by order of the University of California, Dennis Robert Hoagland and Daniel Israel Arnon wrote a classic 1938 agricultural bulletin, "The Water Culture Method for Growing Plants Without Soil," which made the claim that hydroponic crop yields were no better than crop yields with good-quality soils. Ultimately, crop yields would be limited by factors other than mineral nutrients, especially light. However, this study did not adequately appreciate that hydroponics has other key benefits including the fact that the roots of the plant have constant access to oxygen and that the plants have access to as much or as little water as they need. This is important as one of the most common errors when growing is overwatering and underwatering; and hydroponics prevents this from occurring as large amounts of water, which may drown root systems in soil, can be made available to the plant, and any water not used, is drained away, recirculated, or actively aerated, thus, eliminating anoxic conditions. In soil, a grower needs to be very experienced to know exactly how much water to feed the plant. Too much and the plant will be unable to access oxygen; too little and the plant will lose the ability to transport nutrients, which are typically moved into the roots while in solution. Hoagland's views and helpful support by the University prompted these two researchers to develop several new formulas for mineral nutrient solutions, universally known as Hoagland solution. Modified Hoagland solutions will continue to be used, as will the hydroponic techniques proposed by Gericke.

One of the earliest successes of hydroponics occurred on Wake Island, a rocky atoll in the Pacific Ocean used as a refueling stop for Pan American Airlines. Hydroponics was used there in the 1930s to grow vegetables for the passengers. Hydroponics was a necessity on Wake Island because there was no soil, and it was prohibitively expensive to airlift in fresh vegetables.

From 1943 to 1946, Daniel I. Arnon served as a major in the United States Army and used his prior expertise with plant nutrition to feed troops stationed on barren Ponape Island in the western Pacific by growing crops in gravel and nutrient-rich water because there was no arable land available.

In the 1960s, Allen Cooper of England developed the nutrient film technique. The Land Pavilion at Walt Disney World's EPCOT Center opened in 1982 and prominently features a variety of hydroponic techniques.

In recent decades, NASA has done extensive hydroponic research for its Controlled Ecological Life Support System (CELSS). Hydroponics research mimicking a Martian environment uses LED lighting to grow in a different color spectrum with much less heat. Ray Wheeler, a plant physiologist at Kennedy Space Center's Space Life Science Lab, believes that hydroponics will create advances within space travel, as a bioregenerative life support system.

In 2007, Eurofresh Farms in Willcox, Arizona, sold more than 200 million pounds of hydroponically grown tomatoes. Eurofresh has under glass and represents about a third of the commercial hydroponic greenhouse area in the U.S. Eurofresh tomatoes were pesticide-free, grown in rockwool with top irrigation. Eurofresh declared bankruptcy, and the greenhouses were acquired by NatureSweet Ltd. in 2013.

As of 2017, Canada had hundreds of acres of large-scale commercial hydroponic greenhouses, producing tomatoes, peppers and cucumbers.

Due to technological advancements within the industry and numerous economic factors, the global hydroponics market is forecast to grow from US$226.45 million in 2016 to US$724.87 million by 2023.

There are two main variations for each medium: sub-irrigation and top irrigation. For all techniques, most hydroponic reservoirs are now built of plastic, but other materials have been used including concrete, glass, metal, vegetable solids, and wood. The containers should exclude light to prevent algae and fungal growth in the nutrient solution.

In static solution culture, plants are grown in containers of nutrient solution, such as glass Mason jars (typically, in-home applications), pots, buckets, tubs, or tanks. The solution is usually gently aerated but may be un-aerated. If un-aerated, the solution level is kept low enough that enough roots are above the solution so they get adequate oxygen. A hole is cut (or drilled) in the top of the reservoir for each plant; if it a jar or tub, it may be its lid, but otherwise, cardboard, foil, paper, wood or metal may be put on top. A single reservoir can be dedicated to a single plant, or to various plants. Reservoir size can be increased as plant size increases. A home-made system can be constructed from food containers or glass canning jars with aeration provided by an aquarium pump, aquarium airline tubing and aquarium valves. Clear containers are covered with aluminium foil, butcher paper, black plastic, or other material to exclude light, thus helping to eliminate the formation of algae. The nutrient solution is changed either on a schedule, such as once per week, or when the concentration drops below a certain level as determined with an electrical conductivity meter. Whenever the solution is depleted below a certain level, either water or fresh nutrient solution is added. A Mariotte's bottle, or a float valve, can be used to automatically maintain the solution level. In raft solution culture, plants are placed in a sheet of buoyant plastic that is floated on the surface of the nutrient solution. That way, the solution level never drops below the roots.

In continuous-flow solution culture, the nutrient solution constantly flows past the roots. It is much easier to automate than the static solution culture because sampling and adjustments to the temperature, pH, and nutrient concentrations can be made in a large storage tank that has potential to serve thousands of plants. A popular variation is the nutrient film technique or NFT, whereby a very shallow stream of water containing all the dissolved nutrients required for plant growth is recirculated past the bare roots of plants in a watertight thick root mat, which develops in the bottom of the channel and has an upper surface that, although moist, is in the air. Subsequent to this, an abundant supply of oxygen is provided to the roots of the plants. A properly designed NFT system is based on using the right channel slope, the right flow rate, and the right channel length. The main advantage of the NFT system over other forms of hydroponics is that the plant roots are exposed to adequate supplies of water, oxygen, and nutrients. In all other forms of production, there is a conflict between the supply of these requirements, since excessive or deficient amounts of one results in an imbalance of one or both of the others. NFT, because of its design, provides a system where all three requirements for healthy plant growth can be met at the same time, provided that the simple concept of NFT is always remembered and practised. The result of these advantages is that higher yields of high-quality produce are obtained over an extended period of cropping. A downside of NFT is that it has very little buffering against interruptions in the flow (e.g., power outages). But, overall, it is probably one of the more productive techniques.

The same design characteristics apply to all conventional NFT systems. While slopes along channels of 1:100 have been recommended, in practice it is difficult to build a base for channels that is sufficiently true to enable nutrient films to flow without ponding in locally depressed areas. As a consequence, it is recommended that slopes of 1:30 to 1:40 are used. This allows for minor irregularities in the surface, but, even with these slopes, ponding and water logging may occur. The slope may be provided by the floor, benches or racks may hold the channels and provide the required slope. Both methods are used and depend on local requirements, often determined by the site and crop requirements.

As a general guide, flow rates for each gully should be one liter per minute. At planting, rates may be half this and the upper limit of 2 L/min appears about the maximum. Flow rates beyond these extremes are often associated with nutritional problems. Depressed growth rates of many crops have been observed when channels exceed 12 meters in length. On rapidly growing crops, tests have indicated that, while oxygen levels remain adequate, nitrogen may be depleted over the length of the gully. As a consequence, channel length should not exceed 10–15 meters. In situations where this is not possible, the reductions in growth can be eliminated by placing another nutrient feed halfway along the gully and halving the flow rates through each outlet.

Aeroponics is a system wherein roots are continuously or discontinuously kept in an environment saturated with fine drops (a mist or aerosol) of nutrient solution. The method requires no substrate and entails growing plants with their roots suspended in a deep air or growth chamber with the roots periodically wetted with a fine mist of atomized nutrients. Excellent aeration is the main advantage of aeroponics.

Aeroponic techniques have proven to be commercially successful for propagation, seed germination, seed potato production, tomato production, leaf crops, and micro-greens. Since inventor Richard Stoner commercialized aeroponic technology in 1983, aeroponics has been implemented as an alternative to water intensive hydroponic systems worldwide. The limitation of hydroponics is the fact that of water can only hold of air, no matter whether aerators are utilized or not.

Another distinct advantage of aeroponics over hydroponics is that any species of plants can be grown in a true aeroponic system because the microenvironment of an aeroponic can be finely controlled. The limitation of hydroponics is that certain species of plants can only survive for so long in water before they become waterlogged. The advantage of aeroponics is that suspended aeroponic plants receive 100% of the available oxygen and carbon dioxide to the roots zone, stems, and leaves, thus accelerating biomass growth and reducing rooting times. NASA research has shown that aeroponically grown plants have an 80% increase in dry weight biomass (essential minerals) compared to hydroponically grown plants. Aeroponics used 65% less water than hydroponics. NASA also concluded that aeroponically grown plants require ¼ the nutrient input compared to hydroponics. Unlike hydroponically grown plants, aeroponically grown plants will not suffer transplant shock when transplanted to soil, and offers growers the ability to reduce the spread of disease and pathogens.
Aeroponics is also widely used in laboratory studies of plant physiology and plant pathology. Aeroponic techniques have been given special attention from NASA since a mist is easier to handle than a liquid in a zero-gravity environment.

Fogponics is a derivation of aeroponics wherein the nutrient solution is aerosolized by a diaphragm vibrating at ultrasonic frequencies. Solution droplets produced by this method tend to be 5–10 µm in diameter, smaller than those produced by forcing a nutrient solution through pressurized nozzles, as in aeroponics. The smaller size of the droplets allows them to diffuse through the air more easily, and deliver nutrients to the roots without limiting their access to oxygen.

Passive sub-irrigation, also known as passive hydroponics, semi-hydroponics, or "hydroculture", is a method wherein plants are grown in an inert porous medium that transports water and fertilizer to the roots by capillary action from a separate reservoir as necessary, reducing labor and providing a constant supply of water to the roots. In the simplest method, the pot sits in a shallow solution of fertilizer and water or on a capillary mat saturated with nutrient solution. The various hydroponic media available, such as expanded clay and coconut husk, contain more air space than more traditional potting mixes, delivering increased oxygen to the roots, which is important in epiphytic plants such as orchids and bromeliads, whose roots are exposed to the air in nature. Additional advantages of passive hydroponics are the reduction of root rot and the additional ambient humidity provided through evaporations.

Hydroculture compared to traditional farming in terms of crops yield per area in a controlled environment was roughly 10 times more efficient than traditional farming, uses 13 times less water in one crop cycle than traditional farming, but on average uses 100 times more kilojoules per kilogram of energy than traditional farming.

In its simplest form, there is a tray above a reservoir of nutrient solution. Either the tray is filled with growing medium (clay granules being the most common) and then plant directly or place the pot over medium, stand in the tray. At regular intervals, a simple timer causes a pump to fill the upper tray with nutrient solution, after which the solution drains back down into the reservoir. This keeps the medium regularly flushed with nutrients and air. Once the upper tray fills past the drain stop, it begins recirculating the water until the timer turns the pump off, and the water in the upper tray drains back into the reservoirs.

In a run-to-waste system, nutrient and water solution is periodically applied to the medium surface. The method was invented in Bengal in 1946; for this reason it is sometimes referred to as "The Bengal System".

This method can be set up in various configurations. In its simplest form, a nutrient-and-water solution is manually applied one or more times per day to a container of inert growing media, such as rockwool, perlite, vermiculite, coco fibre, or sand. In a slightly more complex system, it is automated with a delivery pump, a timer and irrigation tubing to deliver nutrient solution with a delivery frequency that is governed by the key parameters of plant size, plant growing stage, climate, substrate, and substrate conductivity, pH, and water content.

In a commercial setting, watering frequency is multi-factorial and governed by computers or PLCs.

Commercial hydroponics production of large plants like tomatoes, cucumber, and peppers uses one form or another of run-to-waste hydroponics.

In environmentally responsible uses, the nutrient-rich waste is collected and processed through an on-site filtration system to be used many times, making the system very productive.

Some bonsai are also grown in soil-free substrates (typically consisting of akadama, grit, diatomaceous earth and other inorganic components) and have their water and nutrients provided in a run-to-waste form.

The hydroponic method of plant production by means of suspending the plant roots in a solution of nutrient-rich, oxygenated water. Traditional methods favor the use of plastic buckets and large containers with the plant contained in a net pot suspended from the centre of the lid and the roots suspended in the nutrient solution.
The solution is oxygen saturated by an air pump combined with porous stones. With this method, the plants grow much faster because of the high amount of oxygen that the roots receive. The Kratky Method is similar to deep water culture, but uses a non-circulating water reservoir.

"Top-fed" deep water culture is a technique involving delivering highly oxygenated nutrient solution direct to the root zone of plants. While deep water culture involves the plant roots hanging down into a reservoir of nutrient solution, in top-fed deep water culture the solution is pumped from the reservoir up to the roots (top feeding). The water is released over the plant's roots and then runs back into the reservoir below in a constantly recirculating system. As with deep water culture, there is an airstone in the reservoir that pumps air into the water via a hose from outside the reservoir. The airstone helps add oxygen to the water. Both the airstone and the water pump run 24 hours a day.

The biggest advantage of top-fed deep water culture over standard deep water culture is increased growth during the first few weeks. With deep water culture, there is a time when the roots have not reached the water yet. With top-fed deep water culture, the roots get easy access to water from the beginning and will grow to the reservoir below much more quickly than with a deep water culture system. Once the roots have reached the reservoir below, there is not a huge advantage with top-fed deep water culture over standard deep water culture. However, due to the quicker growth in the beginning, grow time can be reduced by a few weeks.

A rotary hydroponic garden is a style of commercial hydroponics created within a circular frame which rotates continuously during the entire growth cycle of whatever plant is being grown.

While system specifics vary, systems typically rotate once per hour, giving a plant 24 full turns within the circle each 24-hour period. Within the center of each rotary hydroponic garden can be a high intensity grow light, designed to simulate sunlight, often with the assistance of a mechanized timer.

Each day, as the plants rotate, they are periodically watered with a hydroponic growth solution to provide all nutrients necessary for robust growth. Due to the plants continuous fight against gravity, plants typically mature much more quickly than when grown in soil or other traditional hydroponic growing systems. Because rotary hydroponic systems have a small size, it allows for more plant material to be grown per square foot of floor space than other traditional hydroponic systems.

One of the most obvious decisions hydroponic farmers have to make is which medium they should use. Different media are appropriate for different growing techniques.

Baked clay pellets are suitable for hydroponic systems in which all nutrients are carefully controlled in water solution. The clay pellets are inert, pH-neutral, and do not contain any nutrient value.

The clay is formed into round pellets and fired in rotary kilns at . This causes the clay to expand, like popcorn, and become porous. It is light in weight, and does not compact over time. The shape of an individual pellet can be irregular or uniform depending on brand and manufacturing process. The manufacturers consider expanded clay to be an ecologically sustainable and re-usable growing medium because of its ability to be cleaned and sterilized, typically by washing in solutions of white vinegar, chlorine bleach, or hydrogen peroxide (), and rinsing completely.

Another view is that clay pebbles are best not re-used even when they are cleaned, due to root growth that may enter the medium. Breaking open a clay pebble after a crop has been shown to reveal this growth.

Growstones, made from glass waste, have both more air and water retention space than perlite and peat. This aggregate holds more water than parboiled rice hulls. Growstones by volume consist of 0.5 to 5% calcium carbonate – for a standard 5.1 kg bag of Growstones that corresponds to 25.8 to 258 grams of calcium carbonate. The remainder is soda-lime glass.

Regardless of hydroponic demand, coconut coir is a natural byproduct derived from coconut processes. The outer husk of a coconut consists of fibers which are commonly used to make a myriad of items ranging from floor mats to brushes. After the long fibers are used for those applications, the dust and short fibers are merged to create coir. Coconuts absorb high levels of nutrients throughout their life cycle, so the coir must undergo a maturation process before it becomes a viable growth medium. This process removes salt, tannins and phenolic compounds through substantial water washing. Contaminated water is a byproduct of this process, as three hundred to six hundred liters of water per one cubic meter of coir is needed. Additionally, this maturation can take up to six months and one study concluded the working conditions during the maturation process are dangerous and would be illegal in North America and Europe. Despite requiring attention, posing health risks and environmental impacts, coconut coir has impressive material properties. When exposed to water, the brown, dry, chunky and fibrous material expands nearly three-four times its original size. This characteristic combined with coconut coir's water retention capacity and resistance to pests and diseases make it an effective growth medium. Used as an alternative to rock wool, coconut coir, also known as coir peat, offers optimized growing conditions.

Parboiled rice husks (PBH) are an agricultural byproduct that would otherwise have little use. They decay over time, and allow drainage, and even retain less water than growstones. A study showed that rice husks did not affect the effects of plant growth regulators.

Perlite is a volcanic rock that has been superheated into very lightweight expanded glass pebbles. It is used loose or in plastic sleeves immersed in the water. It is also used in potting soil mixes to decrease soil density. Perlite has similar properties and uses to vermiculite but, in general, holds more air and less water and is buoyant.

Like perlite, vermiculite is a mineral that has been superheated until it has expanded into light pebbles. Vermiculite holds more water than perlite and has a natural "wicking" property that can draw water and nutrients in a passive hydroponic system. If too much water and not enough air surrounds the plants roots, it is possible to gradually lower the medium's water-retention capability by mixing in increasing quantities of perlite.

Like perlite, pumice is a lightweight, mined volcanic rock that finds application in hydroponics.

Sand is cheap and easily available. However, it is heavy, does not hold water very well, and it must be sterilized between uses.
Due to sand being easily available and in high demand sand shortages are on our horizon as we are running out. 

The same type that is used in aquariums, though any small gravel can be used, provided it is washed first. Indeed, plants growing in a typical traditional gravel filter bed, with water circulated using electric powerhead pumps, are in effect being grown using gravel hydroponics. Gravel is inexpensive, easy to keep clean, drains well and will not become waterlogged. However, it is also heavy, and, if the system does not provide continuous water, the plant roots may dry out.

Wood fibre, produced from steam friction of wood, is a very efficient organic substrate for hydroponics. It has the advantage that it keeps its structure for a very long time. Wood wool (i.e. wood slivers) have been used since the earliest days of the hydroponics research. However, more recent research suggests that wood fibre may have detrimental effects on "plant growth regulators".

Wool from shearing sheep is a little-used yet promising renewable growing medium. In a study comparing wool with peat slabs, coconut fibre slabs, perlite and rockwool slabs to grow cucumber plants, sheep wool had a greater air capacity of 70%, which decreased with use to a comparable 43%, and water capacity that increased from 23% to 44% with use. Using sheep wool resulted in the greatest yield out of the tested substrates, while application of a biostimulator consisting of humic acid, lactic acid and Bacillus subtilis improved yields in all substrates.

Rock wool (mineral wool) is the most widely used medium in hydroponics. Rock wool is an inert substrate suitable for both run-to-waste and recirculating systems. Rock wool is made from molten rock, basalt or 'slag' that is spun into bundles of single filament fibres, and bonded into a medium capable of capillary action, and is, in effect, protected from most common microbiological degradation. Rock wool is typically used only for the seedling stage, or with newly cut clones, but can remain with the plant base for its lifetime. Rock wool has many advantages and some disadvantages. The latter being the possible skin irritancy (mechanical) whilst handling (1:1000). Flushing with cold water usually brings relief. Advantages include its proven efficiency and effectiveness as a commercial hydroponic substrate. Most of the rock wool sold to date is a non-hazardous, non-carcinogenic material, falling under Note Q of the European Union Classification Packaging and Labeling Regulation (CLP).

Mineral wool products can be engineered to hold large quantities of water and air that aid root growth and nutrient uptake in hydroponics; their fibrous nature also provides a good mechanical structure to hold the plant stable. The naturally high pH of mineral wool makes them initially unsuitable to plant growth and requires "conditioning" to produce a wool with an appropriate, stable pH.

Brick shards have similar properties to gravel. They have the added disadvantages of possibly altering the pH and requiring extra cleaning before reuse.

Polystyrene packing peanuts are inexpensive, readily available, and have excellent drainage. However, they can be too lightweight for some uses. They are used mainly in closed-tube systems. Note that non-biodegradable polystyrene peanuts must be used; biodegradable packing peanuts will decompose into a sludge. Plants may absorb styrene and pass it to their consumers; this is a possible health risk.

The formulation of hydroponic solutions is an application of plant nutrition, with nutrient deficiency symptoms mirroring those found in traditional soil based agriculture. However, the underlying chemistry of hydroponic solutions can differ from soil chemistry in many significant ways. Important differences include:

As in conventional agriculture, nutrients should be adjusted to satisfy Liebig's law of the minimum for each specific plant variety. Nevertheless, generally acceptable concentrations for nutrient solutions exist, with minimum and maximum concentration ranges for most plants being somewhat similar. Most nutrient solutions are mixed to have concentrations between 1,000 and 2,500 ppm. Acceptable concentrations for the individual nutrient ions, which comprise that total ppm figure, are summarized in the following table. For essential nutrients, concentrations below these ranges often lead to nutrient deficiencies while exceeding these ranges can lead to nutrient toxicity. Optimum nutrition concentrations for plant varieties are found empirically by experience or by plant tissue tests.

Organic fertilizers can be used to supplement or entirely replace the inorganic compounds used in conventional hydroponic solutions. However, using organic fertilizers introduces a number of challenges that are not easily resolved. Examples include:

Nevertheless, if precautions are taken, organic fertilizers can be used successfully in hydroponics.

Examples of suitable materials, with their average nutritional contents tabulated in terms of percent dried mass, are listed in the following table.
Micronutrients can be sourced from organic fertilizers as well. For example, composted pine bark is high in manganese and is sometimes used to fulfill that mineral requirement in hydroponic solutions. To satisfy requirements for National Organic Programs, pulverized, unrefined minerals (e.g. Gypsum, Calcite, and glauconite) can also be added to satisfy a plant's nutritional needs.

In addition to chelating agents, humic acids can be added to increase nutrient uptake.

Managing nutrient concentrations and pH values within acceptable ranges is essential for successful hydroponic horticulture. Common tools used to manage hydroponic solutions include:

Chemical equipment can also be used to perform accurate chemical analyses of nutrient solutions. Examples include:

Using chemical equipment for hydroponic solutions can be beneficial to growers of any background because nutrient solutions are often reusable. Because nutrient solutions are virtually never completely depleted, and should never be due to the unacceptably low osmotic pressure that would result, re-fortification of old solutions with new nutrients can save growers money and can control point source pollution, a common source for the eutrophication of nearby lakes and streams.

Although pre-mixed concentrated nutrient solutions are generally purchased from commercial nutrient manufacturers by hydroponic hobbyists and small commercial growers, several tools exist to help anyone prepare their own solutions without extensive knowledge about chemistry. The free and open source tools HydroBuddy and HydroCal have been created by professional chemists to help any hydroponics grower prepare their own nutrient solutions. The first program is available for Windows, Mac and Linux while the second one can be used through a simple JavaScript interface. Both programs allow for basic nutrient solution preparation although HydroBuddy provides added functionality to use and save custom substances, save formulations and predict electrical conductivity values.

Often mixing hydroponic solutions using individual salts is impractical for hobbyists or small-scale commercial growers because commercial products are available at reasonable prices. However, even when buying commercial products, multi-component fertilizers are popular. Often these products are bought as three part formulas which emphasize certain nutritional roles. For example, solutions for vegetative growth (i.e. high in nitrogen), flowering (i.e. high in potassium and phosphorus), and micronutrient solutions (i.e. with trace minerals) are popular. The timing and application of these multi-part fertilizers should coincide with a plant's growth stage. For example, at the end of an annual plant's life cycle, a plant should be restricted from high nitrogen fertilizers. In most plants, nitrogen restriction inhibits vegetative growth and helps induce flowering.

With pest problems reduced and nutrients constantly fed to the roots, productivity in hydroponics is high; however, growers can further increase yield by manipulating a plant's environment by constructing sophisticated growrooms.

To increase yield further, some sealed greenhouses inject CO into their environment to help improve growth and plant fertility.


</doc>
<doc id="14134" url="https://en.wikipedia.org/wiki?curid=14134" title="Humanist (disambiguation)">
Humanist (disambiguation)

Humanist may refer to:




</doc>
<doc id="14135" url="https://en.wikipedia.org/wiki?curid=14135" title="Henry Purcell">
Henry Purcell

Henry Purcell () ( September 1659 – 21 November 1695) was an English composer. Although it incorporated Italian and French stylistic elements, Purcell's was a uniquely English form of Baroque music. He is generally considered to be one of the greatest English composers; no later native-born English composer approached his fame until Edward Elgar, Ralph Vaughan Williams, William Walton and Benjamin Britten in the 20th century.

Purcell was born in St Ann's Lane, Old Pye Street, Westminster – the area of London later known as Devil's Acre – in 1659. Henry Purcell Senior, whose older brother Thomas Purcell was a musician, was a gentleman of the Chapel Royal and sang at the coronation of King Charles II of England. Henry the elder had three sons: Edward, Henry and Daniel. Daniel Purcell, the youngest of the brothers, was also a prolific composer who wrote the music for much of the final act of "The Indian Queen" after Henry Purcell's death. Henry Purcell's family lived just a few hundred yards west of Westminster Abbey from 1659 onwards.

After his father's death in 1664, Purcell was placed under the guardianship of his uncle Thomas, who showed him great affection and kindness. Thomas was himself a gentleman of His Majesty's Chapel, and arranged for Henry to be admitted as a chorister. Henry studied first under Captain Henry Cooke, Master of the Children, and afterwards under Pelham Humfrey, Cooke's successor. The composer Matthew Locke was a family friend and, particularly with his semi-operas, probably also had a musical influence on the young Purcell. Henry was a chorister in the Chapel Royal until his voice broke in 1673, when he became assistant to the organ-builder John Hingston, who held the post of keeper of wind instruments to the King.

Purcell is said to have been composing at nine years old, but the earliest work that can be certainly identified as his is an ode for the King's birthday, written in 1670. (The dates for his compositions are often uncertain, despite considerable research.) It is assumed that the three-part song "Sweet tyranness, I now resign" was written by him as a child. After Humfrey's death, Purcell continued his studies under Dr John Blow. He attended Westminster School and in 1676 was appointed copyist at Westminster Abbey. Henry Purcell's earliest anthem "Lord, who can tell" was composed in 1678. It is a psalm that is prescribed for Christmas Day and also to be read at morning prayer on the fourth day of the month.

In 1679, he wrote songs for John Playford's "Choice Ayres, Songs and Dialogues" and an anthem, the name of which is unknown, for the Chapel Royal. From an extant letter written by Thomas Purcell we learn that this anthem was composed for the exceptionally fine voice of the Rev. John Gostling, then at Canterbury, but afterwards a gentleman of His Majesty's Chapel. Purcell wrote several anthems at different times for Gostling's extraordinary basso profondo voice, which is known to have had a range of at least two full octaves, from D below the bass staff to the D above it. The dates of very few of these sacred compositions are known; perhaps the most notable example is the anthem "They that go down to the sea in ships." In gratitude for the providential escape of King Charles II from shipwreck, Gostling, who had been of the royal party, put together some verses from the Psalms in the form of an anthem and requested Purcell to set them to music. The challenging work opens with a passage which traverses the full extent of Gostling's range, beginning on the upper D and descending two octaves to the lower.

In 1679, Blow, who had been appointed organist of Westminster Abbey 10 years before, resigned his office in favour of Purcell. Purcell now devoted himself almost entirely to the composition of sacred music, and for six years severed his connection with the theatre. However, during the early part of the year, probably before taking up his new office, he had produced two important works for the stage, the music for Nathaniel Lee's "Theodosius", and Thomas d'Urfey's "Virtuous Wife". Between 1680 and 1688 Purcell wrote music for seven plays. The composition of his chamber opera "Dido and Aeneas", which forms a very important landmark in the history of English dramatic music, has been attributed to this period, and its earliest production may well have predated the documented one of 1689. It was written to a libretto furnished by Nahum Tate, and performed in 1689 in cooperation with Josias Priest, a dancing master and the choreographer for the Dorset Garden Theatre. Priest's wife kept a boarding school for young gentlewomen, first in Leicester Fields and afterwards at Chelsea, where the opera was performed. It is occasionally considered the first genuine English opera, though that title is usually given to Blow's "Venus and Adonis": as in Blow's work, the action does not progress in spoken dialogue but in Italian-style recitative. Each work runs to less than one hour. At the time, "Dido and Aeneas" never found its way to the theatre, though it appears to have been very popular in private circles. It is believed to have been extensively copied, but only one song was printed by Purcell's widow in "Orpheus Britannicus", and the complete work remained in manuscript until 1840, when it was printed by the Musical Antiquarian Society under the editorship of Sir George Macfarren. The composition of "Dido and Aeneas" gave Purcell his first chance to write a sustained musical setting of a dramatic text. It was his only opportunity to compose a work in which the music carried the entire drama. The story of "Dido and Aeneas" derives from the original source in Virgil's epic the "Aeneid".

Soon after Purcell's marriage, in 1682, on the death of Edward Lowe, he was appointed organist of the Chapel Royal, an office which he was able to hold simultaneously with his position at Westminster Abbey. His eldest son was born in this same year, but he was short-lived. His first printed composition, "Twelve Sonatas", was published in 1683. For some years after this, he was busy in the production of sacred music, odes addressed to the king and royal family, and other similar works. In 1685, he wrote two of his finest anthems, "I was glad" and "My heart is inditing," for the coronation of King James II. In 1690 he composed a setting of the birthday ode for Queen Mary, "Arise, my muse" and four years later wrote one of his most elaborate, important and magnificent works – a setting for another birthday ode for the Queen, written by Nahum Tate, entitled "Come Ye Sons of Art".
In 1687, he resumed his connection with the theatre by furnishing the music for John Dryden's tragedy "Tyrannick Love". In this year, Purcell also composed a march and passepied called "Quick-step", which became so popular that Lord Wharton adapted the latter to the fatal verses of "Lillibullero"; and in or before January 1688, Purcell composed his anthem "Blessed are they that fear the Lord" by express command of the King. A few months later, he wrote the music for D'Urfey's play, "The Fool's Preferment". In 1690, he composed the music for Betterton's adaptation of Fletcher and Massinger's "Prophetess" (afterwards called "Dioclesian") and Dryden's "Amphitryon". In 1691, he wrote the music for what is sometimes considered his dramatic masterpiece, "King Arthur", or "The British Worthy ". In 1692, he composed "The Fairy-Queen" (an adaptation of Shakespeare's "A Midsummer Night's Dream"), the score of which (his longest for theatre) was rediscovered in 1901 and published by the Purcell Society. "The Indian Queen" followed in 1695, in which year he also wrote songs for Dryden and Davenant's version of Shakespeare's "The Tempest" (recently, this has been disputed by music scholars), probably including "Full fathom five" and "Come unto these yellow sands". "The Indian Queen" was adapted from a tragedy by Dryden and Sir Robert Howard. In these semi-operas (another term for which at the time was "dramatic opera"), the main characters of the plays do not sing but speak their lines: the action moves in dialogue rather than recitative. The related songs are sung "for" them by singers, who have minor dramatic roles.

Purcell's "Te Deum" and "Jubilate Deo" were written for Saint Cecilia's Day, 1694, the first English "Te Deum" ever composed with orchestral accompaniment. This work was annually performed at St Paul's Cathedral until 1712, after which it was performed alternately with Handel's "Utrecht Te Deum and Jubilate" until 1743, when both works were replaced by Handel's "Dettingen Te Deum".

He composed an anthem and two elegies for Queen Mary II's funeral, his "Funeral Sentences and Music for the Funeral of Queen Mary". Besides the operas and semi-operas already mentioned, Purcell wrote the music and songs for Thomas d'Urfey's "The Comical History of Don Quixote", "Bonduca", "The Indian Queen" and others, a vast quantity of sacred music, and numerous odes, cantatas, and other miscellaneous pieces. The quantity of his instrumental chamber music is minimal after his early career, and his keyboard music consists of an even more minimal number of harpsichord suites and organ pieces. In 1693, Purcell composed music for two comedies: "The Old Bachelor", and "The Double Dealer". Purcell also composed for five other plays within the same year. In July 1695, Purcell composed an ode for the Duke of Gloucester for his sixth birthday. The ode is titled "Who can from joy refrain?" Purcell's four-part sonatas were issued in 1697. In the final six years of his life, Purcell wrote music for forty-two plays.

Purcell died in 1695 at his home in Marsham Street, at the height of his career. He is believed to have been 35 or 36 years old at the time. The cause of his death is unclear: one theory is that he caught a chill after returning home late from the theatre one night to find that his wife had locked him out. Another is that he succumbed to tuberculosis. The beginning of Purcell's will reads:

Purcell is buried adjacent to the organ in Westminster Abbey. The music that he had earlier composed for Queen Mary's funeral was performed during his funeral as well. Purcell was universally mourned as "a very great master of music."  Following his death, the officials at Westminster honoured him by unanimously voting that he be buried with no expense in the north aisle of the Abbey. His epitaph reads: "Here lyes Henry Purcell Esq., who left this life and is gone to that Blessed Place where only His harmony can be exceeded."

Purcell fathered six children by his wife Frances, four of whom died in infancy. His wife, as well as his son Edward (1689–1740) and daughter Frances, survived him. His wife Frances died in 1706, having published a number of her husband's works, including the now famous collection called "Orpheus Britannicus", in two volumes, printed in 1698 and 1702, respectively. Edward was appointed organist of St Clement's, Eastcheap, London, in 1711 and was succeeded by his son Edward Henry Purcell (died 1765). Both men were buried in St Clement's near the organ gallery.

Purcell worked in many genres, both in works closely linked to the court, such as symphony song, to the Chapel Royal, such as the symphony anthem, and the theatre.

Among Purcell's most notable works are his opera "Dido and Aeneas" (1688), his semi-operas "Dioclesian" (1690), "King Arthur" (1691), "The Fairy-Queen" (1692) and "Timon of Athens" (1695), as well as the compositions "Hail! Bright Cecilia" (1692), "Come Ye Sons of Art" (1694) and "Funeral Sentences and Music for the Funeral of Queen Mary" (1695).

After his death, Purcell was honoured by many of his contemporaries, including his old friend John Blow, who wrote "An Ode, on the Death of Mr. Henry Purcell (Mark how the lark and linnet sing)" with text by his old collaborator, John Dryden. William Croft's 1724 setting for the Burial Service, was written in the style of "the great Master". Croft preserved Purcell's setting of "Thou knowest Lord" (Z 58) in his service, for reasons "obvious to any artist"; it has been sung at every British state funeral ever since. More recently, the English poet Gerard Manley Hopkins wrote a famous sonnet entitled simply "Henry Purcell", with a headnote reading: "The poet wishes well to the divine genius of Purcell and praises him that, whereas other musicians have given utterance to the moods of man's mind, he has, beyond that, uttered in notes the very make and species of man as created both in him and in all men generally."

Purcell also had a strong influence on the composers of the English musical renaissance of the early 20th century, most notably Benjamin Britten, who arranged many of Purcell's vocal works for voice(s) and piano in "Britten's Purcell Realizations", including from "Dido and Aeneas", and whose "The Young Person's Guide to the Orchestra" is based on a theme from Purcell's "Abdelazar". Stylistically, the aria "I know a bank" from Britten's opera "A Midsummer Night's Dream" is clearly inspired by Purcell's aria "Sweeter than Roses", which Purcell originally wrote as part of incidental music to Richard Norton's "Pausanias, the Betrayer of His Country".

Purcell is honoured together with Johann Sebastian Bach and George Frideric Handel with a feast day on the liturgical calendar of the Episcopal Church (USA) on 28 July. In a 1940 interview Ignaz Friedman stated that he considered Purcell as great as Bach and Beethoven. In Victoria Street, Westminster, England, there is a bronze monument to Purcell, sculpted by Glynn Williams and erected in 1994.

Purcell's works have been catalogued by Franklin Zimmerman, who gave them a number preceded by Z.

A Purcell Club was founded in London in 1836 for promoting the performance of his music, but was dissolved in 1863. In 1876 a Purcell Society was founded, which published new editions of his works. A modern-day Purcell Club has been created, and provides guided tours and concerts in support of Westminster Abbey.

Today there is a Henry Purcell Society of Boston, which performs his music in live concert and currently is online streaming concerts, in response to the pandemic. There is a Purcell Society in London, which collects and studies Purcell manuscripts and musical scores, concentrating on producing revised versions of the scores of all his music.

So strong was his reputation that a popular wedding processional was incorrectly attributed to Purcell for many years. The so-called "Purcell's Trumpet Voluntary" was in fact written around 1700 by a British composer named Jeremiah Clarke as the "Prince of Denmark's March".

Music for the Funeral of Queen Mary was reworked by Wendy Carlos for the title music of the 1971 film by Stanley Kubrick, "A Clockwork Orange". The 1973 "Rolling Stone" review of Jethro Tull's "A Passion Play" compared the musical style of the album with that of Purcell.
In 2009 Pete Townshend of The Who, an English rock band that established itself in the 1960s, identified Purcell's harmonies, particularly the use of suspension and resolution that Townshend had learned from producer Kit Lambert, as an influence on the band's music (in songs such as "Won't Get Fooled Again" (1971), "I Can See for Miles" (1967) and the very Purcellian intro to "Pinball Wizard").

Purcell's music was widely featured as background music in the Academy Award winning 1979 film "Kramer vs. Kramer", with a soundtrack on CBS Masterworks Records.

In the 21st century, the soundtrack of the 2005 film version of "Pride and Prejudice" features a dance titled "A Postcard to Henry Purcell". This is a version by composer Dario Marianelli of Purcell's "Abdelazar" theme. In the German-language 2004 movie, "Downfall", the music of Dido's Lament is used repeatedly as the end of the Third Reich culminates. The 2012 film "Moonrise Kingdom" contains Benjamin Britten's version of the Rondeau in Purcell's "Abdelazar" created for his 1946 "The Young Person's Guide to the Orchestra". In 2013, the Pet Shop Boys released their single "Love Is a Bourgeois Construct" incorporating one of the same ground basses from "King Arthur" used by Nyman in his "Draughtsman's Contract" score. Olivia Chaney performs her adaptation of "There's Not a Swain" on her CD "The Longest River."

The 1995 film, "England, My England" tells the story of an actor who is himself writing a play about Purcell's life and music, and features many of his compositions.

Bibliography



</doc>
<doc id="14136" url="https://en.wikipedia.org/wiki?curid=14136" title="Hydrophobe">
Hydrophobe

In chemistry, hydrophobicity is the physical property of a molecule that is seemingly repelled from a mass of water (known as a hydrophobe). (Strictly speaking, there is no repulsive force involved; it is an absence of attraction.) In contrast, hydrophiles are attracted to water.

Hydrophobic molecules tend to be nonpolar and, thus, prefer other neutral molecules and nonpolar solvents. Because water molecules are polar, hydrophobes do not dissolve well among them. Hydrophobic molecules in water often cluster together, forming micelles. Water on hydrophobic surfaces will exhibit a high contact angle.

Examples of hydrophobic molecules include the alkanes, oils, fats, and greasy substances in general. Hydrophobic materials are used for oil removal from water, the management of oil spills, and chemical separation processes to remove non-polar substances from polar compounds.

Hydrophobic is often used interchangeably with lipophilic, "fat-loving". However, the two terms are not synonymous. While hydrophobic substances are usually lipophilic, there are exceptions, such as the silicones and fluorocarbons.

The term "hydrophobe" comes from the Ancient Greek ὑδρόφόβος (hýdrophóbos), "having a horror of water", constructed .

The hydrophobic interaction is mostly an entropic effect originating from the disruption of the highly dynamic hydrogen bonds between molecules of liquid water by the nonpolar solute forming a clathrate-like structure around the non-polar molecules. This structure formed is more highly ordered than free water molecules due to the water molecules arranging themselves to interact as much as possible with themselves, and thus results in a higher entropic state which causes non-polar molecules to clump together to reduce the surface area exposed to water and decrease the entropy of the system. Thus, the two immiscible phases (hydrophilic vs. hydrophobic) will change so that their corresponding interfacial area will be minimal. This effect can be visualized in the phenomenon called phase separation.

Superhydrophobic surfaces, such as the leaves of the lotus plant, are those that are extremely difficult to wet. The contact angles of a water droplet exceeds 150°. This is referred to as the lotus effect, and is primarily a physical property related to interfacial tension, rather than a chemical property.

In 1805, Thomas Young defined the contact angle "θ" by analyzing the forces acting on a fluid droplet resting on a solid surface surrounded by a gas.

where

"θ" can be measured using a contact angle goniometer.

Wenzel determined that when the liquid is in intimate contact with a microstructured surface, "θ" will change to "θ"

where "r" is the ratio of the actual area to the projected area. Wenzel's equation shows that microstructuring a surface amplifies the natural tendency of the surface. A hydrophobic surface (one that has an original contact angle greater than 90°) becomes more hydrophobic when microstructured – its new contact angle becomes greater than the original. However, a hydrophilic surface (one that has an original contact angle less than 90°) becomes more hydrophilic when microstructured – its new contact angle becomes less than the original.
Cassie and Baxter found that if the liquid is suspended on the tops of microstructures, "θ" will change to "θ":

where "φ" is the area fraction of the solid that touches the liquid. Liquid in the Cassie–Baxter state is more mobile than in the Wenzel state.

We can predict whether the Wenzel or Cassie–Baxter state should exist by calculating the new contact angle with both equations. By a minimization of free energy argument, the relation that predicted the smaller new contact angle is the state most likely to exist. Stated in mathematical terms, for the Cassie–Baxter state to exist, the following inequality must be true.

A recent alternative criterion for the Cassie–Baxter state asserts that the Cassie–Baxter state exists when the following 2 criteria are met:1) Contact line forces overcome body forces of unsupported droplet weight and 2) The microstructures are tall enough to prevent the liquid that bridges microstructures from touching the base of the microstructures.

A new criterion for the switch between Wenzel and Cassie-Baxter states has been developed recently based on surface roughness and surface energy. The criterion focuses on the air-trapping capability under liquid droplets on rough surfaces, which could tell whether Wenzel's model or Cassie-Baxter's model should be used for certain combination of surface roughness and energy.

Contact angle is a measure of static hydrophobicity, and contact angle hysteresis and slide angle are dynamic measures. Contact angle hysteresis is a phenomenon that characterizes surface heterogeneity. When a pipette injects a liquid onto a solid, the liquid will form some contact angle. As the pipette injects more liquid, the droplet will increase in volume, the contact angle will increase, but its three-phase boundary will remain stationary until it suddenly advances outward. The contact angle the droplet had immediately before advancing outward is termed the advancing contact angle. The receding contact angle is now measured by pumping the liquid back out of the droplet. The droplet will decrease in volume, the contact angle will decrease, but its three-phase boundary will remain stationary until it suddenly recedes inward. The contact angle the droplet had immediately before receding inward is termed the receding contact angle. The difference between advancing and receding contact angles is termed contact angle hysteresis and can be used to characterize surface heterogeneity, roughness, and mobility. Surfaces that are not homogeneous will have domains that impede motion of the contact line. The slide angle is another dynamic measure of hydrophobicity and is measured by depositing a droplet on a surface and tilting the surface until the droplet begins to slide. In general, liquids in the Cassie–Baxter state exhibit lower slide angles and contact angle hysteresis than those in the Wenzel state.

Dettre and Johnson discovered in 1964 that the superhydrophobic lotus effect phenomenon was related to rough hydrophobic surfaces, and they developed a theoretical model based on experiments with glass beads coated with paraffin or TFE telomer. The self-cleaning property of superhydrophobic micro-nanostructured surfaces was reported in 1977. Perfluoroalkyl, perfluoropolyether, and RF plasma -formed superhydrophobic materials were developed, used for electrowetting and commercialized for bio-medical applications between 1986 and 1995. Other technology and applications have emerged since the mid 1990s. A durable superhydrophobic hierarchical composition, applied in one or two steps, was disclosed in 2002 comprising nano-sized particles ≤ 100 nanometers overlaying a surface having micrometer-sized features or particles ≤ 100 micrometers. The larger particles were observed to protect the smaller particles from mechanical abrasion.

In recent research, superhydrophobicity has been reported by allowing alkylketene dimer (AKD) to solidify into a nanostructured fractal surface. Many papers have since presented fabrication methods for producing superhydrophobic surfaces including particle deposition, sol-gel techniques, plasma treatments, vapor deposition, and casting techniques. Current opportunity for research impact lies mainly in fundamental research and practical manufacturing. Debates have recently emerged concerning the applicability of the Wenzel and Cassie–Baxter models. In an experiment designed to challenge the surface energy perspective of the Wenzel and Cassie–Baxter model and promote a contact line perspective, water drops were placed on a smooth hydrophobic spot in a rough hydrophobic field, a rough hydrophobic spot in a smooth hydrophobic field, and a hydrophilic spot in a hydrophobic field. Experiments showed that the surface chemistry and geometry at the contact line affected the contact angle and contact angle hysteresis, but the surface area inside the contact line had no effect. An argument that increased jaggedness in the contact line enhances droplet mobility has also been proposed.

Many hydrophobic materials found in nature rely on Cassie's law and are biphasic on the submicrometer level with one component air. The lotus effect is based on this principle. Inspired by it, many functional superhydrophobic surfaces have been prepared.

An example of a bionic or biomimetic superhydrophobic material in nanotechnology is nanopin film.

One study presents a vanadium pentoxide surface that switches reversibly between superhydrophobicity and superhydrophilicity under the influence of UV radiation. According to the study, any surface can be modified to this effect by application of a suspension of rose-like VO particles, for instance with an inkjet printer. Once again hydrophobicity is induced by interlaminar air pockets (separated by 2.1 nm distances). The UV effect is also explained. UV light creates electron-hole pairs, with the holes reacting with lattice oxygen, creating surface oxygen vacancies, while the electrons reduce V to V. The oxygen vacancies are met by water, and it is this water absorbency by the vanadium surface that makes it hydrophilic. By extended storage in the dark, water is replaced by oxygen and hydrophilicity is once again lost.

A significant majority of hydrophobic surfaces have their hydrophobic properties imparted by structural or chemical modification of a surface of a bulk material, through either coatings or surface treatments. That is to say, the presence of molecular species (usually organic) or structural features results in high contact angles of water. In recent years, rare earth oxides have been shown to possess intrinsic hydrophobicity. The intrinsic hydrophobicity of rare earth oxides depends on surface orientation and oxygen vacancy levels, and is naturally more robust than coatings or surface treatments, having potential applications in condensers and catalysts that can operate at high temperatures or corrosive environments.

Hydrophobic concrete has been produced since the mid-20th century.

Active recent research on superhydrophobic materials might eventually lead to more industrial applications.

A simple routine of coating cotton fabric with silica or titania particles by sol-gel technique has been reported, which protects the fabric from UV light and makes it superhydrophobic.

An efficient routine has been reported for making polyethylene superhydrophobic and thus self-cleaning. 99% of dirt on such a surface is easily washed away.

Patterned superhydrophobic surfaces also have promise for lab-on-a-chip microfluidic devices and can drastically improve surface-based bioanalysis.

In pharmaceuticals, hydrophobicity of pharmaceutical blends affects important quality attributes of final products, such as drug dissolution and hardness. Methods have been developed to measure the hydrophobicity of pharmaceutical materials.




</doc>
<doc id="14142" url="https://en.wikipedia.org/wiki?curid=14142" title="Harley-Davidson">
Harley-Davidson

Harley-Davidson, Inc., H-D, or Harley, is an American motorcycle manufacturer founded in 1903 in Milwaukee, Wisconsin. Along with Indian it was one of two major American motorcycle manufacturers to survive the Great Depression, . The company has survived numerous ownership arrangements, subsidiary arrangements, periods of poor economic health and product quality, and intense global competition to become one of the world's largest motorcycle manufacturers and an iconic brand widely known for its loyal following. There are owner clubs and events worldwide, as well as a company-sponsored, brand-focused museum.

Harley-Davidson is noted for a style of customization that gave rise to the chopper motorcycle style. The company traditionally marketed heavyweight, air-cooled cruiser motorcycles with engine displacements greater than 700 cc, but it has broadened its offerings to include more contemporary VRSC (2002) and middle-weight Street (2015) platforms.

Harley-Davidson manufactures its motorcycles at factories in York, Pennsylvania; Milwaukee, Wisconsin; Kansas City, Missouri (closing); Manaus, Brazil; and Bawal, India. Construction of a new plant in Thailand began in late 2018. The company markets its products worldwide, and also licenses and markets merchandise under the Harley-Davidson brand, among them apparel, home decor and ornaments, accessories, toys, scale figures of its motorcycles, and video games based on its motorcycle line and the community.

In 1901, year-old William S. Harley drew up plans for a small engine with a displacement of 7.07 cubic inches (116 cc) and four-inch (102 mm) flywheels designed for use in a regular pedal-bicycle frame. Over the next two years, he and his childhood friend Arthur Davidson worked on their motor-bicycle using the northside Milwaukee machine shop at the home of their friend Henry Melk. It was finished in 1903 with the help of Arthur's brother Walter Davidson. Upon testing their power-cycle, Harley and the Davidson brothers found it unable to climb the hills around Milwaukee without pedal assistance, and they wrote off their first motor-bicycle as a valuable learning experiment.

The three began work on a new and improved machine with an engine of 24.74 cubic inches (405 cc) with flywheels weighing . Its advanced loop-frame pattern was similar to the 1903 Milwaukee Merkel motorcycle designed by Joseph Merkel, later of Flying Merkel fame. The bigger engine and loop-frame design took it out of the motorized bicycle category and marked the path to future motorcycle designs. They also received help with their bigger engine from outboard motor pioneer Ole Evinrude, who was then building gas engines of his own design for automotive use on Milwaukee's Lake Street.
The prototype of the new loop-frame Harley-Davidson was assembled in a shed in the Davidson family backyard. Most of the major parts, however, were made elsewhere, including some probably fabricated at the West Milwaukee railshops where oldest brother William A. Davidson was toolroom foreman. This prototype machine was functional by September 8, 1904, when it competed in a Milwaukee motorcycle race held at State Fair Park. Edward Hildebrand rode it and placed fourth in the race.

In January 1905, the company placed small advertisements in the "Automobile and Cycle Trade Journal" offering bare Harley-Davidson engines to the do-it-yourself trade. By April, they were producing complete motorcycles on a very limited basis. That year, Harley-Davidson dealer Carl H. Lang of Chicago sold three bikes from the five built in the Davidson backyard shed. Years later, the company moved the original shed to the Juneau Avenue factory where it stood for many decades as a tribute.

In 1906, Harley and the Davidson brothers built their first factory on Chestnut Street (later Juneau Avenue), at the current location of Harley-Davidson's corporate headquarters. The first Juneau Avenue plant was a single-story wooden structure. The company produced about 50 motorcycles that year.

In 1907, William S. Harley graduated from the University of Wisconsin–Madison with a degree in mechanical engineering. That year, they expanded the factory with a second floor and later with facings and additions of Milwaukee pale yellow ("cream") brick. With the new facilities, production increased to 150 motorcycles in 1907. The company was officially incorporated that September. They also began selling their motorcycles to police departments around this time, a market that has been important to them ever since. In 1907, William A. Davidson quit his job as tool foreman for the Milwaukee Road railroad and joined the Motor Company.

Production in 1905 and 1906 were all single-cylinder models with 26.84 cubic inch (440 cc) engines. In February 1907, they displayed a prototype model at the Chicago Automobile Show with a 45-degree V-Twin engine. Very few V-Twin models were built between 1907 and 1910. These first V-Twins displaced 53.68 cubic inches (880 cc) and produced about . This gave about double the power of the first singles, and top speed was about . Production jumped from 450 motorcycles in 1908 to 1,149 machines in 1909.

In 1911, the company introduced an improved V-Twin model with a displacement of 49.48 cubic inches (811 cc) and mechanically operated intake valves, as opposed to the "automatic" intake valves used on earlier V-Twins that opened by engine vacuum. It was smaller than earlier twins but gave better performance. After 1913, the majority of bikes produced by Harley-Davidson were V-Twin models.

In 1912, Harley-Davidson introduced their patented "Ful-Floteing Seat" which was suspended by a coil spring inside the seat tube. The spring tension could be adjusted to suit the rider's weight, and more than of travel was available. Harley-Davidson used seats of this type until 1958.

By 1913, the yellow brick factory had been demolished and a new five-story structure had been built on the site which took up two blocks along Juneau Avenue and around the corner on 38th Street. Despite the competition, Harley-Davidson was already pulling ahead of Indian and dominated motorcycle racing after 1914. Production that year swelled to 16,284 machines.
In 1917, the United States entered World War I and the military demanded motorcycles for the war effort. Harleys had already been used by the military in the Pancho Villa Expedition but World War I was the first time that it was adopted for military issue, first with the British Model H produced by Triumph Motorcycles Ltd in 1915. The U.S. military purchased over 20,000 motorcycles from Harley-Davidson.

Harley-Davidson launched a line of bicycles in 1917 in hopes of recruiting customers for its motorcycles. Models included the traditional diamond frame men's bicycle, a step-through frame 3–18 "Ladies Standard", and a 5–17 "Boy Scout" for youth. The effort was discontinued in 1923 because of disappointing sales. The bicycles were built for Harley-Davidson in Dayton, Ohio by the Davis Machine Company from 1917 to 1921, when Davis stopped manufacturing bicycles.

By 1920, Harley-Davidson was the largest motorcycle manufacturer in the world, with 28,189 machines produced and dealers in 67 countries. In 1921, Otto Walker set a record on a Harley-Davidson as the first motorcycle to win a race at an average speed greater than .

Harley-Davidson put several improvements in place during the 1920s, such as a new 74 cubic inch (1,212.6  cc) V-Twin introduced in 1921, and the "teardrop" gas tank in 1925. They added a front brake in 1928, although only on the J/JD models. In the late summer of 1929, Harley-Davidson introduced its 45 cubic inch (737 cc) flathead V-Twin to compete with the Indian 101 Scout and the Excelsior Super X. This was the "D" model produced from 1929 to 1931. Riders of Indian motorcycles derisively referred to it as the "three cylinder Harley" because the generator was upright and parallel to the front cylinder.

The Great Depression began a few months after the introduction of their model. Harley-Davidson's sales fell from 21,000 in 1929 to 3,703 in 1933. Despite this, Harley-Davidson unveiled a new lineup for 1934, which included a flathead engine and Art Deco styling.

In order to survive the remainder of the Depression, the company manufactured industrial powerplants based on their motorcycle engines. They also designed and built a three-wheeled delivery vehicle called the Servi-Car, which remained in production until 1973.

In the mid-1930s, Alfred Rich Child opened a production line in Japan with the VL. The Japanese license-holder, Sankyo Seiyaku Corporation, severed its business relations with Harley-Davidson in 1936 and continued manufacturing the VL under the Rikuo name.

An flathead engine was added to the line in 1935, by which time the single-cylinder motorcycles had been discontinued.

In 1936, the 61E and 61EL models with the "Knucklehead" OHV engines were introduced. Valvetrain problems in early Knucklehead engines required a redesign halfway through its first year of production and retrofitting of the new valvetrain on earlier engines.

By 1937, all Harley-Davidson flathead engines were equipped with dry-sump oil recirculation systems similar to the one introduced in the "Knucklehead" OHV engine. The revised V and VL models were renamed U and UL, the VH and VLH to be renamed UH and ULH, and the R to be renamed W.

In 1941, the 74 cubic-inch "Knucklehead" was introduced as the F and the FL. The flathead UH and ULH models were discontinued after 1941, while the 74 cubic inch U & UL flathead models were produced up to 1948.

One of only two American cycle manufacturers to survive the Great Depression, Harley-Davidson again produced large numbers of motorcycles for the US Army in World War II and resumed civilian production afterwards, producing a range of large V-twin motorcycles that were successful both on racetracks and for private buyers.

Harley-Davidson, on the eve of World War II, was already supplying the Army with a military-specific version of its WL line, called the WLA. The A in this case stood for "Army". Upon the outbreak of war, the company, along with most other manufacturing enterprises, shifted to war work. More than 90,000 military motorcycles, mostly WLAs and WLCs (the Canadian version) were produced, many to be provided to allies. Harley-Davidson received two Army-Navy 'E' Awards, one in 1943 and the other in 1945, which were awarded for Excellence in Production.
Shipments to the Soviet Union under the Lend-Lease program numbered at least 30,000. The WLAs produced during all four years of war production generally have 1942 serial numbers. Production of the WLA stopped at the end of World War II, but was resumed from 1950 to 1952 for use in the Korean War.

The U.S. Army also asked Harley-Davidson to produce a new motorcycle with many of the features of BMW's side-valve and shaft-driven R71. Harley-Davidson largely copied the BMW engine and drive train and produced the shaft-driven 750 cc 1942 Harley-Davidson XA. This shared no dimensions, no parts or no design concepts (except side valves) with any prior Harley-Davidson engine. Due to the superior cooling of the flat-twin engine with the cylinders across the frame, Harley's XA cylinder heads ran 100 °F (56 °C) cooler than its V-twins. The XA never entered full production: the motorcycle by that time had been eclipsed by the Jeep as the Army's general purpose vehicle, and the WLA—already in production—was sufficient for its limited police, escort, and courier roles. Only 1,000 were made and the XA never went into full production. It remains the only shaft-driven Harley-Davidson ever made.

As part of war reparations, Harley-Davidson acquired the design of a small German motorcycle, the DKW RT 125, which they adapted, manufactured, and sold from 1948 to 1966. Various models were made, including the Hummer from 1955 to 1959, but they are all colloquially referred to as "Hummers" at present. BSA in the United Kingdom took the same design as the foundation of their BSA Bantam.
In 1960, Harley-Davidson consolidated the Model 165 and Hummer lines into the Super-10, introduced the Topper scooter, and bought fifty percent of Aermacchi's motorcycle division. Importation of Aermacchi's 250 cc horizontal single began the following year. The bike bore Harley-Davidson badges and was marketed as the Harley-Davidson Sprint. The engine of the Sprint was increased to 350 cc in 1969 and would remain that size until 1974, when the four-stroke Sprint was discontinued.

After the Pacer and Scat models were discontinued at the end of 1965, the Bobcat became the last of Harley-Davidson's American-made two-stroke motorcycles. The Bobcat was manufactured only in the 1966 model year.

Harley-Davidson replaced their American-made lightweight two-stroke motorcycles with the Italian Aermacchi-built two-stroke powered M-65, M-65S, and Rapido. The M-65 had a semi-step-through frame and tank. The M-65S was a M-65 with a larger tank that eliminated the step-through feature. The Rapido was a larger bike with a 125 cc engine. The Aermacchi-built Harley-Davidsons became entirely two-stroke powered when the 250 cc two-stroke SS-250 replaced the four-stroke 350 cc Sprint in 1974.

Harley-Davidson purchased full control of Aermacchi's motorcycle production in 1974 and continued making two-stroke motorcycles there until 1978, when they sold the facility to Cagiva, owned by the Castiglioni family.

Established in 1918, the oldest continuously operating Harley-Davidson dealership outside of the United States is in Australia. Sales in Japan started in 1912 then in 1929, Harley-Davidsons were produced in Japan under license to the company Rikuo (Rikuo Internal Combustion Company) under the name of Harley-Davidson and using the company's tooling, and later under the name Rikuo. Production continued until 1958.

In 1952, following their application to the U.S. Tariff Commission for a 40 percent tax on imported motorcycles, Harley-Davidson was charged with restrictive practices.
In 1969, American Machine and Foundry (AMF) bought the company, streamlined production, and slashed the workforce. This tactic resulted in a labor strike and cost-cutting produced lower-quality bikes. The bikes were expensive and inferior in performance, handling, and quality to Japanese motorcycles. Sales and quality declined, and the company almost went bankrupt. The "Harley-Davidson" name was mocked as "Hardly Ableson", "Hardly Driveable," and "Hogly Ferguson",
and the nickname "Hog" became pejorative.

The early '70s saw the introduction of what the motoring press called the Universal Japanese Motorcycle in North America, that revolutionized the industry and made motorcycling in America more accessible during the 1970s and 1980s..

In 1977, following the successful manufacture of the Liberty Edition to commemorate America's bicentennial in 1976, Harley-Davidson produced what has become one of its most controversial models, the Harley-Davidson Confederate Edition. The bike was essentially a stock Harley-Davidson with Confederate-specific paint and details.

In 1981, AMF sold the company to a group of 13 investors led by Vaughn Beals and Willie G. Davidson for $80 million. Inventory was strictly controlled using the just-in-time system.

In the early eighties, Harley-Davidson claimed that Japanese manufacturers were importing motorcycles into the US in such volume as to harm or threaten to harm domestic producers. After an investigation by the U.S. International Trade Commission, President Reagan in 1983 imposed a 45 percent tariff on imported bikes with engine capacities greater than 700 cc. Harley-Davidson subsequently rejected offers of assistance from Japanese motorcycle makers.<ref name="7/83 US IMPOSES 45% TARIFF ON IMPORTED MOTORCYCLES"> – 7/83 US Imposes 45% Tariff on Imported Motorcycles</ref> However, the company did offer to drop the request for the tariff in exchange for loan guarantees from the Japanese.

Rather than trying to match the Japanese, the new management deliberately exploited the "retro" appeal of the machines, building motorcycles that deliberately adopted the look and feel of their earlier machines and the subsequent customizations of owners of that era. Many components such as brakes, forks, shocks, carburetors, electrics and wheels were outsourced from foreign manufacturers and quality increased, technical improvements were made, and buyers slowly returned.

Harley-Davidson bought the "Sub Shock" cantilever-swingarm rear suspension design from Missouri engineer Bill Davis and developed it into its Softail series of motorcycles, introduced in 1984 with the FXST Softail.

In response to possible motorcycle market loss due to the aging of baby-boomers, Harley-Davidson bought luxury motorhome manufacturer Holiday Rambler in 1986. In 1996, the company sold Holiday Rambler to the Monaco Coach Corporation.

The "Sturgis" model, boasting a dual belt-drive, was introduced initially in 1980 and was made for three years. This bike was then brought back as a commemorative model in 1991.
By 1990, with the introduction of the "Fat Boy", Harley-Davidson once again became the sales leader in the heavyweight (over 750 cc) market. At the time of the Fat Boy model introduction, a story rapidly spread that its silver paint job and other features were inspired by the B-29; and Fat Boy was a combination of the names of the atomic bombs Fat Man and Little Boy. However, the Urban Legend Reference Pages lists this story as an urban legend.

1993 and 1994 saw the replacement of FXR models with the Dyna (FXD), which became the sole rubber mount FX Big Twin frame in 1994. The FXR was revived briefly from 1999 to 2000 for special limited editions (FXR, FXR & FXR).

Construction started on the $75 million, 130,000 square-foot (12,000 m) Harley-Davidson Museum in the Menomonee Valley on June 1, 2006. It opened in 2008 and houses the company's vast collection of historic motorcycles and corporate archives, along with a restaurant, café and meeting space.

Harley-Davidson's association with sportbike manufacturer Buell Motorcycle Company began in 1987 when they supplied Buell with fifty surplus XR1000 engines. Buell continued to buy engines from Harley-Davidson until 1993, when Harley-Davidson bought 49 percent of the Buell Motorcycle Company. Harley-Davidson increased its share in Buell to ninety-eight percent in 1998, and to complete ownership in 2003.

In an attempt to attract newcomers to motorcycling in general and to Harley-Davidson in particular, Buell developed a low-cost, low-maintenance motorcycle. The resulting single-cylinder Buell Blast was introduced in 2000, and was made through 2009, which, according to Buell, was to be the final year of production. The Buell Blast was the training vehicle for the Harley-Davidson Rider's Edge New Rider Course from 2000 until May 2014, when the company re-branded the training academy and started using the Harley-Davidson Street 500 motorcycles. In those 14 years, more than 350,000 participants in the course learned to ride on the Buell Blast.

On October 15, 2009, Harley-Davidson Inc. issued an official statement that it would be discontinuing the Buell line and ceasing production immediately. The stated reason was to focus on the Harley-Davidson brand. The company refused to consider selling Buell. Founder Erik Buell subsequently established Erik Buell Racing and continued to manufacture and develop the company's 1125RR racing motorcycle.

In 1998 the first Harley-Davidson factory outside the US opened in Manaus, Brazil, taking advantage of the free economic zone there. The location was positioned to sell motorcycles in the southern hemisphere market.

Harley-Davidson hired Elton John to headline their 100th anniversary event on September 1, 2003. Other performers included The Doobie Brothers, Kid Rock, and Tim McGraw.

During its period of peak demand, during the late 1990s and early first decade of the 21st century, Harley-Davidson embarked on a program of expanding the number of dealerships throughout the country. At the same time, its current dealers typically had waiting lists that extended up to a year for some of the most popular models. Harley-Davidson, like the auto manufacturers, records a sale not when a consumer buys their product, but rather when it is delivered to a dealer. Therefore, it is possible for the manufacturer to inflate sales numbers by requiring dealers to accept more inventory than desired in a practice called channel stuffing. When demand softened following the unique 2003 model year, this news led to a dramatic decline in the stock price. In April 2004 alone, the price of HOG shares dropped from more than $60 to less than $40. Immediately prior to this decline, retiring CEO Jeffrey Bleustein profited $42 million on the exercise of employee stock options. Harley-Davidson was named as a defendant in numerous class action suits filed by investors who claimed they were intentionally defrauded by Harley-Davidson's management and directors. By January 2007, the price of Harley-Davidson shares reached $70.

Starting around 2000, several police departments started reporting problems with high speed instability on the Harley-Davidson Touring motorcycles. A Raleigh, North Carolina police officer, Charles Paul, was killed when his 2002 police touring motorcycle crashed after reportedly experiencing a high speed wobble. The California Highway Patrol conducted testing of the Police Touring motorcycles in 2006. The CHP test riders reported experiencing wobble or weave instability while operating the motorcycles on the test track.

On February 2, 2007, upon the expiration of their union contract, about 2,700 employees at Harley-Davidson Inc.'s largest manufacturing plant in York, Pennsylvania, went on strike after failing to agree on wages and health benefits. During the pendency of the strike, the company refused to pay for any portion of the striking employees' health care.

The day before the strike, after the union voted against the proposed contract and to authorize the strike, the company shut down all production at the plant. The York facility employs more than 3,200 workers, both union and non-union.

Harley-Davidson announced on February 16, 2007, that it had reached a labor agreement with union workers at its largest manufacturing plant, a breakthrough in the two-week-old strike. The strike disrupted Harley-Davidson's national production and was felt in Wisconsin, where 440 employees were laid off, and many Harley suppliers also laid off workers because of the strike.

On July 11, 2008, Harley-Davidson announced they had signed a definitive agreement to acquire the MV Agusta Group for US$109 million (€70M). MV Agusta Group contains two lines of motorcycles: the high-performance MV Agusta brand and the lightweight Cagiva brand. The acquisition was completed on August 8.

On October 15, 2009, Harley-Davidson announced that it would divest its interest in MV Agusta. Harley-Davidson Inc. sold Italian motorcycle maker MV Agusta to Claudio Castiglioni – a member of the family that had purchased Aermacchi from H-D in 1978 – for a reported 3 euros, ending the transaction in the first week of August 2010. Castiglioni was MV Agusta's former owner, and had been MV Agusta's chairman since Harley-Davidson bought it in 2008. As part of the deal, Harley-Davidson put $26M into MV Agusta's accounts, essentially giving Castiglioni $26M to take the brand.

In August 2009, Harley-Davidson announced plans to enter the market in India and started selling motorcycles there in 2010. The company established a subsidiary, Harley-Davidson India, in Gurgaon, near Delhi, in 2011 and created an Indian dealer network.

According to Interbrand, the value of the Harley-Davidson brand fell by 43 percent to $4.34 billion in 2009. The fall in value is believed to be connected to the 66 percent drop in the company profits in two-quarters of the previous year. On April 29, 2010, Harley-Davidson stated that they must cut $54 million in manufacturing costs from its production facilities in Wisconsin, and that they would explore alternative U.S. sites to accomplish this. The announcement came in the wake of a massive company-wide restructuring, which began in early 2009 and involved the closing of two factories, one distribution center, and the planned elimination of nearly 25 percent of its total workforce (around 3,500 employees). The company announced on September 14, 2010, that it would remain in Wisconsin.

The classic Harley-Davidson engines are V-twin engines, with a 45° angle between the cylinders. The crankshaft has a single pin, and both pistons are connected to this pin through their connecting rods.

This 45° angle is covered under several United States patents and is an engineering tradeoff that allows a large, high-torque engine in a relatively small space. It causes the cylinders to fire at uneven intervals and produces the choppy "potato-potato" sound so strongly linked to the Harley-Davidson brand.

To simplify the engine and reduce costs, the V-twin ignition was designed to operate with a single set of points and no distributor. This is known as a dual fire ignition system, causing both spark plugs to fire regardless of which cylinder was on its compression stroke, with the other spark plug firing on its cylinder's exhaust stroke, effectively "wasting a spark". The exhaust note is basically a throaty growling sound with some popping.
The 45° design of the engine thus creates a plug firing sequencing as such: The first cylinder fires, the second (rear) cylinder fires 315° later, then there is a 405° gap until the first cylinder fires again, giving the engine its unique sound.

Harley-Davidson has used various ignition systems throughout its history – be it the early points and condenser system, (Big Twin up to 1978 and Sportsters up to 1978), magneto ignition system used on some 1958 to 1969 Sportsters, early electronic with centrifugal mechanical advance weights, (all models 1978 and a half to 1979), or the late electronic with transistorized ignition control module, more familiarly known as the black box or the brain, (all models 1980 to present).

Starting in 1995, the company introduced Electronic Fuel Injection (EFI) as an option for the 30th anniversary edition Electra Glide. EFI became standard on all Harley-Davidson motorcycles, including Sportsters, upon the introduction of the 2007 product line.

In 1991, Harley-Davidson began to participate in the Sound Quality Working Group, founded by Orfield Labs, Bruel and Kjaer, TEAC, Yamaha, Sennheiser, SMS and Cortex. This was the nation's first group to share research on psychological acoustics. Later that year, Harley-Davidson participated in a series of sound quality studies at Orfield Labs, based on recordings taken at the Talladega Superspeedway, with the objective to lower the sound level for EU standards while analytically capturing the "Harley Sound". This research resulted in the bikes that were introduced in compliance with EU standards for 1998.

On February 1, 1994, the company filed a sound trademark application for the distinctive sound of the Harley-Davidson motorcycle engine: "The mark consists of the exhaust sound of applicant's motorcycles, produced by V-twin, common crankpin motorcycle engines when the goods are in use". Nine of Harley-Davidson's competitors filed comments opposing the application, arguing that cruiser-style motorcycles of various brands use a single-crankpin V-twin engine which produce a similar sound. These objections were followed by litigation. In June 2000, the company dropped efforts to federally register its trademark.



The Revolution engine is based on the VR-1000 Superbike race program, developed by Harley-Davidson's Powertrain Engineering Porsche only helped them make it for street use . It is a liquid cooled, dual overhead cam, internally counterbalanced 60 degree V-twin engine with a displacement of 69 cubic inch (1,130 cc), producing at 8,250 rpm at the crank, with a redline of 9,000 rpm. It was introduced for the new VRSC (V-Rod) line in 2001 for the 2002 model year, starting with the single VRSCA (V-Twin Racing Street Custom) model. The Revolution marks Harley's first collaboration with Porsche since the V4 Nova project, which, like the V-Rod, was a radical departure from Harley's traditional lineup until it was cancelled by AMF in 1981 in favor of the Evolution engine.

A 1,250 cc Screamin' Eagle version of the Revolution engine was made available for 2005 and 2006, and was present thereafter in a single production model from 2005 to 2007. In 2008, the 1,250 cc Revolution Engine became standard for the entire VRSC line. Harley-Davidson claims at the crank for the 2008 VRSCAW model. The VRXSE "Destroyer" is equipped with a stroker (75 mm crank) Screamin' Eagle 79 cubic inch (1,300 cc) Revolution Engine, producing more than .

750 cc and 500 cc versions of the Revolution engine are used in Harley-Davidson's Street line of light cruisers. These motors, named the Revolution X, use a single overhead cam, screw and locknut valve adjustment, a single internal counterbalancer, and vertically split crankcases; all of these changes making it different from the original Revolution design.

An extreme endurance test of the Revolution engine was performed in a dynometer installation at the harley faculties in millwaukee, simulating the German Autobahn (highways without general speed limit) between the Porsche research and development center in Weissach, near Stuttgart to Düsseldorf. Uncounted samples of engines failed, until an engine successfully passed the 500-hour nonstop run. This was the benchmark for the engineers to approve the start of production for the Revolution engine, which was documented in the Discovery channel special Harley-Davidson: Birth of the V-Rod, October 14, 2001.

The first Harley-Davidson motorcycles were powered by single-cylinder IOE engines with the inlet valve operated by engine vacuum, based on the DeDion-Bouton pattern. Singles of this type continued to be made until 1913, when a pushrod and rocker system was used to operate the overhead inlet valve on the single, a similar system having been used on their V-twins since 1911. Single-cylinder motorcycle engines were discontinued in 1918.

Single-cylinder engines were reintroduced in 1925 as 1926 models. These singles were available either as flathead engines or as overhead valve engines until 1930, after which they were only available as flatheads. The flathead single-cylinder motorcycles were designated Model A for engines with magneto systems only and Model B for engines with battery and coil systems, while overhead valve versions were designated Model AA and Model BA respectively, and a magneto-only racing version was designated Model S. This line of single-cylinder motorcycles ended production in 1934.

Modern Harley-branded motorcycles fall into one of seven model families: Touring, Softail, Dyna, Sportster, Vrod, Street and LiveWire. These model families are distinguished by the frame, engine, suspension, and other characteristics.

Touring models use Big-Twin engines and large-diameter telescopic forks. All Touring designations begin with the letters FL, "e.g.", FLHR (Road King) and FLTR (Road Glide).

The touring family, also known as "dressers" or "baggers", includes Road King, Road Glide, Street Glide and Electra Glide models offered in various trims. The Road Kings have a "retro cruiser" appearance and are equipped with a large clear windshield. Road Kings are reminiscent of big-twin models from the 1940s and 1950s. Electra Glides can be identified by their full front fairings. Most Electra Glides sport a fork-mounted fairing referred to as the "Batwing" due to its unmistakable shape. The Road Glide and Road Glide Ultra Classic have a frame-mounted fairing, referred to as the "Sharknose". The Sharknose includes a unique, dual front headlight.

Touring models are distinguishable by their large saddlebags, rear coil-over air suspension and are the only models to offer full fairings with radios and CBs. All touring models use the same frame, first introduced with a Shovelhead motor in 1980, and carried forward with only modest upgrades until 2009, when it was extensively redesigned. The frame is distinguished by the location of the steering head in front of the forks and was the first H-D frame to rubber mount the drivetrain to isolate the rider from the vibration of the big V-twin.

The frame was modified for the 1993 model year when the oil tank went under the transmission and the battery was moved inboard from under the right saddlebag to under the seat. In 1997, the frame was again modified to allow for a larger battery under the seat and to lower seat height. In 2007, Harley-Davidson introduced the Twin Cam 96 engine, as well the six-speed transmission to give the rider better speeds on the highway.

In 2006, Harley introduced the FLHX Street Glide, a bike designed by Willie G. Davidson to be his personal ride, to its touring line.

In 2008, Harley added anti-lock braking systems and cruise control as a factory installed option on all touring models (standard on CVO and Anniversary models). Also new for 2008 is the fuel tank for all touring models. 2008 also brought throttle-by-wire to all touring models.

For the 2009 model year, Harley-Davidson redesigned the entire touring range with several changes, including a new frame, new swingarm, a completely revised engine-mounting system, front wheels for all but the FLHRC Road King Classic, and a 2–1–2 exhaust. The changes result in greater load carrying capacity, better handling, a smoother engine, longer range and less exhaust heat transmitted to the rider and passenger.
Also released for the 2009 model year is the FLHTCUTG Tri-Glide Ultra Classic, the first three-wheeled Harley since the Servi-Car was discontinued in 1973. The model features a unique frame and a 103-cubic-inch (1,690 cc) engine exclusive to the trike.

In 2014, Harley-Davidson released a redesign for specific touring bikes and called it "Project Rushmore". Changes include a new 103CI High Output engine, one handed easy open saddlebags and compartments, a new Boom! Box Infotainment system with either 4.3-inch (10 cm) or 6.5-inch (16.5 cm) screens featuring touchscreen functionality [6.5-inch (16.5 cm) models only], Bluetooth (media and phone with approved compatible devices), available GPS and SiriusXM, Text-to-Speech functionality (with approved compatible devices) and USB connectivity with charging. Other features include ABS with Reflex linked brakes, improved styling, Halogen or LED lighting and upgraded passenger comfort.

These big-twin motorcycles capitalize on Harley's strong value on tradition. With the rear-wheel suspension hidden under the transmission, they are visually similar to the "hardtail" choppers popular in the 1960s and 1970s, as well as from their own earlier history. In keeping with that tradition, Harley offers Softail models with "Heritage" styling that incorporate design cues from throughout their history and used to offer "Springer" front ends on these Softail models from the factory.

Softail models utilize the big-twin engine (F) and the Softail chassis (ST).

Dyna-frame motorcycles were developed in the 1980s and early 1990s and debuted in the 1991 model year with the FXDB Sturgis offered in limited edition quantities. In 1992 the line continued with the limited edition FXDB Daytona and a production model FXD Super Glide. The new DYNA frame featured big-twin engines and traditional styling. They can be distinguished from the Softail by the traditional coil-over suspension that connects the swingarm to the frame, and from the Sportster by their larger engines. On these models, the transmission also houses the engine's oil reservoir.

Prior to 2006, Dyna models typically featured a narrow, XL-style 39mm front fork and front wheel, as well as footpegs which the manufacturer included the letter "X" in the model designation to indicate. This lineup traditionally included the Super Glide (FXD), Super Glide Custom (FXDC), Street Bob (FXDB), and Low Rider (FXDL). One exception was the Wide Glide (FXDWG), which featured thicker 41mm forks and a narrow front wheel, but positioned the forks on wider triple-trees that give a beefier appearance. In 2008, the Dyna Fat Bob (FXDF) was introduced to the Dyna lineup, featuring aggressive styling like a new 2–1–2 exhaust, twin headlamps, a 180 mm rear tire, and, for the first time in the Dyna lineup, a 130 mm front tire. For the 2012 model year, the Dyna Switchback (FLD) became the first Dyna to break the tradition of having an FX model designation with floorboards, detachable painted hard saddlebags, touring windshield, headlight nacelle and a wide front tire with full fender. The new front end resembled the big-twin FL models from 1968 to 1971.

The Dyna family used the 88-cubic-inch (1,440 cc) twin cam from 1999 to 2006. In 2007, the displacement was increased to 96 cubic inches (1,570 cc) as the factory increased the stroke to . For the 2012 model year, the manufacturer began to offer Dyna models with the 103-cubic-inch (1,690 cc) upgrade. All Dyna models use a rubber-mounted engine to isolate engine vibration. Harley discontinued the Dyna platform in 2017 for the 2018 model year, having been replaced by a completely-redesigned Softail chassis; some of the existing models previously released by the company under the Dyna nameplate have since been carried over to the new Softail line.
Dyna models utilize the big-twin engine (F), footpegs noted as (X) with the exception of the 2012 FLD Switchback, a Dyna model which used floorboards as featured on the Touring (L) models, and the Dyna chassis (D). Therefore, except for the FLD from 2012 to 2016, all Dyna models have designations that begin with FXD, "e.g.", FXDWG (Dyna Wide Glide) and FXDL (Dyna Low Rider).

Introduced in 1957, the Sportster family were conceived as racing motorcycles, and were popular on dirt and flat-track race courses through the 1960s and 1970s. Smaller and lighter than the other Harley models, contemporary Sportsters make use of 883 cc or 1,200 cc Evolution engines and, though often modified, remain similar in appearance to their racing ancestors.

Up until the 2003 model year, the engine on the Sportster was rigidly mounted to the frame. The 2004 Sportster received a new frame accommodating a rubber-mounted engine. This made the bike heavier and reduced the available lean angle, while it reduced the amount of vibration transmitted to the frame and the rider, providing a smoother ride for rider and passenger.

In the 2007 model year, Harley-Davidson celebrated the 50th anniversary of the Sportster and produced a limited edition called the XL50, of which only 2000 were made for sale worldwide. Each motorcycle was individually numbered and came in one of two colors, Mirage Pearl Orange or Vivid Black. Also in 2007, electronic fuel injection was introduced to the Sportster family, and the Nightster model was introduced in mid-year. In 2009, Harley-Davidson added the Iron 883 to the Sportster line, as part of the Dark Custom series.
In the 2008 model year, Harley-Davidson released the XR1200 Sportster in Europe, Africa, and the Middle East. The XR1200 had an Evolution engine tuned to produce , four-piston dual front disc brakes, and an aluminum swing arm. "Motorcyclist" featured the XR1200 on the cover of its July 2008 issue and was generally positive about it in their "First Ride" story, in which Harley-Davidson was repeatedly asked to sell it in the United States.
One possible reason for the delayed availability in the United States was the fact that Harley-Davidson had to obtain the "XR1200" naming rights from Storz Performance, a Harley customizing shop in Ventura, Calif. The XR1200 was released in the United States in 2009 in a special color scheme including Mirage Orange highlighting its dirt-tracker heritage. The first 750 XR1200 models in 2009 were pre-ordered and came with a number 1 tag for the front of the bike, autographed by Kenny Coolbeth and Scott Parker and a thank you/welcome letter from the company, signed by Bill Davidson. The XR1200 was discontinued in model year 2013.

Except for the street-going XR1000 of the 1980s and the XR1200, most Sportsters made for street use have the prefix XL in their model designation. For the Sportster Evolution engines used since the mid-1980s, there have been two engine sizes. Motorcycles with the smaller engine are designated XL883, while those with the larger engine were initially designated XL1100. When the size of the larger engine was increased from 1,100 cc to 1,200 cc, the designation was changed accordingly from XL1100 to XL1200. Subsequent letters in the designation refer to model variations within the Sportster range, e.g. the XL883C refers to an 883 cc Sportster Custom, while the XL1200S designates the now-discontinued 1200 Sportster Sport.

Introduced in 2001 and produced until 2017, the VRSC muscle bike family bears little resemblance to Harley's more traditional lineup. Competing against Japanese and American muscle bikes in the upcoming muscle bike/power cruiser segment, the "V-Rod" makes use of the revolution engine that, for the first time in Harley history, incorporates overhead cams and liquid cooling. The V-Rod is visually distinctive, easily identified by the 60-degree V-Twin engine, the radiator and the hydroformed frame members that support the round-topped air cleaner cover. The VRSC platform was also used for factory drag-racing motorcycles.

In 2008, Harley added the anti-lock braking system as a factory installed option on all VRSC models. Harley also increased the displacement of the stock engine from , which had only previously been available from Screamin' Eagle, and added a slipper clutch as standard equipment.

VRSC models include:


VRSC models utilize the Revolution engine (VR), and the street versions are designated Street Custom (SC). After the VRSC prefix common to all street Revolution bikes, the next letter denotes the model, either A (base V-Rod: discontinued), AW (base V-Rod + W for Wide with a 240 mm rear tire), B (discontinued), D (Night Rod: discontinued), R (Street Rod: discontinued), SE and SEII(CVO Special Edition), or X (Special edition). Further differentiation within models are made with an additional letter, "e.g.", VRSCDX denotes the Night Rod Special.

The VRXSE V-Rod Destroyer is Harley-Davidson's production drag racing motorcycle, constructed to run the quarter mile in less than ten seconds. It is based on the same revolution engine that powers the VRSC line, but the VRXSE uses the Screamin' Eagle 1,300 cc "stroked" incarnation, featuring a 75 mm crankshaft, 105 mm Pistons, and 58 mm throttle bodies.

The V-Rod Destroyer is not a street legal motorcycle. As such, it uses "X" instead of "SC" to denote a non-street bike. "SE" denotes a CVO Special Edition.

The Street, Harley-Davidson's newest platform and their first all new platform in thirteen years, was designed to appeal to younger riders looking for a lighter bike at a cheaper price. The Street 750 model was launched in India at the 2014 Indian Auto Expo, Delhi-NCR on February 5, 2014. The Street 750 weighs 218 kg and has a ground clearance of 144 mm giving it the lowest weight and the highest ground clearance of Harley-Davidson motorcycles currently available.

The Street 750 uses an all-new, liquid-cooled, 60° V-twin engine called the Revolution X. In the Street 750, the engine displaces and produces 65 Nm at 4,000 rpm. A six speed transmission is used.

The Street 750 and the smaller-displacement Street 500 has been available since late 2014. Street series motorcycles for the North American market will be built in Harley-Davidson's Kansas City, Missouri plant, while those for other markets around the world will be built completely in their plant in Bawal, India.

Harley-Davidson's "LiveWire", released in 2019, is their first electric vehicle. The high-voltage battery provides a minimum city range of 98 miles (158 km). The LiveWire targets a different type of customer than their classic V-twin powered motorcycles.

In March 2020, a Harley-Davidson LiveWire was used to break the 24-hour distance record for an electric motorcycle. The bike traveled a reported 1,723 km (1,079 miles) in 23 hours and 48 minutes. The LiveWire offers a Level 1 slow recharge, which uses a regular wall outlet to refill an empty battery overnight, or a quick Level 3 DC Fast Charge. The Fast Charge fills the battery most of the way in about 40 minutes. Swiss rider Michel von Tell used the Level 3 charging to make the 24-hour ride.

Custom Vehicle Operations (CVO) is a team within Harley-Davidson that produces limited-edition customizations of Harley's stock models. Every year since 1999, the team has selected two to five of the company's base models and added higher-displacement engines, performance upgrades, special-edition paint jobs, more chromed or accented components, audio system upgrades, and electronic accessories to create high-dollar, premium-quality customizations for the factory custom market. The models most commonly upgraded in such a fashion are the Ultra Classic Electra Glide, which has been selected for CVO treatment every year from 2006 to the present, and the Road King, which was selected in 2002, 2003, 2007, and 2008. The Dyna, Softail, and VRSC families have also been selected for CVO customization.

The Environmental Protection Agency conducted emissions-certification and representative emissions test in Ann Arbor, Michigan, in 2005. Subsequently, Harley-Davidson produced an "environmental warranty". The warranty ensures each owner that the vehicle is designed and built free of any defects in materials and workmanship that would cause the vehicle to not meet EPA standards. In 2005, the EPA and the Pennsylvania Department of Environmental Protection (PADEP) confirmed Harley-Davidson to be the first corporation to voluntarily enroll in the One Clean-Up Program. This program is designed for the clean-up of the affected soil and groundwater at the former York Naval Ordnance Plant. The program is backed by the state and local government along with participating organizations and corporations.

Paul Gotthold, Director of Operations for the EPA, congratulated the motor company:

Harley-Davidson also purchased most of Castalloy, a South Australian producer of cast motorcycle wheels and hubs. The South Australian government has set forth "protection to the purchaser (Harley-Davidson) against environmental risks".

In August 2016 Harley-Davidson settled with the EPA for $12 million, without admitting wrongdoing, over the sale of after-market "super tuners". Super tuners were devices, marketed for competition, which enabled increased performance of Harley-Davidson products. However, the devices also modified the emission control systems, producing increased hydrocarbon and nitrogen oxide. Harley-Davidson is required to buy back and destroy any super tuners which do not meet Clean Air Act requirements and spend $3 million on air pollution mitigation.

The Milwaukee Bucks of the National Basketball Association sport a Harley Davidson sponsor patch on their jerseys.

According to a recent Harley-Davidson study, in 1987 half of all Harley riders were under age 35. Now, only 15 percent of Harley buyers are under 35, and as of 2005, the median age had risen to 46.7. In 2008, Harley-Davidson stopped disclosing the average age of riders; at this point it was 48 years old.

In 1987, the median household income of a Harley-Davidson rider was $38,000. By 1997, the median household income for those riders had more than doubled, to $83,000.

Many Harley-Davidson Clubs exist nowadays around the world; the oldest one, founded in 1928, is in Prague.

Harley-Davidson attracts a loyal brand community, with licensing of the Harley-Davidson logo accounting for almost 5 percent of the company's net revenue ($41 million in 2004). Harley-Davidson supplies many American police forces with their motorcycle fleets.

From its founding, Harley-Davidson had worked to brand its motorcycles as respectable and refined products, with ads that showed what motorcycling writer Fred Rau called "refined-looking ladies with parasols, and men in conservative suits as the target market". The 1906 Harley-Davidson's effective, and polite, muffler was emphasized in advertisements with the nickname "The Silent Gray Fellow". That began to shift in the 1960s, partially in response to the clean-cut motorcyclist portrayed in Honda's "You meet the nicest people on a Honda" campaign, when Harley-Davidson sought to draw a contrast with Honda by underscoring the more working-class, macho, and even a little anti-social attitude associated with motorcycling's dark side. With the 1971 FX Super Glide, the company embraced, rather than distanced, itself from chopper style, and the counterculture custom Harley scene. Their marketing cultivated the "bad boy" image of biker and motorcycle clubs, and to a point, even outlaw or one-percenter motorcycle clubs.

Beginning in 1920, a team of farm boys, including Ray Weishaar, who became known as the "hog boys", consistently won races. The group had a live hog as their mascot. Following a win, they would put the hog on their Harley and take a victory lap. In 1983, the Motor Company formed a club for owners of its product, taking advantage of the long-standing nickname by turning "hog" into the acronym HOG, for Harley Owners Group. Harley-Davidson attempted to trademark "hog", but lost a case against an independent Harley-Davidson specialist, The Hog Farm of West Seneca, New York, in 1999, when the appellate panel ruled that "hog" had become a generic term for large motorcycles and was therefore unprotectable as a trademark.

On August 15, 2006, Harley-Davidson Inc. had its NYSE ticker symbol changed from HDI to HOG.

Harley-Davidson FL "big twins" normally had heavy steel fenders, chrome trim, and other ornate and heavy accessories. After World War II, riders wanting more speed would often shorten the fenders or take them off completely to reduce the weight of the motorcycle. These bikes were called "bobbers" or sometimes "choppers", because parts considered unnecessary were chopped off. Those who made or rode choppers and bobbers, especially members of motorcycle clubs like the Hells Angels, referred to stock FLs as "garbage wagons".

Harley-Davidson established the Harley Owners Group (HOG) in 1983 to build on the loyalty of Harley-Davidson enthusiasts as a means to promote a lifestyle alongside its products. The HOG also opened new revenue streams for the company, with the production of tie-in merchandise offered to club members, numbering more than one million. Other motorcycle brands,
and other and consumer brands outside motorcycling, have also tried to create factory-sponsored community marketing clubs of their own.
HOG members typically spend 30 percent more than other Harley owners on such items as clothing and Harley-Davidson-sponsored events.

In 1991, HOG went international, with the first official European HOG Rally in Cheltenham, England.
Today, more than one million members and more than 1400 chapters worldwide make HOG the largest factory-sponsored motorcycle organization in the world.

HOG benefits include organized group rides, exclusive products and product discounts, insurance discounts, and the Hog Tales newsletter. A one-year full membership is included with the purchase of a new, unregistered Harley-Davidson.

In 2008, HOG celebrated its 25th anniversary in conjunction with the Harley 105th in Milwaukee, Wisconsin.

3rd Southern HOG Rally set to bring together largest gathering of Harley-Davidson owners in South India. More than 600 Harley-Davidson Owners expected to ride to Hyderabad from across 13 HOG Chapters

Harley-Davidson offers factory tours at four of its manufacturing sites, and the Harley-Davidson Museum, which opened in 2008, exhibits Harley-Davidson's history, culture, and vehicles, including the motor company's corporate archives.

Due to the consolidation of operations, the Capitol Drive Tour Center in Wauwatosa, Wisconsin, was closed in 2009.

Beginning with Harley-Davidson's 90th anniversary in 1993, Harley-Davidson has had celebratory rides to Milwaukee called the "Ride Home". This new tradition has continued every five years, and is referred to unofficially as "Harleyfest", in line with Milwaukee's other festivals (Summerfest, German fest, Festa Italiana, etc.). This event brings Harley riders from all around the world. The 105th anniversary celebration was held on August 28–31, 2008, and included events in Milwaukee, Waukesha, Racine, and Kenosha counties, in Southeast Wisconsin. The 110th anniversary celebration was held on August 29–31, 2013. The 115th anniversary was held in Prague, Czech Republic, the home country of the oldest existing Harley Davidson Club, on July 5–8, 2018 and attracted more than 100.000 visitors and 60.000 bikes.

William S. Harley, Arthur Davidson, William A. Davidson and Walter Davidson, Sr were, in 2004, inducted into the Labor Hall of Fame for their accomplishments for the H-D company and its workforce.

The company's origins were dramatized in a 2016 miniseries entitled "Harley and the Davidsons", starring Robert Aramayo as William Harley, Bug Hall as Arthur Davidson and Michiel Huisman as Walter Davidson, and premiered on the Discovery Channel as a "three-night event series" on September 5, 2016.





</doc>
<doc id="14144" url="https://en.wikipedia.org/wiki?curid=14144" title="Hiberno-English">
Hiberno-English

Hiberno-English (from Latin "Hibernia": "Ireland") or Irish English (, ) is the set of English dialects natively written and spoken within the island of Ireland (including both the Republic of Ireland and Northern Ireland).

English was brought to Ireland as a result of the Norman invasion of Ireland of the late 12th century. Initially, it was mainly spoken in an area known as the Pale around Dublin, with mostly the Irish language spoken throughout the rest of the country. By the Tudor period, Irish culture and language had regained most of the territory lost to the invaders: even in the Pale, "all the common folk… for the most part are of Irish birth, Irish habit, and of Irish language". Some small pockets remained of speakers who predominantly continued to use the English of that time; because of their sheer isolation these dialects developed into later (now-extinct) English-related varieties known as Yola in Wexford and Fingallian in Fingal, Dublin. These were no longer mutually intelligible with other English varieties.

However, the Tudor conquest and colonisation of Ireland in the 16th century led to a second wave of immigration by English speakers, along with the forced suppression and decline in the status and use of the Irish language. By the mid-19th century, English was the majority language spoken in the country. It has retained this status to the present day, with even those whose first language is Irish being fluent in English as well. Today, there is little more than one percent of the population who speaks the Irish language natively, though it is required to be taught in all state-funded schools. Of the 40% of the population who self-identified as speaking some Irish in 2016, 4% speak Irish daily outside the education system. In the Republic of Ireland, English is one of two official languages (along with Irish) and is the country's working language.

Hiberno-English's writing standards align with British rather than American English. However, Hiberno-English's diverse accents and some of its grammatical structures are unique, with some influence by the Irish language and some instances of phonologically conservative features: features no longer common in the accents of England or North America.

Phonologists today often divide Hiberno-English into four or five overarching classes of dialects or accents: Ulster accents, West and South-West Irish accents (like the widely discussed Cork accent), various Dublin accents, and a standard accent expanding since only the last quarter of the twentieth century.
Ulster English (or Northern Irish English) here refers collectively to the varieties of the Ulster province, including Northern Ireland and neighbouring counties outside of Northern Ireland, which has been influenced by Ulster Irish as well as the Scots language, brought over by Scottish settlers during the Plantation of Ulster. Its main subdivisions are Mid-Ulster English, South Ulster English and Ulster Scots, the latter of which is arguably a separate language. 
Ulster varieties distinctly pronounce:


West and South-West Irish English here refers to broad varieties of Ireland's West and South-West Regions. Accents of both regions are known for:

South-West Irish English (often known, by specific county, as Cork English, Kerry English, or Limerick English) also features two major defining characteristics of its own. One is the pin–pen merger: the raising of to when before or (as in "again" or "pen"). The other is the intonation pattern of a slightly higher pitch followed by a significant drop in pitch on stressed long-vowel syllables (across multiple syllables or even within a single one), which is popularly heard in rapid conversation, by speakers of other English dialects, as a noticeable kind of undulating "sing-song" pattern.


Dublin English is highly internally diverse and refers collectively to the Irish English varieties immediately surrounding and within the metropolitan area of Dublin. Modern-day Dublin English largely lies on a phonological continuum, ranging from a more traditional, lower-prestige, local urban accent on the one end to a more recently developing, higher-prestige, non-local (regional and even supraregional) accent on the other end, whose most advanced characteristics only first emerged in the late 1980s and 1990s. The accent that most strongly uses the traditional working-class features has been labelled by linguists as local Dublin English. Most speakers from Dublin and its suburbs, however, have accent features falling variously along the entire middle as well as the newer end of the spectrum, which together form what is called non-local Dublin English, spoken by middle- and upper-class natives of Dublin and the greater eastern Irish region surrounding the city. A subset of this variety, whose middle-class speakers mostly range in the middle section of the continuum, is called mainstream Dublin English. Mainstream Dublin English has become the basis of an accent that has otherwise become supraregional (see more below) everywhere except in the north of the country. The majority of Dubliners born since the 1980s (led particularly by females) has shifted towards the most innovative non-local accent, here called new Dublin English, which has gained ground over mainstream Dublin English and which is the most extreme variety in rejecting the local accent's traditional features. The varieties at either extreme of the spectrum, local and new Dublin English, are both discussed in further detail below. In the most general terms, all varieties of Dublin English have the following identifying sounds that are often distinct from the rest of Ireland, pronouncing:

Local Dublin English (or popular Dublin English) here refers to a traditional, broad, working-class variety spoken in the Republic of Ireland's capital city of Dublin. It is the only Irish English variety that in earlier history was non-rhotic; however, it is today weakly rhotic, Known for diphthongisation of the and vowels, the local Dublin accent is also known for a phenomenon called "vowel breaking", in which , , and in closed syllables are "broken" into two syllables, approximating , , , and , respectively.

Evolving as a fashionable outgrowth of the mainstream non-local Dublin English, new Dublin English (also, advanced Dublin English and, formerly, fashionable Dublin English) is a youthful variety that originally began in the early 1990s among the "avant-garde" and now those aspiring to a non-local "urban sophistication". New Dublin English itself, first associated with affluent and middle-class inhabitants of southside Dublin, is probably now spoken by a majority of Dubliners born since the 1980s. It has replaced (yet was largely influenced by) moribund D4 English (often known as "Dublin 4" or "DART speak" or, mockingly, "Dortspeak"), which originated around the 1970s from Dubliners who rejected traditional notions of Irishness, regarding themselves as more trendy and sophisticated; however, particular aspects of the D4 accent became quickly noticed and ridiculed as sounding affected, causing these features to fall out of fashion by the 1990s. New Dublin English can have a fur–fair merger, horse–hoarse, and witch–which mergers, while resisting the traditionally Irish English cot–caught merger.

Notable lifelong Dublin native speakers

Supraregional Southern Irish English (sometimes, simply Supraregional Irish English or Standard Irish English) refers to a variety spoken particularly by educated and middle- or higher-class Irish people, crossing regional boundaries throughout all of the Republic of Ireland, except the north. As mentioned earlier, mainstream Dublin English of the early- to mid-twentieth century is the direct influence and catalyst for this variety, coming about by the suppression of certain markedly Irish features (and retention of other Irish features) as well as the adoption of certain standard British (i.e., non-Irish) features. The result is a configuration of features that is still unique; in other words, this accent is not simply a wholesale shift towards British English. Most speakers born in the 1980s or later are showing fewer features of this late twentieth-century mainstream supraregional form and more characteristics aligning to a rapidly spreading new Dublin accent (see more above, under "Non-local Dublin English").

Ireland's supraregional dialect pronounces:

The following charts list the vowels typical of each Irish English dialect as well as the several distinctive consonants of Irish English. Phonological characteristics of overall Irish English are given as well as categorisations into five major divisions of Hiberno-English: northern Ireland (or Ulster); West & South-West Ireland; local Dublin; new Dublin; and supraregional (southern) Ireland. Features of mainstream non-local Dublin English fall on a range between "local Dublin" and "new Dublin".

The defining monophthongs of Irish English:

The following pure vowel sounds are defining characteristics of Irish English:

All pure vowels of various Hiberno-English dialects:

Footnotes:
In southside Dublin's once-briefly fashionable "Dublin 4" (or "Dortspeak") accent, the " and broad " set becomes rounded as [ɒː].

Unstressed syllable-final or is realised in Ulster accents uniquely as .

Other notes:


The defining diphthongs of Hiberno-English:

The following gliding vowel (diphthong) sounds are defining characteristics of Irish English:

All diphthongs of various Hiberno-English dialects:

Footnotes:
Due to the local Dublin accent's phenomenon of "vowel breaking", may be realised in that accent as in a closed syllable, and, in the same environment, may be realised as .

The defining "r"-coloured vowels of Hiberno-English:

The following "r"-coloured vowel features are defining characteristics of Hiberno-English: 

All "r"-coloured vowels of various Hiberno-English dialects:

Footnotes:

Every major accent of Irish English is rhotic (pronounces "r" after a vowel sound). The local Dublin accent is the only one that during an earlier time was non-rhotic, though it usually very lightly rhotic today, with a few minor exceptions. The rhotic consonant in this and most other Irish accents is an approximant .

The "r" sound of the mainstream non-local Dublin accent is more precisely a velarised approximant , while the "r" sound of the more recently emerging non-local Dublin (or "new Dublin") accent is more precisely a retroflex approximant .

In southside Dublin's once-briefly fashionable "Dublin 4" (or "Dortspeak") accent, is realised as .

In non-local Dublin's more recently emerging (or "new Dublin") accent, and may both be realised more rounded as .

In local Dublin, West/South-West, and other very conservative and traditional Irish English varieties ranging from the south to the north, the phoneme is split into two distinct phonemes depending on spelling and preceding consonants, which have sometimes been represented as versus , and often more precisely pronounced as versus . As an example, the words "earn" and "urn" are not pronounced the same, as they are in most dialects of English around the world. In the local Dublin and West/South-West accents, when after a labial consonant (e.g. "fern"), when spelled as "ur" or "or" (e.g. "word"), or when spelled as "ir" after an alveolar stop (e.g. "dirt") are pronounced as ; in all other situations, is pronounced as . Example words include:


In non-local Dublin, younger, and supraregional Irish accents, this split is seldom preserved, with both of the phonemes typically merged as .

In rare few local Dublin varieties that are non-rhotic, is either lowered to or backed and raised to .

The distinction between and is widely preserved in Ireland, so that, for example, "horse" and "hoarse" are not merged in most Irish English dialects; however, they are usually merged in Belfast and new Dublin.

In local Dublin, due to the phenomenon of "vowel breaking" may in fact be realised as .

The defining consonants of Hiberno-English:

The consonants of Hiberno-English mostly align to the typical English consonant sounds. However, a few Irish English consonants have distinctive, varying qualities. The following consonant features are defining characteristics of Hiberno-English: 

Unique consonants in various Hiberno-English dialects:

Footnotes:

In traditional, conservative Ulster English, and are palatalised before a low front vowel.

Local Dublin also undergoes cluster simplification, so that stop consonant sounds occurring after fricatives or sonorants may be left unpronounced, resulting, for example, in "poun(d)" and "las(t)".

Rhoticity: Every major accent of Irish English is strongly rhotic (pronounces "r" after a vowel sound), though to a weaker degree with the local Dublin accent. The accents of local Dublin and some smaller eastern towns like Drogheda were historically non-rhotic and now only very lightly rhotic or variably rhotic, with the rhotic consonant being an alveolar approximant, . In extremely traditional and conservative accents (exemplified, for instance, in the speech of older speakers throughout the country, even in South-West Ireland, such as Mícheál Ó Muircheartaigh and Jackie Healy-Rae), the rhotic consonant, before a vowel sound, can also be an alveolar tap, . The rhotic consonant for the northern Ireland and new Dublin accents is a retroflex approximant, . Dublin's retroflex approximant has no precedent outside of northern Ireland and is a genuine innovation of the 1990s and 2000s. A guttural/uvular is found in north-east Leinster. Otherwise, the rhotic consonant of virtually all other Irish accents is the postalveolar approximant, .

The symbol [θ̠] is used here to represent the voiceless alveolar non-sibilant fricative, sometimes known as a "slit fricative", whose articulation is described as being apico-alveolar.

Overall, and are being increasingly merged in supraregional Irish English, for example, making "wine" and "whine" homophones, as in most varieties of English around the world.

Other phonological characteristics of Irish English include that consonant clusters ending in before are distinctive:

The naming of the letter "H" as "haitch" is standard.

Due to Gaelic influence, an epenthetic schwa is sometimes inserted, perhaps as a feature of older and less careful speakers, e.g. "film" and "form" .

A number of Irish-language loan words are used in Hiberno-English, particularly in an official state capacity. For example, the head of government is the Taoiseach, the deputy head is the Tánaiste, the parliament is the Oireachtas and its lower house is Dáil Éireann. Less formally, people also use loan words in day-to-day speech, although this has been on the wane in recent decades and among the young.

Another group of Hiberno-English words are those "derived" from the Irish language. Some are words in English that have entered into general use, while others are unique to Ireland. These words and phrases are often Anglicised versions of words in Irish or direct translations into English. In the latter case, they often give a meaning to a word or phrase that is generally not found in wider English use.

Another class of vocabulary found in Hiberno-English are words and phrases common in Old and Middle English, but which have since become obscure or obsolete in the modern English language generally. Hiberno-English has also developed particular meanings for words that are still in common use in English generally.

In addition to the three groups above, there are also additional words and phrases whose origin is disputed or unknown. While this group may not be unique to Ireland, their usage is not widespread, and could be seen as characteristic of the language in Ireland.

The syntax of the Irish language is quite different from that of English. Various aspects of Irish syntax have influenced Hiberno-English, though many of these idiosyncrasies are disappearing in suburban areas and among the younger population.

The other major influence on Hiberno-English that sets it apart from modern English in general is the retention of words and phrases from Old- and Middle-English.

Reduplication is an alleged trait of Hiberno-English strongly associated with Stage Irish and Hollywood films.


Irish has no words that directly translate as "yes" or "no", and instead repeats the verb used in the question, negated if necessary, to answer. Hiberno-English uses "yes" and "no" less frequently than other English dialects as speakers can repeat the verb, positively or negatively, instead of (or in redundant addition to) using "yes" or "no".


This is not limited only to the verb "to be": it is also used with "to have" when used as an auxiliary; and, with other verbs, the verb "to do" is used. This is most commonly used for intensification, especially in Ulster English.

Irish indicates recency of an action by adding "after" to the present continuous (a verb ending in "-ing"), a construction known as the "hot news perfect" or "after perfect". The idiom for "I had done X when I did Y" is "I was after doing X when I did Y", modelled on the Irish usage of the compound prepositions , , and :  /  / .
A similar construction is seen where exclamation is used in describing a recent event:

When describing less astonishing or significant events, a structure resembling the German perfect can be seen:

This correlates with an analysis of "H1 Irish" proposed by Adger & Mitrovic, in a deliberate parallel to the status of German as a V2 language.

Recent past construction has been directly adopted into Newfoundland English, where it is common in both formal and casual register. In rural areas of the Avalon peninsula, where Newfoundland Irish was spoken until the early 20th century, it is the grammatical standard for describing whether or not an action has occurred.

The reflexive version of pronouns is often used for emphasis or to refer indirectly to a particular person, etc., according to context. "Herself", for example, might refer to the speaker's boss or to the woman of the house. Use of "herself" or "himself" in this way often indicates that the speaker attributes some degree of arrogance or selfishness to the person in question. Note also the indirectness of this construction relative to, for example, "She's coming now". This reflexive pronoun can also be used to describe a partner - "I was with himself last night." or "How's herself doing?"

There are some language forms that stem from the fact that there is no verb "to have" in Irish. Instead, possession is indicated in Irish by using the preposition "at", (in Irish, "ag."). To be more precise, Irish uses a prepositional pronoun that combines "ag" "at" and "mé" "me" to create "agam". In English, the verb "to have" is used, along with a "with me" or "on me" that derives from "Tá … agam." This gives rise to the frequent
Somebody who can speak a language "has" a language, in which Hiberno-English has borrowed the grammatical form used in Irish.

When describing something, many Hiberno-English speakers use the term "in it" where "there" would usually be used. This is due to the Irish word "ann" (pronounced "oun" or "on") fulfilling both meanings.

Another idiom is this thing or that thing described as "this man here" or "that man there", which also features in Newfoundland English in Canada.

Conditionals have a greater presence in Hiberno-English due to the tendency to replace the simple present tense with the conditional (would) and the simple past tense with the conditional perfect (would have).

Bring and take: Irish use of these words differs from that of British English because it follows the Irish grammar for "beir" and "tóg". English usage is determined by direction; person determines Irish usage. So, in English, one takes ""from" here "to" there", and brings it ""to" here "from" there". In Irish, a person takes only when accepting a transfer of possession of the object from someone elseand a person brings at all other times, irrespective of direction (to or from).

The Irish equivalent of the verb "to be" has two present tenses, one (the present tense proper or "aimsir láithreach") for cases which are generally true or are true at the time of speaking and the other (the habitual present or "aimsir ghnáthláithreach") for repeated actions. Thus, "you are [now, or generally]" is "tá tú", but "you are [repeatedly]" is "bíonn tú". Both forms are used with the verbal noun (equivalent to the English present participle) to create compound tenses. This is similar to the distinction between "ser" and "estar" in Spanish or the use of the 'habitual be' in African-American Vernacular English.

The corresponding usage in English is frequently found in rural areas, especially Mayo/Sligo in the west of Ireland and Wexford in the south-east, Inner-City Dublin along with border areas of the North and Republic. In this form, the verb "to be" in English is similar to its use in Irish, with a "does be/do be" (or "bees", although less frequently) construction to indicate the continuous, or habitual, present:

This construction also surfaces in African American Vernacular English, as the famous habitual be.

In old-fashioned usage, "it is" can be freely abbreviated "’tis", even as a standalone sentence. This also allows the double contraction "’tisn’t", for "it is not".

Irish has separate forms for the second person singular ("tú") and the second person plural ("sibh").
Mirroring Irish, and almost every other Indo-European language, the plural "you" is also distinguished from the singular in Hiberno-English, normally by use of the otherwise archaic English word "ye" ; the word "yous" (sometimes written as "youse") also occurs, but primarily only in Dublin and across Ulster. In addition, in some areas in Leinster, north Connacht and parts of Ulster, the hybrid word "ye-s", pronounced "yiz", may be used. The pronunciation differs with that of the northwestern being and the Leinster pronunciation being .


The word "ye", "yis" or "yous", otherwise archaic, is still used in place of "you" for the second-person plural. "Ye'r", "Yisser" or "Yousser" are the possessive forms, e.g. "Where are yous going?"

The verb "mitch" is very common in Ireland, indicating being truant from school. This word appears in Shakespeare (though he wrote in Early Modern English rather than Middle English), but is seldom heard these days in British English, although pockets of usage persist in some areas (notably South Wales, Devon, and Cornwall). In parts of Connacht and Ulster the "mitch" is often replaced by the verb "scheme", while in Dublin it is often replaced by "on the hop/bounce".

Another usage familiar from Shakespeare is the inclusion of the second person pronoun after the imperative form of a verb, as in "Wife, go you to her ere you go to bed" (Romeo and Juliet, Act III, Scene IV). This is still common in Ulster: "Get youse your homework done or you're no goin' out!" In Munster, you will still hear children being told, "Up to bed, let ye" .

For influence from Scotland, see Ulster Scots and Ulster English.

Now is often used at the end of sentences or phrases as a semantically empty word, completing an utterance without contributing any apparent meaning. Examples include "Bye now" (= "Goodbye"), "There you go now" (when giving someone something), "Ah now!" (expressing dismay), "Hold on now" (= "wait a minute"), "Now then" as a mild attention-getter, etc. This usage is universal among English dialects, but occurs more frequently in Hiberno-English. It is also used in the manner of the Italian 'prego' or German 'bitte', for example a barman might say "Now, Sir." when delivering drinks.

So is often used for emphasis ("I can speak Irish, so I can"), or it may be tacked onto the end of a sentence to indicate agreement, where "then" would often be used in Standard English ("Bye so", "Let's go so", "That's fine so", "We'll do that so"). The word is also used to contradict a negative statement ("You're not pushing hard enough" – "I am so!"). (This contradiction of a negative is also seen in American English, though not as often as "I am too", or "Yes, I am".) The practice of indicating emphasis with "so" and including reduplicating the sentence's subject pronoun and auxiliary verb (is, are, have, has, can, etc.) such as in the initial example, is particularly prevalent in more northern dialects such as those of Sligo, Mayo and the counties of Ulster.

Sure/Surely is often used as a tag word, emphasising the obviousness of the statement, roughly translating as but/and/well/indeed. It can be used as "to be sure" (but note that the other stereotype of "Sure and …" is not actually used in Ireland.) Or "Sure, I can just go on Wednesday", "I will not, to be sure." The word is also used at the end of sentences (primarily in Munster), for instance "I was only here five minutes ago, sure!" and can express emphasis or indignation. In Ulster, the reply "Aye, surely" may be given to show strong agreement.

To is often omitted from sentences where it would exist in British English. For example, "I'm not allowed go out tonight", instead of "I'm not allowed "to" go out tonight".

Will is often used where British English would use "shall" or American English "should" (as in "Will I make us a cup of tea?"). The distinction between "shall" (for first-person simple future, and second- and third-person emphatic future) and "will" (second- and third-person simple future, first-person emphatic future), maintained by many in England, does not exist in Hiberno-English, with "will" generally used in all cases.

Once is sometimes used in a different way from how it is used in other dialects; in this usage, it indicates a combination of logical and causal conditionality: "I have no problem laughing at myself once the joke is funny." Other dialects of English would probably use "if" in this situation.




</doc>
