<doc id="21490" url="https://en.wikipedia.org/wiki?curid=21490" title="Nylon">
Nylon

Nylon is a generic designation for a family of synthetic polymers, based on aliphatic or semi-aromatic polyamides. 
Nylon is a thermoplastic silky material 
that can be melt-processed into fibers, films, or shapes. It is made of repeating units linked by amide links similar to the peptide bonds in proteins.
Nylon polymers can be mixed with a wide variety of additives to achieve many different property variations.
Nylon polymers have found significant commercial applications in fabric and fibers (apparel, flooring and rubber reinforcement), in shapes (molded parts for cars, electrical equipment, etc.), and in films (mostly for food packaging).

Nylon was the first commercially successful synthetic thermoplastic polymer. DuPont began its research project in 1927.
The first example of nylon (nylon 6,6) using diamines on February 28, 1935, by Wallace Hume Carothers at DuPont's research facility at the DuPont Experimental Station. In response to Carothers' work, Paul Schlack at IG Farben developed nylon 6, a different molecule based on caprolactam, on January 29, 1938.

Nylon was first used commercially in a nylon-bristled toothbrush in 1938, followed more famously in women's stockings or "nylons" which were shown at the 1939 New York World's Fair and first sold commercially in 1940. During World War II, almost all nylon production was diverted to the military for use in parachutes and parachute cord. Wartime uses of nylon and other plastics greatly increased the market for the new materials.

DuPont, founded by Éleuthère Irénée du Pont, first produced gunpowder and later cellulose-based paints. Following WWI, DuPont produced synthetic ammonia and other chemicals. DuPont began experimenting with the development of cellulose based fibers, eventually producing the synthetic fiber rayon. DuPont's experience with rayon was an important precursor to its development and marketing of nylon.

DuPont's invention of nylon spanned an eleven-year period, ranging from the initial research program in polymers in 1927 to its announcement in 1938, shortly before the opening of the 1939 New York World's Fair. The project grew from a new organizational structure at DuPont, suggested by Charles Stine in 1927, in which the chemical department would be composed of several small research teams that would focus on "pioneering research" in chemistry and would "lead to practical applications". Harvard instructor Wallace Hume Carothers was hired to direct the polymer research group. Initially he was allowed to focus on pure research, building on and testing the theories of German chemist Hermann Staudinger. He was very successful, as research he undertook greatly improved the knowledge of polymers and contributed to science.

In the spring of 1930, Carothers and his team had already synthesized two new polymers. One was neoprene, a synthetic rubber greatly used during World War II. The other was a white elastic but strong paste that would later become nylon. After these discoveries, Carothers' team was made to shift its research from a more pure research approach investigating general polymerization to a more practically-focused goal of finding "one chemical combination that would lend itself to industrial applications".

It wasn't until the beginning of 1935 that a polymer called "polymer 6-6" was finally produced. Carothers' coworker, Washington University alumnus Julian W. Hill had used a cold drawing method to produce a polyester in 1930. This cold drawing method was later used by Carothers in 1935 to fully develop nylon. The first example of nylon (nylon 6,6) was produced on February 28, 1935, at DuPont's research facility at the DuPont Experimental Station. It had all the desired properties of elasticity and strength.
However, it also required a complex manufacturing process that would become the basis of industrial production in the future. DuPont obtained a patent for the polymer in September 1938, and quickly achieved a monopoly of the fiber. Carothers died 16 months before the announcement of nylon, therefore he was never able to see his success.

The production of nylon required interdepartmental collaboration between three departments at DuPont: the Department of Chemical Research, the Ammonia Department, and the Department of Rayon. Some of the key ingredients of nylon had to be produced using high pressure chemistry, the main area of expertise of the Ammonia Department. Nylon was considered a “godsend to the Ammonia Department”, which had been in financial difficulties. The reactants of nylon soon constituted half of the Ammonia department's sales and helped them come out of the period of the Great Depression by creating jobs and revenue at DuPont.

DuPont's nylon project demonstrated the importance of chemical engineering in industry, helped create jobs, and furthered the advancement of chemical engineering techniques. In fact, it developed a chemical plant that provided 1800 jobs and used the latest technologies of the time, which are still used as a model for chemical plants today. The ability to acquire a large number of chemists and engineers quickly was a huge contribution to the success of DuPont's nylon project. The first nylon plant was located at Seaford, Delaware, beginning commercial production on December 15, 1939. On October 26, 1995, the Seaford plant was designated a National Historic Chemical Landmark by the American Chemical Society.

An important part of nylon's popularity stems from DuPont's marketing strategy. DuPont promoted the fiber to increase demand before the product was available to the general market. Nylon's commercial announcement occurred on October 27, 1938, at the final session of the "Herald Tribune"s yearly "Forum on Current Problems", on the site of the approaching New York City world's fair. The "first man-made organic textile fiber" which was derived from "coal, water and air" and promised to be "as strong as steel, as fine as the spider's web" was received enthusiastically by the audience, many of them middle-class women, and made the headlines of most newspapers. Nylon was introduced as part of "The world of tomorrow" at the 1939 New York World's Fair and was featured at DuPont's "Wonder World of Chemistry" at the Golden Gate International Exposition in San Francisco in 1939. Actual nylon stockings were not shipped to selected stores in the national market until May 15, 1940. However, a limited number were released for sale in Delaware before that. The first public sale of nylon stockings occurred on October 24, 1939, in Wilmington, Delaware. 4,000 pairs of stockings were available, all of which were sold within three hours.

Another added bonus to the campaign was that it meant reducing silk imports from Japan, an argument that won over many wary customers. Nylon was even mentioned by President Roosevelt's cabinet, which addressed its "vast and interesting economic possibilities" five days after the material was formally announced.

However, the early excitement over nylon also caused problems. It fueled unreasonable expectations that nylon would be better than silk, a miracle fabric as strong as steel that would last forever and never run. Realizing the danger of claims such as "New Hosiery Held Strong as Steel" and "No More Runs", DuPont scaled back the terms of the original announcement, especially those stating that nylon would possess the strength of steel.

Also, DuPont executives marketing nylon as a revolutionary man-made material did not at first realize that some consumers experienced a sense of unease and distrust, even fear, towards synthetic fabrics.
A particularly damaging news story, drawing on DuPont's 1938 patent for the new polymer, suggested that one method of producing nylon might be to use cadaverine (pentamethylenediamine), a chemical extracted from corpses. Although scientists asserted that cadaverine was also extracted by heating coal, the public often refused to listen. A woman confronted one of the lead scientists at DuPont and refused to accept that the rumour was not true.

DuPont changed its campaign strategy, emphasizing that nylon was made from "coal, air and water", and started focusing on the personal and aesthetic aspects of nylon, rather than its intrinsic qualities. Nylon was thus domesticated, and attention shifted to the material and consumer aspect of the fiber with slogans like "If it's nylon, it's prettier, and oh! How fast it dries!".

After nylon's nationwide release in 1940, production was increased. 1300 tons of the fabric were produced during 1940. During their first year on the market, 64 million pairs of nylon stockings were sold. In 1941, a second plant was opened in Martinsville, Virginia due to the success of the fabric.

While nylon was marketed as the durable and indestructible material of the people, it was sold at almost twice the price of silk stockings ($4.27 per pound of nylon versus $2.79 per pound of silk). Sales of nylon stockings were strong in part due to changes in women's fashion. As Lauren Olds explains: "by 1939 [hemlines] had inched back up to the knee, closing the decade just as it started off". The shorter skirts were accompanied by a demand for stockings that offered fuller coverage without the use of garters to hold them up.

However, as of February 11, 1942, nylon production was redirected from being a consumer material to one used by the military. DuPont's production of nylon stockings and other lingerie stopped, and most manufactured nylon was used to make parachutes and tents for World War II. Although nylon stockings already made before the war could be purchased, they were generally sold on the black market for as high as $20.

Once the war ended, the return of nylon was awaited with great anticipation. Although DuPont projected yearly production of 360 million pairs of stockings, there were delays in converting back to consumer rather than wartime production. In 1946, the demand for nylon stockings could not be satisfied, which led to the Nylon riots. In one case, an estimated 40,000 people lined up in Pittsburgh to buy 13,000 pairs of nylons. In the meantime, women cut up nylon tents and parachutes left from the war in order to make blouses and wedding dresses. Between the end of the war and 1952, production of stockings and lingerie used 80% of the world's nylon. DuPont put a lot of focus on catering to the civilian demand, and continually expanded its production.

As pure nylon hosiery was sold in a wider market, problems became apparent. Nylon stockings were found to be fragile, in the sense that the thread often tended to unravel lengthwise, creating 'runs'. People also reported that pure nylon textiles could be uncomfortable due to nylon's lack of absorbency. Moisture stayed inside the fabric near the skin under hot or moist conditions instead of being "wicked" away. Nylon fabric could also be itchy, and tended to cling and sometimes spark as a result of static electrical charge built up by friction.
Also, under some conditions stockings could decompose turning back into nylon's original components of air, coal, and water. Scientists explained this as a result of air pollution, attributing it to London smog in 1952, as well as poor air quality in New York and Los Angeles.

The solution found to problems with pure nylon fabric was to blend nylon with other existing fibers or polymers such as cotton, polyester, and spandex. This led to the development of a wide array of blended fabrics. The new nylon blends retained the desirable properties of nylon (elasticity, durability, ability to be dyed) and kept clothes prices low and affordable.
As of 1950, the New York Quartermaster Procurement Agency (NYQMPA), which developed and tested textiles for the army and navy, had committed to developing a wool-nylon blend. They were not the only ones to introduce blends of both natural and synthetic fibers. "America's Textile Reporter" referred to 1951 as the "Year of the blending of the fibers". Fabric blends included mixes like "Bunara" (wool-rabbit-nylon) and "Casmet" (wool-nylon-fur). In Britain in November 1951, the inaugural address of the 198th session of the Royal Society for the Encouragement of Arts, Manufactures and Commerce focused on the blending of textiles.

DuPont's Fabric Development Department cleverly targeted French fashion designers, supplying them with fabric samples. In 1955, designers such as Coco Chanel, Jean Patou, and Christian Dior showed gowns created with DuPont fibers, and fashion photographer Horst P. Horst was hired to document their use of DuPont fabrics. "American Fabrics" credited blends with providing "creative possibilities and new ideas for fashions which had been hitherto undreamed of."

DuPont went through an extensive process to generate names for its new product. 
In 1940, John W. Eckelberry of DuPont stated that the letters "nyl" were arbitrary and the "on" was copied from the suffixes of other fibers such as cotton and Rayon. A later publication by DuPont ("Context", vol. 7, no. 2, 1978) explained that the name was originally intended to be "No-Run" ("run" meaning "unravel"), but was modified to avoid making such an unjustified claim. Since the products were not really run-proof, the vowels were swapped to produce "nuron", which was changed to "nilon" "to make it sound less like a nerve tonic". For clarity in pronunciation, the "i" was changed to "y."

In spite of oil shortages in the 1970s, consumption of nylon textiles continued to grow by 7.5 per cent per annum between the 1960s and 1980s. 
Overall production of synthetic fibers, however, dropped from 63% of the worlds textile production in 1965, to 45% of the world's textile production in early 1970s. The appeal of "new" technologies wore off, and nylon fabric "was going out of style in the 1970s". Also, consumers became concerned about environmental costs throughout the production cycle: obtaining the raw materials (oil), energy use during production, waste produced during creation of the fiber, and eventual waste disposal of materials that were not biodegradable. 
Synthetic fibers have not dominated the market since the 1950s and 1960s. , nylon continued to represent about 12% (8 million pounds) of the world's production of synthetic fibers. As one of the largest engineering polymer families, the global demand of nylon resins and compounds was valued at roughly US$20.5 billion in 2013. The market is expected to reach US$30 billion by 2020 by following an average annual growth of 5.5%.

Although pure nylon has many flaws and is now rarely used, its derivatives have greatly influenced and contributed to society. From scientific discoveries relating to the production of plastics and polymerization, to economic impact during the depression and the changing of women's fashion, nylon was a revolutionary product. The Lunar Flag Assembly, the first flag planted on the moon in a symbolic gesture of celebration, was made of nylon. The flag itself cost $5.50, but had to have a specially-designed flagpole with a horizontal bar so that it would appear to "fly".
One historian describes nylon as "an object of desire", comparing the invention to Coca-Cola in the eyes of 20th century consumers.

Nylons are condensation polymers or copolymers, formed by reacting difunctional monomers containing equal parts of amine and carboxylic acid, so that amides are formed at both ends of each monomer in a process analogous to polypeptide biopolymers. Most nylons are made from the reaction of a dicarboxylic acid with a diamine (e.g. PA66) or a lactam or amino acid with itself (e.g. PA6). In the first case, the "repeating unit" consists of one of each monomer, so that they alternate in the chain, similar to the so-called ABAB structure of polyesters and polyurethanes. Since each monomer in this copolymer has the same reactive group on both ends, the direction of the amide bond reverses between each monomer, unlike natural polyamide proteins, which have overall directionality: C terminal → N terminal. In the second case (so called AA), the repeating unit corresponds to the single monomer.

In common usage, the prefix "PA" (polyamide) or the name "Nylon" are used interchangeably and are equivalent in meaning.

The nomenclature used for nylon polymers was devised during the synthesis of the first simple aliphatic nylons and uses numbers to describe the number of carbons in each monomer unit, including the carbon(s) of the carboxylic acid(s). Subsequent use of cyclic and aromatic monomers required the use of letters or sets of letters. One number after "PA" or "Nylon" indicates a homopolymer which is "monadic" or based on one amino acid (minus HO) as monomer:

Two numbers or sets of letters indicate a "dyadic" homopolymer formed from two monomers: one diamine and one dicarboxylic acid. The first number indicates the number of carbons in the diamine. The two numbers should be separated by a comma for clarity, but the comma is often omitted.

For copolymers the comonomers or pairs of comonomers are separated by slashes:

The term polyphthalamide (abbreviated to PPA) is used when 60% or more moles of the carboxylic acid portion of the repeating unit in the polymer chain is composed of a combination of terephthalic acid (TPA) and isophthalic acid (IPA).

Wallace Carothers at DuPont patented nylon 66 using amides. 
In the case of nylons that involve reaction of a diamine and a dicarboxylic acid, it is difficult to get the proportions exactly correct, and deviations can lead to chain termination at molecular weights less than a desirable 10,000 daltons (u). To overcome this problem, a crystalline, solid "nylon salt" can be formed at room temperature, using an exact 1:1 ratio of the acid and the base to neutralize each other. The salt is crystallized to purify it and obtain the desired precise stoichiometry. Heated to 285 °C (545 °F), the salt reacts to form nylon polymer with the production of water.

The synthetic route using lactams (cyclic amides) was developed by Paul Schlack at IG Farben, leading to nylon 6, or polycaprolactam — formed by a ring-opening polymerization. The peptide bond within the caprolactam is broken with the exposed active groups on each side being incorporated into two new bonds as the monomer becomes part of the polymer backbone.

The 428 °F (220 °C) melting point of nylon 6 is lower than the 509 °F (265 °C) melting point of nylon 66.

Nylon 510, made from pentamethylene diamine and sebacic acid, was studied by Carothers even before nylon 66 and has superior properties, but is more expensive to make. In keeping with this naming convention, "nylon 6,12" or "PA 612" is a copolymer of a 6C diamine and a 12C diacid. Similarly for PA 510 PA 611; PA 1012, etc. Other nylons include copolymerized dicarboxylic acid/diamine products that are "not" based upon the monomers listed above. For example, some fully aromatic nylons (known as "aramids") are polymerized with the addition of diacids like terephthalic acid (→ Kevlar, Twaron) or isophthalic acid (→ Nomex), more commonly associated with polyesters. There are copolymers of PA 66/6; copolymers of PA 66/6/12; and others. In general linear polymers are the most useful, but it is possible to introduce branches in nylon by the condensation of dicarboxylic acids with polyamines having three or more amino groups.

The general reaction is:
Two molecules of water are given off and the nylon is formed. Its properties are determined by the R and R' groups in the monomers. In nylon 6,6, R = 4C and R' = 6C alkanes, but one also has to include the two carboxyl carbons in the diacid to get the number it donates to the chain. In Kevlar, both R and R' are benzene rings.

Industrial synthesis is usually done by heating the acids, amines or lactams to remove water, but in the laboratory, diacid chlorides can be reacted with diamines. For example, a popular demonstration of interfacial polymerization (the "nylon rope trick") is the synthesis of nylon 66 from adipoyl chloride and hexamethylene diamine.

Nylons can also be synthesized from dinitriles using acid catalysis. For example, this method is applicable for preparation of nylon 1,6 from adiponitrile, formaldehyde and water. Additionally, nylons can be synthesized from diols and dinitriles using this method as well.

Nylon monomers are manufactured by a variety of routes, starting in most cases from crude oil but sometimes from biomass. Those in current production are described below.



Various diamine components can be used, which are derived from a variety of sources. Most are petrochemicals, but bio-based materials are also being developed.

Due to the large number of diamines, diacids and aminoacids that can be synthesized, many nylon polymers have been made experimentally and characterized to varying degrees. A smaller number have been scaled up and offered commercially, and these are detailed below.

Homopolymer nylons derived from one monomer

Examples of these polymers that are or were commercially available

Homopolymer polyamides derived from pairs of diamines and diacids (or diacid derivatives). Shown in the table below are polymers which are or have been offered commercially either as homopolymers or as a part of a copolymer.

Examples of these polymers that are or were commercially available

It is easy to make mixtures of the monomers or sets of monomers used to make nylons to obtain copolymers. This lowers crystallinity and can therefore lower the melting point.

Some copolymers that have been or are commercially available are listed below: 

Most nylon polymers are miscible with each other allowing a range of blends to be made. The two polymers can react with one another by transamidation to form random copolymers.

According to their crystallinity, polyamides can be:

According to this classification, PA66, for example, is an aliphatic semi-crystalline homopolyamide.

All nylons are susceptible to hydrolysis, especially by strong acids, a reaction essentially the reverse of the synthetic reaction shown above. The molecular weight of nylon products so attacked drops, and cracks form quickly at the affected zones. Lower members of the nylons (such as nylon 6) are affected more than higher members such as nylon 12. This means that nylon parts cannot be used in contact with sulfuric acid for example, such as the electrolyte used in lead–acid batteries.

When being molded, nylon must be dried to prevent hydrolysis in the molding machine barrel since water at high temperatures can also degrade the polymer. The reaction is of the type:

Berners-Lee calculates the average greenhouse gas footprint of nylon in manufacturing carpets at 5.43 kg CO equivalent per kg, when produced in Europe. This gives it almost the same carbon footprint as wool, but with greater durability and therefore a lower overall carbon footprint.

Data published by PlasticsEurope indicates for nylon 66 a greenhouse gas footprint of 6.4 kg CO equivalent per kg, and an energy consumption of 138 kJ/kg. When considering the environmental impact of nylon, it is important to consider the use phase. In particular when cars are lightweight, significant savings in fuel consumption and CO emissions are achieved.

Various nylons break down in fire and form hazardous smoke, and toxic fumes or ash, typically containing hydrogen cyanide. Incinerating nylons to recover the high energy used to create them is usually expensive, so most nylons reach the garbage dumps, decaying slowly. Discarded nylon fabric takes 30–40 years to decompose. Nylon is a robust polymer and lends itself well to recycling. Much nylon resin is recycled directly in a closed loop at the injection molding machine, by grinding sprues and runners and mixing them with the virgin granules being consumed by the molding machine.

Nylon can be recycled but only a few companies do so. Aquafil has demonstrated recycling fishing nets lost in the ocean into apparel Vanden recycles Nylon and other polyamides (PA) and has operations in UK, Australia, Hong Kong, UAE, Turkey and Finland.

Above their melting temperatures, "T", thermoplastics like nylon are amorphous solids or viscous fluids in which the chains approximate random coils. Below "T", amorphous regions alternate with regions which are lamellar crystals. The amorphous regions contribute elasticity and the crystalline regions contribute strength and rigidity. The planar amide (-CO-NH-) groups are very polar, so nylon forms multiple hydrogen bonds among adjacent strands. Because the nylon backbone is so regular and symmetrical, especially if all the amide bonds are in the "trans" configuration, nylons often have high crystallinity and make excellent fibers. The amount of crystallinity depends on the details of formation, as well as on the kind of nylon.

Nylon 66 can have multiple parallel strands aligned with their neighboring peptide bonds at coordinated separations of exactly 6 and 4 carbons for considerable lengths, so the carbonyl oxygens and amide hydrogens can line up to form interchain hydrogen bonds repeatedly, without interruption (see the figure opposite). Nylon 510 can have coordinated runs of 5 and 8 carbons. Thus parallel (but not antiparallel) strands can participate in extended, unbroken, multi-chain β-pleated sheets, a strong and tough supermolecular structure similar to that found in natural silk fibroin and the β-keratins in feathers. (Proteins have only an amino acid α-carbon separating sequential -CO-NH- groups.) Nylon 6 will form uninterrupted H-bonded sheets with mixed directionalities, but the β-sheet wrinkling is somewhat different. The three-dimensional disposition of each alkane hydrocarbon chain depends on rotations about the 109.47° tetrahedral bonds of singly bonded carbon atoms.

When extruded into fibers through pores in an industry spinneret, the individual polymer chains tend to align because of viscous flow. If subjected to cold drawing afterwards, the fibers align further, increasing their crystallinity, and the material acquires additional tensile strength. In practice, nylon fibers are most often drawn using heated rolls at high speeds.

Block nylon tends to be less crystalline, except near the surfaces due to shearing stresses during formation. Nylon is clear and colorless, or milky, but is easily dyed. Multistranded nylon cord and rope is slippery and tends to unravel. The ends can be melted and fused with a heat source such as a flame or electrode to prevent this.

Nylons are hygroscopic, and will absorb or desorb moisture as a function of the ambient humidity. Variations in moisture content have several effects on the polymer. Firstly, the dimensions will change, but more importantly moisture acts as a plasticizer, lowering the glass transition temperature ("T"), and consequently the elastic modulus at temperatures below the "T"

When dry, polyamide is a good electrical insulator. However, polyamide is hygroscopic. The absorption of water will change some of the material's properties such as its electrical resistance. Nylon is less absorbent than wool or cotton.

The characteristic features of nylon 6,6 include:

On the other hand, nylon 6 is easy to dye, more readily fades; it has a higher impact resistance, a more rapid moisture absorption, greater elasticity and elastic recovery.

Nylon clothing tends to be less flammable than cotton and rayon, but nylon fibers may melt and stick to skin.

Nylon was first used commercially in a nylon-bristled toothbrush in 1938, followed more famously in women's stockings or "nylons" which were shown at the 1939 New York World's Fair and first sold commercially in 1940. Its use increased dramatically during World War II, when the need for fabrics increased dramatically.

Bill Pittendreigh, DuPont, and other individuals and corporations worked diligently during the first few months of World War II to find a way to replace Asian silk and hemp with nylon in parachutes. It was also used to make tires, tents, ropes, ponchos, and other military supplies. It was even used in the production of a high-grade paper for U.S. currency. At the outset of the war, cotton accounted for more than 80% of all fibers used and manufactured, and wool fibers accounted for nearly all of the rest. By August 1945, manufactured fibers had taken a market share of 25%, at the expense of cotton. After the war, because of shortages of both silk and nylon, nylon parachute material was sometimes repurposed to make dresses.

Nylon 6 and 66 fibers are used in carpet manufacture.

Nylon is one kind of fibers used in tire cord. Herman E. Schroeder pioneered application of nylon in tires.

Nylon resins are widely used in the automobile industry especially in the engine compartment.

Molded nylon is used in hair combs and mechanical parts such as machine screws, gears, gaskets, and other low- to medium-stress components previously cast in metal. Engineering-grade nylon is processed by extrusion, casting, and injection molding. Type 6,6 Nylon 101 is the most common commercial grade of nylon, and Nylon 6 is the most common commercial grade of molded nylon. For use in tools such as spudgers, nylon is available in glass-filled variants which increase structural and impact strength and rigidity, and molybdenum disulfide-filled variants which increase lubricity. Nylon can be used as the matrix material in composite materials, with reinforcing fibers like glass or carbon fiber; such a composite has a higher density than pure nylon. Such thermoplastic composites (25% to 30% glass fiber) are frequently used in car components next to the engine, such as intake manifolds, where the good heat resistance of such materials makes them feasible competitors to metals.

Nylon was used to make the stock of the Remington Nylon 66 rifle. The frame of the modern Glock pistol is made of a nylon composite.

Nylon resins are used as a component of food packaging films where an oxygen barrier is needed. Some of the terpolymers based upon nylon are used every day in packaging. Nylon has been used for meat wrappings and sausage sheaths. The high temperature resistance of nylon makes it useful for oven bags.

Nylon filaments are primarily used in brushes especially toothbrushes and string trimmers. They are also used as monofilaments in fishing line. Nylon 610 and 612 are the most used polymers for filaments.

Its various properties also make it very useful as a material in additive manufacturing; specifically as a filament in consumer and professional grade fused deposition modeling 3D printers.

Nylon resins can be extruded into rods, tubes and sheets.

Nylon powders are used to powder coat metals. Nylon 11 and nylon 12 are the most widely used.

In the mid-1940s, classical guitarist Andrés Segovia mentioned the shortage of good guitar strings in the United States, particularly his favorite Pirastro catgut strings, to a number of foreign diplomats at a party, including General Lindeman of the British Embassy. A month later, the General presented Segovia with some nylon strings which he had obtained via some members of the DuPont family. Segovia found that although the strings produced a clear sound, they had a faint metallic timbre which he hoped could be eliminated.

Nylon strings were first tried on stage by Olga Coelho in New York in January, 1944.

In 1946, Segovia and string maker Albert Augustine were introduced by their mutual friend Vladimir Bobri, editor of Guitar Review. On the basis of Segovia's interest and Augustine's past experiments, they decided to pursue the development of nylon strings. DuPont, skeptical of the idea, agreed to supply the nylon if Augustine would endeavor to develop and produce the actual strings. After three years of development, Augustine demonstrated a nylon first string whose quality impressed guitarists, including Segovia, in addition to DuPont.

Wound strings, however, were more problematic. Eventually, however, after experimenting with various types of metal and smoothing and polishing techniques, Augustine was also able to produce high quality nylon wound strings.





</doc>
<doc id="21491" url="https://en.wikipedia.org/wiki?curid=21491" title="Nucleus">
Nucleus

Nucleus (plural nuclei) is a Latin word for the seed inside a fruit. It most often refers to:


Nucleus may also refer to:










</doc>
<doc id="21494" url="https://en.wikipedia.org/wiki?curid=21494" title="Nerd">
Nerd

A nerd is a person seen as overly intellectual, obsessive, introverted or lacking social skills. Such a person may spend inordinate amounts of time on unpopular, little known, or non-mainstream activities, which are generally either highly technical, abstract, or relating to topics of science fiction or fantasy, to the exclusion of more mainstream activities. Additionally, many so-called nerds are described as being shy, quirky, pedantic, and unattractive.

Originally derogatory, the term "nerd" was a stereotype, but as with other pejoratives, it has been reclaimed and redefined by some as a term of pride and group identity. However, the augmentative terms, geek and dork, have not experienced a similar positive drift in meaning and usage.

The first documented appearance of the word "nerd" is as the name of a creature in Dr. Seuss's book "If I Ran the Zoo" (1950), in which the narrator Gerald McGrew claims that he would collect "a Nerkle, a Nerd, and a Seersucker too" for his imaginary zoo. The slang meaning of the term dates to 1951. That year, "Newsweek" magazine reported on its popular use as a synonym for "drip" or "square" in Detroit, Michigan. By the early 1960s, usage of the term had spread throughout the United States, and even as far as Scotland. At some point, the word took on connotations of bookishness and social ineptitude.

An alternate spelling, as "nurd" or "gnurd", also began to appear in the mid-1960s or early 1970s. Author Philip K. Dick claimed to have coined the "nurd" spelling in 1973, but its first recorded use appeared in a 1965 student publication at Rensselaer Polytechnic Institute (RPI). Oral tradition there holds that the word is derived from "knurd" ("drunk" spelled backward), which was used to describe people who studied rather than partied. The term "gnurd " (spelled with the "g") was in use at the Massachusetts Institute of Technology (MIT) by 1965. The term "nurd" was also in use at the Massachusetts Institute of Technology as early as 1971.

According to "Online Etymology Dictionary", the word is an alteration of the 1940s term ""nert"" (meaning "stupid or crazy person"), which is itself an alteration of "nut" (nutcase).

The term was popularized in the 1970s by its heavy use in the sitcom "Happy Days".

Because of the nerd stereotype, many smart people are often thought of as nerdy. This belief can be harmful, as it can cause high-school students to "switch off their lights" out of fear of being branded as a nerd, and cause otherwise appealing people to be considered nerdy simply for their intellect. It was once thought that intellectuals were nerdy because they were envied. However, Paul Graham stated in his essay, "Why Nerds are Unpopular", that intellect is neutral, meaning that you are neither loved nor despised for it. He also states that it is only the correlation that makes smart teens automatically seem nerdy, and that a nerd is someone that is not socially adept enough. Additionally, he says that the reason why many smart kids are unpopular is that they "don't have time for the activities required for popularity."
Stereotypical nerd appearance, often lampooned in caricatures, can include very large glasses, braces, buck teeth, severe acne and pants worn high at the waist. Following suit of popular use in emoticons, Unicode released in 2015 its "Nerd Face" character, featuring some of those stereotypes: 🤓 (code point U+1F913). In the media, many nerds are males, portrayed as being physically unfit, either overweight or skinny due to lack of physical exercise. It has been suggested by some, such as linguist Mary Bucholtz, that being a nerd may be a state of being "hyperwhite" and rejecting African-American culture and slang that "cool" white children use. However, after the "Revenge of the Nerds" movie franchise (with multicultural nerds), and the introduction of the Steve Urkel character on the television series "Family Matters", nerds have been seen in all races and colors as well as more recently being a frequent young East Asian or Indian male stereotype in North America. Portrayal of "nerd girls", in films such as "She's Out of Control", "Welcome to the Dollhouse" and "She's All That" depicts that smart but nerdy women might suffer later in life if they do not focus on improving their physical attractiveness.

In the United States, a 2010 study published in the "Journal of International and Intercultural Communication" indicated that Asian Americans are perceived as most likely to be nerds, followed by White Americans, while non-White Hispanics and Black Americans were perceived as least likely to be nerds. These stereotypes stem from concepts of Orientalism and Primitivism, as discussed in Ron Eglash's essay "Race, Sex, and Nerds": "From Black Geeks to Asian American Hipsters".

Some of the stereotypical behaviors associated with the "nerd" stereotype have correlations with the traits of Asperger's Syndrome or other autism-spectrum conditions.

The rise of Silicon Valley and the American computer industry at large has allowed many so-called "nerdy people" to accumulate large fortunes and influence media culture. Many stereotypically nerdy interests, such as superhero, fantasy and science fiction works, are now international popular culture hits. Some measures of nerdiness are now allegedly considered desirable, as, to some, it suggests a person who is intelligent, respectful, interesting, and able to earn a large salary. Stereotypical nerd qualities are evolving, going from awkwardness and social ostracism to an allegedly more widespread acceptance and sometimes even celebration of their differences.

Johannes Grenzfurthner, researcher, self-proclaimed nerd and director of nerd documentary "Traceroute", reflects on the emergence of nerds and nerd culture:
In the 1984 film "Revenge of the Nerds" Robert Carradine worked to embody the nerd stereotype; in doing so, he helped create a definitive image of nerds. Additionally, the storyline presaged, and may have helped inspire, the "nerd pride" that emerged in the 1990s. "American Splendor" regular Toby Radloff claims this was the movie that inspired him to become "The Genuine Nerd from Cleveland, Ohio." In the "American Splendor" film, Toby's friend, "American Splendor" author Harvey Pekar, was less receptive to the movie, believing it to be hopelessly idealistic, explaining that Toby, an adult low income file clerk, had nothing in common with the middle class kids in the film who would eventually attain college degrees, success, and cease being perceived as nerds. Many, however, seem to share Radloff's view, as "nerd pride" has become more widespread in the years since. MIT professor Gerald Sussman, for example, seeks to instill pride in nerds:
The popular computer-related news website Slashdot uses the tagline "News for nerds. Stuff that matters." The Charles J. Sykes quote "Be nice to nerds. Chances are you'll end up working for one" has been popularized on the Internet and incorrectly attributed to Bill Gates. In Spain, Nerd Pride Day has been observed on May 25 since 2006, the same day as Towel Day, another somewhat nerdy holiday.<ref name="geek/nerd"></ref> The date was picked as it is the anniversary of the release of "".

An episode from the animated series "Freakazoid", titled "Nerdator", includes the use of nerds to power the mind of a Predator-like enemy. Towards the middle of the show, he gave this speech. :

The Danish reality TV show "FC Zulu", known in the internationally franchised format as "FC Nerds", established a format wherein a team of nerds, after two or three months of training, competes with a professional soccer team.

Some commentators consider that the word is devalued when applied to people who adopt a sub-cultural pattern of behaviour, rather than being reserved for people with a marked ability.

Although originally being predominately an American stereotype, Nerd culture has grown across the globe and is now more acceptable and common than ever. Australian events such as Oz Comic-Con (a large comic book and Cosplay convention, similar to San Diego Comic-Con International) and Supernova, are incredibly popular events among the culture of people who identify themselves as nerds. In 2016, Oz Comic-Con in Perth saw almost 20,000 cos-players and comic book fans meet to celebrate the event, hence being named a "professionally organised Woodstock for geeks".

Individuals who are labeled as "nerds" are often the target of bullying due to a range of reasons that may include physical appearance or social background. Paul Graham has suggested that the reason nerds are frequently singled out for bullying is their indifference to popularity or social context, in the face of a youth culture that views popularity as paramount. However, research findings suggest that bullies are often as socially inept as their academically better-performing victims, and that popularity fails to confer protection from bullying. Other commentators have pointed out that pervasive harassment of intellectually-oriented youth began only in the mid-twentieth century and some have suggested that its cause involves jealousy over future employment opportunities and earning potential.






</doc>
<doc id="21496" url="https://en.wikipedia.org/wiki?curid=21496" title="Nucleic acid">
Nucleic acid

Nucleic acids are the biopolymers, or large biomolecules, essential to all known forms of life. The term "nucleic acid" is the overall name for DNA and RNA. They are composed of nucleotides, which are the monomers made of three components: a 5-carbon sugar, a phosphate group and a nitrogenous base. If the sugar is a compound ribose, the polymer is RNA (ribonucleic acid); if the sugar is derived from ribose as deoxyribose, the polymer is DNA (deoxyribonucleic acid).

Nucleic acids are the most important of all biomolecules. These are found in abundance in all living things, where they function to create and encode and then store information of every living cell of every life-form organism on Earth. In turn, they function to transmit and express that information inside and outside the cell nucleus—to the interior operations of the cell and ultimately to the next generation of each living organism. The encoded information is contained and conveyed via the nucleic acid sequence, which provides the 'ladder-step' ordering of nucleotides within the molecules of RNA and DNA.

Strings of nucleotides are bonded to form helical backbones—typically, one for RNA, two for DNA—and assembled into chains of base-pairs selected from the five primary, or canonical, nucleobases, which are: adenine, cytosine, guanine, thymine, and uracil. Thymine occurs only in DNA and uracil only in RNA. Using amino acids and the process known as protein synthesis, the specific sequencing in DNA of these nucleobase-pairs enables storing and transmitting coded instructions as genes. In RNA, base-pair sequencing provides for manufacturing new proteins that determine the frames and parts and most chemical processes of all life forms.


Experimental studies of nucleic acids constitute a major part of modern biological and medical research, and form a foundation for genome and forensic science, and the biotechnology and pharmaceutical industries.

Naked NA refers to NA that is not associated with proteins, lipids, or any other molecule to help "protect it". Naked DNA can be found when transcriptional bursting is occurring.

The term "nucleic acid" is the overall name for DNA and RNA, members of a family of biopolymers, and is synonymous with "polynucleotide". Nucleic acids were named for their initial discovery within the nucleus, and for the presence of phosphate groups (related to phosphoric acid). Although first discovered within the nucleus of eukaryotic cells, nucleic acids are now known to be found in all life forms including within bacteria, archaea, mitochondria, chloroplasts, and viruses (There is debate as to whether viruses are living or non-living). All living cells contain both DNA and RNA (except some cells such as mature red blood cells), while viruses contain either DNA or RNA, but usually not both.
The basic component of biological nucleic acids is the nucleotide, each of which contains a pentose sugar (ribose or deoxyribose), a phosphate group, and a nucleobase.
Nucleic acids are also generated within the laboratory, through the use of enzymes (DNA and RNA polymerases) and by solid-phase chemical synthesis. The chemical methods also enable the generation of altered nucleic acids that are not found in nature, for example peptide nucleic acids.

Nucleic acids are generally very large molecules. Indeed, DNA molecules are probably the largest individual molecules known. Well-studied biological nucleic acid molecules range in size from 21 nucleotides (small interfering RNA) to large chromosomes (human chromosome 1 is a single molecule that contains 247 million base pairs).

In most cases, naturally occurring DNA molecules are double-stranded and RNA molecules are single-stranded. There are numerous exceptions, however—some viruses have genomes made of double-stranded RNA and other viruses have single-stranded DNA genomes, and, in some circumstances, nucleic acid structures with three or four strands can form.

Nucleic acids are linear polymers (chains) of nucleotides. Each nucleotide consists of three components: a purine or pyrimidine nucleobase (sometimes termed "nitrogenous base" or simply "base"), a pentose sugar, and a phosphate group. The substructure consisting of a nucleobase plus sugar is termed a nucleoside. Nucleic acid types differ in the structure of the sugar in their nucleotides–DNA contains 2'-deoxyribose while RNA contains ribose (where the only difference is the presence of a hydroxyl group). Also, the nucleobases found in the two nucleic acid types are different: adenine, cytosine, and guanine are found in both RNA and DNA, while thymine occurs in DNA and uracil occurs in RNA.

The sugars and phosphates in nucleic acids are connected to each other in an alternating chain (sugar-phosphate backbone) through phosphodiester linkages. In conventional nomenclature, the carbons to which the phosphate groups attach are the 3'-end and the 5'-end carbons of the sugar. This gives nucleic acids directionality, and the ends of nucleic acid molecules are referred to as 5'-end and 3'-end. The nucleobases are joined to the sugars via an N-glycosidic linkage involving a nucleobase ring nitrogen (N-1 for pyrimidines and N-9 for purines) and the 1' carbon of the pentose sugar ring.

Non-standard nucleosides are also found in both RNA and DNA and usually arise from modification of the standard nucleosides within the DNA molecule or the primary (initial) RNA transcript. Transfer RNA (tRNA) molecules contain a particularly large number of modified nucleosides.

Double-stranded nucleic acids are made up of complementary sequences, in which extensive Watson-Crick base pairing results in a highly repeated and quite uniform double-helical three-dimensional structure. In contrast, single-stranded RNA and DNA molecules are not constrained to a regular double helix, and can adopt highly complex three-dimensional structures that are based on short stretches of intramolecular base-paired sequences including both Watson-Crick and noncanonical base pairs, and a wide range of complex tertiary interactions.

Nucleic acid molecules are usually unbranched and may occur as linear and circular molecules. For example, bacterial chromosomes, plasmids, mitochondrial DNA, and chloroplast DNA are usually circular double-stranded DNA molecules, while chromosomes of the eukaryotic nucleus are usually linear double-stranded DNA molecules. Most RNA molecules are linear, single-stranded molecules, but both circular and branched molecules can result from RNA splicing reactions. The total amount of pyrimidines is equal to the total amount of purines. The diameter of the helix is about 20Å.

One DNA or RNA molecule differs from another primarily in the sequence of nucleotides. Nucleotide sequences are of great importance in biology since they carry the ultimate instructions that encode all biological molecules, molecular assemblies, subcellular and cellular structures, organs, and organisms, and directly enable cognition, memory, and behavior ("see Genetics"). Enormous efforts have gone into the development of experimental methods to determine the nucleotide sequence of biological DNA and RNA molecules, and today hundreds of millions of nucleotides are sequenced daily at genome centers and smaller laboratories worldwide. In addition to maintaining the GenBank nucleic acid sequence database, the National Center for Biotechnology Information (NCBI, https://www.ncbi.nlm.nih.gov) provides analysis and retrieval resources for the data in GenBank and other biological data made available through the NCBI web site.

Deoxyribonucleic acid (DNA) is a nucleic acid containing the genetic instructions used in the development and functioning of all known living organisms. The DNA segments carrying this genetic information are called genes. Likewise, other DNA sequences have structural purposes or are involved in regulating the use of this genetic information. Along with RNA and proteins, DNA is one of the three major macromolecules that are essential for all known forms of life.
DNA consists of two long polymers of simple units called nucleotides, with backbones made of sugars and phosphate groups joined by ester bonds. These two strands run in opposite directions to each other and are, therefore, anti-parallel. Attached to each sugar is one of four types of molecules called nucleobases (informally, bases). It is the sequence of these four nucleobases along the backbone that encodes information. This information is read using the genetic code, which specifies the sequence of the amino acids within proteins. The code is read by copying stretches of DNA into the related nucleic acid RNA in a process called transcription.
Within cells, DNA is organized into long structures called chromosomes. During cell division these chromosomes are duplicated in the process of DNA replication, providing each cell its own complete set of chromosomes. Eukaryotic organisms (animals, plants, fungi, and protists) store most of their DNA inside the cell nucleus and some of their DNA in organelles, such as mitochondria or chloroplasts. In contrast, prokaryotes (bacteria and archaea) store their DNA only in the cytoplasm. Within the chromosomes, chromatin proteins such as histones compact and organize DNA. These compact structures guide the interactions between DNA and other proteins, helping control which parts of the DNA are transcribed.

Ribonucleic acid (RNA) functions in converting genetic information from genes into the amino acid sequences of proteins. The three universal types of RNA include transfer RNA (tRNA), messenger RNA (mRNA), and ribosomal RNA (rRNA). Messenger RNA acts to carry genetic sequence information between DNA and ribosomes, directing protein synthesis. Ribosomal RNA is a major component of the ribosome, and catalyzes peptide bond formation. Transfer RNA serves as the carrier molecule for amino acids to be used in protein synthesis, and is responsible for decoding the mRNA. In addition, many other classes of RNA are now known.

Artificial nucleic acid analogues have been designed and synthesized by chemists, and include peptide nucleic acid, morpholino- and locked nucleic acid, glycol nucleic acid, and threose nucleic acid. Each of these is distinguished from naturally occurring DNA or RNA by changes to the backbone of the molecules.





</doc>
<doc id="21497" url="https://en.wikipedia.org/wiki?curid=21497" title="Nitrate">
Nitrate

Nitrate is a polyatomic ion with the chemical formula . Salts containing this anion are called nitrates. Nitrates are common components of fertilizers and explosives. Almost all nitrates are soluble in water. A common example of an inorganic nitrate salt is potassium nitrate (saltpeter). Removal of one electron yields the nitrate radical, also called nitrogen trioxide .

The anion is the conjugate base of nitric acid, consisting of one central nitrogen atom surrounded by three identically bonded oxygen atoms in a trigonal planar arrangement. The nitrate ion carries a formal charge of −1. This charge results from a combination formal charge in which each of the three oxygens carries a − charge, whereas the nitrogen carries a +1 charge, all these adding up to formal charge of the polyatomic nitrate ion. This arrangement is commonly used as an example of resonance. Like the isoelectronic carbonate ion, the nitrate ion can be represented by resonance structures:

A rich source of inorganic nitrate in the human diets come from leafy green foods, such as spinach and arugula. (inorganic nitrate) is the viable active component within beetroot juice and other vegetables. Drinking water is also a dietary source.

Dietary nitrate supplementation delivers positive results when testing endurance exercise performance.

Ingestion of large doses of nitrate either in the form of pure sodium nitrate or beetroot juice in young healthy individuals rapidly increases plasma nitrate concentration about 2-3 fold, and this elevated nitrate concentration can be maintained for at least 2 weeks. Increased plasma nitrate stimulates the production of nitric oxide. Nitric oxide is important physiological signalling molecule that is used in, among other things, regulation of muscle blood flow and mitochondrial respiration.

Nitrite consumption is primarily determined by the amount of processed meats eaten, and the concentration of nitrates in these meats. Although nitrites are the nitrogen compound chiefly used in meat curing, nitrates are used as well. Nitrates lead to the formation of nitrosamines. The production of carcinogenic nitrosamines may be inhibited by the use of the antioxidants vitamin C and the alpha-tocopherol form of vitamin E during curing.

Anti-hypertensive diets, such as the DASH diet, typically contain high levels of nitrates, which are first reduced to nitrite in the saliva, as detected in saliva testing, prior to forming nitric oxide.

Nitrate salts are found naturally on earth as large deposits, particularly of nitratine, a major source of sodium nitrate.

Nitrates are produced by a number of species of nitrifying bacteria, and the nitrate compounds for gunpowder (see this topic for more) were historically produced, in the absence of mineral nitrate sources, by means of various fermentation processes using urine and dung.

As a byproduct of lightning strikes in earth's nitrogen-oxygen rich atmosphere, nitric acid is produced when nitrogen dioxide reacts with water vapor.

Nitrates are produced industrially from nitric acid.

Nitrates are mainly produced for use as fertilizers in agriculture because of their high solubility and biodegradability. The main nitrate fertilizers are ammonium, sodium, potassium, calcium, and magnesium salts. Several million kilograms are produced annually for this purpose.

The second major application of nitrates is as oxidizing agents, most notably in explosives where the rapid oxidation of carbon compounds liberates large volumes of gases (see gunpowder for an example). Sodium nitrate is used to remove air bubbles from molten glass and some ceramics. Mixtures of the molten salt are used to harden some metals.

Almost all methods for detection of nitrate rely on its conversion to nitrite followed by nitrite-specific tests. The reduction of nitrate to nitrite is effected by copper-cadmium material. The sample is introduced with a flow injection analyzer, and the resulting nitrite-containing effluent is then combined with a reagent for colorimetric or electrochemical detection. The most popular of these assays is the Griess test, whereby nitrite is converted to an deeply colored azo dye, suited for UV-vis spectroscopic analysis. The method exploits the reactivity of nitrous acid derived from acidification of nitrite. Nitrous acid selectively reacts with aromatic amines to give diazonium salts, which in turn couple with a second reagent to give the azo dye. The detection limit is 0.02 to 2 μM. Methods have been highly adapted to biological samples.

The acute toxicity of nitrate is low. "Substantial disagreement" exists about the long-term risks of nitrate exposure. The two areas of possible concern are that (i) nitrate could be a precursor to nitrite in the lower gut, and nitrite is a precursor to nitrosamines, which are implicated in carcinogenesis, and (ii) nitrate is implicated in methemoglobinemia, a disorder of red blood cells hemoglobin.

Nitrates do not affect infants and pregnant women. Blue baby syndrome is caused by a number of other factors such as gastric upset, such as diarrheal infection, protein intolerance, heavy metal toxicity etc., with nitrates playing a minor role.

Through the Safe Drinking Water Act, the United States Environmental Protection Agency has set a maximum contaminant level of 10 mg/L or 10 ppm of nitrates in drinking water.

An acceptable daily intake (ADI) for nitrate ions was established in the range of 0–3.7 mg (kg body weight) day by the Joint FAO/WHO Expert Committee on Food additives (JEFCA).

In freshwater or estuarine systems close to land, nitrate can reach concentrations that are lethal to fish. While nitrate is much less toxic than ammonia, levels over 30 ppm of nitrate can inhibit growth, impair the immune system and cause stress in some aquatic species. Nitrate toxicity remains the subject of debate.

In most cases of excess nitrate concentrations in aquatic systems, the primary source is surface runoff from agricultural or landscaped areas that have received excess nitrate fertilizer. The resulting eutrophication and algae blooms result in anoxia and dead zones. As a consequence, as nitrate forms a component of total dissolved solids, they are widely used as an indicator of water quality.

Symptoms of nitrate poisoning in domestic animals include increased heart rate and respiration; in advanced cases blood and tissue may turn a blue or brown color. Feed can be tested for nitrate; treatment consists of supplementing or substituting existing supplies with lower nitrate material. Safe levels of nitrate for various types of livestock are as follows:

The values above are on a dry (moisture-free) basis.

Nitrate formation with elements of the periodic table.



</doc>
<doc id="21502" url="https://en.wikipedia.org/wiki?curid=21502" title="Nike">
Nike

Nike often refers to:
Nike may also refer to:






</doc>
<doc id="21503" url="https://en.wikipedia.org/wiki?curid=21503" title="Nevis">
Nevis

Nevis is a small island in the Caribbean Sea that forms part of the inner arc of the Leeward Islands chain of the West Indies. Nevis and the neighbouring island of Saint Kitts constitute one country: the Federation of Saint Kitts and Nevis. Nevis is located near the northern end of the Lesser Antilles archipelago, about east-southeast of Puerto Rico and west of Antigua. Its area is and the capital is Charlestown.

Saint Kitts and Nevis are separated by a shallow channel known as "The Narrows". Nevis is roughly conical in shape with a volcano known as Nevis Peak at its centre. The island is fringed on its western and northern coastlines by sandy beaches which are composed of a mixture of white coral sand with brown and black sand which is eroded and washed down from the volcanic rocks that make up the island. The gently-sloping coastal plain ( wide) has natural freshwater springs as well as non-potable volcanic hot springs, especially along the western coast.

The island was named "Oualie" ("Land of Beautiful Waters") by the Caribs and "Dulcina" ("Sweet Island") by the early British settlers. The name "Nevis" is derived from the Spanish "Nuestra Señora de las Nieves" (which means Our Lady of the Snows); the name first appears on maps in the 16th century. Nevis is also known by the sobriquet "Queen of the Caribees", which it earned in the 18th century when its sugar plantations created much wealth for the British.

Nevis is of particular historical significance to Americans because it was the birthplace and early childhood home of Alexander Hamilton. For the British, Nevis is the place where Horatio Nelson was stationed as a young sea captain, and is where he met and married a Nevisian, Frances Nisbet, the young widow of a plantation-owner.

The majority of the approximately 12,000 Nevisians are of primarily African descent, with notable British, Portuguese and Lebanese minority communities. English is the official language, and the literacy rate, 98 percent, is one of the highest in the Western Hemisphere.

In 1498, Christopher Columbus gave the island the name "San Martín" (Saint Martin). However, the confusion of numerous poorly-charted small islands in the Leeward Island chain meant that this name ended up being accidentally transferred to another island, which is still known as Saint-Martin/Sint Maarten.

The current name "Nevis" was derived from a Spanish name "Nuestra Señora de las Nieves" by a process of abbreviation and anglicisation. The Spanish name means Our Lady of the Snows. It is not known who chose this name for the island, but it is a reference to the story of a 4th-century Catholic miracle: a snowfall on the Esquiline Hill in Rome. Presumably the white clouds that usually cover the top of Nevis Peak reminded someone of this story of a miraculous snowfall in a hot climate.

Nevis was part of the Spanish claim to the Caribbean islands, a claim pursued until the Treaty of Madrid (1670), even though there were no Spanish settlements on the island. According to Vincent Hubbard, author of "Swords, Ships & Sugar: History of Nevis", the Spanish ruling caused many of the Arawak groups who were not ethnically Caribs to "be redefined as Caribs overnight". Records indicate that the Spanish enslaved large numbers of the native inhabitants on the more accessible of the Leeward Islands and sent them to Cubagua, Venezuela to dive for pearls. Hubbard suggests that the reason the first European settlers found so few "Caribs" on Nevis is that they had already been rounded up by the Spanish and shipped off to be used as slaves.

Nevis had been settled for more than two thousand years by Amerindian people prior to been sighted by Columbus in 1493. The indigenous people of Nevis during these periods belonged to the Leeward Island Amerindian groups popularly referred to as Arawaks and Caribs, a complex mosaic of ethnic groups with similar culture and language. Dominican anthropologist Lennox Honychurch traces the European use of the term "Carib" to refer to the Leeward Island aborigines to Columbus, who picked it up from the Taínos on Hispaniola. It was not a name the Caribs called themselves. "Carib Indians" was the generic name used for all groups believed involved in cannibalistic war rituals, more particularly, the consumption of parts of a killed enemy's body.

The Amerindian name for Nevis was "Oualie", land of beautiful waters. The structure of the Island Carib language has been linguistically identified as Arawakan.

In spite of the Spanish claim, Nevis continued to be a popular stop-over point for English and Dutch ships on their way to the North American continent. Captain Bartholomew Gilbert of Plymouth visited the island in 1603, spending two weeks to cut twenty tons of lignum vitae wood. Gilbert sailed on to Virginia to seek out survivors of the Roanoke settlement in what is now North Carolina. Captain John Smith visited Nevis also on his way to Virginia in 1607. This was the voyage which founded Jamestown, the first permanent English settlement in the New World.

On 30 August 1620 James VI and I of Scotland and England asserted sovereignty over Nevis by giving a Royal Patent for colonisation to the Earl of Carlisle. However, actual European settlement did not happen until 1628, when Anthony Hilton moved from nearby Saint Kitts following a murder plot against him. He was accompanied by 80 other settlers, soon to be boosted by a further 100 settlers from London who had originally hoped to settle Barbuda. Hilton became the first Governor of Nevis. After the Treaty of Madrid (1670) between Spain and England, Nevis became the seat of the British colony and the Admiralty Court also sat in Nevis. Between 1675 and 1730, the island was the headquarters for the slave trade for the Leeward Islands, with approximately 6,000–7,000 enslaved West Africans passing through en route to other islands each year. The Royal African Company brought all its ships through Nevis. A 1678 census shows a community of Irish people – 22% of the population – existing as either indentured servants or freemen.

Due to the profitable Slave Trade and the high quality of Nevisian sugar cane, the island soon became a dominant source of wealth for Great Britain and the slave-owning British plantocracy. When the Leeward Islands were separated from Barbados in 1671, Nevis became the seat of the Leeward Islands colony and was given the nickname "Queen of the Caribees". It remained colonial capital for the Leeward Islands until the seat was transferred to Antigua for military reasons in 1698. During this period, Nevis was the richest of the British Leeward Islands. The island outranked larger islands like Jamaica in sugar production in the late 17th century. The wealth of the planters on the island is evident in the tax records preserved at the Calendar State Papers in the British Colonial Office Public Records, where the amount of tax collected on the Leeward Islands was recorded. The sums recorded for 1676 as "head tax on slaves", a tax payable in sugar, amounted to 384,600 pounds in Nevis, as opposed to 67,000 each in Antigua and Saint Kitts, 62,500 in Montserrat, and 5,500 total in the other five islands. The profits on sugar cultivation in Nevis was enhanced by the fact that the cane juice from Nevis yielded an unusually high amount of sugar. A gallon (3.79 litres) of cane juice from Nevis yielded 24 ounces (0.71 litres) of sugar, whereas a gallon from Saint Kitts yielded 16 ounces (0.47 litres). Twenty percent of the British Empire's total sugar production in 1700 was derived from Nevisian plantations. Exports from West Indian colonies like Nevis were worth more than all the exports from all the mainland Thirteen Colonies of North America combined at the time of the American Revolution.

The enslaved families formed the large labour force required to work the sugar plantations. After the 1650s the supply of white indentured servants began to dry up due to increased wages in England and less incentive to migrate to the colonies. By the end of the 17th century, the population of Nevis consisted of a small, rich planter elite in control, a marginal population of poor Whites, a great majority of African-descended slaves, and an unknown number of Maroons, escaped slaves living in the mountains. In 1780, 90 percent of the 10 000 people living on Nevis were Black. Some of the maroons joined with the few remaining Caribs in Nevis to form a resistance force. Memories of the Nevisian maroons' struggle under the plantation system are preserved in place names such as Maroon Hill, an early centre of resistance.

The great wealth generated by the colonies of the West Indies led to wars among Spain, Britain, and France. The formation of the United States can be said to be a partial by-product of these wars and the strategic trade aims that often ignored North America. Three privateers (William Kidd being one of them) were employed by the British Crown to help protect ships in Nevis' waters.

During the 17th century, the French, based on Saint Kitts, launched many attacks on Nevis, sometimes assisted by the Island Caribs, who in 1667 sent a large fleet of canoes along in support. In the same year a Franco–Dutch invasion fleet was repelled off Nevis by an English fleet. Letters and other records from the era indicate that the English on Nevis hated and feared the Amerindians. In 1674 and 1683 they participated in attacks on Carib villages in Dominica and St. Vincent, in spite of a lack of official approval from the Crown for the attack.

On Nevis, the English built Fort Charles and a series of smaller fortifications to aid in defending the island. This included Saddle Hill Battery, built in 1740 to replace a deodand on Mount Nevis.

In 1706, Pierre Le Moyne d'Iberville, the French Canadian founder of Louisiana in North America, decided to drive the English out of Nevis and thus also stop pirate attacks on French ships; he considered Nevis the region's headquarters for piracy against French trade. During d'Iberville's invasion of Nevis, French buccaneers were used in the front line, infamous for being ruthless killers after the pillaging during the wars with Spain where they gained a reputation for torturing and murdering non-combatants. In the face of the invading force, the English militiamen of Nevis fled. Some planters burned the plantations, rather than letting the French have them, and hid in the mountains. It was the enslaved Africans who held the French at bay by taking up arms to defend their families and the island. The slave quarters had been looted and burned as well, as the main reward promised the men fighting on the French side in the attack was the right to capture as many slaves as possible and resell them in Martinique.

During the fighting, 3,400 enslaved Nevisians were captured and sent off to Martinique, but about 1,000 more, poorly armed and militarily untrained, held the French troops at bay, by "murderous fire" according to an eyewitness account by an English militiaman. He wrote that "the slaves' brave behaviour and defence there shamed what some of their masters did, and they do not shrink to tell us so." After 18 days of fighting, the French were driven off the island. Among the Nevisian men, women and children carried away on d'Iberville's ships, six ended up in Louisiana, the first persons of African descent to arrive there.

One consequence of the French attack was a collapsed sugar industry and during the ensuing hardship on Nevis, small plots of land on the plantations were made available to the enslaved families in order to control the loss of life due to starvation. With less profitability for the absentee plantation owners, the import of food supplies for the plantation workers dwindled. Between 1776 and 1783, when the food supplies failed to arrive altogether due to the rebellion in North America, 300–400 enslaved Nevisians starved to death. On 1 August 1834, slavery was abolished in the British Empire. In Nevis, 8,815 slaves were freed. The first Monday in August is celebrated as Emancipation Day and is part of the annual Nevis Culturama festival.

A four-year apprenticeship programme followed the abolishment of slavery on the plantations. In spite of the continued use of the labour force, the Nevisian slave owners were paid over £150,000 in compensation from the British Government for the loss of property, whereas the enslaved families received nothing for 200 years of labour. One of the wealthiest planter families in Nevis, the Pinneys of Montravers Plantation, claimed £36,396 (worth close to £1,800,000 today) in compensation for the slaves on the family-owned plantations around the Caribbean.

Because of the early distribution of plots and because many of the planters departed from the island when sugar cultivation became unprofitable, a relatively large percentage of Nevisians already owned or controlled land at emancipation. Others settled on crown land. This early development of a society with a majority of small, landowning farmers and entrepreneurs created a stronger middle class in Nevis than in Saint Kitts, where the sugar industry continued until 2006. Even though the 15 families in the wealthy planter elite no longer control the arable land, Saint Kitts still has a large, landless working class population.

Nevis was united with Saint Kitts and Anguilla in 1882, and they became an associated state with full internal autonomy in 1967, though Anguilla seceded in 1971. Together, Saint Kitts and Nevis became independent on 19 September 1983. On 10 August 1998, a referendum on Nevis to separate from Saint Kitts had 2,427 votes in favour and 1,498 against, falling short of the two-thirds majority needed.

Before 1967, the local government of Saint Kitts was also the government of Nevis and Anguilla. Nevis had two seats and Anguilla one seat in the government. The economic and infrastructural development of the two smaller islands was not a priority to the colonial federal government.

When the hospital in Charlestown was destroyed in a hurricane in 1899, planting of trees in the squares of Saint Kitts and refurbishing of government buildings, also in Saint Kitts, took precedence over the rebuilding of the only hospital in Nevis. After five years without any proper medical facilities, the leaders in Nevis initiated a campaign, threatening to seek independence from Saint Kitts. The British Administrator in Saint Kitts, Charles Cox, was unmoved. He stated that Nevis did not need a hospital since there had been no significant rise in the number of deaths during the time Nevisians had been without a hospital. Therefore, no action was needed on behalf of the government, and besides, Cox continued, the Legislative Council regarded "Nevis and Anguilla as a drag on St. Kitts and would willingly see a separation". Finally, a letter of complaint to the metropolitan British Foreign Office gave result and the federal government in Saint Kitts was ordered by their superiors in London to take speedy action. The Legislative Council took another five years to consider their options. The final decision by the federal government was to not rebuild the old hospital after all but to instead convert the old Government House in Nevis into a hospital, named Alexandra Hospital after Queen Alexandra, wife of King Edward VII. A majority of the funds assigned for the hospital could thus be spent on the construction of a new official residence in Nevis.

After d'Iberville's invasion in 1704, records show Nevis’ sugar industry in ruins and a decimated population begging the English Parliament and relatives for loans and monetary assistance to stave off island-wide starvation. The sugar industry on the island never fully recovered and during the general depression that followed the loss of the West Indian sugar monopoly, Nevis fell on hard times and the island became one of the poorest in the region. The island remained poorer than Saint Kitts until 1991, when the fiscal performance of Nevis edged ahead of the fiscal performance of Saint Kitts for the first time since the French invasion.

Electricity was introduced in Nevis in 1954 when two generators were shipped in to provide electricity to the area around Charlestown. In this regard, Nevis fared better than Anguilla, where there were no paved roads, no electricity and no telephones until 1967. However, electricity did not become available island-wide on Nevis until 1971.

An ambitious infrastructure development programme was introduced in the early 2000s which included a transformation of the Charlestown port, construction of a new deep-water harbour, resurfacing and widening the Island Main Road, a new airport terminal and control tower, and a major airport expansion, which required the relocation of an entire village in order to make room for the runway extension.

Modernised classrooms and better-equipped schools, as well as improvements in the educational system, have contributed to a leap in academic performance on the island. The pass rate among the Nevisian students sitting for the Caribbean Examination Council (CXC) exams, the Cambridge General Certificate of Education Examination (GCE) and the Caribbean Advance Proficiency Examinations is now consistently among the highest in the English-speaking Caribbean.


The formation of the island began in mid-Pliocene times, approximately 3.45 million years ago. Nine distinct eruptive centres from different geological ages, ranging from mid-Pliocene to Pleistocene, have contributed to the formation. No single model of the island's geological evolution can, therefore, be ascertained.

Nevis Peak ( is the dormant remnant of one of these ancient stratovolcanoes. The last activity took place about 100,000 years ago, but active fumaroles and hot springs are still found on the island, the most recent formed in 1953. The composite cone of Nevis volcano has two overlapping summit craters that are partially filled by a lava dome, created in recent, pre-Columbian time. Pyroclastic flows and mudflows were deposited on the lower slopes of the cone simultaneously. Nevis Peak is located on the outer crater rim. Four other lava domes were constructed on the flanks of the volcano, one on the northeast flank (Madden's Mount), one on the eastern flank (Butlers Mountain), one on the northwest coast (Mount Lily) and one on the south coast (Saddle Hill, with a height of 375 metres). The southernmost point on the island is Dogwood Point which is also the southernmost point of the Federation of Saint Kitts and Nevis.

During the last ice age, when the sea level was 60 m lower, the three islands of Saint Kitts, Nevis and Sint Eustatius (also known as Statia) were connected as one island. Saba, however, is separated from these three by a deeper channel.

There are visible wave-breaking reefs along the northern and eastern shorelines. To the south and west, the reefs are located in deeper water and are suitable for scuba diving. The most developed beach on Nevis is the 6.5 km long Pinney's Beach, on the western or Caribbean coast. There are sheltered swimming beaches in Oualie Bay and Cades Bay. The eastern coast of the island faces into the Atlantic Ocean and can have strong surf in parts of the shore which are unprotected by fringing coral reefs. The colour of the sand on the beaches of Nevis is variable: on a lot of the bigger beaches the sand is a yellow-grey in colour, but some beaches on the southern coast have darker, reddish, or even black sand. Under a microscope it becomes clear that Nevis sand is a mixture of tiny fragments of coral, many foraminifera, and small crystals of the various mineral constituents of the volcanic rock of which the island is made.

Seven volcanic centers make up Nevis. These include Round Hill (3.43 Ma), Cades Bay (3.22 Ma), Hurricane Hill (2.7 Ma), Saddle Hill (1.8 Ma), Butlers Mountain (1.1 Ma), Red Cliff and Nevis Peak (0.98 Ma). These are mainly andesite and dacite lava domes, with associated block and ash flows, plus lahars. Nevis Peak has the highest elevation, at 984 m. Cades Bay and Farm Estate Soufriere are noted areas of hydrothermal activity.

Water has been piped since 1911 from a spring called the "Source", located 1800 feet up the mountain, to storage tanks at Rawlins Village, and since 1912, to Butler's Village. Additional drinking water comes from Nelson's Spring near Cotton Ground and Bath Spring. Groundwater has been extracted since the 1990s, and mixed with the Source water.

During the 17th and 18th centuries, massive deforestation was undertaken by the planters as the land was initially cleared for sugar cultivation. This intense land exploitation by the sugar and cotton industry lasted almost 300 years, and greatly changed the island's ecosystem.

In some places along the windswept southeast or "Windward" coast of the island, the landscape is radically altered compared with how it used to be in pre-colonial times. Due to extreme land erosion, the topsoil was swept away, and in some places at the coast, sheer cliffs as high as have developed.

Thick forest once covered the eastern coastal plain, where the Amerindians built their first settlements during the Aceramic period, complementing the ecosystem surrounding the coral reef just offshore. It was the easy access to fresh water on the island and the rich food source represented by the ocean life sheltered by the reef that made it feasible for the Amerindians to settle this area around 600 BC. With the loss of the natural vegetation, the balance in runoff nutrients to the reef was disturbed, eventually causing as much as 80 percent of the large eastern fringing reef to become inactive. As the reef broke apart, it, in turn, provided less protection for the coastline.

During times of maximum cultivation, sugar cane fields stretched from the coastline of Nevis up to an altitude at which the mountain slopes were too steep and rocky to farm. Nonetheless, once the sugar industry was finally abandoned, vegetation on the leeward side of the island regrew reasonably well, as scrub and secondary forest.

Nevis has several natural freshwater springs (including Nelson's Spring). The island also has numerous non-potable volcanic hot springs, including most notably the Bath Spring near Bath village, just south of the capital Charlestown.

After heavy rains, powerful rivers of rainwater pour down the numerous ravines (known as ghauts). When the water reaches the coastline, the corresponding coastal ponds, both freshwater and brackish, fill to capacity and beyond, spilling over into the sea.

With modern development, the existing freshwater springs are no longer enough to supply water to the whole island. The water supply now comes mostly from Government wells. The major source of potable water for the island is groundwater, obtained from 14 active wells. Water is pumped from the wells, stored and allowed to flow by gravity to the various locations.

The climate is tropical with little variation, tempered all year round (but particularly from December through February) by the steady north-easterly winds, called the trade winds. There is a slightly hotter and somewhat rainier season from May to November.

Nevis lies within the track area of tropical storms and occasional hurricanes. These storms can develop between August and October. This time of year has the heaviest rainfalls.

The official currency is the Eastern Caribbean dollar (EC$), which is shared by eight other territories in the region.

The European Commission's Delegation in Barbados and the Eastern Caribbean estimates the annual per capita Gross Domestic Product (GDP) on Nevis to be about 10 percent higher than on St. Kitts.

The major source of revenue for Nevis today is tourism. During the 2003–2004 season, approximately 40,000 tourists visited the island. A five-star hotel "(The Four Seasons Resort Nevis, West Indies)", four exclusive restored plantation inns, and several smaller hotels including Oualie Beach Resort are currently in operation. Larger developments along the west coast have recently been approved and are in the process of being developed.

The introduction of secrecy legislation has made offshore financial services a rapidly growing economic sector in Nevis. Incorporation of companies, international insurance and reinsurance, as well as several international banks, trust companies, asset management firms, have created a boost in the economy. During 2005, the Nevis Island Treasury collected $94.6 million in annual revenue, compared to $59.8 million during 2001. In 1998, 17,500 international banking companies were registered in Nevis. Registration and annual filing fees paid in 1999 by these entities amounted to over 10 percent of Nevis’ revenues. The offshore financial industry gained importance during the financial disaster of 1999 when Hurricane Lenny damaged the major resort on the island, causing the hotel to be closed down for a year and 400 of the 700 employees to be laid off.

In 2000, the Financial Action Task Force, part of the Organisation for Economic Co-operation and Development (OECD), issued a blacklist of 35 nations which were said to be non-cooperative in the campaign against tax evasion and money laundering. The list included the Federation of Saint Kitts and Nevis.

The political structure for the Federation of Saint Kitts and Nevis is based on the Westminster Parliamentary system, but it is a unique structure in that Nevis has its own unicameral legislature, consisting of Her Majesty's representative (the Deputy Governor General) and members of the Nevis Island Assembly. Nevis has considerable autonomy in its legislative branch. The constitution actually empowers the Nevis Island Legislature to make laws that cannot be abrogated by the National Assembly. In addition, Nevis has a constitutionally protected right to secede from the federation, should a two-third majority of the island's population vote for independence in a local referendum. Section 113.(1) of the constitution states: "The Nevis Island Legislature may provide that the island of Nevis shall cease to be federated with the island of Saint Christopher and accordingly that this Constitution shall no longer have effect in the island of Nevis."

Nevis has its own premier and its own government, the Nevis Island Administration. It collects its own taxes and has a separate budget, with a current account surplus. According to a statement released by the Nevis Ministry of Finance in 2005, Nevis had one of the highest growth rates in gross national product and per capita income in the Caribbean at that point.

Nevis elections are scheduled every five years. The Nevis elections of 2013, called on 23 January 2013, was won by the party in opposition, the Concerned Citizens Movement (CCM), led by Vance Amory. The CCM won three of the five seats in the Nevis Island Assembly, while the incumbent party, the Nevis Reformation Party (NRP), won two.

In the federal elections of 2010, the CCM won two of the three Nevis assigned Federal seats, while the NRP won one. Of the eight Saint Kitts assigned federal seats, the St Kitts-Nevis Labour Party won six and the People's Action Movement (PAM) two.

Joseph Parry, leader of the opposition, has indicated that he favours constitutional reform over secession for Nevis. His party, the NRP, has historically been the strongest and most ardent proponent for Nevis independence; the party came to power with secession as the main campaign issue. In 1975, the NRP manifesto declared that: "The Nevis Reformation Party will strive at all costs to gain secession for Nevis from St. Kitts – a privilege enjoyed by the island of Nevis prior to 1882."

A cursory proposal for constitutional reform was presented by the NRP in 1999, but the issue was not prominent in the 2006 election campaign and it appears a detailed proposal has yet to be worked out and agreed upon within the party.

In "Handbook of Federal Countries" published by Forum of Federations, the authors consider the constitution problematic because it does not "specifically outline" the federal financial arrangements or the means by which the central government and Nevis Island Administration can raise revenue: "In terms of the NIA, the constitution only states (in s. 108(1)) that 'all revenues...raised or received by the Administration...shall be paid into and form a fund styled the Nevis Island Consolidated Fund.' [...] Section 110(1) states that the proceeds of all 'takes' collected in St. Kitts and Nevis under any law are to be shared between the federal government and the Nevis Island Administration based on population. The share going to the NIA, however, is subject to deductions (s. 110(2)), such as the cost of common services and debt charges, as determined by the Governor-General (s.110(3)) on the advice of the Prime Minister who can also take advice from the Premier of Nevis (s.110(4))."

According to a 1995 report by the Commonwealth Observer Group of the Commonwealth Secretariat, "the federal government is also the local government of St Kitts and this has resulted in a perception among the political parties in Nevis that the interests of the people of Nevis are being neglected by the federal government which is more concerned with the administration of St Kitts than with the federal administration."

Simeon Daniel, Nevis' first Premier and former leader of the Nevis Reformation Party (NRP) and Vance Amory, Premier and leader of the Concerned Citizens Movement (CCM), made sovereign independence for Nevis from the Federation of Saint Kitts and Nevis part of their parties' agenda. Since independence from the United Kingdom in 1983, the Nevis Island Administration and the Federal Government have been involved in several conflicts over the interpretation of the new constitution which came into effect at independence. During an interview on Voice of America in March 1998, repeated in a government-issued press release headlined "PM Douglas Maintains 1983 Constitution is Flawed", Prime Minister Denzil Douglas called the constitution a "recipe for disaster and disharmony among the people of both islands".

A crisis developed in 1984 when the People's Action Movement (PAM) won a majority in the Federal elections and temporarily ceased honouring the Federal Government's financial obligations to Nevis. Consequently, cheques issued by the Nevis Administration were not honoured by the Bank, public servants in Nevis were not paid on time and the Nevis Island Administration experienced difficulties in meeting its financial obligations.

There is also substantial support in Nevis for British Overseas Territory status similar to Anguilla's, which was formerly the third of the tri-state Saint Christopher-Nevis-Anguilla colony.

In 1996, four new bills were introduced in the National Assembly in Saint Kitts, one of which made provisions to have revenue derived from activities in Nevis paid directly to the treasury in Saint Kitts instead of to the treasury in Nevis. Another bill, The Financial Services Committee Act, contained provisions that all investments in Saint Kitts and Nevis would require approval by an investment committee in Saint Kitts. This was controversial, because ever since 1983 the Nevis Island Administration had approved all investments for Nevis, on the basis that the constitution vests legislative authority for industries, trades and businesses and economic development in Nevis to the Nevis Island Administration.

All three representatives from Nevis, including the leader of the opposition in the Nevis Island Assembly, objected to the introduction of these bills into the National Assembly in Saint Kitts, arguing that the bills would affect the ability of Nevis to develop its offshore financial services sector and that the bills would be detrimental to the Nevis economy. All the representatives in opposition in the National Assembly shared the conviction that the bills if passed into law, would be unconstitutional and undermine the constitutional and legislative authority of the Nevis Island Administration, as well as result in the destruction of the economy of Nevis.

The constitutional crisis initially developed when the newly appointed Attorney General refused to grant permission for the Nevis Island Administration to assert its legal right in the Courts. After a decision of the High Court in favour of the Nevis Island Administration, the Prime Minister gave newspaper interviews stating that he "refused to accept the decision of the High Court". Due to the deteriorating relationship between the Nevis Island Administration and the Federal Government, a Constitutional Committee was appointed in April 1996 to advise on whether or not the present constitutional arrangement between the islands should continue. The committee recommended constitutional reform and the establishment of an island administration for Saint Kitts, separate from the Federal Government.

The Federal Government in Saint Kitts fills both functions today and Saint Kitts does not have an equivalent to the Nevis Island Administration. Disagreements between the political parties in Nevis and between the Nevis Island Administration and the Federal Government have prevented the recommendations by the electoral committee from being implemented. The problematic political arrangement between the two islands, therefore, continues to date.

Nevis has continued developing its own legislation, such as The Nevis International Insurance Ordinance and the Nevis International Mutual Funds Ordinance of 2004, but calls for secession are often based on concerns that the legislative authority of the Nevis Island Administration might be challenged again in the future.

The issues of political dissension between Saint Kitts and Nevis are often centred around perceptions of imbalance in the economic structure. As noted by many scholars, Nevisians have often referred to a structural imbalance in Saint Kitts' favour in how funds are distributed between the two islands and this issue has made the movement for Nevis secession a constant presence in the island's political arena, with many articles appearing in the local press expressing concerns such as those compiled by Everton Powell in "What Motivates Our Call for Independence": 

A referendum on secession from the Federation of St. Kitts and Nevis was held in 1998. Although 62% voted in favor of a secession, a two-thirds majority would have been necessary for the referendum to succeed.

The island of Nevis is divided into five administrative subdivisions called parishes, each of which has an elected representative in the Nevis Island Assembly. The division of this almost round island into parishes was done in a circular sector pattern, so each parish is shaped like a pie slice, reaching from the highest point of Nevis Peak down to the coastline.

The parishes have double names, for example Saint George Gingerland. The first part of the name is the name of the patron saint of the parish church, and the second part of the name is the traditional common name of the parish. Often the parishes are referred to simply by their common names. The religious part of a parish name is sometimes written or pronounced in the possessive: Saint George's Gingerland.

The five parishes of Nevis are:

"Culturama", the annual cultural festival of Nevis, is celebrated during the Emancipation Day weekend, the first week of August. The festivities include many traditional folk dances, such as the masquerade, the Moko jumbies on stilts, Cowboys and Indians, and Plait the Ribbon, a May pole dance. The celebration was given a more organised form in 1974, including a Miss Culture Show and a Calypso Competition, as well as drama performances, old fashion Troupes (including Johnny Walkers, Giant and Spear, Bulls, Red Cross and Blue Ribbon), arts and crafts exhibitions and recipe competitions. According to the Nevis Department of Culture, the aim is to protect and encourage indigenous folklore, in order to make sure that the uniquely Caribbean culture can "reassert itself and flourish".

The official language is English, yet Saint Kitts Creole (known on the island as 'Nevisian' or 'Nevis creole') is also widely spoken. The local creole is actually more widely spoken on Nevis than on the neighbouring island.

Nevisian culture has since the 17th century incorporated African, European and East Indian cultural elements, creating a distinct Afro-Caribbean culture. Several historical anthropologists have done field research Nevis and in Nevisian migrant communities in order to trace the creation and constitution of a Nevisian cultural community. Karen Fog Olwig published her research about Nevis in 1993, writing that the areas where the Afro-Caribbean traditions were especially strong and flourishing relate to kinship and subsistence farming. However, she adds, Afro-Caribbean cultural impulses were not recognised or valued in the colonial society and were therefore often expressed through Euro-Caribbean cultural forms. Examples of European forms appropriated to express Afro-Caribbean culture are the Nevisian and Kittitian "Tea Meetings" and "Christmas Sports". According to anthropologist Roger D. Abrahams, these traditional performance art forms are "Nevisian approximation of British performance codes, techniques, and patterns". He writes that the Tea Meetings were staged as theatrical "battles between decorum and chaos", decorum represented by the ceremony chairmen and chaos the hecklers in the audience, with a diplomatic King or a Queen presiding over the battle to ensure fairness.

The Christmas Sports included a form of comedy and satire based on local events and gossip. They were historically an important part of the Christmas celebrations in Nevis, performed on Christmas Eve by small troupes consisting of five or six men accompanied by string bands from different parts of the island. One of the men in the troupe was dressed as a woman, playing all the female parts in the dramatisations. The troupes moved from yard to yard to perform their skits, using props, face paint and costumes to play the roles of well-known personalities in the community. Examples of gossip about undesired behaviour that could surface in the skits for comic effect were querulous neighbours, adulterous affairs, planters mistreating workers, domestic disputes or abuse, crooked politicians and any form of stealing or cheating experienced in the society. Even though no names were mentioned in these skits, the audience would usually be able to guess who the heckling message in the troupe's dramatised portrayals was aimed at, as it was played out right on the person's own front yard. The acts thus functioned as social and moral commentaries on current events and behaviours in Nevisian society. This particular form is called "Bazzarding" by many locals. Abrahams theorises that Christmas Sports are rooted in the pre-emancipation Christmas and New Year holiday celebrations, when the enslaved population had several days off.

American folklorist and musicologist Alan Lomax visited Nevis in 1962 in order to conduct long-term research into the black folk culture of the island. His field trip to Nevis and surrounding islands resulted in the anthology "Lomax Caribbean Voyage" series. 
Among the Nevisians recorded were chantey-singing fishermen in a session organised in a rum shop in Newcastle; Santoy, the Calypsonian, performing calypsos by Nevisian ballader and local legend Charles Walters to guitar and cuatro; and string bands, fife players and drummers from Gingerland, performing quadrilles.

The island is also known for "Jamband music", which is the kind of music performed by local bands during the "Culturama Festival" and is key to "Jouvert" dancing. The sounds of the so-called "Iron Band" are also popular within the culture; many locals come together using any old pans, sinks, or other kits of any sort; which they use to create sounds and music. This form of music is played throughout the villages during the Christmas and carnival seasons.

A series of earthquakes during the 18th century severely damaged most of the colonial-era stone buildings of Charlestown. The Georgian stone buildings in Charlestown that are visible today had to be partially rebuilt after the earthquakes, and this led to the development of a new architectural style, consisting of a wooden upper floor over a stone ground floor; the new style resisted earthquake damage much more effectively.

Two famous Nevisian buildings from the 18th century are Hermitage Plantation, built of lignum vitae wood in 1740, the oldest surviving wooden house still in use in the Caribbean today, and the Bath Hotel, the first hotel in the Caribbean, a luxury hotel and spa built by John Huggins in 1778. The soothing waters of the hotel's hot spring and the lively social life on Nevis attracted many famous Europeans including Antigua-based Admiral Nelson, and Prince William Henry, Duke of Clarence, (future William IV of the United Kingdom), who attended balls and private parties at the Bath Hotel. Today, the building serves as government offices, and there are two outdoor hot-spring bathing spots which were specially constructed in recent years for public use.

An often repeated legend appears to suggest that a destructive 1680 or 1690 earthquake and tsunami destroyed the buildings of the original capital Jamestown on the west coast. Folk tales say that the town sank beneath the ocean, and the tsunami is blamed for the escape of (possibly fictional) pirate Red Legs Greaves. However, archaeologists from the University of Southampton who have done excavations in the area, have found no evidence to indicate that the story is true. They state that this story may originate with an over-excited Victorian letter writer sharing somewhat exaggerated accounts of his exotic life in the tropical colony with a British audience back home. One such letter recounts that so much damage was done to the town that it was completely evacuated, and was engulfed by the sea. Early maps do not, however, actually show a settlement called "Jamestown", only "Morton's Bay", and later maps show that all that was left of Jamestown/Morton's Bay in 1818 was a building labelled "Pleasure House". Very old bricks that wash up on Pinney's Beach after storms may have contributed to this legend of a sunken town; however these bricks are thought to be dumped ballast from 17th and 18th century sailing ships.





</doc>
<doc id="21504" url="https://en.wikipedia.org/wiki?curid=21504" title="Nicole Kidman">
Nicole Kidman

Nicole Mary Kidman (born 20 June 1967) is an Australian actress and producer. Her awards include an Academy Award, two Primetime Emmy Awards, and four Golden Globe Awards. She was listed among the highest-paid actresses in the world in 2006, 2018, and 2019. "Time" magazine twice named her one of the 100 most influential people in the world, in 2004 and 2018.

Kidman began her acting career in Australia with the 1983 films "Bush Christmas" and "BMX Bandits". Her breakthrough came in 1989 with the thriller film "Dead Calm" and the miniseries "Bangkok Hilton". In 1990, she made her Hollywood debut in the racing film "Days of Thunder", opposite Tom Cruise. She went on to achieve wider recognition with lead roles in "Far and Away" (1992), "Batman Forever" (1995), "To Die For" (1995) and "Eyes Wide Shut" (1999). Kidman won the Academy Award for Best Actress for portraying the writer Virginia Woolf in the drama "The Hours" (2002). Her other Oscar-nominated roles were as a courtesan in the musical "Moulin Rouge!" (2001) and emotionally troubled mothers in the dramas "Rabbit Hole" (2010) and "Lion" (2016).

Kidman's other film credits include "The Others" (2001), "Cold Mountain" (2003), "Dogville" (2003), "Birth" (2004), "The Stepford Wives" (2004) "Australia" (2008), "The Paperboy" (2012), "Paddington" (2014), "Destroyer" (2018), "Aquaman" (2018) and "Bombshell" (2019). Her television roles include two projects for HBO, the biopic "Hemingway & Gellhorn" (2012) and the drama series "Big Little Lies" (2017–2019). The latter earned Kidman the Primetime Emmy Award for Outstanding Lead Actress and Outstanding Limited Series.

Kidman has been a Goodwill ambassador for UNICEF since 1994 and for UNIFEM since 2006. In 2006, she was appointed Companion of the Order of Australia. Since she was born to Australian parents in Hawaii, Kidman has dual citizenship of Australia and the United States. In 2010, she founded the production company Blossom Films. She has been married to singer Keith Urban since 2006, and was earlier married to Tom Cruise.

Kidman was born on 20 June 1967, in Honolulu, Hawaii, while her Australian parents were temporarily in the United States on student visas. Her mother, Janelle Ann (née Glenny), is a nursing instructor who edited her husband's books and was a member of the Women's Electoral Lobby; her father, Antony Kidman, was a biochemist, clinical psychologist and author. Kidman's ancestry includes Irish and Scottish heritage.

Being born in Hawaii, she was given the Hawaiian name "Hōkūlani", meaning "heavenly star". The inspiration came from a baby elephant born around the same time at the Honolulu Zoo.

At the time of Kidman's birth, her father was a graduate student at the University of Hawaiʻi at Mānoa. He became a visiting fellow at the National Institute of Mental Health of the United States. Opposed to the war in Vietnam, Kidman's parents participated in anti-war protests while living in Washington, D.C. The family returned to Australia when Kidman was four and her mother now lives on Sydney's North Shore. Kidman has a younger sister, Antonia Kidman, a journalist and TV presenter.

Kidman grew up in Sydney and attended Lane Cove Public School and North Sydney Girls' High School. She was enrolled in ballet at three and showed her natural talent for acting in her primary and high school years. She says that she was first inspired to become an actress upon seeing Margaret Hamilton's performance as the Wicked Witch of the West in "The Wizard of Oz". Kidman has revealed that she was timid as a child, saying, "I am very shy – really shy – I even had a stutter as a kid, which I slowly got over, but I still regress into that shyness. So I don't like walking into a crowded restaurant by myself; I don't like going to a party by myself."

She initially studied at the Phillip Street Theatre in Sydney, alongside Naomi Watts who had attended the same high school. She also attended the Australian Theatre for Young People. Here she took up drama, mime and performing in her teens, finding acting to be a refuge. Owing to her fair skin and naturally red hair, the Australian sun forced the young Kidman to rehearse in halls of the theatre. A regular at the Phillip Street Theatre, she received praise and encouragement to pursue acting full-time.

In 1983, aged 16, Kidman made her film debut in a remake of the Australian holiday season favourite "Bush Christmas". By the end of 1983, she had a supporting role in the television series "Five Mile Creek". In 1984, her mother was diagnosed with breast cancer, which caused Kidman to halt her acting work temporarily while she studied massage so she could help her mother with physical therapy. She began gaining popularity in the mid-1980s after appearing in several film roles, including "BMX Bandits" (1983), "Watch the Shadows Dance" (1987 aka "Nightmaster"), and the romantic comedy "Windrider" (1986), which earned Kidman attention due to her racy scenes. Also during the decade, she appeared in several Australian productions, including the soap opera "A Country Practice" and the 1987 miniseries "Vietnam". She also made guest appearances on Australian television programs and TV movies.

In 1988, Kidman appeared in "Emerald City", based on the play of the same name. The Australian film earned her an Australian Film Institute award for Best Supporting Actress. Kidman next starred with Sam Neill in "Dead Calm" (1989) as Rae Ingram, playing the wife of a naval officer. The thriller brought Kidman to international recognition; "Variety" commented: "Throughout the film, Kidman is excellent. She gives the character of Rae real tenacity and energy." Meanwhile, critic Roger Ebert noted the excellent chemistry between the leads, stating, "Kidman and Zane do generate real, palpable hatred in their scenes together." She followed that up with the Australian miniseries "Bangkok Hilton". She next moved on to star alongside her then-boyfriend and future husband, Tom Cruise, in the 1990 auto racing film "Days of Thunder", as a young doctor who falls in love with a NASCAR driver. It is Kidman's American debut and was among the highest-grossing films of the year.

In 1991, she co-starred with Thandie Newton and former classmate Naomi Watts in the Australian independent film "Flirting". They portrayed high school girls in this coming of age story, which won the Australian Film Institute Award for Best Film. That same year, her work in the film "Billy Bathgate" earned Kidman her first Golden Globe Award nomination, for Best Supporting Actress. "The New York Times", in its film review, called her "a beauty with, it seems, a sense of humor". The following year, she and Cruise re-teamed for Ron Howard's Irish epic "Far and Away" (1992), which was a modest critical and commercial success. In 1993, she starred in the thriller "Malice" opposite Alec Baldwin and the drama "My Life" opposite Michael Keaton.

In 1995, Kidman played Dr. Chase Meridian, the damsel in distress, in the superhero film "Batman Forever", opposite Val Kilmer as the film's title character. The same year, she starred in Gus Van Sant's critically acclaimed dark comedy "To Die For", in which she played the murderous newscaster Suzanne Stone. Of Kidman's Golden Globe Award-winning performance, Mick LaSalle of the "San Francisco Chronicle" said "[she] brings to the role layers of meaning, intention and impulse. Telling her story in close-up – as she does throughout the film – Kidman lets you see the calculation, the wheels turning, the transparent efforts to charm that succeed in charming all the same." Kidman next appeared, alongside Barbara Hershey and John Malkovich, in "The Portrait of a Lady" (1996), based on the novel of the same name, and starred in "The Peacemaker" (1997) as White House nuclear expert Dr. Julia Kelly, opposite George Clooney. The latter film grossed US$110 million worldwide. Kidman starred in comedy "Practical Magic" (1998) with Sandra Bullock as two witch sisters who face a curse which threatens to prevent them ever finding lasting love. While the film opened atop the chart on its North American opening weekend, it flopped at the box office. She returned to her work on stage the same year in the David Hare play "The Blue Room", which opened in London.

In 1999, Kidman reunited with then husband, Tom Cruise, to portray a Manhattan couple on a sexual odyssey, in "Eyes Wide Shut", the final film of director Stanley Kubrick. It was subject to censorship controversies due to the explicit nature of its sex scenes. After a brief hiatus and a highly publicised divorce from Cruise, Kidman returned to the screen to play a mail-order bride in the British-American drama "Birthday Girl". In 2001, Kidman played the cabaret actress and courtesan Satine in Baz Luhrmann's musical "Moulin Rouge!", opposite Ewan McGregor. Her performance and her singing received positive reviews; Paul Clinton of "CNN.com" called it her best work since "To Die For", and wrote "[she] is smoldering and stunning as Satine. She moves with total confidence throughout the film [...] Kidman seems to specialize in 'ice queen' characters, but with Satine, she allows herself to thaw, just a bit." Subsequently, Kidman received her second Golden Globe Award, for Best Actress in a Motion Picture Musical or Comedy, as well as many other acting awards and nominations. She also received her first Academy Award nomination, for Best Actress.

Kidman also starred in Alejandro Amenábar's horror film "The Others" (2001), as Grace Stewart, a mother living in the Channel Islands during World War II who suspects her house is haunted. Grossing over US$210 million worldwide, the film also earned several Goya Award nominations, including a Best Actress nomination for Kidman. She received her second BAFTA Award and fifth Golden Globe Award nominations. Roger Ebert commented that "Alejandro Amenábar has the patience to create a languorous, dreamy atmosphere, and Nicole Kidman succeeds in convincing us that she is a normal person in a disturbing situation, and not just a standard-issue horror movie hysteric." Kidman was named the World's Most Beautiful Person by "People" magazine.

In 2002, Kidman won critical praise for her portrayal of Virginia Woolf in Stephen Daldry's "The Hours", which stars Meryl Streep and Julianne Moore. Kidman famously wore prosthetics that were applied to her nose making her almost unrecognizable playing the author during her time in 1920s England, and her bouts with depression and mental illness while trying to write her novel, "Mrs. Dalloway". The film earned positive notices and several nominations, including for an Academy Award for Best Picture. "The New York Times" wrote that, "Ms. Kidman, in a performance of astounding bravery, evokes the savage inner war waged by a brilliant mind against a system of faulty wiring that transmits a searing, crazy static into her brain". Kidman won numerous critics' awards, including her first BAFTA Award, third Golden Globe Award, and the Academy Award for Best Actress. As the first Australian actress to win an Academy Award, Kidman made a teary acceptance speech about the importance of art, even during times of war, saying, "Why do you come to the Academy Awards when the world is in such turmoil? Because art is important. And because you believe in what you do and you want to honour that, and it is a tradition that needs to be upheld."

Following her Oscar win, Kidman appeared in three very different films in 2003. The first, a leading role in "Dogville", by Danish director Lars von Trier, was an experimental film set on a bare soundstage. Though the film divided critics in the United States, Kidman still earned praise for her performance. Peter Travers of "Rolling Stone" magazine stated: "Kidman gives the most emotionally bruising performance of her career in Dogville, a movie that never met a cliche it didn't stomp on." The second was an adaptation of Philip Roth's novel "The Human Stain", opposite Anthony Hopkins. Her third film was Anthony Minghella's war drama "Cold Mountain". Kidman appeared opposite Jude Law and Renée Zellweger, playing Southerner Ada Monroe, who is in love with Law's character and separated by the Civil War. "TIME" magazine wrote, "Kidman takes strength from Ada's plight and grows steadily, literally luminous. Her sculptural pallor gives way to warm radiance in the firelight". The film garnered several award nominations and wins for its actors; Kidman received her sixth Golden Globe Award nomination at the 61st Golden Globe Awards for Best Actress.

In 2004 she appeared in the film "Birth", which received controversy over a scene in which Kidman shares a bath with her co-star, 10-year-old Cameron Bright. At a press conference at the Venice Film Festival, Kidman addressed the controversy saying, "It wasn't that I wanted to make a film where I kiss a 10-year-old boy. I wanted to make a film where you understand love". Kidman earned her seventh Golden Globe nomination, for Best Actress – Motion Picture Drama. That same year, she appeared as a successful producer in the black comedy-science-fiction film "The Stepford Wives", a remake of the 1975 film of the same name, directed by Frank Oz. In 2005, Kidman appeared opposite Sean Penn in the Sydney Pollack thriller "The Interpreter", playing UN translator Silvia Broome, and with Will Ferrell in the romantic comedy "Bewitched", based on the 1960s TV sitcom of the same name. While neither film fared well in the United States, both were international successes. Kidman and Ferrell earned the Razzie Award for Worst Screen Couple.

In conjunction with her success in the film industry, Kidman became the face of the "Chanel No. 5" perfume brand. She starred in a campaign of television and print ads with Rodrigo Santoro, directed by "Moulin Rouge!" director Baz Luhrmann, to promote the fragrance during the holiday seasons of 2004, 2005, 2006, and 2008. The three-minute commercial produced for "Chanel No. 5" made Kidman the record holder for the most money paid per minute to an actor after she reportedly earned US$12million for the three-minute advert. During this time, Kidman was also listed as the 45th Most Powerful Celebrity on the 2005 "Forbes" Celebrity 100 List. She made a reported US$14.5 million in 2004–2005. On "People" magazine's list of 2005's highest-paid actresses, Kidman was second behind Julia Roberts, with US$16–17 million per-film price tag. Nintendo in 2007 announced that Kidman would be the new face of Nintendo's advertising campaign for the Nintendo DS game More Brain Training in its European market.

In 2006, Kidman portrayed photographer Diane Arbus in the biographical film "Fur", opposite Robert Downey Jr., and lent her voice to the animated film "Happy Feet", which grossed over US$384 million worldwide. In 2007, she starred in the science-fiction movie "The Invasion" directed by Oliver Hirschbiegel, a remake of the 1956 "Invasion of the Body Snatchers", and starred opposite Jennifer Jason Leigh and Jack Black in Noah Baumbach's comedy-drama "Margot at the Wedding", which earned her a Satellite Award nomination for Best Actress – Musical or Comedy. She also starred in the fantasy-adventure, "The Golden Compass" (2007), playing the villainous Marisa Coulter.

In 2008, she reunited with "Moulin Rouge!" director Baz Luhrmann in the Australian period film "Australia", set in the remote Northern Territory during the Japanese attack on Darwin during World War II. Kidman played opposite Hugh Jackman as an Englishwoman feeling overwhelmed by the continent. The acting was praised and the movie was a box office success worldwide. Kidman appeared in the 2009 Rob Marshall musical "Nine", portraying the Federico Fellini-like character's muse, Claudia Jenssen, with fellow Oscar winners Daniel Day-Lewis, Judi Dench, Marion Cotillard, Penélope Cruz and Sophia Loren. Kidman, whose screen time was brief compared to the other actresses, performed the musical number "Unusual Way", alongside Day-Lewis. The film received several Golden Globe Award and Academy Award nominations, and earned Kidman a fourth Screen Actors Guild Award nomination, as part of the Outstanding Performance by a Cast in a Motion Picture.

In 2010, Kidman starred with Aaron Eckhart in the film adaptation of the Pulitzer Prize-winning play "Rabbit Hole", for which she vacated her role in the Woody Allen picture "You Will Meet a Tall Dark Stranger". Her portrayal as a grieving mother in the film earned her critical acclaim, and received nominations for the Academy Awards, Golden Globe Awards, and Screen Actors Guild Awards. She lent her voice to a promotional video that Australia used to support its bid to host the 2018 FIFA World Cup. In 2011, she starred alongside Nicolas Cage in director Joel Schumacher's action-thriller "Trespass", with the stars playing a married couple taken hostage, and appeared with Adam Sandler and Jennifer Aniston in Dennis Dugan's romantic comedy "Just Go with It", as a trophy wife.

In 2012, Kidman and Clive Owen starred in the HBO film "Hemingway & Gellhorn", and about Ernest Hemingway and his relationship with Martha Gellhorn. In Lee Daniels' adaptation of the Pete Dexter novel, "The Paperboy" (2012), she portrayed death row groupie Charlotte Bless, and performed sex scenes that she claims not to have remembered until seeing the finished film. The film competed in the 2012 Cannes Film Festival, and Kidman's performance drew nominations for the SAG and the Saturn Award for Best Supporting Actress, gave Kidman her second Golden Globe Award nomination for Best Supporting Actress and her tenth nomination overall. In 2012, Kidman's audiobook recording of Virginia Woolf's "To the Lighthouse" was released at Audible.com. Kidman starred as an unstable mother in Park Chan-wook's "Stoker" (2013), to a positive response and a Saturn Award nomination for Best Supporting Actress. In April 2013 she was selected as a member of the main competition jury at the 2013 Cannes Film Festival.

In 2014, Kidman starred in the biographical film "Grace of Monaco" in the title role that chronicles the 1962 crisis, in which Charles de Gaulle blockaded the tiny principality, angered by Monaco's status as a tax haven for wealthy French subjects and Kelly's contemplating a Hollywood return to star in Alfred Hitchcock's "Marnie". Opening out of competition at the 2014 Cannes Film Festival, the film received largely negative reviews. Kidman also starred in two films with Colin Firth that year, the first being the British-Australian historical drama "The Railway Man", in which Kidman played an officer's wife. Katherine Monk of the Montreal Gazette said of Kidman's performance, "It's a truly masterful piece of acting that transcends Teplitzky's store-bought framing, but it's Kidman who delivers the biggest surprise: For the first time since her eyebrows turned into solid marble arches, the Australian Oscar winner is truly terrific". Her second film with Firth was the British thriller film "Before I Go To Sleep", portraying a car crash survivor with brain damage. She also appeared in the family film "Paddington" (2014) as a villain.

In 2015, Kidman starred in the drama "Strangerland", which opened at the 2015 Sundance Film Festival, and the Jason Bateman-directed "The Family Fang", produced by Kidman's production company, Blossom Films, which premiered at the 2015 Toronto International Film Festival. In her other 2015 film release, the biographical drama "Queen of the Desert", she portrayed writer, traveller, political officer, administrator, and archaeologist Gertrude Bell. Kidman played a district attorney, opposite Julia Roberts and Chiwetel Ejiofor, in the little-seen film "Secret in Their Eyes" (also 2015), a remake of the 2009 Argentine film of the same name, both based on the novel "La pregunta de sus ojos" by author Eduardo Sacheri. After more than 15 years, Kidman returned to the West End in the UK premiere of "Photograph 51" at the Noël Coward Theatre. She starred as British scientist Rosalind Franklin, working for the discovery of the structure of DNA, in the production from 5 September to 21 November 2015, directed by Michael Grandage. Her return to the West End was hailed a success, especially after having won an acting award for her portrayal in the play.

In 2016's "Lion", Kidman portrayed Sue, the adoptive mother of Saroo Brierley, an Indian boy who was separated from his birth family, a role she felt connected to as she herself is the mother of adopted children. She earned favorable reviews for her performance, as well as nominations for the Academy Award for Best Supporting Actress, her fourth nomination overall, and her eleventh Golden Globe Award nomination, among others. Richard Roeper of the "Chicago Sun-Times" thought that "Kidman gives a powerful and moving performance as Saroo's adoptive mother, who loves her son with every molecule of her being, but comes to understand his quest. It's as good as anything she's done in the last decade." Budgeted at US$12 million, "Lion" earned over US$140 million globally. She also gave a voice-over performance for the English version of the animated film "The Guardian Brothers."

In 2017, Kidman returned to television for "Big Little Lies", a drama series based on Liane Moriarty's novel, which premiered on HBO. She also served as producer alongside her co-star, Reese Witherspoon, and the show's director, Jean-Marc Vallée. She played Celeste Wright, a former lawyer and housewife, who is concealing her abusive relationship with her husband, played by Alexander Skarsgård. Matthew Jacobs of "The Huffington Post" considered that she "delivered a career-defining performance", while Ann Hornaday of "The Washington Post" wrote that "Kidman belongs in the pantheon of great actresses". She won the Primetime Emmy Award for Outstanding Lead Actress in a Limited Series or Movie for her performance, as well as winning the Primetime Emmy Award for Outstanding Limited Series as a producer. She also won a Critics' Choice Television Award, Golden Globe Award, and Screen Actors Guild Award.

Kidman next played Martha Farnsworth, the headmistress of an all-girls school during the American Civil War, in Sofia Coppola's drama "The Beguiled", a remake of a 1971 film of the same name, which premiered at the 2017 Cannes Film Festival, competing for the Palme d'Or. Both films were adaptations of a novel by Thomas P. Cullinan, The film was an arthouse success, and Katie Walsh of "Tribune News Service" found Kidman to be "particularly, unsurprisingly excellent in her performance as the steely Miss Martha. She is controlled and in control, unflappable. Her genteel manners and femininity co-exist easily with her toughness." Kidman had two other films premiere at the festival, the science-fiction romantic comedy "How to Talk to Girls at Parties", reuniting her with director John Cameron Mitchell, and the psychological thriller "The Killing of a Sacred Deer", directed by Yorgos Lanthimos, which also competed for the Palme d'Or. Also in 2017, Kidman played supporting roles in the television series "" and in the comedy-drama "The Upside", a remake of the 2011 French comedy "The Intouchables", starring Bryan Cranston and Kevin Hart.

Kidman starred in two 2018 dramas —"Destroyer" and "Boy Erased". In the former, she played a detective troubled by a case for two decades. Peter Debruge of "Variety" and Brooke Marine of "W" both found her "unrecognizable" in the role and Debruge added that "she disappears into an entirely new skin, rearranging her insides to fit the character’s tough hide", whereas Marine highlighted Kidman's method acting. The latter film is based on Garrard Conley's "", and features Russell Crowe and Kidman as socially conservative parents who send their son (played by Lucas Hedges) to a gay conversion program. Richard Lawson of "Vanity Fair" credited all three performers for "elevating the fairly standard-issue material to poignant highs". Also that year, Kidman played Queen Atlanna, the mother of the title character, in the DC Extended Universe superhero film "Aquaman".

"Forbes" ranked her as the fourth highest-paid actress in the world in 2019, with an annual income of $34 million. She took on the supporting part of a rich socialite in John Crowley's drama "The Goldfinch", an adaptation of the novel of the same name by Donna Tartt, starring Ansel Elgort. Although it was poorly received, Owen Gleiberman commended Kidman for playing her part with "elegant affection". She next starred alongside Charlize Theron and Margot Robbie in the drama "Bombshell", about sexual harassment at Fox News, in which she portrayed Gretchen Carlson. Manohla Dargis thought that despite having a smaller role to the two other leading ladies, Kidman had successfully made Carlson "ever-so-slightly ridiculous, adding a sharp sliver of comedy that underscores how self-serving and futile her rebellious gestures at the network are". She received another Screen Actor Guild nomination.

Kidman will star in and serve as executive producer on three television miniseries. She will headline the HBO miniseries "The Undoing", based on the novel "You Should Have Known" by Jean Hanff Korelitz. She will then star in the Hulu miniseries "Nine Perfect Strangers" based on the novel of the same name by Liane Moriarty, and in Amazon Prime Video's thriller miniseries based on the upcoming novel "Pretty Things" by Janelle Brown. Kidman will also serve as an executive producer for the television series "The Expatriates" for Amazon Prime Video.

Kidman has been married twice: first to actor Tom Cruise, and later to country singer Keith Urban. Kidman met Cruise in November 1989, while filming "Days of Thunder"; they were married on Christmas Eve in Telluride, Colorado. The couple adopted a daughter, Isabella Jane Cruise (born 1992), and a son, Connor Antony (born 1995). On 5 February 2001, the couple's spokesperson announced their separation. Cruise filed for divorce two days later, and the marriage was dissolved in August of that year, with Cruise citing irreconcilable differences. In a 2007 interview with "Marie Claire", Kidman noted the incorrect reporting of the ectopic pregnancy early in her marriage. "It was wrongly reported as miscarriage, by everyone who picked up the story." "So it's huge news, and it didn't happen."

In the June 2006 issue of "Ladies' Home Journal", she said she still loved Cruise: "He was huge; still is. To me, he was just Tom, but to everybody else, he is huge. But he was lovely to me and I loved him. I still love him." In addition, she has expressed shock about their divorce. In 2015, former Church of Scientology executive Mark Rathbun claimed in a documentary film that he was instructed to "facilitate [Cruise's] break-up with Nicole Kidman". Cruise's auditor further claimed Kidman had been wiretapped on Cruise's suggestion.

Prior to marrying Cruise, Kidman had been involved in relationships with Australian actor Marcus Graham and "Windrider" (1986) co-star Tom Burlinson. She was also said to be involved with Adrien Brody. The film "Cold Mountain" brought rumours that an affair between Kidman and co-star Jude Law was responsible for the break-up of his marriage. Both denied the allegations, and Kidman won an undisclosed sum from the British tabloids that published the story. She met musician Lenny Kravitz in 2003, and dated him into 2004. Kidman was also romantically linked to rapper Q-Tip. Robbie Williams claims he had a short romance with Kidman on her yacht in summer 2004.

In a 2007 "Vanity Fair" interview, Kidman revealed that she had been secretly engaged to someone prior to her present relationship to New Zealand-Australian country singer Keith Urban, whom she met at G'Day LA, an event honouring Australians, in January 2005. Kidman married Urban on 25 June 2006, at Cardinal Cerretti Memorial Chapel in the grounds of St Patrick's Estate, Manly in Sydney. In an interview in 2015, Kidman said, "We didn't really know each other – we got to know each other during our marriage." They maintain homes in Sydney, Sutton Forest (New South Wales, Australia); Los Angeles; Nashville (Tennessee, U.S.); and a condominium in Manhattan purchased for US$10 million. The couple's first daughter, Sunday Rose, was born in 2008, in Nashville. In 2010, Kidman and Urban had their second daughter, Faith Margaret, via gestational surrogacy at Nashville's Centennial Women's Hospital. In an interview by Tina Brown at the 2015 Women in the World conference, she stated that her attention turned to her career after her divorce from Cruise: "Out of my divorce came work that was applauded so that was an interesting thing for me", leading to her Academy Award in 2003.

Kidman is Catholic and even considered becoming a nun at one point. She attended Mary Mackillop Chapel in North Sydney. Following criticism of "The Golden Compass" by Catholic leaders as anti-Catholic, Kidman told "Entertainment Weekly" that the Catholic Church is part of her "essence", and that her religious beliefs would prevent her from taking a role in a film she perceived as anti-Catholic. During her divorce from Tom Cruise, she stated that she did not want their children raised as Scientologists. She has been reluctant to discuss Scientology since her divorce.

A supporter of women's rights, Kidman testified before the United States House of Representatives Committee on Foreign Affairs to support the International Violence Against Women Act in 2009. In January 2017, she stated her support for the legalisation of same-sex marriage in Australia. Kidman has also donated to U.S. Democratic party candidates.

In 2002, Kidman first appeared on the Australian rich list published annually in the "Business Review Weekly" with an estimated net worth of A$122 million. In the 2011 published list, Kidman's wealth was estimated at A$304 million, down from A$329 million in 2010. Kidman has raised money for, and drawn attention to, disadvantaged children around the world. In 1994, she was appointed a goodwill ambassador for UNICEF, and in 2004, she was honoured as a "Citizen of the World" by the United Nations. Kidman joined the Little Tee Campaign for breast cancer care to design T-shirts or vests to raise money to fight the disease; motivated by her mother's own battle with breast cancer in 1984.

In the 2006 Australia Day Honours, Kidman was appointed Companion of Order of Australia (AC) for "service to the performing arts as an acclaimed motion picture performer, to health care through contributions to improve medical treatment for women and children and advocacy for cancer research, to youth as a principal supporter of young performing artists, and to humanitarian causes in Australia and internationally". However, due to film commitments and her wedding to Urban, it wasn't until 13 April 2007 that she was presented with the honour. It was presented by the Governor-General of Australia, Major General Michael Jeffery, in a ceremony at Government House, Canberra.

Kidman was appointed goodwill ambassador of the United Nations Development Fund for Women (UNIFEM) in 2006. She visited Kosovo in 2006 to learn about women's experiences of conflict and UNIFEM's support efforts. She is also the international spokesperson for UNIFEM's Say NO – UNiTE to End Violence against Women initiative. Kidman and the UNIFEM executive director presented over five million signatures collected during the first phase of this to the UN Secretary-General on 25 November 2008. In 2016, Kidman donated $50,000 to UN Women.

In the beginning of 2009, Kidman appeared in a series of postage stamps featuring Australian actors. She, Geoffrey Rush, Russell Crowe and Cate Blanchett each appear twice in the series: once as themselves and once as their Academy Award-nominated character; Kidman's second stamp showed her as Satine from "Moulin Rouge!". On 8 January 2010, alongside Nancy Pelosi, Joan Chen and Joe Torre, Kidman attended the ceremony to help the Family Violence Prevention Fund break ground on a new international centre located in the Presidio of San Francisco. In 2015, Kidman became the brand ambassador for Etihad Airways.

Kidman supports the Nashville Predators, being seen and photographed almost nightly throughout the season. Additionally, she supports the Sydney Swans in the Australian Football League and once served as a club ambassador.

Kidman's discography consists of one spoken word album, one extended play, three singles, three music videos, ten other appearances, a number of unreleased tracks and two tribute songs recorded by various artists.
Kidman, primarily known in the field of acting, entered the music industry in the 2000s after recording a number of tracks for the soundtrack album to Baz Luhrmann's 2001 motion picture "Moulin Rouge!", which she starred in. Her duet with Ewan McGregor entitled "Come What May" was released as her debut and the second single of the OST through Interscope on 24 September 2001. The composition became the eighth-highest selling single by an Australian artist for that year, being certified Gold by Australian Recording Industry Association, while reaching on the UK Singles Chart at number twenty-seven. In addition, the song received a nomination at the 59th Golden Globe Awards as the Best Original Song, and has been listed as the eighty-fifth within AFI's 100 Years...100 Songs by American Film Institute.

"Somethin' Stupid", a cover version of Frank and Nancy Sinatra followed soon. The track, recorded as a duet with English singer-songwriter Robbie Williams, was issued on 14 December 2001 by Chrysalis Records as the lead single of his fourth studio album, "Swing When You're Winning". Kidman's second single topped the official music charts in Italy, New Zealand, Portugal, and England, as well as scored top ten placings all over Europe, including Australia, Austria, Belgium, Denmark, Germany, Netherlands, Norway, and Switzerland. Apart from being certified either Gold or Silver in a number of countries, it was classified as the eleventh best-selling single of 2002 in Italy, thirtieth in the UK, the fifty-ninth in Australia, and the ninety-third in France, respectively. The song peaked at No. 8 in the Australian ARIAnet Singles Chart and at No. 1, for three weeks, in the UK.

On 5 April 2002, Kidman released, through Interscope, her third single, a cover of Randy Crawford's "One Day I'll Fly Away". The song, a Tony Philips remix, was promoted as the pilot single of a follow-up to the original soundtrack of the same name, "Moulin Rouge! Vol. 2". In 2006, she contributed with her vocal for the OST "Happy Feet" on a rendition of the Prince song "Kiss". In 2009, she was featured on the soundtrack of Rob Marshall's 2009 movie musical "Nine", singing the song
"Unusual Way".

Her name was later been credited on a track called "What's the Procedure", issued on 14 March 2013, on the compilation album "I Know Why They Call It Pop: Volume 2" by Rok Lok Records. Among others, Kidman also narrated an audiobook in 2012.

In 2017, she and Nicolle Gaylon sang backing vocals on her husband, country music singer Keith Urban's song "Female".

In 2003, Kidman received a Star on the Hollywood Walk of Fame. In addition to her 2003 Academy Award for Best Actress, Kidman has received Best Actress awards from the following critics' groups or award-granting organisations: the Hollywood Foreign Press Association (Golden Globe Awards), Australian Film Institute, Blockbuster Entertainment Awards, Empire Awards, Hollywood Film Festival, London Film Critics' Circle, Russian Guild of Film Critics, Satellite Awards, and Southeastern Film Critics Association.

Kidman also received recognition from the National Association of Theatre Owners at the ShoWest Convention in 1992 as the Female Star of Tomorrow, and in 2002 for a Distinguished Decade of Achievement in Film. In 2003, she was given the American Cinematheque Award.


Hart Radio honors Nicole for her wonderful relationsho with Keith Urban




</doc>
<doc id="21505" url="https://en.wikipedia.org/wiki?curid=21505" title="Nucleotide">
Nucleotide

Nucleotides are organic molecules consisting of a nucleoside and a phosphate. They serve as monomeric units of the nucleic acid polymers deoxyribonucleic acid (DNA) and ribonucleic acid (RNA), both of which are essential biomolecules within all life-forms on Earth. 

Nucleotides are composed of three subunit molecules: a nitrogenous base (also known as nucleobase), a five-carbon sugar (ribose or deoxyribose), and a phosphate group consisting of one to three phosphates. The four nitrogenous bases in DNA are guanine, adenine, cytosine and thymine; in RNA, uracil is used in place of thymine.

Nucleotides also play a central role in metabolism at a fundamental, cellular level. They provide chemical energy—in the form of the nucleoside triphosphates, adenosine triphosphate (ATP), guanosine triphosphate (GTP), cytidine triphosphate (CTP) and uridine triphosphate (UTP)—throughout the cell for the many cellular functions that demand energy, including: amino acid, protein and cell membrane synthesis, moving the cell and cell parts (both internally and intercellularly), cell division, etc. In addition, nucleotides participate in cell signaling (cyclic guanosine monophosphate or cGMP and cyclic adenosine monophosphate or cAMP), and are incorporated into important cofactors of enzymatic reactions (e.g. coenzyme A, FAD, FMN, NAD, and NADP).

In experimental biochemistry, nucleotides can be radiolabeled using radionuclides to yield radionucleotides.

A nucleotide is composed of three distinctive chemical sub-units: a five-carbon sugar molecule, a nitrogenous base—which two together are called a nucleoside—and one phosphate group. With all three joined, a nucleotide is also termed a "nucleoside "mono"phosphate", "nucleoside "di"phosphate" or "nucleoside "tri"phosphate", depending on how many phosphates make up the phosphate group.

In nucleic acids, nucleotides contain either a purine or a pyrimidine base—i.e., the nitrogenous base molecule, also known as a nucleobase—and are termed "ribo"nucleotides if the sugar is ribose, or "deoxyribo"nucleotides if the sugar is deoxyribose. Individual phosphate molecules repetitively connect the sugar-ring molecules in two adjacent nucleotide monomers, thereby connecting the nucleotide monomers of a nucleic acid end-to-end into a long chain. These chain-joins of sugar and phosphate molecules create a 'backbone' strand for a single- or double helix. In any one strand, the chemical orientation (directionality) of the chain-joins runs from the 5'-end to the 3'-end ("read": 5 prime-end to 3 prime-end)—referring to the five carbon sites on sugar molecules in adjacent nucleotides. In a double helix, the two strands are oriented in opposite directions, which permits base pairing and complementarity between the base-pairs, all which is essential for replicating or transcribing the encoded information found in DNA.

Nucleic acids then are polymeric macromolecules assembled from nucleotides, the monomer-units of nucleic acids. The purine bases adenine and guanine and pyrimidine base cytosine occur in both DNA and RNA, while the pyrimidine bases thymine (in DNA) and uracil (in RNA) occur in just one. Adenine forms a base pair with thymine with two hydrogen bonds, while guanine pairs with cytosine with three hydrogen bonds.

In addition to being building blocks for construction of nucleic acid polymers, singular nucleotides play roles in cellular energy storage and provision, cellular signaling, as a source of phosphate groups used to modulate the activity of proteins and other signaling molecules, and as enzymatic cofactors, often carrying out redox reactions. Signaling cyclic nucleotides are formed by binding the phosphate group twice to the same sugar molecule, bridging the 5'- and 3'- hydroxyl groups of the sugar. Some signaling nucleotides differ from the standard single-phosphate group configuration, in having multiple phosphate groups attached to different positions on the sugar. Nucleotide cofactors include a wider range of chemical groups attached to the sugar via the glycosidic bond, including nicotinamide and flavin, and in the latter case, the ribose sugar is linear rather than forming the ring seen in other nucleotides.

Nucleotides can be synthesized by a variety of means both in vitro and in vivo.

In vitro, protecting groups may be used during laboratory production of nucleotides. A purified nucleoside is protected to create a phosphoramidite, which can then be used to obtain analogues not found in nature and/or to synthesize an oligonucleotide.

In vivo, nucleotides can be synthesized de novo or recycled through salvage pathways. The components used in de novo nucleotide synthesis are derived from biosynthetic precursors of carbohydrate and amino acid metabolism, and from ammonia and carbon dioxide. The liver is the major organ of de novo synthesis of all four nucleotides. De novo synthesis of pyrimidines and purines follows two different pathways. Pyrimidines are synthesized first from aspartate and carbamoyl-phosphate in the cytoplasm to the common precursor ring structure orotic acid, onto which a phosphorylated ribosyl unit is covalently linked. Purines, however, are first synthesized from the sugar template onto which the ring synthesis occurs. For reference, the syntheses of the purine and pyrimidine nucleotides are carried out by several enzymes in the cytoplasm of the cell, not within a specific organelle. Nucleotides undergo breakdown such that useful parts can be reused in synthesis reactions to create new nucleotides.

The synthesis of the pyrimidines CTP and UTP occurs in the cytoplasm and starts with the formation of carbamoyl phosphate from glutamine and CO. Next, aspartate carbamoyltransferase catalyzes a condensation reaction between aspartate and carbamoyl phosphate to form carbamoyl aspartic acid, which is cyclized into 4,5-dihydroorotic acid by dihydroorotase. The latter is converted to orotate by dihydroorotate oxidase. The net reaction is:

Orotate is covalently linked with a phosphorylated ribosyl unit. The covalent linkage between the ribose and pyrimidine occurs at position C of the ribose unit, which contains a pyrophosphate, and N of the pyrimidine ring. Orotate phosphoribosyltransferase (PRPP transferase) catalyzes the net reaction yielding orotidine monophosphate (OMP):

Orotidine 5'-monophosphate is decarboxylated by orotidine-5'-phosphate decarboxylase to form uridine monophosphate (UMP). PRPP transferase catalyzes both the ribosylation and decarboxylation reactions, forming UMP from orotic acid in the presence of PRPP. It is from UMP that other pyrimidine nucleotides are derived. UMP is phosphorylated by two kinases to uridine triphosphate (UTP) via two sequential reactions with ATP. First the diphosphate form UDP is produced, which in turn is phosphorylated to UTP. Both steps are fueled by ATP hydrolysis:

CTP is subsequently formed by amination of UTP by the catalytic activity of CTP synthetase. Glutamine is the NH donor and the reaction is fueled by ATP hydrolysis, too:

Cytidine monophosphate (CMP) is derived from cytidine triphosphate (CTP) with subsequent loss of two phosphates.

The atoms that are used to build the purine nucleotides come from a variety of sources: 

The de novo synthesis of purine nucleotides by which these precursors are incorporated into the purine ring proceeds by a 10-step pathway to the branch-point intermediate IMP, the nucleotide of the base hypoxanthine. AMP and GMP are subsequently synthesized from this intermediate via separate, two-step pathways. Thus, purine moieties are initially formed as part of the ribonucleotides rather than as free bases.

Six enzymes take part in IMP synthesis. Three of them are multifunctional:

The pathway starts with the formation of PRPP. PRPS1 is the enzyme that activates R5P, which is formed primarily by the pentose phosphate pathway, to PRPP by reacting it with ATP. The reaction is unusual in that a pyrophosphoryl group is directly transferred from ATP to C of R5P and that the product has the α configuration about C1. This reaction is also shared with the pathways for the synthesis of Trp, His, and the pyrimidine nucleotides. Being on a major metabolic crossroad and requiring much energy, this reaction is highly regulated.

In the first reaction unique to purine nucleotide biosynthesis, PPAT catalyzes the displacement of PRPP's pyrophosphate group (PP) by an amide nitrogen donated from either glutamine (N), glycine (N&C), aspartate (N), folic acid (C), or CO. This is the committed step in purine synthesis. The reaction occurs with the inversion of configuration about ribose C, thereby forming β-5-phosphorybosylamine (5-PRA) and establishing the anomeric form of the future nucleotide.

Next, a glycine is incorporated fueled by ATP hydrolysis and the carboxyl group forms an amine bond to the NH previously introduced. A one-carbon unit from folic acid coenzyme N-formyl-THF is then added to the amino group of the substituted glycine followed by the closure of the imidazole ring. Next, a second NH group is transferred from a glutamine to the first carbon of the glycine unit. A carboxylation of the second carbon of the glycin unit is concomitantly added. This new carbon is modified by the additional of a third NH unit, this time transferred from an aspartate residue. Finally, a second one-carbon unit from formyl-THF is added to the nitrogen group and the ring covalently closed to form the common purine precursor inosine monophosphate (IMP).

Inosine monophosphate is converted to adenosine monophosphate in two steps. First, GTP hydrolysis fuels the addition of aspartate to IMP by adenylosuccinate synthase, substituting the carbonyl oxygen for a nitrogen and forming the intermediate adenylosuccinate. Fumarate is then cleaved off forming adenosine monophosphate. This step is catalyzed by adenylosuccinate lyase.

Inosine monophosphate is converted to guanosine monophosphate by the oxidation of IMP forming xanthylate, followed by the insertion of an amino group at C. NAD is the electron acceptor in the oxidation reaction. The amide group transfer from glutamine is fueled by ATP hydrolysis.

In humans, pyrimidine rings (C, T, U) can be degraded completely to CO and NH (urea excretion). That having been said, purine rings (G, A) cannot. Instead they are degraded to the metabolically inert uric acid which is then excreted from the body. Uric acid is formed when GMP is split into the base guanine and ribose. Guanine is deaminated to xanthine which in turn is oxidized to uric acid. This last reaction is irreversible. Similarly, uric acid can be formed when AMP is deaminated to IMP from which the ribose unit is removed to form hypoxanthine. Hypoxanthine is oxidized to xanthine and finally to uric acid. Instead of uric acid secretion, guanine and IMP can be used for recycling purposes and nucleic acid synthesis in the presence of PRPP and aspartate (NH donor).

An unnatural base pair (UBP) is a designed subunit (or nucleobase) of DNA which is created in a laboratory and does not occur in nature. In 2012, a group of American scientists led by Floyd Romesberg, a chemical biologist at the Scripps Research Institute in San Diego, California, published that his team designed an unnatural base pair (UBP). The two new artificial nucleotides or "Unnatural Base Pair" (UBP) were named d5SICS and dNaM. More technically, these artificial nucleotides bearing hydrophobic nucleobases, feature two fused aromatic rings that form a (d5SICS–dNaM) complex or base pair in DNA. In 2014 the same team from the Scripps Research Institute reported that they synthesized a stretch of circular DNA known as a plasmid containing natural T-A and C-G base pairs along with the best-performing UBP Romesberg's laboratory had designed, and inserted it into cells of the common bacterium "E. coli" that successfully replicated the unnatural base pairs through multiple generations. This is the first known example of a living organism passing along an expanded genetic code to subsequent generations. This was in part achieved by the addition of a supportive algal gene that expresses a nucleotide triphosphate transporter which efficiently imports the triphosphates of both d5SICSTP and dNaMTP into "E. coli" bacteria. Then, the natural bacterial replication pathways use them to accurately replicate the plasmid containing d5SICS–dNaM.

The successful incorporation of a third base pair is a significant breakthrough toward the goal of greatly expanding the number of amino acids which can be encoded by DNA, from the existing 21 amino acids to a theoretically possible 172, thereby expanding the potential for living organisms to produce novel proteins. The artificial strings of DNA do not encode for anything yet, but scientists speculate they could be designed to manufacture new proteins which could have industrial or pharmaceutical uses.

Nucleotide (abbreviated "nt") is a common unit of length for single-stranded nucleic acids, similar to how base pair is a unit of length for double-stranded nucleic acids.

A study done by the Department of Sports Science at the University of Hull in Hull, UK has shown that nucleotides have significant impact on cortisol levels in saliva. Post exercise, the experimental nucleotide group had lower cortisol levels in their blood than the control or the placebo. Additionally, post supplement values of Immunoglobulin A were significantly higher than either the placebo or the control. The study concluded, "nucleotide supplementation blunts the response of the hormones associated with physiological stress."

Another study conducted in 2013 looked at the impact nucleotide supplementation had on the immune system in athletes. In the study, all athletes were male and were highly skilled in taekwondo. Out of the twenty athletes tested, half received a placebo and half received 480 mg per day of nucleotide supplement. After thirty days, the study concluded that nucleotide supplementation may counteract the impairment of the body's immune function after heavy exercise.

The IUPAC has designated the symbols for nucleotides. Apart from the five (A, G, C, T/U) bases, often degenerate bases are used especially for designing PCR primers. These nucleotide codes are listed here. Some primer sequences may also include the character "I", which codes for the non-standard nucleotide inosine. Inosine occurs in tRNAs, and will pair with adenine, cytosine, or thymine. This character does not appear in the following table however, because it does not represent a degeneracy. While inosine can serve a similar function as the degeneracy "D", it is an actual nucleotide, rather than a representation of a mix of nucleotides that covers each possible pairing needed.




</doc>
<doc id="21506" url="https://en.wikipedia.org/wiki?curid=21506" title="Numerical analysis">
Numerical analysis

Numerical analysis is the study of algorithms that use numerical approximation (as opposed to symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics). Numerical analysis naturally finds application in all fields of engineering and the physical sciences, but in the 21st century also the life sciences, social sciences, medicine, business and even the arts have adopted elements of scientific computations. The growth in computing power has revolutionized the use of realistic mathematical models in science and engineering, and subtle numerical analysis is required to implement these detailed models of the world. For example, ordinary differential equations appear in celestial mechanics (predicting the motions of planets, stars and galaxies); numerical linear algebra is important for data analysis; stochastic differential equations and Markov chains are essential in simulating living cells for medicine and biology.

Before the advent of modern computers, numerical methods often depended on hand interpolation formulas applied to data from large printed tables. Since the mid 20th century, computers calculate the required functions instead, but many of the same formulas nevertheless continue to be used as part of the software algorithms.

The numerical point of view goes back to the earliest mathematical writings. A tablet from the Yale Babylonian Collection (YBC 7289), gives a sexagesimal numerical approximation of the square root of 2, the length of the diagonal in a unit square.

Numerical analysis continues this long tradition: rather than exact symbolic answers, which can only be applied to real-world measurements by translation into digits, it gives approximate solutions within specified error bounds.

The overall goal of the field of numerical analysis is the design and analysis of techniques to give approximate but accurate solutions to hard problems, the variety of which is suggested by the following:


The rest of this section outlines several important themes of numerical analysis.

The field of numerical analysis predates the invention of modern computers by many centuries. Linear interpolation was already in use more than 2000 years ago. Many great mathematicians of the past were preoccupied by numerical analysis, as is obvious from the names of important algorithms like Newton's method, Lagrange interpolation polynomial, Gaussian elimination, or Euler's method.

To facilitate computations by hand, large books were produced with formulas and tables of data such as interpolation points and function coefficients. Using these tables, often calculated out to 16 decimal places or more for some functions, one could look up values to plug into the formulas given and achieve very good numerical estimates of some functions. The canonical work in the field is the NIST publication edited by Abramowitz and Stegun, a 1000-plus page book of a very large number of commonly used formulas and functions and their values at many points. The function values are no longer very useful when a computer is available, but the large listing of formulas can still be very handy.

The mechanical calculator was also developed as a tool for hand computation. These calculators evolved into electronic computers in the 1940s, and it was then found that these computers were also useful for administrative purposes. But the invention of the computer also influenced the field of numerical analysis, since now longer and more complicated calculations could be done.

Consider the problem of solving

for the unknown quantity "x".

For the iterative method, apply the bisection method to "f"("x") = 3"x" − 24. The initial values are "a" = 0, "b" = 3, "f"("a") = −24, "f"("b") = 57.

From this table it can be concluded that the solution is between 1.875 and 2.0625. The algorithm might return any number in that range with an error less than 0.2.

In a two-hour race, the speed of the car is measured at three instants and recorded in the following table.

A discretization would be to say that the speed of the car was constant from 0:00 to 0:40, then from 0:40 to 1:20 and finally from 1:20 to 2:00. For instance, the total distance traveled in the first 40 minutes is approximately . This would allow us to estimate the total distance traveled as + + = , which is an example of numerical integration (see below) using a Riemann sum, because displacement is the integral of velocity.

Ill-conditioned problem: Take the function . Note that "f"(1.1) = 10 and "f"(1.001) = 1000: a change in "x" of less than 0.1 turns into a change in "f"("x") of nearly 1000. Evaluating "f"("x") near "x" = 1 is an ill-conditioned problem.

Well-conditioned problem: By contrast, evaluating the same function near "x" = 10 is a well-conditioned problem. For instance, "f"(10) = 1/9 ≈ 0.111 and "f"(11) = 0.1: a modest change in "x" leads to a modest change in "f"("x").

Direct methods compute the solution to a problem in a finite number of steps. These methods would give the precise answer if they were performed in infinite precision arithmetic. Examples include Gaussian elimination, the QR factorization method for solving systems of linear equations, and the simplex method of linear programming. In practice, finite precision is used and the result is an approximation of the true solution (assuming stability).

In contrast to direct methods, iterative methods are not expected to terminate in a finite number of steps. Starting from an initial guess, iterative methods form successive approximations that converge to the exact solution only in the limit. A convergence test, often involving the residual, is specified in order to decide when a sufficiently accurate solution has (hopefully) been found. Even using infinite precision arithmetic these methods would not reach the solution within a finite number of steps (in general). Examples include Newton's method, the bisection method, and Jacobi iteration. In computational matrix algebra, iterative methods are generally needed for large problems.

Iterative methods are more common than direct methods in numerical analysis. Some methods are direct in principle but are usually used as though they were not, e.g. GMRES and the conjugate gradient method. For these methods the number of steps needed to obtain the exact solution is so large that an approximation is accepted in the same manner as for an iterative method.

Furthermore, continuous problems must sometimes be replaced by a discrete problem whose solution is known to approximate that of the continuous problem; this process is called 'discretization'. For example, the solution of a differential equation is a function. This function must be represented by a finite amount of data, for instance by its value at a finite number of points at its domain, even though this domain is a continuum.

The study of errors forms an important part of numerical analysis. There are several ways in which error can be introduced in the solution of the problem.

Round-off errors arise because it is impossible to represent all real numbers exactly on a machine with finite memory (which is what all practical digital computers are).

Truncation errors are committed when an iterative method is terminated or a mathematical procedure is approximated, and the approximate solution differs from the exact solution. Similarly, discretization induces a discretization error because the solution of the discrete problem does not coincide with the solution of the continuous problem. For instance, in the iteration in the sidebar to compute the solution of formula_1, after 10 or so iterations, it can be concluded that the root is roughly 1.99 (for example). Therefore, there is a truncation error of 0.01.

Once an error is generated, it will generally propagate through the calculation. For instance, already noted is that the operation + on a calculator (or a computer) is inexact. It follows that a calculation of the type is even more inexact.

The truncation error is created when a mathematical procedure is approximated. To integrate a function exactly it is required to find the sum of infinite trapezoids, but numerically only the sum of only finite trapezoids can be found, and hence the approximation of the mathematical procedure. Similarly, to differentiate a function, the differential element approaches zero but numerically only a finite value of the differential element can be chosen.

Numerical stability is a notion in numerical analysis. An algorithm is called 'numerically stable' if an error, whatever its cause, does not grow to be much larger during the calculation. This happens if the problem is 'well-conditioned', meaning that the solution changes by only a small amount if the problem data are changed by a small amount. To the contrary, if a problem is 'ill-conditioned', then any small error in the data will grow to be a large error.

Both the original problem and the algorithm used to solve that problem can be 'well-conditioned' or 'ill-conditioned', and any combination is possible.

So an algorithm that solves a well-conditioned problem may be either numerically stable or numerically unstable. An art of numerical analysis is to find a stable algorithm for solving a well-posed mathematical problem. For instance, computing the square root of 2 (which is roughly 1.41421) is a well-posed problem. Many algorithms solve this problem by starting with an initial approximation "x" to formula_2, for instance "x" = 1.4, and then computing improved guesses "x", "x", etc. One such method is the famous Babylonian method, which is given by "x" = "x"/2 + 1/"x". Another method, called 'method X', is given by "x" = ("x" − 2) + "x". A few iterations of each scheme are calculated in table form below, with initial guesses "x" = 1.4 and "x" = 1.42.

Observe that the Babylonian method converges quickly regardless of the initial guess, whereas Method X converges extremely slowly with initial guess "x" = 1.4 and diverges for initial guess "x" = 1.42. Hence, the Babylonian method is numerically stable, while Method X is numerically unstable.


The field of numerical analysis includes many sub-disciplines. Some of the major ones are:

One of the simplest problems is the evaluation of a function at a given point. The most straightforward approach, of just plugging in the number in the formula is sometimes not very efficient. For polynomials, a better approach is using the Horner scheme, since it reduces the necessary number of multiplications and additions. Generally, it is important to estimate and control round-off errors arising from the use of floating point arithmetic.

Interpolation solves the following problem: given the value of some unknown function at a number of points, what value does that function have at some other point between the given points?

Extrapolation is very similar to interpolation, except that now the value of the unknown function at a point which is outside the given points must be found.

Regression is also similar, but it takes into account that the data is imprecise. Given some points, and a measurement of the value of some function at these points (with an error), the unknown function can be found. The least squares-method is one way to achieve this.

Another fundamental problem is computing the solution of some given equation. Two cases are commonly distinguished, depending on whether the equation is linear or not. For instance, the equation formula_7 is linear while formula_8 is not.

Much effort has been put in the development of methods for solving systems of linear equations. Standard direct methods, i.e., methods that use some matrix decomposition are Gaussian elimination, LU decomposition, Cholesky decomposition for symmetric (or hermitian) and positive-definite matrix, and QR decomposition for non-square matrices. Iterative methods such as the Jacobi method, Gauss–Seidel method, successive over-relaxation and conjugate gradient method are usually preferred for large systems. General iterative methods can be developed using a matrix splitting.

Root-finding algorithms are used to solve nonlinear equations (they are so named since a root of a function is an argument for which the function yields zero). If the function is differentiable and the derivative is known, then Newton's method is a popular choice. Linearization is another technique for solving nonlinear equations.

Several important problems can be phrased in terms of eigenvalue decompositions or singular value decompositions. For instance, the spectral image compression algorithm is based on the singular value decomposition. The corresponding tool in statistics is called principal component analysis.

Optimization problems ask for the point at which a given function is maximized (or minimized). Often, the point also has to satisfy some constraints.

The field of optimization is further split in several subfields, depending on the form of the objective function and the constraint. For instance, linear programming deals with the case that both the objective function and the constraints are linear. A famous method in linear programming is the simplex method.

The method of Lagrange multipliers can be used to reduce optimization problems with constraints to unconstrained optimization problems.

Numerical integration, in some instances also known as numerical quadrature, asks for the value of a definite integral. Popular methods use one of the Newton–Cotes formulas (like the midpoint rule or Simpson's rule) or Gaussian quadrature. These methods rely on a "divide and conquer" strategy, whereby an integral on a relatively large set is broken down into integrals on smaller sets. In higher dimensions, where these methods become prohibitively expensive in terms of computational effort, one may use Monte Carlo or quasi-Monte Carlo methods (see Monte Carlo integration), or, in modestly large dimensions, the method of sparse grids.

Numerical analysis is also concerned with computing (in an approximate way) the solution of differential equations, both ordinary differential equations and partial differential equations.

Partial differential equations are solved by first discretizing the equation, bringing it into a finite-dimensional subspace. This can be done by a finite element method, a finite difference method, or (particularly in engineering) a finite volume method. The theoretical justification of these methods often involves theorems from functional analysis. This reduces the problem to the solution of an algebraic equation.

Since the late twentieth century, most algorithms are implemented in a variety of programming languages. The Netlib repository contains various collections of software routines for numerical problems, mostly in Fortran and C. Commercial products implementing many different numerical algorithms include the IMSL and NAG libraries; a free-software alternative is the GNU Scientific Library.

There are several popular numerical computing applications such as MATLAB, TK Solver, S-PLUS, and IDL as well as free and open source alternatives such as FreeMat, Scilab, GNU Octave (similar to Matlab), and IT++ (a C++ library). There are also programming languages such as R (similar to S-PLUS) and Python with libraries such as NumPy, SciPy and SymPy. Performance varies widely: while vector and matrix operations are usually fast, scalar loops may vary in speed by more than an order of magnitude.

Many computer algebra systems such as Mathematica also benefit from the availability of arbitrary-precision arithmetic which can provide more accurate results.

Also, any spreadsheet software can be used to solve simple problems relating to numerical analysis.






</doc>
<doc id="21508" url="https://en.wikipedia.org/wiki?curid=21508" title="Noosphere">
Noosphere

The noosphere is a philosophical concept developed and popularized by the biogeochemist Vladimir Vernadsky, and the French philosopher and Jesuit priest Pierre Teilhard de Chardin. Vernadsky defined the noosphere as the new state of the biosphere and described as the planetary "sphere of reason". The noosphere represents the highest stage of biospheric development, its defining factor being the development of humankind's rational activities.

The word is derived from the Greek νόος ("mind", "reason") and σφαῖρα (""), in lexical analogy to "atmosphere" and "biosphere". The concept, however, cannot be accredited to a single author. The founding authors Vladimir Ivanovich Vernadsky and Pierre Teilhard de Chardin developed two related but starkly different concepts, the former being grounded in the geological sciences, and the latter in theology. Both conceptions of the noosphere share the common thesis that together human reason and the scientific thought has created, and will continue to create, the next evolutionary geological layer. This geological layer is part of the evolutionary chain. Second generation authors, predominantly of Russian origin, have further developed the Vernadskian concept, creating the related concepts: noocenosis and noocenology.

The term noosphere was first used in the publications of Pierre Teilhard de Chardin in 1922 in his "Cosmogenesis". Vernadsky was most likely introduced to the term by a common acquaintance, Édouard Le Roy, during a stay in Paris. Some sources claim Édouard Le Roy actually first proposed the term. Vernadsky himself wrote that he was first introduced to the concept by Le Roy in his 1927 lectures at the College of France, and that Le Roy had emphasized a mutual exploration of the concept with Teilhard de Chardin. According to Vernadsky's own letters, he took Le Roy’s ideas on the noosphere from Le Roys article "Les origines humaines et l’evolution de l’intelligence", part III: "La noosphere et l’hominisation", before reworking the concept within his own field, biogeochemistry. The historian Bailes concludes that Vernadsky and Teilhard de Chardin were mutual influences on each other, as Teilhard de Chardin also attended the Vernadsky's lectures on biogeochemistry, before creating the concept of the noosphere.

An account stated that Le Roy and Teilhard was not aware of the concept of biosphere in their noosphere concept and that it was Vernadsky who introduced them to this notion, which gave their conceptualization a grounding on natural sciences. Both Teilhard de Chardin and Vernadsky base their conceptions of the noosphere on the term 'biosphere', developed by Edward Suess in 1875. Despite the differing backgrounds, approaches and focuses of Teilhard and Vernadsky, they have a few fundamental themes in common. Both scientists overstepped the boundaries of natural science and attempted to create all-embracing theoretical constructions founded in philosophy, social sciences and authorized interpretations of the evolutionary theory. Moreover, both thinkers were convinced of the teleological character of evolution. They also argued that human activity becomes a geological power and that the manner by which it is directed can influence the environment. There are, however, fundamental differences in the two conceptions.

In the theory of Vernadsky, the noosphere is the third in a succession of phases of development of the Earth, after the geosphere (inanimate matter) and the biosphere (biological life). Just as the emergence of life fundamentally transformed the geosphere, the emergence of human cognition fundamentally transforms the biosphere. In contrast to the conceptions of the Gaia theorists, or the promoters of cyberspace, Vernadsky's noosphere emerges at the point where humankind, through the mastery of nuclear processes, begins to create resources through the transmutation of elements. It is also currently being researched as part of the Global Consciousness Project.

Teilhard perceived a directionality in evolution along an axis of increasing "Complexity/Consciousness". For Teilhard, the noosphere is the sphere of thought encircling the earth that has emerged through evolution as a consequence of this growth in complexity / consciousness. The noosphere is therefore as much part of nature as the barysphere, lithosphere, hydrosphere, atmosphere, and biosphere. As a result, Teilhard sees the "social phenomenon [as] the culmination of and not the attenuation of the biological phenomenon." These social phenomena are part of the noosphere and include, for example, legal, educational, religious, research, industrial and technological systems. In this sense, the noosphere emerges through and is constituted by the interaction of human minds. The noosphere thus grows in step with the organization of the human mass in relation to itself as it populates the earth. Teilhard argued the noosphere evolves towards ever greater personalisation, individuation and unification of its elements. He saw the Christian notion of love as being the principal driver of noogenesis. Evolution would culminate in the Omega Point—an apex of thought/consciousness—which he identified with the eschatological return of Christ.

One of the original aspects of the noosphere concept deals with evolution. Henri Bergson, with his "L'évolution créatrice" (1907), was one of the first to propose evolution is "creative" and cannot necessarily be explained solely by Darwinian natural selection. "L'évolution créatrice" is upheld, according to Bergson, by a constant vital force which animates life and fundamentally connects mind and body, an idea opposing the dualism of René Descartes. In 1923, C. Lloyd Morgan took this work further, elaborating on an "emergent evolution" which could explain increasing complexity (including the evolution of mind). Morgan found many of the most interesting changes in living things have been largely discontinuous with past evolution. Therefore, these living things did not necessarily evolve through a gradual process of natural selection. Rather, he posited, the process of evolution experiences jumps in complexity (such as the emergence of a self-reflective universe, or noosphere), in a sort of qualitative punctuated equilibrium. Finally, the complexification of human cultures, particularly language, facilitated a quickening of evolution in which cultural evolution occurs more rapidly than biological evolution. Recent understanding of human ecosystems and of human impact on the biosphere have led to a link between the notion of sustainability with the "co-evolution" and harmonization of cultural and biological evolution.





</doc>
<doc id="21511" url="https://en.wikipedia.org/wiki?curid=21511" title="Niccolò Paganini">
Niccolò Paganini

Niccolò (or Nicolò) Paganini (; 27 October 178227 May 1840) was an Italian violinist, violist, guitarist, and composer. He was the most celebrated violin virtuoso of his time, and left his mark as one of the pillars of modern violin technique. His 24 Caprices for Solo Violin Op. 1 are among the best known of his compositions, and have served as an inspiration for many prominent composers.
Niccolò Paganini was born in Genoa, then capital of the Republic of Genoa, the third of the six children of Antonio and Teresa (née Bocciardo) Paganini. Paganini's father was an unsuccessful trader, but he managed to supplement his income by playing music on the mandolin. At the age of five, Paganini started learning the mandolin from his father and moved to the violin by the age of seven. His musical talents were quickly recognized, earning him numerous scholarships for violin lessons. The young Paganini studied under various local violinists, including Giovanni Servetto and Giacomo Costa, but his progress quickly outpaced their abilities. Paganini and his father then traveled to Parma to seek further guidance from Alessandro Rolla. But upon listening to Paganini's playing, Rolla immediately referred him to his own teacher, Ferdinando Paer and, later, Paer's own teacher, Gasparo Ghiretti. Though Paganini did not stay long with Paer or Ghiretti, the two had considerable influence on his composition style.

The French invaded northern Italy in March 1796, and Genoa was not spared. The Paganinis sought refuge in their country property in Romairone, near Bolzaneto. It was in this period that Paganini is thought to have developed his relationship with the guitar. He mastered the guitar, but preferred to play it in exclusively intimate, rather than public concerts. He later described the guitar as his "constant companion" on his concert tours. By 1800, Paganini and his father traveled to Livorno, where Paganini played in concerts and his father resumed his maritime work. In 1801, the 18-year-old Paganini was appointed first violin of the Republic of Lucca, but a substantial portion of his income came from freelancing. His fame as a violinist was matched only by his reputation as a gambler and womanizer.

In 1805, Lucca was annexed by Napoleonic France, and the region was ceded to Napoleon's sister, Elisa Baciocchi. Paganini became a violinist for the Baciocchi court, while giving private lessons to Elisa's husband, Felice. In 1807, Baciocchi became the Grand Duchess of Tuscany and her court was transferred to Florence. Paganini was part of the entourage, but, towards the end of 1809, he left Baciocchi to resume his freelance career.

For the next few years, Paganini returned to touring in the areas surrounding Parma and Genoa. Though he was very popular with the local audience, he was still not very well known in the rest of Europe. His first break came from an 1813 concert at La Scala in Milan. The concert was a great success. As a result, Paganini began to attract the attention of other prominent, though more conservative, musicians across Europe. His early encounters with Charles Philippe Lafont and Louis Spohr created intense rivalry. His concert activities, however, were still limited to Italy for the next few years.

In 1827, Pope Leo XII honoured Paganini with the Order of the Golden Spur. His fame spread across Europe with a concert tour that started in Vienna in August 1828, stopping in every major European city in Germany, Poland, and Bohemia until February 1831 in Strasbourg. This was followed by tours in Paris and Britain. His technical ability and his willingness to display it received much critical acclaim. In addition to his own compositions, theme and variations being the most popular, Paganini also performed modified versions of works (primarily concertos) written by his early contemporaries, such as Rodolphe Kreutzer and Giovanni Battista Viotti.

Paganini's travels also brought him into contact with eminent guitar virtuosi of the day, including Ferdinando Carulli in Paris and Mauro Giuliani in Vienna. But this experience did not inspire him to play public concerts with guitar, and even performances of his own guitar trios and quartets were private to the point of being behind closed doors.

Throughout his life, Paganini was no stranger to chronic illnesses. Although no definite medical proof exists, he was reputed to have been affected by Marfan syndrome or Ehlers–Danlos syndrome. In addition, his frequent concert schedule, as well as his extravagant lifestyle, took their toll on his health. He was diagnosed with syphilis as early as 1822, and his remedy, which included mercury and opium, came with serious physical and psychological side effects. In 1834, while still in Paris, he was treated for tuberculosis. Though his recovery was reasonably quick, after the illness his career was marred by frequent cancellations due to various health problems, from the common cold to depression, which lasted from days to months.

In September 1834, Paganini put an end to his concert career and returned to Genoa. Contrary to popular beliefs involving his wishing to keep his music and techniques secret, Paganini devoted his time to the publication of his compositions and violin methods. He accepted students, of whom two enjoyed moderate success: violinist Camillo Sivori and cellist Gaetano Ciandelli. Neither, however, considered Paganini helpful or inspirational. In 1835, Paganini returned to Parma, this time under the employ of Archduchess Marie Louise of Austria, Napoleon's second wife. He was in charge of reorganizing her court orchestra. However, he eventually conflicted with the players and court, so his visions never saw completion. In Paris, he befriended the 11-year-old Polish virtuoso Apollinaire de Kontski, giving him some lessons and a signed testimonial. It was widely put about, falsely, that Paganini was so impressed with de Kontski's skills that he bequeathed him his violins and manuscripts.

In 1836, Paganini returned to Paris to set up a casino. Its immediate failure left him in financial ruin, and he auctioned off his personal effects, including his musical instruments, to recoup his losses. At Christmas of 1838, he left Paris for Marseilles and, after a brief stay, travelled to Nice where his condition worsened. In May 1840, the Bishop of Nice sent Paganini a local parish priest to perform the last rites. Paganini assumed the sacrament was premature, and refused.

A week later, on 27 May 1840, Paganini died from internal hemorrhaging before a priest could be summoned. Because of this, and his widely rumored association with the devil, the Church denied his body a Catholic burial in Genoa. It took four years and an appeal to the Pope before the Church let his body be transported to Genoa, but it was still not buried. His body was finally buried in 1876, in a cemetery in Parma. In 1893, the Czech violinist František Ondříček persuaded Paganini's grandson, Attila, to allow a viewing of the violinist's body. After this episode, Paganini's body was finally reinterred in a new cemetery in Parma in 1896.

Though having no shortage of romantic conquests, Paganini was seriously involved with a singer named Antonia Bianchi from Como, whom he met in Milan in 1813. The two gave concerts together throughout Italy. They had a son, Achille Ciro Alessandro, born on 23 July 1825 in Palermo and baptized at San Bartolomeo's. They never legalized their union and it ended around April 1828 in Vienna. Paganini brought Achille on his European tours, and Achille later accompanied his father until the latter's death. He was instrumental in dealing with his father's burial, years after his death.

Throughout his career, Paganini also became close friends with composers Gioachino Rossini and Hector Berlioz. Rossini and Paganini met in Bologna in the summer of 1818. In January 1821, on his return from Naples, Paganini met Rossini again in Rome, just in time to become the substitute conductor for Rossini's opera "Matilde di Shabran", upon the sudden death of the original conductor. Paganini's efforts earned gratitude from Rossini.

Paganini met Berlioz in Paris, and was a frequent correspondent as a penfriend. He commissioned a piece from the composer, but was not satisfied with the resultant four-movement piece for orchestra and viola obbligato, "Harold en Italie". He never performed it, and instead it was premiered a year later by violist Christian Urhan. He did however write his own "Sonata per Gran Viola" Op. 35 (with orchestra or guitar accompaniment). Despite his alleged lack of interest in "Harold", Paganini often referred to Berlioz as the resurrection of Beethoven and, towards the end of his life, he gave large sums to the composer. They shared an active interest in the guitar, which they both played and used in compositions. Paganini gave Berlioz a guitar, which they both signed on its sound box.

Paganini was in possession of a number of fine stringed instruments. More legendary than these were the circumstances under which he obtained (and lost) some of them. While Paganini was still a teenager in Livorno, a wealthy businessman named Livron lent him a violin, made by the master luthier Giuseppe Guarneri, for a concert. Livron was so impressed with Paganini's playing that he refused to take it back. This particular violin came to be known as "Il Cannone Guarnerius". On a later occasion in Parma, he won another valuable violin (also by Guarneri) after a difficult sight-reading challenge from a man named Pasini.
Other instruments associated with Paganini include the "Antonio Amati" 1600, the "Nicolò Amati" 1657, the "Paganini-Desaint" 1680 Stradivari, the Guarneri-filius "Andrea" 1706, the "Le Brun" 1712 Stradivari, the "Vuillaume" c. 1720 Bergonzi, the "Hubay" 1726 Stradivari, and the "Comte Cozio di Salabue" 1727 violins; the "Countess of Flanders" 1582 da Salò-di Bertolotti, and the "Mendelssohn" 1731 Stradivari violas; the "Piatti" 1700 Goffriller, the "Stanlein" 1707 Stradivari, and the "Ladenburg" 1736 Stradivari cellos; and the "Grobert of Mirecourt" 1820 (guitar). Four of these instruments were played by the Tokyo String Quartet.

Of his guitars, there is little evidence remaining of his various choices of instrument. The aforementioned guitar that he gave to Berlioz is a French instrument made by one Grobert of Mirecourt. The luthier made his instrument in the style of René Lacôte, a more well-known Paris-based guitar-maker. It is preserved and on display in the Musée de la Musique in Paris.

Of the guitars he owned through his life, there was an instrument by Gennaro Fabricatore that he had refused to sell even in his periods of financial stress, and was among the instruments in his possession at the time of his death. There is an unsubstantiated rumour that he also played Stauffer guitars; he may certainly have come across these in his meetings with Giuliani in Vienna.

Paganini composed his own works to play exclusively in his concerts, all of which profoundly influenced the evolution of violin technique. His 24 Caprices were likely composed in the period between 1805 and 1809, while he was in the service of the Baciocchi court. Also during this period, he composed the majority of the solo pieces, duo-sonatas, trios and quartets for the guitar, either as a solo instrument or with strings. These chamber works may have been inspired by the publication, in Lucca, of the guitar quintets of Boccherini. Many of his variations, including "Le Streghe", "The Carnival of Venice", and "Nel cor più non mi sento", were composed, or at least first performed, before his European concert tour.

Generally speaking, Paganini's compositions were technically imaginative, and the timbre of the instrument was greatly expanded as a result of these works. Sounds of different musical instruments and animals were often imitated. One such composition was titled "Il Fandango Spanolo" (The Spanish Dance), which featured a series of humorous imitations of farm animals. Even more outrageous was a solo piece "Duetto Amoroso", in which the sighs and groans of lovers were intimately depicted on the violin. There survives a manuscript of the "Duetto", which has been recorded. The existence of the "Fandango" is known only through concert posters.

However, his works were criticized for lacking characteristics of true polyphonism, as pointed out by Eugène Ysaÿe. Yehudi Menuhin, on the other hand, suggested that this might have been the result of his reliance on the guitar (in lieu of the piano) as an aid in composition. The orchestral parts for his concertos were often polite, unadventurous, and clearly supportive of the soloist. In this, his style is consistent with that of other Italian composers such as Giovanni Paisiello, Gioachino Rossini and Gaetano Donizetti, who were influenced by the guitar-song milieu of Naples during this period.

Paganini was also the inspiration of many prominent composers. Both "La Campanella" and the A minor Caprice (No. 24) have been an object of interest for a number of composers. Franz Liszt, Robert Schumann, Johannes Brahms, Sergei Rachmaninoff, Boris Blacher, Andrew Lloyd Webber, George Rochberg and Witold Lutosławski, among others, wrote well-known variations on these themes.

The Israeli violinist Ivry Gitlis once referred to Paganini as a phenomenon rather than a development. Though some of the techniques frequently employed by Paganini were already present, most accomplished violinists of the time focused on intonation and bowing techniques. Arcangelo Corelli (1653–1713) was considered a pioneer in transforming the violin from an ensemble instrument to a solo instrument. In the meantime, the polyphonic capability of the violin was firmly established through the Sonatas and Partitas BWV 1001–1006 of Johann Sebastian Bach (1685–1750). Other notable violinists included Antonio Vivaldi (1678–1741) and Giuseppe Tartini (1692–1770), who, in their compositions, reflected the increasing technical and musical demands on the violinist. Although the role of the violin in music drastically changed through this period, progress in violin technique was steady but slow. Techniques requiring agility of the fingers and the bow were still considered unorthodox and discouraged by the established community of violinists.

Much of Paganini's playing (and his violin composition) was influenced by two violinists, Pietro Locatelli (1693–1746) and August Duranowski (Auguste Frédéric Durand) (1770–1834). During Paganini's study in Parma, he came across the 24 Caprices of Locatelli (entitled "L'arte di nuova modulazione – Capricci enigmatici" or "The art of the new style – the enigmatic caprices"). Published in the 1730s, they were shunned by the musical authorities for their technical innovations, and were forgotten by the musical community at large. Around the same time, Durand, a former student of Giovanni Battista Viotti (1755–1824), became a celebrated violinist. He was renowned for his use of harmonics, both natural and artificial (which had previously not been attempted in performance), and the left hand pizzicato in his performance. Paganini was impressed by Durand's innovations and showmanship, which later also became the hallmarks of the young violin virtuoso. Paganini was instrumental in the revival and popularization of these violinistic techniques, which are now incorporated into regular compositions.

Another aspect of Paganini's violin techniques concerned his flexibility. He had exceptionally long fingers and was capable of playing three octaves across four strings in a hand span, an extraordinary feat even by today's standards. His seemingly unnatural ability may have been a result of Marfan syndrome.

Notable works inspired by compositions of Paganini include:
The "Caprice No. 24 in A minor", Op. 1, ("Tema con variazioni") has been the basis of works by many other composers. Notable examples include Brahms's "Variations on a Theme of Paganini" and Rachmaninoff's "Rhapsody on a Theme of Paganini".

The Paganini Competition ("Premio Paganini") is an international violin competition created in 1954 in his home city of Genoa and named in his honour.

In 1972 the State of Italy purchased a large collection of Niccolò Paganini manuscripts from the W. Heyer Library of Cologne. They are housed at the Biblioteca Casanatense in Rome.

In 1982 the city of Genoa commissioned a thematic catalogue of music by Paganini, edited by Maria Rosa Moretti and Anna Sorrento, hence the abbreviation "MS" assigned to his catalogued works.

A minor planet 2859 Paganini discovered in 1978 by Soviet astronomer Nikolai Chernykh is named after him.

Although no photographs of Paganini are known to exist, in 1900 Italian violin maker Giuseppe Fiorini forged the now famous fake daguerreotype of the celebrated violinist. So well in fact, that even the great classical author and conversationalist Arthur M. Abell was led to believe it to be true, reprinting the image in the 22 January 1901 issue of the "Musical Courier".

Paganini has been portrayed by a number of actors in film and television productions, including Stewart Granger in the 1946 biographical portrait "The Magic Bow", Roxy Roth in "A Song to Remember" (1945), Klaus Kinski in "Kinski Paganini" (1989) and David Garrett in "The Devil's Violinist" (2013).

In the Soviet 1982 miniseries "Niccolo Paganini", the musician was portrayed by the Armenian actor Vladimir Msryan. The series focuses on Paganini's relationship with the Roman Catholic Church. Another Soviet actor, Armen Dzhigarkhanyan, played Paganini's fictionalized arch-rival, an insidious Jesuit official. The information in the series is generally spurious, and it also plays to some of the myths and legends rampant during the musician's lifetime. One memorable scene shows Paganini's adversaries sabotaging his violin before a high-profile performance, causing all strings but one to break during the concert. An undeterred Paganini continues to perform on three, two, and finally on a single string. In actuality, Paganini himself occasionally broke strings during his performances on purpose so he could further display his virtuosity. He did this by carefully filing notches into them to weaken them, so that they would break when in use.

In Don Nigro's satirical comedy play "Paganini" (1995), the great violinist seeks vainly for his salvation, claiming that he unknowingly sold his soul to the Devil. "Variation upon variation," he cries at one point, "but which variation leads to salvation and which to damnation? Music is a question for which there is no answer." Paganini is portrayed as having killed three of his lovers and sinking repeatedly into poverty, prison, and drink. Each time he is "rescued" by the Devil, who appears in different guises, returning Paganini's violin so he can continue playing. In the end, Paganini's salvation—administered by a god-like Clockmaker—turns out to be imprisonment in a large bottle where he plays his music for the amusement of the public through all eternity. "Do not pity him, my dear," the Clockmaker tells Antonia, one of Paganini's murdered wives. "He is alone with the answer for which there is no question. The saved and the damned are the same."



Images


</doc>
<doc id="21512" url="https://en.wikipedia.org/wiki?curid=21512" title="North Atlantic Current">
North Atlantic Current

The North Atlantic Current (NAC), also known as North Atlantic Drift and North Atlantic Sea Movement, is a powerful warm western boundary current within the Atlantic Ocean that extends the Gulf Stream northeastward.

The NAC originates from where the Gulf Stream turns north at the Southeast Newfoundland Rise, a submarine ridge that stretches southeast from the Grand Banks of Newfoundland. The NAC flows northward east of the Grand Banks, from 40°N to 51°N, before turning sharply east to cross the Atlantic. It transports more warm tropical water to northern latitudes than any other boundary current; more than 40 Sv in the south and 20 Sv as it crosses the Mid-Atlantic Ridge. It reaches speeds of 2 knots near the North American coast. Directed by topography, the NAC meanders heavily, but in contrast to the meanders of the Gulf Stream, the NAC meanders remain stable without breaking off into eddies.

The colder parts of the Gulf Stream turn northward near the "tail" of the Grand Banks at 50°W where the Azores Current branches off to flow south of the Azores. From there the NAC flows northeastward, east of the Flemish Cap (47°N, 45°W). Approaching the Mid-Atlantic Ridge, it then turns eastward and becomes much broader and more diffuse. It then splits into a colder northeastern branch and a warmer eastern branch. As the warmer branch turns southward, most of the subtropical component of the Gulf Stream is diverted southward, and as a consequence, the North Atlantic is mostly supplied by subpolar waters, including a contribution from the Labrador Current recirculated into the NAC at 45°N.

West of Continental Europe, it splits into two major branches. One branch goes southeast, becoming the Canary Current as it passes northwest Africa and turns southwest. The other major branch continues north along the coast of Northwestern Europe.
Other branches include the Irminger Current and the Norwegian Current. Driven by the global thermohaline circulation, the North Atlantic Current is part of the wind-driven Gulf Stream, which goes further east and north from the North American coast across the Atlantic and into the Arctic Ocean.

The North Atlantic Current, together with the Gulf Stream, have a long-lived reputation for having a considerable warming influence on European climate. However, the principal cause for differences in winter climate between North America and Europe seems to be winds rather than ocean currents (although the currents do exert influence at very high latitudes by preventing the formation of sea ice).





</doc>
<doc id="21513" url="https://en.wikipedia.org/wiki?curid=21513" title="North Atlantic Deep Water">
North Atlantic Deep Water

North Atlantic Deep Water (NADW) is a deep water mass formed in the North Atlantic Ocean. Thermohaline circulation (properly described as meridional overturning circulation) of the world's oceans involves the flow of warm surface waters from the southern hemisphere into the North Atlantic. Water flowing northward becomes modified through evaporation and mixing with other water masses, leading to increased salinity. When this water reaches the North Atlantic it cools and sinks through convection, due to its decreased temperature and increased salinity resulting in increased density. NADW is the outflow of this thick deep layer, which can be detected by its high salinity, high oxygen content, nutrient minima, high C/C, and chlorofluorocarbons (CFCs). 

CFCs are anthropogenic substances that enter the surface of the ocean from gas exchange with the atmosphere. This distinct composition allows its path to be traced as it mixes with Circumpolar Deep Water (CDW), which in turn fills the deep Indian Ocean and part of the South Pacific. NADW and its formation is essential to the Atlantic Meridional Overturning Circulation (AMOC), which is responsible for transporting large amounts of water, heat, salt, carbon, nutrients and other substances from the Tropical Atlantic to the Mid and High Latitude Atlantic. 

In the conveyor belt model of thermohaline circulation of the world's oceans, the sinking of NADW pulls the waters of the North Atlantic drift northward. However, this is almost certainly an oversimplification of the actual relationship between NADW formation and the strength of the Gulf Stream/North Atlantic drift.

NADW has a temperature of 2-4 °C with a salinity of 34.9-35.0 psu found at a depth between 1500 and 4000m.

The NADW is a complex of several water masses formed by deep convection and also by overflow of dense water across the Greenland-Iceland-Scotland Ridge.

The upper layers are formed by deep open ocean convection during winter. Labrador Sea Water (LSW), formed in the Labrador Sea can reach depths of 2000 m as dense water sinks downward. Classical Labrador Sea Water (CLSW) production is dependent on preconditioning of water in the Labrador Sea from the previous year, and the strength of the North Atlantic Oscillation.

During a positive NAO phase, conditions exist for strong winter storms to develop. These storms freshen the surface water, and their winds increase cyclonic flow, which allows denser waters to sink. As a result, the temperature, salinity, and density vary yearly. In some years these conditions do not exist and CLSW is not formed. CLSW has characteristic potential temperature of 3 °C, salinity of 34.88 psu, and density of 34.66. 

Another component of LSW is the Upper Labrador Sea Water (ULSW). ULSW forms at a density lower than CLSW and has a CFC maximum between 1200 and 1500 m in the subtropical North Atlantic. Eddies of cold less saline ULSW have similar densities of warmer saltier water and flow along the DWBC, but maintain their high CFCs. The ULSW eddies erode rapidly as they mix laterally with this warmer saltier water.

The lower waters mass of NADW form from overflow of the Greenland-Iceland-Scotland Ridge. They are Iceland-Scotland Overflow Water (ISOW) and Denmark Strait Overflow Water (DSOW). The overflows are a combination of dense Arctic Ocean water (18%), modified Atlantic water (32%), and intermediate water from the Nordic seas (20%), that entrain and mix with other water masses (contributing 30%) as they flow over the Greenland-Iceland-Scotland Ridge. 

The formation of both of these waters involves the conversion of warm salty northward flowing surface waters to cold dense deep waters behind the Greenland-Iceland-Scotland Ridge. Water flow from the North Atlantic current enters the Arctic Ocean through the Norwegian Current which splits into the Fram Strait and Barents Sea Branch. Water from the Fram Strait recirculates, reaching a density of DSOW, sinks, and flows towards the Denmark Strait. Water flowing into the Barent Sea feeds ISOW.

ISOW enters the eastern North Atlantic over the Iceland-Scotland Ridge through the Faeroe Bank Channel at a depth of 850 m, with some water flowing over the shallower Iceland-Faeroe Rise. ISOW has a low CFC concentrations and it has been estimated from these concentrations that ISOW resides behind the ridge for 45 years. As the water flows southward at the bottom of the channel, it entrains surrounding water of the eastern North Atlantic, and flows to the western North Atlantic through the Charlie-Gibbs Fracture Zone, entraining with LSW. This water is less dense than (DSOW) and lays above it as it flows cyclonically in the Irminger Basin.

DSOW is the coldest, densest, and freshest water mass of NADW. DSOW formed behind the ridge flows over the Denmark Strait at a depth of 600m. The most significant water mass contributing to DSOW is Arctic Intermediate Water (AIW). Winter cooling and convection allow AIW to sink and pool behind the Denmark Strait. Upper AIW has a high amount of anthropogenic tracers due its exposure to the atmosphere. AIW's tritium and CFC signature is observed in DSOW at the base of the Greenland continental slope. This also showed that the DSOW flowing 450 km to the south was no older than 2 years. Both the DSOW and ISOW flow around the Irminger Basin and Labrador Sea in a deep boundary current. Leaving the Greenland Sea with 2.5 Sv its flow increases to 10 Sv south of Greenland. It is cold and relatively fresh, flowing below 3500 m in the DWBC and spreading inward the deep Atlantic basins.

The southward spread of NADW along the Deep Western Boundary current (DWBC) can be traced by its high oxygen content, high CFCs, and density.

ULSW is the major source of upper NADW. ULSW advects southward from the Labrador Sea in small eddies that mix into the DWBC. A CFC maxima associated with ULSW has been observed along 24°N in the DWBC at 1500 m. Some of the upper ULSW recirculates into the Gulf Stream, while some remains in the DWBC. High CFCs in the subtropics indicate recirculation in the subtropics.

ULSW that remains in the DWBC dilutes as it moves equatorward. Deep convection in the Labrador Sea during the late 1980s and early 1990s resulted in CLSW with a lower CFC concentration due to downward mixing. Convection allowed the CFCs to penetrate further downward to 2000m. These minimum could be tracked, and were first observed in the subtropics in the early 1990s.

ISOW and DSOW flow around the Irminger Basin and DSOW entering the DWBC. These are the two lower portions of the NADW. Another CFC maximum is seen at 3500 m in the subtropics from the DSOW contribution to NADW. Some of the NADW recirculates with the northern gyre. To the south of the gyre NADW flows under the Gulf Stream where it continues along the DWBC until it reaches another gyre in the subtropics.

Lower North Atlantic Deep Water (LNADW), originating in the Greenland and Norwegian seas, brings high salinity, oxygen, and freon concentrations towards to the Romanche Trench, an equatorial fracture zone in the Mid-Atlantic Ridge (MAR). Found at depths around , LNADW flow east through the trench over AABW, the trench being the only opening in the MAR where inter-basin exchange is possible for these two water masses.

It is believed that North Atlantic Deep Water formation has been dramatically reduced at times during the past (such as during the Younger Dryas or during Heinrich events), and that this might correlate with a decrease in the strength of the Gulf Stream and the North Atlantic drift, in turn cooling the climate of northwestern Europe. 

There is concern that global warming might cause this to happen again. It is also hypothesized that during the Last Glacial Maximum (LGM), NADW was replaced with an analogous watermass that occupied a shallower depth known as Glacial North Atlantic Intermediate Water (GNAIW).




</doc>
<doc id="21514" url="https://en.wikipedia.org/wiki?curid=21514" title="Nanomedicine">
Nanomedicine

Nanomedicine is the medical application of nanotechnology. Nanomedicine ranges from the medical applications of nanomaterials and biological devices, to nanoelectronic biosensors, and even possible future applications of molecular nanotechnology such as biological machines. Current problems for nanomedicine involve understanding the issues related to toxicity and environmental impact of nanoscale materials (materials whose structure is on the scale of nanometers, i.e. billionths of a meter).
Functionalities can be added to nanomaterials by interfacing them with biological molecules or structures. The size of nanomaterials is similar to that of most biological molecules and structures; therefore, nanomaterials can be useful for both in vivo and in vitro biomedical research and applications. Thus far, the integration of nanomaterials with biology has led to the development of diagnostic devices, contrast agents, analytical tools, physical therapy applications, and drug delivery vehicles.

Nanomedicine seeks to deliver a valuable set of research tools and clinically useful devices in the near future. The National Nanotechnology Initiative expects new commercial applications in the pharmaceutical industry that may include advanced drug delivery systems, new therapies, and in vivo imaging. Nanomedicine research is receiving funding from the US National Institutes of Health Common Fund program, supporting four nanomedicine development centers.

Nanomedicine sales reached $16 billion in 2015, with a minimum of $3.8 billion in nanotechnology R&D being invested every year. Global funding for emerging nanotechnology increased by 45% per year in recent years, with product sales exceeding $1 trillion in 2013. As the nanomedicine industry continues to grow, it is expected to have a significant impact on the economy.

 Nanotechnology has provided the possibility of delivering drugs to specific cells using nanoparticles. The overall drug consumption and side-effects may be lowered significantly by depositing the active agent in the morbid region only and in no higher dose than needed. Targeted drug delivery is intended to reduce the side effects of drugs with concomitant decreases in consumption and treatment expenses. Drug delivery focuses on maximizing bioavailability both at specific places in the body and over a period of time. This can potentially be achieved by molecular targeting by nanoengineered devices. A benefit of using nanoscale for medical technologies is that smaller devices are less invasive and can possibly be implanted inside the body, plus biochemical reaction times are much shorter. These devices are faster and more sensitive than typical drug delivery. The efficacy of drug delivery through nanomedicine is largely based upon: a) efficient encapsulation of the drugs, b) successful delivery of drug to the targeted region of the body, and c) successful release of the drug.

Drug delivery systems, lipid- or polymer-based nanoparticles, can be designed to improve the pharmacokinetics and biodistribution of the drug. However, the pharmacokinetics and pharmacodynamics of nanomedicine is highly variable among different patients. When designed to avoid the body's defence mechanisms, nanoparticles have beneficial properties that can be used to improve drug delivery. Complex drug delivery mechanisms are being developed, including the ability to get drugs through cell membranes and into cell cytoplasm. Triggered response is one way for drug molecules to be used more efficiently. Drugs are placed in the body and only activate on encountering a particular signal. For example, a drug with poor solubility will be replaced by a drug delivery system where both hydrophilic and hydrophobic environments exist, improving the solubility. Drug delivery systems may also be able to prevent tissue damage through regulated drug release; reduce drug clearance rates; or lower the volume of distribution and reduce the effect on non-target tissue. However, the biodistribution of these nanoparticles is still imperfect due to the complex host's reactions to nano- and microsized materials and the difficulty in targeting specific organs in the body. Nevertheless, a lot of work is still ongoing to optimize and better understand the potential and limitations of nanoparticulate systems. While advancement of research proves that targeting and distribution can be augmented by nanoparticles, the dangers of nanotoxicity become an important next step in further understanding of their medical uses.

Nanoparticles are under research for their potential to decrease antibiotic resistance or for various antimicrobial uses. Nanoparticles might also be used to circumvent multidrug resistance (MDR) mechanisms.
Advances in lipid nanotechnology were instrumental in engineering medical nanodevices and novel drug delivery systems, as well as in developing sensing applications. Another system for microRNA delivery under preliminary research is nanoparticles formed by the self-assembly of two different microRNAs deregulated in cancer. One potential application is based on small electromechanical systems, such as nanoelectromechanical systems being investigated for the active release of drugs and sensors for possible cancer treatment with iron nanoparticles or gold shells.

Some nanotechnology-based drugs that are commercially available or in human clinical trials include:

Existing and potential drug nanocarriers have been reviewed.

Nanoparticles have high surface area to volume ratio. This allows for many functional groups to be attached to a nanoparticle, which can seek out and bind to certain tumor cells. Additionally, the small size of nanoparticles (5 to 100 nanometers), allows them to preferentially accumulate at tumor sites (because tumors lack an effective lymphatic drainage system). Limitations to conventional cancer chemotherapy include drug resistance, lack of selectivity, and lack of solubility.

"In vivo" imaging is another area where tools and devices are being developed. Using nanoparticle contrast agents, images such as ultrasound and MRI have a favorable distribution and improved contrast. In cardiovascular imaging, nanoparticles have potential to aid visualization of blood pooling, ischemia, angiogenesis, atherosclerosis, and focal areas where inflammation is present.

The small size of nanoparticles endows them with properties that can be very useful in oncology, particularly in imaging. Quantum dots (nanoparticles with quantum confinement properties, such as size-tunable light emission), when used in conjunction with MRI (magnetic resonance imaging), can produce exceptional images of tumor sites. Nanoparticles of cadmium selenide (quantum dots) glow when exposed to ultraviolet light. When injected, they seep into cancer tumors. The surgeon can see the glowing tumor, and use it as a guide for more accurate tumor removal.These nanoparticles are much brighter than organic dyes and only need one light source for excitation. This means that the use of fluorescent quantum dots could produce a higher contrast image and at a lower cost than today's organic dyes used as contrast media. The downside, however, is that quantum dots are usually made of quite toxic elements, but this concern may be addressed by use of fluorescent dopants.

Tracking movement can help determine how well drugs are being distributed or how substances are metabolized. It is difficult to track a small group of cells throughout the body, so scientists used to dye the cells. These dyes needed to be excited by light of a certain wavelength in order for them to light up. While different color dyes absorb different frequencies of light, there was a need for as many light sources as cells. A way around this problem is with luminescent tags. These tags are quantum dots attached to proteins that penetrate cell membranes. The dots can be random in size, can be made of bio-inert material, and they demonstrate the nanoscale property that color is size-dependent. As a result, sizes are selected so that the frequency of light used to make a group of quantum dots fluoresce is an even multiple of the frequency required to make another group incandesce. Then both groups can be lit with a single light source. They have also found a way to insert nanoparticles into the affected parts of the body so that those parts of the body will glow showing the tumor growth or shrinkage or also organ trouble.

Nanotechnology-on-a-chip is one more dimension of lab-on-a-chip technology. Magnetic nanoparticles, bound to a suitable antibody, are used to label specific molecules, structures or microorganisms. In particular silica nanoparticles are inert from the photophysical point of view and might accumulate a large number of dye(s) within the nanoparticle shell. Gold nanoparticles tagged with short segments of DNA can be used for detection of genetic sequence in a sample. Multicolor optical coding for biological assays has been achieved by embedding different-sized quantum dots into polymeric microbeads. Nanopore technology for analysis of nucleic acids converts strings of nucleotides directly into electronic signatures.

Sensor test chips containing thousands of nanowires, able to detect proteins and other biomarkers left behind by cancer cells, could enable the detection and diagnosis of cancer in the early stages from a few drops of a patient's blood. Nanotechnology is helping to advance the use of arthroscopes, which are pencil-sized devices that are used in surgeries with lights and cameras so surgeons can do the surgeries with smaller incisions. The smaller the incisions the faster the healing time which is better for the patients. It is also helping to find a way to make an arthroscope smaller than a strand of hair.

Research on nanoelectronics-based cancer diagnostics could lead to tests that can be done in pharmacies. The results promise to be highly accurate and the product promises to be inexpensive. They could take a very small amount of blood and detect cancer anywhere in the body in about five minutes, with a sensitivity that is a thousand times better a conventional laboratory test. These devices that are built with nanowires to detect cancer proteins; each nanowire detector is primed to be sensitive to a different cancer marker. The biggest advantage of the nanowire detectors is that they could test for anywhere from ten to one hundred similar medical conditions without adding cost to the testing device. Nanotechnology has also helped to personalize oncology for the detection, diagnosis, and treatment of cancer. It is now able to be tailored to each individual's tumor for better performance. They have found ways that they will be able to target a specific part of the body that is being affected by cancer.

Magnetic micro particles are proven research instruments for the separation of cells and proteins from complex media. The technology is available under the name Magnetic-activated cell sorting or Dynabeads among others. More recently it was shown in animal models that magnetic nanoparticles can be used for the removal of various noxious compounds including toxins, pathogens, and proteins from whole blood in an extracorporeal circuit similar to dialysis. In contrast to dialysis, which works on the principle of the size related diffusion of solutes and ultrafiltration of fluid across a semi-permeable membrane, the purification with nanoparticles allows specific targeting of substances. Additionally larger compounds which are commonly not dialyzable can be removed.

The purification process is based on functionalized iron oxide or carbon coated metal nanoparticles with ferromagnetic or superparamagnetic properties. Binding agents such as proteins, antibodies, antibiotics, or synthetic ligands are covalently linked to the particle surface. These binding agents are able to interact with target species forming an agglomerate. Applying an external magnetic field gradient allows exerting a force on the nanoparticles. Hence the particles can be separated from the bulk fluid, thereby cleaning it from the contaminants.

The small size (< 100 nm) and large surface area of functionalized nanomagnets leads to advantageous properties compared to hemoperfusion, which is a clinically used technique for the purification of blood and is based on surface adsorption. These advantages are high loading and accessible for binding agents, high selectivity towards the target compound, fast diffusion, small hydrodynamic resistance, and low dosage.

This approach offers new therapeutic possibilities for the treatment of systemic infections such as sepsis by directly removing the pathogen. It can also be used to selectively remove cytokines or endotoxins or for the dialysis of compounds which are not accessible by traditional dialysis methods. However the technology is still in a preclinical phase and first clinical trials are not expected before 2017.

Nanotechnology may be used as part of tissue engineering to help reproduce or repair or reshape damaged tissue using suitable nanomaterial-based scaffolds and growth factors. Tissue engineering if successful may replace conventional treatments like organ transplants or artificial implants. Nanoparticles such as graphene, carbon nanotubes, molybdenum disulfide and tungsten disulfide are being used as reinforcing agents to fabricate mechanically strong biodegradable polymeric nanocomposites for bone tissue engineering applications. The addition of these nanoparticles in the polymer matrix at low concentrations (~0.2 weight %) leads to significant improvements in the compressive and flexural mechanical properties of polymeric nanocomposites. Potentially, these nanocomposites may be used as a novel, mechanically strong, light weight composite as bone implants.

For example, a flesh welder was demonstrated to fuse two pieces of chicken meat into a single piece using a suspension of gold-coated nanoshells activated by an infrared laser. This could be used to weld arteries during surgery.
Another example is nanonephrology, the use of nanomedicine on the kidney.

Neuro-electronic interfacing is a visionary goal dealing with the construction of nanodevices that will permit computers to be joined and linked to the nervous system. This idea requires the building of a molecular structure that will permit control and detection of nerve impulses by an external computer. A refuelable strategy implies energy is refilled continuously or periodically with external sonic, chemical, tethered, magnetic, or biological electrical sources, while a nonrefuelable strategy implies that all power is drawn from internal energy storage which would stop when all energy is drained. A nanoscale enzymatic biofuel cell for self-powered nanodevices have been developed that uses glucose from biofluids including human blood and watermelons. One limitation to this innovation is the fact that electrical interference or leakage or overheating from power consumption is possible. The wiring of the structure is extremely difficult because they must be positioned precisely in the nervous system. The structures that will provide the interface must also be compatible with the body's immune system.

Molecular nanotechnology is a speculative subfield of nanotechnology regarding the possibility of engineering molecular assemblers, machines which could re-order matter at a molecular or atomic scale. Nanomedicine would make use of these nanorobots, introduced into the body, to repair or detect damages and infections. Molecular nanotechnology is highly theoretical, seeking to anticipate what inventions nanotechnology might yield and to propose an agenda for future inquiry. The proposed elements of molecular nanotechnology, such as molecular assemblers and nanorobots are far beyond current capabilities. Future advances in nanomedicine could give rise to life extension through the repair of many processes thought to be responsible for aging. K. Eric Drexler, one of the founders of nanotechnology, postulated cell repair machines, including ones operating within cells and utilizing as yet hypothetical molecular machines, in his 1986 book "Engines of Creation", with the first technical discussion of medical nanorobots by Robert Freitas appearing in 1999. Raymond Kurzweil, a futurist and transhumanist, stated in his book "The Singularity Is Near" that he believes that advanced medical nanorobotics could completely remedy the effects of aging by 2030. According to Richard Feynman, it was his former graduate student and collaborator Albert Hibbs who originally suggested to him (circa 1959) the idea of a "medical" use for Feynman's theoretical micromachines (see nanotechnology). Hibbs suggested that certain repair machines might one day be reduced in size to the point that it would, in theory, be possible to (as Feynman put it) "swallow the doctor". The idea was incorporated into Feynman's 1959 essay "There's Plenty of Room at the Bottom."


</doc>
<doc id="21518" url="https://en.wikipedia.org/wiki?curid=21518" title="NMR (disambiguation)">
NMR (disambiguation)

NMR, or nuclear magnetic resonance, is a phenomenon in which nuclei in a magnetic field absorb and re-emit electromagnetic radiation.

NMR may also refer to:






</doc>
<doc id="21520" url="https://en.wikipedia.org/wiki?curid=21520" title="Null set">
Null set

In mathematical analysis, a null set formula_1 is a set that can be covered by a countable union of intervals of arbitrarily small total length. The notion of null set in set theory anticipates the development of Lebesgue measure since a null set necessarily has measure zero. More generally, on a given measure space formula_2 a null set is a set formula_3 such that formula_4.

Every countable subset of the real numbers that (i.e. finite or countably infinite) is null. For example, the set of natural numbers is countable, having cardinality formula_5 ("aleph-zero" or "aleph-null"), is null. Another example is the set of rational numbers, which is also countable, and hence null. 

However, there are some uncountable sets, such as the Cantor set, that are null.

Suppose formula_6 is a subset of the real line formula_7 such that 

where the are intervals and is the length of , then is a null set, also known as a set of zero-content.

In terminology of mathematical analysis, this definition requires that there be a sequence of open covers of for which the limit of the lengths of the covers is zero.

Null sets include all finite sets, all countable sets, and even some uncountable sets such as the Cantor set.

The empty set is always a null set. More generally, any countable union of null sets is null. Any measurable subset of a null set is itself a null set. Together, these facts show that the "m"-null sets of "X" form a sigma-ideal on "X". Similarly, the measurable "m"-null sets form a sigma-ideal of the sigma-algebra of measurable sets. Thus, null sets may be interpreted as negligible sets, defining a notion of almost everywhere.

The Lebesgue measure is the standard way of assigning a length, area or volume to subsets of Euclidean space.

A subset "N" of formula_7 has null Lebesgue measure and is considered to be a null set in formula_7 if and only if:
This condition can be generalised to formula_12, using "n"-cubes instead of intervals. In fact, the idea can be made to make sense on any Riemannian manifold, even if there is no Lebesgue measure there.

For instance:

If λ is Lebesgue measure for formula_7 and π is Lebesgue measure for formula_20, then the product measure formula_21. In terms of null sets, the following equivalence has been styled a Fubini's theorem: 

Null sets play a key role in the definition of the Lebesgue integral: if functions "f" and "g" are equal except on a null set, then "f" is integrable if and only if "g" is, and their integrals are equal.

A measure in which all subsets of null sets are measurable is "complete". Any non-complete measure can be completed to form a complete measure by asserting that subsets of null sets have measure zero. Lebesgue measure is an example of a complete measure; in some constructions, it is defined as the completion of a non-complete Borel measure.

The Borel measure is not complete. One simple construction is to start with the standard Cantor set "K", which is closed hence Borel measurable, and which has measure zero, and to find a subset "F" of "K" which is not Borel measurable. (Since the Lebesgue measure is complete, this "F" is of course Lebesgue measurable.)

First, we have to know that every set of positive measure contains a nonmeasurable subset. Let "f" be the Cantor function, a continuous function which is locally constant on "K", and monotonically increasing on [0, 1], with "f"(0) = 0 and "f"(1) = 1. Obviously, "f"("K") is countable, since it contains one point per component of "K". Hence "f"("K") has measure zero, so "f"("K") has measure one. We need a strictly monotonic function, so consider "g"("x") = "f"("x") + "x". Since "g"("x") is strictly monotonic and continuous, it is a homeomorphism. Furthermore, "g"("K") has measure one. Let "E" ⊂ "g"("K") be non-measurable, and let "F" = "g"("E"). Because "g" is injective, we have that "F" ⊂ "K", and so "F" is a null set. However, if it were Borel measurable, then "g"("F") would also be Borel measurable (here we use the fact that the preimage of a Borel set by a continuous function is measurable; "g"("F") = ("g")("F") is the preimage of "F" through the continuous function "h" = "g".) Therefore, "F" is a null, but non-Borel measurable set.

In a separable Banach space ("X", +), the group operation moves any subset "A" ⊂ "X" to the translates "A" + "x" for any "x" ∈ "X". When there is a probability measure μ on the σ-algebra of Borel subsets of "X", such that for all "x", μ("A" + "x") = 0, then "A" is a Haar null set.

The term refers to the null invariance of the measures of translates, associating it with the complete invariance found with Haar measure.

Some algebraic properties of topological groups have been related to the size of subsets and Haar null sets.
Haar null sets have been used in Polish groups to show that when "A" is not a meagre set then "A""A" contains an open neighborhood of the identity element. This property is named for Hugo Steinhaus since it is the conclusion of the Steinhaus theorem.




</doc>
<doc id="21522" url="https://en.wikipedia.org/wiki?curid=21522" title="November 24">
November 24






</doc>
<doc id="21523" url="https://en.wikipedia.org/wiki?curid=21523" title="Artificial neural network">
Artificial neural network

Artificial neural networks (ANNs), usually simply called neural networks (NNs), are computing systems vaguely inspired by the biological neural networks that constitute animal brains. 

An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it. The "signal" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called "edges". Neurons and edges typically have a "weight" that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.

Neural networks learn (or are trained) by processing examples, each of which contains a known "input" and "result," forming probability-weighted associations between the two, which are stored within the data structure of the net itself. The training of a neural network from a given example is usually conducted by determining the difference between the processed output of the network (often a prediction) and a target output. This is the error. The network then adjusts its weighted associations according to a learning rule and using this error value. Successive adjustments will cause the neural network to produce output which is increasingly similar to the target output. After a sufficient number of these adjustments the training can be terminated based upon certain criteria. This is known as supervised learning. 

Such systems "learn" to perform tasks by considering examples, generally without being programmed with task-specific rules. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as "cat" or "no cat" and using the results to identify cats in other images. They do this without any prior knowledge of cats, for example, that they have fur, tails, whiskers and cat-like faces. Instead, they automatically generate identifying characteristics from the examples that they process.

Warren McCulloch and Walter Pitts (1943) opened the subject by creating a computational model for neural networks. In the late 1940s, D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Farley and Wesley A. Clark (1954) first used computational machines, then called "calculators", to simulate a Hebbian network. Rosenblatt (1958) created the perceptron. The first functional networks with many layers were published by Ivakhnenko and Lapa in 1965, as the Group Method of Data Handling. The basics of continuous backpropagation were derived in the context of control theory by Kelley in 1960 and by Bryson in 1961, using principles of dynamic programming.

In 1970, Seppo Linnainmaa published the general method for automatic differentiation (AD) of discrete connected networks of nested differentiable functions. In 1973, Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients. Werbos's (1975) backpropagation algorithm enabled practical training of multi-layer networks. In 1982, he applied Linnainmaa's AD method to neural networks in the way that became widely used. Thereafter research stagnated following Minsky and Papert (1969), who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to process useful neural networks. 

Increasing transistor count in digital electronics provided more processing power that enabled the development of practical artificial neural networks in the 1980s. 

In 1992, max-pooling was introduced to help with least-shift invariance and tolerance to deformation to aid 3D object recognition. Schmidhuber adopted a multi-level hierarchy of networks (1992) pre-trained one level at a time by unsupervised learning and fine-tuned by backpropagation.

Geoffrey Hinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer. In 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images. Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as "deep learning".

Ciresan and colleagues (2010) showed that despite the vanishing gradient problem, GPUs make backpropagation feasible for many-layered feedforward neural networks. Between 2009 and 2012, ANNs began winning prizes in ANN contests, approaching human level performance on various tasks, initially in pattern recognition and machine learning. For example, the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition in 2009 without any prior knowledge about the three languages to be learned.

Ciresan and colleagues built the first pattern recognizers to achieve human-competitive/superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012).

ANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with. They soon reoriented towards improving empirical results, mostly abandoning attempts to remain true to their biological precursors. Neurons are connected to each other in various patterns, to allow the output of some neurons to become the input of others. The network forms a directed, weighted graph.

An artificial neural network consists of a collection of simulated neurons. Each neuron is a node which is connected to other nodes via links that correspond to biological axon-synapse-dendrite connections. Each link has a weight, which determines the strength of one node's influence on another.

ANNs are composed of artificial neurons which are conceptually derived from biological neurons. Each artificial neuron has inputs and produce a single output which can be sent to multiple other neurons. The inputs can be the feature values of a sample of external data, such as images or documents, or they can be the outputs of other neurons. The outputs of the final "output neurons" of the neural net accomplish the task, such as recognizing an object in an image.

To find the output of the neuron, first we take the weighted sum of all the inputs, weighted by the "weights" of the "connections" from the inputs to the neuron. We add a "bias" term to this sum. This weighted sum is sometimes called the "activation". This weighted sum is then passed through a (usually nonlinear) activation function to produce the output. The initial inputs are external data, such as images and documents. The ultimate outputs accomplish the task, such as recognizing an object in an image.

The network consists of connections, each connection providing the output of one neuron as an input to another neuron. Each connection is assigned a weight that represents its relative importance. A given neuron can have multiple input and output connections.

The "propagation function" computes the input to a neuron from the outputs of its predecessor neurons and their connections as a weighted sum. A "bias" term can be added to the result of the propagation.

The neurons are typically organized into multiple layers, especially in deep learning. Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers. The layer that receives external data is the "input layer". The layer that produces the ultimate result is the "output layer". In between them are zero or more "hidden layers". Single layer and unlayered networks are also used. Between two layers, multiple connection patterns are possible. They can be "fully connected", with every neuron in one layer connecting to every neuron in the next layer. They can be "pooling", where a group of neurons in one layer connect to a single neuron in the next layer, thereby reducing the number of neurons in that layer. Neurons with only such connections form a directed acyclic graph and are known as "feedforward networks". Alternatively, networks that allow connections between neurons in the same or previous layers are known as "recurrent networks""."

A hyperparameter is a constant parameter whose value is set before the learning process begins. The values of parameters are derived via learning. Examples of hyperparameters include learning rate, the number of hidden layers and batch size. The values of some hyperparameters can be dependent on those of other hyperparameters. For example, the size of some layers can depend on the overall number of layers.

Learning is the adaptation of the network to better handle a task by considering sample observations. Learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result. This is done by minimizing the observed errors. Learning is complete when examining additional observations does not usefully reduce the error rate. Even after learning, the error rate typically does not reach 0. If after learning, the error rate is too high, the network typically must be redesigned. Practically this is done by defining a cost function that is evaluated periodically during learning. As long as its output continues to decline, learning continues. The cost is frequently defined as a statistic whose value can only be approximated. The outputs are actually numbers, so when the error is low, the difference between the output (almost certainly a cat) and the correct answer (cat) is small. Learning attempts to reduce the total of the differences across the observations. Most learning models can be viewed as a straightforward application of optimization theory and statistical estimation.

The learning rate defines the size of the corrective steps that the model takes to adjust for errors in each observation. A high learning rate shortens the training time, but with lower ultimate accuracy, while a lower learning rate takes longer, but with the potential for greater accuracy. Optimizations such as Quickprop are primarily aimed at speeding up error minimization, while other improvements mainly try to increase reliability. In order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements use an adaptive learning rate that increases or decreases as appropriate. The concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change. A momentum close to 0 emphasizes the gradient, while a value close to 1 emphasizes the last change.

While it is possible to define a cost function ad hoc, frequently the choice is determined by the functions desirable properties (such as convexity) or because it arises from the model (e.g., in a probabilistic model the model's posterior probability can be used as an inverse cost).

Backpropagation is a method to adjust the connection weights to compensate for each error found during learning. The error amount is effectively divided among the connections. Technically, backprop calculates the gradient (the derivative) of the cost function associated with a given state with respect to the weights. The weight updates can be done via stochastic gradient descent or other methods, such as Extreme Learning Machines, "No-prop" networks, training without backtracking, "weightless" networks, and non-connectionist neural networks.

The three major learning paradigms are supervised learning, unsupervised learning and reinforcement learning. They each correspond to a particular learning task

Supervised learning uses a set of paired inputs and desired outputs. The learning task is to produce the desired output for each input. In this case the cost function is related to eliminating incorrect deductions. A commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output and the desired output. Tasks suited for supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). Supervised learning is also applicable to sequential data (e.g., for hand writing, speech and gesture recognition). This can be thought of as learning with a "teacher", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.

In unsupervised learning, input data is given along with the cost function, some function of the data formula_1 and the network's output. The cost function is dependent on the task (the model domain) and any "a priori" assumptions (the implicit properties of the model, its parameters and the observed variables). As a trivial example, consider the model formula_2 where formula_3 is a constant and the cost formula_4. Minimizing this cost produces a value of formula_3 that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example, in compression it could be related to the mutual information between formula_1 and formula_7, whereas in statistical modeling, it could be related to the posterior probability of the model given the data (note that in both of those examples those quantities would be maximized rather than minimized). Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering.

In applications such as playing video games, an actor takes a string of actions, receiving a generally unpredictable response from the environment after each one. The goal is to win the game, i.e., generate the most positive (lowest cost) responses. In reinforcement learning, the aim is to weight the network (devise a policy) to perform actions that minimize long-term (expected cumulative) cost. At each point in time the agent performs an action and the environment generates an observation and an instantaneous cost, according to some (usually unknown) rules. The rules and the long-term cost usually only can be estimated. At any juncture, the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly.

Formally the environment is modeled as a Markov decision process (MDP) with states formula_8 and actions formula_9. Because the state transitions are not known, probability distributions are used instead: the instantaneous cost distribution formula_10, the observation distribution formula_11 and the transition distribution formula_12, while a policy is defined as the conditional distribution over actions given the observations. Taken together, the two define a Markov chain (MC). The aim is to discover the lowest-cost MC.

ANNs serve as the learning component in such applications. Dynamic programming coupled with ANNs (giving neurodynamic programming) has been applied to problems such as those involved in vehicle routing, video games, natural resource management and medicine because of ANNs ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems. Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks.

Self learning in neural networks was introduced in 1982 along with a neural network capable of self-learning named Crossbar Adaptive Array (CAA). It is a system with only one input, situation s, and only one output, action (or behavior) a. It has neither external advice input nor external reinforcement input from the environment. The CAA computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about encountered situations. The system is driven by the interaction between cognition and emotion. Given memory matrix W =||w(a,s)||, the crossbar self learning algorithm in each iteration performs the following computation:

The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is behavioral environment where it behaves, and the other is genetic environment, where from it initially and only once receives initial emotions about to be encountered situations in the behavioral environment. Having received the genome vector (species vector) from the genetic environment, the CAA will learn a goal-seeking behavior, in the behavioral environment that contains both desirable and undesirable situations.

In a Bayesian framework, a distribution over the set of allowed models is chosen to minimize the cost. Evolutionary methods, gene expression programming, simulated annealing, expectation-maximization, non-parametric methods and particle swarm optimization are other learning algorithms. Convergent recursion is a learning algorithm for cerebellar model articulation controller (CMAC) neural networks.

Two modes of learning are available: stochastic and batch. In stochastic learning, each input creates a weight adjustment. In batch learning weights are adjusted based on a batch of inputs, accumulating errors over the batch. Stochastic learning introduces "noise" into the process, using the local gradient calculated from one data point; this reduces the chance of the network getting stuck in local minima. However, batch learning typically yields a faster, more stable descent to a local minimum, since each update is performed in the direction of the batch's average error. A common compromise is to use "mini-batches", small batches with samples in each batch selected stochastically from the entire data set.

ANNs have evolved into a broad family of techniques that have advanced the state of the art across multiple domains. The simplest types have one or more static components, including number of units, number of layers, unit weights and topology. Dynamic types allow one or more of these to evolve via learning. The latter are much more complicated, but can shorten learning periods and produce better results. Some types allow/require learning to be "supervised" by the operator, while others operate independently. Some types operate purely in hardware, while others are purely software and run on general purpose computers.

Some of the main breakthroughs include: convolutional neural networks that have proven particularly successful in processing visual and other two-dimensional data; long short-term memory avoid the vanishing gradient problem and can handle signals that have a mix of low and high frequency components aiding large-vocabulary speech recognition, text-to-speech synthesis, and photo-real talking heads; competitive networks such as generative adversarial networks in which multiple networks (of varying structure) compete with each other, on tasks such as winning a game or on deceiving the opponent about the authenticity of an input.

Neural architecture search (NAS) uses machine learning to automate ANN design. Various approaches to NAS have designed networks that compare well with hand-designed systems. The basic search algorithm is to propose a candidate model, evaluate it against a dataset and use the results as feedback to teach the NAS network. Available systems include AutoML and AutoKeras.

Design issues include deciding the number, type and connectedness of network layers, as well as the size of each and the connection type (full, pooling, ...).

Hyperparameters must also be defined as part of the design (they are not learned), governing matters such as how many neurons are in each layer, learning rate, step, stride, depth, receptive field and padding (for CNNs), etc.

Using Artificial neural networks requires an understanding of their characteristics.
ANN capabilities fall within the following broad categories:


Because of their ability to reproduce and model nonlinear processes, Artificial neural networks have found applications in many disciplines. Application areas include system identification and control (vehicle control, trajectory prediction, process control, natural resource management), quantum chemistry, general game playing, pattern recognition (radar systems, face identification, signal classification, 3D reconstruction, object recognition and more), sequence recognition (gesture, speech, handwritten and printed text recognition), medical diagnosis, finance (e.g. automated trading systems), data mining, visualization, machine translation, social network filtering and e-mail spam filtering. ANNs have been used to diagnose cancers, including lung cancer, prostate cancer, colorectal cancer and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information.

ANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters and to predict foundation settlements. ANNs have also been used for building black-box models in geoscience: hydrology, ocean modelling and coastal engineering, and geomorphology. ANNs have been employed in cybersecurity, with the objective to discriminate between legitimate activities and malicious ones. For example, machine learning has been used for classifying Android malware, for identifying domains belonging to threat actors and for detecting URLs posing a security risk. Research is underway on ANN systems designed for penetration testing, for detecting botnets, credit cards frauds and network intrusions.

ANNs have been proposed as a tool to simulate the properties of many-body open quantum systems. In brain research ANNs have studied short-term behavior of individual neurons, the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems. Studies considered long-and short-term plasticity of neural systems and their relation to learning and memory from the individual neuron to the system level.

The multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem. However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters.

A specific recurrent architecture with rational-valued weights (as opposed to full precision real number-valued weights) has the power of a universal Turing machine, using a finite number of neurons and standard linear connections. Further, the use of irrational values for weights results in a machine with super-Turing power.

A model's "capacity" property corresponds to its ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity.
Two notions of capacity are known by the community. The information capacity and the VC Dimension. The information capacity of a perceptron is intensively discussed in Sir David MacKay's book which summarizes work by Thomas Cover. The capacity of a network of standard neurons (not convolutional) can be derived by four rules that derive from understanding a neuron as an electrical element. The information capacity captures the functions modelable by the network given any data as input. The second notion, is the VC dimension. VC Dimension uses the principles of measure theory and finds the maximum capacity under the best possible circumstances. This is, given input data in a specific form. As noted in , the VC Dimension for arbitrary inputs is half the information capacity of a Perceptron. The VC Dimension for arbitrary points is sometimes referred to as Memory Capacity.

Models may not consistently converge on a single solution, firstly because local minima may exist, depending on the cost function and the model. Secondly, the optimization method used might not guarantee to converge when it begins far from any local minimum. Thirdly, for sufficiently large data or parameters, some methods become impractical.

The convergence behavior of certain types of ANN architectures are more understood than others. When the width of network approaches to infinity, the ANN is well described by its first order Taylor expansion throughout training, and so inherits the convergence behavior of affine models. Another example is when parameters are small, it is observed that ANNs often fits target functions from low to high frequencies. This phenomenon is the opposite to the behavior of some well studied iterative numerical schemes such as Jacobi method.

Applications whose goal is to create a system that generalizes well to unseen examples, face the possibility of over-training. This arises in convoluted or over-specified systems when the network capacity significantly exceeds the needed free parameters. Two approaches address over-training. The first is to use cross-validation and similar techniques to check for the presence of over-training and to select hyperparameters to minimize the generalization error.

The second is to use some form of "regularization". This concept emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.
Supervised neural networks that use a mean squared error (MSE) cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of network output, assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified.

By assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables, the outputs can be interpreted as posterior probabilities. This is useful in classification as it gives a certainty measure on classifications.

The softmax activation function is:

<section end="theory" />

A common criticism of neural networks, particularly in robotics, is that they require too much training for real-world operation. Potential solutions include randomly shuffling training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, grouping examples in so-called mini-batches and/or introducing a recursive least squares algorithm for CMAC.

A fundamental objection is that ANNs do not sufficiently reflect neuronal function. Backpropagation is a critical step, although no such mechanism exists in biological neural networks. How information is coded by real neurons is not known. Sensor neurons fire action potentials more frequently with sensor activation and muscle cells pull more strongly when their associated motor neurons receive action potentials more frequently. Other than the case of relaying information from a sensor neuron to a motor neuron, almost nothing of the principles of how information is handled by biological neural networks is known.

A central claim of ANNs is that they embody new and powerful general principles for processing information. Unfortunately, these principles are ill-defined. It is often claimed that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. Alexander Dewdney commented that, as a result, artificial neural networks have a "something-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one, it seems, has learned anything". One response to Dewdney is that neural networks handle many complex and diverse tasks, ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go.

Technology writer Roger Bridgman commented:

Biological brains use both shallow and deep circuits as reported by brain anatomy, displaying a wide variety of invariance. Weng argued that the brain self-wires largely according to signal statistics and therefore, a serial cascade cannot catch all major statistical dependencies.

Large and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage. Furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons which require enormous CPU power and time.

Schmidhuber noted that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by GPGPUs (on GPUs), has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before. The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.

Neuromorphic engineering addresses the hardware difficulty directly, by constructing non-von-Neumann chips to directly implement neural networks in circuitry. Another type of chip optimized for neural network processing is called a Tensor Processing Unit, or TPU.

Analyzing what has been learned by an ANN, is much easier than to analyze what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example, local vs. non-local learning and shallow vs. deep architecture.

Advocates of hybrid models (combining neural networks and symbolic approaches), claim that such a mixture can better capture the mechanisms of the human mind.




</doc>
<doc id="21525" url="https://en.wikipedia.org/wiki?curid=21525" title="Nutrition">
Nutrition

Nutrition is the science that interprets the nutrients and other substances in food in relation to maintenance, growth, reproduction, health and disease of an organism. It includes ingestion, absorption, assimilation, biosynthesis, catabolism and excretion.

The diet of an organism is what it eats, which is largely determined by the availability and palatability of foods. For humans, a healthy diet includes preparation of food and storage methods that preserve nutrients from oxidation, heat or leaching, and that reduces risk of foodborne illnesses. The seven major classes of human nutrients are carbohydrates, fats, fiber, minerals, proteins, vitamins, and water. Nutrients can be grouped as either macronutrients or micronutrients (needed in small quantities).

In humans, an unhealthy diet can cause deficiency-related diseases such as blindness, anemia, scurvy, preterm birth, stillbirth and cretinism, or nutrient excess health-threatening conditions such as obesity and metabolic syndrome; and such common chronic systemic diseases as cardiovascular disease, diabetes, and osteoporosis. Undernutrition can lead to wasting in acute cases, and the stunting of marasmus in chronic cases of malnutrition.

Carnivore and herbivore diets are contrasting, with basic nitrogen and carbon proportions vary for their particular foods. Many herbivores rely on bacterial fermentation to create digestible nutrients from indigestible plant cellulose, while obligate carnivores must eat animal meats to obtain certain vitamins or nutrients their bodies cannot otherwise synthesize. Animals generally have a higher requirement of energy in comparison to plants.

Plant nutrition is the study of the chemical elements that are necessary for plant growth. There are several principles that apply to plant nutrition. Some elements are directly involved in plant metabolism. However, this principle does not account for the so-called beneficial elements, whose presence, while not required, has clear positive effects on plant growth.

A nutrient that is able to limit plant growth according to Liebig's law of the minimum is considered an essential plant nutrient if the plant cannot complete its full life cycle without it. There are 16 essential plant soil nutrients, besides the three major elemental nutrients carbon and oxygen that are obtained by photosynthetic plants from carbon dioxide in air, and hydrogen, which is obtained from water.

Plants uptake essential elements from the soil through their roots and from the air (consisting of mainly nitrogen and oxygen) through their leaves. Green plants obtain their carbohydrate supply from the carbon dioxide in the air by the process of photosynthesis. Carbon and oxygen are absorbed from the air, while other nutrients are absorbed from the soil. Nutrient uptake in the soil is achieved by cation exchange, wherein root hairs pump hydrogen ions (H) into the soil through proton pumps. These hydrogen ions displace cations attached to negatively charged soil particles so that the cations are available for uptake by the root. In the leaves, stomata open to take in carbon dioxide and expel oxygen. The carbon dioxide molecules are used as the carbon source in photosynthesis.

Although nitrogen is plentiful in the Earth's atmosphere, very few plants can use this directly. Most plants, therefore, require nitrogen compounds to be present in the soil in which they grow. This is made possible by the fact that largely inert atmospheric nitrogen is changed in a nitrogen fixation process to biologically usable forms in the soil by bacteria.

Plant nutrition is a difficult subject to understand completely, partially because of the variation between different plants and even between different species or individuals of a given clone. Elements present at low levels may cause deficiency symptoms, and toxicity is possible at levels that are too high. Furthermore, deficiency of one element may present as symptoms of toxicity from another element, and vice versa.




</doc>
<doc id="21526" url="https://en.wikipedia.org/wiki?curid=21526" title="November 22">
November 22

In the ancient astrology, it is the cusp day between Scorpio and Sagittarius. In some years it is Sagittarius, but others Scorpio.





</doc>
<doc id="21527" url="https://en.wikipedia.org/wiki?curid=21527" title="Number theory">
Number theory

Number theory (or arithmetic or higher arithmetic in older usage) is a branch of pure mathematics devoted primarily to the study of the integers and integer-valued functions. German mathematician Carl Friedrich Gauss (1777–1855) said, "Mathematics is the queen of the sciences—and number theory is the queen of mathematics." Number theorists study prime numbers as well as the properties of objects made out of integers (for example, rational numbers) or defined as generalizations of the integers (for example, algebraic integers). 

Integers can be considered either in themselves or as solutions to equations (Diophantine geometry). Questions in number theory are often best understood through the study of analytical objects (for example, the Riemann zeta function) that encode properties of the integers, primes or other number-theoretic objects in some fashion (analytic number theory). One may also study real numbers in relation to rational numbers, for example, as approximated by the latter (Diophantine approximation).

The older term for number theory is "arithmetic". By the early twentieth century, it had been superseded by "number theory". (The word "arithmetic" is used by the general public to mean "elementary calculations"; it has also acquired other meanings in mathematical logic, as in "Peano arithmetic", and computer science, as in "floating point arithmetic".) The use of the term "arithmetic" for "number theory" regained some ground in the second half of the 20th century, arguably in part due to French influence. In particular, "arithmetical" is preferred as an adjective to "number-theoretic".

The earliest historical find of an arithmetical nature is a fragment of a table: the broken clay tablet Plimpton 322 (Larsa, Mesopotamia, ca. 1800 BCE) contains a list of "Pythagorean triples", that is, integers formula_1 such that formula_2.
The triples are too many and too large to have been obtained by brute force. The heading over the first column reads: "The "takiltum" of the diagonal which has been subtracted such that the width..."

The table's layout suggests that it was constructed by means of what amounts, in modern language, to the identity

which is implicit in routine Old Babylonian exercises. If some other method was used, the triples were first constructed and then reordered by formula_4, presumably for actual use as a "table", for example, with a view to applications.

It is not known what these applications may have been, or whether there could have been any; Babylonian astronomy, for example, truly came into its own only later. It has been suggested instead that the table was a source of numerical examples for school problems.

While Babylonian number theory—or what survives of Babylonian mathematics that can be called thus—consists of this single, striking fragment, Babylonian algebra (in the secondary-school sense of "algebra") was exceptionally well developed. Late Neoplatonic sources state that Pythagoras learned mathematics from the Babylonians. Much earlier sources state that Thales and Pythagoras traveled and studied in Egypt.

Euclid IX 21–34 is very probably Pythagorean; it is very simple material ("odd times even is even", "if an odd number measures [= divides] an even number, then it also measures [= divides] half of it"), but it is all that is needed to prove that formula_5
is irrational. Pythagorean mystics gave great importance to the odd and the even.
The discovery that formula_5 is irrational is credited to the early Pythagoreans (pre-Theodorus). By revealing (in modern terms) that numbers could be irrational, this discovery seems to have provoked the first foundational crisis in mathematical history; its proof or its divulgation are sometimes credited to Hippasus, who was expelled or split from the Pythagorean sect. This forced a distinction between "numbers" (integers and the rationals—the subjects of arithmetic), on the one hand, and "lengths" and "proportions" (which we would identify with real numbers, whether rational or not), on the other hand.

The Pythagorean tradition spoke also of so-called polygonal or figurate numbers. While square numbers, cubic numbers, etc., are seen now as more natural than triangular numbers, pentagonal numbers, etc., the study of the sums
of triangular and pentagonal numbers would prove fruitful in the early modern period (17th to early 19th century).

We know of no clearly arithmetical material in ancient Egyptian or Vedic sources, though there is some algebra in both. The Chinese remainder theorem appears as an exercise in "Sunzi Suanjing" (3rd, 4th or 5th century CE.) (There is one important step glossed over in Sunzi's solution: it is the problem that was later solved by Āryabhaṭa's Kuṭṭaka – see below.)

There is also some numerical mysticism in Chinese mathematics, but, unlike that of the Pythagoreans, it seems to have led nowhere. Like the Pythagoreans' perfect numbers, magic squares have passed from superstition into recreation.

Aside from a few fragments, the mathematics of Classical Greece is known to us either through the reports of contemporary non-mathematicians or through mathematical works from the early Hellenistic period. In the case of number theory, this means, by and large, "Plato" and "Euclid", respectively.

While Asian mathematics influenced Greek and Hellenistic learning, it seems to be the case that Greek mathematics is also an indigenous tradition.

Eusebius, PE X, chapter 4 mentions of Pythagoras:

"In fact the said Pythagoras, while busily studying the wisdom of each nation, visited Babylon, and Egypt, and all Persia, being instructed by the Magi and the priests: and in addition to these he is related to have studied under the Brahmans (these are Indian philosophers); and from some he gathered astrology, from others geometry, and arithmetic and music from others, and different things from different nations, and only from the wise men of Greece did he get nothing, wedded as they were to a poverty and dearth of wisdom: so on the contrary he himself became the author of instruction to the Greeks in the learning which he had procured from abroad."

Aristotle claimed that the philosophy of Plato closely followed the teachings of the Pythagoreans, and Cicero repeats this claim: "Platonem ferunt didicisse Pythagorea omnia" ("They say Plato learned all things Pythagorean").

Plato had a keen interest in mathematics, and distinguished clearly between arithmetic and calculation. (By "arithmetic" he meant, in part, theorising on number, rather than what "arithmetic" or "number theory" have come to mean.) It is through one of Plato's dialogues—namely, "Theaetetus"—that we know that Theodorus had proven that formula_7 are irrational. Theaetetus was, like Plato, a disciple of Theodorus's; he worked on distinguishing different kinds of incommensurables, and was thus arguably a pioneer in the study of number systems. (Book X of Euclid's Elements is described by Pappus as being largely based on Theaetetus's work.)

Euclid devoted part of his "Elements" to prime numbers and divisibility, topics that belong unambiguously to number theory and are basic to it (Books VII to IX of Euclid's Elements). In particular, he gave an algorithm for computing the greatest common divisor of two numbers (the Euclidean algorithm; "Elements", Prop. VII.2) and the first known proof of the infinitude of primes ("Elements", Prop. IX.20).

In 1773, Lessing published an epigram he had found in a manuscript during his work as a librarian; it claimed to be a letter sent by Archimedes to Eratosthenes. The epigram proposed what has become known as
Archimedes's cattle problem; its solution (absent from the manuscript) requires solving an indeterminate quadratic equation (which reduces to what would later be misnamed Pell's equation). As far as we know, such equations were first successfully treated by the Indian school. It is not known whether Archimedes himself had a method of solution.

Very little is known about Diophantus of Alexandria; he probably lived in the third century CE, that is, about five hundred years after Euclid. Six out of the thirteen books of Diophantus's "Arithmetica" survive in the original Greek; four more books survive in an Arabic translation. The "Arithmetica" is a collection of worked-out problems where the task is invariably to find rational solutions to a system of polynomial equations, usually of the form formula_8 or formula_9. Thus, nowadays, we speak of "Diophantine equations" when we speak of polynomial equations to which rational or integer solutions must be found.

One may say that Diophantus was studying rational points, that is, points whose coordinates are rational—on curves and algebraic varieties; however, unlike the Greeks of the Classical period, who did what we would now call basic algebra in geometrical terms, Diophantus did what we would now call basic algebraic geometry in purely algebraic terms. In modern language, what Diophantus did was to find rational parametrizations of varieties; that is, given an equation of the form (say)
formula_10, his aim was to find (in essence) three rational functions formula_11 such that, for all values of formula_12 and formula_13, setting
formula_14 for formula_15 gives a solution to formula_16

Diophantus also studied the equations of some non-rational curves, for which no rational parametrisation is possible. He managed to find some rational points on these curves (elliptic curves, as it happens, in what seems to be their first known occurrence) by means of what amounts to a tangent construction: translated into coordinate geometry

While Diophantus was concerned largely with rational solutions, he assumed some results on integer numbers, in particular that every integer is the sum of four squares (though he never stated as much explicitly).

While Greek astronomy probably influenced Indian learning, to the point of introducing trigonometry, it seems to be the case that Indian mathematics is otherwise an indigenous tradition; in particular, there is no evidence that Euclid's Elements reached India before the 18th century.

Āryabhaṭa (476–550 CE) showed that pairs of simultaneous congruences formula_17, formula_18 could be solved by a method he called "kuṭṭaka", or "pulveriser"; this is a procedure close to (a generalisation of) the Euclidean algorithm, which was probably discovered independently in India. Āryabhaṭa seems to have had in mind applications to astronomical calculations.

Brahmagupta (628 CE) started the systematic study of indefinite quadratic equations—in particular, the Pell equation, in which Archimedes may have first been interested, and which did not start to be solved in the West until the time of Fermat and Euler. Later Sanskrit authors would follow, using Brahmagupta's technical terminology. A general procedure (the chakravala, or "cyclic method") for solving Pell's equation was finally found by Jayadeva (cited in the eleventh century; his work is otherwise lost); the earliest surviving exposition appears in Bhāskara II's Bīja-gaṇita (twelfth century).

Indian mathematics remained largely unknown in Europe until the late eighteenth century; Brahmagupta and Bhāskara's work was translated into English in 1817 by Henry Colebrooke.

In the early ninth century, the caliph Al-Ma'mun ordered translations of many Greek mathematical works and at least one Sanskrit work (the "Sindhind",
which may or may not be Brahmagupta's Brāhmasphuṭasiddhānta).
Diophantus's main work, the "Arithmetica", was translated into Arabic by Qusta ibn Luqa (820–912).
Part of the treatise "al-Fakhri" (by al-Karajī, 953 – ca. 1029) builds on it to some extent. According to Rashed Roshdi, Al-Karajī's contemporary Ibn al-Haytham knew what would later be called Wilson's theorem.

Other than a treatise on squares in arithmetic progression by Fibonacci—who traveled and studied in north Africa and Constantinople—no number theory to speak of was done in western Europe during the Middle Ages. Matters started to change in Europe in the late Renaissance, thanks to a renewed study of the works of Greek antiquity. A catalyst was the textual emendation and translation into Latin of Diophantus' "Arithmetica".
Pierre de Fermat (1607–1665) never published his writings; in particular, his work on number theory is contained almost entirely in letters to mathematicians and in private marginal notes. In his notes and letters, he scarcely wrote any proofs - he had no models in the area.

Over his lifetime, Fermat made the following contributions to the field:



The interest of Leonhard Euler (1707–1783) in number theory was first spurred in 1729, when a friend of his, the amateur Goldbach, pointed him towards some of Fermat's work on the subject. This has been called the "rebirth" of modern number theory, after Fermat's relative lack of success in getting his contemporaries' attention for the subject. Euler's work on number theory includes the following:


Joseph-Louis Lagrange (1736–1813) was the first to give full proofs of some of Fermat's and Euler's work and observations—for instance, the four-square theorem and the basic theory of the misnamed "Pell's equation" (for which an algorithmic solution was found by Fermat and his contemporaries, and also by Jayadeva and Bhaskara II before them.) He also studied quadratic forms in full generality (as opposed to formula_31)—defining their equivalence relation, showing how to put them in reduced form, etc.

Adrien-Marie Legendre (1752–1833) was the first to state the law of quadratic reciprocity. He also
conjectured what amounts to the prime number theorem and Dirichlet's theorem on arithmetic progressions. He gave a full treatment of the equation formula_32 and worked on quadratic forms along the lines later developed fully by Gauss. In his old age, he was the first to prove "Fermat's last theorem" for formula_33 (completing work by Peter Gustav Lejeune Dirichlet, and crediting both him and Sophie Germain).

In his "Disquisitiones Arithmeticae" (1798), Carl Friedrich Gauss (1777–1855) proved the law of quadratic reciprocity and developed the theory of quadratic forms (in particular, defining their composition). He also introduced some basic notation (congruences) and devoted a section to computational matters, including primality tests. The last section of the "Disquisitiones" established a link between roots of unity and number theory:
The theory of the division of the circle...which is treated in sec. 7 does not belong
by itself to arithmetic, but its principles can only be drawn from higher arithmetic.

In this way, Gauss arguably made a first foray towards both Évariste Galois's work and algebraic number theory.

Starting early in the nineteenth century, the following developments gradually took place:


Algebraic number theory may be said to start with the study of reciprocity and cyclotomy, but truly came into its own with the development of abstract algebra and early ideal theory and valuation theory; see below. A conventional starting point for analytic number theory is Dirichlet's theorem on arithmetic progressions (1837), whose proof introduced L-functions and involved some asymptotic analysis and a limiting process on a real variable. The first use of analytic ideas in number theory actually
goes back to Euler (1730s), who used formal power series and non-rigorous (or implicit) limiting arguments. The use of "complex" analysis in number theory comes later: the work of Bernhard Riemann (1859) on the zeta function is the canonical starting point; Jacobi's four-square theorem (1839), which predates it, belongs to an initially different strand that has by now taken a leading role in analytic number theory (modular forms).

The history of each subfield is briefly addressed in its own section below; see the main article of each subfield for fuller treatments. Many of the most interesting questions in each area remain open and are being actively worked on.

The term "elementary" generally denotes a method that does not use complex analysis. For example, the prime number theorem was first proven using complex analysis in 1896, but an elementary proof was found only in 1949 by Erdős and Selberg. The term is somewhat ambiguous: for example, proofs based on complex Tauberian theorems (for example, Wiener–Ikehara) are often seen as quite enlightening but not elementary, in spite of using Fourier analysis, rather than complex analysis as such. Here as elsewhere, an "elementary" proof may be longer and more difficult for most readers than a non-elementary one.

Number theory has the reputation of being a field many of whose results can be stated to the layperson. At the same time, the proofs of these results are not particularly accessible, in part because the range of tools they use is, if anything, unusually broad within mathematics.

"Analytic number theory" may be defined


Some subjects generally considered to be part of analytic number theory, for example, sieve theory, are better covered by the second rather than the first definition: some of sieve theory, for instance, uses little analysis, yet it does belong to analytic number theory.

The following are examples of problems in analytic number theory: the prime number theorem, the Goldbach conjecture (or the twin prime conjecture, or the Hardy–Littlewood conjectures), the Waring problem and the Riemann hypothesis. Some of the most important tools of analytic number theory are the circle method, sieve methods and L-functions (or, rather, the study of their properties). The theory of modular forms (and, more generally, automorphic forms) also occupies an increasingly central place in the toolbox of analytic number theory.

One may ask analytic questions about algebraic numbers, and use analytic means to answer such questions; it is thus that algebraic and analytic number theory intersect. For example, one may define prime ideals (generalizations of prime numbers in the field of algebraic numbers) and ask how many prime ideals there are up to a certain size. This question can be answered by means of an examination of Dedekind zeta functions, which are generalizations of the Riemann zeta function, a key analytic object at the roots of the subject. This is an example of a general procedure in analytic number theory: deriving information about the distribution of a sequence (here, prime ideals or prime numbers) from the analytic behavior of an appropriately constructed complex-valued function.

An "algebraic number" is any complex number that is a solution to some polynomial equation formula_34 with rational coefficients; for example, every solution formula_35 of formula_36 (say) is an algebraic number. Fields of algebraic numbers are also called "algebraic number fields", or shortly "number fields". Algebraic number theory studies algebraic number fields. Thus, analytic and algebraic number theory can and do overlap: the former is defined by its methods, the latter by its objects of study.

It could be argued that the simplest kind of number fields (viz., quadratic fields) were already studied by Gauss, as the discussion of quadratic forms in "Disquisitiones arithmeticae" can be restated in terms of ideals and
norms in quadratic fields. (A "quadratic field" consists of all
numbers of the form formula_37, where
formula_38 and formula_39 are rational numbers and formula_40
is a fixed rational number whose square root is not rational.)
For that matter, the 11th-century chakravala method amounts—in modern terms—to an algorithm for finding the units of a real quadratic number field. However, neither Bhāskara nor Gauss knew of number fields as such.

The grounds of the subject as we know it were set in the late nineteenth century, when "ideal numbers", the "theory of ideals" and "valuation theory" were developed; these are three complementary ways of dealing with the lack of unique factorisation in algebraic number fields. (For example, in the field generated by the rationals
and formula_41, the number formula_42 can be factorised both as formula_43 and
formula_44; all of formula_45, formula_46, formula_47 and
formula_48
are irreducible, and thus, in a naïve sense, analogous to primes among the integers.) The initial impetus for the development of ideal numbers (by Kummer) seems to have come from the study of higher reciprocity laws, that is, generalisations of quadratic reciprocity.

Number fields are often studied as extensions of smaller number fields: a field "L" is said to be an "extension" of a field "K" if "L" contains "K".
Classifying the possible extensions of a given number field is a difficult and partially open problem. Abelian extensions—that is, extensions "L" of "K" such that the Galois group Gal("L"/"K") of "L" over "K" is an abelian group—are relatively well understood.
Their classification was the object of the programme of class field theory, which was initiated in the late 19th century (partly by Kronecker and Eisenstein) and carried out largely in 1900–1950.

An example of an active area of research in algebraic number theory is Iwasawa theory. The Langlands program, one of the main current large-scale research plans in mathematics, is sometimes described as an attempt to generalise class field theory to non-abelian extensions of number fields.

The central problem of "Diophantine geometry" is to determine when a Diophantine equation has solutions, and if it does, how many. The approach taken is to think of the solutions of an equation as a geometric object.

For example, an equation in two variables defines a curve in the plane. More generally, an equation, or system of equations, in two or more variables defines a curve, a surface or some other such object in "n"-dimensional space. In Diophantine geometry, one asks whether there are any "rational points" (points all of whose coordinates are rationals) or
"integral points" (points all of whose coordinates are integers) on the curve or surface. If there are any such points, the next step is to ask how many there are and how they are distributed. A basic question in this direction is if there are finitely
or infinitely many rational points on a given curve (or surface).

In the Pythagorean equation formula_49
we would like to study its rational solutions, that is, its solutions
formula_50 such that
"x" and "y" are both rational. This is the same as asking for all integer solutions
to formula_51; any solution to the latter equation gives
us a solution formula_52, formula_53 to the former. It is also the
same as asking for all points with rational coordinates on the curve
described by formula_54. (This curve happens to be a circle of radius 1 around the origin.)

The rephrasing of questions on equations in terms of points on curves turns out to be felicitous. The finiteness or not of the number of rational or integer points on an algebraic curve—that is, rational or integer solutions to an equation formula_55, where formula_56 is a polynomial in two variables—turns out to depend crucially on the "genus" of the curve. The "genus" can be defined as follows: allow the variables in formula_55 to be complex numbers; then formula_55 defines a 2-dimensional surface in (projective) 4-dimensional space (since two complex variables can be decomposed into four real variables, that is, four dimensions). If we count the number of (doughnut) holes in the surface; we call this number the "genus" of formula_55. Other geometrical notions turn out to be just as crucial.

There is also the closely linked area of Diophantine approximations: given a number formula_35, then finding how well can it be approximated by rationals. (We are looking for approximations that are good relative to the amount of space that it takes to write the rational: call formula_61 (with formula_62) a good approximation to formula_35 if formula_64, where formula_65 is large.) This question is of special interest if formula_35 is an algebraic number. If formula_35 cannot be well approximated, then some equations do not have integer or rational solutions. Moreover, several concepts (especially that of height) turn out to be critical both in Diophantine geometry and in the study of Diophantine approximations. This question is also of special interest in transcendental number theory: if a number can be better approximated than any algebraic number, then it is a transcendental number. It is by this argument that and e have been shown to be transcendental.

Diophantine geometry should not be confused with the geometry of numbers, which is a collection of graphical methods for answering certain questions in algebraic number theory. "Arithmetic geometry", however, is a contemporary term
for much the same domain as that covered by the term "Diophantine geometry". The term "arithmetic geometry" is arguably used
most often when one wishes to emphasise the connections to modern algebraic geometry (as in, for instance, Faltings's theorem) rather than to techniques in Diophantine approximations.

The areas below date from no earlier than the mid-twentieth century, even if they are based on older material. For example, as is explained below, the matter of algorithms in number theory is very old, in some sense older than the concept of proof; at the same time, the modern study of computability dates only from the 1930s and 1940s, and computational complexity theory from the 1970s.

Much of probabilistic number theory can be seen as an important special case of the study of variables that are almost, but not quite, mutually independent. For example, the event that a random integer between one and a million be divisible by two and the event that it be divisible by three are almost independent, but not quite.

It is sometimes said that probabilistic combinatorics uses the fact that whatever happens with probability greater than formula_68 must happen sometimes; one may say with equal justice that many applications of probabilistic number theory hinge on the fact that whatever is unusual must be rare. If certain algebraic objects (say, rational or integer solutions to certain equations) can be shown to be in the tail of certain sensibly defined distributions, it follows that there must be few of them; this is a very concrete non-probabilistic statement following from a probabilistic one.

At times, a non-rigorous, probabilistic approach leads to a number of heuristic algorithms and open problems, notably Cramér's conjecture.

If we begin from a fairly "thick" infinite set formula_69, does it contain many elements in arithmetic progression: formula_38,
formula_71, say? Should it be possible to write large integers as sums of elements of formula_69?

These questions are characteristic of "arithmetic combinatorics". This is a presently coalescing field; it subsumes "additive number theory" (which concerns itself with certain very specific sets formula_69 of arithmetic significance, such as the primes or the squares) and, arguably, some of the "geometry of numbers",
together with some rapidly developing new material. Its focus on issues of growth and distribution accounts in part for its developing links with ergodic theory, finite group theory, model theory, and other fields. The term "additive combinatorics" is also used; however, the sets formula_69 being studied need not be sets of integers, but rather subsets of non-commutative groups, for which the multiplication symbol, not the addition symbol, is traditionally used; they can also be subsets of rings, in which case the growth of formula_75 and formula_69·formula_69 may be
compared.

While the word "algorithm" goes back only to certain readers of al-Khwārizmī, careful descriptions of methods of solution are older than proofs: such methods (that is, algorithms) are as old as any recognisable mathematics—ancient Egyptian, Babylonian, Vedic, Chinese—whereas proofs appeared only with the Greeks of the classical period.
An interesting early case is that of what we now call the Euclidean algorithm. In its basic form (namely, as an algorithm for computing the greatest common divisor) it appears as Proposition 2 of Book VII in "Elements", together with a proof of correctness. However, in the form that is often used in number theory (namely, as an algorithm for finding integer solutions to an equation formula_78,
or, what is the same, for finding the quantities whose existence is assured by the Chinese remainder theorem) it first appears in the works of Āryabhaṭa (5th–6th century CE) as an algorithm called
"kuṭṭaka" ("pulveriser"), without a proof of correctness.

There are two main questions: "Can we compute this?" and "Can we compute it rapidly?" Anyone can test whether a number is prime or, if it is not, split it into prime factors; doing so rapidly is another matter. We now know fast algorithms for testing primality, but, in spite of much work (both theoretical and practical), no truly fast algorithm for factoring.

The difficulty of a computation can be useful: modern protocols for encrypting messages (for example, RSA) depend on functions that are known to all, but whose inverses are known only to a chosen few, and would take one too long a time to figure out on one's own. For example, these functions can be such that their inverses can be computed only if certain large integers are factorized. While many difficult computational problems outside number theory are known, most working encryption protocols nowadays are based on the difficulty of a few number-theoretical problems.

Some things may not be computable at all; in fact, this can be proven in some instances. For instance, in 1970, it was proven, as a solution to Hilbert's 10th problem, that there is no Turing machine which can solve all Diophantine equations. In particular, this means that, given a computably enumerable set of axioms, there are Diophantine equations for which there is no proof, starting from the axioms, of whether the set of equations has or does not have integer solutions. (We would necessarily be speaking of Diophantine equations for which there are no integer solutions, since, given a Diophantine equation with at least one solution, the solution itself provides a proof of the fact that a solution exists. We cannot prove that a particular Diophantine equation is of this kind, since this would imply that it has no solutions.)

The number-theorist Leonard Dickson (1874–1954) said "Thank God that number theory is unsullied by any application". Such a view is no longer applicable to number theory. In 1974, Donald Knuth said "...virtually every theorem in elementary number theory arises in a natural, motivated way in connection with the problem of making computers do high-speed numerical calculations".
Elementary number theory is taught in discrete mathematics courses for computer scientists; on the other hand, number theory also has applications to the continuous in numerical analysis. As well as the well-known applications to cryptography, there are also applications to many other areas of mathematics.

The American Mathematical Society awards the "Cole Prize in Number Theory". Moreover number theory is one of the three mathematical subdisciplines rewarded by the "Fermat Prize".


 

Two of the most popular introductions to the subject are:

Hardy and Wright's book is a comprehensive classic, though its clarity sometimes suffers due to the authors' insistence on elementary methods (Apostol n.d.).
Vinogradov's main attraction consists in its set of problems, which quickly lead to Vinogradov's own research interests; the text itself is very basic and close to minimal. Other popular first introductions are:

Popular choices for a second textbook include:



</doc>
<doc id="21530" url="https://en.wikipedia.org/wiki?curid=21530" title="Nitroglycerin">
Nitroglycerin

Nitroglycerin (NG), also known as nitroglycerine, trinitroglycerin (TNG), nitro, glyceryl trinitrate (GTN), or 1,2,3-trinitroxypropane, is a dense, colorless, oily, explosive liquid most commonly produced by nitrating glycerol with white fuming nitric acid under conditions appropriate to the formation of the nitric acid ester. Chemically, the substance is an organic nitrate compound rather than a nitro compound, yet the traditional name is often retained. Invented in 1847, nitroglycerin has been used ever since as an active ingredient in the manufacture of explosives, mostly dynamite, and as such it is employed in the construction, demolition, and mining industries. Since the 1880s, it has been used by the military as an active ingredient, and a gelatinizer for nitrocellulose, in some solid propellants, such as cordite and ballistite.

Nitroglycerin is a major component in double-based smokeless gunpowders used by reloaders. Combined with nitrocellulose, hundreds of powder combinations are used by rifle, pistol, and shotgun reloaders.

In medicine for over 130 years, nitroglycerin has been used as a potent vasodilator (dilation of the vascular system) to treat heart conditions, such as angina pectoris and chronic heart failure. Though it was previously known that these beneficial effects are due to nitroglycerin being converted to nitric oxide, a potent venodilator, the enzyme for this conversion was not discovered to be mitochondrial aldehyde dehydrogenase (ALDH2) until 2002. Nitroglycerin is available in sublingual tablets, sprays, ointments, and patches.

Nitroglycerin was the first practical explosive produced that was stronger than black powder. It was first synthesized by the Italian chemist Ascanio Sobrero in 1847, working under Théophile-Jules Pelouze at the University of Turin. Sobrero initially called his discovery "pyroglycerine" and warned vigorously against its use as an explosive.

Nitroglycerin was later adopted as a commercially useful explosive by Alfred Nobel, who experimented with safer ways to handle the dangerous compound after his younger brother, Emil Oskar Nobel, and several factory workers were killed in an explosion at the Nobels' armaments factory in 1864 in Heleneborg, Sweden.

One year later, Nobel founded Alfred Nobel and Company in Germany and built an isolated factory in the Krümmel hills of Geesthacht near Hamburg. This business exported a liquid combination of nitroglycerin and gunpowder called "Blasting Oil", but this was extremely unstable and difficult to handle, as evidenced in numerous catastrophes. The buildings of the Krümmel factory were destroyed twice.

In April 1866, three crates of nitroglycerin were shipped to California for the Central Pacific Railroad, which planned to experiment with it as a blasting explosive to expedite the construction of the Summit Tunnel through the Sierra Nevada Mountains. One of the crates exploded, destroying a Wells Fargo company office in San Francisco and killing 15 people. This led to a complete ban on the transportation of liquid nitroglycerin in California. The on-site manufacture of nitroglycerin was thus required for the remaining hard-rock drilling and blasting required for the completion of the First Transcontinental Railroad in North America.

In June 1869, two one-ton wagons loaded with nitroglycerin, then known locally as Powder-Oil, exploded in the road at the North Wales village of Cwm-Y-Glo. The explosion led to the loss of six lives, many injuries and much damage to the village. Little trace was found of the two horses. The UK Government was so alarmed at the damage caused and what could have happened in a city location (these two tons were part of a larger load coming from Germany via Liverpool) that they soon passed The Nitro-Glycerine Act of 1869. Liquid nitroglycerin was widely banned elsewhere, as well, and these legal restrictions led to Alfred Nobel and his company's developing dynamite in 1867. This was made by mixing nitroglycerin with diatomaceous earth (""Kieselguhr"" in German) found in the Krümmel hills. Similar mixtures, such as "dualine" (1867), "lithofracteur" (1869), and "gelignite" (1875), were formed by mixing nitroglycerin with other inert absorbents, and many combinations were tried by other companies in attempts to get around Nobel's tightly held patents for dynamite.

Dynamite mixtures containing nitrocellulose, which increases the viscosity of the mix, are commonly known as "gelatins".

Following the discovery that amyl nitrite helped alleviate chest pain, the physician William Murrell experimented with the use of nitroglycerin to alleviate angina pectoris and to reduce the blood pressure. He began treating his patients with small diluted doses of nitroglycerin in 1878, and this treatment was soon adopted into widespread use after Murrell published his results in the journal "The Lancet" in 1879. A few months before his death in 1896, Alfred Nobel was prescribed nitroglycerin for this heart condition, writing to a friend: "Isn't it the irony of fate that I have been prescribed nitro-glycerin, to be taken internally! They call it Trinitrin, so as not to scare the chemist and the public." The medical establishment also used the name "glyceryl trinitrate" for the same reason.

Large quantities of nitroglycerin were manufactured during World War I and World War II for use as military propellants and in military engineering work. During World War I, HM Factory, Gretna, the largest propellant factory in Britain, produced about 800 tonnes of cordite RDB per week. This amount required at least 336 tonnes of nitroglycerin per week (assuming no losses in production). The Royal Navy had its own factory at the Royal Navy Cordite Factory, Holton Heath, in Dorset, England. A large cordite factory was also built in Canada during World War I. The Canadian Explosives Limited cordite factory at Nobel, Ontario, was designed to produce of cordite per month, requiring about 286 tonnes of nitroglycerin per month.

In its pure form, nitroglycerin is a contact explosive, with physical shock causing it to explode, and it degrades over time to even more unstable forms. This makes nitroglycerin highly dangerous to transport or use. In its undiluted form, it is one of the world's most powerful explosives, comparable to the more recently developed RDX and PETN.

Early in its history, liquid nitroglycerin was found to be "desensitized" by cooling it to about . At this temperature, nitroglycerin freezes, contracting upon solidification. Thawing it out can be extremely sensitizing, especially if impurities are present or the warming is too rapid. Chemically "desensitizing" nitroglycerin is possible to a point where it can be considered about as "safe" as modern high explosives, such as by the addition of roughly 10 to 30% ethanol, acetone, or dinitrotoluene. (The percentage varies with the desensitizing agent used.) Desensitization requires extra effort to reconstitute the "pure" product. Failing this, desensitized nitroglycerin must be assumed to be substantially more difficult to detonate, possibly rendering it useless as an explosive for practical application.

A serious problem in the use of nitroglycerin results from its high freezing point of . Solid nitroglycerin is much less sensitive to shock than the liquid, a common feature in explosives. In the past, nitroglycerin was often shipped in the frozen state, but this resulted in a high number of accidents during the thawing process just before its use. This disadvantage is overcome by using mixtures of nitroglycerin with other polynitrates. For example, a mixture of nitroglycerin and ethylene glycol dinitrate freezes at .

Nitroglycerin and any diluents can certainly deflagrate (burn). The explosive power of nitroglycerin derives from detonation: energy from the initial decomposition causes a strong pressure wave that detonates the surrounding fuel. This is a self-sustained shock wave that propagates through the explosive medium at 30 times the speed of sound as a near-instantaneous pressure-induced decomposition of the fuel into a white-hot gas. Detonation of nitroglycerin generates gases that would occupy more than 1,200 times the original volume at ordinary room temperature and pressure. The heat liberated raises the temperature to about . This is entirely different from deflagration, which depends solely upon available fuel regardless of pressure or shock. The decomposition results in much higher ratio of energy to gas moles released compared to other explosives, making it one of the hottest detonating high explosives.

Nitroglycerin can be produced by acid-catalyzed nitration of glycerol (glycerin).

The industrial manufacturing process often reacts glycerol with a nearly 1:1 mixture of concentrated sulfuric acid and concentrated nitric acid. This can be produced by mixing white fuming nitric acid—a quite expensive pure nitric acid in which the oxides of nitrogen have been removed, as opposed to red fuming nitric acid, which contains nitrogen oxides—and concentrated sulfuric acid. More often, this mixture is attained by the cheaper method of mixing fuming sulfuric acid, also known as oleum—sulfuric acid containing excess sulfur trioxide—and azeotropic nitric acid (consisting of about 70% nitric acid, with the rest being water).

The sulfuric acid produces protonated nitric acid species, which are attacked by glycerol's nucleophilic oxygen atoms. The nitro group is thus added as an ester C−O−NO and water is produced. This is different from an electrophilic aromatic substitution reaction in which nitronium ions are the electrophile.

The addition of glycerol results in an exothermic reaction (i.e., heat is produced), as usual for mixed-acid nitrations. If the mixture becomes too hot, it results in a runaway reaction, a state of accelerated nitration accompanied by the destructive oxidation of organic materials by the hot nitric acid and the release of poisonous nitrogen dioxide gas at high risk of an explosion. Thus, the glycerin mixture is added slowly to the reaction vessel containing the mixed acid (not acid to glycerin). The nitrator is cooled with cold water or some other coolant mixture and maintained throughout the glycerin addition at about , much below which the esterification occurs too slowly to be useful. The nitrator vessel, often constructed of iron or lead and generally stirred with compressed air, has an emergency trap door at its base, which hangs over a large pool of very cold water and into which the whole reaction mixture (called the charge) can be dumped to prevent an explosion, a process referred to as drowning. If the temperature of the charge exceeds about (actual value varying by country) or brown fumes are seen in the nitrator's vent, then it is immediately drowned.

The main use of nitroglycerin, by tonnage, is in explosives such as dynamite and in propellants.

Nitroglycerin is an oily liquid that may explode when subjected to heat, shock, or flame.

Alfred Nobel developed the use of nitroglycerin as a blasting explosive by mixing nitroglycerin with inert absorbents, particularly ""Kieselguhr"", or diatomaceous earth. He named this explosive dynamite and patented it in 1867. It was supplied ready for use in the form of sticks, individually wrapped in greased waterproof paper. Dynamite and similar explosives were widely adopted for civil engineering tasks, such as in drilling highway and railroad tunnels, for mining, for clearing farmland of stumps, in quarrying, and in demolition work. Likewise, military engineers have used dynamite for construction and demolition work.

Nitroglycerin was also used as an ingredient in military propellants for use in firearms.

Nitroglycerin has been used in conjunction with hydraulic fracturing, a process used to recover oil and gas from shale formations. The technique involves displacing and detonating nitroglycerin in natural or hydraulically induced fracture systems, or displacing and detonating nitroglycerin in hydraulically induced fractures followed by wellbore shots using pelletized TNT.

Nitroglycerin has an advantage over some other high explosives that on detonation it produces practically no visible smoke. Therefore, it is useful as an ingredient in the formulation of various kinds of smokeless powder.

Its sensitivity has limited the usefulness of nitroglycerin as a military explosive, and less sensitive explosives such as TNT, RDX, and HMX have largely replaced it in munitions. It remains important in military engineering, and combat engineers still use dynamite.

Alfred Nobel then developed ballistite, by combining nitroglycerin and guncotton. He patented it in 1887. Ballistite was adopted by a number of European governments, as a military propellant. Italy was the first to adopt it. The British government and the Commonwealth governments adopted cordite instead, which had been developed by Sir Frederick Abel and Sir James Dewar of the United Kingdom in 1889. The original Cordite Mk I consisted of 58% nitroglycerin, 37% guncotton, and 5.0% petroleum jelly. Ballistite and cordite were both manufactured in the forms of "cords".

Smokeless powders were originally developed using nitrocellulose as the sole explosive ingredient. Therefore, they were known as single-base propellants. A range of smokeless powders that contains both nitrocellulose and nitroglycerin, known as double-base propellants, were also developed. Smokeless powders were originally supplied only for military use, but they were also soon developed for civilian use and were quickly adopted for sports. Some are known as sporting powders. Triple-base propellants contain nitrocellulose, nitroglycerin, and nitroguanidine, but are reserved mainly for extremely high-caliber ammunition rounds such as those used in tank cannons and naval artillery. Blasting gelatin, also known as gelignite, was invented by Nobel in 1875, using nitroglycerin, wood pulp, and sodium or potassium nitrate. This was an early, low-cost, flexible explosive.

Nitroglycerin belongs to a group of drugs called nitrates, which includes many other nitrates like isosorbide dinitrate (Isordil) and isosorbide mononitrate (Imdur, Ismo, Monoket). These agents all exert their effect by being converted to nitric oxide in the body by mitochondrial aldehyde dehydrogenase (ALDH2), and nitric oxide is a potent natural vasodilator.
In medicine, nitroglycerin is used for angina pectoris, a painful symptom of ischemic heart disease caused by inadequate flow of blood and oxygen to the heart and as a potent antihypertensive agent. Nitroglycerin corrects the imbalance between the flow of oxygen and blood to the heart. At low doses, nitroglycerin dilates veins more than arteries, thereby reducing preload (volume of blood in the heart after filling); this is thought to be its primary mechanism of action. By decreasing preload, the heart has less blood to pump, which decreases oxygen requirement since the heart does not have to work as hard. Additionally, having a smaller preload reduces the ventricular transmural pressure (pressure exerted on the walls of the heart), which decreases the compression of heart arteries to allow more blood to flow through the heart. At higher doses, it also dilates arteries, thereby reducing afterload (decreasing the pressure against which the heart must pump). Improved myocardial oxygen demand vs oxygen delivery ratio leads to the following therapeutic effects during episodes of angina pectoris: subsiding of chest pain, decrease of blood pressure, increase of heart rate, and orthostatic hypotension. Patients experiencing angina when doing certain physical activities can often prevent symptoms by taking nitroglycerin 5 to 10 minutes before the activity. Overdoses may generate methemoglobinemia.

Nitroglycerin is available in tablets, ointment, solution for intravenous use, transdermal patches, or sprays administered sublingually. Some forms of nitroglycerin last much longer in the body than others. Continuous exposure to nitrates has been shown to cause the body to stop responding normally to this medicine. Experts recommend that the patches be removed at night, allowing the body a few hours to restore its responsiveness to nitrates. Shorter-acting preparations of nitroglycerin can be used several times a day with less risk of developing tolerance. Nitroglycerin was first used by William Murrell to treat angina attacks in 1878, with the discovery published that same year.

Infrequent exposure to high doses of nitroglycerin can cause severe headaches known as "NG head" or "bang head". These headaches can be severe enough to incapacitate some people; however, humans develop a tolerance to and dependence on nitroglycerin after long-term exposure. Although rare, withdrawal can be fatal. Withdrawal symptoms include chest pain and other heart problems. These symptoms may be relieved with re-exposure to nitroglycerin or other suitable organic nitrates.

For workers in nitroglycerin (NTG) manufacturing facilities, the effects of withdrawal sometimes include "Sunday heart attacks" in those experiencing regular nitroglycerin exposure in the workplace, leading to the development of tolerance for the venodilating effects. Over the weekend, the workers lose the tolerance, and when they are re-exposed on Monday, the drastic vasodilation produces a fast heart rate, dizziness, and a headache, this is referred to as "Monday disease."

People can be exposed to nitroglycerin in the workplace by breathing it in, skin absorption, swallowing it, or eye contact. The Occupational Safety and Health Administration has set the legal limit (permissible exposure limit) for nitroglycerin exposure in the workplace as 0.2 ppm (2 mg/m) skin exposure over an 8-hour workday. The National Institute for Occupational Safety and Health has set a recommended exposure limit of 0.1 mg/m skin exposure over an 8-hour workday. At levels of 75 mg/m, nitroglycerin is immediately dangerous to life and health.



</doc>
<doc id="21533" url="https://en.wikipedia.org/wiki?curid=21533" title="Navy">
Navy

A navy or sea force is the branch of a nation's armed forces principally designated for naval and amphibious warfare; namely, lake-borne, riverine, littoral, or ocean-borne combat operations and related functions. It includes anything conducted by surface ships, amphibious ships, submarines, and seaborne aviation, as well as ancillary support, communications, training, and other fields. The strategic offensive role of a navy is projection of force into areas beyond a country's shores (for example, to protect sea-lanes, deter or confront piracy, ferry troops, or attack other navies, ports, or shore installations). The strategic defensive purpose of a navy is to frustrate seaborne projection-of-force by enemies. The strategic task of the navy also may incorporate nuclear deterrence by use of submarine-launched ballistic missiles. Naval operations can be broadly divided between riverine and littoral applications (brown-water navy), open-ocean applications (blue-water navy), and something in between (green-water navy), although these distinctions are more about strategic scope than tactical or operational division.

In most nations, the term "naval", as opposed to "navy", is interpreted as encompassing all maritime military forces, e.g., navy, naval infantry/marine corps, and coast guard forces.

First attested in English in the early 14th century, the word "navy" came via Old French "navie", "fleet of ships", from the Latin "navigium", "a vessel, a ship, bark, boat", from "navis", "ship". The word "naval" came from Latin "navalis", "pertaining to ship"; cf. Greek ("naus"), "ship", ("nautes"), "seaman, sailor". The earliest attested form of the word is in the Mycenaean Greek compound word , "na-u-do-mo" (*), "shipbuilders", written in Linear B syllabic script.

The word formerly denoted fleets of both commercial and military nature. In modern usage "navy" used alone always denotes a military fleet, although the term "merchant navy" for a commercial fleet still incorporates the non-military word sense. This overlap in word senses between commercial and military fleets grew out of the inherently dual-use nature of fleets; centuries ago, nationality was a trait that unified a fleet across both civilian and military uses. Although nationality of commercial vessels has little importance in peacetime trade other than for tax avoidance, it can have greater meaning during wartime, when supply chains become matters of patriotic attack and defense, and when in some cases private vessels are even temporarily converted to military vessels. The latter was especially important, and common, before 20th-century military technology existed, when merely adding artillery and naval infantry to any sailing vessel could render it fully as martial as any military-owned vessel. Such privateering has been rendered obsolete in blue-water strategy since modern missile and aircraft systems grew to leapfrog over artillery and infantry in many respects; but privateering nevertheless remains potentially relevant in littoral warfare of a limited and asymmetric nature.

Naval warfare developed when humans first fought from water-borne vessels. Before the introduction of the cannon and ships with enough capacity to carry them, navy warfare primarily involved ramming and boarding actions. In the time of ancient Greece and the Roman Empire, naval warfare centered on long, narrow vessels powered by banks of oarsmen (such as triremes and quinqueremes) designed to ram and sink enemy vessels or come alongside the enemy vessel so its occupants could be attacked hand-to-hand. Naval warfare continued in this vein through the Middle Ages until the cannon became commonplace and capable of being reloaded quickly enough to be reused in the same battle. The Chola Dynasty of medieval Tamil Nadu was known as one of the greatest naval powers of its time from 300 BC to 1279 AD. The Chola Navy, Chola kadarpadai comprised the naval forces of the Chola Empire along with several other Naval-arms of the country. The Chola navy played a vital role in the expansion of the Chola Tamil kingdom, including the conquest of the Sri Lanka islands, Kadaaram (Present day Burma), Sri Vijaya (present day Southeast Asia), the spread of Hinduism, Tamil architecture and Tamil culture to Southeast Asia and in curbing the piracy in Southeast Asia in 900 CE. In ancient China, large naval battles were known since the Qin dynasty ("also see" Battle of Red Cliffs, 208), employing the war junk during the Han dynasty. However, China's first official standing navy was not established until the Southern Song dynasty in the 12th century, a time when gunpowder was a revolutionary new application to warfare.

Nusantaran thalassocracies made extensive use of naval power and technologies. This enabled the seafaring Malay people to attack as far as the coast of Tanganyika and Mozambique with 1000 boats and attempted to take the citadel of Qanbaloh, about 7,000 km to their West, in 945-946 AD. In 1350 AD Majapahit launched its largest military expedition, the invasion of Pasai, with 400 large jong and innumerable smaller vessels. The second largest military expedition, invasion of Singapura in 1398, Majapahit deployed 300 jong with no less than 200,000 men.

The mass and deck space required to carry a large number of cannon made oar-based propulsion impossible, and ships came to rely primarily on sails. Warships were designed to carry increasing numbers of cannon and naval tactics evolved to bring a ship's firepower to bear in a broadside, with ships-of-the-line arranged in a line of battle.

The development of large capacity, sail-powered ships carrying cannon led to a rapid expansion of European navies, especially the Spanish and Portuguese navies which dominated in the 16th and early 17th centuries, and helped propel the age of exploration and colonialism. The repulsion of the Spanish Armada (1588) by the English fleet revolutionized naval warfare by the success of a guns-only strategy and caused a major overhaul of the Spanish Navy, partly along English lines, which resulted in even greater dominance by the Spanish. From the beginning of the 17th century the Dutch cannibalized the Portuguese Empire in the East and, with the immense wealth gained, challenged Spanish hegemony at sea. From the 1620s, Dutch raiders seriously troubled Spanish shipping and, after a number of battles which went both ways, the Dutch Navy finally broke the long dominance of the Spanish Navy in the Battle of the Downs (1639). England emerged as a major naval power in the mid-17th century in the first Anglo-Dutch war with a technical victory. Successive decisive Dutch victories in the second and third Anglo-Dutch Wars confirmed the Dutch mastery of the seas during the Dutch Golden Age, financed by the expansion of the Dutch Empire. The French Navy won some important victories near the end of the 17th century but a focus upon land forces led to the French Navy's relative neglect, which allowed the Royal Navy to emerge with an ever-growing advantage in size and quality, especially in tactics and experience, from 1695. As a response to growing naval influence of the navies of Portuguese, the warrior king of the Marathas, Shivaji laid the foundation of the Maratha navy in 1654. 

Throughout the 18th century the Royal Navy gradually gained ascendancy over the French Navy, with victories in the War of Spanish Succession (1701–1714), inconclusive battles in the War of Austrian Succession (1740–1748), victories in the Seven Years' War (1754–1763), a partial reversal during the American War of Independence (1775–1783), and consolidation into uncontested supremacy during the 19th century from the Battle of Trafalgar in 1805. These conflicts saw the development and refinement of tactics which came to be called the line of battle.

The next stage in the evolution of naval warfare was the introduction of metal plating along the hull sides. The increased mass required steam-powered engines, resulting in an arms race between armor and weapon thickness and firepower. The first armored vessels, the French and British , made wooden vessels obsolete. Another significant improvement came with the invention of the rotating turrets, which allowed the guns to be aimed independently of ship movement. The battle between and during the American Civil War (1861–1865) is often cited as the beginning of this age of maritime conflict. The Russian Navy was considered the third strongest in the world on the eve of the Russo-Japanese War, which turned to be a catastrophe for the Russian military in general and the Russian Navy in particular. Although neither party lacked courage, the Russians were defeated by the Japanese in the Battle of Port Arthur, which was the first time in warfare that mines were used for offensive purposes. The warships of the Baltic Fleet sent to the Far East were lost in the Battle of Tsushima. A further step change in naval firepower occurred when the United Kingdom launched in 1906, but naval tactics still emphasized the line of battle.

The first practical military submarines were developed in the late 19th century and by the end of World War I had proven to be a powerful arm of naval warfare. During World War II, Nazi Germany's submarine fleet of U-boats almost starved the United Kingdom into submission and inflicted tremendous losses on U.S. coastal shipping. The , a sister ship of , was almost put out of action by miniature submarines known as X-Craft. The X-Craft severely damaged her and kept her in port for some months.

A major paradigm shift in naval warfare occurred with the introduction of the aircraft carrier. First at Taranto in 1940 and then at Pearl Harbor in 1941, the carrier demonstrated its ability to strike decisively at enemy ships out of sight and range of surface vessels. The Battle of Leyte Gulf (1944) was arguably the largest naval battle in history; it was also the last battle in which battleships played a significant role. By the end of World War II, the carrier had become the dominant force of naval warfare.

World War II also saw the United States become by far the largest Naval power in the world. In the late 20th and early 21st centuries, the United States Navy possessed over 70% of the world's total numbers and total tonnage of naval vessels of 1,000 tons or greater. Throughout the rest of the 20th century, the United States Navy would maintain a tonnage greater than that of the next 17 largest navies combined. During the Cold War, the Soviet Navy became a significant armed force, with large numbers of large, heavily armed ballistic missile submarines and extensive use of heavy, long-ranged antisurface missiles to counter the numerous United States carrier battle groups. Only three nations (United States, France, and Brazil) presently operate CATOBAR carriers of any size, while Russia, China and India operate sizeable STOBAR carriers (although all three are originally of Russian design). The United Kingdom is also currently constructing two carriers, which will be the largest STOVL vessels in service, and India is currently building one aircraft carrier, , and considering another. France is also looking at a new carrier, probably using a CATOBAR system and possibly based on the British "Queen Elizabeth" design.

A navy typically operates from one or more naval bases. The base is a port that is specialized in naval operations, and often includes housing, a munitions depot, docks for the vessels, and various repair facilities. During times of war temporary bases may be constructed in closer proximity to strategic locations, as it is advantageous in terms of patrols and station-keeping. Nations with historically strong naval forces have found it advantageous to obtain basing rights in other countries in areas of strategic interest.

Navy ships can operate independently or with a group, which may be a small squadron of comparable ships, or a larger naval fleet of various specialized ships. The commander of a fleet travels in the flagship, which is usually the most powerful vessel in the group. Before radio was invented, commands from the flagship were communicated by means of flags. At night signal lamps could be used for a similar purpose. Later these were replaced by the radio transmitter, or the flashing light when radio silence was needed.

A "blue water navy" is designed to operate far from the coastal waters of its home nation. These are ships capable of maintaining station for long periods of time in deep ocean, and will have a long logistical tail for their support. Many are also nuclear powered to save having to refuel. By contrast a "brown water navy" operates in the coastal periphery and along inland waterways, where larger ocean-going naval vessels can not readily enter. Regional powers may maintain a "green water navy" as a means of localized force projection. Blue water fleets may require specialized vessels, such as minesweepers, when operating in the littoral regions along the coast.

A basic tradition is that all ships commissioned in a navy are referred to as ships rather than vessels, with the exception of destroyers and submarines, which are known as boats. The prefix on a ship's name indicates that it is a commissioned ship.

An important tradition on board naval vessels of some nations has been the ship's bell. This was historically used to mark the passage of time, as warning devices in heavy fog, and for alarms and ceremonies.

The ship's captain, and more senior officers are "piped" aboard the ship using a Boatswain's call.

In the United States, the First Navy Jack is a flag that has the words, "Don't Tread on Me" on the flag.

By English tradition, ships have been referred to as a "she". However, it was long considered bad luck to permit women to sail on board naval vessels. To do so would invite a terrible storm that would wreck the ship. The only women that were welcomed on board were figureheads mounted on the prow of the ship.

Firing a cannon salute partially disarms the ship, so firing a cannon for no combat reason showed respect and trust. As the tradition evolved, the number of cannon fired became an indication of the rank of the official being saluted.

Historically, navy ships were primarily intended for warfare. They were designed to withstand damage and to inflict the same, but only carried munitions and supplies for the voyage (rather than merchant cargo). Often, other ships which were not built specifically for warfare, such as the galleon or the armed merchant ships in World War II, did carry armaments. In more recent times, navy ships have become more specialized and have included supply ships, troop transports, repair ships, oil tankers and other logistics support ships as well as combat ships.

Modern navy combat ships are generally divided into seven main categories: aircraft carriers, cruisers, destroyers, frigates, corvettes, submarines, and amphibious assault ships. There are also support and auxiliary ships, including the oiler, minesweeper, patrol boat, hydrographic and oceanographic survey ship and tender. During the age of sail, the ship categories were divided into the ship of the line, frigate, and sloop-of-war.

Naval ship names are typically prefixed by an abbreviation indicating the national navy in which they serve. For a list of the prefixes used with ship names (HMS, USS, LÉ, etc.) see ship prefix.

Today ships are significantly faster than in former times, thanks to much improved propulsion systems. Also, the efficiency of the engines has improved, in terms of fuel, and of how many sailors it takes to operate them. In World War II, ships needed to refuel very often. However, today ships can go on very long journeys without refueling. Also, in World War II, the engine room needed about a dozen sailors to work the many engines, however, today, only about 4–5 are needed (depending on the class of the ship). Today, naval strike groups on longer missions are always followed by a range of support and replenishment ships supplying them with anything from fuel and munitions, to medical treatment and postal services. This allows strike groups and combat ships to remain at sea for several months at a time.

The term "boat" refers to small craft limited in their use by size and usually not capable of making lengthy independent voyages at sea. The old navy adage to differentiate between ships and boats is that boats are capable of being carried by ships. (Submarines by this rule are ships rather than boats, but are customarily referred to as boats reflecting their previous smaller size.)

Navies use many types of boat, ranging from dinghies to landing craft. They are powered by either diesel engines, out-board gasoline engines, or waterjets. Most boats are built of aluminum, fiberglass, or steel. Rigid-hulled inflatable boats are also used.

Patrol boats are used for patrols of coastal areas, lakes and large rivers.
Landing craft are designed to carry troops, vehicles, or cargo from ship to shore under combat conditions, to unload, to withdraw from the beach, and to return to the ship. They are rugged, with powerful engines, and usually armed. There are many types in today's navies including hovercraft. They will typically have a power-operated bow ramp, a cargo well and after structures that house engine rooms, pilot houses, and stowage compartments. These boats are sometimes carried by larger ships.

Special operations craft are high-speed craft used for insertion and extraction of special forces personnel and some may be transportable (and deployed) by air.

Boats used in non-combat roles include lifeboats, mail boats, line handling boats, buoy boats, aircraft rescue boats, torpedo retrievers, explosive ordnance disposal craft, utility boats, dive boats, targets, and work boats. Boats are also used for survey work, tending divers, and minesweeping operations. Boats for carrying cargo and personnel are sometimes known as launches, gigs, barges or shore party boats.

Naval forces are typically arranged into units based on the number of ships included, a single ship being the smallest operational unit. Ships may be combined into squadrons or flotillas, which may be formed into fleets. The largest unit size may be the whole Navy or Admiralty.

A task force can be assembled using ships from different fleets for an operational task.

Despite their acceptance in many areas of naval service, female sailors were not permitted to serve on board U.S. submarines until the U.S. Navy lifted the ban in April 2010. The major reasons historically cited by the U.S. Navy were the extended duty tours and close conditions which afford almost no privacy. The United Kingdom's Royal Navy has had similar restrictions. Australia, Canada, Norway, and Spain previously opened submarine service to women sailors.

A navy will typically have two sets of ranks, one for enlisted personnel and one for officers.

Typical ranks for commissioned officers include the following, in ascending order (Commonwealth ranks are listed first on each line; USA ranks are listed second in those instances where they differ from Commonwealth ranks):

"Flag officers" include any rank that includes the word "admiral" (or commodore in services other than the US Navy), and are generally in command of a battle group, strike group or similar flotilla of ships, rather than a single ship or aspect of a ship. However, commodores can also be temporary or honorary positions. For example, during World War II, a Navy captain was assigned duty as a convoy commodore, which meant that he was still a captain, but in charge of all the merchant vessels in the convoy.

The most senior rank employed by a navy will tend to vary depending on the size of the navy and whether it is wartime or peacetime, for example, few people have ever held the rank of Fleet Admiral in the U.S. Navy, the chief of the Royal Australian Navy holds the rank of Vice Admiral, and the chief of the Irish Naval Service holds the rank of Commodore.

Naval infantry, commonly known as marines, are a category of infantry that form part of a state's naval forces and perform roles on land and at sea, including amphibious operations, as well as other, naval roles. They also perform other tasks, including land warfare, separate from naval operations.

During the era of the Roman empire, naval forces included marine legionaries for maritime boarding actions. These were troops primarily trained in land warfare, and did not need to be skilled at handling a ship. Much later during the age of sail, a component of marines served a similar role, being ship-borne soldiers who were used either during boarding actions, as sharp-shooters, or in raids along shorelines.

The Spanish "Infantería de Marina" was formed in 1537, making it the oldest, current marine force in the world. The British Royal Marines combine being both a ship-based force and also being specially trained in commando-style operations and tactics, operating in some cases separately from the rest of the Royal Navy. The Royal Marines also have their own special forces unit.

In the majority of countries, the marine force is an integral part of the navy. The United States Marine Corps is a separate armed service within the United States Department of the Navy, with its own leadership structure.

Naval aviation is the application of military air power by navies, whether from warships that embark aircraft, or land bases.

In World War I several navies used floatplanes and flying boats - mainly for scouting. By World War II, aircraft carriers could carry bomber aircraft capable of attacking naval and land targets, as well as fighter aircraft for defence. Since World War II helicopters have been embarked on smaller ships in roles such as anti-submarine warfare and transport. Some navies have also operated land-based aircraft in roles such as maritime patrol and training.

Naval aviation forces primarily perform naval roles at sea. However, they are also used in a variety of other roles.





</doc>
<doc id="21538" url="https://en.wikipedia.org/wiki?curid=21538" title="Normed vector space">
Normed vector space

In mathematics, a normed vector space or normed space is a vector space over the real or complex numbers, on which a norm is defined. A norm is the formalization and the generalization to real vector spaces of the intuitive notion of "length" in the real world. A norm is a real-valued function defined on the vector space that is commonly denoted formula_1 and has the following properties:

A norm induces a distance by the formula
Therefore, a normed vector space is a metric space, and thus a topological vector space.

An inner product space is a normed space, where the norm of a vector is the square root of the inner product of the vector by itself. The Euclidean distance in a Euclidean space is related to the norm of the associated vector space (which is an inner product space) by the formula

Two norms on the same vector space are equivalent, in the sense that they define the same topology. On a finite-dimensional vector space, all norms are equivalent. This is not true in infinite dimension, and this makes the study of normed vector spaces fundamental in functional analysis.

A normed vector space is a pair formula_9 where formula_10 is a vector space and formula_11 a norm on formula_10.

A seminormed vector space is a pair formula_13 where formula_10 is a vector space and formula_15 a seminorm on formula_10.

We often omit formula_15 or formula_11 and just write formula_10 for a space if it is clear from the context what (semi) norm we are using.

In a more general sense, a vector norm can be taken to be any real-valued function that satisfies the three properties above.

A useful variation of the triangle inequality is

This also shows that a vector norm is a continuous function.

Note that property 2 depends on a choice of norm formula_21 on the field of scalars. When the scalar field is formula_22 (or more generally a subset of formula_23), this is usually taken to be the ordinary absolute value, but other choices are possible. For example, for a vector space over formula_24 one could take formula_21 to be the "p"-adic norm, which gives rise to a different class of normed vector spaces.

If ("V", ‖·‖) is a normed vector space, the norm ‖·‖ induces a metric (a notion of "distance") and therefore a topology on "V". This metric is defined in the natural way: the distance between two vectors u and v is given by ‖u−v‖. This topology is precisely the weakest topology which makes ‖·‖ continuous and which is compatible with the linear structure of "V" in the following sense:


Similarly, for any semi-normed vector space we can define the distance between two vectors u and v as ‖u−v‖. This turns the seminormed space into a pseudometric space (notice this is weaker than a metric) and allows the definition of notions such as continuity and convergence.
To put it more abstractly every semi-normed vector space is a topological vector space and thus carries a topological structure which is induced by the semi-norm.

Of special interest are complete normed spaces called Banach spaces. Every normed vector space "V" sits as a dense subspace inside a Banach space; this Banach space is essentially uniquely defined by "V" and is called the "completion" of "V".

All norms on a finite-dimensional vector space are equivalent from a topological viewpoint as they induce the same topology (although the resulting metric spaces need not be the same). And since any Euclidean space is complete, we can thus conclude that all finite-dimensional normed vector spaces are Banach spaces. A normed vector space "V" is locally compact if and only if the unit ball "B" = {"x" : ‖"x"‖ ≤ 1} is compact, which is the case if and only if "V" is finite-dimensional; this is a consequence of Riesz's lemma. (In fact, a more general result is true: a topological vector space is locally compact if and only if it is finite-dimensional.
The point here is that we don't assume the topology comes from a norm.)

The topology of a seminormed vector space has many nice properties. Given a neighbourhood system formula_26 around 0 we can construct all other neighbourhood systems as
with

Moreover, there exists a neighbourhood basis for 0 consisting of absorbing and convex sets. As this property is very useful in functional analysis, generalizations of normed vector spaces with this property are studied under the name locally convex spaces.

A topological vector space formula_29 is called normable if there exists a norm formula_30 on "X" such that the canonical metric formula_31 induces the topology formula_32 on "X".
The following theorem is due to Kolmagoroff:

Theorem A Hausdorff topological vector space is normable if and only if there exists a convex, von Neumann bounded neighborhood of formula_33.

A product of a family of normable spaces is normable if and only if only finitely many of the spaces are non-trivial (i.e. formula_34). Furthermore, the quotient of a normable space "X" by a closed vector subspace "C" is normable and if in addition "X"'s topology is given by a norm formula_30 then the map formula_36 given by formula_37 is a well defined norm on "X/C" that induces the quotient topology on "X/C".

The most important maps between two normed vector spaces are the continuous linear maps. Together with these maps, normed vector spaces form a category.

The norm is a continuous function on its vector space. All linear maps between finite dimensional vector spaces are also continuous.

An "isometry" between two normed vector spaces is a linear map "f" which preserves the norm (meaning ‖"f"(v)‖ = ‖v‖ for all vectors v). Isometries are always continuous and injective. A surjective isometry between the normed vector spaces "V" and "W" is called an "isometric isomorphism", and "V" and "W" are called "isometrically isomorphic". Isometrically isomorphic normed vector spaces are identical for all practical purposes.

When speaking of normed vector spaces, we augment the notion of dual space to take the norm into account. The dual "V" ' of a normed vector space "V" is the space of all "continuous" linear maps from "V" to the base field (the complexes or the reals) — such linear maps are called "functionals". The norm of a functional φ is defined as the supremum of |φ(v)| where v ranges over all unit vectors (i.e. vectors of norm 1) in "V". This turns "V" ' into a normed vector space. An important theorem about continuous linear functionals on normed vector spaces is the Hahn–Banach theorem.

The definition of many normed spaces (in particular, Banach spaces) involves a seminorm defined on a vector space and then the normed space is defined as the quotient space by the subspace of elements of seminorm zero. For instance, with the L spaces, the function defined by
is a seminorm on the vector space of all functions on which the Lebesgue integral on the right hand side is defined and finite. However, the seminorm is equal to zero for any function supported on a set of Lebesgue measure zero. These functions form a subspace which we "quotient out", making them equivalent to the zero function.

Given "n" seminormed spaces "X" with seminorms "q" we can define the product space as
with vector addition defined as
and scalar multiplication defined as

We define a new function "q"
for example as
which is a seminorm on "X". The function "q" is a norm if and only if all "q" are norms.

More generally, for each real "p"≥1 we have the seminorm:

For each p this defines the same topological space.

A straightforward argument involving elementary linear algebra shows that the only finite-dimensional seminormed spaces are those arising as the product space of a normed space and a space with trivial seminorm. Consequently, many of the more interesting examples and applications of seminormed spaces occur for infinite-dimensional vector spaces.




</doc>
<doc id="21541" url="https://en.wikipedia.org/wiki?curid=21541" title="Nicene Creed">
Nicene Creed

The Nicene Creed (Greek: or, , Latin: ) is a statement of belief widely used in Christian liturgy. It is called "Nicene" because it was originally adopted in the city of Nicaea (present day İznik, Turkey) by the First Council of Nicaea in 325. In 381, it was amended at the First Council of Constantinople, and the amended form is referred to as the Nicene or the Niceno-Constantinopolitan Creed. It defines Nicene Christianity.

The Oriental Orthodox and Assyrian churches use this profession of faith with the verbs in the original plural ("we believe"), but the Eastern Orthodox and Catholic churches convert those verbs to the singular ("I believe"). The Anglican and many Protestant denominations generally use the singular form, sometimes the plural.

The Apostles' Creed is also used in the Latin West, but not in the Eastern liturgies. On Sundays and solemnities, one of these two creeds is recited in the Roman Rite Mass after the homily. The Nicene Creed is also part of the profession of faith required of those undertaking important functions within the Catholic Church.

In the Byzantine Rite, the Nicene Creed is sung or recited at the Divine Liturgy, immediately preceding the Anaphora (Eucharistic Prayer), and is also recited daily at compline.

The actual purpose of a creed is to provide a doctrinal statement of correct belief or orthodoxy. The creeds of Christianity have been drawn up at times of conflict about doctrine: acceptance or rejection of a creed served to distinguish believers and deniers of particular doctrines. For that reason, a creed was called in Greek a σύμβολον ("symbolon"), which originally meant half of a broken object which, when fitted to the other half, verified the bearer's identity. The Greek word passed through Latin "symbolum" into English "symbol", which only later took on the meaning of an outward sign of something.

The Nicene Creed was adopted to resolve the Arian controversy, whose leader, Arius, a clergyman of Alexandria, "objected to Alexander's (the bishop of the time) apparent carelessness in blurring the distinction of nature between the Father and the Son by his emphasis on eternal generation". In reply, Alexander accused Arius of denying the divinity of the Son and also of being too "Jewish" and "Greek" in his thought. Alexander and his supporters created the Nicene Creed to clarify the key tenets of the Christian faith in response to the widespread adoption of Arius' doctrine, which was henceforth marked as heresy.

The Nicene Creed of 325 explicitly affirms the co-essential divinity of the Son, applying to him the term "consubstantial". The 381 version speaks of the Holy Spirit as worshipped and glorified with the Father and the Son. The later Athanasian Creed (not used in Eastern Christianity) describes in much greater detail the relationship between Father, Son and Holy Spirit. The Apostles' Creed does not explicitly affirm the divinity of the Son and the Holy Spirit, but in the view of many who use it, this doctrine is implicit in it.

The original Nicene Creed was first adopted at the First Council of Nicaea, which opened on 19 June 325. The text ends with anathemas against Arian propositions, and these are preceded by the words "We believe in the Holy Spirit" which terminates the statements of belief.

F. J. A. Hort and Adolf von Harnack argued that the Nicene creed was the local creed of Caesarea (an important center of Early Christianity) recited in the council by Eusebius of Caesarea. Their case relied largely on a very specific interpretation of Eusebius' own account of the Council's proceedings. More recent scholarship has not been convinced by their arguments. The large number of secondary divergences from the text of the creed quoted by Eusebius make it unlikely that it was used as a starting point by those who drafted the conciliar creed. Their initial text was probably a local creed from a Syro–Palestinian source into which they awkwardly inserted phrases to define the Nicene theology. The Eusebian Creed may thus have been either a second or one of many nominations for the Nicene Creed.

The 1911 "Catholic Encyclopedia" says that, soon after the Council of Nicaea, new formulae of faith were composed, most of them variations of the Nicene Symbol, to meet new phases of Arianism, of which there were at least four before the Council of Sardica (341), at which a new form was presented and inserted in its acts, although the council did not accept it.

What is known as the "Niceno-Constantinopolitan Creed" or the "Nicene–Constantinopolitan Creed" received this name because of a belief that it was adopted at the Second Ecumenical Council held in Constantinople in 381 as a modification of the original Nicene Creed of 325. In that light, it also came to be very commonly known simply as the "Nicene Creed". It is the only authoritative "ecumenical" statement of the Christian faith accepted by the Catholic Church, the Eastern Orthodox Church, Oriental Orthodoxy, the Church of the East, much of Protestantism including the Anglican communion. (The Apostles' and Athanasian creeds are not as widely accepted.)

It differs in a number of respects, both by addition and omission, from the creed adopted at the First Council of Nicaea. The most notable difference is the additional section "And [we believe] in the Holy Ghost, the Lord and Giver-of-Life, who proceedeth from the Father, who with the Father and the Son together is worshipped and glorified, who spake by the prophets. And [we believe] in one, holy, Catholic and Apostolic Church. We acknowledge one Baptism for the remission of sins, [and] we look for the resurrection of the dead and the life of the world to come. Amen."

Since the end of the 19th century, scholars have questioned the traditional explanation of the origin of this creed, which has been passed down in the name of the council, whose official acts have been lost over time. A local council of Constantinople in 382 and the third ecumenical council (Ephesus, 431) made no mention of it, with the latter affirming the 325 creed of Nicaea as a valid statement of the faith and using it to denounce Nestorianism. Though some scholarship claims that hints of the later creed's existence are discernible in some writings, no extant document gives its text or makes explicit mention of it earlier than the fourth ecumenical council at Chalcedon in 451. Many of the bishops of the 451 council themselves had never heard of it and initially greeted it skeptically, but it was then produced from the episcopal archives of Constantinople, and the council accepted it "not as supplying any omission but as an authentic interpretation of the faith of Nicaea". In spite of the questions raised, it is considered most likely that this creed was in fact adopted at the 381 second ecumenical council.

On the basis of evidence both internal and external to the text, it has been argued that this creed originated not as an editing of the original Creed proposed at Nicaea in 325, but as an independent creed (probably an older baptismal creed) modified to make it more like the Nicene Creed. Some scholars have argued that the creed may have been presented at Chalcedon as "a precedent for drawing up new creeds and definitions to supplement the Creed of Nicaea, as a way of getting round the ban on new creeds in Canon 7 of Ephesus". It is generally agreed that the Niceno-Constantinopolitan Creed is not simply an expansion of the Creed of Nicaea, and was probably based on another traditional creed independent of the one from Nicaea.

The third Ecumenical Council (Council of Ephesus of 431) reaffirmed the original 325 version of the Nicene Creed and declared that "it is unlawful for any man to bring forward, or to write, or to compose a different () faith as a rival to that established by the holy Fathers assembled with the Holy Ghost in Nicaea" (i.e., the 325 creed). The word is more accurately translated as used by the Council to mean "different", "contradictory", rather than "another". This statement has been interpreted as a prohibition against changing this creed or composing others, but not all accept this interpretation. This question is connected with the controversy whether a creed proclaimed by an Ecumenical Council is definitive in excluding not only excisions from its text but also additions to it.

In one respect, the Eastern Orthodox Church's received text of the Niceno-Constantinopolitan Creed differs from the earliest text, which is included in the acts of the Council of Chalcedon of 451: The Eastern Orthodox Church uses the singular forms of verbs such as "I believe", in place of the plural form ("we believe") used by the council. Byzantine Rite Eastern Catholic Churches use exactly the same form of the Creed, since the Catholic Church teaches that it is wrong to add "and the Son" to the Greek verb "ἐκπορευόμενον", though correct to add it to the Latin "qui procedit", which does not have precisely the same meaning. The form generally used in Western churches does add "and the Son" and also the phrase "God from God", which is found in the original 325 Creed.

The following table, which indicates by [square brackets] the portions of the 325 text that were omitted or moved in 381, and uses "italics" to indicate what phrases, absent in the 325 text, were added in 381, juxtaposes the earlier (AD 325) and later (AD 381) forms of this Creed in the English translation given in Philip Schaff's compilation "The Creeds of Christendom" (1877).

In the late 6th century, some Latin-speaking churches added the words "and from the Son" ("Filioque") to the description of the procession of the Holy Spirit, in what many Eastern Orthodox Christians have at a later stage argued is a violation of Canon VII of the Third Ecumenical Council, since the words were not included in the text by either the Council of Nicaea or that of Constantinople. This was incorporated into the liturgical practice of Rome in 1014. "Filioque" eventually became one of the main causes for the East-West Schism in 1054, and the failures of the repeated union attempts.

The Vatican stated in 1995 that, while the words καὶ τοῦ Υἱοῦ ("and the Son") would indeed be heretical if used with the Greek verb ἐκπορεύομαι (from ἐκ, "out of" and πορεύομαι "to come or go") – which is one of the terms used by St. Gregory of Nazianzus and the one adopted by the Council of Constantinople— the word "Filioque" is not heretical when associated with the Latin verb "procedo" and the related word "processio." Whereas the verb ἐκπορεύομαι in Gregory and other Fathers necessarily means "to originate from a cause or principle," the Latin term "procedo" (from "pro", "forward;" and "cedo", "to go") has no such connotation and simply denotes the communication of the Divine Essence or Substance. In this sense, "processio" is similar in meaning to the Greek term προϊέναι, used by the Fathers from Alexandria (especially Cyril of Alexandria) as well as others. Partly due to the influence of the Latin translations of the New Testament (especially of John 15:26), the term ἐκπορευόμενον (the present participle of ἐκπορεύομαι) in the creed was translated into Latin as "procedentem". In time, the Latin version of the Creed came to be interpreted in the West in the light of the Western concept of "processio", which required the affirmation of the "Filioque" to avoid the heresy of Arianism.

The view that the Nicene Creed can serve as a touchstone of true Christian faith is reflected in the name "symbol of faith", which was given to it in Greek and Latin, when in those languages the word "symbol" meant a "token for identification (by comparison with a counterpart)".

In the Roman Rite Mass, the Latin text of the Niceno-Constantinopolitan Creed, with "Deum de Deo" (God from God) and "Filioque" (and from the Son), phrases absent in the original text, was previously the only form used for the "profession of faith". The Roman Missal now refers to it jointly with the Apostles' Creed as "the Symbol or Profession of Faith or Creed", describing the second as "the baptismal Symbol of the Roman Church, known as the Apostles' Creed".

The liturgies of the ancient Churches of Eastern Christianity (Eastern Orthodox Church, Oriental Orthodoxy, Church of the East and the Eastern Catholic Churches), use the Niceno-Constantinopolitan Creed, never the Western Apostles' Creed.
While in certain places where the Byzantine Rite is used, the choir or congregation sings the Creed at the Divine Liturgy, in many places the Creed is typically recited by the cantor, who in this capacity represents the whole congregation although many, and sometimes all, members of the congregation may join in rhythmic recitation. Where the latter is the practice, it is customary to invite, as a token of honor, any prominent lay member of the congregation who happens to be present, e.g., royalty, a visiting dignitary, the Mayor, etc., to recite the Creed in lieu of the cantor. This practice stems from the tradition that the prerogative to recite the Creed belonged to the Emperor, speaking for his populace.

Some evangelical and other Christians consider the Nicene Creed helpful and to a certain extent authoritative, but not infallibly so in view of their belief that only Scripture is truly authoritative. Non-Trinitarian groups, such as the Church of the New Jerusalem, The Church of Jesus Christ of Latter-day Saints and the Jehovah's Witnesses, explicitly reject some of the statements in the Nicene Creed.

There are several designations for the two forms of the Nicene creed, some with overlapping meanings:

In musical settings, particularly when sung in Latin, this Creed is usually referred to by its first word, "Credo".

This section is not meant to collect the texts of all liturgical versions of the Nicene Creed, and provides only three, the Greek, the Latin, and the Armenian, of special interest. Others are mentioned separately, but without the texts. All ancient liturgical versions, even the Greek, differ at least to some small extent from the text adopted by the First Councils of Nicaea and Constantinople. The Creed was originally written in Greek, owing to the location of the two councils.

But though the councils' texts have "Πιστεύομεν ... ὁμολογοῦμεν ... προσδοκοῦμεν" ("we" believe ... confess ... await), the Creed that the Churches of Byzantine tradition use in their liturgy has "Πιστεύω ... ὁμολογῶ ... προσδοκῶ" ("I" believe ... confess ... await), accentuating the personal nature of recitation of the Creed. The Latin text, as well as using the singular, has two additions: "Deum de Deo" (God from God) and "Filioque" (and from the Son). The Armenian text has many more additions, and is included as showing how that ancient church has chosen to recite the Creed with these numerous elaborations of its contents.

An English translation of the Armenian text is added; English translations of the Greek and Latin liturgical texts are given at English versions of the Nicene Creed in current use.

The Latin text adds "Deum de Deo" and "Filioque" to the Greek. On the latter see The Filioque Controversy above. Inevitably also, the overtones of the terms used, such as "" (pantokratora) and "omnipotentem", differ ("pantokratora" meaning ruler of all; "omnipotentem" meaning omnipotent, almighty). The implications of the difference in overtones of "" and "qui ... procedit" was the object of the study "The Greek and the Latin Traditions regarding the Procession of the Holy Spirit" published by the Pontifical Council for Promoting Christian Unity in 1996.

Again, the terms "" and "consubstantialem", translated as "of one being" or "consubstantial", have different overtones, being based respectively on Greek (stable being, immutable reality, substance, essence, true nature), and Latin "substantia" (that of which a thing consists, the being, essence, contents, material, substance).

"Credo", which in classical Latin is used with the accusative case of the thing held to be true (and with the dative of the person to whom credence is given), is here used three times with the preposition "in", a literal translation of the Greek "" (in unum Deum ..., in unum Dominum ..., in Spiritum Sanctum ...), and once in the classical preposition-less construction (unam, sanctam, catholicam et apostolicam Ecclesiam).

English translation of the Armenian version

The version in the Church Slavonic language, used by several Eastern Orthodox Churches is practically identical with the Greek liturgical version.

This version is used also by some Byzantine Rite Eastern Catholic Churches. Although the Union of Brest excluded addition of the "Filioque", this was sometimes added by Ruthenian Catholics, whose older liturgical books also show the phrase in brackets, and by Ukrainian Catholics. Writing in 1971, the Ruthenian Scholar Fr. Casimir Kucharek noted, "In Eastern Catholic Churches, the "Filioque" may be omitted except when scandal would ensue. Most of the Eastern Catholic Rites use it." However, in the decades that followed 1971 it has come to be used more rarely.

The versions used by Oriental Orthodoxy and the Church of the East differ from the Greek liturgical version in having "We believe", as in the original text, instead of "I believe".

The version found in the 1662 "Book of Common Prayer" is still commonly used by some English speakers, but more modern translations are now more common. The International Consultation on English Texts published an English translation of the Nicene Creed, first in 1970 and then in successive revisions in 1971 and 1975. These texts were adopted by several churches. The Roman Catholic Church in the United States, which adopted the 1971 version in 1973, and the Catholic Church in other English-speaking countries, which in 1975 adopted the version published in that year, continued to use them until 2011, when it replaced them with the version in the "Roman Missal third edition". The 1975 version was included in the 1979 Episcopal Church (United States) "Book of Common Prayer", but with one variation: in the line "For us men and for our salvation", it omitted the word "men".





</doc>
<doc id="21544" url="https://en.wikipedia.org/wiki?curid=21544" title="Nuclear fusion">
Nuclear fusion

Nuclear fusion is a reaction in which two or more atomic nuclei are combined to form one or more different atomic nuclei and subatomic particles (neutrons or protons). The difference in mass between the reactants and products is manifested as either the release or absorption of energy. This difference in mass arises due to the difference in atomic "binding energy" between the atomic nuclei before and after the reaction. Fusion is the process that powers active or "main sequence" stars, or other high magnitude stars.

A fusion process that produces nuclei lighter than iron-56 or nickel-62 will generally release energy. These elements have relatively small mass per nucleon and large binding energy per nucleon. Fusion of nuclei lighter than these releases energy (an exothermic process), while fusion of heavier nuclei results in energy retained by the product nucleons, and the resulting reaction is endothermic. The opposite is true for the reverse process, nuclear fission. This means that the lighter elements, such as hydrogen and helium, are in general more fusible; while the heavier elements, such as uranium, thorium and plutonium, are more fissionable. The extreme astrophysical event of a supernova can produce enough energy to fuse nuclei into elements heavier than iron.

In 1920, Arthur Eddington suggested hydrogen-helium fusion could be the primary source of stellar energy. Quantum tunneling was discovered by Friedrich Hund in 1929, and shortly afterwards Robert Atkinson and Fritz Houtermans used the measured masses of light elements to show that large amounts of energy could be released by fusing small nuclei. Building on the early experiments in nuclear transmutation by Ernest Rutherford, laboratory fusion of hydrogen isotopes was accomplished by Mark Oliphant in 1932. In the remainder of that decade, the theory of the main cycle of nuclear fusion in stars was worked out by Hans Bethe. Research into fusion for military purposes began in the early 1940s as part of the Manhattan Project. Fusion was accomplished in 1951 with the Greenhouse Item nuclear test. Nuclear fusion on a large scale in an explosion was first carried out on 1 November 1952, in the Ivy Mike hydrogen bomb test.

Research into developing controlled fusion inside fusion reactors has been ongoing since the 1940s, but the technology is still in its development phase.

The release of energy with the fusion of light elements is due to the interplay of two opposing forces: the nuclear force, which combines together protons and neutrons, and the Coulomb force, which causes protons to repel each other. Protons are positively charged and repel each other by the Coulomb force, but they can nonetheless stick together, demonstrating the existence of another, short-range, force referred to as nuclear attraction. Light nuclei (or nuclei smaller than iron and nickel) are sufficiently small and proton-poor allowing the nuclear force to overcome repulsion. This is because the nucleus is sufficiently small that all nucleons feel the short-range attractive force at least as strongly as they feel the infinite-range Coulomb repulsion. Building up nuclei from lighter nuclei by fusion releases the extra energy from the net attraction of particles. For larger nuclei, however, no energy is released, since the nuclear force is short-range and cannot continue to act across longer nuclear length scales. Thus, energy is not released with the fusion of such nuclei; instead, energy is required as input for such processes.

Fusion powers stars and produces virtually all elements in a process called nucleosynthesis. The Sun is a main-sequence star, and, as such, generates its energy by nuclear fusion of hydrogen nuclei into helium. In its core, the Sun fuses 620 million metric tons of hydrogen and makes 616 million metric tons of helium each second. The fusion of lighter elements in stars releases energy and the mass that always accompanies it. For example, in the fusion of two hydrogen nuclei to form helium, 0.645% of the mass is carried away in the form of kinetic energy of an alpha particle or other forms of energy, such as electromagnetic radiation.

It takes considerable energy to force nuclei to fuse, even those of the lightest element, hydrogen. When accelerated to high enough speeds, nuclei can overcome this electrostatic repulsion and be brought close enough such that the attractive nuclear force is greater than the repulsive Coulomb force. The strong force grows rapidly once the nuclei are close enough, and the fusing nucleons can essentially "fall" into each other and the result is fusion and net energy produced. The fusion of lighter nuclei, which creates a heavier nucleus and often a free neutron or proton, generally releases more energy than it takes to force the nuclei together; this is an exothermic process that can produce self-sustaining reactions.

Energy released in most nuclear reactions is much larger than in chemical reactions, because the binding energy that holds a nucleus together is greater than the energy that holds electrons to a nucleus. For example, the ionization energy gained by adding an electron to a hydrogen nucleus is —less than one-millionth of the released in the deuterium–tritium (D–T) reaction shown in the adjacent diagram. Fusion reactions have an energy density many times greater than nuclear fission; the reactions produce far greater energy per unit of mass even though "individual" fission reactions are generally much more energetic than "individual" fusion ones, which are themselves millions of times more energetic than chemical reactions. Only direct conversion of mass into energy, such as that caused by the annihilatory collision of matter and antimatter, is more energetic per unit of mass than nuclear fusion. (The complete conversion of one gram of matter would release 9×10 joules of energy.)

Research into using fusion for the production of electricity has been pursued for over 60 years. Although controlled fusion is generally manageable with current technology (e.g. fusors), successful accomplishment of economic fusion has been stymied by scientific and technological difficulties; nonetheless, important progress has been made. At present, controlled fusion reactions have been unable to produce break-even (self-sustaining) controlled fusion. The two most advanced approaches for it are magnetic confinement (toroid designs) and inertial confinement (laser designs).

Workable designs for a toroidal reactor that theoretically will deliver ten times more fusion energy than the amount needed to heat plasma to the required temperatures are in development (see ITER). The ITER facility is expected to finish its construction phase in 2025. It will start commissioning the reactor that same year and initiate plasma experiments in 2025, but is not expected to begin full deuterium-tritium fusion until 2035.

Similarly, Canadian-based General Fusion, which is developing a magnetized target fusion nuclear energy system, aims to build its demonstration plant by 2025.

The US National Ignition Facility, which uses laser-driven inertial confinement fusion, was designed with a goal of break-even fusion; the first large-scale laser target experiments were performed in June 2009 and ignition experiments began in early 2011.

An important fusion process is the stellar nucleosynthesis that powers stars, including the Sun. In the 20th century, it was recognized that the energy released from nuclear fusion reactions accounts for the longevity of stellar heat and light. The fusion of nuclei in a star, starting from its initial hydrogen and helium abundance, provides that energy and synthesizes new nuclei. Different reaction chains are involved, depending on the mass of the star (and therefore the pressure and temperature in its core).

Around 1920, Arthur Eddington anticipated the discovery and mechanism of nuclear fusion processes in stars, in his paper "The Internal Constitution of the Stars". At that time, the source of stellar energy was a complete mystery; Eddington correctly speculated that the source was fusion of hydrogen into helium, liberating enormous energy according to Einstein's equation . This was a particularly remarkable development since at that time fusion and thermonuclear energy had not yet been discovered, nor even that stars are largely composed of hydrogen (see metallicity). Eddington's paper reasoned that:


All of these speculations were proven correct in the following decades.

The primary source of solar energy, and that of similar size stars, is the fusion of hydrogen to form helium (the proton-proton chain reaction), which occurs at a solar-core temperature of 14 million kelvin. The net result is the fusion of four protons into one alpha particle, with the release of two positrons and two neutrinos (which changes two of the protons into neutrons), and energy. In heavier stars, the CNO cycle and other processes are more important. As a star uses up a substantial fraction of its hydrogen, it begins to synthesize heavier elements. The heaviest elements are synthesized by fusion that occurs when a more massive star undergoes a violent supernova at the end of its life, a process known as supernova nucleosynthesis.

A substantial energy barrier of electrostatic forces must be overcome before fusion can occur. At large distances, two naked nuclei repel one another because of the repulsive electrostatic force between their positively charged protons. If two nuclei can be brought close enough together, however, the electrostatic repulsion can be overcome by the quantum effect in which nuclei can tunnel through coulomb forces.

When a nucleon such as a proton or neutron is added to a nucleus, the nuclear force attracts it to all the other nucleons of the nucleus (if the atom is small enough), but primarily to its immediate neighbours due to the short range of the force. The nucleons in the interior of a nucleus have more neighboring nucleons than those on the surface. Since smaller nuclei have a larger surface area-to-volume ratio, the binding energy per nucleon due to the nuclear force generally increases with the size of the nucleus but approaches a limiting value corresponding to that of a nucleus with a diameter of about four nucleons. It is important to keep in mind that nucleons are quantum objects. So, for example, since two neutrons in a nucleus are identical to each other, the goal of distinguishing one from the other, such as which one is in the interior and which is on the surface, is in fact meaningless, and the inclusion of quantum mechanics is therefore necessary for proper calculations.

The electrostatic force, on the other hand, is an inverse-square force, so a proton added to a nucleus will feel an electrostatic repulsion from "all" the other protons in the nucleus. The electrostatic energy per nucleon due to the electrostatic force thus increases without limit as nuclei atomic number grows.

The net result of the opposing electrostatic and strong nuclear forces is that the binding energy per nucleon generally increases with increasing size, up to the elements iron and nickel, and then decreases for heavier nuclei. Eventually, the binding energy becomes negative and very heavy nuclei (all with more than 208 nucleons, corresponding to a diameter of about 6 nucleons) are not stable. The four most tightly bound nuclei, in decreasing order of binding energy per nucleon, are , , , and . Even though the nickel isotope, , is more stable, the iron isotope is an order of magnitude more common. This is due to the fact that there is no easy way for stars to create through the alpha process.

An exception to this general trend is the helium-4 nucleus, whose binding energy is higher than that of lithium, the next heaviest element. This is because protons and neutrons are fermions, which according to the Pauli exclusion principle cannot exist in the same nucleus in exactly the same state. Each proton or neutron's energy state in a nucleus can accommodate both a spin up particle and a spin down particle. Helium-4 has an anomalously large binding energy because its nucleus consists of two protons and two neutrons (it is a doubly magic nucleus), so all four of its nucleons can be in the ground state. Any additional nucleons would have to go into higher energy states. Indeed, the helium-4 nucleus is so tightly bound that it is commonly treated as a single quantum mechanical particle in nuclear physics, namely, the alpha particle.

The situation is similar if two nuclei are brought together. As they approach each other, all the protons in one nucleus repel all the protons in the other. Not until the two nuclei actually come close enough for long enough so the strong nuclear force can take over (by way of tunneling) is the repulsive electrostatic force overcome. Consequently, even when the final energy state is lower, there is a large energy barrier that must first be overcome. It is called the Coulomb barrier.

The Coulomb barrier is smallest for isotopes of hydrogen, as their nuclei contain only a single positive charge. A diproton is not stable, so neutrons must also be involved, ideally in such a way that a helium nucleus, with its extremely tight binding, is one of the products.

Using deuterium–tritium fuel, the resulting energy barrier is about 0.1 MeV. In comparison, the energy needed to remove an electron from hydrogen is 13.6 eV, about 7500 times less energy. The (intermediate) result of the fusion is an unstable He nucleus, which immediately ejects a neutron with 14.1 MeV. The recoil energy of the remaining He nucleus is 3.5 MeV, so the total energy liberated is 17.6 MeV. This is many times more than what was needed to overcome the energy barrier.

The reaction cross section (σ) is a measure of the probability of a fusion reaction as a function of the relative velocity of the two reactant nuclei. If the reactants have a distribution of velocities, e.g. a thermal distribution, then it is useful to perform an average over the distributions of the product of cross section and velocity. This average is called the 'reactivity', denoted <σv>. The reaction rate (fusions per volume per time) is <σv> times the product of the reactant number densities:

If a species of nuclei is reacting with a nucleus like itself, such as the DD reaction, then the product formula_2 must be replaced by formula_3.

formula_4 increases from virtually zero at room temperatures up to meaningful magnitudes at temperatures of 10–100 keV. At these temperatures, well above typical ionization energies (13.6 eV in the hydrogen case), the fusion reactants exist in a plasma state.

The significance of formula_4 as a function of temperature in a device with a particular energy confinement time is found by considering the Lawson criterion. This is an extremely challenging barrier to overcome on Earth, which explains why fusion research has taken many years to reach the current advanced technical state.

If matter is sufficiently heated (hence being plasma) and confined, fusion reactions may occur due to collisions with extreme thermal kinetic energies of the particles. Thermonuclear weapons produce what amounts to an uncontrolled release of fusion energy. Controlled thermonuclear fusion concepts use magnetic fields to confine the plasma.

Inertial confinement fusion (ICF) is a method aimed at releasing fusion energy by heating and compressing a fuel target, typically a pellet containing deuterium and tritium.

Inertial electrostatic confinement is a set of devices that use an electric field to heat ions to fusion conditions. The most well known is the fusor. Starting in 1999, a number of amateurs have been able to do amateur fusion using these homemade devices. Other IEC devices include: the Polywell, MIX POPS and Marble concepts.

If the energy to initiate the reaction comes from accelerating one of the nuclei, the process is called "beam-target" fusion; if both nuclei are accelerated, it is "beam-beam" fusion.

Accelerator-based light-ion fusion is a technique using particle accelerators to achieve particle kinetic energies sufficient to induce light-ion fusion reactions. Accelerating light ions is relatively easy, and can be done in an efficient manner—requiring only a vacuum tube, a pair of electrodes, and a high-voltage transformer; fusion can be observed with as little as 10 kV between the electrodes. The key problem with accelerator-based fusion (and with cold targets in general) is that fusion cross sections are many orders of magnitude lower than Coulomb interaction cross sections. Therefore, the vast majority of ions expend their energy emitting bremsstrahlung radiation and the ionization of atoms of the target. Devices referred to as sealed-tube neutron generators are particularly relevant to this discussion. These small devices are miniature particle accelerators filled with deuterium and tritium gas in an arrangement that allows ions of those nuclei to be accelerated against hydride targets, also containing deuterium and tritium, where fusion takes place, releasing a flux of neutrons. Hundreds of neutron generators are produced annually for use in the petroleum industry where they are used in measurement equipment for locating and mapping oil reserves. 

To overcome the problem of bremsstrahlung radiation in Beam-target fusion, a combinatorial approach has been suggested by Tri-Alpha and Helion energy companies, this method is based on interpenetration of two oppositely directed plasmoids. Theoretical works represent that by creating and warming two accelerated head-on colliding plasmoids up to some kilo electron volts thermal energy which is low in comparison with that of required for thermonuclear fusion, net fusion gain is possible even with aneutronic fuels such as p-B. In order to attain the necessary conditions of break-even by this method the accelerated plasmoids must have enough colliding velocities of the order of some thousands of kilometers per second (10 m/s) depending on the kind of fusion fuel. In addition, the plasmoids density must be between the inertial and magnetic fusion criteria.

Muon-catalyzed fusion is a fusion process that occurs at ordinary temperatures. It was studied in detail by Steven Jones in the early 1980s. Net energy production from this reaction has been unsuccessful because of the high energy required to create muons, their short 2.2 µs half-life, and the high chance that a muon will bind to the new alpha particle and thus stop catalyzing fusion.

Some other confinement principles have been investigated.






At the temperatures and densities in stellar cores the rates of fusion reactions are notoriously slow. For example, at solar core temperature ("T" ≈ 15 MK) and density (160 g/cm), the energy release rate is only 276 μW/cm—about a quarter of the volumetric rate at which a resting human body generates heat. Thus, reproduction of stellar core conditions in a lab for nuclear fusion power production is completely impractical. Because nuclear reaction rates depend on density as well as temperature and most fusion schemes operate at relatively low densities, those methods are strongly dependent on higher temperatures. The fusion rate as a function of temperature (exp(−"E"/"kT")), leads to the need to achieve temperatures in terrestrial reactors 10–100 times higher temperatures than in stellar interiors: "T" ≈ 0.1–1.0×10 K.

In artificial fusion, the primary fuel is not constrained to be protons and higher temperatures can be used, so reactions with larger cross-sections are chosen. Another concern is the production of neutrons, which activate the reactor structure radiologically, but also have the advantages of allowing volumetric extraction of the fusion energy and tritium breeding. Reactions that release no neutrons are referred to as "aneutronic".

To be a useful energy source, a fusion reaction must satisfy several criteria. It must:


Few reactions meet these criteria. The following are those with the largest cross sections:
For reactions with two products, the energy is divided between them in inverse proportion to their masses, as shown. In most reactions with three products, the distribution of energy varies. For reactions that can result in more than one set of products, the branching ratios are given.

Some reaction candidates can be eliminated at once. The D-Li reaction has no advantage compared to p- because it is roughly as difficult to burn but produces substantially more neutrons through - side reactions. There is also a p- reaction, but the cross section is far too low, except possibly when "T" > 1 MeV, but at such high temperatures an endothermic, direct neutron-producing reaction also becomes very significant. Finally there is also a p- reaction, which is not only difficult to burn, but can be easily induced to split into two alpha particles and a neutron.

In addition to the fusion reactions, the following reactions with neutrons are important in order to "breed" tritium in "dry" fusion bombs and some proposed fusion reactors:

The latter of the two equations was unknown when the U.S. conducted the Castle Bravo fusion bomb test in 1954. Being just the second fusion bomb ever tested (and the first to use lithium), the designers of the Castle Bravo "Shrimp" had understood the usefulness of Li in tritium production, but had failed to recognize that Li fission would greatly increase the yield of the bomb. While Li has a small neutron cross-section for low neutron energies, it has a higher cross section above 5 MeV. The 15 Mt yield was 150% greater than the predicted 6 Mt and caused unexpected exposure to fallout.

To evaluate the usefulness of these reactions, in addition to the reactants, the products, and the energy released, one needs to know something about the nuclear cross section. Any given fusion device has a maximum plasma pressure it can sustain, and an economical device would always operate near this maximum. Given this pressure, the largest fusion output is obtained when the temperature is chosen so that <σv>/T is a maximum. This is also the temperature at which the value of the triple product "nT"τ required for ignition is a minimum, since that required value is inversely proportional to <σv>/T (see Lawson criterion). (A plasma is "ignited" if the fusion reactions produce enough power to maintain the temperature without external heating.) This optimum temperature and the value of <σv>/T at that temperature is given for a few of these reactions in the following table.

Note that many of the reactions form chains. For instance, a reactor fueled with and creates some , which is then possible to use in the - reaction if the energies are "right". An elegant idea is to combine the reactions (8) and (9). The from reaction (8) can react with in reaction (9) before completely thermalizing. This produces an energetic proton, which in turn undergoes reaction (8) before thermalizing. Detailed analysis shows that this idea would not work well, but it is a good example of a case where the usual assumption of a Maxwellian plasma is not appropriate.

Any of the reactions above can in principle be the basis of fusion power production. In addition to the temperature and cross section discussed above, we must consider the total energy of the fusion products "E", the energy of the charged fusion products "E", and the atomic number "Z" of the non-hydrogenic reactant.

Specification of the - reaction entails some difficulties, though. To begin with, one must average over the two branches (2i) and (2ii). More difficult is to decide how to treat the and products. burns so well in a deuterium plasma that it is almost impossible to extract from the plasma. The - reaction is optimized at a much higher temperature, so the burnup at the optimum - temperature may be low. Therefore, it seems reasonable to assume the but not the gets burned up and adds its energy to the net reaction, which means the total reaction would be the sum of (2i), (2ii), and (1):

For calculating the power of a reactor (in which the reaction rate is determined by the D-D step), we count the - fusion energy "per D-D reaction" as "E" = (4.03 MeV + 17.6 MeV)×50% + (3.27 MeV)×50% = 12.5 MeV and the energy in charged particles as "E" = (4.03 MeV + 3.5 MeV)×50% + (0.82 MeV)×50% = 4.2 MeV. (Note: if the tritium ion reacts with a deuteron while it still has a large kinetic energy, then the kinetic energy of the helium-4 produced may be quite different from 3.5 MeV, so this calculation of energy in charged particles is only an approximation of the average.) The amount of energy per deuteron consumed is 2/5 of this, or 5.0 MeV (a specific energy of about 225 million MJ per kilogram of deuterium).

Another unique aspect of the - reaction is that there is only one reactant, which must be taken into account when calculating the reaction rate.

With this choice, we tabulate parameters for four of the most important reactions

The last column is the neutronicity of the reaction, the fraction of the fusion energy released as neutrons. This is an important indicator of the magnitude of the problems associated with neutrons like radiation damage, biological shielding, remote handling, and safety. For the first two reactions it is calculated as ("E"-"E")/"E". For the last two reactions, where this calculation would give zero, the values quoted are rough estimates based on side reactions that produce neutrons in a plasma in thermal equilibrium.

Of course, the reactants should also be mixed in the optimal proportions. This is the case when each reactant ion plus its associated electrons accounts for half the pressure. Assuming that the total pressure is fixed, this means that particle density of the non-hydrogenic ion is smaller than that of the hydrogenic ion by a factor 2/("Z"+1). Therefore, the rate for these reactions is reduced by the same factor, on top of any differences in the values of <σv>/T. On the other hand, because the - reaction has only one reactant, its rate is twice as high as when the fuel is divided between two different hydrogenic species, thus creating a more efficient reaction.

Thus there is a "penalty" of (2/(Z+1)) for non-hydrogenic fuels arising from the fact that they require more electrons, which take up pressure without participating in the fusion reaction. (It is usually a good assumption that the electron temperature will be nearly equal to the ion temperature. Some authors, however discuss the possibility that the electrons could be maintained substantially colder than the ions. In such a case, known as a "hot ion mode", the "penalty" would not apply.) There is at the same time a "bonus" of a factor 2 for - because each ion can react with any of the other ions, not just a fraction of them.

We can now compare these reactions in the following table.

The maximum value of <σv>/T is taken from a previous table. The "penalty/bonus" factor is that related to a non-hydrogenic reactant or a single-species reaction. The values in the column "inverse reactivity" are found by dividing 1.24 by the product of the second and third columns. It indicates the factor by which the other reactions occur more slowly than the - reaction under comparable conditions. The column "Lawson criterion" weights these results with "E" and gives an indication of how much more difficult it is to achieve ignition with these reactions, relative to the difficulty for the - reaction. The next-to-last column is labeled "power density" and weights the practical reactivity by "E". The final column indicates how much lower the fusion power density of the other reactions is compared to the - reaction and can be considered a measure of the economic potential.

The ions undergoing fusion in many systems will essentially never occur alone but will be mixed with electrons that in aggregate neutralize the ions' bulk electrical charge and form a plasma. The electrons will generally have a temperature comparable to or greater than that of the ions, so they will collide with the ions and emit x-ray radiation of 10–30 keV energy, a process known as Bremsstrahlung.

The huge size of the Sun and stars means that the x-rays produced in this process will not escape and will deposit their energy back into the plasma. They are said to be opaque to x-rays. But any terrestrial fusion reactor will be optically thin for x-rays of this energy range. X-rays are difficult to reflect but they are effectively absorbed (and converted into heat) in less than mm thickness of stainless steel (which is part of a reactor's shield). This means the bremsstrahlung process is carrying energy out of the plasma, cooling it.
The ratio of fusion power produced to x-ray radiation lost to walls is an important figure of merit. This ratio is generally maximized at a much higher temperature than that which maximizes the power density (see the previous subsection). The following table shows estimates of the optimum temperature and the power ratio at that temperature for several reactions:

The actual ratios of fusion to Bremsstrahlung power will likely be significantly lower for several reasons. For one, the calculation assumes that the energy of the fusion products is transmitted completely to the fuel ions, which then lose energy to the electrons by collisions, which in turn lose energy by Bremsstrahlung. However, because the fusion products move much faster than the fuel ions, they will give up a significant fraction of their energy directly to the electrons. Secondly, the ions in the plasma are assumed to be purely fuel ions. In practice, there will be a significant proportion of impurity ions, which will then lower the ratio. In particular, the fusion products themselves "must" remain in the plasma until they have given up their energy, and "will" remain some time after that in any proposed confinement scheme. Finally, all channels of energy loss other than Bremsstrahlung have been neglected. The last two factors are related. On theoretical and experimental grounds, particle and energy confinement seem to be closely related. In a confinement scheme that does a good job of retaining energy, fusion products will build up. If the fusion products are efficiently ejected, then energy confinement will be poor, too.

The temperatures maximizing the fusion power compared to the Bremsstrahlung are in every case higher than the temperature that maximizes the power density and minimizes the required value of the fusion triple product. This will not change the optimum operating point for - very much because the Bremsstrahlung fraction is low, but it will push the other fuels into regimes where the power density relative to - is even lower and the required confinement even more difficult to achieve. For - and -, Bremsstrahlung losses will be a serious, possibly prohibitive problem. For -, p- and p- the Bremsstrahlung losses appear to make a fusion reactor using these fuels with a quasineutral, isotropic plasma impossible. Some ways out of this dilemma are considered—and rejected—in "fundamental limitations on plasma fusion systems not in thermodynamic equilibrium". This limitation does not apply to non-neutral and anisotropic plasmas; however, these have their own challenges to contend with.

In a classical picture, nuclei can be understood as hard spheres that repel each other through the Coulomb force but fuse once the two spheres come close enough for contact. Estimating the radius of an atomic nuclei as about one femtometer, the energy needed for fusion of two hydrogen is:

formula_6

This would imply that for the core of the sun, which has a Boltzmann distribution with a temperature of around 1.4 keV, the probability hydrogen would reach the threshold is formula_7, that is, fusion would never occur. However, fusion in the sun does occur due to quantum mechanics.

The probability that fusion occurs is greatly increased compared to the classical picture, thanks to the smearing of the effective radius as the DeBroglie wavelength as well as quantum tunnelling through the potential barrier. To determine the rate of fusion reactions, the value of most interest is the cross section, which describes the probability that particle will fuse by giving a characteristic area of interaction. An estimation of the fusion cross sectional area is often broken into three pieces:

Where formula_9 is the geometric cross section, is the barrier transparency and is the reaction characteristics of the reaction. 

formula_9 is of the order of the square of the de-Broglie wavelength formula_11 where formula_12 is the reduced mass of the system and formula_13 is the center of mass energy of the system. 

More detailed forms of the cross section can be derived through nuclear physics based models and R-matrix theory.

The Naval Research Lab's plasma physics formulary gives the total cross section in barns as a function of the energy (in keV) of the incident particle towards a target ion at rest fit by the formula:

formula_19 with the following coefficient values:

Bosch-Hale also reports a R-matrix calculated cross sections fitting observation data with Padé rational approximating coefficients. With energy in units of keV and cross sections in units of millibarn, the factor has the form:

formula_20, with the coefficient values: 
In fusions systems that are in thermal equilibrium the particles are in a Maxwell–Boltzmann distribution, meaning the particles have a range of energies centered around the plasma temperature. The sun, magnetically confined plasmas and inertial confinement fusion systems are well modeled to be in a thermal equilibrium. In these cases, the value of interest is the fusion cross section averaged across the Maxwell-Boltzmann distribution. The Naval Research Lab's plasma physics formulary tabulates Maxwell averaged fusion cross sections reactivities in formula_21.

For energies formula_22 the data can be represented by:

formula_23

formula_24
with formula_25 in units of formula_26. 





</doc>
<doc id="21550" url="https://en.wikipedia.org/wiki?curid=21550" title="National Geographic Society">
National Geographic Society

The National Geographic Society (NGS), headquartered in Washington, D.C., United States, is one of the largest non-profit scientific and educational organizations in the world. Founded in 1888, its interests include geography, archaeology, and natural science, the promotion of environmental and historical conservation, and the study of world culture and history. The National Geographic Society's logo is a yellow portrait frame—rectangular in shape—which appears on the margins surrounding the front covers of its magazines and as its television channel logo. Through National Geographic Partners (a joint venture with The Walt Disney Company), the Society operates the magazine, TV channels, a website, worldwide events, and other media operations.

The National Geographic Society was founded in 1888 "to increase and diffuse geographic knowledge". It is governed by a board of trustees, whose 21 members include distinguished educators, business executives, former government officials and conservationists. The organization sponsors and funds scientific research and exploration. National Geographic maintains a museum for the public in its Washington, D.C., headquarters.

It has helped to sponsor popular traveling exhibits, such as the early 2010s "King Tut" exhibit featuring artifacts from the tomb of the young Egyptian Pharaoh. Its Education Foundation gives grants to education organizations and individuals to improve geography education. Its Committee for Research and Exploration has awarded more than 11,000 grants for scientific research and exploration.

National Geographic has retail stores in Washington, D.C., London, Sydney, and Panama. The locations outside of the United States are operated by Worldwide Retail Store S.L., a Spanish holding company.

The Society's media arm is National Geographic Partners, a joint venture between The Walt Disney Company and the Society, which publishes a journal, "National Geographic" in English and nearly 40 local-language editions. It also publishes other magazines, books, school products, maps, and Web and film products in numerous languages and countries. National Geographic's various media properties reach more than 280 million people monthly.

The National Geographic Society began as a club for an elite group of academics and wealthy patrons interested in travel and exploration. On January 13, 1888, 33 explorers and scientists gathered at the Cosmos Club, a private club then located on Lafayette Square in Washington, D.C., to organize "a society for the increase and diffusion of geographical knowledge." After preparing a constitution and a plan of organization, the National Geographic Society was incorporated two weeks later on January 27. Gardiner Greene Hubbard became its first president and his son-in-law, Alexander Graham Bell, succeeded him in 1897.

In 1899, Bell's son-in-law Gilbert Hovey Grosvenor was named the first full-time editor of National Geographic magazine and served the organization for fifty-five years (until 1954), and members of the Grosvenor family have played important roles in the organization since. Bell and Gilbert Hovey Grosvenor devised the successful marketing notion of Society membership and the first major use of photographs to tell stories in magazines.

The chairman of the National Geographic Society is Jean Case. Michael Ulica is President. Jill Tiefenthaler is the Chief Executive Officer. The editor-in-chief of National Geographic magazine is Susan Goldberg. Gilbert Melville Grosvenor, a former chairman, received the Presidential Medal of Freedom in 2005 for his leadership in geography education.

In 2004, the National Geographic Society headquarters in Washington, D.C., was one of the first buildings to receive a "Green" certification from Global Green USA. The National Geographic received the prestigious Prince of Asturias Award for Communication and Humanities in October 2006 in Oviedo, Spain.

National Geographic Expeditions was launched in 1999 to fulfill one of its mission and for the proceeds to go towards its mission. 

The society purchased in 2006 Hampton-Brown, English-as-a-second-language educational material publisher, using a good part of its endowments. However, the publisher did not generate much profits. By 2009, the society's endowments were about $200 million.

National Geographic Ventures, its commercial arm, launched a music division, National Geographic Music and Radio, in 2007. The society formed in October 2007 National Geographic Entertainment division to include its entertainment units.

In 2013 the society was investigated for possible violation of the Foreign Corrupt Practices Act relating to their close association with an Egyptian government official responsible for antiquities.

On September 9, 2015, the Society announced that it would re-organize its media properties and publications into a new company known as National Geographic Partners, which would be majority-owned by 21st Century Fox (21CF) with a 73% stake. This new, for-profit corporation, would own "National Geographic" and other magazines, as well as its affiliated television networks—most of which were already owned in joint ventures with 21CF. As a consequence, the Society and 21st Century Fox announced on November 2, 2015, that 9 percent of National Geographic's 2,000 employees, approximately 180 people, would be laid off, constituting the biggest staff reduction in the Society's history. Later, The Walt Disney Company assumed 21CF's share in National Geographic Partners, following the completion of Disney's acquisition of most of 21CF assets on March 20, 2019.

The Society has helped sponsor many expeditions and research projects over the years.

The Hubbard Medal is awarded by the National Geographic Society for distinction in exploration, discovery, and research. The medal is named for Gardiner Greene Hubbard, the first National Geographic Society president. The Hubbard Medal has been presented 44 times , the most recent award going to Peter H. Raven.

The National Geographic Society also awards, rarely, the Alexander Graham Bell Medal, for exceptional contributions to geographic research. The award is named after Alexander Graham Bell, scientist, inventor and the second president of the NGS. Up to mid-2011, the medal has been twice presented:

The Society operates the National Geographic Museum, located at 1145 17th Street, NW (17th and M), in Washington, D.C. The museum features changing exhibitions featuring the work of National Geographic explorers, photographers, and scientists. There are also changing exhibits related to natural history, culture, history or society. Permanent exhibits include artifacts like the camera Robert Peary used at the North Pole and pottery that Jacques Cousteau recovered from a shipwreck.

National Geographic Partners, a for-profit joint venture between 21st Century Fox (which owns a 73% stake) and the Society (which owns 27%), was established in 2015 to handle commercial activities of the Society, including television channels worldwide (which were already co-owned by the Society and Fox) and magazine publications. The Walt Disney Company assumed 21CF’s share of National Geographic Partners in March 2019.

Most of National Geographic Partners' businesses predate the establishment in 2015, and even the launch of National Geographic Channel in Asia and Europe by the original News Corporation (of which 21st Century Fox is one of the successors) in the late 1990s.

The society formed in October 2007 National Geographic Entertainment division to include Cinema Ventures, Feature Films, Kids Entertainment, Home Entertainment and Music & Radio divisions. Music and Radio division president David Beal was appointed head of Nat Geo Entertainment.

"The National Geographic Magazine", later shortened to "National Geographic", published its first issue in October 1888, nine months after the Society was founded, as the Society's official journal, a benefit for joining the tax-exempt National Geographic Society. Starting with the February 1910 (Vol XXI, No. 2) issue, the magazine began using its now famous trademarked yellow border around the edge of its covers.

There are 12 monthly issues of "National Geographic" per year. The magazine contains articles about geography, popular science, world history, culture, current events and photography of places and things all over the world and universe. "National Geographic" magazine is currently published in 40 local-language editions in many countries around the world. Combined English and other language circulation is around 6.8 million monthly, with some 60 million readers.

In addition to its flagship magazine, the Society publishes several other periodicals:


The Society also ran an online daily news outlet called "National Geographic News".

Additionally, the Society publishes atlases, books, and maps. It previously published and co-published other magazines, including "National Geographic Adventure", "National Geographic Research" (a scientific journal), and others, and continues to publish special issues of various magazines.

The Society publishes a series of books about natural remedies and medicinal herbs. Titles include "Guide to Medicinal Herbs," "Complete Guide to Natural Home Remedies," "Nature's Best Remedies," "Healing Remedies," and "Natural Home Remedies." The books make claims to describe, among other things, plants, herbs, and essential oils purported to help treat diseases and ailments. While giving some appropriate warnings about such concerns as anecdotal evidence and side effects are given, the books have been criticized from a medical perspective for a number of reasons. These include making recommendations that lack scientific evidence, inconsistent claims from one book to the next as well as internal contradictions, and failure to mention effective and safe alternatives.

National Geographic Films was a wholly owned taxable subsidiary of the National Geographic Society. 

National Geographic Films appointed Adam Leipzig as president in 2004. The society formed in October 2007 National Geographic Entertainment division to include Cinema Ventures and Feature Films. In 2008, the film division and Imagenation formed a $100 million fund to develop, produce, finance and acquire over five years 10-15 films. The first film the fund invested in was "The Way Back".

Leipzig left the company in January 2010. On March 15, 2010, former Miramax president Daniel Battsek started as National Geographic Films president. Basttsek ended up also over seeing Nat Geo Cinema Ventures distribution and big screen production before he left in 2012 becoming president of Cohen Media Group.

Films it has produced include:

In 2005, the National Geographic Society acquired the film distribution arm of Destination Cinema and entered the film distribution business.

National Geographic Cinema Ventures (NGCV) was a giant-screen, 3D and specialty films production and distribution company operated under National Geographic Entertainment.

At the late 2011 American Alliance of Museums conference, National Geographic Cinema Ventures launched the Museum Partnership Program as museums want a brand for their giant screen theaters. Starting on February 1, 2018, Cosmic Pictures gained distribution rights to a number of the NGCV library.
The Museum Partnership Program is branding and content program of National Geographic Cinema Ventures. Partner museums would receive immediate market exclusivity on their 2 new digital 3D films per year and gain access to the National Geographic organization from members to exhibition to television.

There were nine partner museum as of 2012:


Television programs produced by the National Geographic Society are also broadcast on television. National Geographic television specials and series have been aired on PBS and other networks in the United States and globally for many years. The "Geographic" series in the U.S. started on CBS in 1964, moved to ABC in 1973, shifted to PBS (produced by WQED, Pittsburgh) in 1975, shifted to NBC in 1995, and returned to PBS in 2000. It moved to National Geographic Channel in 2005.

It has featured stories on numerous scientific figures such as Jacques Cousteau, Jane Goodall, and Louis Leakey that not only featured their work but as well helped make them world-famous and accessible to millions. Most of the specials were narrated by various actors, including Glenn Close, Linda Hunt, Stacy Keach, Richard Kiley, Burgess Meredith, Susan Sarandon, Alexander Scourby, Martin Sheen, and Peter Strauss. The specials' theme music, by Elmer Bernstein, was also adopted by the National Geographic Channel.

Another long-running show is "National Geographic Explorer".

The original News Corporation launched National Geographic Channel in Asia and Europe in the late 1990s, in partnership with the Society. The Society provides programming to the National Geographic-branded channels worldwide, while, as of March 2019, The Walt Disney Company's subsidiaries (Walt Disney Television in the United States and Fox Networks Group outside the United States) handle distribution of the channels and advertisement sales. The National Geographic Channel has begun to launch a number of sub-branded channels in international markets, such as Nat Geo Wild, Nat Geo People and Nat Geo Kids.

The U.S. domestic version of National Geographic Channel was launched in January 2001 as a joint venture of National Geographic and Fox Cable Networks.

National Geographic Music and Radio (NGMR) is the music and radio division of National Geographic Ventures. The scope of the division includes National Geographic Live! events, digital music distribution, music publishing, radio content, Nat Geo Music TV channel (available in parts of Asia and Europe) and film and TV music. Clear Channel, Salem Communications and NPR were distribution partners.

In early August 2007, National Geographic Ventures announced the existence of the then-recently formed division. The division was already creating music for its feature film and kids units. Initially hired to run the division were Mark Bauman, executive vice president of radio and video production, and David Beal, head of music labels, publishing and radio operations. With National Geographic Channels, Music and Radio on October 15, 2007 launched the Nat Geo Music channel in Italy. 

The society formed in October 2007 National Geographic Entertainment division to include the Music & Radio division and promoted the division president David Beal was appointed head of Nat Geo Entertainment. In 2009, the division became a full-service record label as Nat Geo Music with Mat Whittington appointed as president.




</doc>
<doc id="21556" url="https://en.wikipedia.org/wiki?curid=21556" title="Norns">
Norns

The Norns (, plural: "") in Norse mythology are female beings who rule the destiny of gods and men. They roughly correspond to other controllers of humans' destiny, such as the Fates, elsewhere in European mythology.

In Snorri Sturluson's interpretation of the "Völuspá", Urðr (Wyrd), Verðandi and Skuld, the three most important of the Norns, come out from a hall standing at the Well of Urðr or Well of Fate. They draw water from the well and take sand that lies around it, which they pour over the Yggdrasill tree so that its branches will not rot. These three Norns are described as powerful maiden giantesses (Jotuns) whose arrival from Jötunheimr ended the golden age of the gods. They may be the same as the maidens of Mögþrasir who are described in "Vafþrúðnismál" (see below).

Beside these three famous Norns, there are many others who appear at a person's birth in order to determine his or her future. In the pre-Christian Norse societies, Norns were thought to have visited newborn children. There were both malevolent and benevolent Norns: the former caused all the malevolent and tragic events in the world while the latter were kind and protective goddesses.

The origin of the name "norn" is uncertain, it may derive from a word meaning "to twine" and which would refer to their twining the thread of fate. Bek-Pedersen suggests that the word "norn" has relation to the Swedish dialect word "norna (nyrna)", a verb that means "secretly communicate". This relates to the perception of norns as shadowy, background figures who only really ever reveal their fateful secrets to men as their fates come to pass.

The name "Urðr" (Old English Wyrd, Weird) means "fate". "Wyrd" and "urðr" are etymological cognates, which does not guarantee that "wyrd" and "urðr" share the same semantic quality of "fate" over time. Both "Urðr" and "Verðandi" are derived from the Old Norse verb "verða", "to be". It is commonly asserted that while "Urðr" derives from the past tense ("that which became or happened"), "Verðandi" derives from the present tense of "verða" ("that which is happening"). "Skuld" is derived from the Old Norse verb "skulu", "need/ought to be/shall be"; its meaning is "that which should become, or that needs to occur". Due to this, it has often been inferred that the three norns are in some way connected with the past, present and future respectively, but it has been disputed that their names really imply a temporal distinction and it has been emphasised that the words do not in themselves denote chronological periods in Old Norse.

There is no clear distinction between norns, fylgjas, hamingjas and valkyries, nor with the generic term dísir. Moreover, artistic license permitted such terms to be used for mortal women in Old Norse poetry. To quote Snorri Sturluson's "Skáldskaparmál" on the various names used for women:
These unclear distinctions among norns and other Germanic female deities are discussed in Bek-Pedersen's book "Norns in Old Norse Mythology."

There are a number of surviving Old Norse sources that relate to the norns. The most important sources are the Prose Edda and the Poetic Edda. The latter contains pagan poetry where the norns are frequently referred to, while the former contains, in addition to pagan poetry, retellings, descriptions and commentaries by the 12th and 13th century Icelandic chieftain and scholar Snorri Sturluson.

A skaldic reference to the norns appears in Hvini's poem in "Ynglingatal" 24 found in "Ynglingasaga" 47, where King Halfdan is put to rest by his men at Borró. This reference brings in the phrase ""norna dómr"" which means "judgment of the nornir". In most cases, when the norns pass judgment, it means death to those who have been judged - in this case, Halfdan. Along with being associated with being bringers of death, Bek-Pedersen suggests that this phrase brings in a quasi-legal aspect to the nature of the norns. This legal association is employed quite frequently within skaldic and eddic sources. This phrase can also be seen as a threat, as death is the final and inevitable decision that the norns can make with regard to human life.

The Poetic Edda is valuable in representing older material in poetry from which Snorri tapped information in the "Prose Edda". Like "Gylfaginning", the "Poetic Edda" mentions the existence of many lesser norns beside the three main norns. Moreover, it also agrees with "Gylfaginning" by telling that they were of several races and that the dwarven norns were the daughters of Dvalin. It also suggests that the three main norns were giantesses (female Jotuns).

"Fáfnismál" contains a discussion between the hero Sigurd and the dragon Fafnir who is dying from a mortal wound from Sigurd. The hero asks Fafnir of many things, among them the nature of the norns. Fafnir explains that they are many and from several races:

It appears from "Völuspá" and "Vafþrúðnismál" that the three main norns were not originally goddesses but giants (Jotuns), and that their arrival ended the early days of bliss for the gods, but that they come for the good of humankind.

"Völuspá" relates that three giants of huge might are reported to have arrived to the gods from Jotunheim:

"Vafþrúðnismál" probably refers to the norns when it talks of maiden giants who arrive to protect the people of earth as protective spirits (hamingjas):

The "Völuspá" contains the names of the three main Norns referring to them as maidens like "Vafþrúðnismál" probably does:

The norns visited each newly born child to allot his or her future, and in "Helgakviða Hundingsbana I", the hero Helgi Hundingsbane has just been born and norns arrive at the homestead:

In "Helgakviða Hundingsbana II", Helgi Hundingsbane blames the norns for the fact that he had to kill Sigrún's father Högni and brother Bragi in order to wed her:

Like Snorri Sturluson stated in "Gylfaginning", people's fate depended on the benevolence or the malevolence of particular norns. In "Reginsmál", the water dwelling dwarf Andvari blames his plight on an evil norn, presumably one of the daughters of Dvalin:

Another instance of Norns being blamed for an undesirable situation appears in "Sigurðarkviða hin skamma", where the valkyrie Brynhild blames malevolent norns for her long yearning for the embrace of Sigurd:
Brynhild's solution was to have Gunnarr and his brothers, the lords of the Burgundians, kill Sigurd and afterwards to commit suicide in order to join Sigurd in the afterlife. Her brother Atli (Attila the Hun) avenged her death by killing the lords of the Burgundians, but since he was married to their sister Guðrún, Atli would soon be killed by her. In "Guðrúnarkviða II", the Norns actively enter the series of events by informing Atli in a dream that his wife would kill him. The description of the dream begins with this stanza:
After having killed both her husband Atli and their sons, Guðrún blames the Norns for her misfortunes, as in "Guðrúnarhvöt", where Guðrún talks of trying to escaping the wrath of the norns by trying to kill herself:

"Guðrúnarhvöt" deals with how Guðrún incited her sons to avenge the cruel death of their sister Svanhild. In "Hamðismál", her sons' expedition to the Gothic king Ermanaric to exact vengeance is fateful. Knowing that he is about to die at the hands of the Goths, her son Sörli talks of the cruelty of the norns:

Since the norns were beings of ultimate power who were working in the dark, it should be no surprise that they could be referred to in charms, as they are by Sigrdrífa in "Sigrdrífumál":

In the part of Snorri Sturluson's "Prose Edda" which is called "Gylfaginning", Gylfi, the king of Sweden, has arrived at Valhalla calling himself Gangleri. There, he receives an education in Norse mythology from what is Odin in the shape of three men. They explain to Gylfi that there are three main norns, but also many others of various races, æsir, elves and dwarves:

The three main norns take water out of the well of Urd and water Yggdrasil:

Snorri furthermore informs the reader that the youngest norn, Skuld, is in effect also a valkyrie, taking part in the selection of warriors from the slain:

Some of the legendary sagas also contain references to the norns. The "Hervarar saga" contains a poem named "Hlöðskviða", where the Gothic king Angantýr defeats a Hunnish invasion led by his Hunnish half-brother Hlöðr. Knowing that his sister, the shieldmaiden Hervör, is one of the casualties, Angantýr looks at his dead brother and laments the cruelty of the norns:
In younger legendary sagas, such as "Norna-Gests þáttr" and "Hrólfs saga kraka", the norns appear to have been synonymous with völvas (witches, female shamans). In "Norna-Gests þáttr", where they arrive at the birth of the hero to shape his destiny, the norns are not described as weaving the web of fate, instead "Norna" appears to be interchangeable and possibly a synonym of "vala" (völva).

One of the last legendary sagas to be written down, the "Hrólfs saga kraka" talks of the norns simply as evil witches. When the evil half-elven princess Skuld assembles her army to attack Hrólfr Kraki, it contains in addition to undead warriors, elves and norns.

The belief in the norns as bringers of both gain and loss would last beyond Christianization, as testifies the runic inscription N 351 M from the Borgund stave church:

Three women carved on the right panel of Franks Casket, an Anglo-Saxon whalebone chest from the eighth century, have been identified by some scholars as being three norns.

A number of theories have been proposed regarding the norns.

The Germanic Matres and Matrones, female deities venerated in North-West Europe from the 1st to the 5th century AD depicted on votive objects and altars almost entirely in groups of three from the first to the fifth century AD have been proposed as connected with the later Germanic dísir, valkyries, and norns, potentially stemming from them.

Theories have been proposed that there is no foundation in Norse mythology for the notion that the three main norns should each be associated exclusively with the past, the present, and the future; rather, all three represent "destiny" as it is twined with the flow of time. Moreover, theories have been proposed that the idea that there are three main norns may be due to a late influence from Greek and Roman mythology, where there are also spinning fate goddesses (Moirai and Parcae).

Amon Amarth wrote a Death Metal album named Fate of Norns containing the title track "Fate of Norns" released in 2004.

The Norns are the main characters of the popular anime Ah! My Goddess. Verðandi (here named Belldandy) is the female protagonist of the series, opposite the male protagonist alias the Japanese human Keiichi Morisato; Urðr (Urd) and Skuld are important supporting characters.

Jack and Annie meet the Norns on one of their missions in Magic Tree House.

Norns are present in Philip K. Dick's "Galactic Pot-Healer", as entities keeping a book where the future is already written.

In Neil Gaiman's "American Gods", Norns are shown as three women (one very tall, one average height, the last a dwarf) who assist Shadow in his vigil for Wednesday (Odin) on the ash tree, then stay in a croft nearby; they revive Shadow's dead wife Laura by means of the water from the pit of Urd; and they prophesy to Mr. Town, an associate of Mr. World, that his neck will be broken.
The Norns are alluded to in 2018’s "God of War", the eighth installment in the "God of War" series, developed by Santa Monica Studio and published by Sony Interactive Entertainment (SIE), which began the franchise’s foray into the lore of Norse mythology. As the story’s protagonist Kratos and his young son, Atreus, set off on a journey through the realm of Midgard, they continuously encounter chests known as Nornir Chest, each of which can be opened by locating three hidden rune-seals and quickly striking all three with the Leviathan Axe. Each of the Nornir Chests contain collectibles that gradually upgrade Kratos’ Health and/or Rage meters.



</doc>
<doc id="21557" url="https://en.wikipedia.org/wiki?curid=21557" title="Niflheim">
Niflheim

In Norse cosmology, Niflheim or Niflheimr ("World of Mist", literally "Home of Mist") is a location in which sometimes overlaps with the notions of Niflhel and Hel. The name "Niflheimr" appears only in two extant sources: "Gylfaginning" and the much-debated "Hrafnagaldr Óðins".

Niflheim was primarily a realm of primordial ice and cold, with the frozen rivers of Élivágar and the well of Hvergelmir, from which come all the rivers.

According to "Gylfaginning", Niflheim was the second of the two primordial realms to emanate out of Ginnungagap, the other one being Muspelheim, the realm of fire. Between these two realms of cold and heat, creation began when its waters mixed with the heat of Muspelheim to form a "creating steam". Later, it became the abode of Hel, a goddess daughter of Loki, and the afterlife for her subjects, those who did not die a heroic or notable death.

"Nifl" ("mist"; whence the Icelandic "nifl") is a cognate to the Old English "nifol" ("dark, gloomy"), (Middle) Dutch "nevel", Old High German "nebul" ("fog") and Ancient Greek "νεφέλη", nεˈfε.li, ("cloud").

In "Gylfaginning" by Snorri Sturluson, Gylfi, the king of ancient Scandinavia, receives an education in Norse mythology from Odin in the guise of three men. Gylfi learns from Odin (as "Jafnhárr") that Niflheimr was the first world to be created after Muspelheim:

Odin (as "Þriði") further tells Gylfi that it was when the ice from Niflheimr met the flames from Muspelheimr that creation began and Ymir was formed:
In relation to the world tree Yggdrasill, "Jafnhárr" (Odin) tells Gylfi that Jötunheimr is located under the second root, where Ginnungagap ("Yawning Void") once was:
Gylfi is furthermore informed that when Loki had engendered Hel, she was cast into Niflheimr by Odin:

Hel thus became the mistress of the world of those dead in disease and old age. This is the only instance in which Niflheim and Hel are equated (the Poetic Edda mentions Hel but doesn't say anything about Niflheim).
However, there is some confusion in the different versions of the manuscript, with some of them saying Niflheim where others say Niflhel (the lowest level of Hel). Thus in the passage about the last destination of the "jötunn" who was killed by Thor after he had built Asgard:
In "Hrafnagaldr Óðins", there is a brief mention of Niflheimr as a location in the North, towards which the sun (Alfr's illuminator) chased the night as it rose:


</doc>
<doc id="21558" url="https://en.wikipedia.org/wiki?curid=21558" title="Nanna">
Nanna

Nanna may refer to:








</doc>
<doc id="21559" url="https://en.wikipedia.org/wiki?curid=21559" title="Nasdaq">
Nasdaq

The Nasdaq Stock Market, also known as Nasdaq or NASDAQ, is an American stock exchange located at One Liberty Plaza in New York City. It is ranked second on the list of stock exchanges by market capitalization of shares traded, behind only the New York Stock Exchange. The exchange platform is owned by Nasdaq, Inc., which also owns the Nasdaq Nordic stock market network and several U.S. stock and options exchanges.

"Nasdaq" was initially an acronym for the National Association of Securities Dealers Automated Quotations.

It was founded in 1971 by the National Association of Securities Dealers (NASD), now known as the Financial Industry Regulatory Authority (FINRA).

On February 8, 1971, the Nasdaq stock market began operations as the world's first electronic stock market. At first, it was merely a "quotation system" and did not provide a way to perform electronic trades. The Nasdaq Stock Market helped lower the bid–ask spread (the difference between the bid price and the ask price of the stock), but was unpopular among brokers as it reduced their profits.

The NASDAQ Stock Market eventually assumed the majority of major trades that had been executed by the over-the-counter (OTC) system of trading, but there are still many securities traded in this fashion. As late as 1987, the Nasdaq exchange was still commonly referred to as "OTC" in media reports and also in the monthly Stock Guides (stock guides and procedures) issued by Standard & Poor's Corporation.

Over the years, the Nasdaq Stock Market became more of a stock market by adding trade and volume reporting and automated trading systems.

In 1981, Nasdaq traded 37% of the U.S. securities markets' total of 21 billion shares. By 1991, Nasdaq's share had grown to 46%.

In 1998, it was the first stock market in the United States to trade online, using the slogan "the stock market for the next hundred years". The Nasdaq Stock Market attracted many companies during the dot-com bubble.

Its main index is the NASDAQ Composite, which has been published since its inception. The QQQ exchange-traded fund tracks the large-cap NASDAQ-100 index, which was introduced in 1985 alongside the NASDAQ Financial-100 Index, which tracks the largest 100 companies in terms of market capitalization.

In 1992, the Nasdaq Stock Market joined with the London Stock Exchange to form the first intercontinental linkage of capital markets.

In 2000, the National Association of Securities Dealers spun off the Nasdaq Stock Market to form a public company.

On March 10, 2000, the NASDAQ Composite stock market index peaked at 5,132.52, but fell to 3,227 by April 17, and, in the following 30 months, fell 78% from its peak.

In a series of sales in 2000 and 2001, FINRA sold its stake in the Nasdaq.

On July 2, 2002, Nasdaq Inc. became a public company via an initial public offering.

In 2006, the status of the Nasdaq Stock Market was changed from a stock market to a licensed national securities exchange.

In 2010, Nasdaq merged with OMX, a leading exchange operator in the Nordic countries, expanded its global footprint, and changed its name to the NASDAQ OMX Group.

To qualify for listing on the exchange, a company must be registered with the United States Securities and Exchange Commission (SEC), must have at least three market makers (financial firms that act as brokers or dealers for specific securities) and must meet minimum requirements for assets, capital, public shares, and shareholders.

In February 2011, in the wake of an announced merger of NYSE Euronext with Deutsche Börse, speculation developed that NASDAQ OMX and Intercontinental Exchange (ICE) could mount a counter-bid of their own for NYSE. NASDAQ OMX could be looking to acquire the American exchange's cash equities business, ICE the derivatives business. At the time, "NYSE Euronext's market value was $9.75 billion. Nasdaq was valued at $5.78 billion, while ICE was valued at $9.45 billion." Late in the month, Nasdaq was reported to be considering asking either ICE or the Chicago Mercantile Exchange to join in what would probably have to be, if it proceeded, an $11–12 billion counterbid.

In December 2005, NASDAQ acquired Instinet for $1.9 billion, retaining the Inet ECN and subsequently selling the agency brokerage business to Silver Lake Partners and Instinet management.

The European Association of Securities Dealers Automatic Quotation System (EASDAQ) was founded as a European equivalent to the Nasdaq Stock Market. It was purchased by NASDAQ in 2001 and became NASDAQ Europe. In 2003, operations were shut down as a result of the burst of the dot-com bubble. In 2007, NASDAQ Europe was revived first as Equiduct, and later that year, it was acquired by Börse Berlin.

On June 18, 2012, Nasdaq OMX became a founding member of the United Nations Sustainable Stock Exchanges Initiative on the eve of the United Nations Conference on Sustainable Development (Rio+20). 

In November 2016, chief operating officer Adena Friedman was promoted to chief executive officer, becoming the first woman to run a major exchange in the U.S.

In 2016, Nasdaq earned $272 million in listings-related revenues.

In October 2018, the SEC ruled that the New York Stock Exchange and Nasdaq did not justify the continued price increases when selling market data.

Nasdaq quotes are available at three levels:


The Nasdaq Stock Market sessions, with times in the Eastern Time Zone are:

4:00 a.m. to 9:30 a.m. Extended-hours trading session (premarket)

9:30 a.m. to 4:00 p.m. normal trading session

4:00 p.m. to 8:00 p.m. Extended-hours trading session (postmarket)

The Nasdaq Stock Market averages about 253 trading days per year.

The Nasdaq Stock Market has three different market tiers:


</doc>
<doc id="21560" url="https://en.wikipedia.org/wiki?curid=21560" title="New York Stock Exchange">
New York Stock Exchange

The New York Stock Exchange (NYSE, nicknamed "The Big Board") is an American stock exchange located at 11 Wall Street, Lower Manhattan, New York City, New York. It is by far the world's largest stock exchange by market capitalization of its listed companies at US$30.1 trillion as of February 2018. The average daily trading value was approximately 169 billion in 2013. The NYSE trading floor is located at 11 Wall Street and is composed of 21 rooms used for the facilitation of trading. An additional trading room, located at 30 Broad Street, was closed in February 2007. The main building and the 11 Wall Street building were designated National Historic Landmarks in 1978.

The NYSE is owned by Intercontinental Exchange, an American holding company that it also lists (). Previously, it was part of NYSE Euronext (NYX), which was formed by the NYSE's 2007 merger with Euronext.

The earliest recorded organization of securities trading in New York among brokers directly dealing with each other can be traced to the Buttonwood Agreement. Previously, securities exchange had been intermediated by the auctioneers, who also conducted more mundane auctions of commodities such as wheat and tobacco. On May 17, 1792, twenty four brokers signed the Buttonwood Agreement, which set a floor commission rate charged to clients and bound the signers to give preference to the other signers in securities sales. The earliest securities traded were mostly governmental securities such as War Bonds from the Revolutionary War and First Bank of the United States stock, although Bank of New York stock was a non-governmental security traded in the early days. The Bank of North America, along with the First Bank of the United States and the Bank of New York, were the first shares traded on the New York Stock Exchange.

In 1817, the stockbrokers of New York, operating under the Buttonwood Agreement, instituted new reforms and reorganized. After sending a delegation to Philadelphia to observe the organization of their board of brokers, restrictions on manipulative trading were adopted, as well as formal organs of governance. After re-forming as the New York Stock and Exchange Board, the broker organization began renting out space exclusively for securities trading, which previously had been taking place at the Tontine Coffee House. Several locations were used between 1817 and 1865, when the present location was adopted.

The invention of the electrical telegraph consolidated markets and New York's market rose to dominance over Philadelphia after weathering some market panics better than other alternatives. The Open Board of Stock Brokers was established in 1864 as a competitor to the NYSE. With 354 members, the Open Board of Stock Brokers rivaled the NYSE in membership (which had 533) "because it used a more modern, continuous trading system superior to the NYSE’s twice-daily call sessions". The Open Board of Stock Brokers merged with the NYSE in 1869. Robert Wright of "Bloomberg" writes that the merger increased the NYSE's members as well as trading volume, as "several dozen regional exchanges were also competing with the NYSE for customers. Buyers, sellers and dealers all wanted to complete transactions as quickly and cheaply as technologically possible and that meant finding the markets with the most trading, or the greatest liquidity in today’s parlance. Minimizing competition was essential to keep a large number of orders flowing, and the merger helped the NYSE maintain its reputation for providing superior liquidity." The Civil War greatly stimulated speculative securities trading in New York. By 1869, membership had to be capped, and has been sporadically increased since. The latter half of the nineteenth century saw rapid growth in securities trading.

Securities trade in the latter nineteenth and early twentieth centuries was prone to panics and crashes. Government regulation of securities trading was eventually seen as necessary, with arguably the most dramatic changes occurring in the 1930s after a major stock market crash precipitated the Great Depression.

The Stock Exchange Luncheon Club was situated on the seventh floor from 1898 until its closure in 2006.

The main building, located at 18 Broad Street, between the corners of Wall Street and Exchange Place, was designated a National Historic Landmark in 1978, as was the 11 Wall Street building.
On April 21, 2005, the NYSE announced its plans to merge with Archipelago in a deal intended to reorganize the NYSE as a publicly traded company. NYSE's governing board voted to merge with rival Archipelago on December 6, 2005, and became a for-profit, public company. It began trading under the name NYSE Group on March 8, 2006. On April 4, 2007, the NYSE Group completed its merger with Euronext, the European combined stock market, thus forming NYSE Euronext, the first transatlantic stock exchange.

Wall Street is the leading US money center for international financial activities and the foremost US location for the conduct of wholesale financial services. "It comprises a matrix of wholesale financial sectors, financial markets, financial institutions, and financial industry firms" (Robert, 2002). The principal sectors are securities industry, commercial banking, asset management, and insurance.

Prior to the acquisition of NYSE Euronext by the ICE in 2013, Marsh Carter was the Chairman of the NYSE and the CEO was Duncan Niederauer. Currently, the chairman is Jeffrey Sprecher. In 2016, NYSE owner Intercontinental Exchange Inc. earned $419 million in listings-related revenues.

The exchange was closed shortly after the beginning of World War I (July 31, 1914), but it partially re-opened on November 28 of that year in order to help the war effort by trading bonds, and completely reopened for stock trading in mid-December.

On September 16, 1920, a bomb exploded on Wall Street outside the NYSE building, killing 33 people and injuring more than 400. The perpetrators were never found. The NYSE building and some buildings nearby, such as the JP Morgan building, still have marks on their façades caused by the bombing.

The Black Thursday crash of the Exchange on October 24, 1929, and the sell-off panic which started on Black Tuesday, October 29, are often blamed for precipitating the Great Depression. In an effort to restore investor confidence, the Exchange unveiled a fifteen-point program aimed to upgrade protection for the investing public on October 31, 1938.

On October 1, 1934, the exchange was registered as a national securities exchange with the U.S. Securities and Exchange Commission, with a president and a thirty-three-member board. On February 18, 1971, the non-profit corporation was formed, and the number of board members was reduced to twenty-five.

One of Abbie Hoffman's well-known publicity stunts took place in 1967, when he led members of the Yippie movement to the Exchange's gallery. The provocateurs hurled fistfuls of dollars toward the trading floor below. Some traders booed, and some laughed and waved. Three months later the stock exchange enclosed the gallery with bulletproof glass. Hoffman wrote a decade later, "We didn't call the press; at that time we really had no notion of anything called a media event."

On October 19, 1987, the Dow Jones Industrial Average (DJIA) dropped 508 points, a 22.6% loss in a single day, the second-biggest one-day drop the exchange had experienced. Black Monday was followed by Terrible Tuesday, a day in which the Exchange's systems did not perform well and some people had difficulty completing their trades.

Subsequently, there was another major drop for the Dow on October 13, 1989—the Mini-Crash of 1989. The crash was apparently caused by a reaction to a news story of a $6.75 billion leveraged buyout deal for UAL Corporation, the parent company of United Airlines, which broke down. When the UAL deal fell through, it helped trigger the collapse of the junk bond market causing the Dow to fall 190.58 points, or 6.91 percent.

Similarly, there was a panic in the financial world during the year of 1997; the Asian Financial Crisis. Like the fall of many foreign markets, the Dow suffered a 7.18% drop in value (554.26 points) on October 27, 1997, in what later became known as the 1997 Mini-Crash but from which the DJIA recovered quickly. This was the first time that the "circuit breaker" rule had operated.

On January 26, 2000, an altercation during filming of the music video for Rage Against the Machine's "Sleep Now in the Fire", directed by Michael Moore, caused the doors of the exchange to be closed and the band to be escorted from the site by security after the members attempted to gain entry into the exchange.

In the aftermath of the September 11 attacks, the NYSE was closed for four trading sessions, resuming on Monday, September 17, one of the rare times the NYSE was closed for more than one session and only the third time since March 1933. On the first day, the NYSE suffered a 7.1% drop in value (684 points); after a week, it dropped by 14% (1370 points). An estimated of $1.4 trillion was lost within five days of trading. The NYSE was only 5 blocks from Ground Zero.

On May 6, 2010, the Dow Jones Industrial Average posted its largest intraday percentage drop since the crash on October 19, 1987, with a 998-point loss later being called the 2010 Flash Crash (as the drop occurred in minutes before rebounding). The SEC and CFTC published a report on the event, although it did not come to a conclusion as to the cause. The regulators found no evidence that the fall was caused by erroneous ("fat finger") orders.

On October 29, 2012, the stock exchange was shut down for two days due to Hurricane Sandy. The last time the stock exchange was closed due to weather for a full two days was on March 12 and 13, 1888.

On May 1, 2014, the stock exchange was fined $4.5 million by the Securities and Exchange Commission to settle charges that it had violated market rules.

On August 14, 2014, Berkshire Hathaway's A Class shares, the highest priced shares on the NYSE, hit $200,000 a share for the first time.

On July 8, 2015, technical issues affected the stock exchange, halting trading at 11:32 am ET. The NYSE reassured stock traders that the outage was "not a result of a cyber breach", and the Department of Homeland Security confirmed that there was "no sign of malicious activity". Trading eventually resumed at 3:10 pm ET the same day.

On May 25, 2018, Stacey Cunningham, the NYSE's chief operating officer, became the Big Board's 67th president, succeeding Thomas Farley. She is the first female leader in the exchange's 226-year history.

The NYSE plans to temporarily move to all-electronic trading on March 23, 2020, due to the COVID-19 pandemic in New York City.

The New York Stock Exchange is closed on New Year's Day, Martin Luther King, Jr. Day, Washington's Birthday, Good Friday, Memorial Day, Fourth of July, Labor Day, Thanksgiving, and Christmas. When those holidays occur on a weekend, the holiday is observed on the closest weekday. In addition, the Stock Exchange closes early on the day before Independence Day, the day after Thanksgiving, and Christmas Eve. The NYSE averages about 253 trading days per year.

The New York Stock Exchange (sometimes referred to as "the Big Board") provides a means for buyers and sellers to trade shares of stock in companies registered for public trading. The NYSE is open for trading Monday through Friday from 9:30 am – 4:00 pm ET, with the exception of holidays declared by the Exchange in advance.

The NYSE trades in a continuous auction format, where traders can execute stock transactions on behalf of investors. They will gather around the appropriate post where a specialist broker, who is employed by a NYSE member firm (that is, he/she is not an employee of the New York Stock Exchange), acts as an auctioneer in an open outcry auction market environment to bring buyers and sellers together and to manage the actual auction. They do on occasion (approximately 10% of the time) facilitate the trades by committing their own capital and as a matter of course disseminate information to the crowd that helps to bring buyers and sellers together. The auction process moved toward automation in 1995 through the use of wireless hand held computers (HHC). The system enabled traders to receive and execute orders electronically via wireless transmission. On September 25, 1995, NYSE member Michael Einersen, who designed and developed this system, executed 1000 shares of IBM through this HHC ending a 203-year process of paper transactions and ushering in an era of automated trading.

As of January 24, 2007, all NYSE stocks can be traded via its electronic hybrid market (except for a small group of very high-priced stocks). Customers can now send orders for immediate electronic execution, or route orders to the floor for trade in the auction market. In the first three months of 2007, in excess of 82% of all order volume was delivered to the floor electronically. NYSE works with US regulators such as the SEC and CFTC to coordinate risk management measures in the electronic trading environment through the implementation of mechanisms like circuit breakers and liquidity replenishment points.

Until 2005, the right to directly trade shares on the exchange was conferred upon owners of the 1,366 "seats". The term comes from the fact that up until the 1870s NYSE members sat in chairs to trade. In 1868, the number of seats was fixed at 533, and this number was increased several times over the years. In 1953, the number of seats was set at 1,366. These seats were a sought-after commodity as they conferred the ability to directly trade stock on the NYSE, and seat holders were commonly referred to as members of the NYSE. The Barnes family is the only known lineage to have five generations of NYSE members: Winthrop H. Barnes (admitted 1894), Richard W.P. Barnes (admitted 1926), Richard S. Barnes (admitted 1951), Robert H. Barnes (admitted 1972), Derek J. Barnes (admitted 2003). Seat prices varied widely over the years, generally falling during recessions and rising during economic expansions. The most expensive inflation-adjusted seat was sold in 1929 for $625,000, which, today, would be over six million dollars. In recent times, seats have sold for as high as $4 million in the late 1990s and as low as $1 million in 2001. In 2005, seat prices shot up to $3.25 million as the exchange entered into an agreement to merge with Archipelago and became a for-profit, publicly traded company. Seat owners received $500,000 in cash per seat and 77,000 shares of the newly formed corporation. The NYSE now sells one-year licenses to trade directly on the exchange. Licenses for floor trading are available for $40,000 and a license for bond trading is available for as little as $1,000 as of 2010. Neither are resell-able, but may be transferable during a change of ownership of a corporation holding a trading license.

Following the Black Monday market crash in 1987, NYSE imposed trading curbs to reduce market volatility and massive panic sell-offs. Following the 2011 rule change, at the start of each trading day, the NYSE sets three circuit breaker levels at levels of 7% (Level 1), 13% (Level 2), and 20% (Level 3) of the average closing price of the S&P 500 for the preceding trading day. Level 1 and Level 2 declines result in a 15-minute trading halt unless they occur after 3:25 pm, when no trading halts apply. A Level 3 decline results in trading being suspended for the remainder of the day. (The biggest one-day decline in the S&P 500 since 1987 was the 11.98% drop on March 16, 2020.)

In the mid-1960s, the NYSE Composite Index (NYSE: NYA) was created, with a base value of 50 points equal to the 1965 yearly close. This was done to reflect the value of all stocks trading at the exchange instead of just the 30 stocks included in the Dow Jones Industrial Average. To raise the profile of the composite index, in 2003, the NYSE set its new base value of 5,000 points equal to the 2002 yearly close. Its close at the end of 2013 was 10,400.32.


In October 2008, NYSE Euronext completed acquisition of the American Stock Exchange (AMEX) for $260 million in stock.

On February 15, 2011, NYSE and Deutsche Börse announced their merger to form a new company, as yet unnamed, wherein Deutsche Börse shareholders would have 60% ownership of the new entity, and NYSE Euronext shareholders would have 40%.

On February 1, 2012, the European Commission blocked the merger of NYSE with Deutsche Börse, after commissioner Joaquín Almunia stated that the merger "would have led to a near-monopoly in European financial derivatives worldwide". Instead, Deutsche Börse and NYSE would have to sell either their Eurex derivatives or LIFFE shares in order to not create a monopoly. On February 2, 2012, NYSE Euronext and Deutsche Börse agreed to scrap the merger.

In April 2011, Intercontinental Exchange (ICE), an American futures exchange, and NASDAQ OMX Group had together made an unsolicited proposal to buy NYSE Euronext for approximately , a deal in which NASDAQ would have taken control of the stock exchanges. NYSE Euronext rejected this offer twice, but it was finally terminated after the United States Department of Justice indicated their intention to block the deal due to antitrust concerns.

In December 2012, ICE had proposed to buy NYSE Euronext in a stock swap with a valuation of $8 billion. NYSE Euronext shareholders would receive either $33.12 in cash, or $11.27 in cash and approximately a sixth of a share of ICE. Jeffrey Sprecher, the chairman and CEO of ICE, will retain those positions, but four members of the NYSE board of directors will be added to the ICE board.

The NYSE's opening and closing bells mark the beginning and the end of each trading day. The opening bell is rung at 9:30 am ET to mark the start of the day's trading session. At 4 pm ET the closing bell is rung and trading for the day stops. There are bells located in each of the four main sections of the NYSE that all ring at the same time once a button is pressed. There are three buttons that control the bells, located on the control panel behind the podium which overlooks the trading floor. The main bell, which is rung at the beginning and end of the trading day, is controlled by a green button. The second button, colored orange, activates a single-stroke bell that is used to signal a moment of silence. A third, red button controls a backup bell which is used in case the main bell fails to ring.

The signal to start and stop trading was not always a bell. The original signal was a gavel (which is still in use today along with the bell), but during the late 1800s, the NYSE decided to switch the gavel for a gong to signal the day's beginning and end. After the NYSE changed to its present location at 18 Broad Street in 1903, the gong was switched to the bell format that is currently being used.

A common sight today is the highly publicized events in which a celebrity or executive from a corporation stands behind the NYSE podium and pushes the button that signals the bells to ring. Due to the amount of coverage that the opening/closing bells receive, many companies coordinate new product launches and other marketing-related events to start on the same day as when the company's representatives ring the bell. It was only in 1995 that the NYSE began having special guests ring the bells on a regular basis; prior to that, ringing the bells was usually the responsibility of the exchange's floor managers.

Many of the people who ring the bell are business executives whose companies trade on the exchange. However, there have also been many famous people from outside the world of business that have rung the bell. Athletes such as Joe DiMaggio of the New York Yankees and Olympic swimming champion Michael Phelps, entertainers such as rapper Snoop Dogg, members of ESPN’s College GameDay crew, singer and actress Liza Minnelli and members of the band Kiss, and politicians such as Mayor of New York City Rudy Giuliani and President of South Africa Nelson Mandela have all had the honor of ringing the bell. Two United Nations Secretaries General have also rung the bell. On April 27, 2006, Secretary-General Kofi Annan rang the opening bell to launch the United Nations Principles for Responsible Investment. On July 24, 2013, Secretary-General Ban Ki-moon rang the closing bell to celebrate the NYSE joining the United Nations Sustainable Stock Exchanges Initiative.

In addition, there have been many bell-ringers who are famous for heroic deeds, such as members of the New York police and fire departments following the events of 9/11, members of the United States Armed Forces serving overseas, and participants in various charitable organizations.

There have also been several fictional characters that have rung the bell, including Mickey Mouse, the Pink Panther, Mr. Potato Head, the Aflac Duck, Gene of The Emoji Movie, and Darth Vader.


Notes
Bibliography


</doc>
<doc id="21561" url="https://en.wikipedia.org/wiki?curid=21561" title="Nanoengineering">
Nanoengineering

Nanoengineering is the practice of engineering on the nanoscale. It derives its name from the nanometre, a unit of measurement equalling one billionth of a meter.

Nanoengineering is largely a synonym for nanotechnology, but emphasizes the engineering rather than the pure science aspects of the field.


The first nanoengineering program was started at the University of Toronto within the Engineering Science program as one of the options of study in the final years. In 2003, the Lund Institute of Technology started a program in Nanoengineering. In 2004, the College of Nanoscale Science and Engineering at SUNY Polytechnic Institute was established on the campus of the University at Albany. In 2005, the University of Waterloo established a unique program which offers a full degree in Nanotechnology Engineering. Louisiana Tech University started the first program in the U.S. in 2005. In 2006 the University of Duisburg-Essen started a Bachelor and a Master program NanoEngineering. Unlike early NanoEngineering programs, the first NanoEngineering Department in the world, offering both undergraduate and graduate degrees, was established by the University of California, San Diego in 2007.
In 2009, the University of Toronto began offering all Options of study in Engineering Science as degrees, bringing the second nanoengineering degree to Canada. Rice University established in 2016 a Department of Materials Science and NanoEngineering (MSNE).
DTU Nanotech - the Department of Micro- and Nanotechnology - is a department at the Technical University of Denmark established in 1990.

In 2013, Wayne State University began offering a Nanoengineering Undergraduate Certificate Program, which is funded by a Nanoengineering Undergraduate Education (NUE) grant from the National Science Foundation. The primary goal is to offer specialized undergraduate training in nanotechnology. Other goals are: 1) to teach emerging technologies at the undergraduate level, 2) to train a new adaptive workforce, and 3) to retrain working engineers and professionals.



 


</doc>
<doc id="21562" url="https://en.wikipedia.org/wiki?curid=21562" title="NP (complexity)">
NP (complexity)

In computational complexity theory, NP (nondeterministic polynomial time) is a complexity class used to classify decision problems. NP is the set of decision problems for which the problem instances, where the answer is "yes", have proofs verifiable in polynomial time by a deterministic Turing machine.

An equivalent definition of NP is the set of decision problems "solvable" in polynomial time by a non-deterministic Turing machine. This definition is the basis for the abbreviation NP; "nondeterministic, polynomial time." These two definitions are equivalent because the algorithm based on the Turing machine consists of two phases, the first of which consists of a guess about the solution, which is generated in a non-deterministic way, while the second phase consists of a deterministic algorithm that verifies if the guess is a solution to the problem.

Decision problems are assigned complexity classes (such as NP) based on the fastest known algorithms. Therefore, decision problems may change classes if faster algorithms are discovered.

It is easy to see that the complexity class P (all problems solvable, deterministically, in polynomial time) is contained in NP (problems where solutions can be verified in polynomial time), because if a problem is solvable in polynomial time then a solution is also verifiable in polynomial time by simply solving the problem. But NP contains many more problems, the hardest of which are called NP-complete problems. An algorithm solving such a problem in polynomial time is also able to solve any other NP problem in polynomial time. The most important P versus NP (“P = NP?”) problem, asks whether polynomial time algorithms exist for solving NP-complete, and by corollary, all NP problems. It is widely believed that this is not the case.

The complexity class NP is related to the complexity class co-NP for which the answer "no" can be verified in polynomial time. Whether or not NP = co-NP is another outstanding question in complexity theory.

The complexity class NP can be defined in terms of NTIME as follows:

where formula_2 is the set of decision problems that can be solved by a non-deterministic Turing machine in formula_3 time.

Alternatively, NP can be defined using deterministic Turing machines as verifiers. A language "L" is in NP if and only if there exist polynomials "p" and "q", and a deterministic Turing machine "M", such that


Many computer science problems are contained in NP, like decision versions of many search and optimization problems.

In order to explain the verifier-based definition of NP, consider the subset sum problem:
Assume that we are given some integers, {−7, −3, −2, 5, 8}, and we wish to know whether some of these integers sum up to zero. Here, the answer is "yes", since the integers {−3, −2, 5} corresponds to the sum The task of deciding whether such a subset with zero sum exists is called the "subset sum problem".

To answer if some of the integers add to zero we can create an algorithm which obtains all the possible subsets. As the number of integers that we feed into the algorithm becomes larger, both the number of subsets and the computation time grows exponentially. 

But notice that if we are given a particular subset we can "efficiently verify" whether the subset sum is zero, by summing the integers of the subset. If the sum is zero, that subset is a "proof" or witness for the answer is "yes". An algorithm that verifies whether a given subset has sum zero is a "verifier". Clearly, summing the integers of a subset can be done in polynomial time and the subset sum problem is therefore in NP.

The above example can be generalized for any decision problem. Given any instance I of problem formula_4 and witness W, if there exists a "verifier" V so that given the ordered pair (I, W) as input, V returns "yes" in polynomial time if the witness proves that the answer is "yes" or "no" in polynomial time otherwise, then formula_4 is in NP.

The "no"-answer version of this problem is stated as: "given a finite set of integers, does every non-empty subset have a nonzero sum?". The verifier-based definition of NP does "not" require an efficient verifier for the "no"-answers. The class of problems with such verifiers for the "no"-answers is called co-NP. In fact, it is an open question whether all problems in NP also have verifiers for the "no"-answers and thus are in co-NP.

In some literature the verifier is called the "certifier" and the witness the "certificate".

Equivalent to the verifier-based definition is the following characterization: NP is the class of decision problems solvable by a non-deterministic Turing machine that runs in polynomial time. That is to say, a decision problem formula_4 is in NP whenever formula_4 is recognized by some polynomial-time non-deterministic Turing machine formula_8 with an existential acceptance condition, meaning that formula_9 if and only if some computation path of formula_10 leads to an accepting state. This definition is equivalent to the verifier-based definition because a non-deterministic Turing machine could solve an NP problem in polynomial time by non-deterministically selecting a certificate and running the verifier on the certificate. Similarly, if such a machine exists, then a polynomial time verifier can naturally be constructed from it.

In this light, we can define co-NP dually as the class of decision problems recognizable by polynomial-time non-deterministic Turing machines with an existential rejection condition. Since an existential rejection condition is exactly the same thing as a universal acceptance condition, we can understand the "NP vs. co-NP" question as asking whether the existential and universal acceptance conditions have the same expressive power for the class of polynomial-time non-deterministic Turing machines.

NP is closed under union, intersection, concatenation, Kleene star and reversal. It is not known whether NP is closed under complement (this question is the so-called "NP versus co-NP" question)

Because of the many important problems in this class, there have been extensive efforts to find polynomial-time algorithms for problems in NP. However, there remain a large number of problems in NP that defy such attempts, seeming to require super-polynomial time. Whether these problems are not decidable in polynomial time is one of the greatest open questions in computer science (see P versus NP ("P=NP") problem for an in-depth discussion).

An important notion in this context is the set of NP-complete decision problems, which is a subset of NP and might be informally described as the "hardest" problems in NP. If there is a polynomial-time algorithm for even "one" of them, then there is a polynomial-time algorithm for "all" the problems in NP. Because of this, and because dedicated research has failed to find a polynomial algorithm for any NP-complete problem, once a problem has been proven to be NP-complete this is widely regarded as a sign that a polynomial algorithm for this problem is unlikely to exist.

However, in practical uses, instead of spending computational resources looking for an optimal solution, a good enough (but potentially suboptimal) solution may often be found in polynomial time. Also, the real life applications of some problems are easier than their theoretical equivalents.

The two definitions of NP as the class of problems solvable by a nondeterministic Turing machine (TM) in polynomial time and the class of problems verifiable by a deterministic Turing machine in polynomial time are equivalent. The proof is described by many textbooks, for example Sipser's "Introduction to the Theory of Computation", section 7.3.

To show this, first suppose we have a deterministic verifier. A nondeterministic machine can simply nondeterministically run the verifier on all possible proof strings (this requires only polynomially many steps because it can nondeterministically choose the next character in the proof string in each step, and the length of the proof string must be polynomially bounded). If any proof is valid, some path will accept; if no proof is valid, the string is not in the language and it will reject.

Conversely, suppose we have a nondeterministic TM called A accepting a given language L. At each of its polynomially many steps, the machine's computation tree branches in at most a finite number of directions. There must be at least one accepting path, and the string describing this path is the proof supplied to the verifier. The verifier can then deterministically simulate A, following only the accepting path, and verifying that it accepts at the end. If A rejects the input, there is no accepting path, and the verifier will always reject.

NP contains all problems in P, since one can verify any instance of the problem by simply ignoring the proof and solving it. NP is contained in PSPACE—to show this, it suffices to construct a PSPACE machine that loops over all proof strings and feeds each one to a polynomial-time verifier. Since a polynomial-time machine can only read polynomially many bits, it cannot use more than polynomial space, nor can it read a proof string occupying more than polynomial space (so we do not have to consider proofs longer than this). NP is also contained in EXPTIME, since the same algorithm operates in exponential time.

co-NP contains those problems which have a simple proof for "no" instances, sometimes called counterexamples. For example, primality testing trivially lies in co-NP, since one can refute the primality of an integer by merely supplying a nontrivial factor. NP and co-NP together form the first level in the polynomial hierarchy, higher only than P.

NP is defined using only deterministic machines. If we permit the verifier to be probabilistic (this however, is not necessarily a BPP machine), we get the class MA solvable using an Arthur-Merlin protocol with no communication from Arthur to Merlin.

NP is a class of decision problems; the analogous class of function problems is FNP.

The only known strict inclusions came from the time hierarchy theorem and the space hierarchy theorem, and respectively they are formula_11 and formula_12.

In terms of descriptive complexity theory, NP corresponds precisely to the set of languages definable by existential second-order logic (Fagin's theorem).

NP can be seen as a very simple type of interactive proof system, where the prover comes up with the proof certificate and the verifier is a deterministic polynomial-time machine that checks it. It is complete because the right proof string will make it accept if there is one, and it is sound because the verifier cannot accept if there is no acceptable proof string.

A major result of complexity theory is that NP can be characterized as the problems solvable by probabilistically checkable proofs where the verifier uses O(log "n") random bits and examines only a constant number of bits of the proof string (the class PCP(log "n", 1)). More informally, this means that the NP verifier described above can be replaced with one that just "spot-checks" a few places in the proof string, and using a limited number of coin flips can determine the correct answer with high probability. This allows several results about the hardness of approximation algorithms to be proven.

This is a list of some problems that are in NP:

All problems in P, denoted formula_13. Given a certificate for a problem in P, we can ignore the certificate and just solve the problem in polynomial time. 

The decision version of the travelling salesman problem is in NP. Given an input matrix of distances between "n" cities, the problem is to determine if there is a route visiting all cities with total distance less than "k".

A proof can simply be a list of the cities. Then verification can clearly be done in polynomial time. It simply adds the matrix entries corresponding to the paths between the cities.

A non-deterministic Turing machine can find such a route as follows:


One can think of each guess as "forking" a new copy of the Turing machine to follow each of the possible paths forward, and if at least one machine finds a route of distance less than "k", that machine accepts the input. (Equivalently, this can be thought of as a single Turing machine that always guesses correctly)

A binary search on the range of possible distances can convert the decision version of Traveling Salesman to the optimization version, by calling the decision version repeatedly (a polynomial number of times). 

The decision problem version of the integer factorization problem: given integers "n" and "k", is there a factor "f" with 1 < "f" < "k" and "f" dividing "n"? 

The Subgraph isomorphism problem of determining whether graph contains a subgraph that is isomorphic to graph .

The boolean satisfiability problem, where we want to know whether or not a certain formula in propositional logic with boolean variables is true for some value of the variables.





</doc>
<doc id="21565" url="https://en.wikipedia.org/wiki?curid=21565" title="November 5">
November 5





</doc>
<doc id="21566" url="https://en.wikipedia.org/wiki?curid=21566" title="Noam Chomsky">
Noam Chomsky

Avram Noam Chomsky (born December 7, 1928) is an American linguist, philosopher, cognitive scientist, historian, social critic, and political activist. Sometimes called "the father of modern linguistics", Chomsky is also a major figure in analytic philosophy, and is one of the founders of the field of cognitive science. He is Laureate Professor of Linguistics at the University of Arizona and Institute Professor Emeritus at the Massachusetts Institute of Technology (MIT), and is the author of more than 100 books on topics such as linguistics, war, politics, and mass media. Ideologically, he aligns with anarcho-syndicalism and libertarian socialism.
Born to Ashkenazi Jewish immigrants in Philadelphia, Chomsky developed an early interest in anarchism from alternative bookstores in New York City. He studied at the University of Pennsylvania. During his postgraduate work in the Harvard Society of Fellows, Chomsky developed the theory of transformational grammar for which he earned his doctorate in 1955. That year he began teaching at MIT, and in 1957 emerged as a significant figure in linguistics with his landmark work "Syntactic Structures", which played a major role in remodeling the study of language. From 1958 to 1959 Chomsky was a National Science Foundation fellow at the Institute for Advanced Study. He created or co-created the universal grammar theory, the generative grammar theory, the Chomsky hierarchy, and the minimalist program. Chomsky also played a pivotal role in the decline of linguistic behaviorism, and was particularly critical of the work of B. F. Skinner.
An outspoken opponent of U.S. involvement in the Vietnam War, which he saw as an act of American imperialism, in 1967 Chomsky rose to national attention for his antiwar essay "The Responsibility of Intellectuals". Associated with the New Left, he was arrested multiple times for his activism and placed on President Richard Nixon's Enemies List. While expanding his work in linguistics over subsequent decades, he also became involved in the linguistics wars. In collaboration with Edward S. Herman, Chomsky later articulated the propaganda model of media criticism in "Manufacturing Consent" and worked to expose the Indonesian occupation of East Timor. His defense of freedom of speech, including Holocaust denial, generated significant controversy in the Faurisson affair of the 1980s. Since retiring from MIT, he has continued his vocal political activism, including opposing the 2003 invasion of Iraq and supporting the Occupy movement. Chomsky began teaching at the University of Arizona in 2017.
One of the most cited scholars alive, Chomsky has influenced a broad array of academic fields. He is widely recognized as having helped to spark the cognitive revolution in the human sciences, contributing to the development of a new cognitivistic framework for the study of language and the mind. In addition to his continued scholarship, he remains a leading critic of U.S. foreign policy, neoliberalism and contemporary state capitalism, the Israeli–Palestinian conflict, and mainstream news media. His ideas are highly influential in the anti-capitalist and anti-imperialist movements, but have also drawn criticism, with some accusing Chomsky of anti-Americanism.

Avram Noam Chomsky was born on December 7, 1928, in the East Oak Lane neighborhood of Philadelphia, Pennsylvania. His parents, Ze'ev "William" Chomsky and Elsie Simonofsky, were Jewish immigrants. William had fled the Russian Empire in 1913 to escape conscription and worked in Baltimore sweatshops and Hebrew elementary schools before attending university. After moving to Philadelphia, William became principal of the Congregation Mikveh Israel religious school and joined the Gratz College faculty. He placed great emphasis on educating people so that they would be "well integrated, free and independent in their thinking, concerned about improving and enhancing the world, and eager to participate in making life more meaningful and worthwhile for all", a mission that shaped and was subsequently adopted by his son. Elsie was a teacher and activist born in Belarus. They met at Mikveh Israel, where they both worked.

Noam was the Chomskys' first child. His younger brother, David Eli Chomsky, was born five years later, in 1934. The brothers were close, though David was more easygoing while Noam could be very competitive. Chomsky and his brother were raised Jewish, being taught Hebrew and regularly involved with discussing the political theories of Zionism; the family was particularly influenced by the Left Zionist writings of Ahad Ha'am. Chomsky faced antisemitism as a child, particularly from Philadelphia's Irish and German communities.

Chomsky attended the independent, Deweyite Oak Lane Country Day School and Philadelphia's Central High School, where he excelled academically and joined various clubs and societies, but was troubled by the school's hierarchical and regimented teaching methods. He also attended Hebrew High School at Gratz College, where his father taught.

Chomsky has described his parents as "normal Roosevelt Democrats" with center-left politics, but relatives involved in the International Ladies' Garment Workers' Union exposed him to socialism and far-left politics. He was substantially influenced by his uncle and the Jewish leftists who frequented his New York City newspaper stand to debate current affairs. Chomsky himself often visited left-wing and anarchist bookstores when visiting his uncle in the city, voraciously reading political literature. He wrote his first article at age 10 on the spread of fascism following the fall of Barcelona during the Spanish Civil War and, from the age of 12 or 13, identified with anarchist politics. He later described his discovery of anarchism as "a lucky accident" that made him critical of Stalinism and other forms of Marxism–Leninism.

In 1945, aged 16, Chomsky began a general program of study at the University of Pennsylvania, where he explored philosophy, logic, and languages and developed a primary interest in learning Arabic. Living at home, he funded his undergraduate degree by teaching Hebrew. Frustrated with his experiences at the university, he considered dropping out and moving to a kibbutz in Mandatory Palestine, but his intellectual curiosity was reawakened through conversations with the Russian-born linguist Zellig Harris, whom he first met in a political circle in 1947. Harris introduced Chomsky to the field of theoretical linguistics and convinced him to major in the subject. Chomsky's BA honors thesis, "Morphophonemics of Modern Hebrew", applied Harris's methods to the language. Chomsky revised this thesis for his MA, which he received from the University of Pennsylvania in 1951; it was subsequently published as a book. He also developed his interest in philosophy while at university, in particular under the tutelage of Nelson Goodman.

From 1951 to 1955 Chomsky was a member of the Society of Fellows at Harvard University, where he undertook research on what became his doctoral dissertation. Having been encouraged by Goodman to apply, Chomsky was attracted to Harvard in part because the philosopher Willard Van Orman Quine was based there. Both Quine and a visiting philosopher, J. L. Austin of the University of Oxford, strongly influenced Chomsky. In 1952 Chomsky published his first academic article, "Systems of Syntactic Analysis", which appeared not in a journal of linguistics but in "The Journal of Symbolic Logic". Highly critical of the established behaviorist currents in linguistics, in 1954 he presented his ideas at lectures at the University of Chicago and Yale University. He had not been registered as a student at Pennsylvania for four years, but in 1955 he submitted a thesis setting out his ideas on transformational grammar; he was awarded a Doctor of Philosophy degree for it, and it was privately distributed among specialists on microfilm before being published in 1975 as part of "The Logical Structure of Linguistic Theory". Harvard professor George Armitage Miller was impressed by Chomsky's thesis and collaborated with him on several technical papers in mathematical linguistics. Chomsky's doctorate exempted him from compulsory military service, which was otherwise due to begin in 1955.

In 1947 Chomsky began a romantic relationship with Carol Doris Schatz, whom he had known since early childhood. They married in 1949. After Chomsky was made a Fellow at Harvard, the couple moved to the Allston area of Boston and remained there until 1965, when they relocated to the suburb of Lexington. In 1953 the couple took a Harvard travel grant to Europe, from the United Kingdom through France, Switzerland into Italy, and Israel, where they lived in Hashomer Hatzair's HaZore'a kibbutz. Despite enjoying himself, Chomsky was appalled by the country's Jewish nationalism, anti-Arab racism and, within the kibbutz's leftist community, pro-Stalinism.

On visits to New York City, Chomsky continued to frequent the office of the Yiddish anarchist journal "Fraye Arbeter Shtime" and became enamored with the ideas of Rudolf Rocker, a contributor whose work introduced Chomsky to the link between anarchism and classical liberalism. Chomsky also read other political thinkers: the anarchists Mikhail Bakunin and Diego Abad de Santillán, democratic socialists George Orwell, Bertrand Russell, and Dwight Macdonald, and works by Marxists Karl Liebknecht, Karl Korsch, and Rosa Luxemburg. His readings convinced him of the desirability of an anarcho-syndicalist society, and he became fascinated by the anarcho-syndicalist communes set up during the Spanish Civil War, as documented in Orwell's "Homage to Catalonia" (1938). He read the leftist journal "Politics", which furthered his interest in anarchism, and the council communist periodical "Living Marxism", though he rejected the orthodoxy of its editor, Paul Mattick. He was also greatly interested in the Marlenite ideas of the Leninist League of the United States, an anti-Stalinist Marxist–Leninist group, sharing their view that the Second World War was orchestrated by Western capitalists and the Soviet Union's "state capitalists" to crush Europe's proletariat.

Chomsky befriended two linguists at the Massachusetts Institute of Technology (MIT), Morris Halle and Roman Jakobson, the latter of whom secured him an assistant professor position there in 1955. At MIT, Chomsky spent half his time on a mechanical translation project and half teaching a course on linguistics and philosophy. He described MIT as "a pretty free and open place, open to experimentation and without rigid requirements. It was just perfect for someone of my idiosyncratic interests and work." In 1957 MIT promoted him to the position of associate professor, and from 1957 to 1958 he was also employed by Columbia University as a visiting professor. The Chomskys had their first child that same year, a daughter named Aviva. He also published his first book on linguistics, "Syntactic Structures", a work that radically opposed the dominant Harris–Bloomfield trend in the field. Responses to Chomsky's ideas ranged from indifference to hostility, and his work proved divisive and caused "significant upheaval" in the discipline. The linguist John Lyons later asserted that "Syntactic Structures" "revolutionized the scientific study of language". From 1958 to 1959 Chomsky was a National Science Foundation fellow at the Institute for Advanced Study in Princeton, New Jersey.

In 1959, Chomsky published a review of B. F. Skinner's 1957 book "Verbal Behavior" in the academic journal "Language", in which he argued against Skinner's view of language as learned behavior. The review argued that Skinner ignored the role of human creativity in linguistics and helped to establish Chomsky as an intellectual. With Halle, Chomsky proceeded to found MIT's graduate program in linguistics. In 1961 he was awarded tenure, becoming a full professor in the Department of Modern Languages and Linguistics. Chomsky went on to be appointed plenary speaker at the Ninth International Congress of Linguists, held in 1962 in Cambridge, Massachusetts, which established him as the "de facto" spokesperson of American linguistics. Between 1963 and 1965 he consulted on a military-sponsored project "to establish natural language as an operational language for command and control"; Barbara Partee, a collaborator on this project and then-student of Chomsky, has said this research was justified to the military on the basis that "in the event of a nuclear war, the generals would be underground with some computers trying to manage things, and that it would probably be easier to teach computers to understand English than to teach the generals to program."

Chomsky continued to publish his linguistic ideas throughout the decade, including in "Aspects of the Theory of Syntax" (1965), "Topics in the Theory of Generative Grammar" (1966), and "" (1966). Along with Halle, he also edited the "Studies in Language" series of books for Harper and Row. As he began to accrue significant academic recognition and honors for his work, Chomsky lectured at the University of California, Berkeley, in 1966. His Beckman lectures at Berkeley were assembled and published as "Language and Mind" in 1968. Despite his growing stature, an intellectual falling-out between Chomsky and some of his early colleagues and doctoral students—including Paul Postal, John "Haj" Ross, George Lakoff, and James D. McCawley—triggered a series of academic debates that came to be known as the "Linguistics Wars", although they revolved largely around philosophical issues rather than linguistics proper.

Chomsky joined protests against U.S. involvement in the Vietnam War in 1962, speaking on the subject at small gatherings in churches and homes. His 1967 critique of U.S. involvement, "The Responsibility of Intellectuals", among other contributions to "The New York Review of Books", debuted Chomsky as a public dissident. This essay and other political articles were collected and published in 1969 as part of Chomsky's first political book, "American Power and the New Mandarins". He followed this with further political books, including "At War with Asia" (1971), "The Backroom Boys" (1973), "For Reasons of State" (1973), and "Peace in the Middle East?" (1975), published by Pantheon Books. These publications led to Chomsky's association with the American New Left movement, though he thought little of prominent New Left intellectuals Herbert Marcuse and Erich Fromm and preferred the company of activists to that of intellectuals. Chomsky remained largely ignored by the mainstream press throughout this period.

He also became involved in left-wing activism. Chomsky refused to pay half his taxes, publicly supported students who refused the draft, and was arrested while participating an antiwar teach-in outside the Pentagon. During this time, Chomsky co-founded the antiwar collective RESIST with Mitchell Goodman, Denise Levertov, William Sloane Coffin, and Dwight Macdonald. Although he questioned the objectives of the 1968 student protests, Chomsky gave many lectures to student activist groups and, with his colleague Louis Kampf, ran undergraduate courses on politics at MIT independently of the conservative-dominated political science department. When student activists campaigned to stop weapons and counterinsurgency research at MIT, Chomsky was sympathetic but felt that the research should remain under MIT's oversight and limited to systems of deterrence and defense. In 1970 he visited southeast Asia to lecture at Vietnam's Hanoi University of Science and Technology and toured war refugee camps in Laos. In 1973 he helped lead a committee commemorating the 50th anniversary of the War Resisters League.

Because of his antiwar activism, Chomsky was arrested on multiple occasions and included on President Richard Nixon's master list of political opponents. Chomsky was aware of the potential repercussions of his civil disobedience and his wife began studying for her own doctorate in linguistics to support the family in the event of Chomsky's imprisonment or joblessness. Chomsky's scientific reputation insulated him from administrative action based on his beliefs.

His work in linguistics continued to gain international recognition as he received multiple honorary doctorates. He delivered public lectures at the University of Cambridge, Columbia University (Woodbridge Lectures), and Stanford University. His appearance in a 1971 debate with French continental philosopher Michel Foucault positioned Chomsky as a symbolic figurehead of analytic philosophy. He continued to publish extensively on linguistics, producing "Studies on Semantics in Generative Grammar" (1972), an enlarged edition of "Language and Mind" (1972), and "Reflections on Language" (1975). In 1974 Chomsky became a corresponding fellow of the British Academy.

In the late 1970s and 1980s, Chomsky's linguistic publications expanded and clarified his earlier work, addressing his critics and updating his grammatical theory. His political talks often generated considerable controversy, particularly when he criticized the Israeli government and military. In the early 1970s Chomsky began collaborating with Edward S. Herman, who had also published critiques of the U.S. war in Vietnam. Together they wrote "", a book that criticized U.S. military involvement in Southeast Asia and the mainstream media's failure to cover it. Warner Modular published it in 1973, but its parent company disapproved of the book's contents and ordered all copies destroyed.

While mainstream publishing options proved elusive, Chomsky found support from Michael Albert's South End Press, an activist-oriented publishing company. In 1979, South End published Chomsky and Herman's revised "Counter-Revolutionary Violence" as the two-volume "The Political Economy of Human Rights", which compares U.S. media reactions to the Cambodian genocide and the Indonesian occupation of East Timor. It argues that because Indonesia was a U.S. ally, U.S. media ignored the East Timorese situation while focusing on events in Cambodia, a U.S. enemy. Chomsky's response included two testimonials before the United Nations' Special Committee on Decolonization, successful encouragement for American media to cover the occupation, and meetings with refugees in Lisbon. The Marxist academic Steven Lukes publicly accused Chomsky of betraying his anarchist ideals and acting as an apologist for Cambodian leader Pol Pot. The controversy damaged Chomsky's reputation, and he maintains that his critics deliberately printed lies to defame him.

Chomsky had long publicly criticized Nazism, and totalitarianism more generally, but his commitment to freedom of speech led him to defend the right of French historian Robert Faurisson to advocate a position widely characterized as Holocaust denial. Without Chomsky's knowledge, his plea for Faurisson's freedom of speech was published as the preface to the latter's 1980 book . Chomsky was widely condemned for defending Faurisson, and France's mainstream press accused Chomsky of being a Holocaust denier himself, refusing to publish his rebuttals to their accusations. Critiquing Chomsky's position, sociologist Werner Cohn later published an analysis of the affair titled "Partners in Hate: Noam Chomsky and the Holocaust Deniers". The Faurisson affair had a lasting, damaging effect on Chomsky's career, especially in France.

In 1985, during the Nicaraguan Contra War—in which the U.S. supported the contra militia against the Sandinista government—Chomsky traveled to Managua to meet with workers' organizations and refugees of the conflict, giving public lectures on politics and linguistics. Many of these lectures were published in 1987 as "On Power and Ideology: The Managua Lectures". In 1983 he published "The Fateful Triangle", which argued that the U.S. had continually used the Israeli–Palestinian conflict for its own ends. In 1988, Chomsky visited the Palestinian territories to witness the impact of Israeli occupation.

In 1988, Chomsky and Herman published "", in which they outlined their propaganda model for understanding mainstream media. They argued that even in countries without official censorship, the news is censored through five filters that have great impact on what stories are reported and how they are presented. The book was inspired by Alex Carey and adapted into . In 1989, Chomsky published "Necessary Illusions: Thought Control in Democratic Societies," in which he suggests that democratic citizens, to make a worthwhile democracy, undertake intellectual self-defense against the media and elite intellectual culture that seeks to control them. By the 1980s, Chomsky's students had become prominent linguists who, in turn, expanded and revised his linguistic theories.

In the 1990s, Chomsky embraced political activism to a greater degree than before. Retaining his commitment to the cause of East Timorese independence, in 1995 he visited Australia to talk on the issue at the behest of the East Timorese Relief Association and the National Council for East Timorese Resistance. The lectures he gave on the subject were published as "Powers and Prospects" in 1996. As a result of the international publicity Chomsky generated, his biographer Wolfgang Sperlich opined that he did more to aid the cause of East Timorese independence than anyone but the investigative journalist John Pilger. After East Timor attained independence from Indonesia in 1999, the Australian-led International Force for East Timor arrived as a peacekeeping force; Chomsky was critical of this, believing it was designed to secure Australian access to East Timor's oil and gas reserves under the Timor Gap Treaty.

After the September 11 attacks in 2001, Chomsky was widely interviewed; Seven Stories Press collated and published these interviews that October. Chomsky argued that the ensuing War on Terror was not a new development but a continuation of U.S. foreign policy and concomitant rhetoric since at least the Reagan era. He gave the D.T. Lakdawala Memorial Lecture in New Delhi in 2001, and in 2003 visited Cuba at the invitation of the Latin American Association of Social Scientists. Chomsky's 2003 "Hegemony or Survival" articulated what he called the United States' "imperial grand strategy" and critiqued the Iraq War and other aspects of the War on Terror. Chomsky toured internationally with greater regularity during this period.

Chomsky retired from MIT in 2002, but continued to conduct research and seminars on campus as an emeritus. That same year he visited Turkey to attend the trial of a publisher who had been accused of treason for printing one of Chomsky's books; Chomsky insisted on being a co-defendant and amid international media attention the Security Courts dropped the charge on the first day. During that trip Chomsky visited Kurdish areas of Turkey and spoke out in favor of the Kurds' human rights. A supporter of the World Social Forum, he attended its conferences in Brazil in both 2002 and 2003, also attending the Forum event in India.

Chomsky supported the Occupy movement, delivering talks at encampments and producing two works that chronicled its influence: "Occupy" (2012), a pamphlet, and "Occupy: Reflections on Class War, Rebellion and Solidarity" (2013). He attributed Occupy's growth to a perception that the Democratic Party had abandoned the interests of the white working class. In March 2014, Chomsky joined the advisory council of the Nuclear Age Peace Foundation, an organization that advocates the global abolition of nuclear weapons, as a senior fellow. The 2016 documentary "Requiem for the American Dream" summarizes his views on capitalism and economic inequality through a "75-minute teach-in".
In 2017, Chomsky taught a short-term politics course at the University of Arizona in Tucson and was later hired as a part-time professor in the linguistics department there, with his duties including teaching and public seminars. His salary is covered by philanthropic donations.

Chomsky signed the Declaration on the Common Language of the Croats, Serbs, Bosniaks and Montenegrins in 2018.

The basis of Chomsky's linguistic theory lies in biolinguistics, the linguistic school that holds that the principles underpinning the structure of language are biologically preset in the human mind and hence genetically inherited. As such he argues that all humans share the same underlying linguistic structure, irrespective of sociocultural differences. In adopting this position Chomsky rejects the radical behaviorist psychology of B. F. Skinner, who viewed behavior (including talking and thinking) as a completely learned product of the interactions between organisms and their environments. Accordingly, Chomsky argues that language is a unique evolutionary development of the human species and distinguished from modes of communication used by any other animal species. Chomsky's nativist, internalist view of language is consistent with the philosophical school of "rationalism" and contrasts with the anti-nativist, externalist view of language consistent with the philosophical school of "empiricism", which contends that all knowledge, including language, comes from external stimuli.

Since the 1960s Chomsky has maintained that syntactic knowledge is at least partially inborn, implying that children need only learn certain language-specific features of their native languages. He bases his argument on observations about human language acquisition and describes a "poverty of the stimulus": an enormous gap between the linguistic stimuli to which children are exposed and the rich linguistic competence they attain. For example, although children are exposed to only a very small and finite subset of the allowable syntactic variants within their first language, they somehow acquire the highly organized and systematic ability to understand and produce an infinite number of sentences, including ones that have never before been uttered, in that language. To explain this, Chomsky reasoned that the primary linguistic data must be supplemented by an innate linguistic capacity. Furthermore, while a human baby and a kitten are both capable of inductive reasoning, if they are exposed to exactly the same linguistic data, the human will always acquire the ability to understand and produce language, while the kitten will never acquire either ability. Chomsky labeled whatever relevant capacity the human has that the cat lacks the language acquisition device, and suggested that one of linguists' tasks should be to determine what that device is and what constraints it imposes on the range of possible human languages. The universal features that result from these constraints would constitute "universal grammar". Multiple scholars have challenged universal grammar on the grounds of the evolutionary infeasibility of its genetic basis for language, the lack of universal characteristics between languages, and the unproven link between innate/universal structures and the structures of specific languages. Scholar Michael Tomasello has challenged Chomsky's theory of innate syntactic knowledge as based in logic and not empiricism.

Transformational-generative grammar is a broad theory used to model, encode, and deduce a native speaker's linguistic capabilities. These models, or "formal grammars", show the abstract structures of a specific language as they may relate to structures in other languages. Chomsky developed transformational grammar in the mid-1950s, whereupon it became the dominant syntactic theory in linguistics for two decades. "Transformations" refers to syntactic relationships within language, e.g., being able to infer that the subject between two sentences is the same person. Chomsky's theory posits that language consists of both deep structures and surface structures: Outward-facing surface structures relate phonetic rules into sound, while inward-facing deep structures relate words and conceptual meaning. Transformational-generative grammar uses mathematical notation to express the rules that govern the connection between meaning and sound (deep and surface structures, respectively). By this theory, linguistic principles can mathematically generate potential sentences structures in a language.

Based on this rule-based notation of grammars, Chomsky grouped natural languages into a series of four nested subsets and increasingly complex types, together known as the Chomsky hierarchy. This classification was and remains foundational to formal language theory, and relevant to theoretical computer science, especially programming language theory, compiler construction, and automata theory.

Following transformational grammar's heyday through the mid-1970s, a derivative government and binding theory became a dominant research framework through the early 1990s (and remains an influential theory) when linguists turned to a "minimalist" approach to grammar. This research focused on the principles and parameters framework, which explained children's ability to learn any language by filling open parameters (a set of universal grammar principles) that adapt as the child encounters linguistic data. The minimalist program, initiated by Chomsky, asks which minimal principles and parameters theory fits most elegantly, naturally, and simply. In an attempt to simplify language into a system that relates meaning and sound using the minimum possible faculties, Chomsky dispenses with concepts such as "deep structure" and "surface structure" and instead emphasizes the plasticity of the brain's neural circuits, with which come an infinite number of concepts, or "logical forms". When exposed to linguistic data, a hearer-speaker's brain proceeds to associate sound and meaning, and the rules of grammar we observe are in fact only the consequences, or side effects, of the way language works. Thus while much of Chomsky's prior research focused on the rules of language, he now focuses on the mechanisms the brain uses to generate these rules and regulate speech.

Chomsky is a prominent political dissident. His political views have changed little since his childhood, when he was influenced by the emphasis on political activism that was ingrained in Jewish working-class tradition. He usually identifies as an anarcho-syndicalist or a libertarian socialist. He views these positions not as precise political theories but as ideals that he thinks best meet human needs: liberty, community, and freedom of association. Unlike some other socialists, such as Marxists, Chomsky believes that politics lies outside the remit of science, but he still roots his ideas about an ideal society in empirical data and empirically justified theories.

In Chomsky's view, the truth about political realities is systematically distorted or suppressed by an elite corporatocracy, which uses corporate media, advertising, and think tanks to promote its own propaganda. His work seeks to reveal such manipulations and the truth they obscure. Chomsky believes this web of falsehood can be broken by "common sense", critical thinking, and understanding the roles of self-interest and self-deception, and that intellectuals abdicate their moral responsibility to tell the truth about the world in fear of losing prestige and funding. He argues that, as such an intellectual, it is his duty to use his social privilege, resources, and training to aid popular democracy movements in their struggles.

Although he has joined protest marches and organized activist groups, Chomsky's primary political outlets are education and publication. He offers a wide range of political writings as well as free lessons and lectures to encourage wider political consciousness. He is a member of the Industrial Workers of the World international union.

Chomsky has been a prominent critic of American imperialism; he believes that the basic principle of the foreign policy of the United States is the establishment of "open societies" that are economically and politically controlled by the United States and where U.S.-based businesses can prosper. He argues that the U.S. seeks to suppress any movements within these countries that are not compliant with U.S. interests and to ensure that U.S.-friendly governments are placed in power. When discussing current events, he emphasizes their place within a wider historical perspective. He believes that official, sanctioned historical accounts of U.S. and British extraterritorial operations have consistently whitewashed these nations' actions in order to present them as having benevolent motives in either spreading democracy or, in older instances, spreading Christianity; criticizing these accounts, he seeks to correct them. Prominent examples he regularly cites are the actions of the British Empire in India and Africa and the actions of the U.S. in Vietnam, the Philippines, Latin America, and the Middle East.

Chomsky's political work has centered heavily on criticizing the actions of the United States. He has said he focuses on the U.S. because the country has militarily and economically dominated the world during his lifetime and because its liberal democratic electoral system allows the citizenry to influence government policy. His hope is that, by spreading awareness of the impact U.S. foreign policies have on the populations affected by them, he can sway the populations of the U.S. and other countries into opposing the policies. He urges people to criticize their governments' motivations, decisions, and actions, to accept responsibility for their own thoughts and actions, and to apply the same standards to others as to themselves.

Chomsky has been critical of U.S. involvement in the Israeli–Palestinian conflict, arguing that it has consistently blocked a peaceful settlement. Chomsky also criticizes the U.S.'s close ties with Saudi Arabia and involvement in Saudi Arabian-led intervention in Yemen, highlighting that Saudi Arabia has "one of the most grotesque human rights records in the world".

In his youth, Chomsky developed a dislike of capitalism and the pursuit of material wealth. At the same time, he developed a disdain for authoritarian socialism, as represented by the Marxist–Leninist policies of the Soviet Union. Rather than accepting the common view among U.S. economists that a spectrum exists between total state ownership of the economy and total private ownership, he instead suggests that a spectrum should be understood between total democratic control of the economy and total autocratic control (whether state or private). He argues that Western capitalist countries are not really democratic, because, in his view, a truly democratic society is one in which all persons have a say in public economic policy. He has stated his opposition to ruling elites, among them institutions like the IMF, World Bank, and GATT (precursor to the WTO).

Chomsky highlights that, since the 1970s, the U.S. has become increasingly economically unequal as a result of the repeal of various financial regulations and the rescinding of the Bretton Woods financial control agreement. He characterizes the U.S. as a "de facto" one-party state, viewing both the Republican Party and Democratic Party as manifestations of a single "Business Party" controlled by corporate and financial interests. Chomsky highlights that, within Western capitalist liberal democracies, at least 80% of the population has no control over economic decisions, which are instead in the hands of a management class and ultimately controlled by a small, wealthy elite.

Noting the entrenchment of such an economic system, Chomsky believes that change is possible through the organized cooperation of large numbers of people who understand the problem and know how they want to reorganize the economy more equitably. Acknowledging that corporate domination of media and government stifles any significant change to this system, he sees reason for optimism in historical examples such as the social rejection of slavery as immoral, the advances in women's rights, and the forcing of government to justify invasions. He views violent revolution to overthrow a government as a last resort to be avoided if possible, citing the example of historical revolutions where the population's welfare has worsened as a result of upheaval.

Chomsky sees libertarian socialist and anarcho-syndicalist ideas as the descendants of the classical liberal ideas of the Age of Enlightenment, arguing that his ideological position revolves around "nourishing the libertarian and creative character of the human being". He envisions an anarcho-syndicalist future with direct worker control of the means of production and government by workers' councils, who would select representatives to meet together at general assemblies. The point of this self-governance is to make each citizen, in Thomas Jefferson's words, "a direct participator in the government of affairs." He believes that there will be no need for political parties. By controlling their productive life, he believes that individuals can gain job satisfaction and a sense of fulfillment and purpose. He argues that unpleasant and unpopular jobs could be fully automated, carried out by workers who are specially remunerated, or shared among everyone.

Chomsky has written prolifically on the Israeli-Palestinian conflict, aiming to raise public awareness of it. He has long endorsed a left binationalist program in Israel and Palestine, seeking to create a democratic state in the Levant that is home to both Jews and Arabs. Nevertheless, given the realpolitik of the situation, he has also considered a two-state solution on the condition that the nation-states exist on equal terms. Chomsky was denied entry to the West Bank in 2010 because of his criticisms of Israel. He had been invited to deliver a lecture at Bir Zeit University and was to meet with Palestinian Prime Minister Salam Fayyad. An Israeli Foreign Ministry spokesman later said that Chomsky was denied entry by mistake.

Chomsky's political writings have largely focused on ideology, social and political power, the media, and state policy. One of his best-known works, "Manufacturing Consent", dissects the media's role in reinforcing and acquiescing to state policies across the political spectrum while marginalizing contrary perspectives. Chomsky asserts that this version of censorship, by government-guided "free market" forces, is subtler and harder to undermine than was the equivalent propaganda system in the Soviet Union. As he argues, the mainstream press is corporate-owned and thus reflects corporate priorities and interests. Acknowledging that many American journalists are dedicated and well-meaning, he argues that the mass media's choices of topics and issues, the unquestioned premises on which that coverage rests, and the range of opinions expressed are all constrained to reinforce the state's ideology: although mass media will criticize individual politicians and political parties, it will not undermine the wider state-corporate nexus of which it is a part. As evidence, he highlights that the U.S. mass media does not employ any socialist journalists or political commentators. He also points to examples of important news stories that the U.S. mainstream media has ignored because reporting on them would reflect badly upon the country, including the murder of Black Panther Fred Hampton with possible FBI involvement, the massacres in Nicaragua perpetrated by U.S.-funded Contras, and the constant reporting on Israeli deaths without equivalent coverage of the far larger number of Palestinian deaths in that conflict. To remedy this situation, Chomsky calls for grassroots democratic control and involvement of the media.

Chomsky considers most conspiracy theories fruitless, distracting substitutes for thinking about policy formation in an institutional framework, where individual manipulation is secondary to broader social imperatives. While not dismissing them outright, he considers them unproductive to challenging power in a substantial way. In response to the labeling of his own ideas as a conspiracy theory, Chomsky has said that it is very rational for the media to manipulate information in order to sell it, like any other business. He asks whether General Motors would be accused of conspiracy if it deliberately selected what it used or discarded to sell its product.

Chomsky has also been active in a number of philosophical fields, including philosophy of mind, philosophy of language, and philosophy of science. In these fields he is credited with ushering in the "cognitive revolution", a significant paradigm shift that rejected logical positivism, the prevailing philosophical methodology of the time, and reframed how philosophers think about language and the mind. Chomsky views the cognitive revolution as rooted in 17th-century rationalist ideals. His position—the idea that the mind contains inherent structures to understand language, perception, and thought—has more in common with rationalism (Enlightenment and Cartesian) than behaviorism. He named one of his key works "Cartesian Linguistics: A Chapter in the History of Rationalist Thought" (1966). In philosophy of language, Chomsky is particularly known for his criticisms of the notion of reference and meaning in human language and his perspective on the nature and function of mental representations.

Chomsky's famous 1971 debate on human nature with the French philosopher Michel Foucault was symbolic in positioning Chomsky as the prototypical analytic philosopher against Foucault, a stalwart of the continental tradition. It showed what appeared to be irreconcilable differences between two moral and intellectual luminaries of the 20th century. Foucault's position was that of critique, that human nature could not be conceived in terms foreign to present understanding, while Chomsky held that human nature contained universalities such as a common standard of moral justice as deduced through reason based on what rationally serves human necessity. Chomsky criticized postmodernism and French philosophy generally, arguing that the obscure language of postmodern, leftist philosophers gives little aid to the working classes. He has also debated analytic philosophers, including Tyler Burge, Donald Davidson, Michael Dummett, Saul Kripke, Thomas Nagel, Hilary Putnam, Willard Van Orman Quine, and John Searle.

Chomsky's contributions span intellectual and world history, including history of philosophy. Irony is a recurring characteristic of his writing, as he often implies that his readers know better, which can make them more engaged in the veracity of his claims.

Chomsky endeavors to keep his family life, linguistic scholarship, and political activism strictly separate from one another, calling himself "scrupulous at keeping my politics out of the classroom". An intensely private person, he is uninterested in appearances and the fame his work has brought him. He also has little interest in modern art and music. McGilvray suggests that Chomsky was never motivated by a desire for fame, but impelled to tell what he perceived as the truth and a desire to aid others in doing so. Chomsky acknowledges that his income affords him a privileged life compared to the majority of the world's population; nevertheless, he characterizes himself as a "worker", albeit one who uses his intellect as his employable skill. He reads four or five newspapers daily; in the US, he subscribes to "The Boston Globe", "The New York Times", "The Wall Street Journal", "Financial Times", and "The Christian Science Monitor". Chomsky is non-religious, but has expressed approval of forms of religion such as liberation theology.

Chomsky has attracted controversy for calling established political and academic figures "corrupt", "fascist", and "fraudulent". His colleague Steven Pinker has said that he "portrays people who disagree with him as stupid or evil, using withering scorn in his rhetoric", and that this contributes to the extreme reactions he receives from critics. Chomsky avoids attending academic conferences, including left-oriented ones such as the Socialist Scholars Conference, preferring to speak to activist groups or hold university seminars for mass audiences. His approach to academic freedom has led him to support MIT academics whose actions he deplores; in 1969, when Chomsky heard that Walt Rostow, a major architect of the Vietnam war, wanted to return to work at MIT, Chomsky threatened "to protest publicly" if Rostow was denied a position at MIT. In 1989, when Pentagon adviser John Deutch applied to be president of MIT, Chomsky supported his candidacy. Later, when Deutch became head of the CIA, "The New York Times" quoted Chomsky as saying, "He has more honesty and integrity than anyone I've ever met. ... If somebody's got to be running the CIA, I'm glad it's him."

Chomsky was married to Carol () from 1949 until her death in 2008. They had three children together: Aviva (b. 1957), Diane (b. 1960), and Harry (b. 1967). In 2014, Chomsky married Valeria Wasserman.

Chomsky has been a defining Western intellectual figure, central to the field of linguistics and definitive in cognitive science, computer science, philosophy, and psychology. In addition to being known as one of the most important intellectuals of his time, Chomsky carries a dual legacy as both a "leader in the field" of linguistics and "a figure of enlightenment and inspiration" for political dissenters. Despite his academic success, his political viewpoints and activism have resulted in his being distrusted by the mainstream media apparatus, and he is regarded as being "on the outer margin of acceptability". The reception of his work is intertwined with his public image as an anarchist, a gadfly, an historian, a Jew, a linguist, and a philosopher.

McGilvray observes that Chomsky inaugurated the "cognitive revolution" in linguistics, and that he is largely responsible for establishing the field as a formal, natural science, moving it away from the procedural form of structural linguistics dominant during the mid-20th century. As such, some have called Chomsky "the father of modern linguistics". Linguist John Lyons further remarked that within a few decades of publication, Chomskyan linguistics had become "the most dynamic and influential" school of thought in the field. By the 1970s his work had also come to exert a considerable influence on philosophy, and a Minnesota State University Moorhead poll ranked "Syntactic Structures" as the single most important work in cognitive science. In addition, his work in automata theory and the Chomsky hierarchy have become well known in computer science, and he is much cited in computational linguistics.

Chomsky's criticisms of behaviorism contributed substantially to the decline of behaviorist psychology; in addition, he is generally regarded as one of the primary founders of the field of cognitive science. Some arguments in evolutionary psychology are derived from his research results; Nim Chimpsky, a chimpanzee who was the subject of a study in animal language acquisition at Columbia University, was named after Chomsky in reference to his view of language acquisition as a uniquely human ability.

ACM Turing Award winner Donald Knuth credited Chomsky's work with helping him combine his interests in mathematics, linguistics, and computer science. IBM computer scientist John Backus, another Turing Award winner, used some of Chomsky's concepts to help him develop FORTRAN, the first widely used high-level computer programming language. The laureates of the 1984 Nobel Prize in Physiology or Medicine—Georges J. F. Köhler, César Milstein, and Niels Kaj Jerne—used Chomsky's generative model to explain the human immune system, equating "components of a generative grammar ... with various features of protein structures." Chomsky's theory of generative grammar has also influenced work in music theory and analysis.

An MIT press release stated that Chomsky was cited within the Arts and Humanities Citation Index more often than any other living scholar from 1980 to 1992. Chomsky was also extensively cited in the Social Sciences Citation Index and Science Citation Index during the same time period, with the librarian who conducted the research commenting that the statistics show that "he is very widely read across disciplines and that his work is used by researchers across disciplines ... it seems that you can't write a paper without citing Noam Chomsky." As a result of his influence, there are dueling camps of Chomskyan and non-Chomskyan linguistics, with the disputes between the two camps often acrimonious.

Chomsky's status as the "most-quoted living author" is credited to his political writings, which vastly outnumber his writings on linguistics. Chomsky biographer Wolfgang B. Sperlich characterizes him as "one of the most notable contemporary champions of the people"; journalist John Pilger has described him as a "genuine people's hero; an inspiration for struggles all over the world for that basic decency known as freedom. To a lot of people in the margins—activists and movements—he's unfailingly supportive." Arundhati Roy has called him "one of the greatest, most radical public thinkers of our time", and Edward Said thought him "one of the most significant challengers of unjust power and delusions". Fred Halliday has said that by the start of the 21st century Chomsky had become a "guru" for the world's anti-capitalist and anti-imperialist movements. The propaganda model of media criticism that he and Herman developed has been widely accepted in radical media critiques and adopted to some level in mainstream criticism of the media, also exerting a significant influence on the growth of alternative media, including radio, publishers, and the Internet, which in turn have helped to disseminate his work.

Sperlich also notes that Chomsky has been vilified by corporate interests, particularly in the mainstream press. University departments devoted to history and political science rarely include Chomsky's work on their undergraduate syllabi. Critics have argued that despite publishing widely on social and political issues, Chomsky has no formal expertise in these areas; he has responded that such issues are not as complex as many social scientists claim and that almost everyone is able to comprehend them regardless of whether they have been academically trained to do so. According to McGilvray, many of Chomsky's critics "do not bother quoting his work or quote out of context, distort, and create straw men that cannot be supported by Chomsky's text".

Chomsky drew criticism for not calling the Srebrenica massacre during the Bosnian War a "genocide", which he said would devalue the word, and in appearing to deny Ed Vulliamy's reporting on the existence of Bosnian concentration camps. The subsequent editorial correction of his comments, viewed as a capitulation, was criticized by multiple Balkan watchers.

Chomsky's far-reaching criticisms of U.S. foreign policy and the legitimacy of U.S. power have raised controversy. A document obtained pursuant to a Freedom of Information Act (FOIA) request from the U.S. government revealed that the Central Intelligence Agency (CIA) monitored his activities and for years denied doing so. The CIA also destroyed its files on Chomsky at some point, possibly in violation of federal law. He has often received undercover police protection at MIT and when speaking on the Middle East, but has refused uniformed police protection. German newspaper "Der Spiegel" described Chomsky as "the Ayatollah of anti-American hatred", while conservative commentator David Horowitz called him "the most devious, the most dishonest and ... the most treacherous intellect in America", whose work is infused with "anti-American dementia" and evidences his "pathological hatred of his own country". Writing in "Commentary" magazine, the journalist Jonathan Kay described Chomsky as "a hard-boiled anti-American monomaniac who simply refuses to believe anything that any American leader says".

Chomsky's criticism of Israel has led to his being called a traitor to the Jewish people and an anti-Semite. Criticizing Chomsky's defense of the right of individuals to engage in Holocaust denial on the grounds that freedom of speech must be extended to all viewpoints, Werner Cohn called Chomsky "the most important patron" of the neo-Nazi movement. The Anti-Defamation League (ADL), called him a Holocaust denier, describing him as a "dupe of intellectual pride so overweening that he is incapable of making distinctions between totalitarian and democratic societies, between oppressors and victims". In turn, Chomsky has claimed that the ADL is dominated by "Stalinist types" who oppose democracy in Israel. The lawyer Alan Dershowitz has called Chomsky a "false prophet of the left"; Chomsky called Dershowitz "a complete liar" who is on "a crazed jihad, dedicating much of his life to trying to destroy my reputation". In early 2016 President Recep Tayyip Erdoğan of Turkey publicly rebuked Chomsky after he signed an open letter condemning Erdoğan for his anti-Kurdish repression and double standards on terrorism. Chomsky accused Erdoğan of hypocrisy, noting that Erdoğan supports al-Qaeda's Syrian affiliate, the al-Nusra Front.

In February 2020, before attending the 2020 Hay Festival in Abu Dhabi, United Arab Emirates, Chomsky signed a letter of condemnation of the violation of freedom of speech in the emirate, referring to the arrest of human rights activist Ahmed Mansoor. Other signers included authors Stephen Fry and Jung Chang.

In 1970, the London "Times" named Chomsky one of the "makers of the twentieth century". He was voted the world's leading public intellectual in The 2005 Global Intellectuals Poll jointly conducted by American magazine "Foreign Policy" and British magazine "Prospect". "New Statesman" readers listed Chomsky among the world's foremost heroes in 2006.

In the United States he is a Member of the National Academy of Sciences, the American Academy of Arts and Sciences, the Linguistic Society of America, the American Philosophical Association, and the American Association for the Advancement of Science. Abroad he is a corresponding fellow of the British Academy, an honorary member of the British Psychological Society, a member of the Deutsche Akademie der Naturforscher Leopoldina, and a foreign member of the Department of Social Sciences of the Serbian Academy of Sciences and Arts. He received a 1971 Guggenheim Fellowship, the 1984 American Psychological Association Award for Distinguished Contributions to Psychology, the 1988 Kyoto Prize in Basic Sciences, the 1996 Helmholtz Medal, the 1999 Benjamin Franklin Medal in Computer and Cognitive Science, the 2010 Erich Fromm Prize, and the British Academy's 2014 Neil and Saras Smith Medal for Linguistics. He is also a two-time winner of the NCTE George Orwell Award for Distinguished Contribution to Honesty and Clarity in Public Language (1987 and 1989). He has also received the Rabindranath Tagore Centenary Award from The Asiatic Society.

Chomsky received the 2004 Carl-von-Ossietzky Prize from the city of Oldenburg, Germany, to acknowledge his body of work as a political analyst and media critic. He received an honorary fellowship in 2005 from the Literary and Historical Society of University College Dublin. He received the 2008 President's Medal from the Literary and Debating Society of the National University of Ireland, Galway. Since 2009, he has been an honorary member of International Association of Professional Translators and Interpreters (IAPTI). He received the University of Wisconsin's A.E. Havens Center's Award for Lifetime Contribution to Critical Scholarship and was inducted into IEEE Intelligent Systems' AI's Hall of Fame for "significant contributions to the field of AI and intelligent systems." Chomsky has an Erdős number of four.

In 2011, the US Peace Memorial Foundation awarded Chomsky the US Peace Prize for antiwar activities over five decades. For his work in human rights, peace, and social criticism, he received the 2011 Sydney Peace Prize, the 2017 Seán MacBride Peace Prize and the Dorothy Eldridge Peacemaker Award.

Chomsky has received honorary doctorates from institutions including the University of London and the University of Chicago (1967), Loyola University Chicago and Swarthmore College (1970), Bard College (1971), Delhi University (1972), and the University of Massachusetts (1973) among others. His public lectures have included the 1969 John Locke Lectures, 1975 Whidden Lectures, 1977 Huizinga Lecture, and 1988 Massey Lectures, among others.

Various tributes to Chomsky have been dedicated over the years. He is the eponym for a bee species, a frog species, and a building complex at the Indian university Jamia Millia Islamia. Actor Viggo Mortensen and avant-garde guitarist Buckethead dedicated their 2003 album "Pandemoniumfromamerica" to Chomsky.

Linguistics
Politics



</doc>
<doc id="21571" url="https://en.wikipedia.org/wiki?curid=21571" title="Nial">
Nial

Nial (from "Nested Interactive Array Language") is a high-level array programming language developed from about 1981 by Mike Jenkins of Queen's University, Kingston, Ontario, Canada. Jenkins co-created the Jenkins–Traub algorithm.

Nial combines a functional programming notation for arrays based on an array theory developed by Trenchard More with structured programming concepts for numeric, character and symbolic data.

It is most often used for prototyping and artificial intelligence.

In 1982, Jenkins formed a company (Nial Systems Ltd) to market the language and the Q'Nial implementation of Nial. As of 2014, the company website supports an Open Source project for the Q'Nial software with the binary and source available for download. Its license is derived from Artistic License 1.0, the only differences being the preamble, the definition of "Copyright Holder" (which is changed from "whoever is named in the copyright or copyrights for the package" to "NIAL Systems Limited"), and an instance of "whoever" (which is changed to "whomever").

Nial uses a generalized and expressive Array Theory in its Version 4, but sacrificed some of the generality of functional model, and modified the Array Theory in the Version 6. Only Version 6 is available now.

Nial defines all its data types as nested rectangular arrays. ints, booleans, chars etc. are considered as a solitary array or an array containing a single member. Arrays themselves can contain other arrays to form arbitrarily deep structures. Nial also provides Records. They are defined as non-homogenous array structure.

Functions in Nial are called Operations. From Nial manual: "An operation is a functional object that is given an argument array and returns a result array. The process of executing an operation by giving it an argument value is called an operation call or an operation application."

Nial like other APL-derived languages allows the unification of binary operators and operations. Thus the below notations have the same meaning.
Note: codice_1 is same as codice_2

Binary operation:

Array notation:

Strand notation:

Grouped notation:

Nial also uses transformers which are higher order functions. They use the argument operation to construct a new modified operation.

An atlas in Nial is an operation made up of an array of component operations. When an atlas is applied to a value, each element of the atlas is applied in turn to the value to provide an end result. This is used to provide point free (without-variables) style of definitions. It is also used by the transformers. In the below examples 'inner [+,*]' the list '[+,*]' is an atlas.

 count 6

Arrays can also be literal

Shape gives the array dimensions and reshape can be used to reshape the dimensions.

Definitions are of the form '<name> is <expression>'

 fact is recur [ 0 =, 1 first, pass, product, -1 +]

 rev is reshape [ shape, across [pass, pass, converse append ] ]

Contrast with [[APL programming language|APL]]

 Checking the divisibility of A by B

Defining is_prime filter

Count generates an array [1..N] and pass is N (identity operation).
eachright applies is_divisible(pass,element) in each element of count-generated array. 
Thus this transforms the count-generated array into an array where numbers that can divide N are replaced by '1' and others by '0'. Hence if the number N is prime, sum [transformed array] must be 2 (itself and 1).

Now all that remains is to generate another array using count N, and filter all that are not prime.


 quicksort is fork [ >= [1 first,tally],

Using it:

[[Category:Array programming languages]]

</doc>
<doc id="21572" url="https://en.wikipedia.org/wiki?curid=21572" title="Nag Hammadi">
Nag Hammadi

Nag Hammadi ( ; ) is a city in Upper Egypt.
It is located on the west bank of the Nile in the Qena Governorate, about 80 kilometres north-west of Luxor. It had a population of close to 43,000 .

The town of Nag Hammadi is named for its founder, Mahmoud Pasha Hammadi, a member of the Hammadi family in Sohag, Egypt. Mahmoud Pasha Hammadi was a major landholder in Sohag, and known for his strong opposition to the British occupation of 1882.

Nag Hammadi is about 5 km west of ancient Chenoboskion ()
The "Nag Hammadi library", an important collection of 2nd-century Gnostic texts, was found at 
Jabal al-Ṭārif near Nag Hammadi in 1945.

The city was the site of the Nag Hammadi massacre in January 2010, wherein eight Coptic Christians were shot dead by three men. In total, nineteen Coptic Christians were attacked.

Sugar and aluminium are produced in Nag Hammadi. The Nag Hammadi Sugar factory was built in 1895-1897 by French contractors Cail and Fives. It is still in operation in 2018. Egyptalum is the largest aluminium producer in the Middle East. Wood particleboard is manufactured from sugar cane bagasse.



</doc>
<doc id="21573" url="https://en.wikipedia.org/wiki?curid=21573" title="Niels Henrik Abel">
Niels Henrik Abel

Niels Henrik Abel (; ; 5 August 1802 – 6 April 1829) was a Norwegian mathematician who made pioneering contributions in a variety of fields. His most famous single result is the first complete proof demonstrating the impossibility of solving the general quintic equation in radicals. This question was one of the outstanding open problems of his day, and had been unresolved for over 250 years. He was also an innovator in the field of elliptic functions, discoverer of Abelian functions. He made his discoveries while living in poverty and died at the age of 26 from tuberculosis.

Most of his work was done in six or seven years of his working life. Regarding Abel, the French mathematician Charles Hermite said: "Abel has left mathematicians enough to keep them busy for five hundred years." Another French mathematician, Adrien-Marie Legendre, said: ""quelle tête celle du jeune Norvégien!"" ("what a head the young Norwegian has!").

The Abel Prize in mathematics, originally proposed in 1899 to complement the Nobel Prizes, is named in his honour.

Niels Henrik Abel was born in Nedstrand, Norway, as the second child of the pastor Søren Georg Abel and Anne Marie Simonsen. When Niels Henrik Abel was born, the family was living at a rectory on Finnøy. Much suggests that Niels Henrik was born in the neighboring parish, as his parents were guests of the bailiff in Nedstrand in July / August of his year of birth.

Niels Henrik Abel's father, Søren Georg Abel, had a degree in theology and philosophy and served as pastor at Finnøy. Søren's father, Niels's grandfather, Hans Mathias Abel, was also a pastor, at Gjerstad Church near the town of Risør. Søren had spent his childhood at Gjerstad, and had also served as chaplain there; and after his father's death in 1804, Søren was appointed pastor at Gjerstad and the family moved there. The Abel family originated in Schleswig and came to Norway in the 17th century.

Anne Marie Simonsen was from Risør; her father, Niels Henrik Saxild Simonsen, was a tradesman and merchant ship-owner, and said to be the richest person in Risør. Anne Marie had grown up with two stepmothers, in relatively luxurious surroundings. At Gjerstad rectory, she enjoyed arranging balls and social gatherings. Much suggests she was early on an alcoholic and took little interest in the upbringing of the children. Niels Henrik and his brothers were given their schooling by their father, with handwritten books to read. An addition table in a book of mathematics reads: 1+0=0.

With Norwegian independence and the first election held in Norway, in 1814, Søren Abel was elected as a representative to the Storting. Meetings of the Storting were held until 1866 in the main hall of the Cathedral School in Christiania (now known as Oslo). Almost certainly, this is how he came into contact with the school, and he decided that his eldest son, Hans Mathias, should start there the following year. However, when the time for his departure approached, Hans was so saddened and depressed over having to leave home that his father did not dare send him away. He decided to send Niels instead.

In 1815, Niels Abel entered the Cathedral School at the age of 13. His elder brother Hans joined him there a year later. They shared rooms and had classes together. Hans got better grades than Niels; however, a new mathematics teacher, Bernt Michael Holmboe, was appointed in 1818. He gave the students mathematical tasks to do at home. He saw Niels Henrik's talent in mathematics, and encouraged him to study the subject to an advanced level. He even gave Niels private lessons after school.

In 1818, Søren Abel had a public theological argument with the theologian Stener Johannes Stenersen regarding his catechism from 1806. The argument was well covered in the press. Søren was given the nickname "Abel Treating" "(Norwegian: "Abel Spandabel")". Niels' reaction to the quarrel was said to have been "excessive gaiety". At the same time, Søren also almost faced impeachment after insulting Carsten Anker, the host of the Norwegian Constituent Assembly; and in September 1818 he returned to Gjerstad with his political career in ruins. He began drinking heavily and died only two years later, in 1820, aged 48.

Bernt Michael Holmboe supported Niels Henrik Abel with a scholarship to remain at the school and raised money from his friends to enable him to study at the Royal Frederick University.

When Abel entered the university in 1821, he was already the most knowledgeable mathematician in Norway. Holmboe had nothing more he could teach him and Abel had studied all the latest mathematical literature in the university library. During that time, Abel started working on the quintic equation in radicals. Mathematicians had been looking for a solution to this problem for over 250 years. In 1821, Abel thought he had found the solution. The two professors of mathematics in Christiania, Søren Rasmussen and Christopher Hansteen, found no errors in Abel's formulas, and sent the work on to the leading mathematician in the Nordic countries, Carl Ferdinand Degen in Copenhagen. He too found no faults but still doubted that the solution, which so many outstanding mathematicians had sought for so long, could really have been found by an unknown student in far-off Christiania. Degen noted, however, Abel's unusually sharp mind, and believed that such a talented young man should not waste his abilities on such a "sterile object" as the fifth degree equation, but rather on elliptic functions and transcendence; for then, wrote Degen, he would "discover Magellanian thoroughfares to large portions of a vast analytical ocean". Degen asked Abel to give a numerical example of his method. While trying to provide an example, Abel found a mistake in his paper. This led to a discovery in 1823 that a solution to a fifth- or higher-degree equation was impossible.

Abel graduated in 1822. His performance was exceptionally high in mathematics and average in other matters.

After he graduated, professors from university supported Abel financially, and Professor Christopher Hansteen let him live in a room in the attic of his home. Abel would later view Ms. Hansteen as his second mother. While living here, Abel helped his younger brother, Peder Abel, through examen artium. He also helped his sister Elisabeth to find work in the town.

In early 1823, Niels Abel published his first article in "Magazin for Naturvidenskaberne", Norway's first scientific journal, which had been co-founded by Professor Hansteen. Abel published several articles, but the journal soon realized that this was not material for the common reader. In 1823, Abel also wrote a paper in French. It was "a general representation of the possibility to integrate all differential formulas" ("Norwegian: en alminnelig Fremstilling af Muligheten at integrere alle mulige Differential-Formler)". He applied for funds at the university to publish it. However the work was lost, while being reviewed, never to be found thereafter.

In mid-1823, Professor Rasmussen gave Abel a gift of 100 speciedaler so he could travel to Copenhagen and visit Ferdinand Degen and other mathematicians there. While in Copenhagen, Abel did some work on Fermat's Last Theorem. Abel's uncle, Peder Mandrup Tuxen, lived at the naval base in Christianshavn, Copenhagen, and at a ball there Niels Abel met Christine Kemp, his future fiancée. In 1824, Christine moved to Son, Norway to work as a governess and the couple got engaged over Christmas.

After returning from Copenhagen, Abel applied for a government scholarship in order to visit top mathematicians in Germany and France, but he was instead granted 200 speciedaler yearly for two years, to stay in Christiania and study German and French. In the next two years, he was promised a scholarship of 600 speciedaler yearly and he would then be permitted to travel abroad. While studying these languages, Abel published his first notable work in 1824, "Mémoire sur les équations algébriques où on démontre l'impossibilité de la résolution de l'équation générale du cinquième degré" (Memoir on algebraic equations, in which the impossibility of solving the general equation of the fifth degree is proven). For, in 1823, Abel had at last proved the impossibility of solving the quintic equation in radicals (now referred to as the Abel–Ruffini theorem). However, this paper was in an abstruse and difficult form, in part because he had restricted himself to only six pages, in order to save money on printing. A more detailed proof was published in 1826 in the first volume of "Crelle's Journal".

In 1825, Abel wrote a personal letter to King Carl Johan of Norway/Sweden requesting permission to travel abroad. He was granted this permission, and in September 1825 he left Christiania together with four friends from university (Christian P.B Boeck, Balthazar M. Keilhau, Nicolay B. Møller and Otto Tank). These four friends of Abel were traveling to Berlin and to the Alps to study geology. Abel wanted to follow them to Copenhagen and from there make his way to Göttingen. The terms for his scholarship stipulated that he was to visit Gauss in Göttingen and then continue to Paris. However, when he got as far as Copenhagen he changed his plans. He wanted to follow his friends to Berlin instead, intending to visit Göttingen and Paris afterwards.

On the way, he visited the astronomer Heinrich Christian Schumacher in Altona, now a district of Hamburg. He then spent four months in Berlin, where he became well acquainted with August Leopold Crelle, who was then about to publish his mathematical journal, "Journal für die reine und angewandte Mathematik". This project was warmly encouraged by Abel, who contributed much to the success of the venture. Abel contributed seven articles to it in its first year.

From Berlin Abel also followed his friends to the Alps. He went to Leipzig and Freiberg to visit Georg Amadeus Carl Friedrich Naumann and his brother the mathematician August Naumann. In Freiberg Abel did research in the theory of functions, particularly, elliptic, hyperelliptic, and a new class now known as abelian functions.

From Freiberg they went on to Dresden, Prague, Vienna, Trieste, Venice, Verona, Bolzano, Innsbruck, Luzern and Basel. From July 1826 Abel traveled on his own from Basel to Paris. Abel had sent most of his work to Berlin to be published in Crelle's Journal, but he had saved what he regarded as his most important work for the French Academy of Sciences, a theorem on addition of algebraic differentials. With the help of a painter, Johan Gørbitz, he found an apartment in Paris and continued his work on the theorem. He finished in October 1826, and submitted it to the academy. It was to be reviewed by Augustin-Louis Cauchy. Abel's work was scarcely known in Paris, and his modesty restrained him from proclaiming his research. The theorem was put aside and forgotten until his death.

Abel's limited finances finally compelled him to abandon his tour in January 1827. He returned to Berlin, and was offered a position as editor of Crelle's Journal, but opted out. By May 1827 he was back in Norway. His tour abroad was viewed as a failure. He had not visited Gauss in Göttingen and he had not published anything in Paris. His scholarship was therefore not renewed and he had to take up a private loan in Norges Bank of 200 spesidaler. He never repaid this loan. He also started tutoring. He continued to send most of his work to Crelle's Journal. But in mid-1828 he published, in rivalry with Carl Jacobi, an important work on elliptic functions in "Astronomische Nachrichten" in Altona.

While in Paris, Abel contracted tuberculosis. At Christmas 1828, he traveled by sled to Froland, Norway to visit his fiancée. He became seriously ill on the journey; and, although a temporary improvement allowed the couple to enjoy the holiday together, he died relatively soon after on 6 April 1829, just two days before a letter arrived from August Crelle. Crelle had been searching for a new job for Abel in Berlin and had actually managed to have him appointed as a Professor at the University of Berlin. Crelle wrote to Abel to tell him, but the good news came too late.

Abel showed that there is no general algebraic solution for the roots of a quintic equation, or any general polynomial equation of degree greater than four, in terms of explicit algebraic operations. To do this, he invented (independently of Galois) a branch of mathematics known as group theory, which is invaluable not only in many areas of mathematics, but for much of physics as well. Abel sent a paper on the unsolvability of the quintic equation to Carl Friedrich Gauss, who proceeded to discard without a glance what he believed to be the worthless work of a crank.

As a 16-year-old, Abel gave a rigorous proof of the binomial theorem valid for all numbers, extending Euler's result which had held only for rationals. Abel wrote a fundamental work on the theory of elliptic integrals, containing the foundations of the theory of elliptic functions.
While travelling to Paris he published a paper revealing the double periodicity of elliptic functions, which Adrien-Marie Legendre later described to Augustin-Louis Cauchy as "a monument more lasting than bronze" (borrowing a famous sentence by the Roman poet Horatius). The paper was, however, misplaced by Cauchy.

While abroad Abel had sent most of his work to Berlin to be published in the "Crelle's Journal", but he had saved what he regarded as his most important work for the French Academy of Sciences, a theorem on addition of algebraic differentials. The theorem was put aside and forgotten until his death. While in Freiberg, Abel did research in the theory of functions, particularly, elliptic, hyperelliptic, and a new class now known as abelian functions.

In 1823 Abel wrote a paper titled "a general representation of the possibility to integrate all differential formulas" (Norwegian: "en alminnelig Fremstilling af Muligheten at integrere alle mulige Differential-Formler"). He applied for funds at the university to publish it. However the work was lost, while being reviewed, never to be found thereafter.

Abel said famously of Carl Friedrich Gauss's writing style, "He is like the fox, who effaces his tracks in the sand with his tail." Gauss replied him by saying, "No self respecting architect leaves the scaffolding in place after completing his building."

Under Abel's guidance, the prevailing obscurities of analysis began to be cleared, new fields were entered upon and the study of functions so advanced as to provide mathematicians with numerous ramifications along which progress could be made. His works, the greater part of which originally appeared in "Crelle's Journal", were edited by Bernt Michael Holmboe and published in 1839 by the Norwegian government, and a more complete edition by Ludwig Sylow and Sophus Lie was published in 1881. The adjective "abelian", derived from his name, has become so commonplace in mathematical writing that it is conventionally spelled with a lower-case initial "a" (e.g., abelian group, abelian category, and abelian variety).

On 6 April 1929, four Norwegian stamps were issued for the centenary of Abel's death. His portrait appears on the 500-kroner banknote (version V) issued during 1978–1985. On 5 June 2002, four Norwegian stamps were issued in honour of Abel two months before the bicentenary of his birth. There is also a 20-kroner coin issued by Norway in his honour. A statue of Abel stands in Oslo, and crater Abel on the Moon was named after him. In 2002, the Abel Prize was established in his memory.

Mathematician Felix Klein wrote about Abel:





</doc>
<doc id="21574" url="https://en.wikipedia.org/wiki?curid=21574" title="November 19">
November 19





</doc>
<doc id="21575" url="https://en.wikipedia.org/wiki?curid=21575" title="November 20">
November 20





</doc>
<doc id="21576" url="https://en.wikipedia.org/wiki?curid=21576" title="November 21">
November 21





</doc>
<doc id="21577" url="https://en.wikipedia.org/wiki?curid=21577" title="November 30">
November 30





</doc>
<doc id="21578" url="https://en.wikipedia.org/wiki?curid=21578" title="November 29">
November 29





</doc>
<doc id="21579" url="https://en.wikipedia.org/wiki?curid=21579" title="November 28">
November 28





</doc>
<doc id="21580" url="https://en.wikipedia.org/wiki?curid=21580" title="November 25">
November 25





</doc>
<doc id="21581" url="https://en.wikipedia.org/wiki?curid=21581" title="November 26">
November 26






</doc>
<doc id="21582" url="https://en.wikipedia.org/wiki?curid=21582" title="Nicolaus von Amsdorf">
Nicolaus von Amsdorf

Nicolaus von Amsdorf (German: Nikolaus von Amsdorf, 3 December 1483 – 14 May 1565) was a German Lutheran theologian and an early Protestant reformer. As bishop of Naumburg (1542–1546), he became the first Lutheran bishop in the Holy Roman Empire.

He was born in Torgau, on the Elbe.

He was educated at Leipzig, and then at Wittenberg, where he was one of the first who matriculated (1502) in the recently founded university. He soon obtained various academic honours, and became professor of theology in 1511.

Like Andreas Karlstadt, he was at first a leading exponent of the older type of scholastic theology, but under the influence of Luther abandoned his Aristotelian positions for a theology based on the Augustinian doctrine of grace. Throughout his life he remained one of Luther's most determined supporters; he was with him at the Leipzig conference (1519), and the Diet of Worms (1521); and was privy to the secret of his Wartburg seclusion. He assisted the first efforts of the Reformation at Magdeburg (1524), at Goslar (1531) and at Einbeck (1534); took an active part in the debates at Schmalkalden (1537), where he defended the use of the sacrament by the unbelieving; and (1539) spoke out strongly against the bigamy of the Landgrave of Hesse.

After the death of Philip of the Palatinate, bishop of Naumburg-Zeitz, he was installed there on 20 January 1542, though in opposition to the chapter, by the Prince-elector of Saxony and Luther. His position was a painful one, and he longed to get back to Magdeburg, but was persuaded by Luther to stay. After Luther's death (1546) and the Battle of Mühlberg (1547) he had to yield to his rival, Julius von Pflug, and retire to the protection of the young duke of Weimar. Here he took part in founding Jena University (1558); opposed the "Augsburg Interim" (1548); superintended the publication of the Jena edition of Luther's works; and debated on the freedom of the will, original sin, and, more noticeably, on the Christian value of good works, in regard to which he held that they were not only useless, but prejudicial in the matter of man's salvation. He urged the separation of the High Lutheran party from Melanchthon (1557), got the Saxon dukes to oppose the Frankfurt Recess (1558) and continued to fight for the purity of Lutheran doctrine.

He died at Eisenach in 1565, and was buried in the church of St. Georg there, where his effigy shows a well-knit frame and sharp-cut features.

He was a man of strong will, of great aptitude for controversy, and considerable learning, and thus exercised a decided influence on the Reformation. Many letters and other short productions of his pen are extant in manuscript, especially five thick volumes of Amsdorfiana, in the Weimar library. They are a valuable source for our knowledge of Luther. A small sect, which adopted his opinion on good works, was called after him; but it is now of mere historical interest.




</doc>
<doc id="21583" url="https://en.wikipedia.org/wiki?curid=21583" title="Nationality">
Nationality

Nationality is a legal identification of a person in international law, establishing the person as a subject, a "national", of a sovereign state. It affords the state jurisdiction over the person and affords the person the protection of the state against other states.

Article 15 of the Universal Declaration of Human Rights states that "Everyone has the right to a nationality," and "No one shall be arbitrarily deprived of his nationality nor denied the right to change his nationality." By international custom and conventions, it is the right of each state to determine who its nationals are. Such determinations are part of nationality law. In some cases, determinations of nationality are also governed by public international law—for example, by treaties on statelessness and the European Convention on Nationality.

What the rights and duties of nationals are varies from state to state, and is often complemented by citizenship law, in some contexts to the point where it is synonymous with nationality. Nationality though differs technically and legally from citizenship, which is a different legal relationship between a person and a country. The noun "national" can include both citizens and non-citizens. The most common distinguishing feature of citizenship is that citizens have the right to participate in the political life of the state, such as by voting or standing for election. However, in most modern countries all nationals are citizens of the state, and full citizens are always nationals of the state.

In older texts or other languages the word "nationality", rather than "ethnicity", is often used to refer to an ethnic group (a group of people who share a common ethnic identity, language, culture, lineage, history, and so forth). This older meaning of "nationality" is not defined by political borders or passport ownership and includes nations that lack an independent state (such as the Arameans, Scots, Welsh, English, Andalusians, Basques, Catalans, Kurds, Kabyles, Baloch, Berbers, Bosniaks, Kashmiris,
Palestinians, Sindhi, Tamils, Hmong, Inuit, Copts, Māori, Sikhs, Wakhi, Székelys, Xhosas and Zulus).
Individuals may also be considered nationals of groups with autonomous status that have ceded some power to a larger sovereign state.

Nationality is also employed as a term for national identity, with some cases of identity politics and nationalism conflating the legal nationality as well as ethnicity with a national identity.

Nationality is the status that allows a nation to grant rights to the subject and to impose obligations upon the subject. In most cases, no rights or obligations are automatically attached to this status, although the status is a necessary precondition for any rights and obligations created by the state.

In European law, nationality is the status or relationship that gives a nation the right to protect a person from other nations. Diplomatic and consular protection are dependent upon this relationship between the person and the state. A person's status as being the national of a country is used to resolve the conflict of laws.

Within the broad limits imposed by few treaties and international law, states may freely define who are and are not their nationals. However, since the "Nottebohm" case, other states are only required to respect claim by a state to protect an alleged national if the nationality is based on a true social bond. In the case of dual nationality, states may determine the most effective nationality for a person, to determine which state's laws are most relevant. There are also limits on removing a person's status as a national. Article 15 of the Universal Declaration of Human Rights states that "Everyone has the right to a nationality," and "No one shall be arbitrarily deprived of his nationality nor denied the right to change his nationality."

Nationals normally have the right to enter or return to the country they belong to. Passports are issued to nationals of a state, rather than only to citizens, because the passport is the travel document used to enter the country. However, nationals may not have the right of abode (the right to live permanently) in the countries that grant them passports.

Conceptually, citizenship is focused on the internal political life of the state and nationality is a matter of international law. Article 15 of the human rights states that everyone has the right to a nationality. As such nationality in international law can be called and understood as citizenship, or more generally as subject or belonging to a sovereign state, and not as ethnicity. This notwithstanding, around 10 million people are stateless.

In the modern era, the concept of full citizenship encompasses not only active political rights, but full civil rights and social rights. Nationality is a necessary but not sufficient condition to exercise full political rights within a state or other polity. Nationality is required for full citizenship.

Historically, the most significant difference between a national and a citizen is that the citizen has the right to vote for elected officials, and to be elected. This distinction between full citizenship and other, lesser relationships goes back to antiquity. Until the 19th and 20th centuries, it was typical for only a small percentage of people who belonged to a city or state to be full citizens. In the past, most people were excluded from citizenship on the basis of sex, socioeconomic class, ethnicity, religion, and other factors. However, they held a legal relationship with their government akin to the modern concept of nationality.

United States nationality law defines some persons born in some U.S. outlying possessions as U.S. nationals but not citizens. British nationality law defines six classes of British national, among which "British citizen" is one class (having the right of abode in the United Kingdom, along with some "British subjects"). Similarly, in the Republic of China, commonly known as Taiwan, the status of national without household registration applies to people who have Republic of China nationality, but do not have an automatic entitlement to enter or reside in the Taiwan Area, and do not qualify for civic rights and duties there. Under the nationality laws of Mexico, Colombia, and some other Latin American countries, nationals do not become citizens until they turn 18. Israeli law distinguishes nationality from citizenship. The nationality of an Arab citizen of Israel is "Arab", not Israeli, while the nationality of a Jewish citizen is "Jewish" not Israeli.

Nationality is sometimes used simply as an alternative word for ethnicity or national origin, just as some people assume that citizenship and nationality are identical. In some countries, the cognate word for "nationality" in local language may be understood as a synonym of ethnicity or as an identifier of cultural and family-based self-determination, rather than on relations with a state or current government. For example, some Kurds say that they have Kurdish nationality, even though there is no Kurdish sovereign state at this time in history.
In the context of former Soviet Union and former Socialist Federal Republic of Yugoslavia, "nationality" is often used as translation of the Russian "nacional'nost' " and Serbo-Croatian "narodnost", which were the terms used in those countries for ethnic groups and local affiliations within the member states of the federation. In the Soviet Union, more than 100 such groups were formally recognized. Membership in these groups was identified on Soviet internal passports, and recorded in censuses in both the USSR and Yugoslavia. In the early years of the Soviet Union's existence, ethnicity was usually determined by the person's native language, and sometimes through religion or cultural factors, such as clothing. Children born after the revolution were categorized according to their parents' recorded ethnicities. Many of these ethnic groups are still recognized by modern Russia and other countries.

Similarly, the term "nationalities of China" refers to ethnic and cultural groups in China. Spain is one nation, made up of nationalities, which are not politically recognized as nations (state), but can be considered smaller nations within the Spanish nation. Spanish law recognizes the autonomous communities of Andalusia, Aragon, Balearic Islands, Canary Islands, Catalonia, Valencia, Galicia and the Basque Country as "nationalities" ("nacionalidades").

In 2013, the Supreme Court of Israel unanimously affirmed the position that "citizenship" (e.g. Israeli) is separate from "le'om" (; "nationality" or "ethnic affiliation"; e.g. Jewish, Arab, Druze, Circassian), and that the existence of a unique "Israeli" "le'om" has not been proven. Israel recognizes more than 130 "le'umim" in total.

National identity is a person's subjective sense of belonging to one state or to one nation. A person may be a national of a state, in the sense of being its citizen, without subjectively or emotionally feeling a part of that state, for example many migrants in Europe often identify with their ancestral and/or religious background rather than with the state of which they are citizens. Conversely, a person may feel that he belongs to one state without having any legal relationship to it. For example, children who were brought to the U.S. illegally when quite young and grow up there with little contact with their native country and its culture often have a national identity of feeling American, despite legally being nationals of a different country.

Dual nationality is when a single person has a formal relationship with two separate, sovereign states. This might occur, for example, if a person's parents are nationals of separate countries, and the mother's country claims all offspring of the mother's as their own nationals, but the father's country claims all offspring of the father's.

Nationality, with its historical origins in allegiance to a sovereign monarch, was seen originally as a permanent, inherent, unchangeable condition, and later, when a change of allegiance was permitted, as a strictly exclusive relationship, so that becoming a national of one state required rejecting the previous state.

Dual nationality was considered a problem that caused conflict between states and sometimes imposed mutually exclusive requirements on affected people, such as simultaneously serving in two countries' military forces. Through the middle of the 20th century, many international agreements were focused on reducing the possibility of dual nationality. Since then, many accords recognizing and regulating dual nationality have been formed.

Statelessness is the condition in which an individual has no formal or protective relationship with any state. This might occur, for example, if a person's parents are nationals of separate countries, and the mother's country rejects all offspring of mothers married to foreign fathers, but the father's country rejects all offspring born to foreign mothers. Although this person may have an emotional national identity, he or she may not legally be the national of any state.

Another stateless situation arises when a person holds a travel document (passport) which recognizes the bearer as having the nationality of a "state" which is not internationally recognized, has no entry in the International Organization for Standardization's country list, is not a member of the United Nations, etc. In the current era, persons native to Taiwan who hold Republic of China passports are one example.
Some countries ( like the Kuwait, UAE, Saudi Arabia) can also remove your citizenship; the reasons for removal are fraud and security issues. There are also people who are abandoned at birth and the parents whereabouts are not known.

The following list includes states in which parents are able to confer nationality on their children or spouses.



</doc>
<doc id="21585" url="https://en.wikipedia.org/wiki?curid=21585" title="Nereus">
Nereus

In Greek mythology, Nereus (; Ancient Greek: Νηρεύς) was the eldest son of Gaia (the Earth) and of her son, Pontus (the Sea). Nereus and Doris became the parents of 50 daughters (the Nereids) and a son (Nerites), with whom Nereus lived in the Aegean Sea.

R. S. P. Beekes suggests a Pre-Greek origin.

In the "Iliad" the Old Man of the Sea is the father of Nereids, though Nereus is not directly named. He was never more manifestly the Old Man of the Sea than when he was described, like Proteus, as a shapeshifter with the power of prophecy, who would aid heroes such as Heracles who managed to catch him even as he changed shapes. Nereus and Proteus (the "first") seem to be two manifestations of the god of the sea who was supplanted by Poseidon when Zeus overthrew Cronus.

The earliest poet to link Nereus with the labours of Heracles was Pherekydes, according to a "scholion" on Apollonius of Rhodes.

During the course of the 5th century BC, Nereus was gradually replaced by Triton, who does not appear in Homer, in the imagery of the struggle between Heracles and the sea-god who had to be restrained in order to deliver his information that was employed by the vase-painters, independent of any literary testimony.

In a late appearance, according to a fragmentary papyrus, Alexander the Great paused at the Syrian seashore before the climacteric battle of Issus (333 BC), and resorted to prayers, "calling on Thetis, Nereus and the Nereids, nymphs of the sea, and invoking Poseidon the sea-god, for whom he ordered a four-horse chariot to be cast into the waves."

Nereus was known for his truthfulness and virtue:

The Attic vase-painters showed the draped torso of Nereus issuing from a long coiling scaly fishlike tail. Bearded Nereus generally wields a staff of authority. He was also shown in scenes depicting the flight of the Nereides as Peleus wrestled their sister Thetis.

In Aelian's natural history, written in the early third century CE, Nereus was also the father of a watery consort of Aphrodite named Nerites who was transformed into "a shellfish with a spiral shell, small in size but of surpassing beauty."

Nereus was father to Thetis, one of the Nereids, who in turn was mother to the great Greek hero Achilles, and Amphitrite, who married Poseidon.




</doc>
<doc id="21586" url="https://en.wikipedia.org/wiki?curid=21586" title="Nereid">
Nereid

In Greek mythology, the Nereids ( ; "Nereides", sg. "Nereis") are sea nymphs (female spirits of sea waters), the 50 daughters of Nereus and Doris, sisters to their brother Nerites. They often accompany Poseidon, the god of the sea, and can be friendly and helpful to sailors (such as the Argonauts in their search for the Golden Fleece).

Nereids are particularly associated with the Aegean Sea, where they dwelt with their father Nereus in the depths within a golden palace. The most notable of them are Thetis, wife of Peleus and mother of Achilles; Amphitrite, wife of Poseidon and mother of Triton; and Galatea, the vain love interest of the Cyclops Polyphemus.They symbolized everything that is beautiful and kind about the sea. Their melodious voices sang as they danced around their father. They are represented as very beautiful girls, crowned with branches of red coral and dressed in white silk robes trimmed with gold, but who went barefoot. They were part of Poseidon's entourage and carried his trident.

In Homer's "Iliad" XVIII, when Thetis cries out in sympathy for the grief of Achilles for the slain Patroclus, her sisters appear. The Nereid Opis is mentioned in Virgil's "Aeneid". She is called by the goddess Diana to avenge the death of the Amazon-like female warrior Camilla. Diana gives Opis magical weapons for revenge on Camilla's killer, the Etruscan Arruns. Opis sees and laments Camilla's death, and kills Arruns with an arrow in revenge as directed by Diana.

In modern Greek folklore, the term "nereid" (, "neráida") has come to be used for all nymphs, fairies, or mermaids, not merely nymphs of the sea.

Nereid, a moon of the planet Neptune, is named after the Nereids. Nereid Lake in Antarctica is named after the nymphs.

This list is correlated from four sources: Homer's "Iliad", Hesiod's "Theogony", the "Bibliotheca" of Pseudo-Apollodorus and the "Fabulae" of Hyginus. Because of this, the total number of names goes beyond fifty.



</doc>
<doc id="21587" url="https://en.wikipedia.org/wiki?curid=21587" title="Nemesis (disambiguation)">
Nemesis (disambiguation)

Nemesis is a Greek mythological spirit of divine retribution against those who succumb to hubris. Nemesis may also refer to:















</doc>
<doc id="21588" url="https://en.wikipedia.org/wiki?curid=21588" title="Nereid (moon)">
Nereid (moon)

Nereid , or Neptune II, is the third-largest moon of Neptune. Of all known moons in the Solar System, it has the most eccentric orbit. It was the second moon of Neptune to be discovered, by Gerard Kuiper in 1949.

Nereid was discovered on 1 May 1949 by Gerard P. Kuiper on photographic plates taken with the 82-inch telescope at the McDonald Observatory. He proposed the name in the report of his discovery. It is named after the Nereids, sea-nymphs of Greek mythology and attendants of the god Neptune. It was the second and last moon of Neptune to be discovered before the arrival of Voyager 2 (not counting a single observation of an occultation by Larissa in 1981).

Nereid is third-largest of Neptune's satellites, and has an average radius of about . It is rather large for an irregular satellite. The shape of Nereid is unknown.

Since 1987 some photometric observations of Nereid have detected large (by ~1 of magnitude) variations of its brightness, which can happen over years and months, but sometimes even over a few days. They persist even after a correction for distance and phase effects. On the other hand, not all astronomers who have observed Nereid have noticed such variations. This means that they may be quite chaotic. To date there is no credible explanation of the variations, but, if they exist, they are likely related to the rotation of Nereid. Nereid's rotation could be either in the state of forced precession or even chaotic rotation (like Hyperion) due to its highly elliptical orbit.

In 2016, extended observations with the Kepler space telescope showed only low-amplitude variations (0.033 magnitudes). Thermal modeling based on infrared observations from the Spitzer and Herschel space telescopes suggest that Nereid is only moderately elongated with an aspect ratio of 1.3:1, which disfavors forced precession of the rotation. The thermal model also indicates that the surface roughness of Nereid is very high, likely similar to the Saturnian moon Hyperion.

Spectrally, Nereid appears neutral in colour and water ice has been detected on its surface. Its spectrum appears to be intermediate between Uranus's moons Titania and Umbriel, which suggests that Nereid's surface is composed of a mixture of water ice and some spectrally neutral material. The spectrum is markedly different from minor planets of the outer solar system, centaurs Pholus, Chiron and Chariklo, suggesting that Nereid formed around Neptune rather than being a captured body.

Halimede, which displays a similar gray neutral colour, may be a fragment of Nereid that was broken off during a collision.

Nereid orbits Neptune in the prograde direction at an average distance of , but its high eccentricity of 0.7507 takes it as close as and as far as .

The unusual orbit suggests that it may be either a captured asteroid or Kuiper belt object, or that it was an inner moon in the past and was perturbed during the capture of Neptune's largest moon Triton. If the latter is true, it may be the only survivor of Neptune's original (pre-Triton capture) set of regular satellites.

In 1991, a rotation period of Nereid of about 13.6 hours was determined by an analysis of its light curve. In 2003, another rotation period of about was measured. However, this determination was later disputed, and other researchers for a time failed to detect any periodic modulation in Nereid's light curve from ground-based observations. In 2016, a clear rotation period of 11.594 ± 0.017 hours was determined based on observations with the Kepler space telescope.

The only spacecraft to visit Nereid was "Voyager 2", which passed it at a distance of between 20 April and 19 August 1989. "Voyager 2" obtained 83 images with observation accuracies of to . Prior to "Voyager 2"'s arrival, observations of Nereid had been limited to ground-based observations that could only establish its intrinsic brightness and orbital elements. Although the images obtained by "Voyager 2" do not have a high enough resolution to allow surface features to be distinguished, "Voyager 2" was able to measure the size of Nereid and found that it was grey in colour and had a higher albedo than Neptune's other small satellites.



</doc>
<doc id="21592" url="https://en.wikipedia.org/wiki?curid=21592" title="Netball">
Netball

Netball is a ball sport played by two teams of seven players. Netball is most popular in many Commonwealth nations, specifically in schools, and is predominantly played by women. According to the INF, netball is played by more than 20 million people in more than 80 countries. Major domestic leagues in the sport include the Netball Superleague in Great Britain, Suncorp Super Netball in Australia and the ANZ Premiership in New Zealand. Four major competitions take place internationally: the quadrennial World Netball Championships, the Commonwealth Games, and the yearly Quad Series and Fast5 Series. In 1995, netball became an International Olympic Committee recognised sport, but it has not been played at the Olympics.

Games are played on a rectangular court with raised goal rings at each end. Each team attempts to score goals by passing a ball down the court and shooting it through its goal ring. Players are assigned specific positions, which define their roles within the team and restrict their movement to certain areas of the court. During general play, a player with the ball can hold on to it for only three seconds before shooting for a goal or passing to another player. The winning team is the one that scores the most goals. Netball games are 60 minutes long. Variations have been developed to increase the game's pace and appeal to a wider audience.

Its development, derived from early versions of basketball, began in England in the 1890s. By 1960, international playing rules had been standardised for the game, and the International Federation of Netball and Women's Basketball (later renamed the International Netball Federation (INF)) was formed. As of 2019, the INF comprises more than 70 national teams organized into five global regions.

Netball emerged from early versions of basketball and evolved into its own sport as the number of women participating in sports increased. Basketball was invented in 1891 by James Naismith in the United States. The game was initially played indoors between two teams of nine players, using an association football that was thrown into closed-end peach baskets. Naismith's game spread quickly across the United States and variations of the rules soon emerged. Physical education instructor Senda Berenson developed modified rules for women in 1892; these eventually gave rise to women's basketball. Around this time separate intercollegiate rules were developed for men and women. The various basketball rules converged into a universal set in the United States.

Martina Bergman-Österberg introduced a version of basketball in 1893 to her female students at the Physical Training College in Hampstead, London. The rules of the game were modified at the college over several years: the game moved outdoors and was played on grass; the baskets were replaced by rings that had nets; and in 1897 and 1899, rules from women's basketball in the United States were incorporated. Österberg's new sport acquired the name "net ball". The first codified rules of netball were published in 1901 by the Ling Association, later the Physical Education Association of the United Kingdom. From England, netball spread to other countries in the British Empire. Variations of the rules and even names for the sport arose in different areas: "women's (outdoor) basketball" arrived in Australia around 1900 and in New Zealand from 1906, while "netball" was being played in Jamaican schools by 1909.

From the start, it was considered socially appropriate for women to play netball; netball's restricted movement appealed to contemporary notions of women's participation in sports, and the sport was distinct from potential rival male sports. Netball became a popular women's sport in countries where it was introduced and spread rapidly through school systems. School leagues and domestic competitions emerged during the first half of the 20th century, and in 1924 the first national governing body was established in New Zealand. International competition was initially hampered by a lack of funds and varying rules in different countries. Australia hosted New Zealand in the first international game of netball in Melbourne on 20 August 1938; Australia won 40–11. Efforts began in 1957 to standardise netball rules globally: by 1960 international playing rules had been standardised, and the International Federation of Netball and Women's Basketball, later the International Netball Federation (INF), was formed to administer the sport worldwide.

Representatives from England, Australia, New Zealand, South Africa, and the West Indies were part of a 1960 meeting in Sri Lanka that standardised the rules for the game. The game spread to other African countries in the 1970s. South Africa was prohibited from competing internationally from 1969 to 1994 due to apartheid. In the United States, Netball's popularity also increased during the 1970s, particularly in the New York area, and the United States of America Netball Association was created in 1992. The game also became popular in the Pacific Island nations of the Cook Islands, Fiji and Samoa during the 1970s. Netball Singapore was created in 1962, and the Malaysian Netball Association was created in 1978.

In Australia, the term "women's basketball" was used to refer to both netball and basketball. During the 1950s and 1960s, a movement arose to change the Australian name of the game from "women's basketball" to "netball" in order to avoid confusion between the two sports. The Australian Basketball Union offered to pay the costs involved to alter the name, but the netball organisation rejected the change. In 1970, the Council of the All Australia Netball Association officially changed the name to "netball" in Australia.

In 1963, the first international tournament was held in Eastbourne, England. Originally called the World Tournament, it later became known as the World Netball Championships. Following the first tournament, one of the organisers, Miss R. Harris, declared,
The World Netball Championships have been held every four years since then. The World Youth Netball Championships started in Canberra in 1988, and have been held roughly every four years since. In 1995, the International Olympic Committee recognized the International Federation of Netball Associations. Three years later netball debuted at the 1998 Commonwealth Games in Kuala Lumpur. Other international competitions also emerged in the late 20th century, including the Nations Cup and the Asian Netball Championship.

As of 2006, the IFNA recognises only women's netball. Men's netball teams exist in some areas but attract less attention from sponsors and spectators. Men's netball started to become popular in Australia during the 1980s, and the first men's championship was held in 1985. In 2004, New Zealand and Fiji sent teams to compete in the Australian Mixed and Men's National Championships. By 2006, mixed netball teams in Australia had as many male participants as rugby union. Other countries with men's national teams include Canada, Fiji, Jamaica, Kenya, Pakistan and the United Arab Emirates. Unlike women's netball at elite and national levels, men's and mixed gender teams are largely self-funded.

An all-transgender netball team from Indonesia competed at the 1994 Gay Games in New York City. The team had been the Indonesian national champions. At the 2000 Gay Games VI in Sydney, netball and volleyball were the two sports with the highest rates of transgender athletes participating. There were eight teams of indigenous players, with seven identifying as transgender. They came from places like Palm Island in northern Queensland, Samoa, Tonga and Papua New Guinea. Teams with transgender players were allowed to participate in several divisions including men's, mixed and transgender; they were not allowed to compete against the cisgender women's teams.

The objective of a game is to score more goals than the opposition. Goals are scored when a team member positioned in the attacking shooting circle shoots the ball through the goal ring. The goal rings are in diameter and sit atop -high goal posts that have no backboards. A -radius semi-circular "shooting circle" is an area at each end of the court. The goal posts are located within the shooting circle. Each team defends one shooting circle and attacks the other. The netball court is long, wide, and divided lengthwise into thirds. The ball is usually made of leather or rubber, measures in circumference (~ in diameter), and weighs . A normal game consists of four 15-minute quarters and can be played outdoors or in a covered stadium.

Each team is allowed seven players on the court. Each player is assigned a specific position, which limits their movement to a certain area of the court. A "bib" worn by each player contains a one- or two-letter abbreviation indicating this position. Only two positions are permitted in the attacking shooting circle, and can therefore shoot for a goal. Similarly, only two positions are permitted in the defensive shooting circle; they try to prevent the opposition from shooting goals. Other players are restricted to two thirds of the court, with the exception of the Centre, who may move anywhere on the court except for a shooting circle.
At the beginning of every quarter and after a goal has been scored, play starts with a player in the centre position passing the ball from the centre of the court. These "centre passes" alternate between the teams, regardless of which team scored the last goal. When the umpire blows the whistle to restart play, four players from each team can move into the centre third to receive the pass. The centre pass must be caught or touched in the centre third. The ball is then moved up and down the court through passing and must be touched by a player in each adjacent third of the court. Players can hold the ball for only three seconds at any time. It must be released before the foot they were standing on when they caught it touches the ground again. Contact between players is only permitted if it does not impede an opponent or the general play. When defending a pass or shot players must be at least away from the player with the ball. If illegal contact is made, the player who contacted cannot participate in play until the player taking the penalty has passed or shot the ball. If the ball is held in two hands and either dropped or a shot at goal is missed, the same player cannot be the first to touch it unless it first rebounds off the goal.

Indoor netball is a variation of netball, played exclusively indoors, in which the playing court is often surrounded on each side and overhead by a net. The net prevents the ball from leaving the court, permitting faster play by reducing playing stoppages.

Different forms of indoor netball exist. In a seven-per-side version called "action netball", seven players per team play with rules similar to netball. However, a game is split into 15-minute halves with a three-minute break in between. This version is played in Australia, New Zealand, South Africa and England.

A six-per-side version of the sport is also played in New Zealand. Two Centres per team can play in the whole court except the shooting circles; the remaining attacking and defending players are each restricted to one half of the court, including the shooting circles. The attacking and Centre players may shoot from outside the shooting circle for a two-point goal.

A five-per-side game is also common in indoor netball. Players can move throughout the court, with the exception of the shooting circles, which are restricted to certain attacking or defending players.

Fast5 (originally called Fastnet) is a variation on the rules of netball designed to make games faster and more television-friendly. The World Netball Series promotes it to raise the sport's profile and attract more spectators and greater sponsorship. The game is much shorter, with each quarter lasting only six minutes and only a two-minute break between quarters. The coaches can give instructions from the sideline during play, and unlimited substitutions are allowed. Like six-per-side indoor netball, attacking players may shoot two-point goals from outside the shooting circle. Each team can separately nominate one "power play" quarter, in which each goal scored by that team is worth double points and the centre pass is taken by the team that conceded the goal.

Netball has been adapted in several ways to meet children's needs. The rules for children are similar to those for adults, but various aspects of the game (such as the length of each quarter, goal height, and ball size) are modified.

Fun Net is a version of netball developed by Netball Australia for five- to seven-year-olds. It aims to improve basic netball skills using games and activities. The Fun Net program runs for 8–16 weeks. There are no winners or losers. The goal posts are high, and a smaller ball is used.

Netball Australia also runs a modified game called Netta aimed at 8- to 11-year-olds. The goal height and ball size are the same as for adults, but players rotate positions during the game, permitting each player to play each position. Netta was created to develop passing and catching skills. Its rules permit six seconds between catching and passing the ball, instead of the three seconds permitted in the adult game. Most players under 11 play this version at netball clubs.

A version called High Five Netball is promoted by the All England Netball Association. It is aimed at 9- to 11-year-old girls and includes only five positions. The players swap positions during the game. When a player is not on the court, she is expected to help the game in some other way, such as being the timekeeper or scorekeeper. High Five Netball has four six-minute quarters.

The recognised international governing body of netball is the International Federation of Netball Associations (IFNA), based in Manchester, England. Founded in 1960, the organisation was initially called the International Federation of Netball and Women's Basketball. The IFNA is responsible for compiling world rankings for national teams, maintaining the rules for netball and organising several major international competitions.

As of July 2019, the IFNA has 53 full and 19 associate national members in five regions. Each region has an IFNA regional federation.
The IFNA is affiliated with the General Association of International Sports Federations, the International World Games Association and the Association of IOC Recognised International Sports Federations. It is also a signatory to the World Anti-Doping Code.

Netball is a popular participant sport in countries of the Commonwealth of Nations. Non-Commonwealth entities with full IFNA membership include Switzerland, Taiwan, Thailand, Argentina, Bermuda, the Cayman Islands and the United States, along with former Commonwealth members Zimbabwe, Ireland and Hong Kong. According to the IFNA, over 20 million people play netball in more than 80 countries. International tournaments are held among countries in each of the five IFNA regions, either annually or every four years. School leagues and national club competitions have been organised in England, Australia, New Zealand and Jamaica since the early twentieth century. Franchise-based netball leagues did not emerge until the late 1990s. These competitions sought to increase the profile of the sport in their respective countries. Despite widespread local interest, participation was largely amateur.

Netball was first included in the 1998 Commonwealth Games and has been a fixture ever since; it is currently one of the "core" sports that must be contested at each edition of the Games.

The major international tournament in Africa is organised by the Confederation of African Netball Associations, which invites teams from Botswana, Namibia, Zambia, Malawi, South Africa, Lesotho, Swaziland, Zimbabwe and the Seychelles to take part. The tournament is hosted by a country within the region; senior and under 21 teams compete. The tournament has served as a qualifier for the World Championships. South Africa launched a new domestic competition in 2011 called Netball Grand Series. It features eight regional teams from South Africa and is aimed at increasing the amount of playing time for players. It runs for 17 weeks and replaces the National Netball League, which was played over only two weeks. According to Proteas captain Elsje Jordaan, it was hoped that the competition would create an opportunity for players to become professional.
The American Federation of Netball Associations (AFNA) hosts two tournaments each year: the Caribbean Netball Association (CNA) Under 16 Championship and the AFNA Senior Championship. The CNA championship involves two divisions of teams from the Caribbean islands. In 2010 five teams competed in two rounds of round robin matches in the Championship Division, while four teams competed in the Developmental Division. Jamaica, which has lost only once in the tournament, decided not to play the 2011 tournament. The AFNA Senior Championship includes Canada and the US along with the Caribbean nations. The tournament serves as a qualifier for the World Championship. Jamaica, with its high ranking, does not have to qualify; this leaves two spots to the other teams in the tournament.

The Asian Netball Championship is held every four years. The seventh Asian games were held in 2009 and featured Singapore, Thailand, Maldives, Taiwan, Malaysia, Sri Lanka, Hong Kong, India and Pakistan. There is also an Asian Youth Netball Championship for girls under 21 years of age, the seventh of which was held in 2010.

The major netball competition in Europe is the Netball Superleague, which features nine teams from England, Wales and Scotland. The league was created in 2005. Matches are broadcast on Sky Sports.

Netball has been featured at the Pacific Games, a multi-sport event with participation from 22 countries from around the South Pacific. The event is held every four years and has 12 required sports; the host country chooses the other four. Netball is not a required sport and has missed selection, particularly when former French or American territories host the games.

The ANZ Championship was a Trans-Tasman competition held between 2008 and 2016 that was broadcast on television in both New Zealand and Australia. It was contested among ten teams from Australia and New Zealand. It began in April 2008, succeeding Australia's Commonwealth Bank Trophy and New Zealand's National Bank Cup as the pre-eminent netball league in those countries. The competition was held annually between April and July, consisting of 69 matches played over 17 weeks. The ANZ Championship saw netball become a semi-professional sport in both countries, with increased media coverage and player salaries. The competition was replaced by new leagues in 2017, the Suncorp Super Netball (Australia) and ANZ Premiership (New Zealand).

There are four major international netball competitions; the Netball World Cup, Netball at the Commonwealth Games, Netball Quad Series and Fast5 Netball World Series.

Netball's important competition is the World Netball Championships (also known as the Netball World Cup), held every four years. It was first held in 1963 at the Chelsea College of Physical Education at Eastbourne, England, with eleven nations competing. Since its inception the competition has been dominated primarily by the Australian and New Zealand teams, which hold ten and four titles, respectively. Trinidad and Tobago is the only other team to win a championship title. That title, won in 1979, was shared with New Zealand and Australia; all three teams finished with equal points at the end of the round robin, and there were no finals.

The Fast5 Series is a competition among the top six national netball teams, as ranked by the INF World Rankings. It is organised by the INF in conjunction with the national governing bodies of the six competing nations, UK Sport, and the host city's local council. The All England Netball Association covers air travel, accommodation, food and local travel expenses for all teams, while the respective netball governing bodies cover player allowances. It is held over three days, with each team playing each other once during the first two days in a round-robin format. The four highest-scoring teams advance to the semi-finals; the winners face each other in the Grand Final. The competition features modified fastnet rules and has been likened to Twenty20 cricket and rugby sevens. A new format featuring shorter matches with modified rules was designed to make the game more appealing to spectators and television audiences. The World Netball Series was held annually in England from 2009 to 2011.

Netball gained Olympic recognition in 1995 after 20 years of lobbying. Although it has never been played at the Summer Olympics, politicians and administrators have been campaigning to have it included in the near future. Its absence from the Olympics has been seen by the netball community as a hindrance in the global growth of the game by limiting access to media attention and funding sources. Some funding sources became available with recognition in 1995, including the International Olympic Committee, national Olympic committees, national sport organisations, and state and federal governments.

One study found that over 14 weeks of play about 5% of people develop an injury. The most common injury is of the ankle (usually lateral ligament ankle strain and less often an ankle fracture). Knee injuries were less common and included anterior cruciate ligament (ACL) injuries. The main cause of these injuries is believed to be due to incorrect landing. One study found not warming-up as a risk factor. Hypermobility (having a range of motion beyond normal limits) has been associated with injuries in one small study. Higher grade players, in both senior and junior competitions, are more susceptible to injuries than lower grade players, due to the high intensity and rapid pace of the game.

In October 2005, Australian captain Liz Ellis, tore her ACL in a match against New Zealand. This injury ruled her out of the chance to play at the 2006 Melbourne Commonwealth games. In October 2014, Casey Kopua ruptured the patella tendon in her left knee resulted in her missing up to 6 months of netball.





</doc>
<doc id="21594" url="https://en.wikipedia.org/wiki?curid=21594" title="Njörðr">
Njörðr

In Norse mythology, Njörðr is a god among the Vanir. Njörðr, father of the deities Freyr and Freyja by his unnamed sister, was in an ill-fated marriage with the goddess Skaði, lives in Nóatún and is associated with the sea, seafaring, wind, fishing, wealth, and crop fertility.

Njörðr is attested in the "Poetic Edda", compiled in the 13th century from earlier traditional sources, the "Prose Edda", written in the 13th century by Snorri Sturluson, in euhemerized form as a beloved mythological early king of Sweden in "Heimskringla", also written by Snorri Sturluson in the 13th century, as one of three gods invoked in the 14th century "Hauksbók" ring oath, and in numerous Scandinavian place names. Veneration of Njörðr survived into the 18th or 19th century Norwegian folk practice, where the god is recorded as Njor and thanked for a bountiful catch of fish.

Njörðr has been the subject of an amount of scholarly discourse and theory, often connecting him with the figure of the much earlier attested Germanic goddess Nerthus, the hero Hadingus, and theorizing on his formerly more prominent place in Norse paganism due to the appearance of his name in numerous place names. "Njörðr" is sometimes modernly anglicized as Njord, Njoerd, or Njorth.

The name "Njörðr" corresponds to that of the older Germanic fertility goddess "Nerthus", and both derive from the Proto-Germanic "*Nerþuz". The original meaning of the name is contested, but it may be related to the Irish word "nert" which means "force" and "power". It has been suggested that the change of sex from the female "Nerthus" to the male "Njörðr" is due to the fact that feminine nouns with u-stems disappeared early in Germanic language while the masculine nouns with u-stems prevailed. However, other scholars hold the change to be based not on grammatical gender but on the evolution of religious beliefs; that *Nerþuz and Njörðr appear as different genders because they are to be considered separate beings. The name "Njörðr" may be related to the name of the Norse goddess Njörun.

Njörðr's name appears in various place names in Scandinavia, such as "Nærdhæwi" (now Nalavi, Närke), "Njærdhavi" (now Mjärdevi, Linköping; both using the religious term vé), "Nærdhælunda" (now Närlunda, Helsingborg), "Nierdhatunum" (now Närtuna, Uppland) in Sweden, Njarðvík in southwest Iceland, Njarðarlög and Njarðey (now Nærøy) in Norway. Njörðr's name appears in a word for sponge; "Njarðarvöttr" (Old Norse "Njörðr's glove"). Additionally, in Old Icelandic translations of Classical mythology the Roman god Saturn's name is glossed as "Njörðr."

Njörðr is described as a future survivor of Ragnarök in stanza 39 of the poem "Vafþrúðnismál". In the poem, the god Odin, disguised as "Gagnráðr" faces off with the wise jötunn Vafþrúðnir in a battle of wits. While Odin states that Vafþrúðnir knows all the fates of the gods, Odin asks Vafþrúðnir "from where Njörðr came to the sons of the Æsir," that Njörðr rules over quite a lot of temples and hörgrs (a type of Germanic altar), and further adds that Njörðr was not raised among the Æsir. In response, Vafþrúðnir says:

In stanza 16 of the poem "Grímnismál", Njörðr is described as having a hall in Nóatún made for himself. The stanza describes Njörðr as a "prince of men," that he is "lacking in malice," and that he "rules over the "high-timbered temple." In stanza 43, the creation of the god Freyr's ship Skíðblaðnir is recounted, and Freyr is cited as the son of Njörðr. In the prose introduction to the poem "Skírnismál", Freyr is mentioned as the son of Njörðr, and stanza 2 cites the goddess Skaði as the mother of Freyr. Further in the poem, Njörðr is again mentioned as the father of Freyr in stanzas 38, 39, and 41.

In the late flyting poem "Lokasenna", an exchange between Njörðr and Loki occurs in stanzas 33, 34, 35, and 36. After Loki has an exchange with the goddess Freyja, in stanza 33 Njörðr states:

Loki responds in the stanza 34, stating that "from here you were sent east as hostage to the gods" (a reference to the Æsir-Vanir War) and that "the daughters of Hymir used you as a pisspot, and pissed in your mouth." In stanza 35, Njörðr responds that:

Loki tells Njörðr to "stop" and "keep some moderation," and that he "won't keep it a secret any longer" that Njörðr's son Freyr was produced with his unnamed sister, "though you'd expect him to be worse than he is." The god Tyr then interjects and the flyting continues in turn.

Njörðr is referenced in stanza 22 of the poem "Þrymskviða", where he is referred to as the father of the goddess Freyja. In the poem, the jötunn Þrymr mistakenly thinks that he will be receiving the goddess Freyja as his bride, and while telling his fellow jötunn to spread straw on the benches in preparation for the arrival of Freyja, he refers to her as the daughter of Njörðr of Nóatún. Towards the end of the poem "Sólarljóð", Njörðr is cited as having nine daughters. Two of the names of these daughters are given; the eldest Ráðveig and the youngest Kreppvör.

Njörðr is also mentioned in the "Prose Edda" books "Gylfaginning" and "Skáldskaparmál".

In the "Prose Edda", Njörðr is introduced in chapter 23 of the book "Gylfaginning". In this chapter, Njörðr is described by the enthroned figure of High as living in the heavens at Nóatún, but also as ruling over the movement of the winds, having the ability to calm both sea and fire, and that he is to be invoked in seafaring and fishing. High continues that Njörðr is very wealthy and prosperous, and that he can also grant wealth in land and valuables to those who request his aid. Njörðr originates from Vanaheimr and is devoid of Æsir stock, and he is described as having been traded with Hœnir in hostage exchange with between the Æsir and Vanir.

High further states that Njörðr's wife is Skaði, that she is the daughter of the jötunn Þjazi, and recounts a tale involving the two. High recalls that Skaði wanted to live in the home once owned by her father called Þrymheimr ("Thunder Home"). However, Njörðr wanted to live nearer to the sea. Subsequently, the two made an agreement that they would spend nine nights in Þrymheimr and then next three nights in Nóatún (or nine winters in Þrymheimr and another nine in Nóatún according to the "Codex Regius" manuscript). However, when Njörðr returned from the mountains to Nóatún, he says:

Skaði then responds:

High states that afterward Skaði went back up to the mountains to Þrymheimr and recites a stanza where Skaði skis around, hunts animals with a bow, and lives in her fathers old house. Chapter 24 begins, which describes Njörðr as the father of two beautiful and powerful children: Freyr and Freyja. In chapter 37, after Freyr has spotted the beautiful jötunn Gerðr, he becomes overcome with sorrow, and refuses to sleep, drink, or talk. Njörðr then sends for Skírnir to find out who he seems to be so angry at, and, not looking forward to being treated roughly, Skírnir reluctantly goes to Freyr.

Njörðr is introduced in "Skáldskaparmál" within a list of 12 Æsir attending a banquet held for Ægir. Further in "Skáldskaparmál", the skaldic god Bragi recounds the death of Skaði's father Þjazi by the Æsir. As one of the three acts of reparation performed by the Æsir for Þjazi's death, Skaði was allowed by the Æsir to choose a husband from amongst them, but given the stipulation that she may not see any part of them but their feet when making the selection. Expecting to choose the god Baldr by the beauty of the feet she selects, Skaði instead finds that she has picked Njörðr.

In chapter 6, a list of kennings is provided for Njörðr: "God of chariots," "Descendant of Vanir," "a Van," father of Freyr and Freyja, and "the giving God." This is followed by an excerpt from a composition by the 11th century skald Þórðr Sjáreksson, explained as containing a reference to Skaði leaving Njörðr:

Chapter 7 follows and provides various kennings for Freyr, including referring to him as the son of Njörðr. This is followed by an excerpt from a work by the 10th-century skald Egill Skallagrímsson that references Njörðr (here anglicized as "Niord"):

In chapter 20, "daughter of Njörðr" is given as a kenning for Freyja. In chapter 33, Njörðr is cited among the gods attending a banquet held by Ægir. In chapter 37, Freyja is again referred to as Njörðr's daughter in a verse by the 12th century skald Einarr Skúlason. In chapter 75, Njörðr is included in a list of the Æsir. Additionally, "Njörðr" is used in kennings for "warrior" or "warriors" various times in "Skáldskaparmál".

Njörðr appears in or is mentioned in three Kings' sagas collected in "Heimskringla"; "Ynglinga saga", the "Saga of Hákon the Good" and the "Saga of Harald Graycloak". In chapter 4 of "Ynglinga saga", Njörðr is introduced in connection with the Æsir-Vanir War. When the two sides became tired of war, they came to a peace agreement and exchanged hostages. For their part, the Vanir send to the Æsir their most "outstanding men"; Njörðr, described as wealthy, and Freyr, described as his son, in exchange for the Æsir's Hœnir. Additionally, the Æsir send Mímir in exchange for the wise Kvasir.

Further into chapter 4, Odin appoints Njörðr and Freyr as priests of sacrificial offerings, and they became gods among the Æsir. Freyja is introduced as a daughter of Njörðr, and as the priestess at the sacrifices. In the saga, Njörðr is described as having once wed his unnamed sister while he was still among the Vanir, and the couple produced their children Freyr and Freyja from this union, though this custom was forbidden among the Æsir.

Chapter 5 relates that Odin gave all of his temple priests dwelling places and good estates, in Njörðr's case being Nóatún. Chapter 8 states that Njörðr married a woman named Skaði, though she would not have intercourse with him. Skaði then marries Odin, and the two had numerous sons.

In chapter 9, Odin dies and Njörðr takes over as ruler of the Swedes, and he continues the sacrifices. The Swedes recognize him as their king, and pay him tribute. Njörðr's rule is marked with peace and many great crops, so much so that the Swedes believed that Njörðr held power over the crops and over the prosperity of mankind. During his rule, most of the Æsir die, their bodies are burned, and sacrifices are made by men to them. Njörðr has himself "marked for" Odin and he dies in his bed. Njörðr's body is burnt by the Swedes, and they weep heavily at his tomb. After Njörðr's reign, his son Freyr replaces him, and he is greatly loved and "blessed by good seasons like his father."

In chapter 14 of "Saga of Hákon the Good" a description of the pagan Germanic custom of Yule is given. Part of the description includes a series of toasts. The toasts begin with Odin's toasts, described as for victory and power for the king, followed by Njörðr and Freyr's toast, intended for good harvests and peace. Following this, a beaker is drank for the king, and then a toast is given for departed kin. Chapter 28 quotes verse where the kenning "Njörðr-of-roller-horses" is used for "sailor". In the "Saga of Harald Graycloak", a stanza is given of a poem entitled "Vellekla" ("Lack of Gold") by the 10th century Icelandic skald Einarr skálaglamm that mentions Njörðr in a kenning for "warrior."

In chapter 80 of the 13th century Icelandic saga "Egils saga", Egill Skallagrímsson composes a poem in praise of Arinbjörn ("Arinbjarnarkviða"). In stanza 17, Egill writes that all others watch in marvel how Arinbjörn gives out wealth, as he has been so endowed by the gods Freyr and Njörðr.

Veneration of Njörðr survived into 18th or 19th century Norwegian folk practice, as recorded in a tale collected by Halldar O. Opedal from an informant in Odda, Hordaland, Norway. The informant comments on a family tradition in which the god is thanked for a bountiful catch of fish: 

Scholar Georges Dumézil further cites various tales of "havmennesker" (Norwegian "sea people") who govern over sea weather, wealth, or, in some incidents, give magic boats, and proposes that they are historically connected to Njörðr.

Njörðr is often identified with the goddess Nerthus, whose reverence by various Germanic tribes is described by Roman historian Tacitus in his 1st CE century work "Germania". The connection between the two is due to the linguistic relationship between "Njörðr" and the reconstructed "*Nerþuz", "Nerthus" being the feminine, Latinized form of what "Njörðr" would have looked like around 1 CE. This has led to theories about the relation of the two, including that Njörðr may have once been a hermaphroditic god or, generally considered more likely, that the name may indicate an otherwise unattested divine brother and sister pair such as Freyr and Freyja. Consequently, Nerthus has been identified with Njörðr's unnamed sister with whom he had Freyja and Freyr, which is mentioned in "Lokasenna".

In Saami mythology, Bieka-Galles (or Biega-, Biegga-Galles, depending on dialect; "The Old Man of the Winds") is a deity who rules over rain and wind, and is the subject of boat and wooden shovel (or, rather, oar) offerings. Due to similarities in between descriptions of Njörðr in "Gylfaginning" and descriptions of Bieka-Galles in 18th century missionary reports, Axel Olrik identified this deity as the result of influence from the seafaring North Germanic peoples on the landbound Saami.

Parallels have been pointed out between Njörðr and the figure of Hadingus, attested in book I of Saxo Grammaticus' 13th century work "Gesta Danorum". Some of these similarities include that, in parallel to Skaði and Njörðr in "Skáldskaparmál", Hadingus is chosen by his wife Ragnhild after selecting him from other men at a banquet by his lower legs, and, in parallel to Skaði and Njörðr in "Gylfaginning", Hadingus complains in verse of his displeasure at his life away from the sea and how he is disturbed by the howls of wolves, while his wife Regnhild complains of life at the shore and states her annoyance at the screeching sea birds. Georges Dumézil theorized that in the tale Hadingus passes through all three functions of his trifunctional hypothesis, before ending as an Odinic hero, paralleling Njörðr's passing from the Vanir to the Æsir in the Æsir-Vanir War.

In stanza 8 of the poem "Fjölsvinnsmál", Svafrþorinn is stated as the father of Menglöð by an unnamed mother, who the hero Svipdagr seeks. Menglöð has often been theorized as the goddess Freyja, and according to this theory, Svafrþorinn would therefore be Njörðr. The theory is complicated by the etymology of the name "Svafrþorinn" ("þorinn" meaning "brave" and "svafr" means "gossip") (or possibly connects to "sofa" "sleep"), which Rudolf Simek says makes little sense when attempting to connect it to Njörðr.

Njörðr has been the subject of an amount of artistic depictions. Depictions include "Freyr und Gerda; Skade und Niurd" (drawing, 1883) by K. Ehrenberg, "Njörðr" (1893) by Carl Frederick von Saltza, "Skadi" (1901) by E. Doepler d. J., and "Njörd's desire of the Sea" (1908) by W. G. Collingwood.

Njörðr is one of the incarnated gods in the New Zealand comedy/drama "The Almighty Johnsons". The part of "Johan Johnson/Njörðr" is played by Stuart Devenie.

 


</doc>
<doc id="21597" url="https://en.wikipedia.org/wiki?curid=21597" title="Neutral">
Neutral

Neutral or neutrality may refer to:











</doc>
<doc id="21601" url="https://en.wikipedia.org/wiki?curid=21601" title="Niger–Congo languages">
Niger–Congo languages

The Niger–Congo languages are the world's third largest language family in terms of number of speakers and Africa's largest in terms of geographical area, number of speakers, and number of distinct languages. It is generally considered to be the world's largest language family in terms of number of distinct languages, just ahead of Austronesian, although this is complicated by the ambiguity about what constitutes a distinct language; the number of named Niger–Congo languages listed by "Ethnologue" is 1,540. 

It is the third-largest language family in the world by a number of native speakers, comprising around 700 million people as of 2015. Within Niger–Congo, the Bantu languages alone account for 350 million people (2015), or half the total Niger–Congo speaking population. The most widely spoken Niger–Congo languages by number of native speakers are Yoruba, Igbo, Fula and Zulu. The most widely spoken by the total number of speakers is Swahili, which is used as a lingua franca in parts of eastern and southeastern Africa.

While the ultimate genetic unity of the core of Niger–Congo (called Atlantic–Congo) is widely accepted, the internal cladistic structure is not well established. Other primary branches may include Dogon, Mande, Ijo, Katla and Rashad. The connection of the Mande languages especially has never been demonstrated, and without them, the validity of Niger–Congo family as a whole (as opposed to Atlantic–Congo or a similar subfamily) has not been established.

One of the most distinctive characteristics common to Atlantic–Congo languages is the use of a noun-class system, which is essentially a gender system with multiple genders.

The language family most likely originated in or near the area where these languages were spoken prior to Bantu expansion (i.e. West Africa or Central Africa). Its expansion may have been associated with the expansion of Sahel agriculture in the African Neolithic period, following the desiccation of the Sahara in c. 3500 BCE.

According to Roger Blench (2004), all specialists in Niger–Congo languages believe the languages to have a common origin, rather than merely constituting a typological classification, for reasons including their shared noun-class system, shared verbal extensions and shared basic lexicon. Similar classifications to Niger–Congo have been made ever since Diedrich Westermann in 1922. Joseph Greenberg continued that tradition, making it the starting point for modern linguistic classification in Africa, with some of his most notable publications going to press starting in the 1960s. However, there has been active debate for many decades over the appropriate subclassifications of the languages in this language family, which is a key tool used in localising a language's place of origin. No definitive "Proto-Niger–Congo" lexicon or grammar has been developed for the language family as a whole.

An important unresolved issue in determining the time and place where the Niger–Congo languages originated and their range prior to recorded history is this language family's relationship to the Kordofanian languages, now spoken in the Nuba mountains of Sudan, which is not contiguous with the remainder of the Niger–Congo-language-speaking region and is at the northeasternmost extent of the current Niger–Congo linguistic region. The current prevailing linguistic view is that Kordofanian languages are part of the Niger–Congo language family and that these may be the first of the many languages still spoken in that region to have been spoken in the region. The evidence is insufficient to determine if this outlier group of Niger–Congo language speakers represent a prehistoric range of a Niger–Congo linguistic region that has since contracted as other languages have intruded, or if instead, this represents a group of Niger–Congo language speakers who migrated to the area at some point in prehistory where they were an isolated linguistic community from the beginning.

There is more agreement regarding the place of origin of Benue–Congo, the largest subfamily of the group. Within Benue–Congo, the place of origin of the Bantu languages as well as time at which it started to expand is known with great specificity. Blench (2004), relying particularly on prior work by Kay Williamson and P. De Wolf, argued that Benue–Congo probably originated at the confluence of the Benue and Niger Rivers in central Nigeria. These estimates of the place of origin of the Benue-Congo language family do not fix a date for the start of that expansion, other than that it must have been sufficiently prior to the Bantu expansion to allow for the diversification of the languages within this language family that includes Bantu.

The classification of the relatively divergent family of the Ubangian languages, centred in the Central African Republic, as part of the Niger–Congo language family is disputed. Ubangian was grouped with Niger–Congo by Greenberg (1963), and later authorities concurred, but it was questioned by Dimmendaal (2008).

The Bantu expansion, beginning around 1000 BC, swept across much of Central and Southern Africa, leading to the extinction of much of the indigenous Pygmy and Bushmen (Khoisan) populations there.

The following is an overview of the language groups usually included in Niger–Congo. The genetic relationship of some branches is not universally accepted, and the cladistic connection between those who are accepted as related may also be unclear.

The core phylum of the Niger–Congo group are the Atlantic–Congo languages. The non-Atlantic–Congo languages within Niger–Congo are grouped as Dogon, Mande, Ijo (sometimes with Defaka as Ijoid), Katla and Rashad.

Atlantic–Congo combines the Atlantic languages, which do not form one branch, and Volta–Congo. It comprises more than 80% of the Niger–Congo speaking population, or close to 600 million people (2015).

The proposed Savannas group combines Adamawa, Ubangian and Gur. Outside of the Savannas group, Volta–Congo comprises Kru, Kwa (or "West Kwa"), Volta–Niger (also "East Kwa" or "West Benue–Congo") and Benue–Congo (or "East Benue–Congo"). Volta–Niger includes the two largest languages of Nigeria, Yoruba and Igbo. Benue–Congo includes the Southern Bantoid group, which is dominated by the Bantu languages, which account for 350 million people (2015), or half the total Niger–Congo speaking population.

The strict genetic unity of any of these subgroups may themselves be under dispute. For example, Roger Blench (2012) argued that Adamawa, Ubangian, Kwa, Bantoid, and Bantu are not coherent groups.

"Glottolog" 3.4 (2019) does not accept that the Kordofanian branches (Lafofa, Talodi and Heiban) or the difficult-to-classify Laal language have been demonstrated to be Atlantic–Congo languages. It otherwise accepts the family but not its inclusion within a broader Niger–Congo. Glottolog also considers Ijoid, Mande, and Dogon to be independent language phyla that have not been demonstrated to be related to each other.

The Atlantic–Congo group is characterised by the noun class systems of its languages. Atlantic–Congo largely corresponds to Mukarovsky's "Western Nigritic" phylum.


The polyphyletic Atlantic group accounts for about 35 million speakers as of 2016, mostly accounted for by Fula and Wolof speakers. Atlantic is not considered to constitute a valid group.


The putative Niger–Congo languages outside of the Atlantic–Congo family are centred in the upper Senegal and Niger river basins, south and west of Timbuktu (Mande, Dogon), the Niger Delta (Ijoid), and far to the east in south-central Sudan, around the Nuba Mountains (the Kordofanian families). They account for a total population of about 100 million (2015), mostly Mandé and Ijaw.

The various Kordofanian languages are spoken in south-central Sudan, around the Nuba Mountains. "Kordofanian" is a geographic grouping, not a genetic one, named for the Kordofan region. These are minor languages, spoken by a total of about 100,000 people according to 1980s estimates. Katla and Rashad languages show isoglosses with Benue-Congo that the other families lack. 


The endangered or extinct Laal, Mpre and Jalaa languages are often assigned to Niger–Congo.

Niger–Congo as it is known today was only gradually recognized as a linguistic unit. In early classifications of the languages of Africa, one of the principal criteria used to distinguish different groupings was the languages' use of prefixes to classify nouns, or the lack thereof. A major advance came with the work of Sigismund Wilhelm Koelle, who in his 1854 "Polyglotta Africana" attempted a careful classification, the groupings of which in quite a number of cases correspond to modern groupings. An early sketch of the extent of Niger–Congo as one language family can be found in Koelle's observation, echoed in Bleek (1856), that the Atlantic languages used prefixes just like many Southern African languages. Subsequent work of Bleek, and some decades later the comparative work of Meinhof, solidly established Bantu as a linguistic unit.

In many cases, wider classifications employed a blend of typological and racial criteria. Thus, Friedrich Müller, in his ambitious classification (1876–88), separated the 'Negro' and Bantu languages. Likewise, the Africanist Karl Richard Lepsius considered Bantu to be of African origin, and many 'Mixed Negro languages' as products of an encounter between Bantu and intruding Asiatic languages.

In this period a relation between Bantu and languages with Bantu-like (but less complete) noun class systems began to emerge. Some authors saw the latter as languages which had not yet completely evolved to full Bantu status, whereas others regarded them as languages which had partly lost original features still found in Bantu. The Bantuist Meinhof made a major distinction between Bantu and a 'Semi-Bantu' group which according to him was originally of the unrelated Sudanic stock.

Westermann, a pupil of Meinhof, set out to establish the internal classification of the then Sudanic languages. In a 1911 work he established a basic division between 'East' and 'West'. A historical reconstruction of West Sudanic was published in 1927, and in his 1935 'Charakter und Einteilung der Sudansprachen' he conclusively established the relationship between Bantu and West Sudanic.

Joseph Greenberg took Westermann's work as a starting-point for his own classification. In a series of articles published between 1949 and 1954, he argued that Westermann's 'West Sudanic' and Bantu formed a single genetic family, which he named Niger–Congo; that Bantu constituted a subgroup of the Benue–Congo branch; that Adamawa–Eastern, previously not considered to be related, was another member of this family; and that Fula belonged to the West Atlantic languages. Just before these articles were collected in final book form ("The Languages of Africa") in 1963, he amended his classification by adding Kordofanian as a branch co-ordinate with Niger–Congo as a whole; consequently, he renamed the family "Congo–Kordofanian", later "Niger–Kordofanian". Greenberg's work on African languages, though initially greeted with scepticism, became the prevailing view among scholars.

Bennet and Sterk (1977) presented an internal reclassification based on lexicostatistics that laid the foundation for the regrouping in Bendor-Samuel (1989). Kordofanian was presented as one of several primary branches rather than being coordinate to the family as a whole, prompting re-introduction of the term "Niger–Congo", which is in current use among linguists. Many classifications continue to place Kordofanian as the most distant branch, but mainly due to negative evidence (fewer lexical correspondences), rather than positive evidence that the other languages form a valid genealogical group. Likewise, Mande is often assumed to be the second-most distant branch based on its lack of the noun-class system prototypical of the Niger–Congo family. Other branches lacking any trace of the noun-class system are Dogon and Ijaw, whereas the Talodi branch of Kordofanian does have cognate noun classes, suggesting that Kordofanian is also not a unitary group.

"Glottolog" (2013) accepts the core with noun-class systems, the Atlantic–Congo languages, apart from the recent inclusion of some of the Kordofanian groups, but not Niger–Congo as a whole. They list the following as separate families: Atlantic–Congo, Mande, Dogon, Ijoid, Lafofa, Katla–Tima, Heiban, Talodi, and Rashad.

Oxford Handbooks Online (2016) has indicated that the continuing reassessment of Niger-Congo's "internal structure is due largely to the preliminary nature of Greenberg’s classification, explicitly based as it was on a methodology that doesn’t produce proofs for genetic affiliations between languages but rather aims at identifying “likely candidates.”...The ongoing descriptive and documentary work on individual languages and their varieties, greatly expanding our knowledge on formerly little-known linguistic regions, is helping to identify clusters and units that allow for the application of the historical-comparative method. Only the reconstruction of lower-level units, instead of “big picture” contributions based on mass comparison, can help to verify (or disprove) our present concept of Niger-Congo as a genetic grouping consisting of Benue-Congo plus Volta-Niger, Kwa, Adamawa plus Gur, Kru, the so-called Kordofanian languages, and probably the language groups traditionally classified as Atlantic."

The coherence of Niger-Congo as a language phylum is supported by Grollemund, et al. (2016), using computational phylogenetic methods. The East/West Volta-Congo division, West/East Benue-Congo division, and North/South Bantoid division are not supported, whereas a Bantoid group consisting of Ekoid, Bendi, Dakoid, Jukunoid, Tivoid, Mambiloid, Beboid, Mamfe, Tikar, Grassfields, and Bantu is supported.

The Automated Similarity Judgment Program (ASJP) also groups many Niger-Congo branches together.

Proto-Niger–Congo (or Proto-Atlantic–Congo) has not been reconstructed, and few of the demonstrably coherent branches of it have been either. The major success has been several reconstructions of Proto-Bantu, which has consequently had an outsize influence on conceptions of what Proto-Niger–Congo may have been like. The only stage higher than Proto-Bantu that has been reconstructed is a pilot project by Stewart, who since the 1970s has reconstructed the common ancestor of the Potou–Tano and Bantu languages, without so far considering the hundreds of other languages which presumably descend from that same ancestor. Konstantin Pozdniakov has reconstructed the numeral system.

Over the years, several linguists have suggested a link between Niger–Congo and Nilo-Saharan, probably starting with Westermann's comparative work on the "Sudanic" family in which 'Eastern Sudanic' (now classified as Nilo-Saharan) and 'Western Sudanic' (now classified as Niger–Congo) were united. Gregersen (1972) proposed that Niger–Congo and Nilo-Saharan be united into a larger phylum, which he termed "Kongo–Saharan". His evidence was mainly based on the uncertainty in the classification of Songhay, morphological resemblances, and lexical similarities. A more recent proponent was Roger Blench (1995), who puts forward phonological, morphological and lexical evidence for uniting Niger–Congo and Nilo-Saharan in a "Niger–Saharan" phylum, with special affinity between Niger–Congo and Central Sudanic. However, fifteen years later his views had changed, with Blench (2011) proposing instead that the noun-classifier system of Central Sudanic, commonly reflected in a tripartite general–singulative–plurative number system, triggered the development or elaboration of the noun-class system of the Atlantic–Congo languages, with tripartite number marking surviving in the Plateau and Gur languages of Niger–Congo, and the lexical similarities being due to loans.

Niger–Congo languages have a clear preference for open syllables of the type CV (Consonant Vowel). The typical word structure of Proto-Niger–Congo (though it has not been reconstructed) is thought to have been CVCV, a structure still attested in, for example, Bantu, Mande and Ijoid – in many other branches this structure has been reduced through phonological change. Verbs are composed of a root followed by one or more extensional suffixes. Nouns consist of a root originally preceded by a noun class prefix of (C)V- shape which is often eroded by phonological change.

Several branches of Niger–Congo have a regular phonological contrast between two classes of consonants. Pending more clarity as to the precise nature of this contrast, it is commonly characterized as a contrast between fortis and lenis consonants.

Many Niger–Congo languages' vowel harmony is based on the [ATR] (advanced tongue root) feature. In this type of vowel harmony, the position of the root of the tongue in regards to backness is the phonetic basis for the distinction between two harmonizing sets of vowels. In its fullest form, this type involves two classes, each of five vowels:

The roots are then divided into [+ATR] and [−ATR] categories. This feature is lexically assigned to the roots because there is no determiner within a normal root that causes the [ATR] value.

There are two types of [ATR] vowel harmony controllers in Niger–Congo. The first controller is the root. When a root contains a [+ATR] or [−ATR] vowel, then that value is applied to the rest of the word, which involves crossing morpheme boundaries. For example, suffixes in Wolof assimilate to the [ATR] value of the root to which they attach. Some examples of these suffixes that alternate depending on the root are:

Furthermore, the directionality of assimilation in [ATR] root-controlled vowel harmony need not be specified. The root features [+ATR] and [−ATR] spread left and/or right as needed, so that no vowel would lack a specification and be ill-formed.

Unlike in the root-controlled harmony system, where the two [ATR] values behave symmetrically, a large number of Niger–Congo languages exhibit a pattern where the [+ATR] value is more active or dominant than the [−ATR] value. This results in the second vowel harmony controller being the [+ATR] value. If there is even one vowel that is [+ATR] in the whole word, then the rest of the vowels harmonize with that feature. However, if there is no vowel that is [+ATR], the vowels appear in their underlying form. This form of vowel harmony control is best exhibited in West African languages. For example, in Nawuri, the diminutive suffix /-bi/ will cause the underlying [−ATR] vowels in a word to become phonetically [+ATR].

There are two types of vowels which affect the harmony process. These are known as neutral or opaque vowels. Neutral vowels do not harmonize to the [ATR] value of the word, and instead maintain their own [ATR] value. The vowels that follow them, however, will receive the [ATR] value of the root. Opaque vowels maintain their own [ATR] value as well, but they affect the harmony process behind them. All of the vowels following an opaque vowel will harmonize with the [ATR] value of the opaque vowel instead of the [ATR] vowel of the root.

The vowel inventory listed above is a ten-vowel language. This is a language in which all of the vowels of the language participate in the harmony system, producing five harmonic pairs. Vowel inventories of this type are still found in some branches of Niger-Congo, for example in the Ghana Togo Mountain languages. However, this is the rarer inventory as oftentimes there are one or more vowels that are not part of a harmonic pair. This has resulted in seven-and nine-vowel systems being the more popular systems. The majority of languages with [ATR] controlled vowel harmony have either seven- or nine-vowel phonemes, with the most common non-participatory vowel being /a/. It has been asserted that this is because vowel quality differences in the mid-central region where /ə/, the counterpart of /a/, is found, are difficult to perceive. Another possible reason for the non-participatory status of /a/ is that there is articulatory difficulty in advancing the tongue root when the tongue body is low in order to produce a low [+ATR] vowel. Therefore, the vowel inventory for nine-vowel languages is generally:

And seven-vowel languages have one of two inventories:

Note that in the nine-vowel language, the missing vowel is, in fact, [ə], [a]'s counterpart, as would be expected.

The fact that ten vowels have been reconstructed for proto-Ijoid has led to the hypothesis that the original vowel inventory of Niger–Congo was a full ten-vowel system. On the other hand, Stewart, in recent comparative work, reconstructs a seven-vowel system for his proto-Potou-Akanic-Bantu.

Several scholars have documented a contrast between oral and nasal vowels in Niger–Congo. In his reconstruction of proto-Volta–Congo, Steward (1976) postulates that nasal consonants have originated under the influence of nasal vowels; this hypothesis is supported by the fact that there are several Niger–Congo languages that have been analysed as lacking nasal consonants altogether. Languages like this have nasal vowels accompanied with complementary distribution between oral and nasal consonants before oral and nasal vowels. Subsequent loss of the nasal/oral contrast in vowels may result in nasal consonants becoming part of the phoneme inventory. In all cases reported to date, the bilabial /m/ is the first nasal consonant to be phonologized. Niger–Congo thus invalidates two common assumptions about nasals: that all languages have at least one primary nasal consonant, and that if a language has only one primary nasal consonant it is /n/.

Niger–Congo languages commonly show fewer nasalized than oral vowels. Kasem, a language with a ten-vowel system employing ATR vowel harmony, has seven nasalized vowels. Similarly, Yoruba has seven oral vowels and only five nasal ones. However, the language of Zialo has a nasal equivalent for each of its seven oral vowels.

The large majority of present-day Niger–Congo languages are tonal. A typical Niger–Congo tone system involves two or three contrastive level tones. Four-level systems are less widespread, and five-level systems are rare. Only a few Niger–Congo languages are non-tonal; Swahili is perhaps the best known, but within the Atlantic branch some others are found. Proto-Niger–Congo is thought to have been a tone language with two contrastive levels. Synchronic and comparative-historical studies of tone systems show that such a basic system can easily develop more tonal contrasts under the influence of depressor consonants or through the introduction of a downstep. Languages which have more tonal levels tend to use tone more for lexical and less for grammatical contrasts.

Niger–Congo languages are known for their system of noun classification, traces of which can be found in every branch of the family but Mande, Ijoid, Dogon, and the Katla and Rashad branches of Kordofanian. These noun-classification systems are somewhat analogous to grammatical gender in other languages, but there are often a fairly large number of classes (often 10 or more), and the classes may be male human/female human/animate/inanimate, or even completely gender-unrelated categories such as places, plants, abstracts, and groups of objects. For example, in Bantu, the Swahili language is called "Kiswahili," while the Swahili people are "Waswahili." Likewise, in Ubangian, the Zande language is called "Pazande," while the Zande people are called "Azande."

In the Bantu languages, where noun classification is particularly elaborate, it typically appears as prefixes, with verbs and adjectives marked according to the class of the noun they refer to. For example, in Swahili, "watu wazuri wataenda" is 'good "(zuri)" people "(tu)" will go "(ta-enda)"'.

The same Atlantic–Congo languages which have noun classes also have a set of verb applicatives and other verbal extensions, such as the reciprocal suffix "-na" (Swahili "penda" 'to love', "pendana" 'to love each other'; also applicative "pendea" 'to love for' and causative "pendeza" 'to please').

A subject–verb–object word order is quite widespread among today's Niger–Congo languages, but SOV is found in branches as divergent as Mande, Ijoid and Dogon. As a result, there has been quite some debate as to the basic word order of Niger–Congo.

Whereas Claudi (1993) argues for SVO on the basis of existing SVO > SOV grammaticalization paths, Gensler (1997) points out that the notion of 'basic word order' is problematic as it excludes structures with, for example, auxiliaries. However, the structure SC-OC-VbStem (Subject concord, Object concord, Verb stem) found in the "verbal complex" of the SVO Bantu languages suggests an earlier SOV pattern (where the subject and object were at least represented by pronouns).

Noun phrases in most Niger–Congo languages are characteristically "noun-initial", with adjectives, numerals, demonstratives and genitives all coming after the noun. The major exceptions are found in the western areas where verb-final word order predominates and genitives precede nouns, though other modifiers still come afterwards. Degree words almost always follow adjectives, and except in verb-final languages adpositions are prepositional.

The verb-final languages of the Mende region have two quite unusual word order characteristics. Although verbs follow their direct objects, oblique adpositional phrases (like "in the house", "with timber") typically come after the verb, creating a SOVX word order. Also noteworthy in these languages is the prevalence of internally headed and correlative relative clauses, in both of which the head occurs "inside" the relative clause rather than the main clause.





</doc>
<doc id="21606" url="https://en.wikipedia.org/wiki?curid=21606" title="Napo River">
Napo River

The Napo River () is a tributary to the Amazon River that rises in Ecuador on the flanks of the east Andean volcanoes of Antisana, Sincholagua and Cotopaxi.

The total length is . The river drains an area of . The mean annual discharge is per second.
Before it reaches the plains it receives a great number of small streams from impenetrable, saturated and much broken mountainous districts, where the dense and varied vegetation seems to fight for every piece of ground. This river is one of Ecuador's Physical Features. From the north it is joined by the Coca River, having its sources in the gorges of Cayambe volcano on the equator, and also a powerful river, the Aguarico having its headwaters between Cayambe and the Colombia frontier.

From the west, it receives a secondary tributary, the Curaray, from the Andean slopes, between Cotopaxi and the Tungurahua volcano. From its Coca branch to the mouth of the Curaray the Napo is full of snags and shelving sandbanks and throws out numerous canoes among jungle-tangled islands, which in the wet season are flooded, giving the river an immense width. From the Coca to the Amazon it runs through a forested plain where not a hill is visible from the river - its uniformly level banks being only interrupted by swamps and lagoons.

From the Amazon the Napo is navigable for river craft up to its Curaray branch, a distance of about , and perhaps a bit further; thence, by painful canoe navigation, its upper waters may be ascended as far as Santa Rosa, the usual point of embarkation for any venturesome traveller who descends from the Quito tableland. The Coca river may be penetrated as far up as its middle course, where it is jammed between two mountain walls, in a deep canyon, along which it dashes over high falls and numerous reefs. This is the stream made famous by the expedition of Gonzalo Pizarro.



</doc>
<doc id="21607" url="https://en.wikipedia.org/wiki?curid=21607" title="Nanay River">
Nanay River

The Nanay River is a river in northern Peru. It is a tributary of the Amazon River, merging into this river at the city of Iquitos. The lower part of the Nanay flows to the north and west of the city, while the Itaya River flows to the south and east. Other nearby settlements on the Nanay River include the villages of Santo Tomás, Padre Cocha, and Santa Clara. During periods when the river is low, the many beaches along the Nanay are popular destinations. The Nanay belongs entirely to the lowlands, and is very crooked, has a slow current and divides into many "canos" and strings of lagoons which flood the flat, low areas of country on either side. It is simply the drainage ditch of districts which are extensively overflowed in the rainy season. Captain Archibald Butt USN, ascended it , to near its source. A part of the Nanay River flows through the Allpahuayo-Mishana National Reserve.

The Nanay is a blackwater river and it has a high fish species richness, including several that are well known from the aquarium industry. Some of these, notably green discus, are the result of accidental introductions that happened in the 1970s.


</doc>
<doc id="21609" url="https://en.wikipedia.org/wiki?curid=21609" title="Nine-ball">
Nine-ball

Nine-ball (sometimes written 9-ball) is a discipline of the cue sport pool. The game is traceable to origins in the 1920s in the United States. It is played on a rectangular billiard table with at each of the four corners and in the middle of each long side. Using a cue stick, players must strike the white cue ball to nine colored billiard balls in ascending numerical order. An individual game (or ) is won by the player pocketing the . Matches are usually played as a to a set number of racks, with the player who reaches the set number winning the match.

The game is currently governed by the World Pool-Billiard Association (WPA), with multiple regional tours. The most prestigious nine-ball tournaments are the WPA World Nine-ball Championship, and the U.S. Open Nine-ball Championships. Notable players in the game include Efren Reyes, Francisco Bustamante, Thorsten Hohmann, Earl Strickland, and Shane Van Boening. The game is often associated with hustling and gambling, with tournaments often having a "buy-in" amount to become a participant. The sport has featured in popular culture, notably in the 1961 film "The Hustler" and its 1986 sequel "The Color of Money".

Nine-ball has been played with varied rules, with games such as ten-ball, seven-ball and three-ball being derived from the game. While usually a singles sport, the game can be played in doubles, with players completing alternate shots. Examples of tournaments featuring doubles include the World Cup of Pool, World Team Championship and the Mosconi Cup.

The game was established in America by 1920, although the exact origins are unknown. Nine-ball is played with the same equipment as eight-ball and other pool games.

The game of nine-ball is played on a billiard table with six pockets and with ten balls. The , which is usually a solid shade of white (but may be spotted in some tournaments), is struck to hit the other balls on the table. The remaining balls are numbered 1 through 9, each a distinct color, with the 9-ball being striped yellow and white. The aim of the game is to hit the lowest numbered ball on the table (often referred to as the ) and balls in succession to eventually pocket the nine-ball. As long as the lowest numbered ball on the table is hit first, the player may continue to shoot as long as any ball is pocketed in any of the 6 pockets. A shot where the player hits the object ball and pockets any other ball is sometimes called a . The winner is the player who pockets the nine-ball, even if doing so by a combination shot.

Each rack begins with the object balls placed in a rack and one player playing a . The object balls are placed in a diamond-shaped configuration, with the 1-ball positioned at the front on the , and the 9-ball placed in the center. The rack used to position the balls may be either triangle-shaped, as is used for eight-ball and other pool games, or a specific diamond-shaped rack that holds only nine balls may be used. Racks are usually made of wood or plastic. A template that lies on the table during the break has also come into use.

The break consists of hitting the 1-ball, with the attempt to pocket any ball. If the nine-ball is successfully potted, the player automatically wins the rack. This is sometimes known as a . Additional rules in some tournaments exist, such as a number of balls having to reach the , and players can be chosen to break alternatively or whoever won the preceding rack. The break is often the most crucial shot in nine-ball, as it is possible to win a rack without the opponent having a single shot. This is often called a , or running the rack. Earl Strickland holds the record for break and runs, after he successfully ran 11 consecutive racks in a tournament in 1996. The first break of a match is sometimes decided by a flip of a coin, but often by playing a , with both players playing a cue ball down the table, the closest to the top rail winning the initial break.

After the break, if no balls were pocketed, the opponent has the option to continue the rack as usual, or to play a . The rules on a push out are different to those of a regular shot, as the shot does not need to hit a rail or ball. Any balls pocketed are returned to the table, including the nine-ball. After the push out, the breaking player has the option to play the shot that has been left, or to force the opponent to play on from that location. In early versions of nine-ball the push out could be called at any time during the game, but is now only for the shot after the break. The ideal position to leave the balls in after a push out is to leave a shot that the player believes they can pocket, but that their opponent would struggle with.

If a player misses potting a ball on a shot, or commits a foul shot, then their opponent plays the next shot. A foul shot can involve not making first contact with the lowest numbered ball, pocketing the cue ball, or not making contact with a with the object ball. A foul shot for any reason offers the opponent , which means they can place the cue ball at any location on the table. A player making three successive fouls (for any reason) awards that rack to the opponent. Unlike some other cue sports, such as snooker, players are allowed to jump the cue ball over other balls. However, if any ball leaves the cloth at the end of a shot, it is counted as a foul. Jumping is common in nine-ball, and players often have a dedicated jump cue.

As of the 2000s, the rules have been somewhat in flux in certain contexts, especially in Europe. The European Pocket Billiard Federation (EPBF), the WPA-affiliate in Europe, has instituted a requirement on the Euro Tour is that the break shot be taken from a "" a rectangular box smaller than the regular nine-ball breaking area. While making the money ball on breaks are still possible, they are much more difficult with the break box. This was later used on the annual international Mosconi Cup tournaments. Another Mosconi Cup rule change in 2007 called for racking such that the 9-ball rather than the 1-ball is on the , which further stops overpowered break-off shots.

The general rules of the game are fairly consistent and usually do not stray too far from the earliest format set by the Billiard Congress of America (BCA). These later formed the basis of the standardized WPA rules, which the BCA follows as a member, although amateur league play may be governed by similar but slightly different rules promulgated by the American Poolplayers Association (APA) and other organizations.

Nine-ball events worldwide are run at the highest level by the WPA. The WPA World Nine-ball Championship has events for men, women and junior players. Events are generally open to any player who can pay the entry fee, however, some events are based on qualification. The WPA hosts a world ranking schedule based on WPA events, with other ranking systems also operated by the APA and the EPBF. Other major events held by the WPA include the U.S. Open Nine-ball Championship, China Open and Turning Stone Classic. In addition, Matchroom Sport runs major events such as the Mosconi Cup, World Cup of Pool and World Pool Masters.

Outside those events held on an worldwide basis, nine-ball is played in continental tour series. Events are held on series such as the Diamond Pool Tour, Asian Tour and Euro Tour.

Several games have been derived from nine-ball. Six-ball is essentially identical to nine-ball but with three fewer balls, which are racked in a three-row triangle, with the money ball, placed in the center of the back row. According to Rudolph Wanderone Jr., the game arose in early 20th century billiard halls; halls charged for matches by the 15 ball rack rather than by table, so players of nine-ball had six balls leftover. For this reason, the game is often played with the balls numbered between 10 and 15, with the 15-ball as the money ball. 

Seven-ball is also similar, though it differs in two key ways: the game uses only seven object balls, which are racked in a hexagon, and players are restricted to pockets on their designated side the table. William D. Clayton is credited with the game's invention in the early 1980s. While not a common game, it was featured on television broadcaster ESPN's "Sudden Death Seven-ball" which aired in the early 2000s.
The most common derivative game is the game of Ten-ball. The game is a more stringent variant, using ten balls in which all pocketed balls must be . Unlike in nine-ball, the money ball cannot be pocketed on the break for an instant win. Due to its more challenging nature, and the fact that there is no publicly known technique for reliably pocketing specific object balls on the break shot, there have been suggestions among the professional circuit that ten-ball should replace nine-ball as the pro game of choice, especially since the rise of the nine-ball soft break, which is still legal in most international and non-European competition. Ten-ball has its own world championship known as the WPA World Ten-ball Championship.

The sport has featured in popular culture, most notably in the 1956 novel "The Hustler" and its 1961 film adaptation, and the 1984 novel sequel "The Color of Money" and 1986 film "The Color of Money".




</doc>
<doc id="21611" url="https://en.wikipedia.org/wiki?curid=21611" title="New World Order">
New World Order

New World Order may refer to:






</doc>
<doc id="21615" url="https://en.wikipedia.org/wiki?curid=21615" title="Nostradamus">
Nostradamus

Michel de Nostredame (depending on the source, 14 or 21 December 1503 – 1 or 2 July 1566), usually Latinised as Nostradamus, was a French astrologer, physician and reputed seer, who is best known for his book "Les Prophéties", a collection of 942 poetic quatrains allegedly predicting future events. The book was first published in 1555 and has rarely been out of print since his death.

Nostradamus's family was originally Jewish, but had converted to Catholic Christianity before he was born. He studied at the University of Avignon, but was forced to leave after just over a year when the university closed due to an outbreak of the plague. He worked as an apothecary for several years before entering the University of Montpellier, hoping to earn a doctorate, but was almost immediately expelled after his work as an apothecary (a manual trade forbidden by university statutes) was discovered. He first married in 1531, but his wife and two children died in 1534 during another plague outbreak. He fought alongside doctors against the plague before remarrying to Anne Ponsarde, with whom he had six children. He wrote an almanac for 1550 and, as a result of its success, continued writing them for future years as he began working as an astrologer for various wealthy patrons. Catherine de' Medici became one of his foremost supporters. His "Les Prophéties", published in 1555, relied heavily on historical and literary precedent, and initially received mixed reception. He suffered from severe gout toward the end of his life, which eventually developed into edema. He died on 2 July 1566. Many popular authors have retold apocryphal legends about his life.

In the years since the publication of his "Les Prophéties", Nostradamus has attracted many supporters, who, along with much of the popular press, credit him with having accurately predicted many major world events. Most academic sources reject the notion that Nostradamus had any genuine supernatural prophetic abilities and maintain that the associations made between world events and Nostradamus's quatrains are the result of misinterpretations or mistranslations (sometimes deliberate). These academics argue that Nostradamus's predictions are characteristically vague, meaning they could be applied to virtually anything, and are useless for determining whether their author had any real prophetic powers. They also point out that English translations of his quatrains are almost always of extremely poor quality, based on later manuscripts, produced by authors with little knowledge of sixteenth-century French, and often deliberately mistranslated to make the prophecies fit whatever events the translator believed they were supposed to have predicted.

Nostradamus was born on either 14 or 21 December 1503 in Saint-Rémy-de-Provence, Provence, France, where his claimed birthplace still exists, and baptized Michel. He was one of at least nine children of notary Jaume (or Jacques) de Nostredame and Reynière, granddaughter of Pierre de Saint-Rémy who worked as a physician in Saint-Rémy. Jaume's family had originally been Jewish, but his father, Cresquas, a grain and money dealer based in Avignon, had converted to Catholicism around 1459–60, taking the Christian name "Pierre" and the surname "Nostredame" (Our Lady), the saint on whose day his conversion was solemnised. The earliest ancestor who can be identified on the paternal side is Astruge of Carcassonne, who died about 1420. Michel's known siblings included Delphine, Jean (c. 1507–77), Pierre, Hector, Louis, Bertrand, Jean II (born 1522) and Antoine (born 1523).
Little else is known about his childhood, although there is a persistent tradition that he was educated by his maternal great-grandfather Jean de St. Rémy — a tradition which is somewhat undermined by the fact that the latter disappears from the historical record after 1504 when the child was only one year old.

At the age of 14 Nostradamus entered the University of Avignon to study for his baccalaureate. After little more than a year (when he would have studied the regular trivium of grammar, rhetoric and logic rather than the later quadrivium of geometry, arithmetic, music, and astronomy/astrology), he was forced to leave Avignon when the university closed its doors during an outbreak of the plague. After leaving Avignon, Nostradamus, by his own account, traveled the countryside for eight years from 1521 researching herbal remedies. In 1529, after some years as an apothecary, he entered the University of Montpellier to study for a doctorate in medicine. He was expelled shortly afterwards by the student "procurator", Guillaume Rondelet, when it was discovered that he had been an apothecary, a "manual trade" expressly banned by the university statutes, and had been slandering doctors. The expulsion document, "BIU Montpellier, Register S 2 folio 87", still exists in the faculty library. However, some of his publishers and correspondents would later call him "Doctor". After his expulsion, Nostradamus continued working, presumably still as an apothecary, and became famous for creating a "rose pill" that purportedly protected against the plague.

In 1531 Nostradamus was invited by Jules-César Scaliger, a leading Renaissance scholar, to come to Agen. There he married a woman of uncertain name (possibly Henriette d'Encausse), who bore him two children. In 1534 his wife and children died, presumably from the plague. After their deaths, he continued to travel, passing through France and possibly Italy.

On his return in 1545, he assisted the prominent physician Louis Serre in his fight against a major plague outbreak in Marseille, and then tackled further outbreaks of disease on his own in Salon-de-Provence and in the regional capital, Aix-en-Provence. Finally, in 1547, he settled in Salon-de-Provence in the house which exists today, where he married a rich widow named Anne Ponsarde, with whom he had six children—three daughters and three sons. Between 1556 and 1567 he and his wife acquired a one-thirteenth share in a huge canal project, organised by Adam de Craponne, to create the Canal de Craponne to irrigate the largely waterless Salon-de-Provence and the nearby Désert de la Crau from the river Durance.

After another visit to Italy, Nostradamus began to move away from medicine and toward the "occult", although evidence suggests that he remained a Catholic and was opposed to the Protestant Reformation. But it seems he could have dabbled in horoscopes, necromancy, scrying, and good luck charms such as the hawthorn rod. Following popular trends, he wrote an almanac for 1550, for the first time in print Latinising his name to Nostradamus. He was so encouraged by the almanac's success that he decided to write one or more annually. Taken together, they are known to have contained at least 6,338 prophecies, as well as at least eleven annual calendars, all of them starting on 1 January and not, as is sometimes supposed, in March. It was mainly in response to the almanacs that the nobility and other prominent persons from far away soon started asking for horoscopes and "psychic" advice from him, though he generally expected his clients to supply the birth charts on which these would be based, rather than calculating them himself as a professional astrologer would have done. When obliged to attempt this himself on the basis of the published tables of the day, he frequently made errors and failed to adjust the figures for his clients' place or time of birth.

He then began his project of writing a book of one thousand mainly French quatrains, which constitute the largely undated prophecies for which he is most famous today. Feeling vulnerable to opposition on religious grounds, however, he devised a method of obscuring his meaning by using "Virgilianised" syntax, word games and a mixture of other languages such as Greek, Italian, Latin, and Provençal. For technical reasons connected with their publication in three installments (the publisher of the third and last installment seems to have been unwilling to start it in the middle of a "Century," or book of 100 verses), the last fifty-eight quatrains of the seventh "Century" have not survived in any extant edition.

The quatrains, published in a book titled "Les Prophéties" (The Prophecies), received a mixed reaction when they were published. Some people thought Nostradamus was a servant of evil, a fake, or insane, while many of the elite evidently thought otherwise. Catherine de' Medici, wife of King Henry II of France, was one of Nostradamus's greatest admirers. After reading his almanacs for 1555, which hinted at unnamed threats to the royal family, she summoned him to Paris to explain them and to draw up horoscopes for her children. At the time, he feared that he would be beheaded, but by the time of his death in 1566, Queen Catherine had made him Counselor and Physician-in-Ordinary to her son, the young King Charles IX of France.

Some accounts of Nostradamus's life state that he was afraid of being persecuted for heresy by the Inquisition, but neither prophecy nor astrology fell in this bracket, and he would have been in danger only if he had practised magic to support them. In 1538 he came into conflict with the Church in Agen after an Inquisitor visited the area looking for anti-Catholic views. His brief imprisonment at Marignane in late 1561 was solely because he had violated a recent royal decree by publishing his 1562 almanac without the prior permission of a bishop.

By 1566, Nostradamus's gout, which had plagued him painfully for many years and made movement very difficult, turned into edema. In late June he summoned his lawyer to draw up an extensive will bequeathing his property plus 3,444 crowns (around US$300,000 today), minus a few debts, to his wife pending her remarriage, in trust for her sons pending their twenty-fifth birthdays and her daughters pending their marriages. This was followed by a much shorter codicil. On the evening of 1 July, he is alleged to have told his secretary Jean de Chavigny, "You will not find me alive at sunrise." The next morning he was reportedly found dead, lying on the floor next to his bed and a bench (Presage 141 [originally 152] "for November 1567", as posthumously edited by Chavigny to fit what happened). He was buried in the local Franciscan chapel in Salon (part of it now incorporated into the restaurant "La Brocherie") but re-interred during the French Revolution in the Collégiale Saint-Laurent, where his tomb remains to this day.

In "The Prophecies" Nostradamus compiled his collection of major, long-term predictions. The first installment was published in 1555 and contained 353 quatrains. The third edition, with three hundred new quatrains, was reportedly printed in 1558, but now survives as only part of the omnibus edition that was published after his death in 1568. This version contains one unrhymed and 941 rhymed quatrains, grouped into nine sets of 100 and one of 42, called "Centuries".

Given printing practices at the time (which included type-setting from dictation), no two editions turned out to be identical, and it is relatively rare to find even two copies that are exactly the same. Certainly there is no warrant for assuming—as would-be "code-breakers" are prone to do—that either the spellings or the punctuation of any edition are Nostradamus's originals.

The "Almanacs", by far the most popular of his works, were published annually from 1550 until his death. He often published two or three in a year, entitled either "Almanachs" (detailed predictions), "Prognostications" or "Presages" (more generalised predictions).

Nostradamus was not only a diviner, but a professional healer. It is known that he wrote at least two books on medical science. One was an extremely free translation (or rather a paraphrase) of "The Protreptic" of Galen ("Paraphrase de C. GALIEN, sus l'Exhortation de Menodote aux estudes des bonnes Artz, mesmement Medicine"), and in his so-called "Traité des fardemens" (basically a medical cookbook containing, once again, materials borrowed mainly from others), he included a description of the methods he used to treat the plague, including bloodletting, none of which apparently worked. The same book also describes the preparation of cosmetics.

A manuscript normally known as the "Orus Apollo" also exists in the Lyon municipal library, where upwards of 2,000 original documents relating to Nostradamus are stored under the aegis of Michel Chomarat. It is a purported translation of an ancient Greek work on Egyptian hieroglyphs based on later Latin versions, all of them unfortunately ignorant of the true meanings of the ancient Egyptian script, which was not correctly deciphered until Champollion in the 19th century.

Since his death, only the "Prophecies" have continued to be popular, but in this case they have been quite extraordinarily so. Over two hundred editions of them have appeared in that time, together with over 2,000 commentaries. Their persistence in popular culture seems to be partly because their vagueness and lack of dating make it easy to quote them selectively after every major dramatic event and retrospectively claim them as "hits".

Nostradamus claimed to base his published predictions on judicial astrology—the astrological 'judgment', or assessment, of the 'quality' (and thus potential) of events such as births, weddings, coronations etc.—but was heavily criticised by professional astrologers of the day such as Laurens Videl for incompetence and for assuming that "comparative horoscopy" (the comparison of future planetary configurations with those accompanying known past events) could actually predict what would happen in the future.

Research suggests that much of his prophetic work paraphrases collections of ancient end-of-the-world prophecies (mainly Bible-based), supplemented with references to historical events and anthologies of omen reports, and then projects those into the future in part with the aid of comparative horoscopy. Hence the many predictions involving ancient figures such as Sulla, Gaius Marius, Nero, and others, as well as his descriptions of "battles in the clouds" and "frogs falling from the sky". Astrology itself is mentioned only twice in Nostradamus's "Preface" and 41 times in the "Centuries" themselves, but more frequently in his dedicatory "Letter to King Henry II". In the last quatrain of his sixth "century" he specifically attacks astrologers.

His historical sources include easily identifiable passages from Livy, Suetonius' "The Twelve Caesars", Plutarch and other classical historians, as well as from medieval chroniclers such as Geoffrey of Villehardouin and Jean Froissart. Many of his astrological references are taken almost word for word from Richard Roussat's "" of 1549–50.

One of his major prophetic sources was evidently the "Mirabilis Liber" of 1522, which contained a range of prophecies by Pseudo-Methodius, the Tiburtine Sibyl, Joachim of Fiore, Savonarola and others (his "Preface" contains 24 biblical quotations, all but two in the order used by Savonarola). This book had enjoyed considerable success in the 1520s, when it went through half a dozen editions, but did not sustain its influence, perhaps owing to its mostly Latin text, Gothic script and many difficult abbreviations. Nostradamus was one of the first to re-paraphrase these prophecies in French, which may explain why they are credited to him. Modern views of plagiarism did not apply in the 16th century; authors frequently copied and paraphrased passages without acknowledgement, especially from the classics. The latest research suggests that he may in fact have used bibliomancy for this—randomly selecting a book of history or prophecy and taking his cue from whatever page it happened to fall open at.

Further material was gleaned from the "De honesta disciplina" of 1504 by Petrus Crinitus, which included extracts from Michael Psellos's "De daemonibus", and the "De Mysteriis Aegyptiorum" ("Concerning the mysteries of Egypt"), a book on Chaldean and Assyrian magic by Iamblichus, a 4th-century Neo-Platonist. Latin versions of both had recently been published in Lyon, and extracts from both are paraphrased (in the second case almost literally) in his first two verses, the first of which is appended to this article. While it is true that Nostradamus claimed in 1555 to have burned all of the occult works in his library, no one can say exactly what books were destroyed in this fire.

Only in the 17th century did people start to notice his reliance on earlier, mainly classical sources.

Nostradamus's reliance on historical precedent is reflected in the fact that he explicitly rejected the label "prophet" (i.e. a person having prophetic powers of his own) on several occasions:

Given this reliance on literary sources, it is unlikely that Nostradamus used any particular methods for entering a trance state, other than contemplation, meditation and incubation. His sole description of this process is contained in "letter 41" of his collected Latin correspondence. The popular legend that he attempted the ancient methods of flame gazing, water gazing or both simultaneously is based on a naive reading of his first two verses, which merely liken his efforts to those of the Delphic and Branchidic oracles. The first of these is reproduced at the bottom of this article and the second can be seen by visiting the relevant facsimile site (see External Links). In his dedication to King Henry II, Nostradamus describes "emptying my soul, mind and heart of all care, worry and unease through mental calm and tranquility", but his frequent references to the "bronze tripod" of the Delphic rite are usually preceded by the words "as though" (compare, once again, External References to the original texts).

Most of the quatrains deal with disasters, such as plagues, earthquakes, wars, floods, invasions, murders, droughts, and battles—all undated and based on foreshadowings by the "Mirabilis Liber". Some quatrains cover these disasters in overall terms; others concern a single person or small group of people. Some cover a single town, others several towns in several countries. A major, underlying theme is an impending invasion of Europe by Muslim forces from farther east and south headed by the expected Antichrist, directly reflecting the then-current Ottoman invasions and the earlier Saracen equivalents, as well as the prior expectations of the "Mirabilis Liber". All of this is presented in the context of the supposedly imminent end of the world—even though this is not in fact mentioned—a conviction that sparked numerous collections of end-time prophecies at the time, including an unpublished collection by Christopher Columbus. Views on Nostradamus have varied widely throughout history. Academic views such as those of Jacques Halbronn regard Nostradamus's "Prophecies" as antedated forgeries written by later hands with a political axe to grind.

Many of Nostradamus's supporters believe his prophecies are genuine. Owing to the subjective nature of these interpretations, however, no two of them completely agree on what Nostradamus predicted, whether for the past or for the future. Many supporters, however, do agree, for example, that he predicted the Great Fire of London, the French Revolution, the rises of Napoleon and Adolf Hitler, both world wars, and the nuclear destruction of Hiroshima and Nagasaki. Popular authors frequently claim that he predicted whatever major event had just happened at the time of each book's publication, such as the Apollo moon landings in 1969, the Space Shuttle "Challenger" disaster in 1986, the death of Diana, Princess of Wales in 1997, and the September 11 attacks on the World Trade Center in 2001. This 'movable feast' aspect appears to be characteristic of the genre.

Possibly the first of these books to become popular in English was Henry C. Roberts' "The Complete Prophecies of Nostradamus" of 1947, reprinted at least seven times during the next forty years, which contained both transcriptions and translations, with brief commentaries. This was followed in 1961 (reprinted in 1982) by Edgar Leoni's "Nostradamus and His Prophecies". After that came Erika Cheetham's "The Prophecies of Nostradamus", incorporating a reprint of the posthumous 1568 edition, which was reprinted, revised and republished several times from 1973 onwards, latterly as "The Final Prophecies of Nostradamus". This served as the basis for the documentary "The Man Who Saw Tomorrow" and both did indeed mention possible generalised future attacks on New York (via nuclear weapons), though not specifically on the World Trade Center or on any particular date.

A two-part translation of Jean-Charles de Fontbrune's "Nostradamus: historien et prophète" was published in 1980, and John Hogue has published a number of books on Nostradamus from about 1987, including "Nostradamus and the Millennium: Predictions of the Future", "Nostradamus: The Complete Prophecies" (1999) and "Nostradamus: A Life and Myth" (2003). In 1992 one commentator who claimed to be able to contact Nostradamus under hypnosis even had him "interpreting" his own verse X.6 (a prediction specifically about floods in southern France around the city of Nîmes and people taking refuge in its "collosse", or Colosseum, a Roman amphitheatre now known as the "Arènes") as a prediction of an undated "attack on the Pentagon", despite the historical seer's clear statement in his dedicatory letter to King Henri II that his prophecies were about Europe, North Africa and part of Asia Minor.

With the exception of Roberts, these books and their many popular imitators were almost unanimous not merely about Nostradamus's powers of prophecy but also in inventing intriguing aspects of his purported biography: that he had been a descendant of the Israelite tribe of Issachar; he had been educated by his grandfathers, who had both been physicians to the court of Good King René of Provence; he had attended Montpellier University in 1525 to gain his first degree; after returning there in 1529, he had successfully taken his medical doctorate; he had gone on to lecture in the Medical Faculty there, until his views became too unpopular; he had supported the heliocentric view of the universe; he had travelled to the Habsburg Netherlands, where he had composed prophecies at the abbey of Orval; in the course of his travels, he had performed a variety of prodigies, including identifying future Pope, Sixtus V, who was then only a seminary monk. He is credited with having successfully cured the Plague at Aix-en-Provence and elsewhere; he had engaged in scrying, using either a magic mirror or a bowl of water; he had been joined by his secretary Chavigny at Easter 1554; having published the first installment of his "Prophéties", he had been summoned by Queen Catherine de' Medici to Paris in 1556 to discuss with her his prophecy at quatrain I.35 that her husband King Henri II would be killed in a duel; he had examined the royal children at Blois; he had bequeathed to his son a "lost book" of his own prophetic paintings; he had been buried standing up; and he had been found, when dug up at the French Revolution, to be wearing a medallion bearing the exact date of his disinterment. This was first recorded by Samuel Pepys as early as 1667, long before the French Revolution. Pepys records in his celebrated diary a legend that, before his death, Nostradamus made the townsfolk swear that his grave would never be disturbed; but that 60 years later his body was exhumed, whereupon a brass plaque was found on his chest correctly stating the date and time when his grave would be opened and cursing the exhumers.

In 2000, Li Hongzhi claimed that the 1999 prophecy at X.72 was a prediction of the Chinese Falun Gong persecution which began in July 1999, leading to an increased interest in Nostradamus among Falun Gong members.

From the 1980s onward, however, an academic reaction set in, especially in France. The publication in 1983 of Nostradamus's private correspondence and, during succeeding years, of the original editions of 1555 and 1557 discovered by Chomarat and Benazra, together with the unearthing of much original archival material revealed that much that was claimed about Nostradamus did not fit the documented facts. The academics revealed that not one of the claims just listed was backed up by any known contemporary documentary evidence. Most of them had evidently been based on unsourced rumours relayed as fact by much later commentators, such as Jaubert (1656), Guynaud (1693) and Bareste (1840), on modern misunderstandings of the 16th-century French texts, or on pure invention. Even the often-advanced suggestion that quatrain I.35 had successfully prophesied King Henry II's death did not actually appear in print for the first time until 1614, 55 years after the event.

Skeptics such as James Randi suggest that his reputation as a prophet is largely manufactured by modern-day supporters who fit his words to events that have either already occurred or are so imminent as to be inevitable, a process sometimes known as "retroactive clairvoyance" (postdiction). No Nostradamus quatrain is known to have been interpreted as predicting a specific event before it occurred, other than in vague, general terms that could equally apply to any number of other events. This even applies to quatrains that contain specific dates, such as III.77, which predicts "in 1727, in October, the king of Persia [shall be] captured by those of Egypt"—a prophecy that has, as ever, been interpreted retrospectively in the light of later events, in this case as though it presaged the known peace treaty between the Ottoman Empire and Persia of that year; Egypt was also an important Ottoman territory at this time. Similarly, Nostradamus's notorious "1999" prophecy at X.72 (see Nostradamus in popular culture) describes no event that commentators have succeeded in identifying either before or since, other than by twisting the words to fit whichever of the many contradictory happenings they claim as "hits". Moreover, no quatrain suggests, as is often claimed by books and films on the alleged Mayan Prophecy, that the world would end in December 2012. In his preface to the "Prophecies", Nostradamus himself stated that his prophecies extend 'from now to the year 3797'—an extraordinary date which, given that the preface was written in 1555, may have more than a little to do with the fact that 2242 (3797–1555) had recently been proposed by his major astrological source Richard Roussat as a possible date for the end of the world.

Additionally, scholars have pointed out that almost all English translations of Nostradamus's quatrains are of extremely poor quality, seem to display little or no knowledge of 16th-century French, are tendentious, and are sometimes intentionally altered in order to make them fit whatever events the translator believed they were supposed to refer (or vice versa). None of them were based on the original editions: Roberts had based his writings on that of 1672, Cheetham and Hogue on the posthumous edition of 1568. Even Leoni accepted on page 115 that he had never seen an original edition, and on earlier pages, he indicated that much of his biographical material was unsourced.

None of this research and criticism was originally known to most of the English-language commentators, by dint of the dates when they were writing and, to some extent, the language in which it was written. Hogue was in a position to take advantage of it, but it was only in 2003 that he accepted that some of his earlier biographical material had in fact been apocryphal. Meanwhile, some of the more recent sources listed (Lemesurier, Gruber, Wilson) have been particularly scathing about later attempts by some lesser-known authors and Internet enthusiasts to extract alleged hidden meanings from the texts, whether with the aid of anagrams, numerical codes, graphs or otherwise.

The prophecies retold and expanded by Nostradamus figured largely in popular culture in the 20th and 21st centuries. As well as being the subject of hundreds of books (both fiction and nonfiction), Nostradamus's life has been depicted in several films and videos, and his life and writings continue to be a subject of media interest.

There have also been several well-known Internet hoaxes, where quatrains in the style of Nostradamus have been circulated by e-mail as the real thing. The best-known examples concern the collapse of the World Trade Center in the 11 September attacks.

With the arrival of the year 2012, Nostradamus's prophecies started to be co-opted (especially by the History Channel) as evidence suggesting that the end of the world was imminent, notwithstanding the fact that his book never mentions the end of the world, let alone the year 2012.





</doc>
<doc id="21619" url="https://en.wikipedia.org/wiki?curid=21619" title="List of multi-level marketing companies">
List of multi-level marketing companies

This is a list of companies which use multi-level marketing (also known as network marketing, direct selling, referral marketing, and pyramid selling) for most of their sales.


</doc>
<doc id="21620" url="https://en.wikipedia.org/wiki?curid=21620" title="Noah Webster">
Noah Webster

Noah Webster Jr. (October 16, 1758 – May 28, 1843) was an American lexicographer, textbook pioneer, English-language spelling reformer, political writer, editor, and prolific author. He has been called the "Father of American Scholarship and Education". His "Blue-backed Speller" books taught five generations of American children how to spell and read. Webster's name has become synonymous with "dictionary" in the United States, especially the modern Merriam-Webster dictionary that was first published in 1828 as "An American Dictionary of the English Language".

Born in West Hartford, Connecticut, Webster was graduated from Yale College in 1778. He passed the bar examination after studying law under Oliver Ellsworth and others, but was unable to find work as a lawyer. He found some financial success by opening a private school and writing a series of educational books, including the "Blue-Backed Speller." A strong supporter of the American Revolution and the ratification of the United States Constitution, Webster later criticized American society for being in need of an intellectual foundation. He believed that American nationalism was superior to Europe because American values were superior.

In 1793, Alexander Hamilton recruited Webster to move to New York City and become an editor for a Federalist Party newspaper. He became a prolific author, publishing newspaper articles, political essays, and textbooks. He returned to Connecticut in 1798 and served in the Connecticut House of Representatives. Webster founded the Connecticut Society for the Abolition of Slavery in 1791 but later became somewhat disillusioned with the abolitionist movement.

In 1806, Webster published his first dictionary, "A Compendious Dictionary of the English Language". The following year, he started working on an expanded and comprehensive dictionary, finally publishing it in 1828. He was very influential in popularizing certain spellings in the United States. He was also influential in establishing the Copyright Act of 1831, the first major statutory revision of U.S. copyright law. While working on a second volume of his dictionary, Webster died in 1843, and the rights to the dictionary were acquired by George and Charles Merriam.

Webster was born in the Western Division of Hartford (which became West Hartford, Connecticut) to an established family. His father Noah Webster Sr. (1722–1813) was a descendant of Connecticut Governor John Webster; his mother Mercy (Steele) Webster (1727–1794) was a descendant of Governor William Bradford of Plymouth Colony. His father was primarily a farmer, though he was also deacon of the local Congregational church, captain of the town's militia, and a founder of a local book society (a precursor to the public library). After American independence, he was appointed a justice of the peace.

Webster's father never attended college, but he was intellectually curious and prized education. Webster's mother spent long hours teaching her children spelling, mathematics, and music. At age six, Webster began attending a dilapidated one-room primary school built by West Hartford's Ecclesiastical Society. Years later, he described the teachers as the "dregs of humanity" and complained that the instruction was mainly in religion. Webster's experiences there motivated him to improve the educational experience of future generations.

At age fourteen, his church pastor began tutoring him in Latin and Greek to prepare him for entering Yale College. Webster enrolled at Yale just before his 16th birthday, studying during his senior year with Ezra Stiles, Yale's president. His four years at Yale overlapped the American Revolutionary War and, because of food shortages and threatened British invasions, many of his classes had to be held in other towns. Webster served in the Connecticut Militia. His father had mortgaged the farm to send Webster to Yale, but he was now on his own and had nothing more to do with his family.

Webster lacked career plans after graduating from Yale in 1779, later writing that a liberal arts education "disqualifies a man for business". He taught school briefly in Glastonbury, but the working conditions were harsh and the pay low. He quit to study law. While studying law under future U.S. Supreme Court Chief Justice Oliver Ellsworth, Webster also taught full-time in Hartford—which was grueling, and ultimately impossible to continue. He quit his legal studies for a year and lapsed into a depression; he then found another practicing attorney to tutor him, and completed his studies and passed the bar examination in 1781. As the Revolutionary War was still going on, he could not find work as a lawyer. He received a master's degree from Yale by giving an oral dissertation to the Yale graduating class. Later that year, he opened a small private school in western Connecticut that was a success. Nevertheless, he soon closed it and left town, probably because of a failed romance. Turning to literary work as a way to overcome his losses and channel his ambitions, he began writing a series of well-received articles for a prominent New England newspaper justifying and praising the American Revolution and arguing that the separation from Britain would be a permanent state of affairs. He then founded a private school catering to wealthy parents in Goshen, New York and, by 1785, he had written his speller, a grammar book and a reader for elementary schools. Proceeds from continuing sales of the popular blue-backed speller enabled Webster to spend many years working on his famous dictionary.

Webster was by nature a revolutionary, seeking American independence from the cultural thralldom to Europe. To replace it, he sought to create a utopian America, cleansed of luxury and ostentation and the champion of freedom. By 1781, Webster had an expansive view of the new nation. American nationalism was superior to Europe because American values were superior, he claimed.

Webster dedicated his "Speller" and "Dictionary" to providing an intellectual foundation for American nationalism. From 1787 to 1789, Webster was an outspoken supporter of the new Constitution. In October 1787, he wrote a pamphlet entitled "An Examination into the Leading Principles of the Federal Constitution Proposed by the Late Convention Held at Philadelphia," published under the pen name "A Citizen of America." The pamphlet was influential, particularly outside New York State.

In terms of political theory, he de-emphasized virtue (a core value of republicanism) and emphasized widespread ownership of property (a key element of Federalism). He was one of the few Americans who paid much attention to French theorist Jean-Jacques Rousseau. It was not Rousseau's politics but his ideas on pedagogy in "Emile" (1762) that influenced Webster in adjusting his "Speller" to the stages of a child's development.

Webster married well and had joined the elite in Hartford but did not have much money. In 1793, Alexander Hamilton lent him $1,500 to move to New York City to edit the leading Federalist Party newspaper. In December, he founded New York's first daily newspaper "American Minerva" (later known as the "Commercial Advertiser"), which he edited for four years, writing the equivalent of 20 volumes of articles and editorials. He also published the semi-weekly publication "The Herald, A Gazette for the country" (later known as "The New York Spectator").

As a Federalist spokesman, he defended the administrations of George Washington and John Adams, especially their policy of neutrality between Britain and France, and he especially criticized the excesses of the French Revolution and its Reign of Terror. When French ambassador Citizen Genêt set up a network of pro-Jacobin "Democratic-Republican Societies" that entered American politics and attacked President Washington, he condemned them. He later defended Jay's Treaty between the United States and Britain. As a result, he was repeatedly denounced by the Jeffersonian Republicans as "a pusillanimous, half-begotten, self-dubbed patriot," "an incurable lunatic," and "a deceitful newsmonger ... Pedagogue and Quack." 

Webster was elected a Fellow of the American Academy of Arts and Sciences in 1799.

For decades, he was one of the most prolific authors in the new nation, publishing textbooks, political essays, a report on infectious diseases, and newspaper articles for his Federalist party. He wrote so much that a modern bibliography of his published works required 655 pages. He moved back to New Haven in 1798; he was elected as a Federalist to the Connecticut House of Representatives in 1800 and 1802–1807.
The Copyright Act of 1831 was the first major statutory revision of U.S. copyright law, a result of intensive lobbying by Noah Webster and his agents in Congress. Webster also played a critical role lobbying individual states throughout the country during the 1780s to pass the first American copyright laws, which were expected to have distinct nationalistic implications for the infant nation.

As a teacher, he had come to dislike American elementary schools. They could be overcrowded, with up to seventy children of all ages crammed into one-room schoolhouses. They had poor, underpaid staff, no desks, and unsatisfactory textbooks that came from England. Webster thought that Americans should learn from American books, so he began writing the three volume compendium "A Grammatical Institute of the English Language". The work consisted of a speller (published in 1783), a grammar (published in 1784), and a reader (published in 1785). His goal was to provide a uniquely American approach to training children. His most important improvement, he claimed, was to rescue "our native tongue" from "the clamour of pedantry" that surrounded English grammar and pronunciation. He complained that the English language had been corrupted by the British aristocracy, which set its own standard for proper spelling and pronunciation. Webster rejected the notion that the study of Greek and Latin must precede the study of English grammar. The appropriate standard for the American language, argued Webster, was "the same republican principles as American civil and ecclesiastical constitutions." This meant that the people-at-large must control the language; popular sovereignty in government must be accompanied by popular usage in language.

The "Speller" was arranged so that it could be easily taught to students, and it progressed by age. From his own experiences as a teacher, Webster thought that the "Speller" should be simple and gave an orderly presentation of words and the rules of spelling and pronunciation. He believed that students learned most readily when he broke a complex problem into its component parts and had each pupil master one part before moving to the next. Ellis argues that Webster anticipated some of the insights currently associated with Jean Piaget's theory of cognitive development. Webster said that children pass through distinctive learning phases in which they master increasingly complex or abstract tasks. Therefore, teachers must not try to teach a three-year-old how to read; they could not do it until age five. He organized his speller accordingly, beginning with the alphabet and moving systematically through the different sounds of vowels and consonants, then syllables, then simple words, then more complex words, then sentences.

The speller was originally titled "The First Part of the Grammatical Institute of the English Language". Over the course of 385 editions in his lifetime, the title was changed in 1786 to "The American Spelling Book", and again in 1829 to "The Elementary Spelling Book". Most people called it the "Blue-Backed Speller" because of its blue cover and, for the next one hundred years, Webster's book taught children how to read, spell, and pronounce words. It was the most popular American book of its time; by 1837, it had sold 15 million copies, and some 60 million by 1890—reaching the majority of young students in the nation's first century. Its royalty of a half-cent per copy was enough to sustain Webster in his other endeavors. It also helped create the popular contests known as spelling bees.

As time went on, Webster changed the spellings in the book to more phonetic ones. Most of them already existed as alternative spellings. He chose spellings such as "defense", "color", and "traveler", and changed the "re" to "er" in words such as "center". He also changed "tongue" to the older spelling "tung", but this did not catch on.

Part three of his "Grammatical Institute" (1785) was a reader designed to uplift the mind and "diffuse the principles of virtue and patriotism."

"In the choice of pieces," he explained, "I have not been inattentive to the political interests of America. Several of those masterly addresses of Congress, written at the commencement of the late Revolution, contain such noble, just, and independent sentiments of liberty and patriotism, that I cannot help wishing to transfuse them into the breasts of the rising generation."

Students received the usual quota of Plutarch, Shakespeare, Swift, and Addison, as well as such Americans as Joel Barlow's "Vision of Columbus", Timothy Dwight's "Conquest of Canaan", and John Trumbull's poem "M'Fingal." He included excerpts from Tom Paine's "The Crisis" and an essay by Thomas Day calling for the abolition of slavery in accord with the Declaration of Independence.

Webster's "Speller" was entirely secular by design. It ended with two pages of important dates in American history, beginning with Columbus's discovery of America in 1492 and ending with the battle of Yorktown in 1781. There was no mention of God, the Bible, or sacred events. "Let sacred things be appropriated for sacred purposes," wrote Webster. As Ellis explains, "Webster began to construct a secular catechism to the nation-state. Here was the first appearance of 'civics' in American schoolbooks. In this sense, Webster's speller becoming what was to be the secular successor to "The New England Primer" with its explicitly biblical injunctions." Later in life, Webster became intensely religious and added religious themes. However, after 1840, Webster's books lost market share to the "McGuffey Eclectic Readers" of William Holmes McGuffey, which sold over 120 million copies.

Vincent P. Bynack (1984) examines Webster in relation to his commitment to the idea of a unified American national culture that would stave off the decline of republican virtues and solidarity. Webster acquired his perspective on language from such theorists as Maupertuis, Michaelis, and Herder. There he found the belief that a nation's linguistic forms and the thoughts correlated with them shaped individuals' behavior. Thus, the etymological clarification and reform of American English promised to improve citizens' manners and thereby preserve republican purity and social stability. This presupposition animated Webster's "Speller" and "Grammar".

In 1806, Webster published his first dictionary, . In 1807 Webster began compiling an expanded and fully comprehensive dictionary, "An American Dictionary of the English Language;" it took twenty-six years to complete. To evaluate the etymology of words, Webster learned twenty-eight languages, including Old English, Gothic, German, Greek, Latin, Italian, Spanish, French, Dutch, Welsh, Russian, Hebrew, Aramaic, Persian, Arabic, and Sanskrit. Webster hoped to standardize American speech, since Americans in different parts of the country used different languages. They also spelled, pronounced, and used English words differently.

Webster completed his dictionary during his year abroad in January 1825 in a boarding house in Cambridge, England. His book contained seventy thousand words, of which twelve thousand had never appeared in a published dictionary before. As a spelling reformer, Webster preferred spellings that matched pronunciation better. In "A Companion to the American Revolution" (2008), John Algeo notes: "It is often assumed that characteristically American spellings were invented by Noah Webster. He was very influential in popularizing certain spellings in America, but he did not originate them. Rather ... he chose already existing options such as "center, color" and "check" on such grounds as simplicity, analogy or etymology." He also added American words, like "skunk", that did not appear in British dictionaries. At the age of seventy, Webster published his dictionary in 1828, registering the copyright on April 14.

Though it now has an honored place in the history of American English, Webster's first dictionary only sold 2,500 copies. He was forced to mortgage his home to develop a second edition, and his life from then on was plagued with debt.

In 1840, the second edition was published in two volumes. On May 28, 1843, a few days after he had completed revising an appendix to the second edition, and with much of his efforts with the dictionary still unrecognized, Noah Webster died. The rights to his dictionary were acquired by George and Charles Merriam in 1843 from Webster's estate and all contemporary Merriam-Webster dictionaries trace their lineage to that of Webster, although many others have adopted his name, attempting to share in the popularity.

Lepore (2008) demonstrates Webster's paradoxical ideas about language and politics and shows why Webster's endeavors were at first so poorly received. Culturally conservative Federalists denounced the work as radical—too inclusive in its lexicon and even bordering on vulgar. Meanwhile, Webster's old foes the Republicans attacked the man, labeling him mad for such an undertaking.

Scholars have long seen Webster's 1844 dictionary to be an important resource for reading poet Emily Dickinson's life and work; she once commented that the "Lexicon" was her "only companion" for years. One biographer said, "The dictionary was no mere reference book to her; she read it as a priest his breviary—over and over, page by page, with utter absorption."

Nathan Austin has explored the intersection of lexicographical and poetic practices in American literature, and attempts to map out a "lexical poetics" using Webster's definitions as his base. Poets mined his dictionaries, often drawing upon the lexicography in order to express word play. Austin explicates key definitions from both the "Compendious" (1806) and "American" (1828) dictionaries, and finds a range of themes such as the politics of "American" versus "British" English and issues of national identity and independent culture. Austin argues that Webster's dictionaries helped redefine Americanism in an era of highly flexible cultural identity. Webster himself saw the dictionaries as a nationalizing device to separate America from Britain, calling his project a "federal language", with competing forces towards regularity on the one hand and innovation on the other. Austin suggests that the contradictions of Webster's lexicography were part of a larger play between liberty and order within American intellectual discourse, with some pulled toward Europe and the past, and others pulled toward America and the new future.

In 1850 Blackie and Son in Glasgow published the first general dictionary of English that relied heavily upon pictorial illustrations integrated with the text. Its "The Imperial Dictionary, English, Technological, and Scientific, Adapted to the Present State of Literature, Science, and Art; On the Basis of Webster's English Dictionary" used Webster's for most of their text, adding some additional technical words that went with illustrations of machinery.

Webster in early life was something of a freethinker, but in 1808 he became a convert to Calvinistic orthodoxy, and thereafter became a devout Congregationalist who preached the need to Christianize the nation. Webster grew increasingly authoritarian and elitist, fighting against the prevailing grain of Jacksonian Democracy. Webster viewed language as a tool to control unruly thoughts. His "American Dictionary" emphasized the virtues of social control over human passions and individualism, submission to authority, and fear of God; they were necessary for the maintenance of the American social order. As he grew older, Webster's attitudes changed from those of an optimistic revolutionary in the 1780s to those of a pessimistic critic of man and society by the 1820s.

His 1828 "American Dictionary" contained the greatest number of Biblical definitions given in any reference volume. Webster said of education, "Education is useless without the Bible. The Bible was America's basic text book in all fields. God's Word, contained in the Bible, has furnished all necessary rules to direct our conduct." Webster released his own edition of the Bible in 1833, called the Common Version. He used the King James Version (KJV) as a base and consulted the Hebrew and Greek along with various other versions and commentaries. Webster molded the KJV to correct grammar, replaced words that were no longer used, and did away with words and phrases that could be seen as offensive.

In 1834, he published "Value of the Bible and Excellence of the Christian Religion", an apologetic book in defense of the Bible and Christianity itself.

Webster helped found the Connecticut Society for the Abolition of Slavery in 1791, but by the 1830s rejected the new tone among abolitionists that emphasized that Americans who tolerated slavery were themselves sinners. In 1837, Webster warned his daughter Eliza about her fervent support of the abolitionist cause. Webster wrote, "slavery is a great sin and a general calamity—but it is not "our" sin, though it may prove to be a terrible calamity to us in the north. But we cannot legally interfere with the South on this subject." He added, "To come north to preach and thus disturb "our" peace, when we can legally do nothing to effect this object, is, in my view, highly criminal and the preachers of abolitionism deserve the penitentiary."

Noah Webster married Rebecca Greenleaf (1766–1847) on October 26, 1789, New Haven, Connecticut. They had eight children:

He moved to Amherst, Massachusetts in 1812, where he helped to found Amherst College. In 1822 the family moved back to New Haven, where Webster was awarded an honorary degree from Yale the following year. He is buried in New Haven's Grove Street Cemetery.








</doc>
<doc id="21626" url="https://en.wikipedia.org/wiki?curid=21626" title="Near-Earth object">
Near-Earth object

A near-Earth object (NEO) is any small Solar System body whose orbit brings it to proximity with Earth. By convention, a Solar System body is a NEO if its closest approach to the Sun (perihelion) is less than 1.3 astronomical units (AU). If a NEO's orbit crosses the Earth's, and the object is larger than across, it is considered a potentially hazardous object (PHO). Most known PHOs and NEOs are asteroids, but a small fraction are comets.

There are over 20,000 known near-Earth asteroids (NEAs), over a hundred short-period near-Earth comets (NECs), and a number of solar-orbiting meteoroids large enough to be tracked in space before striking the Earth. It is now widely accepted that collisions in the past have had a significant role in shaping the geological and biological history of the Earth. NEOs have become of increased interest since the 1980s because of greater awareness of this potential danger. Asteroids as small as 20 m can damage the local environment and populations. Larger asteroids penetrate the atmosphere to the surface of the Earth, producing craters if they impact a continent or tsunamis if they impact sea. Asteroid impact avoidance by deflection is possible in principle, and methods of mitigation are being researched.

Two scales, the Torino scale and the more complex Palermo scale, rate a risk based on how probable the orbit calculations of an identified NEO make an Earth impact and on how bad the consequences of such an impact would be. Some NEOs have had temporarily positive Torino or Palermo scale ratings after their discovery, but , more precise calculations based on longer observation arcs led in all cases to a reduction of the rating to or below 0.

Since 1998, the United States, the European Union, and other nations are scanning the sky for NEOs in an effort called Spaceguard. The initial US Congress mandate to NASA was to catalog at least 90% of NEOs that are at least in diameter, which could cause a global catastrophe, and had been met by 2011. In later years, the survey effort has been expanded to smaller objects which have the potential for large-scale, though not global, damage.

NEOs have low surface gravity, and many have Earth-like orbits that make them easy targets for spacecrafts. , five near-Earth comets and five near-Earth asteroids have been visited by spacecraft. A small sample of one NEO was returned to Earth in 2010, and similar missions are in progress. Preliminary plans for commercial asteroid mining have been drafted by private startup companies.

Near-Earth objects (NEOs) are technically and by convention defined as all small Solar System bodies with orbits around the Sun that lie partly between 0.983 and 1.3 astronomical units (AU; Sun–Earth distance) away from the Sun. Thus, NEOs are not necessarily currently near the Earth, but they can potentially approach the Earth relatively closely. The term is also used more flexibly sometimes, for example for objects in orbit around the Earth or for quasi-satellites, which have a more complex orbital relationship with the Earth.

When a NEO is detected, like all other small Solar System bodies, its positions and brightness are submitted to the International Astronomical Union's (IAU's) Minor Planet Center (MPC) for cataloging. The MPC maintains separate lists of confirmed NEOs and potential NEOs. The orbits of some NEOs intersect that of the Earth, so they pose a collision danger. These are considered potentially hazardous objects (PHOs) if their estimated diameter is above 140 meters. The MPC maintains a separate list for the asteroids among PHOs, the potentially hazardous asteroids (PHAs). NEOs are also catalogued by two separate units of the Jet Propulsion Laboratory (JPL) of the National Aeronautics and Space Administration (NASA): the Center for Near Earth Object Studies (CNEOS) and the Solar System Dynamics Group.

PHAs are currently defined based on parameters relating to their potential to approach the Earth dangerously closely and the estimated consequences that an impact would have. Mostly objects with an Earth minimum orbit intersection distance (MOID) of 0.05 AU or less and an absolute magnitude of 22.0 or brighter (a rough indicator of large size) are considered PHAs. Objects that either cannot approach closer to the Earth (i.e. MOID) than , or are fainter than H = 22.0 (about in diameter with assumed albedo of 14%), are not considered PHAs. NASA's catalog of near-Earth objects also includes the approach distances of asteroids and comets (expressed in lunar distances).

The first near-Earth objects to be observed by humans were comets. Their extraterrestrial nature was recognised and confirmed only after Tycho Brahe tried to measure the distance of a comet through its parallax in 1577 and the lower limit he obtained was well above the Earth diameter; the periodicity of some comets was first recognised in 1705, when Edmond Halley published his orbit calculations for the returning object now known as Halley's Comet. The 1758–1759 return of Halley's Comet was the first comet appearance predicted in advance. It has been said that Lexell's comet of 1770 was the first discovered Near-Earth object.

The first near-Earth asteroid to be discovered was 433 Eros in 1898. The asteroid was subject to several extensive observation campaigns, primarily because measurements of its orbit enabled a precise determination of the then imperfectly known distance of the Earth from the Sun.

In 1937, asteroid 69230 Hermes was discovered when it passed the Earth at twice the distance of the Moon. Hermes was considered a threat because it was lost after its discovery; thus its orbit and potential for collision with Earth were not known precisely. Hermes was only re-discovered in 2003, and it is now known to be no threat for at least the next century.

On June 14, 1968, the 1.4 km diameter asteroid 1566 Icarus passed Earth at a distance of , or 16 times the distance of the Moon. During this approach, Icarus became the first minor planet to be observed using radar, with measurements obtained at the Haystack Observatory and the Goldstone Tracking Station. This was the first close approach predicted years in advance (Icarus had been discovered in 1949), and also earned significant public attention, due to alarmist news reports. A year before the approach, MIT students launched Project Icarus, devising a plan to deflect the asteroid with rockets in case it was found to be on a collision course with Earth. Project Icarus received wide media coverage, and inspired the 1979 disaster movie "Meteor", in which the US and the USSR join forces to blow up an Earth-bound fragment of an asteroid hit by a comet.

On March 23, 1989, the diameter Apollo asteroid 4581 Asclepius (1989 FC) missed the Earth by . If the asteroid had impacted it would have created the largest explosion in recorded history, equivalent to 20,000 megatons of TNT. It attracted widespread attention because it was discovered only after the closest approach.

In March 1998, early orbit calculations for recently discovered asteroid showed a potential 2028 close approach from the Earth, well within the orbit of the Moon, but with a large error margin allowing for a direct hit. Further data allowed a revision of the 2028 approach distance to , with no chance of collision. By that time, inaccurate reports of a potential impact had caused a media storm.

From the late 1990s, a typical frame of reference in searches for NEOs has been the scientific concept of risk. The risk that any near-Earth object poses is viewed having regard to both the culture and the technology of human society. Through history, humans have associated NEOs with changing risks, based on religious, philosophical or scientific views, as well as humanity's technological or economical capability to deal with such risks. Thus, NEOs have been seen as omens of natural disasters or wars; harmless spectacles in an unchanging universe; the source of era-changing cataclysms or potentially poisonous fumes (during Earth's passage through the tail of Halley's Comet in 1910); and finally as a possible cause of a crater-forming impact that could even cause extinction of humans and other life on Earth.

The potential of catastrophic impacts by near-Earth comets was recognised as soon as the first orbit calculations provided an understanding of their orbits: in 1694, Edmond Halley presented a theory that Noah's flood in the Bible was caused by a comet impact. Human perception of near-Earth asteroids as benign objects of fascination or killer objects with high risk to human society has ebbed and flowed during the short time that NEAs have been scientifically observed. Scientists have recognised the threat of impacts that create craters much bigger than the impacting bodies and have indirect effects on an even wider area since the 1980s, after the confirmation of a theory that the Cretaceous–Paleogene extinction event (in which dinosaurs died out) 65 million years ago was caused by a large asteroid impact.

The awareness of the wider public of the impact risk rose after the observation of the impact of the fragments of Comet Shoemaker–Levy 9 into Jupiter in July 1994. In 1998, the movies "Deep Impact" and "Armageddon" popularised the notion that near-Earth objects could cause catastrophic impacts. Also at that time, a conspiracy theory arose about the supposed 2003 impact of the fictitious planet Nibiru, which persisted on the internet as the predicted impact date was moved to 2012 and then 2017.

There are two schemes for the scientific classification of impact hazards from NEOs:

On both scales, risks of any concern are indicated by values above zero.

The annual background frequency used in the Palermo scale for impacts of energy greater than "E" megatonnes is estimated as:

For instance, this formula implies that the expected value of the time from now until the next impact greater than 1 megatonne is 33 years, and that when it occurs, there is a 50% chance that it will be above 2.4 megatonnes. This formula is only valid over a certain range of "E".

However, another paper published in 2002 – the same year as the paper on that the Palermo scale is based – found a power law with different constants:

This formula gives considerably lower rates for a given "E". For instance, it gives the rate for bolides of 10 megatonnes or more (like the Tunguska explosion) as 1 per thousand years, rather than 1 per 210 years as in the Palermo formula. However, the authors give a rather large uncertainty (once in 400 to 1800 years for 10 megatonnes), due in part to uncertainties in determining the energies of the atmospheric impacts that they used in their determination.

NASA maintains an automated system to evaluate the threat from known NEOs over the next 100 years, which generates the continuously updated Sentry Risk Table. All or nearly all of the objects are highly likely to drop off the list eventually as more observations come in, reducing the uncertainties and enabling more accurate orbital predictions.

In March 2002, became the first asteroid with a positive rating on the Torino Scale, with about a 1 in 9,300 chance of an impact in 2049. Additional observations reduced the estimated risk to zero, and the asteroid was removed from the Sentry Risk Table in April 2002. It is now known that in the next two centuries, will pass the Earth at a safe closest distance (perigee) of on August 31, 2080.
Asteroid was lost after its 1950 discovery, since its observations over just 17 days were insufficient to determine its orbit; it was rediscovered on December 31, 2000. It has a diameter of about a kilometer (0.6 miles). It was also observed by radar during its close approach in 2001, allowing much more precise orbit calculations. Although this asteroid will not strike for at least 800 years and thus has no Torino scale rating, it was added to the Sentry list in April 2002 because it was the first object with a Palermo scale value greater than zero. The then-calculated 1 in 300 maximum chance of impact and +0.17 Palermo scale value was roughly 50% greater than the background risk of impact by all similarly large objects until 2880. Uncertainties in the orbit calculations were further reduced using radar observations in 2012, and this decreased the odds of an impact. Taking all radar and optical observations until 2015 into account, the probability of impact is, , assessed at 1 in 8,300. The corresponding Palermo scale value of −1.42 is still the highest for all objects on the Sentry List Table. , only one other object () has a Palermo scale value above −2 for a single impact date.

On December 24, 2004, asteroid 99942 Apophis (at the time known by its provisional designation ) was assigned a 4 on the Torino scale, the highest rating ever given, as the information available at the time translated to a 2.7% chance of Earth impact on Friday, April 13, 2029. By December 28, 2004, additional observations had produced a smaller uncertainty zone for the 2029 approach which no longer included the Earth. The 2029 risk of impact consequently dropped to zero, but later potential impact dates were still rated 1 on the Torino scale. Further observations lowered this 2036 risk to a Torino rating of 0 in August 2006. , calculations show Apophis has no chance of impacting Earth before 2060.

In February 2006, has been assigned a Torino Scale rating of 2 due to a close encounter predicted for May 4, 2102. After more precise calculations, the rating was lowered to 1 in May 2006 and 0 in October 2006, and the asteroid was removed from the Sentry Risk Table entirely in February 2008.

, is listed with the highest chance of impacting Earth, at 1 in 20 on September 5, 2095. At only across, the asteroid however is much too small to be considered a Potentially Hazardous Asteroid and it poses no serious threat: the possible 2095 impact therefore rates only −3.32 on the Palermo Scale. Observations during the August 2022 close approach are expected to ascertain whether the asteroid will impact Earth in 2095.

The first astronomical program dedicated to the discovery of near-Earth asteroids was the Palomar Planet-Crossing Asteroid Survey, started in 1973 by astronomers Eugene Shoemaker and Eleanor Helin. The link to impact hazard, the need for dedicated survey telescopes and options to head off an eventual impact were first discussed at a 1981 interdisciplinary conference in Snowmass, Colorado. Plans for a more comprehensive survey, named the Spaceguard Survey, were developed by NASA from 1992, under a mandate from the United States Congress. To promote the survey on an international level, the International Astronomical Union (IAU) organised a workshop at Vulcano, Italy in 1995, and set up the Spaceguard Foundation also in Italy a year later. In 1998, the United States Congress gave NASA a mandate to detect 90% of near-earth asteroids over diameter (that threaten global devastation) by 2008.

Several surveys have undertaken "Spaceguard" activities (an umbrella term), including Lincoln Near-Earth Asteroid Research (LINEAR), Spacewatch, Near-Earth Asteroid Tracking (NEAT), Lowell Observatory Near-Earth-Object Search (LONEOS), Catalina Sky Survey (CSS), Campo Imperatore Near-Earth Object Survey (CINEOS), Japanese Spaceguard Association, Asiago-DLR Asteroid Survey (ADAS) and Near-Earth Object WISE (NEOWISE). As a result, the ratio of the known and the estimated total number of near-Earth asteroids larger than 1 km in diameter rose from about 20% in 1998 to 65% in 2004, 80% in 2006, and 93% in 2011. The original Spaceguard goal has thus been met, only three years late. , 893 NEAs larger than 1 km have been discovered, or 97% of an estimated total of about 920.

In 2005, the original USA Spaceguard mandate was extended by the George E. Brown, Jr. Near-Earth Object Survey Act, which calls for NASA to detect 90% of NEOs with diameters of or greater, by 2020. As of January, 2020, it is estimated that less than half of these have been found, but objects of this size hit the earth only about once in 2000 years. In January 2016, NASA announced the creation of the Planetary Defense Coordination Office (PDCO) to track NEOs larger than about in diameter and coordinate an effective threat response and mitigation effort.

Survey programs aim to identify threats years in advance, giving humanity time to prepare a space mission to avert the threat.

The ATLAS project, by contrast, aims to find impacting asteroids shortly before impact, much too late for deflection maneuvers but still in time to evacuate and otherwise prepare the affected Earth region. Another project, the Zwicky Transient Facility (ZTF), which surveys for objects that change their brightness rapidly, also detects asteroids passing close to Earth.

Scientists involved in NEO research have also considered options for actively averting the threat if an object is found to be on a collision course with Earth. All viable methods aim to deflect rather than destroy the threatening NEO, because the fragments would still cause widespread destruction. Deflection, which means a change in the object's orbit months to years prior to the predicted impact, also requires orders of magnitude less energy.

Near-Earth objects are classified as meteoroids, asteroids, or comets depending on size, composition, and orbit. Those which are asteroids can additionally be members of an asteroid family, and comets create meteoroid streams that can generate meteor showers.

, 893 NEAs appear on the Sentry impact risk page at the NASA website. A significant number of these NEAs are at most 50 meters in diameter and none of the listed objects are placed even in the "green zone" (Torino Scale 1), meaning that none warrant the attention of general public.

The main problem with estimating the number of NEOs is that probablility of detecting one is influenced by a number of its characteristics, starting naturally with its size but also including the characteristics of its orbit. What is easily detected will be more counted,
and these observational biases need to be compensated when trying to calculate the number of bodies in a population from the list of its detected members.

Bigger asteroids reflect more light, and the two biggest Near-Earth objects, 433 Eros and 1036 Ganymed, were naturally also among the first to be detected. 1036 Ganymed is about in diameter and 433 Eros is about in diameter .

The other major detection bias is that it is much easier to spot objects on the night-side of Earth. There is much less noise from the bright sky, and the searcher is looking at the sunlit side of the asteroids. In the daytime sky, a searcher looking towards the sun sees the backside of the object (e.g. comparing a Full moon at night to a New Moon in daytime). The light of sun hitting asteroids has been called "full asteroid" similar to a "full moon" and the greater amount of light, creates a bias that they are easier to detect in this case. In addition, opposition surge make them even brighter when the Earth is along the axis of sunlight. Finally, the day sky near the Sun is much brighter than the night sky. 
Evidencing this bias, over half (53%) of the known Near Earth objects were discovered in just 3.8% of the sky, in a 22.5° cone facing directly away from the Sun, and the vast majority (87%) were first found in only 15% of the sky, in the 45° cone facing away from the Sun, as depicted in the diagram below. One way around this opposition bias is to use thermal infrared telescopes that observe their heat emissions instead of the light they reflect.

Asteroids with orbits which make them spend more time on the day-side of the Earth are therefore less likely to be discovered than those that spend most of their time beyond the orbit of the Earth. For example, one study noted that detection of bodies in low-eccentricity Earth-crossing orbits is favored, making Atens more likely to be detected than Apollos.

Such observational biases must be identified and quantified to determine NEO populations, as studies of asteroid populations then take those known observational selection biases into account to make a more accurate assessment. In the year 2000 and taking into account all known observational biases, it was estimated that there are approximately 900 near earth "asteroids" of at least kilometer size, or technically and more accurately, with an absolute magnitude brighter than 17.75.

These are asteroids in a near-Earth orbit without the tail or coma of a comet. , 22,261 near-Earth asteroids are known, 1,955 of which are both sufficiently large and come sufficiently close to Earth to be considered potentially hazardous.

NEAs survive in their orbits for just a few million years. They are eventually eliminated by planetary perturbations, causing ejection from the Solar System or a collision with the Sun or a planet. With orbital lifetimes short compared to the age of the Solar System, new asteroids must be constantly moved into near-Earth orbits to explain the observed asteroids. The accepted origin of these asteroids is that main-belt asteroids are moved into the inner Solar System through orbital resonances with Jupiter. The interaction with Jupiter through the resonance perturbs the asteroid's orbit and it comes into the inner Solar System. The asteroid belt has gaps, known as Kirkwood gaps, where these resonances occur as the asteroids in these resonances have been moved onto other orbits. New asteroids migrate into these resonances, due to the Yarkovsky effect that provides a continuing supply of near-Earth asteroids. Compared to the entire mass of the asteroid belt, the mass loss necessary to sustain the NEA population is relatively small; totalling less than 6% over the past 3.5 billion years. The composition of near-Earth asteroids is comparable to that of asteroids from the asteroid belt, reflecting a variety of asteroid spectral types.

A small number of NEAs are extinct comets that have lost their volatile surface materials, although having a faint or intermittent comet-like tail does not necessarily result in a classification as a near-Earth comet, making the boundaries somewhat fuzzy. The rest of the near-Earth asteroids are driven out of the asteroid belt by gravitational interactions with Jupiter.

Many asteroids have natural satellites (minor-planet moons). , 74 NEAs were known to have at least one moon, including three known to have two moons. The asteroid 3122 Florence, one of the largest PHAs with a diameter of , has two moons measuring across, which were discovered by radar imaging during the asteroid's 2017 approach to Earth.

While the size of a small fraction of these asteroids is known to better than 1%, from radar observations, from images of the asteroid surface, or from stellar occultations, the diameter of the vast majority of near Earth asteroids has only been estimated on the basis of their brightness and a representative asteroid surface reflectivity or albedo, which is commonly assumed to be 14%. Such indirect size estimates are uncertain by over a factor of 2 for individual asteroids, since asteroid albedos can range at least as low as 0.05 and as high as 0.3. This makes the volume of those asteroids uncertain by a factor of 8, and their mass by at least as much, since their assumed density also has its own uncertainty. Using this crude method, an absolute magnitude of 17.75 roughly corresponds to a diameter of and an absolute magnitude of 22.0 corresponds to a diameter of . Diameters of intermediate precision, better than from an assumed albedo but not nearly as precise as direct measurements, can be obtained from the combination of reflected light and thermal infrared emission, using a thermal model of the asteroid. In May 2016, the precision of such asteroid diameter estimates arising from the Wide-field Infrared Survey Explorer and NEOWISE missions was questioned by technologist Nathan Myhrvold, His early original criticism did not pass peer review and faced criticism for its methodology itself, but a revised version was subsequently published.

In 2000, NASA reduced its estimate of the number of existing near-Earth asteroids over one kilometer in diameter from 1,000–2,000 to 500–1,000. Shortly thereafter, the LINEAR survey provided an alternative estimate of . In 2011, on the basis of NEOWISE observations, the estimated number of one-kilometer NEAs was narrowed to (of which 93% had been discovered at the time), while the number of NEAs larger than 140 meters across was estimated at . The NEOWISE estimate differed from other estimates primarily in assuming a slightly lower average asteroid albedo, which produces larger estimated diameters for the same asteroid brightness. This resulted in 911 then known asteroids at least 1 km across, as opposed to the 830 then listed by CNEOS which assumed a slightly higher albedo. In 2017, two studies using an improved statistical method reduced the estimated number of NEAs brighter than absolute magnitude 17.75 (approximately over one kilometer in diameter) slightly to . The estimated number of asteroids brighter than absolute magnitude of 22.0 (approximately over 140 m across) rose to , double the WISE estimate, of which about a third are known as of 2018.

As of January 4, 2019 and using diameters mostly estimated crudely from a measured absolute magnitude and an assumed albedo, 897 NEAs listed by CNEOS, including 156 PHAs, measure at least 1 km in diameter, and 8,452 known NEAs are larger than 140 m in diameter.
The smallest known near-Earth asteroid is with an absolute magnitude of 33.2, corresponding to an estimated diameter of about . The largest such object is 1036 Ganymed, with an absolute magnitude of 9.45 and a directly measured equivalent diameter of about .

The number of asteroids brighter than , which corresponds to about in diameter, is estimated at about —of which about 1.3 percent had been discovered by February 2016; the number of asteroids brighter than (larger than ) is estimated at about million—of which about 0.003 percent had been discovered by February 2016.

Near-Earth asteroids are divided into groups based on their semi-major axis (a), perihelion distance (q), and aphelion distance (Q):

Atiras and Amors do not cross the Earth's orbit and are not immediate impact threats, but their orbits may change to become Earth-crossing orbits in the future.

, 36 Atiras, 1,510 Atens, 10,199 Apollos and 8,583 Amors have been discovered and cataloged.

NEAs on a co-orbital configuration have the same orbital period as the Earth. All co-orbital asteroids have special orbits that are relatively stable and, paradoxically, can prevent them from getting close to Earth:

In 1961, the IAU defined meteoroids as a class of solid interplanetary objects distinct from asteroids by their considerably smaller size. This definition was useful at the time because, with the exception of the Tunguska event, all historically observed meteors were produced by objects significantly smaller than the smallest asteroids observable by telescopes. As the distinction began to blur with the discovery of ever smaller asteroids and a greater variety of observed NEO impacts, revised definitions with size limits have been proposed from the 1990s. In April 2017, the IAU adopted a revised definition that generally limits meteoroids to a size between 30 µm and 1 m in diameter, but permits the use of the term for any object of any size that caused a meteor, thus leaving the distinction between asteroid and meteoroid blurred.

Near-Earth comets (NECs) are objects in a near-Earth orbit with a tail or coma. Comet nuclei are typically less dense than asteroids but they pass Earth at higher relative speeds, thus the impact energy of comet nucleus is slightly larger than that of a similar-sized asteroid. NECs may pose an additional hazard due to fragmentation: the meteoroid streams which produce meteor showers may include large inactive fragments, effectively NEAs. Although no impact of a comet in Earth's history has been conclusively confirmed, the Tunguska event may have been caused by a fragment of Comet Encke.

Comets are commonly divided between short-period and long-period comets. Short-period comets, with an orbital period of less than 200 years, originate in the Kuiper belt, beyond the orbit of Neptune; while long-period comets originate in the Oort Cloud, in the outer reaches of the Solar System. The orbital period distinction is of importance in the evaluation of the risk from near-Earth comets because short-period NECs are likely to have been observed during multiple apparitions and thus their orbits can be determined with some precision, while long-period NECs can be assumed to have been seen for the first and last time when they appeared during the Age of Science, thus their approaches cannot be predicted well in advance. Since the threat from long-period NECs is estimated to be at most 1% of the threat from NEAs, and long-period comets are very faint and thus difficult to detect at large distances from the Sun, Spaceguard efforts consistently focused on asteroids and short-period comets. CNEOS even restricts its definition of NECs to short-period comets—, 107 such objects have been discovered.

, only 20 comets have been observed to pass within of Earth, including 10 which are or have been short-period comets. Two of these comets, Halley's Comet and 73P/Schwassmann–Wachmann, have been observed during multiple close approaches. The closest observed approach was 0.0151 AU (5.88 LD) for Lexell's Comet on July 1, 1770. After an orbit change due to a close approach of Jupiter in 1779, this object is no longer a NEC. The closest approach ever observed for a current short-period NEC is 0.0229 AU (8.92 LD) for Comet Tempel–Tuttle in 1366. This comet is the parent body of the Leonid meteor shower, which also produced the Great Meteor Storm of 1833. Orbital calculations show that P/1999 J6 (SOHO), a faint sungrazing comet and confirmed short-period NEC observed only during its close approaches to the Sun, passed Earth undetected at a distance of 0.0121 AU (4.70 LD) on June 12, 1999.

Comet 109P/Swift–Tuttle, which is also the source of the Perseid meteor shower every year in August, has a roughly 130-year orbit which passes close to the Earth. During the comet's September 1992 recovery, when only the two previous returns in 1862 and 1737 had been identified, calculations showed that the comet would pass close to Earth during its next return in 2126, with an impact within the range of uncertainty. By 1993, even earlier returns (back to at least 188 AD) have been identified, and the longer observation arc eliminated the impact risk, and the comet will pass Earth in 2126 at a distance of 23 million kilometers. In 3044, the comet is expected to pass Earth at less than 1.6 million kilometers.

Defunct space probes and final stages of rockets can end up in near-Earth orbits around the Sun, and be re-discovered by NEO surveys when they return to Earth's vicinity.

In September 2002, astronomers found an object designated J002E3. The object was on a temporary satellite orbit around Earth, leaving for a solar orbit in June 2003. Calculations showed that it was also on a solar orbit before 2002, but was close to Earth in 1971. J002E3 was identified as the third stage of the Saturn V rocket that carried Apollo 12 to the Moon. In 2006, two more apparent temporary satellites were discovered which were suspected of being artificial. One of them was eventually confirmed as an asteroid and classified as the temporary satellite . The other, 6Q0B44E, was confirmed as an artificial object, but its identity is unknown. Another temporary satellite was discovered in 2013, and was designated as a suspected asteroid. It was later found to be an artificial object of unknown origin. is no longer listed as an asteroid by the Minor Planet Center.

In some cases, active space probes on solar orbits have been observed by NEO surveys and erroneously catalogued as asteroids before identification. During its 2007 flyby of Earth on its route to a comet, ESA's space probe "Rosetta" was detected unidentified and classified as asteroid , with an alert issued due to its close approach. The designation was similarly removed from asteroid catalogues when the observed object was identified with "Gaia", ESA's space observatory for astrometry.

When a near-Earth object impacts Earth, objects up to a few tens of metres across ordinarily explode in the upper atmosphere (usually harmlessly), with most or all of the solids vaporized, while larger objects hit the water surface, forming tsunami waves, or the solid surface, forming impact craters.

The frequency of impacts of objects of various sizes is estimated on the basis of orbit simulations of NEO populations, the frequency of impact craters on the Earth and the Moon, and the frequency of close encounters. The study of impact craters indicates that impact frequency has been more or less steady for the past 3.5 billion years, which requires a steady replenishment of the NEO population from the asteroid main belt. One impact model based on widely accepted NEO population models estimates the average time between the impact of two stony asteroids with a diameter of at least at about one year; for asteroids across (which impacts with as much energy as the atomic bomb dropped on Hiroshima, approximately 15 kilotonnes of TNT) at five years, for asteroids across (an impact energy of 10 megatons, comparable to the Tunguska event in 1908) at 1,300 years, for asteroids across at half a million years, and for asteroids across at 18 million years. Some other models estimate similar impact frequencies, while others calculate higher frequencies. For Tunguska-sized (10-megaton) impacts, the estimates range from one event every 2,000–3,000 years to one event every 300 years.

The second-largest observed impact after the Tunguska meteor was a 1.1-megaton air blast in 1963 near the Prince Edward Islands between South Africa and Antarctica, which was detected only by infrasound sensors. The third-largest, but by far best-observed impact, was the Chelyabinsk meteor of February 15, 2013. A previously unknown asteroid exploded above this Russian city with an equivalent blast yield of 400–500 kilotons. The calculated orbit of the pre-impact asteroid is similar to that of Apollo asteroid 2011 EO40, making the latter the meteor's possible parent body.

On October 7, 2008, 19 hours after it was first observed, asteroid blew up above the Nubian Desert in Sudan. It was the first time that an asteroid was observed and its impact was predicted prior to its entry into the atmosphere as a meteor. 10.7 kg of meteorites were recovered after the impact.

On January 2, 2014, just 21 hours after it was the first asteroid to be discovered in 2014, 2–4 m blew up in Earth's atmosphere above the Atlantic Ocean. Far from any land, the meteor explosion was only observed by three infrasound detectors of the Comprehensive Nuclear-Test-Ban Treaty Organization. This impact was the second to be predicted in advance.

Asteroid impact prediction is however in its infancy and successfully predicted asteroid impacts are rare. The vast majority of impacts recorded by infrasound sensors designed to detect detonation of nuclear devices: are not predicted in advance.

Observed impacts aren't restricted to the surface and atmosphere of Earth. Dust-sized NEOs have impacted man-made spacecraft, including NASA's Long Duration Exposure Facility, which collected interplanetary dust in low Earth orbit for six years from 1984. Impacts on the Moon can be observed as flashes of light with a typical duration of a fraction of a second. The first lunar impacts were recorded during the 1999 Leonid storm. Subsequently, several continuous monitoring programs were launched. , the largest observed lunar impact occurred on September 11, 2013, lasted 8 seconds, and was likely caused by an object in diameter.

Each year, several mostly small NEOs pass Earth closer than the distance of the Moon.

On August 10, 1972, a meteor that became known as the 1972 Great Daylight Fireball was witnessed by many people; it moved north over the Rocky Mountains from the U.S. Southwest to Canada. It was an Earth-grazing meteoroid that passed within of the Earth's surface, and was filmed by a tourist at the Grand Teton National Park in Wyoming with an 8-millimeter color movie camera.

On October 13, 1990, Earth-grazing meteoroid EN131090 was observed above Czechoslovakia and Poland, moving at along a trajectory from south to north. The closest approach to the Earth was above the surface. It was captured by two all-sky cameras of the European Fireball Network, which for the first time enabled geometric calculations of the orbit of such a body.

On March 18, 2004, LINEAR announced that a asteroid, 2004 FH, would pass the Earth that day at only , about one-tenth the distance to the Moon, and the closest miss ever noticed until then. They estimated that similar-sized asteroids come as close about every two years.

On March 31, 2004, two weeks after 2004 FH, set a new record for closest recorded approach above the atmosphere, passing Earth's surface only away (about one Earth radius or one-sixtieth of the distance to the Moon). Because it was very small (6 meters/20 feet), FU was detected only hours before its closest approach. If it had collided with Earth, it probably would have disintegrated harmlessly in the atmosphere.

On February 4, 2011, an asteroid designated , estimated at in diameter, passed within of the Earth, setting a new record for closest approach without impact, which still stands .

On November 8, 2011, asteroid , relatively large at about in diameter, passed within (0.85 lunar distances) of Earth.

On February 15, 2013, the asteroid 367943 Duende () passed approximately above the surface of Earth, closer than satellites in geosynchronous orbit. The asteroid was not visible to the unaided eye. This was the first close passage of an object discovered during a previous passage, and was thus the first to be predicted well in advance.

Some NEOs are of special interest because they can be physically explored with lower mission velocity than is necessary for even the Moon, due to their combination of low velocity with respect to Earth and weak gravity. They may present interesting scientific opportunities both for direct geochemical and astronomical investigation, and as potentially economical sources of extraterrestrial materials for human exploitation. This makes them an attractive target for exploration.

The IAU held a minor planets workshop in Tucson, Arizona, in March 1971. At that point, launching a spacecraft to asteroids was considered premature; the workshop only inspired the first astronomical survey specifically aiming for NEAs. Missions to asteroids were considered again during a workshop at the University of Chicago held by NASA's Office of Space Science in January 1978. Of all of the near-Earth asteroids (NEA) that had been discovered by mid-1977, it was estimated that spacecraft could rendezvous with and return from only about 1 in 10 using less propulsive energy than is necessary to reach Mars. It was recognised that due to the low surface gravity of all NEAs, moving around on the surface of a NEA would cost very little energy, and thus space probes could gather multiple samples. Overall, it was estimated that about one percent of all NEAs might provide opportunities for human-crewed missions, or no more than about ten NEAs known at the time. A five-fold increase in the NEA discovery rate was deemed necessary to make a manned mission within ten years worthwhile.

The first near-Earth asteroid to be visited by a spacecraft was asteroid 433 Eros when NASA's "Near Earth Asteroid Rendezvous" ("NEAR") probe orbited it from February 2001, landing on the asteroid surface in February 2002. A second near-Earth asteroid, the long peanut-shaped 25143 Itokawa, was visited in September 2005 by JAXA's "Hayabusa" mission, which succeeded in taking material samples back to Earth. A third near-Earth asteroid, the long elongated 4179 Toutatis, was explored by CNSA's "Chang'e 2" spacecraft during a flyby in December 2012.

The Apollo asteroid 162173 Ryugu is the target of JAXA's "Hayabusa 2" mission. The space probe was launched in December 2014, is expected to arrive at the asteroid in June 2018, and to return a sample to Earth in December 2020. The Apollo asteroid 101955 Bennu, which, , has the second-highest cumulative Palermo scale rating (−1.71 for several close encounters between 2175 and 2199), is the target of NASA's "OSIRIS-REx" probe. The New Frontiers program mission was launched in September 2016. On its two-year journey to Bennu, the probe had searched for Earth's Trojan asteroids, rendezvoused with Bennu in August 2018, and had entered into orbit around the asteroid in December 2018. "OSIRIS-REx" will return samples from the asteroid in September 2023.

In April 2012, the company Planetary Resources announced its plans to mine asteroids commercially. In a first phase, the company reviewed data and selected potential targets among NEAs. In a second phase, space probes would be sent to the selected NEAs; mining spacecraft would be sent in a third phase. Planetary Resources launched two testbed satellites in April 2015 and January 2018, and the first prospecting satellite for the second phase is planned for a 2020 launch.

The Near-Earth Object Surveillance Mission (NEOSM) is planned for launch no earlier than 2025 to discover and characterize the orbit of most of the potentially hazardous asteroids larger than over the course of its mission.

The first near-Earth comet visited by a space probe was 21P/Giacobini–Zinner in 1985, when the NASA/ESA probe "International Cometary Explorer" ("ICE") passed through its coma. In March 1986, ICE, along with Soviet probes "Vega 1" and "Vega 2", ISAS probes "Sakigake" and "Suisei" and ESA probe "Giotto" flew by the nucleus of Halley's Comet. In 1992, "Giotto" also visited another NEC, 26P/Grigg–Skjellerup.

In November 2010, the NASA probe "Deep Impact" flew by the near-Earth comet 103P/Hartley. Earlier, in July 2005, this probe flew by the non-near-Earth comet Tempel 1, hitting it with a large copper mass.

In August 2014, ESA probe "Rosetta" began orbiting near-Earth comet 67P/Churyumov–Gerasimenko, while its lander "Philae" landed on its surface in November 2014. After the end of its mission, Rosetta was crashed into the comet's surface in 2016.




</doc>
<doc id="21627" url="https://en.wikipedia.org/wiki?curid=21627" title="Nation state">
Nation state

A nation state is a state in which a great majority shares the same culture and is conscious of it. The nation state is an ideal in which cultural boundaries match up with political boundaries. According to one definition, "a nation state is a sovereign state of which most of its subjects are united also by factors which defined a nation such as language or common descent." It is a more precise concept than "country", since a country does not need to have a predominant ethnic group.

A nation, in the sense of a common ethnicity, may include a diaspora or refugees who live outside the nation state; some nations of this sense do not have a state where that ethnicity predominates. In a more general sense, a nation state is simply a large, politically sovereign country or administrative territory. A nation state may be contrasted with:

This article mainly discusses the more specific definition of a nation-state as a typically sovereign country dominated by a particular ethnicity.

The relationship between a nation (in the ethnic sense) and a state can be complex. The presence of a state can encourage ethnogenesis, and a group with a pre-existing ethnic identity can influence the drawing of territorial boundaries or argue for political legitimacy.

This definition of a "nation-state" is not universally accepted. "All attempts to develop terminological consensus around "nation" resulted in failure", concludes academic Valery Tishkov.

Walker Connor discusses the impressions surrounding the characters of "nation", "(sovereign) state", "nation state", and "nationalism". Connor, who gave the term "ethnonationalism" wide currency, also discusses the tendency to confuse nation and state and the treatment of all states as if nation states. In "Globalization and Belonging", Sheila L. Crouche discusses "The Definitional Dilemma".

The origins and early history of nation states are disputed. A major theoretical question is: "Which came first, the nation or the nation state?" Scholars such as Steven Weber, David Woodward, Michel Foucault and Jeremy Black have advanced the hypothesis that the nation state did not arise out of political ingenuity or an unknown undetermined source, nor was it a political invention; but is an inadvertent byproduct of 15th-century intellectual discoveries in political economy, capitalism, mercantilism, political geography, and geography combined together with cartography and advances in map-making technologies. It was with these intellectual discoveries and technological advances that the nation state arose. For others, the nation existed first, then nationalist movements arose for sovereignty, and the nation state was created to meet that demand. Some "modernization theories" of nationalism see it as a product of government policies to unify and modernize an already existing state. Most theories see the nation state as a 19th-century European phenomenon, facilitated by developments such as state-mandated education, mass literacy and mass media. However, historians also note the early emergence of a relatively unified state and identity in Portugal and the Dutch Republic.

In France, Eric Hobsbawm argues, the French state preceded the formation of the French people. Hobsbawm considers that the state made the French nation, not French nationalism, which emerged at the end of the 19th century, the time of the Dreyfus Affair. At the time of the 1789 French Revolution, only half of the French people spoke some French, and 12–13% spoke the version of it that was to be found in literature and in educational facilities, according to Hobsbawm.

During the Italian unification, the number of people speaking the Italian language was even lower. The French state promoted the replacement of various regional dialects and languages by a centralised French language. The introduction of conscription and the Third Republic's 1880s laws on public instruction facilitated the creation of a national identity under this theory.
Some nation states, such as Germany and Italy, came into existence at least partly as a result of political campaigns by nationalists, during the 19th century. In both cases, the territory was previously divided among other states, some of them very small. The sense of common identity was at first a cultural movement, such as in the "Völkisch movement" in German-speaking states, which rapidly acquired a political significance. In these cases, the nationalist sentiment and the nationalist movement clearly precede the unification of the German and Italian nation states.

Historians Hans Kohn, Liah Greenfeld, Philip White and others have classified nations such as Germany or Italy, where cultural unification preceded state unification, as "ethnic nations" or "ethnic nationalities". However, "state-driven" national unifications, such as in France, England or China, are more likely to flourish in multiethnic societies, producing a traditional national heritage of "civic nations", or "territory-based nationalities". Some authors deconstruct the distinction between ethnic nationalism and civic nationalism because of the ambiguity of the concepts. They argue that the paradigmatic case of Ernest Renan is an idealisation and it should be interpreted within the German tradition and not in opposition to it. For example, they argue that the arguments used by Renan at the conference "What is a nation?" are not consistent with his thinking. This alleged civic conception of the nation would be determined only by the case of the loss gives Alsace and Lorraine in the Franco-Prussian War.

The idea of a nation state was and is associated with the rise of the modern system of states, often called the "Westphalian system" in reference to the Treaty of Westphalia (1648). The balance of power, which characterized that system, depended on its effectiveness upon clearly defined, centrally controlled, independent entities, whether empires or nation states, which recognize each other's sovereignty and territory. The Westphalian system did not create the nation state, but the nation state meets the criteria for its component states (by assuming that there is no disputed territory).

The nation state received a philosophical underpinning in the era of Romanticism, at first as the "natural" expression of the individual peoples (romantic nationalism: see Johann Gottlieb Fichte's conception of the "Volk", later opposed by Ernest Renan). The increasing emphasis during the 19th century on the ethnic and racial origins of the nation, led to a redefinition of the nation state in these terms. Racism, which in Boulainvilliers's theories was inherently antipatriotic and antinationalist, joined itself with colonialist imperialism and "continental imperialism", most notably in pan-Germanic and pan-Slavic movements.

The relation between racism and ethnic nationalism reached its height in the 20th century fascism and Nazism. The specific combination of "nation" ("people") and "state" expressed in such terms as the "Völkische Staat" and implemented in laws such as the 1935 Nuremberg laws made fascist states such as early Nazi Germany qualitatively different from non-fascist nation states. Minorities were not considered part of the people ("Volk"), and were consequently denied to have an authentic or legitimate role in such a state. In Germany, neither Jews nor the Roma were considered part of the people and were specifically targeted for persecution. German nationality law defined "German" on the basis of German ancestry, excluding "all" non-Germans from the people.

In recent years, a nation state's claim to absolute sovereignty within its borders has been criticized. A global political system based on international agreements and supra-national blocs characterized the post-war era. Non-state actors, such as international corporations and non-governmental organizations, are widely seen as eroding the economic and political power of nation states.

According to Andreas Wimmer and Yuval Feinstein, nation-states tended to emerge when power shifts allowed nationalists to overthrow existing regimes or absorb existing administrative units. Xue Li and Alexander Hicks links the frequency of nation-state creation to processes of diffusion that emanate from international organizations.

In Europe, during the 18th century, the classic non-national states were the "multiethnic" empires, the Austrian Empire, Kingdom of France, Kingdom of Hungary, the Russian Empire, the Spanish Empire, the Ottoman Empire, the British Empire and smaller nations at what would now be called sub-state level. The multi-ethnic empire was an absolute monarchy ruled by a king, emperor or sultan. The population belonged to many ethnic groups, and they spoke many languages. The empire was dominated by one ethnic group, and their language was usually the language of public administration. The ruling dynasty was usually, but not always, from that group.

This type of state is not specifically European: such empires existed in Asia, Africa and the Americas. In the Muslim world, immediately after Muhammad's death in 632, Caliphates were established. Caliphates were Islamic states under the leadership of a political-religious successor to the Islamic prophet Muhammad. These polities developed into multi-ethnic trans-national empires. The Ottoman sultan, Selim I (1512–1520) reclaimed the title of caliph, which had been in dispute and asserted by a diversity of rulers and "shadow caliphs" in the centuries of the Abbasid-Mamluk Caliphate since the Mongols' sacking of Baghdad and the killing of the last Abbasid Caliph in Baghdad, Iraq 1258.
The Ottoman Caliphate as an office of the Ottoman Empire was abolished under Mustafa Kemal Atatürk in 1924 as part of Atatürk's Reforms.

Some of the smaller European states were not so ethnically diverse, but were also dynastic states, ruled by a royal house. Their territory could expand by royal intermarriage or merge with another state when the dynasty merged. In some parts of Europe, notably Germany, very small territorial units existed. They were recognized by their neighbors as independent, and had their own government and laws. Some were ruled by princes or other hereditary rulers, some were governed by bishops or abbots. Because they were so small, however, they had no separate language or culture: the inhabitants shared the language of the surrounding region.

In some cases these states were simply overthrown by nationalist uprisings in the 19th century. Liberal ideas of free trade played a role in German unification, which was preceded by a customs union, the Zollverein. However, the Austro-Prussian War, and the German alliances in the Franco-Prussian War, were decisive in the unification. The Austro-Hungarian Empire and the Ottoman Empire broke up after the First World War, and the Russian Empire became the Soviet Union after the Russian Civil War.

A few of the smaller states survived: the independent principalities of Liechtenstein, Andorra, Monaco, and the republic of San Marino. (Vatican City is a special case. All of the larger Papal States save the Vatican itself were occupied and absorbed by Italy by 1870. The resulting Roman Question was resolved with the rise of the modern state under the 1929 Lateran treaties between Italy and the Holy See.)

"Legitimate states that govern effectively and dynamic industrial economies are widely regarded today as the defining characteristics of a modern nation-state."

Nation states have their own characteristics, differing from those of the pre-national states. For a start, they have a different attitude to their territory when compared with dynastic monarchies: it is semisacred and nontransferable. No nation would swap territory with other states simply, for example, because the king's daughter married. They have a different type of border, in principle defined only by the area of settlement of the national group, although many nation states also sought natural borders (rivers, mountain ranges). They are constantly changing in population size and power because of the limited restrictions of their borders.

The most noticeable characteristic is the degree to which nation states use the state as an instrument of national unity, in economic, social and cultural life.

The nation state promoted economic unity, by abolishing internal customs and tolls. In Germany, that process, the creation of the Zollverein, preceded formal national unity. Nation states typically have a policy to create and maintain a national transportation infrastructure, facilitating trade and travel. In 19th-century Europe, the expansion of the rail transport networks was at first largely a matter for private railway companies, but gradually came under control of the national governments. The French rail network, with its main lines radiating from Paris to all corners of France, is often seen as a reflection of the centralised French nation state, which directed its construction. Nation states continue to build, for instance, specifically national motorway networks. Specifically transnational infrastructure programmes, such as the Trans-European Networks, are a recent innovation.

The nation states typically had a more centralised and uniform public administration than its imperial predecessors: they were smaller, and the population less diverse. (The internal diversity of the Ottoman Empire, for instance, was very great.) After the 19th-century triumph of the nation state in Europe, regional identity was subordinate to national identity, in regions such as Alsace-Lorraine, Catalonia, Brittany and Corsica. In many cases, the regional administration was also subordinated to central (national) government. This process was partially reversed from the 1970s onward, with the introduction of various forms of regional autonomy, in formerly centralised states such as France.

The most obvious impact of the nation state, as compared to its non-national predecessors, is the creation of a uniform national culture, through state policy. The model of the nation state implies that its population constitutes a nation, united by a common descent, a common language and many forms of shared culture. When the implied unity was absent, the nation state often tried to create it. It promoted a uniform national language, through language policy. The creation of national systems of compulsory primary education and a relatively uniform curriculum in secondary schools, was the most effective instrument in the spread of the national languages. The schools also taught the national history, often in a propagandistic and mythologised version, and (especially during conflicts) some nation states still teach this kind of history.

Language and cultural policy was sometimes negative, aimed at the suppression of non-national elements. Language prohibitions were sometimes used to accelerate the adoption of national languages and the decline of minority languages (see examples: Anglicisation, Bulgarization, Croatization, Czechization, Francisation, Italianization, Germanisation, Magyarisation, Polonisation, Russification, Serbization, Slovakisation).

In some cases, these policies triggered bitter conflicts and further ethnic separatism. But where it worked, the cultural uniformity and homogeneity of the population increased. Conversely, the cultural divergence at the border became sharper: in theory, a uniform French identity extends from the Atlantic coast to the Rhine, and on the other bank of the Rhine, a uniform German identity begins. To enforce that model, both sides have divergent language policy and educational systems.

In some cases, the geographic boundaries of an ethnic population and a political state largely coincide. In these cases, there is little immigration or emigration, few members of ethnic minorities, and few members of the "home" ethnicity living in other countries.

Nation states where a single ethnic group makes up more than 85% of the population include the following:

The notion of a unifying "national identity" also extends to countries that host multiple ethnic or language groups, such as India. For example, Switzerland is constitutionally a confederation of cantons, and has four official languages, but it has also a "Swiss" national identity, a national history and a classic national hero, Wilhelm Tell.

Innumerable conflicts have arisen where political boundaries did not correspond with ethnic or cultural boundaries.

After World War II in the Josip Broz Tito era, nationalism was appealed to for uniting South Slav peoples. Later in the 20th century, after the break-up of the Soviet Union, leaders appealed to ancient ethnic feuds or tensions that ignited conflict between the Serbs, Croats and Slovenes, as well as Bosniaks, Montenegrins and Macedonians, eventually breaking up the long collaboration of peoples. Ethnic cleansing was carried out in the Balkans, resulting in the destruction of the formerly socialist republic and producing the civil wars in Bosnia and Herzegovina in 1992–95, resulting in mass population displacements and segregation that radically altered what was once a highly diverse and intermixed ethnic makeup of the region. These conflicts were largely about creating a new political framework of states, each of which would be ethnically and politically homogeneous. Serbs, Croats and Bosniaks insisted they were ethnically distinct although many communities had a long history of intermarriage. Presently Slovenia (89% Slovene), Croatia (90.4% Croat) and Serbia (83% Serb) could be classified as nation states per se, whereas North Macedonia (66% Macedonian), Montenegro (42% Montenegrin) and Bosnia and Herzegovina (50.1% Bosniak) are multinational states.

Belgium is a classic example of a state that is not a nation state. The state was formed by secession from the United Kingdom of the Netherlands in 1830, whose neutrality and integrity was protected by the Treaty of London 1839; thus it served as a buffer state after the Napoleonic Wars between the European powers France, Prussia (after 1871 the German Empire) and the United Kingdom until World War I, when its neutrality was breached by the Germans. Currently, Belgium is divided between the Flemings in the north and the French-speaking or the German-speaking population in the south. The Flemish population in the north speaks Dutch, the Walloon population in the south speaks French or German. The Brussels population speaks French or Dutch.

The Flemish identity is also cultural, and there is a strong separatist movement espoused by the political parties, the right-wing Vlaams Belang and the Nieuw-Vlaamse Alliantie. The Francophone Walloon identity of Belgium is linguistically distinct and regionalist. There is also unitary Belgian nationalism, several versions of a Greater Netherlands ideal, and a German-speaking community of Belgium annexed from Germany in 1920, and re-annexed by Germany in 1940–1944. However these ideologies are all very marginal and politically insignificant during elections.

China covers a large geographic area and uses the concept of "Zhonghua minzu" or Chinese nationality, in the sense of ethnic groups, but it also officially recognizes the majority Han ethnic group which accounts for over 90% of the population, and no fewer than 55 ethnic national minorities.

According to Philip G. Roeder, Moldova is an example of a Soviet era "segment-state" (Moldavian SSR), where the "nation-state project of the segment-state trumped the nation-state project of prior statehood. In Moldova, despite strong agitation from university faculty and students for reunification with Romania, the nation-state project forged within the Moldavian SSR trumped the project for a return to the interwar nation-state project of Greater Romania." See Controversy over linguistic and ethnic identity in Moldova for further details.

The United Kingdom is an unusual example of a nation state due to its claimed "countries within a country" status. The United Kingdom, which is formed by the union of England, Scotland, Wales and Northern Ireland, is a unitary state formed initially by the merger of two independent kingdoms, the Kingdom of England (which already included Wales) and the Kingdom of Scotland, but the Treaty of Union (1707) that set out the agreed terms has ensured the continuation of distinct features of each state, including separate legal systems and separate national churches.

In 2003, the British Government described the United Kingdom as "countries within a country". While the Office for National Statistics and others describe the United Kingdom as a "nation state", others, including a then Prime Minister, describe it as a "multinational state", and the term Home Nations is used to describe the four national teams that represent the four nations of the United Kingdom (England, Northern Ireland, Scotland, Wales). Some refer to it as a "Union State".

There has been academic debate over whether the United Kingdom can be legally dissolved as it is normally recognized internationally as a single nation state. English law jurist A.V. Dicey from an English legal perspective wrote that the question is based on whether the legislation giving rise to the union (the Union with Scotland Act), one of the two pieces of legislation which created the state, can be repealed. Dicey claimed because the Law of England does not acknowledge the word "unconstitutional", as a matter of English law it can be repealed. He also stated any tampering with the Acts of Union 1707 would be political madness.

A similar unusual example is the Kingdom of the Netherlands. As of 10 October 2010, the Kingdom of the Netherlands consists of four countries:

Each is expressly designated as a "land" in Dutch law by the Charter for the Kingdom of the Netherlands. Unlike the German "Länder" and the Austrian "Bundesländer", "landen" is consistently translated as "countries" by the Dutch government.

Israel was founded as a Jewish state in 1948. Its "Basic Laws" describe it as both a Jewish and a democratic state. The (2018) explicitly specifies the nature of the State of Israel as the nation-state of the Jewish people. According to the Israel Central Bureau of Statistics, 75.7% of Israel's population are Jews. Arabs, who make up 20.4% of the population, are the largest ethnic minority in Israel. Israel also has very small communities of Armenians, Circassians, Assyrians, Samaritans. There are also some non-Jewish spouses of Israeli Jews. However, these communities are very small, and usually number only in the hundreds or thousands.

Pakistan, even being an ethnically diverse country and officially a federation, is regarded as a nation state due to its ideological basis on which it was given independence from British India as a separate nation rather than as part of a unified India. Different ethnic groups in Pakistan are strongly bonded by their common Muslim identity, common cultural and social values, common historical heritage, a national "lingua franca" (Urdu) and joint political, strategic and economic interests.

The most obvious deviation from the ideal of "one nation, one state" is the presence of minorities, especially ethnic minorities, which are clearly not members of the majority nation. An ethnic nationalist definition of a nation is necessarily exclusive: ethnic nations typically do not have open membership. In most cases, there is a clear idea that surrounding nations are different, and that includes members of those nations who live on the "wrong side" of the border. Historical examples of groups who have been specifically singled out as "outsiders" are the Roma and Jews in Europe.

Negative responses to minorities within the nation state have ranged from cultural assimilation enforced by the state, to expulsion, persecution, violence, and extermination. The assimilation policies are usually enforced by the state, but violence against minorities is not always state initiated: it can occur in the form of mob violence such as lynching or pogroms. Nation states are responsible for some of the worst historical examples of violence against minorities not considered part of the nation.

However, many nation states accept specific minorities as being part of the nation, and the term "national minority" is often used in this sense. The Sorbs in Germany are an example: for centuries they have lived in German-speaking states, surrounded by a much larger ethnic German population, and they have no other historical territory. They are now generally considered to be part of the German nation and are accepted as such by the Federal Republic of Germany, which constitutionally guarantees their cultural rights. Of the thousands of ethnic and cultural minorities in nation states across the world, only a few have this level of acceptance and protection.

Multiculturalism is an official policy in many states, establishing the ideal of peaceful existence among multiple ethnic, cultural, and linguistic groups. Many nations have laws protecting minority rights.

When national boundaries that do not match ethnic boundaries are drawn, such as in the Balkans and Central Asia, ethnic tension, massacres and even genocide, sometimes has occurred historically (see Bosnian genocide and 2010 South Kyrgyzstan ethnic clashes).

Ideally, the border of a nation state extends far enough to include all the members of the nation, and all of the national homeland. Again, in practice some of them always live on the 'wrong side' of the border. Part of the national homeland may be there too, and it may be governed by the 'wrong' nation. The response to the non-inclusion of territory and population may take the form of irredentism: demands to annex "unredeemed" territory and incorporate it into the nation state.

Irredentist claims are usually based on the fact that an identifiable part of the national group lives across the border. However, they can include claims to territory where no members of that nation live at present, because they lived there in the past, the national language is spoken in that region, the national culture has influenced it, geographical unity with the existing territory, or a wide variety of other reasons. Past grievances are usually involved and can cause revanchism.

It is sometimes difficult to distinguish irredentism from pan-nationalism, since both claim that all members of an ethnic and cultural nation belong in one specific state. Pan-nationalism is less likely to specify the nation ethnically. For instance, variants of Pan-Germanism have different ideas about what constituted Greater Germany, including the confusing term "Grossdeutschland", which, in fact, implied the inclusion of huge Slavic minorities from the Austro-Hungarian Empire.

Typically, irredentist demands are at first made by members of non-state nationalist movements. When they are adopted by a state, they typically result in tensions, and actual attempts at annexation are always considered a "casus belli", a cause for war. In many cases, such claims result in long-term hostile relations between neighbouring states. Irredentist movements typically circulate maps of the claimed national territory, the "greater" nation state. That territory, which is often much larger than the existing state, plays a central role in their propaganda.

Irredentism should not be confused with claims to overseas colonies, which are not generally considered part of the national homeland. Some French overseas colonies would be an exception: French rule in Algeria unsuccessfully treated the colony as a "département" of France.

It has been speculated by both proponents of globalization and various science fiction writers that the concept of a nation state may disappear with the ever-increasing interconnectedness of the world. Such ideas are sometimes expressed around concepts of a world government. Another possibility is a societal collapse and move into communal anarchy or zero world government, in which nation states no longer exist.

Globalization especially has helped to bring about the discussion about the disappearance of nation states, as global trade and the rise of the concepts of a 'global citizen' and a common identity have helped to reduce differences and 'distances' between individual nation states, especially with regards to the internet.

The theory of the clash of civilizations lies in direct contrast to cosmopolitan theories about an ever more-connected world that no longer requires nation states. According to political scientist Samuel P. Huntington, people's cultural and religious identities will be the primary source of conflict in the post–Cold War world.

The theory was originally formulated in a 1992 lecture at the American Enterprise Institute, which was then developed in a 1993 "Foreign Affairs" article titled "The Clash of Civilizations?", in response to Francis Fukuyama's 1992 book, "The End of History and the Last Man". Huntington later expanded his thesis in a 1996 book "The Clash of Civilizations and the Remaking of World Order".

Huntington began his thinking by surveying the diverse theories about the nature of global politics in the post–Cold War period. Some theorists and writers argued that human rights, liberal democracy and capitalist free market economics had become the only remaining ideological alternative for nations in the post–Cold War world. Specifically, Francis Fukuyama, in "The End of History and the Last Man", argued that the world had reached a Hegelian "end of history".

Huntington believed that while the age of ideology had ended, the world had reverted only to a normal state of affairs characterized by cultural conflict. In his thesis, he argued that the primary axis of conflict in the future will be along cultural and religious lines.

As an extension, he posits that the concept of different civilizations, as the highest rank of cultural identity, will become increasingly useful in analyzing the potential for conflict.

In the 1993 "Foreign Affairs" article, Huntington writes:

Sandra Joireman suggests that Huntington may be characterised as a neo-primordialist, as, while he sees people as having strong ties to their ethnicity, he does not believe that these ties have always existed.

Historians often look to the past to find the origins of a particular nation state. Indeed, they often put so much emphasis on the importance of the nation state in modern times, that they distort the history of earlier periods in order to emphasize the question of origins. Lansing and English argue that much of the medieval history of Europe was structured to follow the historical winners—especially the nation states that emerged around Paris and London. Important developments that did not directly lead to a nation state get neglected, they argue:





</doc>
<doc id="21628" url="https://en.wikipedia.org/wiki?curid=21628" title="Nicolas-Louis de Lacaille">
Nicolas-Louis de Lacaille

Abbé Nicolas-Louis de Lacaille, formerly sometimes spelled de la Caille, (; 15 March 1713 – 21 March 1762) was a French astronomer and geodesist who named 14 out of the 88 constellations. From 1750 to 1754 he studied the sky at the Cape of Good Hope in present-day South Africa. Lacaille observed over 10,000 stars using just a half-inch refractor.

Born at Rumigny, he attended school in Mantes-sur-Seine (now Mantes-la-Jolie). Afterwards, he studied rhetoric and philosophy at the and then theology at the Collège de Navarre. He was left destitute in 1731 by the death of his father, who had held a post in the household of the duchess of Vendôme. However, he was supported in his studies by the Duc de Bourbon, his father's former patron.

After he graduated, he did not accept ordination as a priest but took deacon's orders, becoming an Abbé. He concentrated thereafter on science, and, through the patronage of Jacques Cassini, obtained employment, first in surveying the coast from Nantes to Bayonne, then, in 1739, in remeasuring the French meridian arc, for which he is honored with a pyramid at Juvisy-sur-Orge. The success of this difficult operation, which occupied two years, and achieved the correction of the anomalous result published by Jacques Cassini in 1718, was mainly due to Lacaille's industry and skill. He was rewarded by admission to the Royal Academy of Sciences and appointment as Professor of mathematics in the Mazarin college of the University of Paris, where he constructed a small observatory fitted for his own use. He was the author of a number of influential textbooks and a firm advocate of Newtonian gravitational theory. His students included Antoine Lavoisier and Jean Sylvain Bailly, both of whom were later guillotined during the Revolution.

His desire to determine the distances of the planets trigonometrically, using the longest possible baseline, led him to propose, in 1750, an expedition to the Cape of Good Hope. This was officially sanctioned by Roland-Michel Barrin de La Galissonière. There, he constructed an observatory on the shore of Table Bay with the support of the Dutch Governor Ryk Tulbagh. The primary result of his two-year stay was observed nearly 10,000 southern stars, the production of which required observing every night for over a year. In the course of his survey he took note of 42 nebulous objects. He also achieved his aim of determining the lunar and solar parallaxes (Mars serving as an intermediary). This work required near-simultaneous observations from Europe which were carried out by Jérôme Lalande.

His southern catalogue, called "Coelum Australe Stelliferum", was published posthumously in 1763. He found it necessary to introduce 14 new constellations which have since become standard. One of these was Mons Mensae, the only constellation named after a terrestrial feature (the Table Mountain).

While at the Cape, Lacaille determined the radius of the earth in the southern hemisphere. He set out a baseline in the Swartland plain north of present-day Darling. Using triangulation he then measured a 137 km arc of meridian between Cape Town and Aurora, determining the latitudes of the end points by means of astronomical-geodetic observations. There is a memorial to his work at a location near Aurora, pictured here. His result suggested that the earth was more flattened towards the south pole than towards the north. George Everest, of the Indian Survey, while recuperating from an illness at the Cape nearly seventy years later, suggested that Lacaille's latitude observations had been affected by the gravitational attraction of Table Mountain at the southern end and by the Piketberg Mountain at the northern. In 1838, Thomas Maclear, who was Astronomer Royal at the Cape, repeated the measurements over a longer baseline and ultimately confirmed Everest's conjecture. Maclear's Beacon was erected on the Table Mountain in Cape Town to help with the verification.

During his voyage to the southern hemisphere as a passenger on the vessel "Le Glorieux", captained by the noted hydrographer Jean-Baptiste d'Après de Mannevillette, Lacaille became conscious of the difficulties in determining positions at sea. On his return to Paris he prepared the first set of tables of the Moon's position that was accurate enough to use for determining time and longitude by the method of 'Lunars' (Lunar distances) using the orbital theory of Clairaut. Lacaille was in fact an indefatigable calculator. Apart from constructing astronomical ephemerides and mathematical tables, he calculated a table of eclipses for 1800 years. Lalande said of him that, during a comparatively short life, he had made more observations and calculations than all the astronomers of his time put together. The quality of his work rivalled its quantity, while the disinterestedness and rectitude of his moral character earned him universal respect.

On his return to Paris in 1754, following a diversion to Mauritius, Lacaille was distressed to find himself an object of public attention. He resumed his work at the Mazarin College.

In 1757 he published his "Astronomiae Fundamenta Novissimus", containing a list of about 400 bright stars with positions corrected for aberration and nutation. He carried out calculations on comet orbits and was responsible for giving Halley's Comet its name. His last public lecture, given on 14 September 1761 at the Royal Academy of Sciences, summarised the improvements to astronomy that had occurred during his lifetime, to which he had made no small contribution.
His death, probably caused in part by over-work, occurred in 1762. He was buried in the vaults of the Mazarin College, now the Institut de France in Paris.

In 1754, he was elected a foreign member of the Royal Swedish Academy of Sciences. He was also an honorary member of the academies of Saint Petersburg and Berlin, the Royal Society of London and the Royal Society of Göttingen, and the Institute of Bologna.

Lacaille has the honor of naming 14 different constellations.

List of credited constellations:


The crater "La Caille" on the Moon is named after him. Asteroid 9135 Lacaille (AKA 7609 P-L and 1994 EK6), discovered on 17 October 1960 by Cornelis Johannes van Houten, Ingrid van Houten-Groeneveld and Tom Gehrels at Palomar Observatory, was also named after him.

In honor of his contribution to the study of the southern hemisphere sky, a 60-cm telescope at Reunion Island will be named the Lacaille Telescope.





</doc>
<doc id="21630" url="https://en.wikipedia.org/wiki?curid=21630" title="Nawaf al-Hazmi">
Nawaf al-Hazmi

Nawaf Muhammed Salim al-Hazmi (, ; also known as "Rabia al-Makki") (August 9, 1976 – September 11, 2001) was a Saudi Arabian terrorist. He was one of five hijackers of American Airlines Flight 77, which they crashed into the Pentagon as part of the September 11 attacks in the United States.

Hazmi and a long-time friend, Khalid al-Mihdhar, left their homes in Saudi Arabia in 1995 to fight for Muslims in the Bosnian War. Hazmi later traveled to Afghanistan to fight with the Taliban against the Afghan Northern Alliance. He returned to Saudi Arabia in early 1999.

Already long-time affiliates of al-Qaeda with extensive fighting experience, Hazmi and Mihdhar were chosen by Osama bin Laden for an ambitious terrorist plot to pilot commercial airliners into designated targets in the United States. Hazmi and Mihdhar both obtained US tourist visas in April 1999. Hazmi trained in an al-Qaeda training camp in the fall of 1999 and traveled to Malaysia for the 2000 Al-Qaeda Summit.

Hazmi arrived in Los Angeles, California, from Bangkok, Thailand, on January 15, 2000, alongside Mihdhar. The two settled in San Diego, staying at the Parkwood Apartments until May 2000. While in San Diego, they attended its mosque, led by Anwar al-Awlaki. The two took flying lessons in San Diego, but due to their poor English skills they did not perform well during their flight lessons and their flight instructor regarded them as suspicious.

Mihdhar left Hazmi in California for Yemen in June 2000. Hazmi stayed in California until he met up with Hani Hanjour in December 2000, and they both traveled to Phoenix, Arizona. They later moved to Falls Church, Virginia, in April 2001, where the rest of the hijackers began to join them. Hazmi met frequently with Mohamed Atta, the ringleader of the attacks, during the summer of 2001.

The CIA reportedly received Hazmi's name on a list of 19 persons suspected of planning an attack in the near future. Hazmi was one of the four names on the list who were known for certain. A search for Hazmi and other suspected terrorists commenced, but they were not located until after the attacks.

On September 10, 2001, Hazmi, Mihdhar, and Hanjour checked into a hotel in Herndon, Virginia. The next morning, Hazmi and four other terrorists, including Hazmi's younger brother, Salem al-Hazmi, boarded American Airlines Flight 77 at Dulles Airport and hijacked the plane so that Hanjour could pilot and crash the plane into the Pentagon as part of the September 11 attacks. The crash killed all 64 passengers aboard the aircraft and 125 in the Pentagon. Following the attacks, Hazmi's participation was initially dismissed as that of a "muscle hijacker", but he was later revealed to have played a larger role in the operational planning than previously believed.

Nawaf was born in Mecca in Saudi Arabia to Muhammad Salim al-Hazmi, a grocer. He traveled to Afghanistan as a teenager in 1993. CNN's preliminary report following the attacks claimed that an unnamed acquaintance relayed ""He told me once that his father had tried to kill him when he was a child. He never told me why, but he had a long knife scar on his forearm"", and claimed that his older brother was a police chief in Jizan.

In 1995, he and his childhood friend, Khalid al-Mihdhar, joined a group that went to fight alongside Bosnian Muslims in the Bosnian War. Afterwards, Nawaf returned to Afghanistan along with his brother Salem, and Mihdhar. In Afghanistan, they fought alongside the Taliban against the Afghan Northern Alliance, and joined up with al-Qaeda. Nawaf al-Hazmi returned to Saudi Arabia in early 1999.

Osama bin Laden held Hazmi and Mihdhar in high respect, with their experience fighting during the 1990s in Bosnia and elsewhere. Al-Qaeda later referred to Hazmi as Mihdhar's "Second-in-command". When bin Laden committed to the "planes operation" plot in spring 1999, he personally selected Hazmi and Mihdhar to be involved in the plot as pilot hijackers. In addition to Hazmi and Mihdhar, two Yemenis were selected for a southeast Asia component of the plot, which was later scrapped for being too difficult to coordinate with the operations in the United States. Known as "Rabi'ah al-Makki" during the preparations, Hazmi had been so eager to participate in operations within the United States, he already had a US visa when bin Laden selected him. Hazmi obtained a B-1/B-2 tourist visa on April 3, 1999, from the US consulate in Jeddah, Saudi Arabia, using a new passport he acquired a few weeks earlier. Hazmi's passport did have indicators of al-Qaeda association, but immigration inspectors were not trained to look for those.

In the autumn of 1999, these four attended the Mes Aynak training camp in Afghanistan, which provided advanced training. Hazmi went with the two Yemenis, Tawfiq bin Attash (Khallad) and Abu Bara al Yemeni, to Karachi, Pakistan, where Khalid Sheikh Mohammed, the plot's coordinator, instructed him on western culture, travel, as well as taught some basic English phrases. Mihdhar did not go with him to Karachi, but instead left for Yemen. Khalid Sheikh Mohammed then sent Hazmi and the other men to Malaysia for a meeting. Before leaving for Malaysia, Khalid Sheikh Mohammed doctored Hazmi's Saudi passport in order to conceal his travel to Pakistan and Afghanistan, and make it appear that Hazmi had come to Malaysia from Saudi Arabia via Dubai.

After the attacks, the Associated Press would re-publish a "bizarre" story by the "Cody Enterprise" that quoted witnesses stating that Nawaf entered the United States during the autumn of 1999, crossing along the Canada–US border as one of two men delivering skylights to the local high school in Cody, Wyoming. Leaving the city 45 minutes later with the remaining cardboard boxes, the men allegedly asked "how to get to Florida."

Based on information uncovered by the FBI in the 1998 United States embassy bombings case, the National Security Agency (NSA) began tracking the communications of Mihdhar's father-in-law, Ahmad Muhammad Ali al-Hada, who was facilitating al-Qaeda communications, in 1999. Authorities also became aware of Hazmi, as a friend and associate of Mihdhar. Saudi Intelligence was also aware that Hazmi was associated with al-Qaeda, and associated with the 1998 African embassy bombings and attempts to smuggle arms into the kingdom in 1997. He also said that he revealed this to the CIA, saying "What we told them was these people were on our watch list from previous activities of al-Qaeda" The CIA strongly denies having received any such warning.

In late 1999, the NSA informed the CIA of an upcoming meeting in Malaysia, which Hada mentioned would involve "Khalid", "Nawaf", and "Salem". On January 5, Hazmi arrived in Kuala Lumpur, where he met up with Mihdhar, Attash, and Abu Bara. The group was in Malaysia to meet with Hambali for the 2000 Al Qaeda Summit, during which key details of the attacks may have been arranged. At this time, there was an East Asia component to the September 11 attacks plot, but bin Laden later canceled it for being too difficult to coordinate with operations in the United States. Ramzi bin al-Shibh was also at the summit, and Khalid Sheikh Mohammed possibly attended the summit. In Malaysia, the group stayed with Yazid Sufaat, a local member of Jemaah Islamiyah, who provided accommodations at request of Hambali. Both Mihdhar and Hazmi were secretly photographed at the meeting by Malaysian authorities, who provided surveillance at the request of the CIA. Malaysian authorities reported that Mihdhar spoke at length with Tawfiq bin Attash, one of the Yemenis, and others who were later involved in the USS Cole bombing. Hazmi and Mihdhar also met with Fahd al-Quso, who was later involved in the USS Cole bombing. After the meeting, Mihdhar and Hazmi traveled to Bangkok in Thailand on January 8, and left a week later on January 15 to travel to the United States.

On January 15, 2000, Hazmi and Mihdhar arrived together at Los Angeles International Airport from Bangkok, and were admitted for a six-month period. Immediately after entering the country, Nawaf and Mihdhar met Omar al-Bayoumi in an airport restaurant. Bayoumi claims he was merely being charitable in helping the two seemingly out-of-place Muslims to move to San Diego where he helped them find an apartment near his own, co-signed their lease, and gave them $1,500 to help pay their rent.

While in San Diego, witnesses told the FBI he and Mindhar had a close relationship with Anwar Al Awlaki. Authorities say the two regularly attended the Masjid Ar-Ribat al-Islami mosque Awlaki led in San Diego, and Awlaki had many closed-door meetings with them. Hazmi got a part-time job through the mosque at a nearby car wash.

In the beginning of February 2000, Mihdhar and Hazmi rented an apartment at the Parkwood Apartments, a 175-unit complex in the Clairemont Mesa section of San Diego, near the Balboa Drive Mosque. In February, Mihdhar purchased a used 1988 Toyota Corolla. While living at the Parkwood Apartments, neighbors thought that Mihdhar and Hazmi were odd. Months passed without them getting any furniture for the apartment. Instead, the men slept on mattresses on the floor, yet they carried briefcases, were frequently on their mobile phones, and were occasionally picked up by a limousine. After the attacks, their neighbors told the media that the pair constantly played flight simulator games. Residents said a total of four men spent time together at Parkwood, playing in the pool like children.

On April 4, 2000, Hazmi took a one-hour introductory flight lesson at the National Air College in San Diego. Both Mihdhar and Hazmi took flight lessons in May 2000 at the Sorbi Flying Club, located at Montgomery Field in San Diego. On May 5, Hazmi and Mihdhar took a lesson for one hour, and additional lessons on May 10 at the Sorbi Flying Club, with Hazmi flying an aircraft for 30 minutes. However, their English skills were very poor, and they did not do well with flight lessons. The first day that they showed up, they told instructors that they wanted to learn how to fly Boeings. Mihdhar and Hazmi raised some suspicion when they offered extra money to their flight instructor, Richard Garza, if he would train them to fly jets. Suspicious of the two men, Garza refused the offer but did not report them to authorities. Garza described the two men as "impatient students" who "wanted to learn to fly jets, specifically Boeings."

Adel Rafeea received a wire transfer of $5,000, on April 18, from Ali Abdul Aziz Ali in the UAE, which he later claimed was money Nawaf had asked him to accept on his behalf.

At the end of May 2000, Hazmi and Mihdhar moved out of Parkwood Apartments, and moved to nearby Lemon Grove, California. At this time, Mihdhar transferred his vehicle's registration to Hazmi, and he left San Diego on June 10, 2000. Mihdhar returned to Yemen, which angered Khalid Sheikh Mohammed, who did not want Hazmi to be left alone in California.

On July 12, 2000, Hazmi filed for an extension of his visa, which was due to expire. His visa was extended until January 2001, though Hazmi never filed any further requests to extend it beyond that.

In September, Nawaf and Mihdhar both moved into the house of FBI informant Abdussattar Shaikh, although he did not report the pair as suspicious. Mihdhar is believed to have left the apartment in early October, less than two weeks before the USS Cole Bombing. Nawaf continued living with Shaikh until December.

Hani Hanjour arrived in San Diego in early December 2000, joining Hazmi, but on December 10, they were seen leaving their Mount Vernon address leaving for Phoenix, Arizona where Hanjour could take refresher flight training. On December 12, they arrived at Mesa, Arizona. On December 22, Hanjour and Hazmi signed a lease for an apartment in the Indian Springs Village complex in Mesa, moving in on January 9.

In March, al-Hazmi received a shipment of VHS videos including videos about Boeing 747 and 777 flight decks and "how an airline captain should look and act" and later a road atlas, map of New York City and a World aeronautical chart.

On March 30, al-Hazmi notified his utility company that he might be moving to another state or Saudi Arabia. He and Hanjour moved out before the apartment rental expired at the end of the month on their way to Virginia. 2 days later on April 1, 2001, Oklahoma police officer C. L. Parkins pulled Hazmi over for speeding in their Corolla along with an additional citation for failing to use a seatbelt together totaling $138. A routine inspection of his California drivers license turned up no warrants or alerts, although his name was known to both the NSA and the CIA as a suspected terrorist.

Anwar al-Awlaki had already headed east and served as Imam at the Dar al-Hijrah mosque in the metropolitan Washington, DC area starting in January 2001. Shortly after this, his sermons were attended by three of the 9/11 hijackers (the new one being Hanjour).

By April 3, he was likely with companion Hani Hanjour when he was recorded at an ATM in Front Royal, Virginia, arriving in Falls Church, Virginia, by April 4. They met a man believed to be a Jordanian named Eyad Alrababah at a 7-11 that day. The 9-11 commission wrote that Hazmi and Hanjour met Alrababah at the Dar al Hijra mosque who was computer technician who had moved from West Paterson, New Jersey and was there to ask imam Anwar al-Awlaki about finding a job. He helped the pair rent an apartment in Alexandria where they moved in.

The September 11 Commission concluded that two of the hijackers "reportedly respected Awlaki as a religious figure". Police found his telephone number in the contacts of Ramzi bin al-Shibh (the "20th hijacker") when they searched his Hamburg apartment while investigating the 9/11 attacks.

On May 1, 2001, Hazmi reported to police that men tried to take his wallet outside his Fairfax, Virginia, residence, but before the county officer left, Hazmi signed a "statement of release" indicating he did not want the incident investigated.

In May 2001, Nidal Hasan's mother's funeral was held at the Falls Church mosque, although it is not known if al-Hazmi attended the service. On May 2, two other hijackers, Ahmed al-Ghamdi and Majed Moqed, arrived in Virginia and moved in with them. On May 8, Alrababah suggested that al-Hazmi and al-Mihdhar move with him to Fairfield, Connecticut, and helped all four hijackers move to a hotel there. They called area flight schools and after a few days Alrababah drove the four to Paterson, New Jersey, to show them around. Some FBI agents suspected that Awlaki gave Alrababah the job of helping Hazmi and Hanjour. Alrababah was later arrested as a witness convicted after 9/11 in a fraudulent driver's license scheme and deported to Jordan.

On May 21, al-Hazmi moved in with Hanjour into an apartment in Paterson New Jersey. Mohamed Atta was living in the same city at another location.

On June 30, al-Hazmi's car was involved in a minor traffic accident on the east-bound George Washington Bridge.
On June 25, 2001, al-Hazmi obtained a drivers' license in Florida, providing an address in Delray Beach, Florida, and he obtained a USA ID card on July 10. On August 2, al-Hazmi also obtained a Virginia drivers' license, and made a request for it to be reissued on September 7.

On July 20, al-hazmi and fellow hijacker Hani Hanjour flew to the Montgomery County Airpark in Maryland from on a practice flight from Fairfield, New Jersey.

Al-Hazmi, along with at least five other future hijackers, traveled to Las Vegas, Nevada, at least six times in the summer of 2001. They reportedly drank alcohol, gambled, and paid strippers to perform lap dances for them.

Throughout the summer, al-Hazmi met with leader Mohamed Atta to discuss the status of the operation of a monthly basis.

On August 23, Israeli Mossad reportedly gave his name to the CIA as part of a list of 19 names they said were planning an attack in the near future. Only four of the names are known for certain – Narwaf, Atta, al-Shehhi and al-Mihdhar, but again, the connection was not made with previous contacts by local law enforcement. On the same day, he was added to an INS watch list, together with Mihdhar to prevent entry into the US.

An internal review after 9/11 found that "everything was done [to find them] that could have been done." However, the search does not appear to have been particularly aggressive. A national motor vehicle index was reportedly checked, but Hazmi's speeding ticket was not detected for some reason. The FBI did not search credit card databases, bank account databases, or car registration, all of which would have produced positive results. Hazmi was even listed in the 2000–2001 San Diego phone book, but this too was not searched until after the attacks.
He had not been placed on terrorist watch lists, nor did the CIA or NSA alert the FBI, Customs and Immigration, or local police and enforcement agencies.

On August 27, brothers Nawaf and Salem purchased flight tickets through Travelocity.com using Nawaf's Visa card.

On September 1, Nawaf registered Room #7 at the Pin-Del Motel in Laurel, Maryland. On the registration, he listed his driver's license number as 3402142-D, and gave a New York hotel as his permanent residence. Ziad Jarrah had checked into the hotel on August 27.

Nawaf and Mihdhar purchased their 9/11 plane tickets online using a credit card with their real names. This raised no red flags, since the FAA had not been informed that the two were on a terrorist watchlist.

On September 10, 2001, Hanjour, Mihdhar, and Nawaf checked into the Marriott Residence Inn in Herndon, Virginia, where Saleh Ibn Abdul Rahman Hussayen, a prominent Saudi government official, was staying – although no evidence was ever uncovered that they had met, or knew of each other's presence.

On September 11, Hazmi boarded American Airlines Flight 77. The flight was scheduled to depart at 08:10 but ended up departing 10 minutes late from Gate D26 at Dulles. The last normal radio communications from the aircraft to air traffic control occurred at 08:50:51. At 08:54, the hijackers sent pilots Charles Burlingame and David Charlesbois to the back of the plane. Flight 77 began to deviate from its normal, assigned flight path and turned south. The hijackers then set the flight's autopilot in the direction of Washington, D.C. Passenger Barbara Olson called her husband, United States Solicitor General Theodore Olson, and reported that the plane had been hijacked and that the assailants had box cutters and knives. At 09:37, American Airlines Flight 77 crashed into the west facade of the Pentagon, killing all 64 aboard (including the hijackers) along with 125 in the Pentagon.

Nawaf al-Hazmi's 1988 blue Toyota Corolla was found on the next day in Dulles International Airport's hourly parking lot. Inside the vehicle, authorities found a letter written by Mohamed Atta, maps of Washington, D.C. and New York City, a cashier's check made out to a Phoenix flight school, four drawings of a Boeing 757 cockpit, a box cutter, and a page with notes and phone numbers.

In the recovery process at the Pentagon, remains of all five Flight 77 hijackers were identified through a process of elimination, as not matching any DNA samples for the victims, and put into custody of the FBI. Forensics teams confirmed that it seemed two of the hijackers were brothers, based on their DNA similarities.

Several weeks after the attacks, a Las Vegas Days Inn employee went to the FBI and stated that she recognized Hazmi's photographs from the media as being a man she had met at the hotel, who had asked for details on hotels near Los Angeles. She admitted that he never gave his name.

Late in 2005, Army Lt. Col. Kevin Shaffer and Congressman Curt Weldon alleged that the Defense Department data mining project Able Danger had kept Nawaf, Khalid al-Mihdhar, Mohamed Atta and Marwan al-Shehhi all under surveillance as al-Qaeda agents.




</doc>
<doc id="21631" url="https://en.wikipedia.org/wiki?curid=21631" title="November 12">
November 12






</doc>
<doc id="21632" url="https://en.wikipedia.org/wiki?curid=21632" title="Nero">
Nero

Nero ( ; Nero Claudius Caesar Augustus Germanicus; 15 December 37 – 9 June 68 AD) was the fifth Roman emperor, ruling from 54 to 68. His infamous reign is usually associated with tyranny, extravagance and debauchery. Nero, originally named Lucius Domitius Ahenobarbus, belonged to the Julio-Claudian dynasty, and was adopted as heir by the emperor Claudius, his great-uncle and stepfather. Nero succeeded Claudius while not yet aged 17, and his mother, Agrippina, tried to dominate his early life and decisions, but Nero cast her off and had her killed five years into his reign.

During the early years of his reign, Nero was content to be guided by his mother Agrippina, his tutor Seneca, and his Praetorian prefect Afranius Burrus. As time passed, he began to play a more active and independent role in government and foreign policy. During his reign, the redoubtable general Corbulo conducted a successful war and negotiated peace with the Parthian Empire. His general Suetonius Paulinus crushed a major revolt in Britain, led by the Iceni Queen Boudica. The Bosporan Kingdom was briefly annexed to the empire, and the First Jewish–Roman War began. Nero focused much of his attention on diplomacy and trade, as well as the cultural life of the empire, ordering theatres built and promoting athletic games. He made public appearances as an actor, poet, musician, and charioteer. In the eyes of traditionalists, this undermined the dignity and authority of his person, status, and office. His extravagant, empire-wide program of public and private works was funded by a rise in taxes that was much resented by the upper classes. In contrast, his populist style of rule remained well-admired among the lower classes of Rome and the provinces until his death and beyond. Various plots against Nero's life developed, and Nero had many of those involved put to death.

In AD 68 Vindex, governor of the Gaulish territory Gallia Lugdunensis, rebelled, with support from Galba, governor of Hispania Tarraconensis. Vindex's revolt failed in its immediate aim, though Nero fled Rome when its discontented civil and military authorities chose Galba as emperor. On 9 June in AD 68, he committed suicide, becoming the first Roman Emperor to do so, after learning that he had been tried "in absentia" and condemned to death as a public enemy. His death ended the Julio-Claudian dynasty, sparking a brief period of civil wars known as the Year of the Four Emperors.

Most Roman sources, including Suetonius and Cassius Dio, offer overwhelmingly negative assessments of his personality and reign; likewise, Tacitus claims that the Roman people thought him compulsive and corrupt. Suetonius tells that many Romans believed that the Great Fire of Rome was instigated by Nero to clear the way for his planned palatial complex, the Domus Aurea. According to Tacitus he was said to have seized Christians as scapegoats for the fire and burned them alive, seemingly motivated not by public justice but by personal cruelty. Some modern historians question the reliability of the ancient sources on Nero's tyrannical acts, however. There is evidence of his popularity among the Roman commoners, especially in the eastern provinces of the Empire, where a popular legend arose that Nero had not died and would return. At least three leaders of short-lived, failed rebellions presented themselves as "Nero reborn" to enlist popular support.

Nero was born Lucius Domitius Ahenobarbus on 15 December 37in Antium. He was the only son of Gnaeus Domitius Ahenobarbus and Agrippina the Younger. His maternal grandparents were Germanicus and Agrippina the Elder; his mother, Caligula's sister. He was Augustus' great-great grandson, descended from the first Emperor's only daughter, Julia.

The ancient biographer Suetonius, who was critical of Nero's ancestors, wrote that Augustus had reproached Nero's grandfather for his unseemly enjoyment of violent gladiator games. According to Jürgen Malitz, Suetonius tells that Nero's father was known to be "irascible and brutal", and that both "enjoyed chariot races and theater performances to a degree not befitting their position."

Nero's father, Domitius, died in 40 AD. A few years before his death, Domitius had been involved in a political scandal that, according to Malitz, "could have cost him his life if Tiberius had not died in the year 37." In the previous year, Nero's mother Agrippina had been caught up in a scandal of her own. Caligula's beloved sister Drusilla had recently died and Caligula began to feel threatened by his brother-in-law Marcus Aemilius Lepidus. Agrippina, suspected of adultery with her brother-in-law, was forced to carry the funerary urn after Lepidus' execution. Caligula then banished his two surviving sisters, Agrippina and Julia Livilla, to a remote island in the Mediterranean Sea. According to "The Oxford Encyclopedia of Ancient Greece and Rome", Agrippina was exiled for plotting to overthrow Caligula. Nero's inheritance was taken from him and he was sent to live with his paternal aunt Domitia Lepida the Younger, the mother of Claudius' third wife Valeria Messalina.

Caligula's reign lasted from 37 until 41. He died from multiple stab wounds in January of 41 after being ambushed by his own Praetorian Guard on the Palatine Hill. Claudius succeeded Caligula as Emperor. Agrippina married Claudius in 49 and became his fourth wife. By February 49, she had persuaded Claudius to adopt her son Nero. After Nero's adoption, "Claudius" became part of his name: Nero Claudius Caesar Drusus Germanicus. Claudius had gold coins issued to mark the adoption. Classics professor Josiah Osgood has written that "the coins, through their distribution and imagery alike, showed that a new Leader was in the making." David Shotter noted that, despite events in Rome, Nero's step-brother Britannicus was more prominent in provincial coinages during the early 50s.

Nero formally entered public life as an adult in 51—he was around 14 years old. When he turned 16, Nero married Claudius' daughter (his step-sister), Claudia Octavia. Between the years 51 and 53, he gave several speeches on behalf of various communities including the Ilians; the Apameans, requesting a five-year tax reprieve after an earthquake; and the northern colony of Bologna, after their settlement suffered a devastating fire.

Claudius died in 54; many ancient historians claim that he was poisoned by Agrippina. Shotter has written that "Claudius' death in 54 has usually been regarded as an event hastened by Agrippina because of signs that Claudius was showing a renewed affection for his natural son," but he notes that among ancient sources Josephus was uniquely reserved in describing the poisoning as a rumor. Contemporary sources differ in their accounts. Tacitus says that Locusta prepared the poison, which was served to the Emperor by his food taster Halotus. Tacitus also writes that Agrippina arranged for Claudius' doctor Xenophon to administer poison, in the event that the Emperor survived. Suetonius differs in some details, but also implicates Halotus and Agrippina. Like Tacitus, Cassius Dio writes that the poison was prepared by Locusta, but in Dio's account it is administered by Agrippina instead of Halotus. In Apocolocyntosis, Seneca the Younger does not mention mushrooms at all. Agrippina's involvement in Claudius' death is not accepted by all modern scholars.

Before Claudius' death, Agrippina had maneuvered to remove Britannicus' tutors and replace them with tutors that she had selected. She was also able to convince Claudius to replace with a single commander, Burrus, two prefects of the Praetorian guard who were suspected of supporting Brittanicus. Since Agrippina had replaced the guard officers with men loyal to her, Nero was able to assume power without incident.

Most of what we know about Nero's reign comes from three ancient writers: Tacitus, Suetonius, and Greek historian Cassius Dio.

According to ancient historians, Nero's construction projects were overly extravagant and the large number of expenditures under Nero left Italy "thoroughly exhausted by contributions of money" with "the provinces ruined." Modern historians, though, note that the period was riddled with deflation and that it is likely that Nero's spending came in the form of public-works projects and charity intended to ease economic troubles.

Nero became emperor in 54 , aged sixteen years. This made him the youngest sole emperor until Elagabalus, who became emperor aged 14 in 218. The first five years of Nero's reign were described as "Quinquennium Neronis" by Trajan; the interpretation of the phrase is a matter of dispute amongst scholars. As Pharaoh of Egypt, Nero adopted the royal titulary "Autokrator Neron Heqaheqau Meryasetptah Tjemaahuikhasut Wernakhtubaqet Heqaheqau Setepennenu Merur" ('Emperor Nero, Ruler of rulers, chosen by Ptah, beloved of Isis, the sturdy-armed one who struck the foreign lands, victorious for Egypt, ruler of rulers, chosen of Nun who loves him').

Nero's tutor, Seneca, prepared Nero's first speech before the Senate. During this speech, Nero spoke about "eliminating the ills of the previous regime." H.H. Scullard writes that "he promised to follow the Augustan model in his principate, to end all secret trials "intra cubiculum", to have done with the corruption of court favorites and freedmen, and above all to respect the privileges of the Senate and individual Senators." His respect of the Senatorial autonomy, which distinguished him from Caligula and Claudius, was generally well received by the Roman Senate.

Scullard writes that Nero's mother, Agrippina, "meant to rule through her son." Agrippina murdered her political rivals: Domitia Lepida the Younger, the aunt that Nero had lived with during Agrippina's exile; Marcus Junius Silanus, a great grandson of Augustus; and Narcissus. One of the earliest coins that Nero issues during his reign shows Agrippina on the coin's obverse side; usually, this would be reserved for a portrait of the emperor. The Senate also allowed Agrippina two lictors during public appearances, an honor that was customarily bestowed upon only magistrates and the Vestalis Maxima. In 55, Nero removed Agrippina's ally Marcus Antonius Pallas from his position in the treasury. Shotter writes the following about Agrippina's deteriorating relationship with Nero: "What Seneca and Burrus probably saw as relatively harmless in Nero—his cultural pursuits and his affair with the slave girl Claudia Acte—were to her signs of her son's dangerous emancipation of himself from her influence." Britannicus was poisoned after Agrippina threatened to side with him. Nero, who was having an affair with Acte, exiled Agrippina from the palace when she began to cultivate a relationship with his wife Octavia.

Jürgen Malitz writes that ancient sources do not provide any clear evidence to evaluate the extent of Nero's personal involvement in politics during the first years of his reign. He describes the policies that are explicitly attributed to Nero as "well-meant but incompetent notions" like Nero's failed initiative to abolish taxes in 58. Scholars generally credit Nero's advisors Burrus and Seneca with the administrative successes of these years. Malitz writes that in later years, Nero panicked when he had to make decisions on his own during times of crisis.

"The Oxford Encyclopedia of Ancient Greece and Rome" cautiously notes that Nero's reasons for killing his mother in 59 are "not fully understood." According to Tacitus, the source of conflict between Nero and his mother was Nero's affair with Poppaea Sabina. In "Histories" Tacitus writes that the affair began while Poppaea was still married to Rufrius Crispinus, but in his later work "Annals" Tacitus says Poppaea was married to Otho when the affair began. In "Annals" Tacitus writes that Agrippina opposed Nero's affair with Poppaea because of her affection for his wife Octavia. Anthony Barrett writes that Tacitus' account in "Annals" "suggests that Poppaea's challenge drove [Nero] over the brink." A number of modern historians have noted that Agrippina's death would not have offered much advantage for Poppaea, as Nero did not marry Poppaea until 62. Barrett writes that Poppaea seems to serve as a "literary device, utilized [by Tacitus] because [he] could see no plausible explanation for Nero's conduct and also incidentally [served] to show that Nero, like Claudius, had fallen under the malign influence of a woman." According to Suetonius, Nero had his former freedman Anicetus arrange a shipwreck; Agrippina survived the wreck, swam ashore and was executed by Anicetus, who reported her death as a suicide.

Modern scholars believe that Nero's reign had been going well in the years before Agrippina's death. For example, Nero promoted the exploration of the Nile river sources with a successful expedition. After Agrippina's exile, Burrus and Seneca were responsible for the administration of the Empire. However, Nero's "conduct became far more egregious" after his mother's death. Miriam T. Griffins suggests that Nero's decline began as early as 55 with the murder of his stepbrother Britannicus, but also notes that "Nero lost all sense of right and wrong and listened to flattery with total credulity" after Agrippina's death. Griffin points out that Tacitus "makes explicit the significance of Agrippina's removal for Nero's conduct".

In 62, Nero's adviser Burrus died. That same year Nero called for the first treason trial of his reign ("maiestas" trial) against Antistius Sosianus. He also executed his rivals Cornelius Sulla and Rubellius Plautus. Jürgen Malitz considers this to be a turning point in Nero's relationship with the Roman Senate. Malitz writes that "Nero abandoned the restraint he had previously shown because he believed a course supporting the Senate promised to be less and less profitable."

After Burrus' death, Nero appointed two new Praetorian Prefects: Faenius Rufus and Ofonius Tigellinus. Politically isolated, Seneca was forced to retire. According to Tacitus, Nero divorced Octavia on grounds of infertility, and banished her. After public protests over Octavia's exile, Nero accused her of adultery with Anicetus and she was executed.

In 64, Nero married Pythagoras, a freedman.

The Great Fire of Rome erupted on the night of 18 to 19 July, AD 64. The fire started on the slope of the Aventine overlooking the Circus Maximus.

Tacitus, the main ancient source for information about the fire, wrote that countless mansions, residences and temples were destroyed. Tacitus and Cassius Dio have both written of extensive damage to the Palatine, which has been supported by subsequent archaeological excavations. The fire is reported to have burned for over a week. It destroyed three of fourteen Roman districts and severely damaged seven more.

Tacitus wrote that some ancient accounts described the fire as an accident, while others had claimed that it was a plot of Nero. Tacitus is the only surviving source which does not blame Nero for starting the fire; he says he is "unsure." Pliny the Elder, Suetonius and Cassius Dio all wrote that Nero was responsible for the fire. These accounts give several reasons for Nero's alleged arson like Nero's envy of King Priam and a dislike for the city's ancient construction. Suetonius wrote that Nero started the fire because he wanted the space to build his Golden House. This Golden House or "Domus Aurea" included lush artificial landscapes and a 30-meter-tall statue of himself, the Colossus of Nero. The size of this complex is debated (from 100 to 300 acres).

Tacitus wrote that Nero accused Christians of starting the fire to remove suspicion from himself. According to this account, many Christians were arrested and brutally executed by "being thrown to the beasts, crucified, and being burned alive".

Suetonius and Cassius Dio alleged that Nero sang the "Sack of Ilium" in stage costume while the city burned. The popular legend that Nero played the fiddle while Rome burned "is at least partly a literary construct of Flavian propaganda [...] which looked askance on the abortive Neronian attempt to rewrite Augustan models of rule." In fact, the fiddle would not be invented until nearly 1400 years after Nero's death.

According to Tacitus, Nero was in Antium during the fire. Upon hearing news of the fire, Nero returned to Rome to organize a relief effort, providing for the removal of bodies and debris, which he paid for from his own funds. After the fire, Nero opened his palaces to provide shelter for the homeless, and arranged for food supplies to be delivered in order to prevent starvation among the survivors.

In the wake of the fire, he made a new urban development plan. Houses built after the fire were spaced out, built in brick, and faced by porticos on wide roads. Nero also built a new palace complex known as the Domus Aurea in an area cleared by the fire. To find the necessary funds for the reconstruction, tributes were imposed on the provinces of the empire. The cost to rebuild Rome was immense, requiring funds the state treasury did not have. Nero devalued the Roman currency for the first time in the Empire's history. He reduced the weight of the denarius from 84 per Roman pound to 96 (3.80 grams to 3.30 grams). He also reduced the silver purity from 99.5% to 93.5%—the silver weight dropping from 3.80 grams to 2.97 grams. Furthermore, Nero reduced the weight of the aureus from 40 per Roman pound to 45 (7.9 grams to 7.2 grams).

In 65, Gaius Calpurnius Piso, a Roman statesman, organized a conspiracy against Nero with the help of Subrius Flavus and Sulpicius Asper, a tribune and a centurion of the Praetorian Guard. According to Tacitus, many conspirators wished to "rescue the state" from the emperor and restore the Republic. The freedman Milichus discovered the conspiracy and reported it to Nero's secretary, Epaphroditos. As a result, the conspiracy failed and its members were executed including Lucan, the poet. Nero's previous advisor Seneca was accused by Natalis; he denied the charges but was still ordered to commit suicide as by this point he had fallen out of favor with Nero.

Nero was said to have kicked Poppaea to death in 65, before she could have his second child. Modern historians, noting the probable biases of Suetonius, Tacitus, and Cassius Dio, and the likely absence of eyewitnesses to such an event, propose that Poppaea may have died after miscarriage or in childbirth. Nero went into deep mourning; Poppaea was given a sumptuous state funeral, divine honors, and was promised a temple for her cult. A year's importation of incense was burned at the funeral. Her body was not cremated, as would have been strictly customary, but embalmed after the Egyptian manner and entombed; it is not known where.

In 67, Nero married Sporus, a young boy who is said to have greatly resembled Poppaea. Nero had him castrated, tried to make a woman out of him, and married him in a dowry and bridal veil. It is believed that he did this out of regret for his killing of Poppaea.

In March 68, Gaius Julius Vindex, the governor of Gallia Lugdunensis, rebelled against Nero's tax policies. Lucius Verginius Rufus, the governor of Germania Superior, was ordered to put down Vindex's rebellion. In an attempt to gain support from outside his own province, Vindex called upon Servius Sulpicius Galba, the governor of Hispania Tarraconensis, to join the rebellion and further, to declare himself emperor in opposition to Nero.

At the Battle of Vesontio in May 68, Verginius' forces easily defeated those of Vindex and the latter committed suicide. However, after putting down this one rebel, Verginius' legions attempted to proclaim their own commander as Emperor. Verginius refused to act against Nero, but the discontent of the legions of Germany and the continued opposition of Galba in Spain did not bode well for him.

While Nero had retained some control of the situation, support for Galba increased despite his being officially declared a public enemy ("hostis publicus"). The prefect of the Praetorian Guard, Gaius Nymphidius Sabinus, also abandoned his allegiance to the Emperor and came out in support of Galba.

In response, Nero fled Rome with the intention of going to the port of Ostia and, from there, to take a fleet to one of the still-loyal eastern provinces. According to Suetonius, Nero abandoned the idea when some army officers openly refused to obey his commands, responding with a line from Virgil's "Aeneid": "Is it so dreadful a thing then to die?" Nero then toyed with the idea of fleeing to Parthia, throwing himself upon the mercy of Galba, or appealing to the people and begging them to pardon him for his past offences "and if he could not soften their hearts, to entreat them at least to allow him the prefecture of Egypt". Suetonius reports that the text of this speech was later found in Nero's writing desk, but that he dared not give it from fear of being torn to pieces before he could reach the Forum.

Nero returned to Rome and spent the evening in the palace. After sleeping, he awoke at about midnight to find the palace guard had left. Dispatching messages to his friends' palace chambers for them to come, he received no answers. Upon going to their chambers personally, he found them all abandoned. When he called for a gladiator or anyone else adept with a sword to kill him, no one appeared. He cried, "Have I neither friend nor foe?" and ran out as if to throw himself into the Tiber.

Returning, Nero sought a place where he could hide and collect his thoughts. An imperial freedman, Phaon, offered his villa, located outside the city. Travelling in disguise, Nero and four loyal freedmen, Epaphroditos, Phaon, Neophytus, and Sporus, reached the villa, where Nero ordered them to dig a grave for him.

At this time, a courier arrived with a report that the Senate had declared Nero a public enemy, that it was their intention to execute him by beating him to death, and that armed men had been sent to apprehend him for the act to take place in the Roman Forum. The Senate actually was still reluctant and deliberating on the right course of action, as Nero was the last member of the Julio-Claudian Family. Indeed, most of the senators had served the imperial family all their lives and felt a sense of loyalty to the deified bloodline, if not to Nero himself. The men actually had the goal of returning Nero back to the Senate, where the Senate hoped to work out a compromise with the rebelling governors that would preserve Nero's life, so that at least a future heir to the dynasty could be produced.

Nero, however, did not know this, and at the news brought by the courier, he prepared himself for suicide, pacing up and down muttering "Qualis artifex pereo" ("What an artist dies in me"). Losing his nerve, he begged one of his companions to set an example by killing himself first. At last, the sound of approaching horsemen drove Nero to face the end. However, he still could not bring himself to take his own life but instead he forced his private secretary, Epaphroditos, to perform the task.

When one of the horsemen entered and saw that Nero was dying, he attempted to stop the bleeding, but efforts to save Nero's life were unsuccessful. Nero's final words were "Too late! This is fidelity!" He died on 9 June 68, the anniversary of the death of Octavia, and was buried in the Mausoleum of the Domitii Ahenobarbi, in what is now the Villa Borghese (Pincian Hill) area of Rome.

According to Sulpicius Severus, it is unclear whether Nero took his own life.

With his death, the Julio-Claudian dynasty ended. When news of his death reached Rome, the Senate posthumously declared Nero a public enemy to appease the coming Galba (as the Senate had initially declared Galba as a public enemy) and proclaimed Galba as the new emperor. Chaos would ensue in the year of the Four Emperors.

According to Suetonius and Cassius Dio, the people of Rome celebrated the death of Nero. Tacitus, though, describes a more complicated political environment. Tacitus mentions that Nero's death was welcomed by Senators, nobility and the upper class. The lower-class, slaves, frequenters of the arena and the theater, and "those who were supported by the famous excesses of Nero", on the other hand, were upset with the news. Members of the military were said to have mixed feelings, as they had allegiance to Nero, but had been bribed to overthrow him.

Eastern sources, namely Philostratus and Apollonius of Tyana, mention that Nero's death was mourned as he "restored the liberties of Hellas with a wisdom and moderation quite alien to his character" and that he "held our liberties in his hand and respected them."

Modern scholarship generally holds that, while the Senate and more well-off individuals welcomed Nero's death, the general populace was "loyal to the end and beyond, for Otho and Vitellius both thought it worthwhile to appeal to their nostalgia."

Nero's name was erased from some monuments, in what Edward Champlin regards as an "outburst of private zeal". Many portraits of Nero were reworked to represent other figures; according to Eric R. Varner, over fifty such images survive. This reworking of images is often explained as part of the way in which the memory of disgraced emperors was condemned posthumously (see damnatio memoriae). Champlin, however, doubts that the practice is necessarily negative and notes that some continued to create images of Nero long after his death. Damaged portraits of Nero, often with hammer-blows directed to the face, have been found in many provinces of the Roman Empire, three recently having been identified from the United Kingdom (see damnatio memoriae).

The civil war during the year of the Four Emperors was described by ancient historians as a troubling period. According to Tacitus, this instability was rooted in the fact that emperors could no longer rely on the perceived legitimacy of the imperial bloodline, as Nero and those before him could. Galba began his short reign with the execution of many of Nero's allies. One such notable enemy included Nymphidius Sabinus, who claimed to be the son of Emperor Caligula.

Otho overthrew Galba. Otho was said to be liked by many soldiers because he had been a friend of Nero and resembled him somewhat in temperament. It was said that the common Roman hailed Otho as Nero himself. Otho used "Nero" as a surname and reerected many statues to Nero. Vitellius overthrew Otho. Vitellius began his reign with a large funeral for Nero complete with songs written by Nero.

After Nero's suicide in 68, there was a widespread belief, especially in the eastern provinces, that he was not dead and somehow would return. This belief came to be known as the Nero Redivivus Legend. The legend of Nero's return lasted for hundreds of years after Nero's death. Augustine of Hippo wrote of the legend as a popular belief in 422.

At least three Nero imposters emerged leading rebellions. The first, who sang and played the cithara or lyre and whose face was similar to that of the dead emperor, appeared in 69 during the reign of Vitellius. After persuading some to recognize him, he was captured and executed. Sometime during the reign of Titus (79–81), another impostor appeared in Asia and sang to the accompaniment of the lyre and looked like Nero but he, too, was killed. Twenty years after Nero's death, during the reign of Domitian, there was a third pretender. He was supported by the Parthians, who only reluctantly gave him up, and the matter almost came to war.

In Britannia (Britain) in 59, Prasutagus, leader of the Iceni tribe, and a client king of Rome during Claudius' reign, died. The client state arrangement was unlikely to survive the death of the former Emperor. Prasutagus' will leaving control of the Iceni to his wife Boudica was denied, and, when Catus Decianus scourged Boudica and raped her daughters, the Iceni revolted. They were joined by the Trinovantes tribe, and their uprising became the most significant provincial rebellion of the 1st century. Under Boudica the towns of Camulodunum (Colchester), Londinium (London) and Verulamium (St Albans) were burned and a substantial body of legion infantry destroyed. The governor of the province Gaius Suetonius Paulinus assembled his remaining forces and defeated the Britons and restored order but for a while Nero considered abandoning the province. Julius Classicianus replaced Decianus as procurator. Classicianus advised Nero to replace Paulinus, who continued to punish the population even after the rebellion was over. Nero decided to adopt a more lenient approach to governing the province, and appointed a new governor, Petronius Turpilianus.

Nero began preparing for war in the early years of his reign, after the Parthian king Vologeses set his brother Tiridates on the Armenian throne. Around 57 and 58 Domitius Corbulo and his legions advanced on Tiridates and captured the Armenian capital Artaxata. Tigranes was chosen to replace Tiridates on the Armenian throne. When Tigranes attacked Adiabene, Nero had to send further legions to defend Armenia and Syria from Parthia.

The Roman victory came at a time when the Parthians were troubled by revolts; when this was dealt with they were able to devote resources to the Armenian situation. A Roman army under Paetus surrendered under humiliating circumstances and though both Roman and Parthian forces withdrew from Armenia, it was under Parthian control. The triumphal arch for Corbulo's earlier victory was part-built when Parthian envoys arrived in 63 AD to discuss treaties. 
Given "imperium" over the eastern regions, Corbulo organised his forces for an invasion but was met by this Parthian delegation. An agreement was thereafter reached with the Parthians: Rome would recognize Tiridates as king of Armenia, only if he agreed to receive his diadem from Nero. A coronation ceremony was held in Italy 66. Dio reports that Tiridates said "I have come to you, my God, worshiping you as Mithras." Shotter says this parallels other divine designations that were commonly applied to Nero in the East including "The New Apollo" and "The New Sun." After the coronation, friendly relations were established between Rome and the eastern kingdoms of Parthia and Armenia. Artaxata was temporarily renamed Neroneia.

In 66, there was a Jewish revolt in Judea stemming from Greek and Jewish religious tension. In 67, Nero dispatched Vespasian to restore order. This revolt was eventually put down in 70, after Nero's death. This revolt is famous for Romans breaching the walls of Jerusalem and destroying the Second Temple of Jerusalem.

Nero studied poetry, music, painting and sculpture. He both sang and played the "cithara" (a type of lyre). Many of these disciplines were standard education for the Roman elite, but Nero's devotion to music exceeded what was socially acceptable for a Roman of his class. Ancient sources were critical of Nero's emphasis on the arts, chariot-racing and athletics. Pliny described Nero as an "actor-emperor" ("scaenici imperatoris") and Suetonius wrote that he was "carried away by a craze for popularity...since he was acclaimed as the equal of Apollo in music and of the Sun in driving a chariot, he had planned to emulate the exploits of Hercules as well."

In 67 Nero participated in the Olympics. He had bribed organizers to postpone the games for a year so he could participate, and artistic competitions were added to the athletic events. Nero won every contest in which he was a competitor. During the games Nero sang and played his lyre on stage, acted in tragedies and raced chariots. He won a 10-horse chariot race, despite being thrown from the chariot and leaving the race. He was crowned on the basis that he would have won if he had completed the race. After he died a year later, his name was removed from the list of winners. Champlin writes that though Nero's participation "effectively stifled true competition, [Nero] seems to have been oblivious of reality."

Nero established the Neronian games in 60. Modeled on Greek style games, these games included "music" "gymnastic" and "questrian" contents. According to Suetonius the gymnastic contests were held in the Saepta area of the Campus Martius.

The history of Nero's reign is problematic in that no historical sources survived that were contemporary with Nero. These first histories, while they still existed, were described as biased and fantastical, either overly critical or praising of Nero. The original sources were also said to contradict on a number of events. Nonetheless, these lost primary sources were the basis of surviving secondary and tertiary histories on Nero written by the next generations of historians. A few of the contemporary historians are known by name. Fabius Rusticus, Cluvius Rufus and Pliny the Elder all wrote condemning histories on Nero that are now lost. There were also pro-Nero histories, but it is unknown who wrote them or for what deeds Nero was praised.

The bulk of what is known of Nero comes from Tacitus, Suetonius and Cassius Dio, who were all of the senatorial class. Tacitus and Suetonius wrote their histories on Nero over fifty years after his death, while Cassius Dio wrote his history over 150 years after Nero's death. These sources contradict one another on a number of events in Nero's life including the death of Claudius, the death of Agrippina, and the Roman fire of 64, but they are consistent in their condemnation of Nero.

A handful of other sources also add a limited and varying perspective on Nero. Few surviving sources paint Nero in a favourable light. Some sources, though, portray him as a competent emperor who was popular with the Roman people, especially in the east.

Cassius Dio (c. 155–229) was the son of Cassius Apronianus, a Roman senator. He passed the greater part of his life in public service. He was a senator under Commodus and governor of Smyrna after the death of Septimius Severus; and afterwards suffect consul around 205, and also proconsul in Africa and Pannonia.

Books 61–63 of Dio's "Roman History" describe the reign of Nero. Only fragments of these books remain and what does remain was abridged and altered by John Xiphilinus, an 11th-century monk.

Dio Chrysostom (c. 40–120), a Greek philosopher and historian, wrote the Roman people were very happy with Nero and would have allowed him to rule indefinitely. They longed for his rule once he was gone and embraced imposters when they appeared:

Epictetus (c. 55–135) was the slave to Nero's scribe Epaphroditos. He makes a few passing negative comments on Nero's character in his work, but makes no remarks on the nature of his rule. He describes Nero as a spoiled, angry and unhappy man.
The historian Josephus (c. 37–100), while calling Nero a tyrant, was also the first to mention bias against Nero. Of other historians, he said:

Although more of a poet than historian, Lucanus (c. 39–65) has one of the kindest accounts of Nero's rule. He writes of peace and prosperity under Nero in contrast to previous war and strife. Ironically, he was later involved in a conspiracy to overthrow Nero and was executed.

Philostratus II "the Athenian" (c. 172–250) spoke of Nero in the Life of Apollonius Tyana (Books 4–5). Although he has a generally bad or dim view of Nero, he speaks of others' positive reception of Nero in the East.

The history of Nero by Pliny the Elder (c. 24–79) did not survive. Still, there are several references to Nero in Pliny's "Natural Histories". Pliny has one of the worst opinions of Nero and calls him an "enemy of mankind."

Plutarch (c. 46–127) mentions Nero indirectly in his account of the Life of Galba and the Life of Otho, as well as in the Vision of Thespesius in Book 7 of the Moralia, where a voice orders that Nero's soul be transferred to a more offensive species. Nero is portrayed as a tyrant, but those that replace him are not described as better.

It is not surprising that Seneca (c. 4 BC–65 AD), Nero's teacher and advisor, writes very well of Nero.


Suetonius (c. 69–130) was a member of the equestrian order, and he was the head of the department of the imperial correspondence. While in this position, Suetonius started writing biographies of the emperors, accentuating the anecdotal and sensational aspects. By this account, Nero raped the vestal virgin Rubria. 


The "Annals" by Tacitus (c. 56–117) is the most detailed and comprehensive history on the rule of Nero, despite being incomplete after the year 66. Tacitus described the rule of the Julio-Claudian emperors as generally unjust. He also thought that existing writing on them was unbalanced:

Tacitus was the son of a procurator, who married into the elite family of Agricola. He entered his political life as a senator after Nero's death and, by Tacitus' own admission, owed much to Nero's rivals. Realising that this bias may be apparent to others, Tacitus protests that his writing is true.

In 1562 Girolamo Cardano published in Basel his "Encomium Neronis", which was one of the first historical references of the Modern era to portray Nero in a positive light.

At the end of 66, conflict broke out between Greeks and Jews in Jerusalem and Caesarea. According to the Talmud, Nero went to Jerusalem and shot arrows in all four directions. All the arrows landed in the city. He then asked a passing child to repeat the verse he had learned that day. The child responded, "I will lay my vengeance upon Edom by the hand of my people Israel" (Ezekiel 25:14). Nero became terrified, believing that God wanted the Second Temple to be destroyed, but that he would punish the one to carry it out. Nero said, "He desires to lay waste His House and to lay the blame on me," whereupon he fled and converted to Judaism to avoid such retribution. Vespasian was then dispatched to put down the rebellion.

The Talmud adds that the sage Reb Meir Baal HaNess lived in the time of the Mishnah, and was a prominent supporter of the Bar Kokhba rebellion against Roman rule. Rabbi Meir was considered one of the greatest of the Tannaim of the third generation (139–163). According to the Talmud, his father was a descendant of Nero who had converted to Judaism. His wife Bruriah is one of the few women cited in the Gemara. He is the third-most-frequently-mentioned sage in the Mishnah.

Roman and Greek sources nowhere report Nero's alleged trip to Jerusalem or his alleged conversion to Judaism. There is also no record of Nero having any offspring who survived infancy: his only recorded child, Claudia Augusta, died aged 4 months.

Non-Christian historian Tacitus describes Nero extensively torturing and executing Christians after the fire of 64. Suetonius also mentions Nero punishing Christians, though he does so because they are "given to a new and mischievous superstition" and does not connect it with the fire.

Christian writer Tertullian (c. 155–230) was the first to call Nero the first persecutor of Christians. He wrote, "Examine your records. There you will find that Nero was the first that persecuted this doctrine." Lactantius (c. 240–320) also said that Nero "first persecuted the servants of God." as does Sulpicius Severus. However, Suetonius writes that, "since the Jews constantly made disturbances at the instigation of Chrestus, the [emperor Claudius] expelled them from Rome" (""Iudaeos impulsore Chresto assidue tumultuantis Roma expulit""). These expelled "Jews" may have been early Christians, although Suetonius is not explicit. Nor is the Bible explicit, calling Aquila of Pontus and his wife, Priscilla, both expelled from Italy at the time, "Jews" (Acts 18:2).

The first text to suggest that Nero ordered the execution of an apostle is a letter by Clement to the Corinthians traditionally dated to around AD 96. The apocryphal Ascension of Isaiah, a Christian writing from the 2nd century, says, "the slayer of his mother, who himself (even) this king, will persecute the plant which the Twelve Apostles of the Beloved have planted. Of the Twelve one will be delivered into his hands"; this is interpreted as referring to Nero.

Bishop Eusebius of Caesarea (c. 275–339) was the first to write explicitly that Paul was beheaded in Rome during the reign of Nero. He states that Nero's persecution led to Peter and Paul's deaths, but that Nero did not give any specific orders. However, several other accounts going back to the 1st century have Paul surviving his two years in Rome and travelling to Hispania, before facing trial in Rome again prior to his death.

Peter is first said to have been crucified upside-down in Rome during Nero's reign (but not by Nero) in the apocryphal Acts of Peter (c. 200). The account ends with Paul still alive and Nero abiding by God's command not to persecute any more Christians.

By the 4th century, a number of writers were stating that Nero killed Peter and Paul.

The Sibylline Oracles, Book 5 and 8, written in the 2nd century, speak of Nero returning and bringing destruction. Within Christian communities, these writings, along with others, fueled the belief that Nero would return as the Antichrist. In 310, Lactantius wrote that Nero "suddenly disappeared, and even the burial place of that noxious wild beast was nowhere to be seen. This has led some persons of extravagant imagination to suppose that, having been conveyed to a distant region, he is still reserved alive; and to him they apply the Sibylline verses". Lactantius maintains that it is not right to believe this.

In 422, Augustine of Hippo wrote about 2 Thessalonians 2:1–11, where he believed that Paul mentioned the coming of the Antichrist. Although he rejects the theory, Augustine mentions that many Christians believed Nero was the Antichrist or would return as the Antichrist. He wrote, "so that in saying, 'For the mystery of iniquity doth already work,' he alluded to Nero, whose deeds already seemed to be as the deeds of Antichrist."

Some modern biblical scholars such as Delbert Hillers (Johns Hopkins University) of the American Schools of Oriental Research and the editors of the "Oxford Study Bible" and "Harper Collins Study Bible", contend that the number 666 in the Book of Revelation is a code for Nero, a view that is also supported in Roman Catholic Biblical commentaries.






</doc>
<doc id="21634" url="https://en.wikipedia.org/wiki?curid=21634" title="Neoclassical economics">
Neoclassical economics

Neoclassical economics is an approach to economics focusing on the determination of goods, outputs, and income distributions in markets through supply and demand. This determination is often mediated through a hypothesized maximization of utility by income-constrained individuals and of profits by firms facing production costs and employing available information and factors of production, in accordance with rational choice theory, a theory that has come under considerable question in recent years.

Neoclassical economics dominates microeconomics and, together with Keynesian economics, forms the neoclassical synthesis which dominates mainstream economics today. Although neoclassical economics has gained widespread acceptance by contemporary economists, there have been many critiques of neoclassical economics, often incorporated into newer versions of neoclassical theory, but some remaining distinct fields.

The term was originally introduced by Thorstein Veblen in his 1900 article 'Preconceptions of Economic Science', in which he related marginalists in the tradition of Alfred Marshall et al. to those in the Austrian School.

No attempt will here be made even to pass a verdict on the relative claims of the recognized two or three main "schools" of theory, beyond the somewhat obvious finding that, for the purpose in hand, the so-called Austrian school is scarcely distinguishable from the neo-classical, unless it be in the different distribution of emphasis. The divergence between the modernized classical views, on the one hand, and the historical and Marxist schools, on the other hand, is wider, so much so, indeed, as to bar out a consideration of the postulates of the latter under the same head of inquiry with the former. – Veblen
It was later used by John Hicks, George Stigler, and others to include the work of Carl Menger, William Stanley Jevons, Léon Walras, John Bates Clark, and many others. Today it is usually used to refer to mainstream economics, although it has also been used as an umbrella term encompassing a number of other schools of thought, notably excluding institutional economics, various historical schools of economics, and Marxian economics, in addition to various other heterodox approaches to economics.

Neoclassical economics is characterized by several assumptions common to many schools of economic thought. There is not a complete agreement on what is meant by neoclassical economics, and the result is a wide range of neoclassical approaches to various problem areas and domains—ranging from neoclassical theories of labor to neoclassical theories of demographic changes.

It was expressed by E. Roy Weintraub that neoclassical economics rests on three assumptions, although certain branches of neoclassical theory may have different approaches:


From these three assumptions, neoclassical economists have built a structure to understand the allocation of scarce resources among alternative ends—in fact understanding such allocation is often considered the definition of economics to neoclassical theorists. Here's how William Stanley Jevons presented "the problem of Economics".

Given, a certain population, with various needs and powers of production, in possession of certain lands and other sources of material: required, the mode of employing their labour which will maximize the utility of their produce.

From the basic assumptions of neoclassical economics comes a wide range of theories about various areas of economic activity. For example, profit maximization lies behind the neoclassical theory of the firm, while the derivation of demand curves leads to an understanding of consumer goods, and the supply curve allows an analysis of the factors of production. Utility maximization is the source for the neoclassical theory of consumption, the derivation of demand curves for consumer goods, and the derivation of labor supply curves and reservation demand.

Market supply and demand are aggregated across firms and individuals. Their interactions determine equilibrium output and price. The market supply and demand for each factor of production is derived analogously to those for market final output to determine equilibrium income and the income distribution. Factor demand incorporates the marginal-productivity relationship of that factor in the output market.

Neoclassical economics emphasizes equilibria, which are the solutions of agent maximization problems. Regularities in economies are explained by methodological individualism, the position that economic phenomena can be explained by aggregating over the behavior of agents. The emphasis is on microeconomics. Institutions, which might be considered as prior to and conditioning individual behavior, are de-emphasized. Economic subjectivism accompanies these emphases. See also general equilibrium.

Classical economics, developed in the 18th and 19th centuries, included a value theory and distribution theory. The value of a product was thought to depend on the costs involved in producing that product. The explanation of costs in classical economics was simultaneously an explanation of distribution. A landlord received rent, workers received wages, and a capitalist tenant farmer received profits on their investment. This classic approach included the work of Adam Smith and David Ricardo.

However, some economists gradually began emphasizing the perceived value of a good to the consumer. They proposed a theory that the value of a product was to be explained with differences in utility (usefulness) to the consumer. (In England, economists tended to conceptualize utility in keeping with the utilitarianism of Jeremy Bentham and later of John Stuart Mill.)

The third step from political economy to economics was the introduction of marginalism and the proposition that economic actors made decisions based on margins. For example, a person decides to buy a second sandwich based on how full he or she is after the first one, a firm hires a new employee based on the expected increase in profits the employee will bring. This differs from the aggregate decision making of classical political economy in that it explains how vital goods such as water can be cheap, while luxuries can be expensive.

The change in economic theory from classical to neoclassical economics has been called the "marginal revolution", although it has been argued that the process was slower than the term suggests. It is frequently dated from William Stanley Jevons's "Theory of Political Economy" (1871), Carl Menger's "Principles of Economics" (1871), and Léon Walras's "Elements of Pure Economics" (1874–1877). Historians of economics and economists have debated:

In particular, Jevons saw his economics as an application and development of Jeremy Bentham's utilitarianism and never had a fully developed general equilibrium theory. Menger did not embrace this hedonic conception, explained diminishing marginal utility in terms of subjective prioritization of possible uses, and emphasized disequilibrium and the discrete; further Menger had an objection to the use of mathematics in economics, while the other two modeled their theories after 19th century mechanics. Jevons built on the hedonic conception of Bentham or of Mill, while Walras was more interested in the interaction of markets than in explaining the individual psyche.

Alfred Marshall's textbook, "Principles of Economics" (1890), was the dominant textbook in England a generation later. Marshall's influence extended elsewhere; Italians would compliment Maffeo Pantaleoni by calling him the "Marshall of Italy". Marshall thought classical economics attempted to explain prices by the cost of production. He asserted that earlier marginalists went too far in correcting this imbalance by overemphasizing utility and demand. Marshall thought that "We might as reasonably dispute whether it is the upper
or the under blade of a pair of scissors that cuts a piece of paper, as whether value is governed by utility or cost of production".

Marshall explained price by the intersection of supply and demand curves. The introduction of different market "periods" was an important innovation of Marshall's:
Marshall took supply and demand as stable functions and extended supply and demand explanations of prices to all runs. He argued supply was easier to vary in longer runs, and thus became a more important determinant of price in the very long run.

An important change in neoclassical economics occurred around 1933. Joan Robinson and Edward H. Chamberlin, with the near simultaneous publication of their respective books, "The Economics of Imperfect Competition" (1933) and "The Theory of Monopolistic Competition" (1933), introduced models of imperfect competition. Theories of market forms and industrial organization grew out of this work. They also emphasized certain tools, such as the marginal revenue curve.

Joan Robinson's work on imperfect competition, at least, was a response to certain problems of Marshallian partial equilibrium theory highlighted by Piero Sraffa. Anglo-American economists also responded to these problems by turning towards general equilibrium theory, developed on the European continent by Walras and Vilfredo Pareto. J. R. Hicks's "Value and Capital" (1939) was influential in introducing his English-speaking colleagues to these traditions. He, in turn, was influenced by the Austrian School economist Friedrich Hayek's move to the London School of Economics, where Hicks then studied.

These developments were accompanied by the introduction of new tools, such as indifference curves and the theory of ordinal utility. The level of mathematical sophistication of neoclassical economics increased. Paul Samuelson's "Foundations of Economic Analysis" (1947) contributed to this increase in mathematical modelling.

The interwar period in American economics has been argued to have been pluralistic, with neoclassical economics and institutionalism competing for allegiance. Frank Knight, an early Chicago school economist attempted to combine both schools. But this increase in mathematics was accompanied by greater dominance of neoclassical economics in Anglo-American universities after World War II. Some argue that outside political interventions, such as McCarthyism, and internal ideological bullying played an important role in this rise to dominance.

Hicks' book, "Value and Capital" had two main parts. The second, which was arguably not immediately influential, presented a model of temporary equilibrium. Hicks was influenced directly by Hayek's notion of intertemporal coordination and paralleled by earlier work by Lindhal. This was part of an abandonment of disaggregated long run models. This trend probably reached its culmination with the Arrow–Debreu model of intertemporal equilibrium. The Arrow–Debreu model has canonical presentations in Gérard Debreu's "Theory of Value" (1959) and in Arrow and Hahn's "General Competitive Analysis" (1971).

Many of these developments were against the backdrop of improvements in both econometrics, that is the ability to measure prices and changes in goods and services, as well as their aggregate quantities, and in the creation of macroeconomics, or the study of whole economies. The attempt to combine neo-classical microeconomics and Keynesian macroeconomics would lead to the neoclassical synthesis which has been the dominant paradigm of economic reasoning in English-speaking countries since the 1950s. Hicks and Samuelson were for example instrumental in mainstreaming Keynesian economics.

Macroeconomics influenced the neoclassical synthesis from the other direction, undermining foundations of classical economic theory such as Say's law, and assumptions about political economy such as the necessity for a hard-money standard. These developments are reflected in neoclassical theory by the search for the occurrence in markets of the equilibrium conditions of Pareto optimality and self-sustainability.

Perhaps the best way to frame a criticism of Neoclassical Economics is in the terms offered by Leijonhufvud in the contention that "Instead of looking for an alternative to replace it, we should try to imagine an economic theory to transcend its limitations." The contention also points to the need to bring in empirical science... testing and re-testing Neoclassical Economics propositions... in order to nudge the Framework and Theory toward a foundation of empirical reality. It is with such empirical reality we might transcend limitations. Leijonhufvud is speaking from the perspective of Experimental Economics; Behavioral Economics, too, uses experimental techniques, but also relies on surveys and other observations of what drives economic choice, also seeking ways to bring economic reality into the Framework and Theory. For overviews of the many empirical findings in both Experimental and Behavioral Economics, some supporting Neoclassical Economics and many suggesting changes needed in the Framework and Theory, see Altman and Tomer. Also, for an overview of the empirical findings relating to conservation (and recycling) behavior, as in the notion of Empathy Conservation, see Lynne et al. Neoclassical Economics Framing and Theory has a history of not being able to adequately explain choices related to the interdependence of a person with the natural system. 

Neoclassical economics is sometimes criticized for having a normative bias. In this view, it does not focus on explaining actual economies, but instead on describing a theoretical world in which Pareto optimality applies.

Perhaps the strongest criticism lies in its disregard for the physical limits of the Earth and its ecosphere which are the physical container of all human economies. This disregard becomes hot denial by neoclassical economists when limits are asserted, since to accept such limits creates fundamental contradictions with the foundational presumptions that growth in scale of the human economy forever is both possible and desirable. The disregard/denial of limits includes both resources and "waste sinks", the capacity to absorb human waste products and man-made toxins. Ecological Economics sees interdependent Travelers on a Spaceship Earth, a Spaceship having limits. Neoclassical Economics, instead, sees each Traveler as independent of every other Traveler, and with the natural systems that make Travel on the Spaceship Earth possible, which are presumed (without empirical test) to be unlimited, or, at best limited only by knowledge. The empirical reality that people are interdependent with one another and with nature (i.e. the Spaceship Earth systems) is also recognized in Humanistic Economics, Buddhist Economics and Metaeconomics. 

Neoclassical Economics addresses the reality of interdependence through the notion of an externality, which is only occasional, and of no real consequence in that the market can always resolve it. Just change the property rights, privatizing the resource or good in question. Empirical reality points to the matter of interdependence being far more complex than can be fixed only with changing to private property rights, seeing the essential need, pragmatically speaking, for a good mix of both private and public property, a major theme in Institutional Economics. 

The assumption that individuals act rationally may be viewed as ignoring important aspects of human behavior. Many see the "economic man" as being quite different from real people, the Econ different from the Human. Many economists, even contemporaries, have criticized this model of economic man... with empirical evidence (as noted, especially in Behavioral Economics) growing in support of representing a person as a Human rather than an Econ. Thorstein Veblen put it most sardonically that neoclassical economics assumes a person to be:

[A] lightning calculator of pleasures and pains, who oscillates like a homogeneous globule of desire of happiness under the impulse of stimuli that shift about the area, but leave him intact.

As a result, neoclassical economics has extreme difficulty explaining such things as voting behavior, or someone running into a burning building to save a complete stranger, perhaps even perishing in the process. Clearly such choices are not much, if at all, in the self-interest. Such "non-rational" decision making has been examined deeply and widely in Behavioral Economics. Perhaps most importantly, Behavioral Economics has empirically demonstrated that while the Econ almost exclusively pursues only self-interest, the Human pursues a Dual Interest. The Dual Interest includes both the Ego-based self-interest and the Empathy-based other (shared with others, yet internalized within the own-self)-interest. And, most importantly, it is quite rational to seek balance in Self&Other-interest, even sacrificing a bit in the domain of Self-interest in order to do so. 

Voting behavior, as well as running into a burning building, is rational in that it produces payoff in the realm of shared other-interest... as in the right-thing-to-do... which often requires a bit of sacrifice in the domain of self-interest. Rationality is all about maximizing a joint, non-separable and interdependent self&other-interest, which represents the own-interest. Maximizing own-interest generally means a bit of sacrifice in both domains of self-interest and other-interest, with own-interest all about finding balance. The Dual Interest analytical system now represents the analytical engine of a Metaeconomics... the Meta pointing to bringing considerations of both ethics and the moral dimension...the right-thing-to-do... back into the formal structure of Neoclassical Economics. The Moral Dimension was there at the beginning, in the Moral Philosophy of Adam Smith. It is also quite rational to seek a balance in the Own-interest, with the Moral Dimension tempering the Self-interest.

Large corporations might perhaps come closer to the neoclassical ideal of profit maximization, but this is not necessarily viewed as desirable if this comes at the expense of neglect of wider social issues. The wider social issues are represented in the shared other-interest while profit maximization is represented in the self-interest. Balance is needed. 

Problems exist with making the neoclassical general equilibrium theory compatible with an economy that develops over time and includes capital goods. This was explored in a major debate in the 1960s—the "Cambridge capital controversy"—about the validity of neoclassical economics, with an emphasis on economic growth, capital, aggregate theory, and the marginal productivity theory of distribution. There were also internal attempts by neoclassical economists to extend the Arrow–Debreu model to disequilibrium investigations of stability and uniqueness. However a result known as the Sonnenschein–Mantel–Debreu theorem suggests that the assumptions that must be made to ensure that equilibrium is stable and unique are quite restrictive.

Neoclassical economics is also often seen as relying too heavily on complex mathematical models, such as those used in general equilibrium theory, without enough regard to whether these actually describe the real economy. Many see an attempt to model a system as complex as a modern economy by a mathematical model as unrealistic and doomed to failure. A famous answer to this criticism is Milton Friedman's claim that theories should be judged by their ability to predict events rather than by the realism of their assumptions. Mathematical models also include those in game theory, linear programming, and econometrics. Some see mathematical models used in contemporary research in mainstream economics as having transcended neoclassical economics, while others disagree. Critics of neoclassical economics are divided into those who think that highly mathematical method is inherently wrong and those who think that mathematical method is potentially good even if contemporary methods have problems.

In general, allegedly overly unrealistic assumptions are one of the most common criticisms towards neoclassical economics. It is fair to say that many (but not all) of these criticisms can only be directed towards a subset of the neoclassical models (for example, there are many neoclassical models where unregulated markets fail to achieve Pareto-optimality and there has recently been an increased interest in modeling non-rational decision making). Its disregard for social reality and its alleged role in aiding the elites to widen the wealth gap and social inequality is also frequently criticized.

It has been argued within the field of Ecological Economics that the Neoclassical Economics system is by nature dysfunctional. It considers the destruction of the natural world through the accelerating consumption of non-renewable resources as well as the exhaustion of the "waste sinks" of the ecosphere as mere "externalities." Such externalities, in turn, are viewed as occurring only occasionally, and easily rectified by shifting public property to private property: The Market will resolve any externalitity, given the opportunity to do so; so, there is no need for any kind of Government, or any other kind of Community "intervention." The Spaceship Earth system is viewed as a subset of the Human Economy, and fully subject to control (which is essential in order to have independence). Neoclassical Economics sees independence between the Human Economy and the Spaceship, between each Human and Nature. Ecological Economics points, instead, to the Human Economy as being embedded in the Spaceship Earth system, so everything is internal: It sees interdependence between each Human and Nature. In effect, there are no externalities, except for some material and energy exchange beyond the atmosphere of the Spaceship. So, a Framework and Theory is needed to transcend the limitation of the Neoclassical Economics presumption of independence, transcending the focus on only the Ego-based Self-interest of an independent person, in both consumption and production. The inherent interdependence of each person and nature, as well as each person with every other person, is recognized in Frameworks and Theory that see the role of Empathy in forming a shared Other-interest in the outcomes on the Spaceship. The essential need to consider Empathy, in order to address the matter of achieving sustainability on this Spaceship Earth, is also becoming a theme in the natural and environmental sciences.




</doc>
<doc id="21636" url="https://en.wikipedia.org/wiki?curid=21636" title="Naomi Wolf">
Naomi Wolf

Naomi R. Wolf (born November 12, 1962) is an American liberal progressive feminist author, journalist, and former political advisor to Al Gore and Bill Clinton.

Via Wolf's first book "The Beauty Myth" (1991), she became a leading spokeswoman of what has been described as the third wave of the feminist movement. Such leading feminists as Gloria Steinem and Betty Friedan praised the work; others, including Camille Paglia and Christina Hoff Sommers, criticized it. Her later books include the bestseller "" in 2007 and "". Critics have challenged the quality and accuracy of the scholarship in her books, including "Outrages" (2019). In this case, her serious misreading of court records led to its publication in the U.S. being cancelled.

Her career in journalism began in 1995 and has included topics such as abortion, the Occupy Wall Street movement, Edward Snowden and ISIS. She has written for media outlets such as "The Nation", "The New Republic", "The Guardian" and "The Huffington Post".

Wolf was born in San Francisco, to a Jewish family. Her mother is Deborah Goleman Wolf, an anthropologist and the author of "The Lesbian Community". Her father was Leonard Wolf, a Romanian-born gothic horror scholar at University of California, Berkeley and Yiddish translator. Leonard Wolf died from advanced Parkinson's Disease on March 20, 2019. Wolf has a brother, Aaron, and a half-brother, Julius, from her father's earlier relationship; it remained his secret until his daughter was in her 30s. She attended Lowell High School and debated in regional speech tournaments as a member of the Lowell Forensic Society.

Wolf attended Yale University receiving her Bachelor of Arts in English literature in 1984. From 1985 to 1987, she was a Rhodes Scholar at New College, Oxford. Her initial period at Oxford University was difficult for Wolf as she experienced "raw sexism, overt snobbery and casual antisemitism". Her writing became so personal and subjective that her tutor advised against submitting her doctoral thesis. Wolf told interviewer Rachel Cooke, writing for "The Observer", in 2019: "My subject didn’t exist. I wanted to write feminist theory, and I kept being told by the dons there was no such thing." Her feminist writing at this time formed the basis of her first book, "The Beauty Myth".

Wolf ultimately returned to Oxford, completing her Doctor of Philosophy degree in English literature in 2015. Her thesis, supervised by Dr. Stefano Evangelista of Trinity College, formed the basis for her 2019 book "Outrages: Sex, Censorship, and the Criminalization of Love".

Wolf was involved in Bill Clinton's 1996 re-election bid, brainstorming with the president's team about ways to reach female voters. During Al Gore's bid for the presidency in the 2000 election, Wolf was hired as a consultant to target female voters, reprising her role in the Clinton campaign. Wolf's ideas and participation in the Gore campaign generated considerable media coverage and criticism. According to a report by Michael Duffy in "Time", Wolf was paid a salary of $15,000 (by November 1999, $5,000) per month "in exchange for advice on everything from how to win the women's vote to shirt-and-tie combinations." This article was the original source of the assertion that Wolf was responsible for Gore's "three-buttoned, earth-toned look."

In an interview with Melinda Henneberger in "The New York Times", Wolf said she had been appointed in January 1999 and denied ever advising Gore on his wardrobe. Wolf said she had mentioned the term "alpha male" only once in passing and that "[it] was just a truism, something the pundits had been saying for months, that the vice president is in a supportive role and the President is in an initiatory role ... I used those terms as shorthand in talking about the difference in their job descriptions".

In 1991, Wolf gained international attention as a spokeswoman of third-wave feminism from the publication of her first book "The Beauty Myth", an international bestseller. It was named "one of the seventy most influential books of the twentieth century" by "The New York Times". She argues that "beauty" as a normative value is entirely socially constructed, and that the patriarchy determines the content of that construction with the objective of maintaining women's subjugation.

Wolf posits the idea of an "iron-maiden", an intrinsically unattainable standard that is then used to punish women physically and psychologically for their failure to achieve and conform to it. Wolf criticized the fashion and beauty industries as exploitative of women, but added that the beauty myth extended into all areas of human functioning. Wolf writes that women should have "the choice to do whatever we want with our faces and bodies without being punished by an ideology that is using attitudes, economic pressure, and even legal judgments regarding women's appearance to undermine us psychologically and politically". Wolf argues that women were under assault by the "beauty myth" in five areas: work, religion, sex, violence, and hunger. Ultimately, Wolf argues for a relaxation of normative standards of beauty. In her introduction, Wolf positioned her argument against the concerns of second-wave feminists and offered the following analysis:

Although "The Beauty Myth" was a bestseller, it received mixed responses from feminists and the media. Second-wave feminist Germaine Greer wrote that "The Beauty Myth" was "the most important feminist publication since "The Female Eunuch"", and Gloria Steinem wrote, ""The Beauty Myth" is a smart, angry, insightful book, and a clarion call to freedom. Every woman should read it." British novelist Fay Weldon called the book "essential reading for the New Woman". Betty Friedan wrote in "Allure" magazine that ""The Beauty Myth" and the controversy it is eliciting could be a hopeful sign of a new surge of feminist consciousness."

However, Camille Paglia, whose "Sexual Personae" was published in the same year as "The Beauty Myth", derided Wolf as unable to perform "historical analysis", and called her education "completely removed from reality." Her comments touched off a series of debates between Wolf and Paglia in the pages of "The New Republic".

Likewise, Christina Hoff Sommers criticized Wolf for publishing the estimate that 150,000 women were dying every year from anorexia. Sommers states that she tracked down the source to the American Anorexia and Bulimia Association who stated that they were misquoted; the figure refers to sufferers, not fatalities. Wolf's citation for the incorrect figure came from a book by Brumberg, who referred to an American Anorexia and Bulimia Association newsletter and misquoted the newsletter. Wolf accepted the error and changed it in future editions. Sommers gave an estimate for the number of fatalities in 1990 as 100–400. The annual anorexia casualties in the US were estimated to be around 50 to 60 per year in the mid-1990s. In 1995, for an article in "The Independent on Sunday", British journalist Joan Smith recalled asking Wolf to explain her unsourced assertion in "The Beauty Myth" that the UK "has 3.5 million anorexics or bulimics (95 per cent of them female), with 6,000 new cases yearly". Wolf replied, according to Smith, that she had calculated the statistics from patients with eating disorders at one clinic. 

In "The New York Times", Caryn James lambasted the book as a "sloppily researched polemic as dismissible as a hackneyed adventure film ... Even by the standards of pop-cultural feminist studies, "The Beauty Myth" is a mess." She called the statistics Wolf that cited "shamefully secondhand and outdated. In contrast, "The Washington Post" called the book "persuasive" and praised its "accumulated evidence".

Caspar Schoemaker of the Netherlands Trimbos Institute published a paper in the academic journal "Eating disorders" demonstrating that of the 23 statistics cited by Wolf in "Beauth Myth", 18 were incorrect, with Wolf citing numbers that average out to 8 times the number in the source she was citing. For example, Wolf wrote that 7.5% of girls and women have anexoria, the accurate figure is 0.065%.

Revisiting "Beauty Myth" in 2019 for "The New Republic", literary critic Maris Kreizman recalls that reading it as an undergraduate made her "world burst open." It "remains one of the most formative books in (Kreizman's) life." However, as she matured, Kreizman saw Wolf's books as "poorly argued tracts" that made "wilder and wilder assertions" even, in 2014, spreading a conspiracy theory that the beheadings of American journalists James Foley and Steven Sotloff by ISIS were "faked and staged." Kreizman "began to write (Wolf) off as a fringe character" despite the fact that she had "once informed my own feminism so deeply."

In "Fire with Fire" (1993), Wolf writes on politics, female empowerment and women's sexual liberation. "The New York Times" assailed the work for its "dubious oversimplifications and highly debatable assertions" and its "disconcerting penchant for inflationary prose," nonetheless approving of Wolf's "efforts to articulate an accessible, pragmatic feminism, ... helping to replace strident dogma with common sense." The "Time" magazine reviewer Martha Duffy dismissed the book as "flawed," although she commented that Wolf was "an engaging raconteur" who was also "savvy about the role of TV – especially the Thomas-Hill hearings and daytime talk shows – in radicalizing women, including homemakers." She characterized the book as advocating an inclusive strain of feminism that welcomed abortion opponents. In the UK, feminist author Natasha Walter writing in "The Independent" said that the book "has its faults, but compared with "The Beauty Myth" it has energy and spirit, and generosity too." Walter, however, criticized it for having a "narrow agenda" where "you will look in vain for much discussion of older women, of black women, of women with low incomes, of mothers." Characterizing Wolf as a "media star", Walter wrote: "She is particularly good, naturally, on the role of women in the media."

"Promiscuities" (1997) reports on and analyzes the shifting patterns of contemporary adolescent sexuality. Wolf argues that literature is rife with examples of male coming-of-age stories, covered autobiographically by D.H. Lawrence, Tobias Wolff, J.D. Salinger and Ernest Hemingway, and covered misogynistically by Henry Miller, Philip Roth and Norman Mailer. Wolf insists, however, that female accounts of adolescent sexuality have been systematically suppressed. She adduces cross-cultural material to demonstrate that women have, across history, been celebrated as more carnal than men. Wolf also argues that women must reclaim the legitimacy of their own sexuality by shattering the polarization of women between virgin and whore.

"Promiscuities" generally received negative reviews. In "The New York Times", Michiko Kakutani called Wolf a "frustratingly inept messenger: a sloppy thinker and incompetent writer. She tries in vain to pass off tired observations as radical "aperçus", subjective musings as generational truths, sappy suggestions as useful ideas". However, two days earlier in the "Times" Sunday edition, Weaver Courtney praised the book: "Anyone—particularly anyone who, like Ms. Wolf, was born in the 1960s—will have a very hard time putting down "Promiscuities". Told through a series of confessions, her book is a searing and thoroughly fascinating exploration of the complex wildlife of female sexuality and desire." In contrast, "The Library Journal" excoriated the work, writing, "Overgeneralization abounds as she attempts to apply the microcosmic events of this mostly white, middle-class, liberal milieu to a whole generation. ... There is a desperate defensiveness in the tone of this book which diminishes the force of her argument."

"Misconceptions" (2001) examines pregnancy and childbirth. Most of the book is told through the prism of Wolf's personal experience of her first pregnancy. She describes the "vacuous impassivity" of the ultrasound technician who gives her the first glimpse of her new baby. Wolf laments her C-section and examines why the procedure is commonplace in the United States, advocating a return to midwifery. The second half of the book is anecdotal, focusing on inequalities between parents to child care.

In her "New York Times" review, Claire Dederer suggested it was inappropriate to consider "Wolf as a political theorist, and instead call her a memoirist. She does her best writing when she's observing her own life." Her capability as a memoirist is not "self-indulgent. It seems vital, and in a sense radical, in the tradition of 1970's feminists who sought to speak to every aspect of women's lives."

Wolf's "The Treehouse: Eccentric Wisdom from my Father on How to Live, Love, and See" (2005) is an account of her midlife crisis attempt to reclaim her creative and poetic vision and revalue her father's love, and her father's force as an artist and a teacher.

In "" (2007), Wolf takes a historical look at the rise of fascism, outlining 10 steps necessary for a fascist group (or government) to destroy the democratic character of a nation-state. The book details how this pattern was implemented in Nazi Germany, Fascist Italy, and elsewhere, and analyzes its emergence and application of all the 10 steps in American political affairs since the September 11 attacks. Alex Beam wrote in "The New York Times": "In the book, Wolf insists that she is not equating [George W.] Bush with Hitler, nor the United States with Nazi Germany, then proceeds to do just that."

Several years later, Mark Nuckols, argued in "The Atlantic" that Wolf's supposed historical parallels between incidents from the era of the European dictators and modern America are based on a highly selective reading in which Wolf omits significant details and misuses her sources. For "The Daily Beast", Michael Moynihan, characterized the book as "an astoundingly lazy piece of writing." 

"The End of America" was adapted for the screen as a documentary by filmmakers Annie Sundberg and Ricki Stern, best known for "The Devil Came on Horseback" and "The Trials of Darryl Hunt". It premiered in October 2008, and was favorably reviewed in "The New York Times" by Stephen Holden "Variety" magazine, and Nigel Andrews in the "Financial Times".

Wolf returned to this general theme in an article in 2014 considering how modern Western women, born in inclusive, egalitarian liberal democracies, are assuming positions of leadership in neofascist political movements.

"Give Me Liberty: A Handbook for American Revolutionaries" (2008) was written as a sequel to "The End of America: Letter of Warning to a Young Patriot." In the book, Wolf looks at times and places in history where citizens were faced with the closing of an open society and successfully fought back.

Published in 2012 on the topic of the vagina, "Vagina: A New Biography" was much criticized, especially by feminist authors. Katie Roiphe described it as "ludicrous" in "Slate": "I doubt the most brilliant novelist in the world could have created a more skewering satire of Naomi Wolf's career than her latest book." In "The Nation", Katha Pollitt considered it a "silly book" containing "much dubious neuroscience and much foolishness." It becomes "loopier as it goes on. We learn that women think and feel through their vagina, which can 'grieve' and feel insulted." Toni Bentley wrote in "The New York Times Book Review" that Wolf used "shoddy research methodology", while with "her graceless writing, Wolf opens herself to ridicule on virtually every page." In "The New York Review of Books", Zoë Heller wrote that the book "offers an unusually clear insight into the workings of her mystic feminist philosophy". Part of the book concerns the history of the vagina's representation, but is "full of childlike generalizations" and her understanding of science "is pretty shaky too". "Los Angeles Times" columnist Meghan Daum decried the book's "painful" writing and its "hoary ideas about how women think." In "The New York Observer", Nina Burleigh suggested that critics of the book were so vehement "because (a) their editors handed the book to them for review because they thought it was an Important Feminist Book when it's actually slight and (b) there's a grain of truth in what she's trying to say."

In response to the criticism, Wolf stated in a television interview:
[A]nything that shows documentation of the brain and vagina connection is going to alarm some feminists... . ..also feminism has kind of retreated into the academy and sort of embraced the idea that all gender is socially constructed and so here is a book that is actually looking at science ... though there has been some criticisms of the book from some feminists ... who say, well you can't look at the science because that means we have to grapple with the science ... to me the feminist task of creating a just world isn't changed at all by this fascinating neuroscience that shows some differences between men and women.

Wolf's book "Outrages: Sex, Censorship, and the Criminalization of Love" was published in 2019, a work based on the 2015 D.Phil. thesis she had completed under the supervision of Trinity College, Oxford literary scholar Dr. Stefano-Maria Evangelista. In the book, she studies the repression of homosexuality in relation to attitudes towards divorce and prostitution, and also in relation to the censorship of books.

The book was published in the UK in May 2019 by Virago Press. On June 12, 2019, "Outrages" was named to the "O, The Oprah Magazine"s "The 32 Best Books by Women of Summer 2019" list. The following day, the U.S. publisher recalled all copies from U.S. bookstores.

An error in a central tenet of the book — a misunderstanding of the term "death recorded" — was identified in a 2019 BBC radio interview with broadcaster and author Matthew Sweet. He cited a website for the Old Bailey Criminal Court, the same site which Wolf had referred to as one of her sources earlier in the interview. Sweet stated the following:"'Death Recorded' ... this is the definition I'm reading ... the definition from the Old Bailey website." He challenged other points of the book to which Wolf replied: "I was going by the Old Bailey Records and Regional Crime tables." Sweet then interrupted her:“Well, that’s how I got this, through that same sort of, uh, that same portal!" Reviewers have described other errors of scholarship in the work.

Wolf appeared at the Hay Festival, Wales in late May 2019, a few days after her exchange with Matthew Sweet, where she defended her book and said she had already corrected the error, but, as of October 2019, she has yet to do so. She stated at an event in Manhattan in June that she was not embarrassed by the correction, but rather felt grateful towards Sweet for the correction. On October 18, 2019, it became known the release of the book by Houghton Mifflin Harcourt in the United States was being canceled. Wolf expressed the hope that the book would still be published in the US.

In an October 1995 article for "The New Republic" Wolf was critical of contemporary pro-choice positions, arguing that the movement had "developed a lexicon of dehumanization" and urged feminists to accept abortion as a form of homicide and defend the procedure within the ambiguity of this moral conundrum. She continued, "Abortion should be legal; it is sometimes even necessary. Sometimes the mother must be able to decide that the fetus, in its full humanity, must die."

Wolf concluded by speculating that in a world of "real gender equality," passionate feminists "might well hold candlelight vigils at abortion clinics, standing shoulder to shoulder with the doctors who work there, commemorating and saying goodbye to the dead." In an article for "New York" magazine on the subtle manipulation of George W. Bush's image among women, Wolf wrote in 2005: "Abortion is an issue not of "Ms." Magazine-style fanaticism or suicidal Republican religious reaction, but a complex issue."

Wolf suggested in a 2003 article for "New York" magazine that the ubiquity of internet pornography tends to enervate the sexual attraction of men toward typical real women. She writes, "The onslaught of porn is responsible for deadening male libido in relation to real women, and leading men to see fewer and fewer women as 'porn-worthy.' Far from having to fend off porn-crazed young men, according to Wolf, young women are worrying that as mere flesh and blood, they can scarcely get, let alone hold, their attention." Wolf advocated abstaining from porn not on moral grounds, but because "greater supply of the stimulant equals diminished capacity."

Wolf has commented about the dress required of women living in Muslim countries. In "The Sydney Morning Herald" in August 2008, she wrote:
In the January 2013 issue of "The Atlantic", law and business professor Mark Nuckols wrote: "In her various books, articles, and public speeches, Wolf has demonstrated recurring disregard for the historical record and consistently mutilated the truth with selective and ultimately deceptive use of her sources." He further stated: "[W]hen she distorts facts to advance her political agenda, she dishonors the victims of history and poisons present-day public discourse about issues of vital importance to a free society." Nuckols argued that Wolf "has for many years now been claiming that a fascist coup in America is imminent. ... [I]n "The Guardian" she alleged, with no substantiation, that the U.S. government and big American banks are conspiring to impose a 'totally integrated corporate-state repression of dissent'."

"Vox" journalist Max Fisher urged Wolf's readers "to understand the distinction between her earlier work, which rose on its merits, and her newer conspiracy theories, which are unhinged, damaging, and dangerous."

Charles C. W. Cooke wrote in the "National Review Online",
Over the last eight years, Naomi Wolf has written hysterically about coups and about vaginas and about little else besides. She has repeatedly insisted that the country is on the verge of martial law, and transmogrified every threat—both pronounced and overhyped—into a government-led plot to establish a dictatorship. She has made prediction after prediction that has simply not come to pass. Hers are not sober and sensible forecasts of runaway human nature, institutional atrophy, and constitutional decline, but psychedelic fever-dreams that are more typically suited to the "InfoWars" crowd.

Under the headline "Naomi Wolf Went Off the Deep End Long Ago", Aaron Goldstein in "The American Spectator" advised, "Her words must be taken not just with a grain of salt, but a full shaker's worth."

Shortly after the WikiLeaks founder Julian Assange was arrested in 2010, she wrote in an article for "The Huffington Post" that the allegations made against him by his two reputed victims amounted to no more than bad manners from a boyfriend. His accusers, she later wrote in several contexts, were working for the CIA and Assange had been falsely incriminated.

On December 20, 2010, "Democracy Now!" featured a debate between Wolf and Jaclyn Friedman on the Assange case. According to Wolf, the alleged victims should have said no, asserted that they consented to having sex with him, and said the claims were politically motivated and demeaned the cause of legitimate rape victims. In a 2011 "Guardian" article she objected to Assange's two accusers having their anonymity preserved. In response, Katha Pollitt wrote in "The Nation" that the "point is a little bizarre: doesn’t Wolf realize that anonymity applies only to the media? Everyone in the justice system knows who the complainants are."

On October 18, 2011, Wolf was arrested and detained in New York during the Occupy Wall Street protests, having ignored a police warnings not to remain on the street in front of a building. Wolf spent about 30 minutes in a cell. She disputed the NYPD's interpretation of applicable laws: "I was taken into custody for disobeying an unlawful order. The issue is that I actually know New York City permit law ... I didn't choose to get myself arrested. I chose to obey the law and that didn't protect me."

A month later, Wolf argued in "The Guardian", citing leaked documents, that attacks on the Occupy movement were a coordinated plot, orchestrated by federal law enforcement agencies. Those leaks, she alleged, showed that the FBI was privately treating OWS as a terrorist threat, rather than the public assertions acknowledging it is a peaceful organization. The response to this article ranged from praise to criticism of Wolf for being overly speculative and creating a "conspiracy theory". Wolf responded that there is ample evidence for her argument, and proceeded to review the information available to her at the time of the article, and what she alleged was new evidence since that time.

Imani Gandy of Balloon Juice, wrote that "nothing substantiates Wolf's claims", that "Wolf's article has no factual basis whatsoever and is, therefore, a journalistic failure of the highest order" and that "it was incumbent upon (Wolf) to fully research her claims and to provide facts to back them up." Corey Robin, a political theorist, journalist, and associate professor of political science at Brooklyn College and the Graduate Center of the City University of New York, stated on his blog: "The reason Wolf gets her facts wrong is that she's got her theory wrong."

In early 2012, WikiLeaks began publishing the Global Intelligence Files, a trove of e-mails obtained via a hack by Anonymous and Jeremy Hammond. Among them was an email with an official Department of Homeland Security document from October 2011 attached. It indicated that DHS was closely watching Occupy, and concluded, "While the peaceful nature of the protests has served so far to mitigate their impact, larger numbers and support from groups such as Anonymous substantially increase the risk for potential incidents and enhance the potential security risk to critical infrastructure." In late December 2012, FBI documents released following an FOIA request from the Partnership for Civil Justice Fund revealed that the FBI used counterterrorism agents and other resources to extensively monitor the national Occupy movement. The documents contained no references to agency personnel covertly infiltrating Occupy branches, but did indicate that the FBI gathered information from police departments and other law enforcement agencies relating to planned protests. Additionally, the blog Techdirt reported that the documents disclosed a plot by unnamed parties "to murder OWS leadership in Texas" but that "the FBI never bothered to inform the targets of the threats against their lives."

In a December 2012 article for "The Guardian", Wolf wrote:
"Mother Jones" claimed that none of the documents revealed efforts by federal law enforcement agencies to disband the Occupy camps, and that the documents did not provide much evidence that federal officials attempted to suppress protesters' free speech rights. It was, said "Mother Jones", "a far cry from Wolf's contention."

In June 2013, "New York" magazine reported Wolf, in a recent Facebook post, had expressed her "creeping concern" that NSA leaker Edward Snowden "is not who he purports to be, and that the motivations involved in the story may be more complex than they appear to be." Wolf was similarly skeptical of Snowden's "very pretty pole-dancing Facebooking girlfriend who appeared for, well, no reason in the media coverage ... and who keeps leaking commentary, so her picture can be recycled in the press." She pondered whether he was planted by "the Police State".

Wolf responded on her website: "I do find a great deal of media/blog discussion about serious questions such as those I raised, questions that relate to querying some sources of news stories, and their potential relationship to intelligence agencies or to other agendas that may not coincide with the overt narrative, to be extraordinarily ill-informed and naive." Specifically regarding Snowden, she wrote, "Why should it be seen as bizarre to wonder, if there are some potential red flags—the key term is 'wonder'—if a former NSA spy turned apparent whistleblower might possibly still be—working for the same people he was working for before?"

She was accused by the "Salon" website of making factual errors and misreadings.

In a series of Facebook postings in October 2014, Wolf questioned the authenticity of videos purporting to show beheadings of two American journalists and two Britons by the Islamic State implying that they had been staged by the U.S. government and that the victims and their parents were actors. Wolf also charged that the U.S. was dispatching military troops not to assist in treating the Ebola virus epidemic in West Africa, but to carry the disease back home to justify a military takeover of America. She further said that the Scottish independence referendum, in which Scots voted to remain in the United Kingdom, was faked. Speaking about this at a demonstration in Glasgow on October 12, Wolf said, "I truly believe it was rigged."

Responding to such criticism, Wolf said, "All the people who are attacking me right now for 'conspiracy theories' have no idea what they are talking about ... people who assume the dominant narrative MUST BE TRUE and the dominant reasons MUST BE REAL are not experienced in how that world works." To her nearly 100,000 Facebook followers, Wolf maintained, "I stand by what I wrote." However, in a later Facebook post, Wolf retracted her statement: "I am not asserting that the ISIS videos have been staged", she wrote.
I certainly sincerely apologize if one of my posts was insensitively worded. I have taken that one down. ... I am not saying the ISIS beheading videos are not authentic. I am not saying they are not records of terrible atrocities. I am saying that they are not yet independently confirmed by two sources as authentic, which any Journalism School teaches, and the single source for several of them, SITE, which received half a million dollars in government funding in 2004, and which is the only source cited for several, has conflicts of interest that should be disclosed to readers of news outlets. Max Fisher commented that "the videos were widely distributed on open-source jihadist online outlets" while the "Maryland-based nonprofit SITE monitors extremist social media." Wolf deleted her original Facebook posts.

Wolf's first marriage was to journalist David Shipley, then an editor at "The New York Times". The couple had two children, a son and daughter. Wolf and Shipley divorced in 2005.

On 23 November 2018, Wolf married Brian William O'Shea, a disabled U.S. Army Veteran, private detective, and owner of Striker Pierce Investigations. According to a "New York Times" article published in November 2018, Wolf and O'Shea met in 2014 due to threats against Wolf after reporting on human rights violations in the Middle East. The couple live in New York City.

In 2004, in an article for "New York" magazine, Wolf accused literary scholar Harold Bloom of a "sexual encroachment" in late Fall 1983 by touching her inner thigh. She said that what she alleged Bloom did was not harassment, either legally or emotionally, and she did not think herself a "victim", but that she had harbored this secret for 21 years. Explaining why she had finally gone public with the charges, Wolf wrote,

I began, nearly a year ago, to try—privately—to start a conversation with my alma mater that would reassure me that steps had been taken in the ensuing years to ensure that unwanted sexual advances of this sort weren't still occurring. I expected Yale to be responsive. After nine months and many calls and e-mails, I was shocked to conclude that the atmosphere of collusion that had helped to keep me quiet twenty years ago was still intact—as secretive as a Masonic lodge. Sexual encroachment in an educational context or a workplace is, most seriously, a corruption of meritocracy; it is in this sense parallel to bribery. I was not traumatized personally, but my educational experience was corrupted. If we rephrase sexual transgression in school and work as a civil-rights and civil-society issue, everything becomes less emotional, less personal. If we see this as a systemic corruption issue, then when people bring allegations, the focus will be on whether the institution has been damaged in its larger mission.

In "Slate" magazine around the time the allegations against Bloom first surfaced, Meghan O'Rourke wrote that Wolf generalized about sexual assault at Yale on the basis of her alleged personal experience. Moreover, O'Rourke commented, that despite Wolf's assertion sexual assault existed at Yale, she did not interview any Yale students for her story. In addition, O'Rourke wrote, "She jumps through verbal hoops to make it clear she was not 'personally traumatized,' yet she spends paragraphs describing the incident in precisely those terms." O'Rourke wrote that, despite Wolf's claim that her educational experience was corrupted, "(s)he neglects to mention that she later was awarded a Rhodes (scholarship)." O'Rourke concluded Wolf's "gaps and imprecision" in the "New York" article "give fodder to skeptics who think sexual harassment charges are often just a form of hysteria."

Separately, a formal complaint was filed with the U.S. Department of Education Office for Civil Rights on March 15, 2011, by 16 current and former Yale students—12 female and 4 male—describing a sexually hostile environment at Yale. A federal investigation of Yale University began in March 2011 in response to the complaints. Wolf stated on CBS's "The Early Show" in April: "Yale has been systematically covering up much more serious crimes than the ones that can be easily identified." More specifically, she alleged "they use the sexual harassment grievance procedure in a very cynical way, purporting to be supporting victims, but actually using a process to stonewall victims, to isolate them, and to protect the university." Yale settled the federal complaint in June 2012, acknowledging "inadequacies" but not facing "disciplinary action with the understanding that it keeps in place policy changes instituted after the complaint was filed. The school (was) required to report on its progress to the Office of Civil Rights until May, 2014."

In January 2018, Wolf accused Yale officials of blocking her from filing a formal grievance against Bloom. She told "The New York Times" that she had attempted to file the complaint in 2015 with Yale's University-Wide Committee on Sexual Misconduct, but that the university had refused to accept it. On January 16, 2018, Wolf said, she determined to see Yale's provost, Ben Polak, in another attempt to present her case. "As she documented on Twitter," the newspaper reported, "she brought a suitcase and a sleeping bag, because she said she did not know how long she would have to stay. When she arrived at the provost's office, she said, security guards prevented her from entering any elevators. Eventually, she said, Aley Menon, the secretary of the sexual misconduct committee, appeared and they met in the committee's offices for an hour, during which she gave Ms. Menon a copy of her complaint." This was reported and confirmed by Norman Vanamee who apparently met Wolf at Yale on this morning. In "Town & Country" magazine in January 2018, Vanamee returned to the story and wrote, "Yale University has a 93-person police department, and, after the guard called for backup, three of its armed and uniformed officers appeared and stationed themselves between Wolf and the elevator bank."

During an interview for "Time" magazine in spring 2015, Bloom denied ever being indoors with "this person" whom he referred to as "Dracula's daughter."




</doc>
<doc id="21637" url="https://en.wikipedia.org/wiki?curid=21637" title="New Year">
New Year

New Year is the time or day at which a new calendar year begins and the calendar's year count increments by one.

Many cultures celebrate the event in some manner and the 1st day of January is often marked as a national holiday.

In the Gregorian calendar, the most widely used calendar system today, New Year occurs on January 1 (New Year's Day). This was also the first day of the year in the original Julian calendar and of the Roman calendar (after 153 BC).. 

During the Middle Ages in western Europe, while the Julian calendar was still in use, authorities moved New Year's Day, depending upon locale, to one of several other days, including March 1, March 25, Easter, September 1, and December 25. Beginning in 1582, the adoptions of the Gregorian calendar has meant that many national or local dates in the Western World and beyond have changed to using one fixed date for New Year's Day, January 1. 

Other cultures observe their traditional or religious New Years Day according to their own customs, sometimes in addition to a (Gregorian) civil calendar. Chinese New Year, the Islamic New Year, the traditional Japanese New Year and the Jewish New Year are the more well-known examples. India and other countries continue to celebrate New Year on different dates. 







The new year of many South and Southeast Asian calendars falls between April 13–15, marking the beginning of spring.






The early development of the Christian liturgical year coincided with the Roman Empire (east and west), and later the Byzantine Empire, both of which employed a taxation system labeled the Indiction, the years for which began on September 1. This timing may account for the ancient church's establishment of September 1 as the beginning of the liturgical year, despite the official Roman New Year's Day of January 1 in the Julian calendar, because the indiction was the principal means for counting years in the empires, apart from the reigns of the Emperors. The September 1 date prevailed throughout all of Christendom for many centuries, until subsequent divisions eventually produced revisions in some places.

After the sack of Rome in 410, communications and travel between east and west deteriorated. Liturgical developments in Rome and Constantinople did not always match, although a rigid adherence to form was never mandated in the church. Nevertheless, the principal points of development were maintained between east and west. The Roman and Constantinopolitan liturgical calendars remained compatible even after the East-West Schism in 1054. Separations between the Roman Catholic ecclesiastical year and Eastern Orthodox liturgical calendar grew only over several centuries' time.

During those intervening centuries, the Roman Catholic ecclesiastic year was moved to the first day of Advent, the Sunday nearest to St. Andrew's Day (November 30). According to the Latin Rite of the Catholic Church, the liturgical year begins at 4:00 PM on Saturday preceding the fourth Sunday prior to December 25 (between November 26 and December 2). By the time of the Reformation (early 16th century), the Roman Catholic general calendar provided the initial basis for the calendars for the liturgically-oriented Protestants, including the Anglican and Lutheran Churches, who inherited this observation of the liturgical new year.

The present-day Eastern Orthodox liturgical calendar is the virtual culmination of the ancient eastern development cycle, though it includes later additions based on subsequent history and lives of saints. It still begins on September 1, proceeding annually into the Nativity of the Theotokos (September 8) and Exaltation of the Cross (September 14) to the celebration of Nativity of Christ (Christmas), through his death and resurrection (Pascha/Easter), to his Ascension and the Dormition of the Theotokos ("falling asleep" of the Virgin Mary, August 15). This last feast is known in the Roman Catholic church as the Assumption. The dating of "September 1" is according to the "new" (revised) Julian calendar or the "old" (standard) Julian calendar, depending on which is used by a particular Orthodox Church. Hence, it may fall on 1 September on the civil calendar, or on 14 September (between 1900 and 2099 inclusive).

The Coptic and Ethiopian liturgical calendars are unrelated to these systems but instead follow the Alexandrian calendar which fixed the wandering ancient Egyptian calendar to the Julian year. Their New Year celebrations on Neyrouz and Enkutatash were fixed; however, at a point in the Sothic cycle close to the Indiction, between the years 1900 and 2100, they fall on September 11 during most years and September 12 in the years before a leap year.

During the Roman Republic and the Roman Empire years beginning on the date on which each consul first entered the office. This was probably May 1 before 222 BC, March 15 from 222 BC to 154 BC, and January 1 from 153 BC. In 45 BC, when Julius Caesar's new Julian calendar took effect, the Senate fixed January 1 as the first day of the year. At that time, this was the date on which those who were to hold civil office assumed their official position, and it was also the traditional annual date for the convening of the Roman Senate. This civil new year remained in effect throughout the Roman Empire, east and west, during its lifetime and well after, wherever the Julian calendar continued in use.

In England, the Angle, Saxon, and Viking invasions of the fifth through tenth centuries plunged the region back into pre-history for a time. While the reintroduction of Christianity brought the Julian calendar with it, its use was primarily in the service of the church to begin with. After William the Conqueror became king in 1066, he ordered that January 1 be re-established as the civil New Year. Later, however, England and Scotland joined much of Europe to celebrate the New Year on March 25.

In the Middle Ages in Europe a number of significant feast days in the ecclesiastical calendar of the Roman Catholic Church came to be used as the beginning of the Julian year:


Southward equinox day (usually September 22) was "New Year's Day" in the French Republican Calendar, which was in use from 1793 to 1805. This was "primidi Vendémiaire", the first day of the first month.

It took quite a long time before January 1 again became the universal or standard start of the civil year. The years of adoption of 1 January as the new year are as follows:

March 1 was the first day of the numbered year in the Republic of Venice until its destruction in 1797, and in Russia from 988 until 1492 (Anno Mundi 7000 in the Byzantine calendar). September 1 was used in Russia from 1492 (A.M. 7000) until the adoption of the Anno Domini notation in 1700 via a December 1699 decree of Tsar Peter I.

Because of the division of the globe into time zones, the new year moves progressively around the globe as the start of the day ushers in the New Year. The first time zone to usher in the New Year, just west of the International Date Line, is located in the Line Islands, a part of the nation of Kiribati, and has a time zone 14 hours ahead of UTC. All other time zones are 1 to 25 hours behind, most in the previous day (December 31); on American Samoa and Midway, it is still 11 PM on December 30. These are among the last inhabited places to observe New Year. However, uninhabited outlying U.S. territories Howland Island and Baker Island are designated as lying within the time zone 12 hours behind UTC, the last places on earth to see the arrival of January 1. These small coral islands are found about midway between Hawaii and Australia, about 1,000 miles west of the Line Islands. This is because the International Date Line is a composite of local time zone arrangements, which winds through the Pacific Ocean, allowing each locale to remain most closely connected in time with the nearest or largest or most convenient political and economic locales with which each associate. By the time Howland Island sees the new year, it is 2 AM on January 2 in the Line Islands of Kiribati.


</doc>
<doc id="21638" url="https://en.wikipedia.org/wiki?curid=21638" title="Northern Territory">
Northern Territory

The Northern Territory (NT; formally the Northern Territory of Australia) is an Australian territory in the central and central northern regions of Australia. It shares borders with Western Australia to the west (129th meridian east), South Australia to the south (26th parallel south), and Queensland to the east (138th meridian east). To the north, the territory looks out to the Timor Sea, the Arafura Sea and the Gulf of Carpentaria, including Western New Guinea and other islands of the Indonesian archipelago. 

The NT covers , making it the third-largest Australian federal division, and the 11th-largest country subdivision in the world. It is sparsely populated, with a population of only 244,761, fewer than half as many people as Tasmania.

The archaeological history of the Northern Territory may have begun over 60,000 years ago when humans first settled this region of the Sahul Continent. Reportedly the Makassan traders began a relationship with the indigenous people of the Northern Territory around the trading of trepang from at least the 18th century. The coast of the territory was first seen by Europeans in the 17th century. The British were the first Europeans to attempt to settle the coastal regions. After three failed attempts to establish a settlement (1824–28, 1838–49, and 1864–66), success was achieved in 1869 with the establishment of a settlement at Port Darwin. 

The economy is based largely on mining and petroleum, which during 2018–2019 contributed 23% of the Gross State Product, or $5.68 billion, accounting for 92.4% of exports.

The capital and largest city is Darwin. The population is concentrated in coastal regions and along the Stuart Highway. The other major settlements are (in order of size) Palmerston, Alice Springs, Katherine, Nhulunbuy and Tennant Creek. Residents of the Northern Territory are often known simply as "Territorians" and fully as "Northern Territorians", or more informally as "Top Enders" and "Centralians".

Humans have lived in the present area of the Northern Territory since at least 48.4 to 68.7 thousand years ago and for at least the last five centuries of that time, extensive seasonal trade links are said to have existed between the indigenous peoples of this area and what is now Indonesia.

With the coming of the British, there were four early attempts to settle the harsh environment of the northern coast, of which three failed in starvation and despair. The land now occupied by the Northern Territory was part of colonial New South Wales from 1825 to 1863, except for a brief time from February to December 1846, when it was part of the short-lived colony of North Australia. The Northern Territory was part of South Australia from 1863 to 1911. Under the administration of colonial South Australia, the overland telegraph was constructed between 1870 and 1872.

From its establishment in 1869 the Port of Darwin was the major Territory supply for many decades.

A railway was built between Palmerston and Pine Creek between 1883 and 1889. The economic pattern of cattle raising and mining was established so that by 1911 there were 513,000 cattle. Victoria River Downs was at one time the largest cattle station in the world. Gold was found at Grove Hill in 1872 and at Pine Creek, Brocks Creek, Burundi, and copper was found at Daly River.

On 1 January 1911, a decade after federation, the Northern Territory was separated from South Australia and transferred to federal control. Alfred Deakin opined at this time "To me the question has been not so much commercial as national, first, second, third and last. Either we must accomplish the peopling of the northern territory or submit to its transfer to some other nation."

In late 1912 there was growing sentiment that the name "Northern Territory" was unsatisfactory. The names "Kingsland" (after King George V and to correspond with Queensland), "Centralia" and "Territoria" were proposed with Kingsland becoming the preferred choice in 1913. However, the name change never went ahead.

For a brief time between 1927 and 1931 the Northern Territory was divided into North Australia and Central Australia at the 20th parallel of South latitude. Soon after this time, parts of the Northern Territory were considered in the Kimberley Plan as a possible site for the establishment of a Jewish Homeland, understandably considered the "Unpromised Land".

During World War II, most of the Top End was placed under military government. This is the only time since Federation that part of an Australian state or territory has been under military control. After the war, control for the entire area was handed back to the Commonwealth. The Bombing of Darwin occurred on 19 February 1942. It was the largest single attack ever mounted by a foreign power on Australia. Evidence of Darwin's World War II history is found at a variety of preserved sites in and around the city, including ammunition bunkers, airstrips, oil tunnels and museums. The port was damaged in the 1942 Japanese air raids. It was subsequently restored.

In the late 1960s improved roads in adjoining States linking with the territory, port delays and rapid economic development led to uncertainty in port and regional infrastructure development. As a result of the Commission of Enquiry established by the Administrator, port working arrangements were changed, berth investment deferred and a port masterplan prepared. Extension of rail transport was then not considered because of low freight volumes.

Indigenous Australians had struggled for rights to fair wages and land. An important event in this struggle was the strike and walk off by the Gurindji people at Wave Hill Cattle Station in 1966. The federal government of Gough Whitlam set up the Woodward Royal Commission in February 1973, which set to enquire into how land rights might be achieved in the Northern Territory. Justice Woodward's first report in July 1973 recommended that a Central Land Council and a Northern Land Council be established to present to him the views of Aboriginal people. In response to the report of the Royal Commission a Land Rights Bill was drafted, but the Whitlam Government was dismissed before it was passed.

The Aboriginal Land Rights (Northern Territory) Act 1976 was eventually passed by the Fraser government on 16 December 1976 and began operation on 26 January 1977).

In 1974, from Christmas Eve to Christmas Day, Darwin was devastated by tropical Cyclone Tracy. Cyclone Tracy killed 71 people, caused A$837 million in damage (1974 dollars), or approximately A$6.85 billion (2018 dollars), and destroyed more than 70 per cent of Darwin's buildings, including 80 per cent of houses. Tracy left more than 41,000 out of the 47,000 inhabitants of the city homeless. The city was rebuilt with much-improved construction codes and is a modern, landscaped metropolis today.

In 1978 the territory was granted responsible government, with a Legislative Assembly headed by a chief minister. The territory also publishes official notices in its own "Government Gazette". The administrator of the Northern Territory is an official acting as the Queen's "indirect" representative in the territory.

During 1995–96 the Northern Territory was briefly one of the few places in the world with legal voluntary euthanasia, until the Federal Parliament overturned the legislation.
Before the over-riding legislation was enacted, four people used the law supported by Dr. Philip Nitschke.

There are many very small settlements scattered across the territory, but the larger population centres are located on the single paved road that links Darwin to southern Australia, the Stuart Highway, known to locals simply as "the track".

The Northern Territory is home to two spectacular natural rock formations, Uluru (Ayers Rock) and Kata Tjuta (The Olgas), which are sacred to the local Aboriginal peoples and which have become major tourist attractions.

The northern portion of the territory is principally tropical savannas, composed of several distinct ecoregions – Arnhem Land tropical savanna, Carpentaria tropical savanna, Kimberley tropical savanna, Victoria Plains tropical savanna, and Mitchell Grass Downs. The southern portion of the territory is covered in deserts and xeric shrublands, including the Great Sandy-Tanami desert, Simpson Desert, and Central Ranges xeric scrub.

In the northern part of the territory lies Kakadu National Park, which features extensive wetlands and native wildlife. To the north of that lies the Arafura Sea, and to the east lies Arnhem Land, whose regional centre is Maningrida on the Liverpool River delta.
There is an extensive series of river systems in the Northern Territory. These rivers include: the Alligator Rivers, Daly River, Finke River, McArthur River, Roper River, Todd River and Victoria River. The Hay River is a river south-west of Alice Springs, with the Marshall River, Arthur Creek, Camel Creek and Bore Creek flowing into it.


The Northern Territory has two distinctive climate zones.

The northern end, including Darwin, has a tropical climate with high humidity and two seasons, the wet (October to April) and dry season (May to September). During the dry season nearly every day is warm and sunny, and afternoon humidity averages around 30%. There is very little rainfall between May and September. In the coolest months of June and July, the daily minimum temperature may dip as low as , but very rarely lower, and frost has never been recorded.

The wet season is associated with tropical cyclones and monsoon rains. The majority of rainfall occurs between December and March (the southern hemisphere summer), when thunderstorms are common and afternoon relative humidity averages over 70% during the wettest months. On average more than of rain falls in the north. Rainfall is highest in north-west coastal areas, where rainfall averages from .

The central region is the desert centre of the country, which includes Alice Springs and Uluru (Ayers Rock), and is semi-arid with little rain usually falling during the hottest months from October to March. Seasons are more distinct in central Australia, with very hot summers and cool winters. Frost is recorded a few times a year. The region receives less than of rain per year.

The highest temperature recorded in the territory was at Finke on 1 and 2 January 1960. The lowest temperature was at Alice Springs on 17 July 1976.

The Northern Territory Parliament is one of the three unicameral parliaments in the country. Based on the Westminster System, it consists of the Northern Territory Legislative Assembly which was created in 1974, replacing the Northern Territory Legislative Council. It also produces the "Northern Territory of Australia Government Gazette".

The Northern Territory Legislative Council was the partly elected governing body from 1947 until its replacement by the fully elected Northern Territory Legislative Assembly in 1974. The total enrolment for the 1947 election was 4,443. The Northern Territory was split into five electorates: Darwin, Alice Springs, Tennant Creek, Batchelor, and Stuart.

While this assembly exercises powers similar to those of the parliaments of the states of Australia, it does so by legislated devolution of powers from the Commonwealth Government, rather than by any constitutional right. As such, the Commonwealth Government retains the right to legislate for the territory, including the power to override legislation passed by the Legislative Assembly. The Monarch is represented by the Administrator of the Northern Territory, who performs a role similar to that of a state governor.

Twenty-five members of the Legislative Assembly are elected to four-year terms from single-member electorates.

For some years there has been agitation for full statehood. A referendum of voters in the Northern Territory was held on the issue in 1998, which resulted in a 'no' vote. This was a shock to both the Northern Territory and Commonwealth governments, as opinion polls showed most Territorians supported statehood. But under the Australian Constitution, the federal government may set the terms of entry to full statehood. The Northern Territory was offered three senators, rather than the 12 guaranteed to original states. (Because of the difference in populations, equal numbers of Senate seats would mean a Territorian's vote for a senator would have been worth more than 30 votes in New South Wales or Victoria.) Alongside what was cited as an arrogant approach adopted by then chief minister Shane Stone, it is believed that most Territorians, regardless of their general views on statehood, were reluctant to adopt the particular offer that was made.

The chief minister is the head of government of a self-governing territory (the head of a state government is a "premier"). The chief minister is appointed by the administrator, who in normal circumstances appoints the leader of whichever party holds the majority of seats in the Northern Territory Legislative Assembly. The current chief minister is Michael Gunner of the Australian Labor Party. He replaced Adam Giles on 31 August 2016.

The Northern Territory became self-governing on 1 July 1978 under its own administrator appointed by the Governor-General of Australia. The federal government, not the NT government, advises the governor-general on the appointment of the administrator, but by convention consults first with the Territory government. The current administrator is Vicki O'Halloran.

The Northern Territory is represented in the federal parliament by two members in the House of Representatives and two members in the Senate. , resulting from the 2019 federal election, Warren Snowdon from the Australian Labor Party (ALP) and Luke Gosling from the Australian Labor Party (ALP) serve in the House of Representatives, and Malarndirri McCarthy from the ALP and Sam McMahon from the Country Liberal Party serve in the Senate.

The Northern Territory is divided into 17 local government areas, including 11 shires and five municipalities. Shire, city and town councils are responsible for functions delegated by the Northern Territory parliament, such as road infrastructure and waste management. Council revenue comes mostly from property taxes and government grants.

Aboriginal land councils in the Northern Territory are groups of Aboriginal landowners, set up under the "Aboriginal Land Rights Act 1976".

The two historically dominant political parties in the Northern Territory are the conservative Country Liberal Party, and the social-democratic Australian Labor Party. The Territory Alliance surpassed the CLP in terms of parliamentary seats in 2020. Minor parties that are also active in the NT include the Northern Territory Greens, the Shooters and Fishers Party and various others. It is common for independent politicians to win election.

The population of the Northern Territory at the 2011 Australian census was 211,945, a 10 per cent increase from the 2006 census. The Australian Bureau of Statistics estimated a June 2015 resident population of 244,300, taking into account residents overseas or interstate. The territory's population represents 1% of the total population of Australia.

The Northern Territory's population is the youngest in Australia and has the largest proportion (23.2%) under 15 years of age and the smallest proportion (5.7%) aged 65 and over. The median age of residents of the Northern Territory is 31 years, six years younger than the national median age.

Indigenous Australians own some 49% of the land. The life expectancy of Aboriginal Australians is well below that of non-Indigenous Australians in the Northern Territory, a fact that is mirrored elsewhere in Australia. ABS statistics suggest that Indigenous Australians die about 11 years earlier than the average non-Indigenous Australian. There are Aboriginal communities in many parts of the territory, the largest ones being the Pitjantjatjara near Uluru, the Arrernte near Alice Springs, the Luritja between those two, the Warlpiri further north, and the Yolngu in eastern Arnhem Land.

More than 54% of Territorians live in Darwin, located in the territory's north (Top End). Less than half of the territory's population live in the rural Northern Territory. Despite this, the Northern Territory is the least urbanised federal division in the Commonwealth (followed by Tasmania).

Not all communities are incorporated cities, or towns. They are referred to as "Statistical Local Areas."
At the 2016 census, the most commonly nominated ancestries were: 
31.2% of the population was born overseas at the 2016 census. The five largest groups of overseas-born were from the Philippines (2.6%), England (2.4%), New Zealand (2%), India (1.6%) and Greece (0.6%).

25.5% of the population, or 58,248 people, identified as Indigenous Australians (Aboriginal Australians and Torres Strait Islanders) in 2016.

At the 2016 census, 58% of the population spoke only English at home. The other languages most commonly spoken at home were Kriol (1.9%), Djambarrpuyngu (1.9%), Greek (1.4%) Tagalog (1.3%), and Warlpiri (0.9%).

There are more than 100 Aboriginal languages and dialects spoken in the Northern Territory, in addition to English which is most common in cities such as Darwin or Alice Springs. Major indigenous languages spoken in the Northern Territory include Murrinh-patha and Ngangikurrungurr in the northwest around Wadeye, Warlpiri and Warumungu in the centre around Tennant Creek, Arrernte around Alice Springs, Pintupi-Luritja to the south east, Pitjantjatjara in the south near Uluru, Yolngu Matha to the far north in Arnhem Land (where the dialect Djambarrpuyngu of Dhuwal is considered a lingua franca), and Burarra, Maung, Iwaidja and Kunwinjku in the centre north and on Croker Island and the Goulburn Islands. Tiwi is spoken on Melville Island and Bathurst Island. Literature in many of these languages is available in the Living Archive of Aboriginal Languages.

In the 2016 census Roman Catholics form the single largest religious group in the territory with 19.9% of the Northern Territory's population, followed by Anglican (8.4%), Uniting Church (5.7%) and Lutheran (2.6%). Buddhism is the territory's largest non-Christian religion (2.0%), followed by Hinduism (1.6%), which is the fastest growing religion population percentage wise in the state. Australian Aboriginal religion and mythology (1.4%) is also practiced. Around 30% of Territorians do not profess any religion.

Many Aborigines practise their traditional religion, their belief in the Dreamtime.

A Northern Territory school education consists of six years of primary schooling, including one transition year, three years of middle schooling, and three years of secondary schooling. In the beginning of 2007, the Northern Territory introduced Middle School for Years 7–9 and High School for Years 10–12. Northern Territory children generally begin school at age five. On completing secondary school, students earn the Northern Territory Certificate of Education (NTCE). Students who successfully complete their secondary education also receive a tertiary entrance ranking, or ATAR score, to determine university admittance.

Northern Territory schools are either publicly or privately funded. Public schools, also known as state or government schools, are funded and run directly by the Department of Education. Private fee-paying schools include schools run by the Catholic Church and independent schools, some elite ones similar to English public schools. Some Northern Territory Independent schools are affiliated with Protestant, Lutheran, Anglican, Greek Orthodox or Seventh-day Adventist Churches, but include non-church schools and an Indigenous school.

As of 2009, the Northern Territory had 151 public schools, 15 Catholic schools and 21 independent schools. 39,492 students were enrolled in schools around the territory with 29,175 in public schools, and 9,882 in independent schools. The Northern Territory has about 4,000 full-time teachers.

The Northern Territory has one university which opened in 1989 under the name of the Northern Territory University. Now renamed as the Charles Darwin University, it had about 19,000 students enrolled: about 5,500 higher education students and about 13,500 students on vocational education and training (VET) courses. The first tertiary institution in the territory was the Batchelor Institute of Indigenous Tertiary Education which was established in the mid-1960s.

The Northern Territory Library is the territory's research and reference library. It is responsible for collecting and preserving the Northern Territory documentary heritage and making it available through a range of programs and services. Material in the collection includes books, newspapers, magazines, journals, manuscripts, maps, pictures, objects, sound and video recordings and databases.

The Northern Territory's economy is largely driven by mining, which is concentrated on energy producing minerals, petroleum and energy and contributes around $2.5 billion to the gross state product and employs over 4,600 people. Mining accounts for 14.9% of the gross state product in 2014–15 compared to just 7% nationally.

In recent years, largely due to the effect of major infrastructure projects and mine expansions, construction has overtaken mining as the largest single industry in the territory. Construction, mining and manufacturing, and government and community services, combine to account for about half of the territory's gross state product (GSP), compared to about a third of national gross domestic product (GDP).

The economy has grown considerably over the past decade, from a value of $15 billion in 2004–05 to over $22 billion in 2014–15. In 2012–13 the territory economy expanded by 5.6%, over twice the level of national growth, and in 2014–15 it grew by 10.5%, four times the national growth rate.

Between 2003 and 2006 the gross state product had risen from $8.67 billion to $11.476 billion and increase of 32.4%. During the three years to 2006–2007 the Northern Territory gross state product grew by an average annual rate of 5.5%. Gross state product per capita in the Northern Territory ($72,496) is higher than any Australian state or territory and is also higher than the gross domestic product per capita for Australia ($54,606).

The Northern Territory's exports were up 12.9% or $681 million in 2012–13. The largest contributor to the territory's exports was: mineral fuels (largely LNG), crude materials (mainly mineral ores) and food and live animals (primarily live cattle). The main international markets for territory exports are Japan, China, Indonesia, the United States and Korea.

Imports to the Northern Territory totalled $2,887.8 million which consisted of mainly machinery and equipment manufacturing (58.4%) and petroleum, coal, chemical and associated product manufacturing (17.0%).

The principal mining operations are bauxite at Gove Peninsula where the production is estimated to increase 52.1% to $254 million in 2007–08, manganese at Groote Eylandt, production is estimated to increase 10.5% to $1.1 billion which will be helped by the newly developed mines include Bootu Creek and Frances Creek, gold which is estimated to increase 21.7 per cent to $672 million at the Union Reefs plant and uranium at Ranger Uranium Mine.

Tourism is an important economic driver for the territory and a significant industry in regional areas. Iconic destinations such as Uluru and Kakadu make the Northern Territory a popular destination for domestic and international travellers. Diverse landscapes, waterfalls, wide open spaces, aboriginal culture and wild and untamed wildlife provides the opportunity for visitors to immerse themselves in the natural wonder that the Northern Territory offers. In 2015, the territory received a total of about 1.6 million domestic and international visitors contributing an estimated $2.0 billion to the local economy. Holiday visitors made up the majority of total visitation (about 792,000 visitors).

Tourism has strong links to other sectors in the economy including accommodation and food services, retail trade, recreation and culture, and transport.

The Northern Territory is the most sparsely populated state or territory in Australia.

The NT has a connected network of sealed roads, including two National Highways, linking with adjoining States and connecting the major Territory population centres, and other important centres such as Uluru (Ayers Rock), Kakadu and Litchfield National Parks. The Stuart Highway, once known as "The Track", runs north to south, connecting Darwin and Alice Springs to Adelaide. Some of the sealed roads are single lane bitumen. Many unsealed (dirt) roads connect the more remote settlements.

The Adelaide–Darwin railway, a new standard gauge railway, connects Adelaide via Alice Springs with Darwin, replacing earlier narrow gauge railways which had a gap between Alice Springs and Birdum. The Ghan passenger train runs from Darwin to Adelaide, stopping at Katherine, Tennant Creek, Alice Springs and Kulgera in the NT.

The Northern Territory was one of the few remaining places in the world with no speed restrictions on select public roads, until 21 November 2016. On 1 January 2007 a default speed limit of 110 km/h was introduced on roads outside of urban areas (Inside urban areas of 40, 50 or 60 km/h). Speeds of up to 130 km/h are permitted on some major highways, such as the Stuart Highway. On 1 February 2014, the speed limit was removed on a 204 km portion of the Stuart Highway for a one-year trial period. The maximum speed limit was changed to 130 km/h on 21 November 2016. Darwin International Airport is the major domestic and international airport for the territory. Several smaller airports are also scattered throughout the territory and are served by smaller airlines; including Alice Springs Airport, Ayers Rock Airport, Katherine Airport and Tennant Creek Airport.

The Northern Territory has only one daily tabloid newspaper, News Corporation's "Northern Territory News," or "NT News". "The Sunday Territorian" is the sister paper to the "NT News" and is the only dedicated Sunday tabloid newspaper in the Northern Territory.

The "Centralian Advocate" is circulated around the Alice Springs region twice a week. There are also five weekly community newspapers. The territory receives the national daily, "The Australian", while "The Sydney Morning Herald, The Age" and the "Guardian Weekly" are also available in Darwin. Katherine's paper is the "Katherine Times".

There is an LGBT community publication, QNews Magazine, which is published in Darwin and Alice Springs.

Metropolitan Darwin has had five broadcast television stations:
Darwin also has a single open-narrowcast station:

Regional Northern Territory has a similar availability of stations:

Remote areas are generally required to receive television via the Viewer Access Satellite Television service, which carries the same channels as the regional areas, as well as some extra open-narrowcast services, including Indigenous Community Television and Westlink.

Darwin has radio stations on both AM and FM frequencies. ABC stations include ABC NewsRadio (102.5FM), 105.7 ABC Darwin (8DDD 105.7FM), ABC Radio National (657AM), ABC Classic FM (107.3FM) and Triple J (103.3FM). The two commercial stations are Mix 104.9 (8MIX) and Hot 100 FM (8HOT).

The leading community stations are 104.1 Territory FM, and Radio Larrakia (8KNB).

The radio stations in Alice Springs are also broadcast on the AM and FM frequencies. ABC stations include Triple J (94.9FM), ABC Classic FM (97.9FM), 783 ABC Alice Springs (783AM) and ABC Radio National (99.7FM). There are two community stations in the town—CAAMA (100.5FM) and 8CCC (102.1FM). The commercial stations, which are both owned by the same company are Sun 96.9 (96.9FM) and 8HA (900AM). Two additional stations, Territory FM (98.7FM) and Radio TAB (95.9FM) are syndicated from Darwin and Brisbane, respectively.




</doc>
<doc id="21640" url="https://en.wikipedia.org/wiki?curid=21640" title="Low-alcohol beer">
Low-alcohol beer

Low-alcohol beer is beer with little or no alcohol content and aims to reproduce the taste of beer without (or at least reduce) the inebriating effects of standard alcoholic brews. Most low-alcohol beers are lagers, but there are some low-alcohol ales. Low-alcohol beer is also known as light beer, non-alcoholic beer, small beer, small ale, or near-beer.

Low-alcoholic brews such as small beer date back at least to Medieval Europe, where they served as a less risky alternative to water (which often was polluted by feces and parasites) and were less expensive than the full strength brews used at festivals.

More recently, the temperance movements and the need to avoid alcohol while driving, operating machinery, taking certain medications, etc. led to the development of non-intoxicating beers.

In the United States, non-alcoholic brews were promoted during Prohibition, according to John Naleszkiewicz. In 1917, President Wilson proposed limiting the alcohol content of malt beverages to 2.75% to try to appease avid prohibitionists. In 1919, Congress approved the Volstead Act, which limited the alcohol content of all beverages to 0.5%. These very low alcohol beverages became known as tonics, and many breweries began brewing them in order to stay in business during Prohibition. Since removing the alcohol from the beer requires just one simple extra step, many breweries saw it as an easy change. In 1933, when Prohibition was repealed, breweries easily removed this extra step.

By the 1980s and 1990s, growing concerns about alcoholism led to the growing popularity of "light" beers. In the 2010s, breweries have focused on marketing low-alcohol beers to counter the popularity of homebrew. Declining consumption has also led to the introduction of mass-market non-alcoholic beverages, dubbed as "near beer". Low-alcohol and alcohol-free bars and pubs have also started to open to cater for drinkers of non-alcoholic beverages, such as Scottish brewer BrewDog's London bar opened in early 2020.

In the UK, the introduction of a lower rate of beer duty for low-strength beer (of 2.8% ABV or less) in October 2011 spurred many small brewers to revive old small beer styles and create higher-hopped craft beers at the lower alcohol level to be able to lower the cost of their beer to consumers.

At the start of the 21st century, alcohol-free beer has seen a rise in popularity in the Middle East (which now makes up a third of the market). One reason for this is that Islamic scholars issued fatawa which permitted the consumption of beer as long as large quantities could be consumed without getting drunk.

Positive features of non-alcoholic brews include the ability to drive after consuming several drinks, the reduction in alcohol-related illness, and less severe hangover symptoms.

Some common complaints about non-alcoholic brews include a loss of flavor, addition of one step in the brewing process, sugary taste, and a shorter shelf life. There are also legal implications. Some state governments, e.g. Pennsylvania, prohibit the sale of non-alcoholic brews to persons under the age of 21. A study conducted by the department of psychology at Indiana University said, "Because non-alcoholic beer provides sensory cues that simulate alcoholic beer, this beverage may be more effective than other placebos in contributing to a credible manipulation of expectancies to receive alcohol", making people feel "drunk" when physically they are not.

In the United States, beverages containing less than 0.5% alcohol by volume (ABV) were legally called non-alcoholic, according to the now-defunct Volstead Act. Because of its very low alcohol content, non-alcoholic beer may be legally sold to people under age 21 in many American states.

In the United Kingdom, Government guidance recommends the following descriptions for "alcohol substitute" drinks including alcohol-free beer. The use of these descriptions is voluntary:


In some parts of the European Union, beer must contain no more than 0.5% ABV if it is labelled "alcohol-free".

In Australia, the term "light beer" refers to any beer with less than 3.5% alcohol.

Light beers are beers with reduced caloric content compared to regular beer, and typically also have a lower alcoholic content, depending on the brand and where they are sold. The spelling "lite beer" is also commonly used. Light beers are manufactured by reducing the carbohydrate content, and secondarily by reducing the alcohol content, since both carbohydrates and alcohol contribute to the caloric content of beer.

Light beers are marketed primarily to drinkers who wish to manage their calorie intake. However, these beers are sometimes criticized for being less flavorful than full-strength beers, being "watered down" (whether in perception or in fact), and thus advertising campaigns for light beers generally advertise their retention of flavor.

In Australia, regular beers have approximately 4%-5% ABV, while reduced-alcohol beers have 2.2%–3.2%.

In Canada, a reduced-alcohol beer contains 2.6%–4.0% ABV, and an "extra-light" beer contains less than 2.5%.

In the United States, most mass-market light beer brands, including Bud Light, Coors Light, and Miller Lite, have 4.2% ABV, 16% less than ordinary beers from the same makers which are 5% ABV.

In Sweden, low alcohol beer is either 2.2%, 2.8% or 3.5%, and can be purchased in an ordinary supermarket whereas normal strength beers of above 3.5% must be purchased at "Systembolaget". Beer containing 2.8-3.5% ABV (called Folköl or "Peoples' Beer") may be legally sold in any convenience store to people over 18 years of age, whereas stronger beer may only be sold in state-run liquor stores to people older than 20. In addition, businesses selling food for on-premises consumption do not need an alcohol license to serve 3.5% beer. Virtually all major Swedish brewers, and several international ones, in addition to their full-strength beer, make 3.5% folköl versions as well. Beer below or equaling 2.25% ABV ("lättöl") is not legally subject to age restrictions; however, some stores voluntarily opt out from selling it to minors anyway.

Low-point beer, which is often known in the United States as "three-two beer" or "3 point 2 brew", is beer that contains 3.2% alcohol by weight (equivalent to about 4% ABV).

The term "low-point beer" is unique to the United States, where some states limit the sale of beer, but beers of this type are also available in countries (such as Sweden and Finland) that tax or otherwise regulate beer according to its alcohol content.

In the United States, 3.2 beer was the highest alcohol content beer allowed to be produced legally for nine months in 1933. As part of his New Deal, President Franklin D. Roosevelt signed the Cullen–Harrison Act that repealed the Volstead Act on March 22, 1933. In December 1933, the Twenty-first Amendment to the United States Constitution was passed, negating the federal government's power to regulate the sale of alcoholic beverages, though states retained the power to regulate.

After the repeal of Prohibition, a number of state laws prohibiting the sale of intoxicating liquors remained in effect. As these were repealed, they were first replaced by laws limiting the maximum alcohol content allowed for sale as 3.2 ABW. As of 2019, the states of Minnesota and Utah permit general establishments such as supermarket chains and convenience stores to sell only low-point beer; in the 2010s, Colorado, Kansas, and Oklahoma revised state laws to end this practice. In these states, all alcoholic beverages containing more than 3.2% alcohol by weight (ABW) must be sold from state-licensed liquor stores.

Missouri also has a legal classification for low-point beer, which it calls "nonintoxicating beer". Unlike Minnesota and Utah, Missouri does not limit supermarket chains and convenience stores to selling only low-point beer. Instead, Missouri's alcohol laws permit grocery stores, drug stores, gas stations, and even "general merchandise stores" (a term that Missouri law does not define) to sell any alcoholic beverage; consequently, 3.2% beer is rarely sold in Missouri.

Originally, "near beer" was a term for malt beverages containing little or no alcohol (less than 0.5% ABV), which were mass-marketed during Prohibition in the United States. Near beer could not legally be labeled as "beer" and was officially classified as a "cereal beverage". The public, however, almost universally called it "near beer".

The most popular "near beer" was Bevo, brewed by the Anheuser-Busch company. The Pabst company brewed "Pablo", Miller brewed "Vivo", and Schlitz brewed "Famo". Many local and regional breweries stayed in business by marketing their own near-beers. By 1921, production of near beer had reached over 300 million US gallons (1 billion L) a year (36 L/s).

A popular illegal practice was to add alcohol to near beer. The resulting beverage was known as "spiked beer" or "needle beer", so called because a needle was used to inject alcohol through the cork of the bottle or keg.

Food critic and writer Waverley Root described the common American near beer as "such a wishy-washy, thin, ill-tasting, discouraging sort of slop that it might have been dreamed up by a Puritan Machiavelli with the intent of disgusting drinkers with genuine beer forever."

Beginning in the late 2000s, the term "near beer" has been revived to refer to modern non-alcoholic beer. In the early 2010s, major breweries began experimenting with mass-market non-alcoholic beers to counter with declining alcohol consumption amid growing preference for craft beer, launching beverages like Anheuser-Busch's Budweiser Prohibition Brew, launched in 2016.

A drink similar to "near beer", "bjórlíki" was quite popular in Iceland before alcoholic beer was made legal in 1989. The Icelandic variant normally consisted of a shot of vodka added to a half-a-litre glass of light beer.

Small beer (also, small ale) is a beer/ale that contains very little alcohol. Sometimes unfiltered and porridge-like, it was a favored drink in Medieval Europe and colonial North America as opposed to the often polluted water and the expensive beer used for festivities. Small beer was also produced in households for consumption by children and servants at those occasions.

However, small beer/small ale can also refer to a beer made of the "second runnings" from a very strong beer (e.g., scotch ale) mash. These beers can be as strong as a mild ale, depending on the strength of the original mash. (Drake's 24th Anniversary Imperial Small Beer was expected to reach above 9.5% abv.) This was done as an economy measure in household brewing in England up to the 18th century and is still done by some homebrewers. One commercial brewery, San Francisco's Anchor Brewing Company, also produces their "Anchor Small Beer" using the second runnings from their Old Foghorn Barleywine. The term is also used for commercially produced beers which are thought to taste too weak.

The Middle East accounts for almost a third of worldwide sales of nonalcoholic and alcohol-free beer.

The market for nonalcoholic beer in Malaysia has been slow in comparison to other Muslim-majority countries, and as of 2015, the Malaysian government has not approved any nonalcoholic beers as halal.

In 2008, the sale of non-alcoholic beers in Iran continued its high performance with double-digit growth rates in both value and volume and is expected to more than double its total volume sales between 2008 and 2013.

Non alcoholic beer sales in India are relatively low.

North America is seeing a rise in non-alcoholic beer consumption. Former President George W. Bush, who was the presidential candidate who voters most wanted to have a beer with, and Vice President Mike Pence are known to drink non-alcoholic beer.

Spain is the main consumer and producer of low-alcohol beer in the European Union.

As of March 2020, sales of alcohol-free beer are up by 30% since 2016, with younger generations shunning alcoholic beverages.

With the global non-alcoholic beer market expected to double by 2024, there has been an increase in breweries producing the product. As more people lean towards non-alcoholic beverages for health reasons, social reasons, or just because they want to enjoy the taste of beer without the effects of alcohol, companies are producing beers that cater to these audiences.

Craft Non-Alcoholic Beer began to take off in early 2018, as beer companies slowed down on trying to put as high of an ABV% in their brews as possible, and started producing more sessionable beers. Some beers that are still classified as "alcoholic" can have an ABV of as low as 2.4%, and the companies producing these are still seeing sales.

With an ever growing health conscious market segment, breweries began to produce craft non-alcoholic beers with as little as 10 calories per can, so that those who crave beer can fulfill their cravings without breaking their health resolution.

Beers that are labeled "non-alcoholic" still contain a very small amount of alcohol. Thus, some US states require the purchaser to be of a legal drinking age. Exceptions include:

According to the Birmingham Beverage Company, the brewing process of traditional brews consists of eight basic steps, nine for brewing non-alcoholic brews.


Low-alcohol beer starts out as regular alcoholic beer, which is then processed to remove the alcohol.

Older processes simply heat the beer to evaporate most of the alcohol. Since alcohol is more volatile than water, as the beer is heated alcohol boils off first. The alcohol is allowed to escape and the remaining liquid becomes the product, essentially the opposite of the process used to make distilled beverages. Most modern breweries utilize vacuum evaporation to reduce the boiling temperature and maintain flavor. In essence, the beer is placed under a light vacuum to facilitate the alcohol molecules going into the gaseous phase. If a sufficient vacuum is applied, it is not necessary to "cook" the beer at a temperature that destroys the flavor. Some heat must nevertheless be supplied to counter the heat lost to enthalpy of vaporization.

A more modern alternative process uses reverse osmosis to avoid heating the product at all. Under pressure, the beer is passed through a polymeric filter with pores small enough that only alcohol and water (and a few volatile acids) can pass through. A syrupy mixture of complex carbohydrates and most of the flavor compounds are retained by the filter. Alcohol is distilled out of the filtered alcohol-water mix using conventional distillation methods. Adding the water and remaining acids back into the syrup left behind on the filter completes the process.

Sometimes beer is simply diluted with water to give the desired alcohol level.

The conversion from a traditional alcoholic beer to a non-alcoholic beer takes place after the seventh step and preceding the finishing step. The uncarbonated beer is heated up to its boiling point. Another method of removing the alcohol is to decrease the pressure so the alcohol boils at room temperature. This is the preferred method because raising the temperature this late in the brewing process can greatly affect the flavor of the brew. Another tip would be avoiding using sugar from maize; this simply increases the alcohol content without adding to the flavor or body of the beer.

Once the alcohol is removed, one proceeds with the normal finishing process in which the beer is carbonated and bottled.

Many low-alcohol beer brands incorporate the colour blue into the packaging design, including Becks Blue, Heineken 0.0%, Ožujsko Cool and Erdinger Alkoholfrei.



</doc>
<doc id="21641" url="https://en.wikipedia.org/wiki?curid=21641" title="Norman Foster, Baron Foster of Thames Bank">
Norman Foster, Baron Foster of Thames Bank

Norman Robert Foster, Baron Foster of Thames Bank, (born 1 June 1935), is an English architect whose company, Foster + Partners, maintains an international design practice.
He is the President of the Norman Foster Foundation. The Norman Foster Foundation promotes interdisciplinary thinking and research to help new generations of architects, designers and urbanists to anticipate the future. The foundation, which opened in June 2017, is based in Madrid and operates globally.

He is one of the most prolific British architects of his generation. In 1999, he was awarded the Pritzker Architecture Prize, often referred to as the Nobel Prize of architecture.

Norman Robert Foster was born in 1935 in Reddish, two miles north of Stockport, then a part of Cheshire. The only child of Robert and Lilian Foster ("née" Smith), the family moved to Levenshulme, near Manchester, where they lived in poverty. His father was a machine painter at the Metropolitan-Vickers works in Trafford Park which influenced him to take up engineering, design, and to pursue a career designing buildings. His mother worked in a local bakery. Foster's parents were diligent and hard workers who often had neighbours and family members look after their son, which Foster later believed restricted his relationship with his mother and father.

Foster attended Burnage Grammar School for Boys in Burnage, where he was bullied by fellow pupils and took up reading. He considered himself quiet and awkward in his early years. At 16, he left school and passed an entrance exam for a trainee scheme set up by Manchester Town Hall, which led to his first job, an office junior and clerk in the treasurer's department. In 1953, Foster completed his national service in the Royal Air Force, choosing the air force because aircraft had been a longtime hobby.
Upon returning to Manchester, Foster went against his parents' wishes and sought employment elsewhere. He had seven O-Levels by this time, and applied to work at a duplicating machine company, telling the interviewer he had applied for the prospect of a company car and a £1,000 salary. Instead, he became an assistant to a contract manager at a local architects, John E. Beardshaw and Partners. The staff advised him, that if he wished to become an architect, he should prepare a portfolio of drawings using the perspective and shop drawings from Beardshaw's practice as an example. Beardshaw was so impressed with Foster's drawings that he promoted him to the drawing department.

In 1956, Foster began study at the School of Architecture and City Planning, part of the University of Manchester. He was ineligible for a maintenance grant, so he took part-time jobs to fund his studies, including an ice-cream salesman, bouncer, and night shifts at a bakery making crumpets. During this time, he also studied at the local library in Levenshulme. His talent and hard work was recognised in 1959 when he won £100 and a RIBA silver medal for what he described as "a measured drawing of a windmill". After graduating in 1961, Foster won the Henry Fellowship to Yale School of Architecture in New Haven, Connecticut, where he met future business partner Richard Rogers and earned his master's degree. At the suggestion of Vincent Scully, the pair travelled across America for a year.

In 1963, Foster returned to England and established his own an architectural practice, Team 4, with Rogers, Su Brumwell, and sisters Georgie and Wendy Cheesman. The team earned a reputation for their high-tech industrial designs. After the four separated in 1967, Foster and Wendy founded a new practice, Foster Associates. From 1968 to 1983, Foster collaborated with American architect Richard Buckminster Fuller on several projects that became catalysts in the development of an environmentally sensitive approach to design, such as the Samuel Beckett Theatre at St Peter's College, Oxford. In 1999, the company was renamed Foster + Partners.
Foster Associates concentrated on industrial buildings until 1969, when the practice worked on the administrative and leisure centre for Fred. Olsen Lines based in the London Docklands, which integrated workers and managers within the same office space. Its breakthrough building in England followed in 1974 with the completion of the Willis Faber & Dumas headquarters in Ipswich. The client was a family run insurance company that wanted to restore a sense of community to the workplace. Foster created open plan office floors, long before open-plan became the norm, and placed a roof garden, 25-metre swimming pool, and gymnasium in the building to enhance the quality of life for the company's 1,200 employees. The building has a full-height glass façade moulded to the medieval street plan and contributes drama, subtly shifting from opaque, reflective black to a glowing back-lit transparency as the sun sets. The design was inspired by the Daily Express Building in Manchester that Foster had admired as a youngster. The building is now Grade I* listed.
The Sainsbury Centre for Visual Arts, an art gallery and museum on the campus of the University of East Anglia, Norwich, was one of the first major public buildings to be designed by Foster, completed in 1978, and became grade II* listed in December 2012. In 1990 Foster's design for the Terminal Building at London Stansted Airport was awarded the European Union Prize for Contemporary Architecture / Mies van der Rohe Award.
Foster gained a reputation for designing office buildings. In the 1980s he designed the HSBC Main Building in Hong Kong for HSBC. The building is marked by its high level of light transparency, as all 3500 workers have a view to Victoria Peak or Victoria Harbour. Foster said that if the firm had not won the contract it would probably have been bankrupted.

Foster believes that attracting young talent is essential, and is proud that the average age of people working for Foster and Partners is 32, just like it was in 1967.

Foster was assigned the brief for a development on the site of the Baltic Exchange in the 1990s. The Exchange was damaged beyond repair by a bomb left by the IRA. Foster + Partners submitted a plan for a 385-metre tall skyscraper, the London Millennium Tower, but its height was seen as excessive for London's skyline. The proposal was scrapped and instead Foster proposed 30 St Mary Axe, popularly referred to as "the gherkin", after its shape. Foster worked with engineers to integrate complex computer systems with the most basic physical laws, such as convection.
Foster's earlier designs reflected a sophisticated, machine-influenced high-tech vision. His style has evolved into a more sharp-edged modernity. In 2004, Foster designed the tallest bridge in the world, the Millau Viaduct in Southern France, with the Millau Mayor Jacques Godfrain stating; "The architect, Norman Foster, gave us a model of art."

Foster worked with Steve Jobs from about 2009 until Jobs' death to design the Apple offices, Apple Campus 2 now called Apple Park, in Cupertino, California, US. Apple's board and staff continued to work with Foster as the design was completed and the construction in progress. The circular building was opened to employees in April 2017, six years after Jobs died in 2011.

In January 2007, the "Sunday Times" reported that Foster had called in Catalyst, a corporate finance house, to find buyers for Foster + Partners. Foster does not intend to retire, but sell his 80–90% holding in the company valued at £300 million to £500 million.

In 2007, he worked with Philippe Starck and Sir Richard Branson of the Virgin Group for the Virgin Galactic plans.

Foster currently sits on the Board of Trustees at architectural charity Article 25 who design, construct and manage innovative, safe, sustainable buildings in some of the most inhospitable and unstable regions of the world. He has also been on the Board of Trustees of the Architecture Foundation.

In 2012, Foster was among the British cultural figures selected by artist Sir Peter Blake to appear in a new version of his most famous artwork – the Beatles' "Sgt. Pepper's Lonely Hearts Club Band" album cover – to celebrate the British cultural figures of his life that he most admires.

Foster has been married three times. His first wife, Wendy Cheeseman, one of the four founders of Team 4, died from cancer in 1989. From 1991 to 1995, he was married to Begum Sabiha Rumani Malik. The marriage ended in divorce. In 1996, Foster married Spanish psychologist and art curator Elena Ochoa. He has five children; two of the four sons he had with Cheeseman are adopted.

In the 2000s, Foster was diagnosed with bowel cancer and was told he had weeks to live. He received chemotherapy treatment and made a full recovery. He also suffered from a heart attack.

Foster was made a Knight Bachelor in the 1990 Birthday Honours, and thereby granted the title "sir". He was appointed to the Order of Merit (OM) in 1997. In the 1999 Birthday Honours, Foster's elevation to the peerage was announced in June 1999 and was raised to the peerage as Baron Foster of Thames Bank, of Reddish in the County of Greater Manchester in July.

Foster was elected an Associate of the Royal Academy (ARA) on 19 May 1983, and a Royal Academician (RA) on 26 June 1991. In 1995, he was elected an Honorary Fellow of the Royal Academy of Engineering (HonFREng). On 24 April 2017, he was given the Freedom of the City of London. The Bloomberg London building received a Stirling Prize in October 2018.

Foster received The Lynn S. Beedle Lifetime Achievement Award from the Council on Tall Buildings and Urban Habitat in 2007 to honour his contributions to the advancement of tall buildings.

He was awarded the Aga Khan Award for Architecture, for the University of Technology Petronas in Malaysia, and in 2008 he was granted an honorary degree from the Dundee School of Architecture at the University of Dundee. In 2009 he received the Prince of Asturias Award in the category "Arts".





</doc>
<doc id="21642" url="https://en.wikipedia.org/wiki?curid=21642" title="Niklaus Wirth">
Niklaus Wirth

Niklaus Emil Wirth (born 15 February 1934) is a Swiss computer scientist. He has designed several programming languages, including Pascal, and pioneered several classic topics in software engineering. In 1984 he won the Turing Award, generally recognized as the highest distinction in computer science, for developing a sequence of innovative computer languages.

Wirth was born in Winterthur, Switzerland, in 1934. In 1959, he earned a Bachelor of Science (B.S.) degree in electronic engineering from the Swiss Federal Institute of Technology Zürich (ETH Zürich). In 1960, he earned a Master of Science (MSc) from Université Laval, Canada. Then in 1963, he was awarded a PhD in Electrical Engineering and Computer Science (EECS) from the University of California, Berkeley, supervised by the computer design pioneer Harry Huskey.

From 1963 to 1967, he served as assistant professor of computer science at Stanford University and again at the University of Zurich. Then in 1968, he became Professor of Informatics at ETH Zürich, taking two one-year sabbaticals at Xerox PARC in California (1976–1977 and 1984–1985). He retired in 1999.

He was a member of the International Federation for Information Processing (IFIP) IFIP Working Group 2.1 on Algorithmic Languages and Calculi, which supports and maintains the programming languages ALGOL 60 and ALGOL 68.

In 2004, he was made a Fellow of the Computer History Museum "for seminal work in programming languages and algorithms, including Euler, Algol-W, Pascal, Modula, and Oberon."

Wirth was the chief designer of the programming languages Euler, Algol W, Pascal, Modula, Modula-2, Oberon, Oberon-2, and Oberon-07. He was also a major part of the design and implementation team for the Lilith and Oberon operating systems, and for the Lola digital hardware design and simulation system. He received the Association for Computing Machinery (ACM) Turing Award for the development of these languages in 1984, and in 1994 he was inducted as a Fellow of the ACM.

His book, written jointly with Kathleen Jensen, "The Pascal User Manual and Report", served as the basis of many language implementation efforts in the 1970s and 1980s in the United States and across Europe.

His article "Program Development by Stepwise Refinement", about the teaching of programming, is considered to be a classic text in software engineering. In 1975 he wrote the book "Algorithms + Data Structures = Programs", which gained wide recognition. Major revisions of this book with the new title "Algorithms + Data Structures" were published in 1985 and 2004. The examples in the first edition were written in Pascal. These were replaced in the later editions with examples written in Modula-2 and Oberon respectively.

His textbook, "Systematic Programming: An Introduction", was considered a good source for students who wanted to do more than just coding. Regarded as a challenging text to work through, it was sought as imperative reading for those interested in numerical mathematics.

In 1992, he published (with Jürg Gutknecht) the full documentation of the Oberon OS. A second book (with Martin Reiser) was intended as a programmer's guide.

In 1995, he popularized the adage now named Wirth's law, which states that software is getting slower more rapidly than hardware becomes faster. In his 1995 paper "A Plea for Lean Software" he attributes it to Martin Reiser.




</doc>
<doc id="21646" url="https://en.wikipedia.org/wiki?curid=21646" title="National Cartoonists Society">
National Cartoonists Society

The National Cartoonists Society (NCS) is an organization of professional cartoonists in the United States. It presents the National Cartoonists Society Awards. The Society was born in 1946 when groups of cartoonists got together to entertain the troops. They enjoyed each other's company and decided to meet on a regular basis.

NCS members work in many branches of the profession, including advertising, animation, newspaper comic strips and syndicated single-panel cartoons, comic books, editorial cartoons, gag cartoons, graphic novels, greeting cards, magazine and book illustration. Only recently has the National Cartoonists Society embraced web comics. Membership is limited to established professional cartoonists, with a few exceptions of outstanding persons in affiliated fields. The NCS is not a guild or labor union.

The organization's stated primary purposes are "to advance the ideals and standards of professional cartooning in its many forms", "to promote and foster a social, cultural and intellectual interchange among professional cartoonists of all types" and "to stimulate and encourage interest in and acceptance of the art of cartooning by aspiring cartoonists, students and the general public."

The National Cartoonists Society had its origins during World War II when cartoonists Gus Edson, Otto Soglow, Clarence D. Russell, Bob Dunn and others did chalk talks at hospitals for the USO in 1943. Edson recalled, “We played two spots. Fort Hamilton and Governor’s Island. And then we quit the USO.” They were lured away by choreographer and former Rockette Toni Mendez. When she learned of these chalk talks, she recruited the cartoonists to do shows for the Hospital Committee of the American Theatre Wing. Beginning with a performance emceed by humor columnist Bugs Baer at Halloran Hospital on Staten Island, these shows were produced and directed by Mendez. The group expanded to junkets on military transport planes, flying to military bases along the southeastern seaboard. On one of those flights, Russell proposed a club to Rube Goldberg and others so the group could still get together after WWII ended. Mendez recalled:

The Society was organized on a Friday evening, March 1, 1946, when 26 cartoonists gathered at 7pm in the Barberry Room on East 52nd Street in Manhattan. After drinks and dinner, they voted to determine officers and a name for their new organization. It was initially known as The Cartoonists Society. Goldberg was elected president with Russell Patterson as vice president, C. D. Russell as secretary and Milton Caniff, treasurer. Soglow was later added as second vice president (“to follow the first vice president around”). Mendez functioned as the Society's trouble-shooter and later became an agent representing more than 50 cartoonists.

The 26 founding members came from the group of 32 members who had paid dues by March 13, including strip cartoonists Wally Bishop ("Muggs and Skeeter"), Martin Branner ("Winnie Winkle"), Ernie Bushmiller ("Nancy"), Milton Caniff, Gus Edson ("The Gumps"), Ham Fisher ("Joe Palooka"), Harry Haenigsen ("Penny"), Fred Harman ("Red Ryder"), Bill Holman ("Smokey Stover"), Jay Irving ("Willie Doodle"), Stan MacGovern ("Silly Milly"), Al Posen ("Sweeney and Son"), Clarence Russell ("Pete the Tramp"), Otto Soglow ("The Little King"), Jack Sparling ("Claire Voyant"), Raeburn Van Buren ("Abbie an' Slats"), Dow Walling ("Skeets") and Frank Willard ("Moon Mullins").

Also among the early 32 members were syndicated panel cartoonists Dave Breger ("Mister Breger"), George Clark ("The Neighbors"), Bob Dunn ("Just the Type") and Jimmy Hatlo ("They'll Do It Every Time"); freelance magazine cartoonists Abner Dean and Mischa Richter, editorial cartoonists Rube Goldberg ("New York Sun"), Burris Jenkins ("New York Journal American"), C. D. Batchelor ("Daily News") and Richard Q. Yardley ("The Baltimore Sun"); sports cartoonist Lou Hanlon; illustrator Russell Patterson and comic book artists Joe Shuster and Joe Musial.

More members joined by mid-May 1946, including Harold Gray ("Little Orphan Annie") and the Society’s first animator, Paul Terry, followed in the summer by letterer Frank Engli, Bela Zaboly ("Popeye"), Al Capp ("Li’l Abner") and ("Bruce Gentry"). By March 1947, the NCS had 112 members, including Bud Fisher ("Mutt and Jeff"), Don Flowers (Glamor Girls), Bob Kane ("Batman"), Fred Lasswell ("Barney Google and Snuffy Smith"), George Lichty ("Grin and Bear It"), Zack Mosley ("The Adventures of Smilin' Jack"), Alex Raymond ("Rip Kirby"), Cliff Sterrett ("Polly and Her Pals") and Chic Young ("Blondie"), plus editorial cartoonists Reg Manning and Fred O. Seibel and sports cartoonist Willard Mullin.

Marge Devine Duffy, a secretary in King Features public relations department, had been helping Russell handle correspondence to the NCS, and in 1948, she was installed as the official NCS secretary and later given the title Scribe of the Society. Her name was on all the Society’s publications, and her address was the permanent mailing address of the NCS for more than 30 years. As the organizing secretary, she handled agendas, organization and publicity. “She practically ran the damn thing,” Caniff recalled. “A real autocrat, and everyone was delighted to have her be an autocrat because that’s what we needed.”

In the fall of 1949, the NCS cooperated with Treasury Department to sell savings bonds, engaging in a nationwide tour to 17 major cities with a team of 10 to 12 cartoonists and a traveling display, "20,000 Years of Comics", a 95-foot pictorial history of the comic strip.

Despite the contributions of Duffy and Mendez, there were no female members, as stipulated in the NCS' constitution which specified that “any cartoonist (male) who signs his name to his published work” could apply for membership. In 1949, Hilda Terry wrote a letter challenging that rule, and after more than six months of debates and votes, three women were finally admitted for membership in 1950—Terry, Edwina Dumm and gag cartoonist Barbara Shermund.

On November 6, 1951, 49 members of the NCS arrived at Washington's Carlton Hotel for breakfast with Harry S. Truman. Gathered in Washington to help the Treasury Department sell Defense Stamps, the group presented Truman with a bound volume of their comic strip characters, some interacting with caricatures of Truman.

When Al Posen originated the idea of National Cartoonists Society tours to entertain American servicemen, he became the NCS Director of Overseas Shows. On October 4, 1952, nine cartoonists left on a USO-Camp Shows tour of U.S. Armed Forces installations in Europe, traveling via a Military Air Transport Service plane from Westover Air Force Base in Massachusetts and landing at Rhein-Main Air Base in Germany. On the tour, the cartoonists engaged models in each country to join in their "Laff Time" show of audience participation stunts and gags. The cartoonists were Posen, Charles Biro, Bob Dunn, Gus Edson, Bill Holman, Bob Montana, Russell Patterson, Clarence Russell and Dick Wingert ("Hubert"). The comic strip "Dondi" came about because of a friendship that developed between Edson and Irwin Hasen during a USO trip to Korea.

Hy Eisman described the atmosphere at the NCS when he joined in 1955:

During the 1960s, cartoonists of military comic strips went to the White House and met with Lyndon B. Johnson in the Oval Office. The group included Caniff, Bill Mauldin and Mort Walker.

In 1977-78, the National Cartoonists Society released "The National Cartoonists Society Portfolio of Fine Comic Art", published by Collector's Press. The portfolio featured a total of 34 art prints. Each 12" x 16" print was printed on archival fine art paper.

In 2011, to memorialize and commemorate the 10th anniversary of the September 11 attacks, many NCS cartoonists auctioned off art that gave commentary to the tragedy and raised money for families victimized by the event in a reflective homage called, Cartoonists Remember. These cartoon tributes raised over $50,000 to benefit the 9/11 families. The art was featured and displayed in both nationally syndicated newspapers and museums across America, including the Newseum in Washington, DC, the Cartoon Art Museum in San Francisco and the Museum of Comic and Cartoon Art in New York City.

In 2005, the Society formed a Foundation to continue the charitable works of its fund for indigent cartoonists, the Milt Gross Fund.

The Society's offices are in Winter Park, Florida. In addition, the NCS has chartered 16 regional chapters throughout the United States and one in Canada. Chapter Chairpersons sit on the NCS Regional Council and are represented by a National Representative, who is a voting member of the Board of Directors. As NCS president for two consecutive terms, Jeff Keane, cartoonist for the "Family Circus" and son of comic creator, Bil Keane, returned to the charter and spirit of the NCS by extending the society's outreach to the military by visiting and cartooning for vets who served in the Iraq War and Afghanistan War, during the years 2007-2011.

In 2008, NCS joined over 60 other art licensing businesses (including the Artists Rights Society, Association of American Editorial Cartoonists, Society of Children's Book Writers and Illustrators, the Stock Artists Alliance, Illustrator's Partnership of America and the Advertising Photographers of America) in opposing both The Orphan Works Act of 2008 and the Shawn Bentley Orphan Works Act of 2008. Known collectively as "Artists United Against the U.S. Orphan Works Acts", the diverse organizations joined forces to oppose the bills, which the groups believe "permits, and even encourages, wide-scale infringements while depriving creators of protections currently available under the Copyright Act."

The earliest NCS award was the Billy DeBeck Memorial Award, also known as "the Barney" from the character in Billy DeBeck's popular comic strip "Barney Google and Snuffy Smith". After DeBeck died on Veteran's Day, 1942, Mary DeBeck remarried (as Mary Bergman) and created the DeBeck Award in 1946. She also made the annual presentation of engraved silver cigarette cases (with DeBeck's characters etched on the cover) to the eight winners spanning the years 1946 to 1953.

Mary Bergman died February 14, 1953, aboard a National Airlines DC-6 which went down in the Gulf of Mexico during a thunderstorm on a flight from Tampa to New Orleans. In 1954, following her death, the DeBeck Award was renamed the Reuben Award, also known "the Reuben." When the award name was changed in 1954, all of the prior eight winners were given Reuben statuettes designed by and named after the NCS' first president, Rube Goldberg. The Reuben Award was executed in bronze by sculptor and editorial cartoonist Bill Crawford.
The National Cartoonists Society's Reuben Awards weekend is an annual gala event which takes place at a site selected by the President. During the formal, black-tie banquet evening, the Reuben Award (determined by secret ballot) is presented to the Outstanding Cartoonist of the Year. Cartoonists in various professional divisions are also honored with special plaques for excellence. These awards are voted by a combination of the general membership (by secret ballot) and specially-formed juries overseen by various NCS Regional Chapters. A cartoonist does not need to be a member of the NCS to receive one of the Society's awards.

Prior to 1983, the Reuben Awards Dinner was held in New York City, usually at the Plaza Hotel. Since then, the event has expanded into a full weekend and is held in a different city each year. Recent Reuben locations have included New York City; Boca Raton; San Francisco; Cancún; Kansas City, Missouri; Las Vegas; and Pittsburgh, Pennsylvania in 2013.

Each year, during the NCS Annual Reuben Awards Weekend, the Society honors the year's outstanding achievements in all walks of the profession. Excellence in the fields of newspaper strips, newspaper panels, TV animation, feature animation, newspaper illustration, gag cartoons, book illustration, greeting cards, comic books, magazine feature/magazine illustration and editorial cartoons, is honored in the NCS Division Awards, which are chosen by specially-convened juries at the chapter level. An Online Comic Strip Award was added in 2011.

The recipient of the profession's highest honor, the Reuben Award for Outstanding Cartoonist of the Year, is chosen by a secret ballot of the members. As part of the presentations and general frivolity, the NCS has produced videos to initiate the festivities, some of which have been parodies of iconic entertainment.

Billy DeBeck Memorial Award


Reuben Award

This award was for recognition of the American cartoon as an instrument in war, peace, education and in the artistic betterment of our cultural environment. On September 22, 1965, the following were honored:

The Milton Caniff Lifetime Achievement Award is awarded by unanimous vote of the NCS Board of Directors.

The Gold T-Square is awarded for 50 years as professional cartoonist.

The Silver T-Square is awarded, by unanimous vote of the NCS Board of Directors, to persons who have demonstrated outstanding dedication or service to the Society or the profession.

This award is presented to a person who has made a unique and outstanding contribution to the profession of cartooning.
The winner was selected by the NCS Board and later by King Features Syndicate, in honor of "Popeye" creator, Elzie Segar.




</doc>
<doc id="21647" url="https://en.wikipedia.org/wiki?curid=21647" title="Nebraska">
Nebraska

Nebraska is a state that lies both in the Great Plains and in the Midwestern United States. It is bordered by South Dakota to the north; Iowa to the east and Missouri to the southeast, both across the Missouri River; Kansas to the south; Colorado to the southwest; and Wyoming to the west. It is the only triply landlocked U.S. state.

Indigenous peoples, including Omaha, Missouria, Ponca, Pawnee, Otoe, and various branches of the Lakota (Sioux) tribes, lived in the region for thousands of years before European exploration. The state is crossed by many historic trails, including that of the Lewis and Clark Expedition.

Nebraska's area is just over with a population of almost 1.9 million. Its capital is Lincoln, and its largest city is Omaha, which is on the Missouri River. Nebraska was admitted into the United States in 1867, two years after the end of the American Civil War. The Nebraska Legislature is unlike any other American legislature in that it is unicameral, and its members are elected without any official reference to political party affiliation.

Nebraska is composed of two major land regions: the Dissected Till Plains and the Great Plains. The Dissected Till Plains region consist of gently rolling hills and contains the state's largest cities, Omaha and Lincoln. The Great Plains region, occupying most of western Nebraska, is characterized by treeless prairie. Nebraska has two major climatic zones. The eastern half of the state has a humid continental climate (Köppen climate classification "Dfa"); a unique warmer subtype considered "warm-temperate" exists near the southern plains, which is analogous to that in Kansas and Oklahoma, which have a predominantly humid subtropical climate. The western half of the state has a primarily semi-arid climate (Köppen "BSk"). The state has wide variations between winter and summer temperatures, variations that decrease moving south within the state. Violent thunderstorms and tornadoes occur primarily during spring and summer and sometimes in autumn. Chinook wind tends to warm the state significantly in the winter and early spring.

Nebraska's name is the result of anglicization of the archaic Otoe words "Ñí Brásge", pronounced (contemporary Otoe "Ñí Bráhge"), or the Omaha "Ní Btháska", pronounced , meaning "flat water", after the Platte River which flows through the state.

Indigenous peoples lived in the region of present-day Nebraska for thousands of years before European exploration. The historic tribes in the state included the Omaha, Missouria, Ponca, Pawnee, Otoe, and various branches of the Lakota (Sioux), some of which migrated from eastern areas into this region. When European exploration, trade, and settlement began, both Spain and France sought to control the region. In the 1690s, Spain established trade connections with the Apaches, whose territory then included western Nebraska. By 1703, France had developed a regular trade with the native peoples along the Missouri River in Nebraska, and by 1719 had signed treaties with several of these peoples. After war broke out between the two countries, Spain dispatched an armed expedition to Nebraska under Lieutenant General Pedro de Villasur in 1720. The party was attacked and destroyed near present-day Columbus by a large force of Pawnees and Otoes, both allied with the French. The massacre ended Spanish exploration of the area for the remainder of the 18th century.

In 1762, during the Seven Years' War, France ceded the Louisiana territory to Spain. This left Britain and Spain competing for dominance along the Mississippi; by 1773, the British were trading with the native peoples of Nebraska. In response, Spain dispatched two trading expeditions up the Missouri in 1794 and 1795; the second, under James Mackay, established the first European settlement in Nebraska near the mouth of the Platte. Later that year, Mackay's party built a trading post, dubbed Fort Carlos IV (Fort Charles), near present-day Homer.

In 1819, the United States established Fort Atkinson as the first U.S. Army post west of the Missouri River, just east of present-day Fort Calhoun. The army abandoned the fort in 1827 as migration moved further west. European-American settlement was scarce until 1848 and the California Gold Rush. On May 30, 1854, the US Congress created the Kansas and the Nebraska territories, divided by the Parallel 40° North, under the Kansas–Nebraska Act. The Nebraska Territory included parts of the current states of Colorado, North Dakota, South Dakota, Wyoming, and Montana. The territorial capital of Nebraska was Omaha.
In the 1860s, after the U.S. government forced many of the Native American tribes to cede their lands and settle on reservations, it opened large tracts of land to agricultural development by Europeans and Americans. Under the Homestead Act, thousands of settlers migrated into Nebraska to claim free land granted by the federal government. Because so few trees grew on the prairies, many of the first farming settlers built their homes of sod, as had Native Americans such as the Omaha. The first wave of settlement gave the territory a sufficient population to apply for statehood. Nebraska became the 37th state on March 1, 1867, and the capital was moved from Omaha to the center at Lancaster, later renamed Lincoln after the recently assassinated President of the United States, Abraham Lincoln. The battle of Massacre Canyon, on August 5, 1873, was the last major battle between the Pawnee and the Sioux.

During the 1870s to the 1880s, Nebraska experienced a large growth in population. Several factors contributed to attracting new residents. The first was that the vast prairie land was perfect for cattle grazing. This helped settlers to learn the unfamiliar geography of the area. The second factor was the invention of several farming technologies. Agricultural inventions such as barbed wire, wind mills, and the steel plow, combined with good weather, enabled settlers to use Nebraska as prime farming land. By the 1880s, Nebraska's population had soared to more than 450,000 people. The Arbor Day holiday was founded in Nebraska City by territorial governor J. Sterling Morton. The National Arbor Day Foundation is still headquartered in Nebraska City, with some offices in Lincoln.

In the late 19th century, many African Americans migrated from the South to Nebraska as part of the Great Migration, primarily to Omaha which offered working class jobs in meat packing, the railroads and other industries. Omaha has a long history of civil rights activism. Blacks encountered discrimination from other Americans in Omaha and especially from recent European immigrants, ethnic whites who were competing for the same jobs. In 1912, African Americans founded the Omaha chapter of the National Association for the Advancement of Colored People to work for improved conditions in the city and state.

Since the 1960s, Native American activism in the state has increased, both through open protest, activities to build alliances with state and local governments, and in the slower, more extensive work of building tribal institutions and infrastructure. Native Americans in federally recognized tribes have pressed for self-determination, sovereignty and recognition. They have created community schools to preserve their cultures, as well as tribal colleges and universities. Tribal politicians have also collaborated with state and county officials on regional issues.

The state is bordered by South Dakota to the north; Iowa to the east and Missouri to the southeast, across the Missouri River; Kansas to the south; Colorado to the southwest; and Wyoming to the west. The state has 93 counties and is split between two time zones, with the state's eastern half observing Central Time and the western half observing Mountain Time. Three rivers cross the state from west to east. The Platte River, formed by the confluence of the North Platte and the South Platte, runs through the state's central portion, the Niobrara River flows through the northern part, and the Republican River runs across the southern part.

The first Constitution of Nebraska in 1866 described Nebraska's boundaries as follows (Note that the description of the Northern border is no longer accurate, since the Keya Paha River and the Niobrara River no longer form the boundary of the state of Nebraska. Instead, Nebraska's Northern border now extends east along the forty-third degree of north latitude until it meets the Missouri River directly.): 

Nebraska is composed of two major land regions: the Dissected Till Plains and the Great Plains. The easternmost portion of the state was scoured by Ice Age glaciers; the Dissected Till Plains were left after the glaciers retreated. The Dissected Till Plains is a region of gently rolling hills; Omaha and Lincoln are in this region. The Great Plains occupy most of western Nebraska, with the region consisting of several smaller, diverse land regions, including the Sandhills, the Pine Ridge, the Rainwater Basin, the High Plains and the Wildcat Hills. Panorama Point, at , is Nebraska's highest point; though despite its name and elevation, it is a relatively low rise near the Colorado and Wyoming borders. A past tourism slogan for the state of Nebraska was "Where the West Begins" (it has since been changed to "Honestly, it's not for everyone"). Locations given for the beginning of the "West" in Nebraska include the Missouri River, the intersection of 13th and O Streets in Lincoln (where it is marked by a red brick star), the 100th meridian, and Chimney Rock.

Areas under the management of the National Park Service include:

Areas under the management of the National Forest Service include:

Two major climatic zones are represented in Nebraska. The eastern two-thirds of the state has a humid continental climate (Köppen "Dfa"), although the southwest of this region may be classed as a humid subtropical climate ("Cfa") using the boundary. The Panhandle and adjacent areas bordering Colorado have a semi-arid climate (Köppen "BSk"). The entire state experiences wide seasonal variations in both temperature and precipitation. Average temperatures are fairly uniform across Nebraska, with hot summers and generally cold winters. However, chinook winds from the Rocky Mountains provide a temporary moderating effect on temperatures in the state's western portion during the winter. Consequently, average January maximum temperatures are highest at around in southwestern Dundy County, and lowest in the northeaThe chinook winds from the Rocky Mountains provide a temporary moderating effect on temperatures in the state's western portion during the winter. Thus, average January maximum temperatures are highest at around in southwestern Dundy County, and lowest at about around South Sioux City in the northeast.

Average annual precipitation decreases east to west from about in the southeast corner of the state to about in the Panhandle. Humidity also decreases significantly from east to west. Snowfall across the state is fairly even, with most of Nebraska receiving between of snow each year. Nebraska's highest-recorded temperature was in Minden on July 24, 1936. The state's lowest-recorded temperature was in Camp Clarke on February 12, 1899.

Nebraska is located in Tornado Alley. Thunderstorms are common during both the spring and the summer. Violent thunderstorms and tornadoes happen primarily during those two seasons, although they also can occur occasionally during the autumn.

The United States Census Bureau estimates that the population of Nebraska was 1,934,408 on July 1, 2019, a 5.92% increase since the 2010 United States Census. The center of population of Nebraska is in Polk County, in the city of Shelby.

The table below shows the racial composition of Nebraska's population as of 2016.

According to the 2016 American Community Survey, 10.2% of Nebraska's population were of Hispanic or Latino origin (of any race): Mexican (7.8%), Puerto Rican (0.2%), Cuban (0.2%), and other Hispanic or Latino origin (2.0%). The five largest ancestry groups were: German (36.1%), Irish (13.1%), English (7.8%), Czech (4.7%), and American (4.0%).

Nebraska has the largest Czech American and non-Mormon Danish American population (as a percentage of the total population) in the nation. German Americans are the largest ancestry group in most of the state, particularly in the eastern counties. Thurston County (made up entirely of the Omaha and Winnebago reservations) has an American Indian majority, and Butler County is one of only two counties in the nation with a Czech-American plurality.

In recent years, Nebraska has become home to many refugee communities. In 2016, it welcomed more refugees per capita than any other state. Nebraska, and in particular Lincoln, is the largest home of Yazidis refugees and Yazidi Americans in the United States.

As of 2011, 31.0% of Nebraska's population younger than age1 were minorities.

"Note: Births in table don't add up, because Hispanics are counted both by their ethnicity and by their race, giving a higher overall number."


The religious affiliations of the people of Nebraska are:

The largest single denominations by number of adherents in 2010 were the Roman Catholic Church (372,838), the Lutheran Church–Missouri Synod (112,585), the Evangelical Lutheran Church in America (110,110) and the United Methodist Church (109,283).

Eighty-nine percent of the cities in Nebraska have fewer than 3,000 people. Nebraska shares this characteristic with five other Midwestern states: Kansas, Oklahoma, North Dakota and South Dakota, and Iowa. Hundreds of towns have a population of fewer than 1,000. Regional population declines have forced many rural schools to consolidate.

Fifty-three of Nebraska's 93 counties reported declining populations between 1990 and 2000, ranging from a 0.06% loss (Frontier County) to a 17.04% loss (Hitchcock County).
More urbanized areas of the state have experienced substantial growth. In 2000, the city of Omaha had a population of 390,007; in 2005, the city's estimated population was 414,521 (427,872 including the recently annexed city of Elkhorn), a 6.3% increase over five years. The 2010 census showed that Omaha has a population of 408,958. The city of Lincoln had a 2000 population of 225,581 and a 2010 population of 258,379, a 14.5% increase.

As of the 2010 Census, there were 530 cities and villages in the state of Nebraska. There are five classifications of cities and villages in Nebraska, which is based upon population. All population figures are 2017 Census Bureau estimates unless flagged by a reference number.

Metropolitan Class City (300,000 or more)

Primary Class City (100,000–299,999)

First Class City (5,000–99,999)

Second Class Cities (800–4,999) and Villages (100–800) make up the rest of the communities in Nebraska. There are 116 second-class cities and 382 villages in the state.

Metropolitan areas 2017 estimate data

Micropolitan areas 2012 estimate data
Other areas

Nebraska has a progressive income tax. The portion of income from $0 to $2,400 is taxed at 2.56%; from $2,400 to $17,500, at 3.57%; from $17,500 to $27,000, at 5.12%; and income over $27,000, at 6.84%. The standard deduction for a single taxpayer is $5,700; the personal exemption is $118.

Nebraska has a state sales and use tax of 5.5%. In addition to the state tax, some Nebraska cities assess a city sales and use tax, in 0.5% increments, up to a maximum of 1.5%. Dakota County levies an additional 0.5% county sales tax. Food and ingredients that are generally for home preparation and consumption are not taxable. All real property within the state of Nebraska is taxable unless specifically exempted by statute. Since 1992, only depreciable personal property is subject to tax and all other personal property is exempt from tax. Inheritance tax is collected at the county level.


The Bureau of Economic Analysis estimates of Nebraska's gross state product in 2010 was $89.8 billion. Per capita personal income in 2004 was $31,339, 25th in the nation. Nebraska has a large agriculture sector, and is a major producer of beef, pork, corn (maize), soybeans, and sorghum. Other important economic sectors include freight transport (by rail and truck), manufacturing, telecommunications, information technology, and insurance.

As of November 2018, the state's unemployment rate was 2.8%, the fifth lowest in the nation.

Kool-Aid was created in 1927 by Edwin Perkins in the city of Hastings, which celebrates the event the second weekend of every August with Kool-Aid Days, and Kool-Aid is the official soft drink of Nebraska. "CliffsNotes" were developed by Clifton Hillegass of Rising City. He adapted his pamphlets from the Canadian publications, "Coles Notes".

Omaha is home to Berkshire Hathaway, whose chief executive officer (CEO), Warren Buffett, was ranked in March 2009 by "Forbes" magazine as the second-richest person in the world. The city is also home to Mutual of Omaha, InfoUSA, TD Ameritrade, West Corporation, Valmont Industries, Woodmen of the World, Kiewit Corporation, Union Pacific Railroad, and Gallup. Ameritas Life Insurance Corp., Nelnet, Sandhills Publishing Company, Duncan Aviation, and Hudl are based in Lincoln. The Buckle is based in Kearney. Sidney is the national headquarters for Cabela's, a specialty retailer of outdoor goods now owned by Bass Pro Shops. Grand Island is the headquarters of Hornady, a manufacturer of ammunition.

The world's largest train yard, Union Pacific's Bailey Yard, is in North Platte. The Vise-Grip was invented by William Petersen in 1924, and was manufactured in De Witt until the plant was closed and moved to China in late 2008.

Lincoln's Kawasaki Motors Manufacturing is the only Kawasaki plant in the world to produce the Jet Ski, all-terrain vehicle (ATV), and Mule product lines. The facility employs more than 1,200 people.

The Spade Ranch, in the Sandhills, is one of Nebraska's oldest and largest beef cattle operations.

Nebraska is the only state in the US where all electric utilities are publicly owned.

The Union Pacific Railroad, headquartered in Omaha, was incorporated on July 1, 1862, in the wake of the Pacific Railway Act of 1862. Bailey Yard, in North Platte, is the largest railroad classification yard in the world. The route of the original transcontinental railroad runs through the state.

Other major railroads with operations in the state are: Amtrak; BNSF Railway; Canadian National Railway; and Iowa Interstate Railroad.

Nebraska's government operates under the framework of the Nebraska Constitution, adopted in 1875, and is divided into three branches: executive, legislative, and judicial.

The head of the executive branch is Governor Pete Ricketts. Other elected officials in the executive branch are Lieutenant Governor Mike Foley, Attorney General Doug Peterson, Secretary of State Bob Evnen, State Treasurer John Murante, and State Auditor Charlie Janssen. All elected officials in the executive branch serve four-year terms.

Nebraska is the only state in the United States with a unicameral legislature. Although this house is officially known simply as the "Legislature", and more commonly called the "Unicameral", its members call themselves "senators". Nebraska's Legislature is also the only state legislature in the United States that is officially nonpartisan. The senators are elected with no party affiliation next to their names on the ballot, and members of any party can be elected to the positions of speaker and committee chairs. The Nebraska Legislature can also override the governor's veto with a three-fifths majority, in contrast to the two-thirds majority required in some other states.

When Nebraska became a state in 1867, its legislature consisted of two houses: a House of Representatives and a Senate. For years, U.S. Senator George Norris and other Nebraskans encouraged the idea of a unicameral legislature, and demanded the issue be decided in a referendum. Norris argued:

The Legislature meets in the third Nebraska State Capitol building, built between 1922 and 1932. It was designed by Bertram G. Goodhue. Built from Indiana limestone, the capitol's base is a cross within a square. A 400-foot domed tower rises from this base. The Sower, a 19-foot bronze statue representing agriculture, crowns the building.

The judicial system in Nebraska is unified, with the Nebraska Supreme Court having administrative authority over all the courts within the state. Nebraska uses the Missouri Plan for the selection of judges at all levels, including county courts (as the lowest-level courts) and twelve district courts, which contain one or more counties. The Nebraska State Court of Appeals hears appeals from the district courts, juvenile courts, and workers' compensation courts, and is the final court of appeal.

Nebraska's U.S. senators are Deb Fischer and Ben Sasse, both Republicans; Fischer, elected in 2012, is the senior.

Nebraska has three representatives in the House of Representatives: Jeff Fortenberry (R) of the 1st district; Don Bacon (R) of the 2nd district; and Adrian Smith (R) of the 3rd district.

Nebraska is one of two states (Maine is the other) that allow for a split in the state's allocation of electoral votes in presidential elections. Under a 1991 law, two of Nebraska's five votes are awarded to the winner of the statewide popular vote, while the other three go to the highest vote-getter in each of the state's three congressional districts.

For most of its history, Nebraska has been a solidly Republican state. Republicans have carried the state in all but one presidential election since 1940: the 1964 landslide election of Lyndon B. Johnson. In the 2004 presidential election, George W. Bush won the state's five electoral votes by a margin of 33 percentage points (making Nebraska's the fourth-strongest Republican vote among states) with 65.9% of the overall vote; only Thurston County, which is majority-Native American, voted for his Democratic challenger John Kerry. In 2008, the state split its electoral votes for the first time: Republican John McCain won the popular vote in Nebraska as a whole and two of its three congressional districts; the second district, which includes the city of Omaha, went for Democrat Barack Obama.

Despite the current Republican domination of Nebraska politics, the state has a long tradition of electing centrist members of both parties to state and federal office; examples include George W. Norris (who served a few years in the Senate as an independent), J. James Exon, Bob Kerrey, and Chuck Hagel. Voters have tilted to the right in recent years, a trend evidenced when Hagel retired from the Senate in 2008 and was succeeded by conservative Republican Mike Johanns to the U.S. Senate, as well as with the 2006 re-election of Ben Nelson, who was considered the most conservative Democrat in the Senate until his retirement in 2013. Johanns retired in 2015 and was succeeded by another conservative, Sasse. Nelson retired in 2013 and was replaced by conservative Republican Fischer.

Former President Gerald Ford was born in Nebraska, but moved away shortly after birth. Illinois native William Jennings Bryan represented Nebraska in Congress, served as U.S. Secretary of State under President Woodrow Wilson, and unsuccessfully ran for president three times.

University of Nebraska system
Nebraska State College System

Community Colleges

Private colleges/universities

Museums

Performing Arts

Nebraska is currently home to seven member schools of the NCAA, eight of the NAIA, seven of the NJCAA, one of the NCCAA, and one independent school.

The College World Series has been held in Omaha since 1950. It was held at Rosenblatt Stadium from 1950 through 2010, and has been domiciled at TD Ameritrade Park Omaha since 2011.




</doc>
