<doc id="18836" url="https://en.wikipedia.org/wiki?curid=18836" title="Middle Ages">
Middle Ages

In the history of Europe, the Middle Ages or Medieval Period lasted from the 5th to the 15th century. It began with the fall of the Western Roman Empire and merged into the Renaissance and the Age of Discovery. The Middle Ages is the middle period of the three traditional divisions of Western history: classical antiquity, the medieval period, and the modern period. The medieval period is itself subdivided into the Early, High, and Late Middle Ages.

Population decline, counterurbanisation, collapse of centralized authority, invasions, and mass migrations of tribes, which had begun in Late Antiquity, continued in the Early Middle Ages. The large-scale movements of the Migration Period, including various Germanic peoples, formed new kingdoms in what remained of the Western Roman Empire. In the 7th century, North Africa and the Middle East—once part of the Byzantine Empire—came under the rule of the Umayyad Caliphate, an Islamic empire, after conquest by Muhammad's successors. Although there were substantial changes in society and political structures, the break with classical antiquity was not complete. The still-sizeable Byzantine Empire, Rome's direct continuation, survived in the Eastern Mediterranean and remained a major power. The empire's law code, the "Corpus Juris Civilis" or "Code of Justinian", was rediscovered in Northern Italy in the 11th century. In the West, most kingdoms incorporated the few extant Roman institutions. Monasteries were founded as campaigns to Christianise pagan Europe continued. The Franks, under the Carolingian dynasty, briefly established the Carolingian Empire during the later 8th and early 9th centuries. It covered much of Western Europe but later succumbed to the pressures of internal civil wars combined with external invasions: Vikings from the north, Magyars from the east, and Saracens from the south.

During the High Middle Ages, which began after 1000, the population of Europe increased greatly as technological and agricultural innovations allowed trade to flourish and the Medieval Warm Period climate change allowed crop yields to increase. Manorialism, the organisation of peasants into villages that owed rent and labour services to the nobles, and feudalism, the political structure whereby knights and lower-status nobles owed military service to their overlords in return for the right to rent from lands and manors, were two of the ways society was organised in the High Middle Ages. The Crusades, first preached in 1095, were military attempts by Western European Christians to regain control of the Holy Land from Muslims. Kings became the heads of centralised nation-states, reducing crime and violence but making the ideal of a unified Christendom more distant. Intellectual life was marked by scholasticism, a philosophy that emphasised joining faith to reason, and by the founding of universities. The theology of Thomas Aquinas, the paintings of Giotto, the poetry of Dante and Chaucer, the travels of Marco Polo, and the Gothic architecture of cathedrals such as Chartres are among the outstanding achievements toward the end of this period and into the Late Middle Ages.

The Late Middle Ages was marked by difficulties and calamities including famine, plague, and war, which significantly diminished the population of Europe; between 1347 and 1350, the Black Death killed about a third of Europeans. Controversy, heresy, and the Western Schism within the Catholic Church paralleled the interstate conflict, civil strife, and peasant revolts that occurred in the kingdoms. Cultural and technological developments transformed European society, concluding the Late Middle Ages and beginning the early modern period.

The Middle Ages is one of the three major periods in the most enduring scheme for analysing European history: classical civilisation or Antiquity, the Middle Ages and the Modern Period. The "Middle Ages" first appears in Latin in 1469 as "media tempestas" or "middle season". In early usage, there were many variants, including "medium aevum", or "middle age", first recorded in 1604, and "media saecula", or "middle centuries", first recorded in 1625. The adjective "medieval" (or sometimes "mediaeval" or "mediæval"), meaning pertaining to the Middle Ages, derives from "medium aevum".

Medieval writers divided history into periods such as the "Six Ages" or the "Four Empires", and considered their time to be the last before the end of the world. When referring to their own times, they spoke of them as being "modern". In the 1330s, the humanist and poet Petrarch referred to pre-Christian times as "antiqua" (or "ancient") and to the Christian period as "nova" (or "new"). Leonardo Bruni was the first historian to use tripartite periodisation in his "History of the Florentine People" (1442), with a middle period "between the fall of the Roman Empire and the revival of city life sometime in late eleventh and twelfth centuries". Tripartite periodisation became standard after the 17th-century German historian Christoph Cellarius divided history into three periods: ancient, medieval, and modern.

The most commonly given starting point for the Middle Ages is around 500, with the date of 476 first used by Bruni. Later starting dates are sometimes used in the outer parts of Europe. For Europe as a whole, 1500 is often considered to be the end of the Middle Ages, but there is no universally agreed upon end date. Depending on the context, events such as the conquest of Constantinople by the Turks in 1453, Christopher Columbus's first voyage to the Americas in 1492, or the Protestant Reformation in 1517 are sometimes used. English historians often use the Battle of Bosworth Field in 1485 to mark the end of the period. For Spain, dates commonly used are the death of King Ferdinand II in 1516, the death of Queen Isabella I of Castile in 1504, or the conquest of Granada in 1492.

Historians from Romance-speaking countries tend to divide the Middle Ages into two parts: an earlier "High" and later "Low" period. English-speaking historians, following their German counterparts, generally subdivide the Middle Ages into three intervals: "Early", "High", and "Late". In the 19th century, the entire Middle Ages were often referred to as the "Dark Ages", but with the adoption of these subdivisions, use of this term was restricted to the Early Middle Ages, at least among historians.

The Roman Empire reached its greatest territorial extent during the 2nd century AD; the following two centuries witnessed the slow decline of Roman control over its outlying territories. Economic issues, including inflation, and external pressure on the frontiers combined to create the Crisis of the Third Century, with emperors coming to the throne only to be rapidly replaced by new usurpers. Military expenses increased steadily during the 3rd century, mainly in response to the war with the Sasanian Empire, which revived in the middle of the 3rd century. The army doubled in size, and cavalry and smaller units replaced the Roman legion as the main tactical unit. The need for revenue led to increased taxes and a decline in numbers of the curial, or landowning, class, and decreasing numbers of them willing to shoulder the burdens of holding office in their native towns. More bureaucrats were needed in the central administration to deal with the needs of the army, which led to complaints from civilians that there were more tax-collectors in the empire than tax-payers.

The Emperor Diocletian (r. 284–305) split the empire into separately administered eastern and western halves in 286; the empire was not considered divided by its inhabitants or rulers, as legal and administrative promulgations in one division were considered valid in the other. In 330, after a period of civil war, Constantine the Great (r. 306–337) refounded the city of Byzantium as the newly renamed eastern capital, Constantinople. Diocletian's reforms strengthened the governmental bureaucracy, reformed taxation, and strengthened the army, which bought the empire time but did not resolve the problems it was facing: excessive taxation, a declining birthrate, and pressures on its frontiers, among others. Civil war between rival emperors became common in the middle of the 4th century, diverting soldiers from the empire's frontier forces and allowing invaders to encroach. For much of the 4th century, Roman society stabilised in a new form that differed from the earlier classical period, with a widening gulf between the rich and poor, and a decline in the vitality of the smaller towns. Another change was the Christianisation, or conversion of the empire to Christianity, a gradual process that lasted from the 2nd to the 5th centuries.

In 376, the Goths, fleeing from the Huns, received permission from Emperor Valens (r. 364–378) to settle in the Roman province of Thracia in the Balkans. The settlement did not go smoothly, and when Roman officials mishandled the situation, the Goths began to raid and plunder. Valens, attempting to put down the disorder, was killed fighting the Goths at the Battle of Adrianople on 9 August 378. As well as the threat from such tribal confederacies from the north, internal divisions within the empire, especially within the Christian Church, caused problems. In 400, the Visigoths invaded the Western Roman Empire and, although briefly forced back from Italy, in 410 sacked the city of Rome. In 406 the Alans, Vandals, and Suevi crossed into Gaul; over the next three years they spread across Gaul and in 409 crossed the Pyrenees Mountains into modern-day Spain. The Migration Period began, when various peoples, initially largely Germanic peoples, moved across Europe. The Franks, Alemanni, and the Burgundians all ended up in northern Gaul while the Angles, Saxons, and Jutes settled in Britain, and the Vandals went on to cross the strait of Gibraltar after which they conquered the province of Africa. In the 430s the Huns began invading the empire; their king Attila (r. 434–453) led invasions into the Balkans in 442 and 447, Gaul in 451, and Italy in 452. The Hunnic threat remained until Attila's death in 453, when the Hunnic confederation he led fell apart. These invasions by the tribes completely changed the political and demographic nature of what had been the Western Roman Empire.

By the end of the 5th century the western section of the empire was divided into smaller political units, ruled by the tribes that had invaded in the early part of the century. The deposition of the last emperor of the west, Romulus Augustulus, in 476 has traditionally marked the end of the Western Roman Empire. By 493 the Italian peninsula was conquered by the Ostrogoths. The Eastern Roman Empire, often referred to as the Byzantine Empire after the fall of its western counterpart, had little ability to assert control over the lost western territories. The Byzantine emperors maintained a claim over the territory, but while none of the new kings in the west dared to elevate himself to the position of emperor of the west, Byzantine control of most of the Western Empire could not be sustained; the reconquest of the Mediterranean periphery and the Italian Peninsula (Gothic War) in the reign of Justinian (r. 527–565) was the sole, and temporary, exception.

The political structure of Western Europe changed with the end of the united Roman Empire. Although the movements of peoples during this period are usually described as "invasions", they were not just military expeditions but migrations of entire peoples into the empire. Such movements were aided by the refusal of the Western Roman elites to support the army or pay the taxes that would have allowed the military to suppress the migration. The emperors of the 5th century were often controlled by military strongmen such as Stilicho (d. 408), Aetius (d. 454), Aspar (d. 471), Ricimer (d. 472), or Gundobad (d. 516), who were partly or fully of non-Roman background. When the line of Western emperors ceased, many of the kings who replaced them were from the same background. Intermarriage between the new kings and the Roman elites was common. This led to a fusion of Roman culture with the customs of the invading tribes, including the popular assemblies that allowed free male tribal members more say in political matters than was common in the Roman state. Material artefacts left by the Romans and the invaders are often similar, and tribal items were often modelled on Roman objects. Much of the scholarly and written culture of the new kingdoms was also based on Roman intellectual traditions. An important difference was the gradual loss of tax revenue by the new polities. Many of the new political entities no longer supported their armies through taxes, instead relying on granting them land or rents. This meant there was less need for large tax revenues and so the taxation systems decayed. Warfare was common between and within the kingdoms. Slavery declined as the supply weakened, and society became more rural.

Between the 5th and 8th centuries, new peoples and individuals filled the political void left by Roman centralised government. The Ostrogoths, a Gothic tribe, settled in Roman Italy in the late fifth century under Theoderic the Great (d. 526) and set up a kingdom marked by its co-operation between the Italians and the Ostrogoths, at least until the last years of Theodoric's reign. The Burgundians settled in Gaul, and after an earlier realm was destroyed by the Huns in 436 formed a new kingdom in the 440s. Between today's Geneva and Lyon, it grew to become the realm of Burgundy in the late 5th and early 6th centuries. Elsewhere in Gaul, the Franks and Celtic Britons set up small polities. Francia was centred in northern Gaul, and the first king of whom much is known is Childeric I (d. 481). His grave was discovered in 1653 and is remarkable for its grave goods, which included weapons and a large quantity of gold.

Under Childeric's son Clovis I (r. 509–511), the founder of the Merovingian dynasty, the Frankish kingdom expanded and converted to Christianity. The Britons, related to the natives of Britannia – modern-day Great Britain – settled in what is now Brittany. Other monarchies were established by the Visigothic Kingdom in the Iberian Peninsula, the Suebi in northwestern Iberia, and the Vandal Kingdom in North Africa. In the sixth century, the Lombards settled in Northern Italy, replacing the Ostrogothic kingdom with a grouping of duchies that occasionally selected a king to rule over them all. By the late sixth century, this arrangement had been replaced by a permanent monarchy, the Kingdom of the Lombards.

The invasions brought new ethnic groups to Europe, although some regions received a larger influx of new peoples than others. In Gaul for instance, the invaders settled much more extensively in the north-east than in the south-west. Slavs settled in Central and Eastern Europe and the Balkan Peninsula. The settlement of peoples was accompanied by changes in languages. Latin, the literary language of the Western Roman Empire, was gradually replaced by vernacular languages which evolved from Latin, but were distinct from it, collectively known as Romance languages. These changes from Latin to the new languages took many centuries. Greek remained the language of the Byzantine Empire, but the migrations of the Slavs added Slavic languages to Eastern Europe.

As Western Europe witnessed the formation of new kingdoms, the Eastern Roman Empire remained intact and experienced an economic revival that lasted into the early 7th century. There were fewer invasions of the eastern section of the empire; most occurred in the Balkans. Peace with the Sasanian Empire, the traditional enemy of Rome, lasted throughout most of the 5th century. The Eastern Empire was marked by closer relations between the political state and Christian Church, with doctrinal matters assuming an importance in Eastern politics that they did not have in Western Europe. Legal developments included the codification of Roman law; the first effort—the "Codex Theodosianus"—was completed in 438. Under Emperor Justinian (r. 527–565), another compilation took place—the "Corpus Juris Civilis". Justinian also oversaw the construction of the Hagia Sophia in Constantinople and the reconquest of North Africa from the Vandals and Italy from the Ostrogoths, under Belisarius (d. 565). The conquest of Italy was not complete, as a deadly outbreak of plague in 542 led to the rest of Justinian's reign concentrating on defensive measures rather than further conquests.

At the Emperor's death, the Byzantines had control of most of Italy, North Africa, and a small foothold in southern Spain. Justinian's reconquests have been criticised by historians for overextending his realm and setting the stage for the early Muslim conquests, but many of the difficulties faced by Justinian's successors were due not just to over-taxation to pay for his wars but to the essentially civilian nature of the empire, which made raising troops difficult.

In the Eastern Empire the slow infiltration of the Balkans by the Slavs added a further difficulty for Justinian's successors. It began gradually, but by the late 540s Slavic tribes were in Thrace and Illyrium, and had defeated an imperial army near Adrianople in 551. In the 560s the Avars began to expand from their base on the north bank of the Danube; by the end of the 6th-century, they were the dominant power in Central Europe and routinely able to force the Eastern emperors to pay tribute. They remained a strong power until 796.

An additional problem to face the empire came as a result of the involvement of Emperor Maurice (r. 582–602) in Persian politics when he intervened in a succession dispute. This led to a period of peace, but when Maurice was overthrown, the Persians invaded and during the reign of Emperor Heraclius (r. 610–641) controlled large chunks of the empire, including Egypt, Syria, and Anatolia until Heraclius' successful counterattack. In 628 the empire secured a peace treaty and recovered all of its lost territories.

In Western Europe, some of the older Roman elite families died out while others became more involved with ecclesiastical than secular affairs. Values attached to Latin scholarship and education mostly disappeared, and while literacy remained important, it became a practical skill rather than a sign of elite status. In the 4th century, Jerome (d. 420) dreamed that God rebuked him for spending more time reading Cicero than the Bible. By the 6th century, Gregory of Tours (d. 594) had a similar dream, but instead of being chastised for reading Cicero, he was chastised for learning shorthand. By the late 6th century, the principal means of religious instruction in the Church had become music and art rather than the book. Most intellectual efforts went towards imitating classical scholarship, but some original works were created, along with now-lost oral compositions. The writings of Sidonius Apollinaris (d. 489), Cassiodorus (d. c. 585), and Boethius (d. c. 525) were typical of the age.

Changes also took place among laymen, as aristocratic culture focused on great feasts held in halls rather than on literary pursuits. Clothing for the elites was richly embellished with jewels and gold. Lords and kings supported entourages of fighters who formed the backbone of the military forces. Family ties within the elites were important, as were the virtues of loyalty, courage, and honour. These ties led to the prevalence of the feud in aristocratic society, examples of which included those related by Gregory of Tours that took place in Merovingian Gaul. Most feuds seem to have ended quickly with the payment of some sort of compensation. Women took part in aristocratic society mainly in their roles as wives and mothers of men, with the role of mother of a ruler being especially prominent in Merovingian Gaul. In Anglo-Saxon society the lack of many child rulers meant a lesser role for women as queen mothers, but this was compensated for by the increased role played by abbesses of monasteries. Only in Italy does it appear that women were always considered under the protection and control of a male relative.

Peasant society is much less documented than the nobility. Most of the surviving information available to historians comes from archaeology; few detailed written records documenting peasant life remain from before the 9th century. Most of the descriptions of the lower classes come from either law codes or writers from the upper classes. Landholding patterns in the West were not uniform; some areas had greatly fragmented landholding patterns, but in other areas large contiguous blocks of land were the norm. These differences allowed for a wide variety of peasant societies, some dominated by aristocratic landholders and others having a great deal of autonomy. Land settlement also varied greatly. Some peasants lived in large settlements that numbered as many as 700 inhabitants. Others lived in small groups of a few families and still others lived on isolated farms spread over the countryside. There were also areas where the pattern was a mix of two or more of those systems. Unlike in the late Roman period, there was no sharp break between the legal status of the free peasant and the aristocrat, and it was possible for a free peasant's family to rise into the aristocracy over several generations through military service to a powerful lord.

Roman city life and culture changed greatly in the early Middle Ages. Although Italian cities remained inhabited, they contracted significantly in size. Rome, for instance, shrank from a population of hundreds of thousands to around 30,000 by the end of the 6th century. Roman temples were converted into Christian churches and city walls remained in use. In Northern Europe, cities also shrank, while civic monuments and other public buildings were raided for building materials. The establishment of new kingdoms often meant some growth for the towns chosen as capitals. Although there had been Jewish communities in many Roman cities, the Jews suffered periods of persecution after the conversion of the empire to Christianity. Officially they were tolerated, if subject to conversion efforts, and at times were even encouraged to settle in new areas.

Religious beliefs in the Eastern Empire and Iran were in flux during the late sixth and early seventh centuries. Judaism was an active proselytising faith, and at least one Arab political leader converted to it. Christianity had active missions competing with the Persians' Zoroastrianism in seeking converts, especially among residents of the Arabian Peninsula. All these strands came together with the emergence of Islam in Arabia during the lifetime of Muhammad (d. 632). After his death, Islamic forces conquered much of the Eastern Empire and Persia, starting with Syria in 634–635, continuing with Persia between 637 and 642, reaching Egypt in 640–641, North Africa in the later seventh century, and the Iberian Peninsula in 711. By 714, Islamic forces controlled much of the peninsula in a region they called Al-Andalus.

The Islamic conquests reached their peak in the mid-eighth century. The defeat of Muslim forces at the Battle of Tours in 732 led to the reconquest of southern France by the Franks, but the main reason for the halt of Islamic growth in Europe was the overthrow of the Umayyad Caliphate and its replacement by the Abbasid Caliphate. The Abbasids moved their capital to Baghdad and were more concerned with the Middle East than Europe, losing control of sections of the Muslim lands. Umayyad descendants took over the Iberian Peninsula, the Aghlabids controlled North Africa, and the Tulunids became rulers of Egypt. By the middle of the 8th century, new trading patterns were emerging in the Mediterranean; trade between the Franks and the Arabs replaced the old Roman economy. Franks traded timber, furs, swords and slaves in return for silks and other fabrics, spices, and precious metals from the Arabs.

The migrations and invasions of the 4th and 5th centuries disrupted trade networks around the Mediterranean. African goods stopped being imported into Europe, first disappearing from the interior and by the 7th century found only in a few cities such as Rome or Naples. By the end of the 7th century, under the impact of the Muslim conquests, African products were no longer found in Western Europe. The replacement of goods from long-range trade with local products was a trend throughout the old Roman lands that happened in the Early Middle Ages. This was especially marked in the lands that did not lie on the Mediterranean, such as northern Gaul or Britain. Non-local goods appearing in the archaeological record are usually luxury goods. In the northern parts of Europe, not only were the trade networks local, but the goods carried were simple, with little pottery or other complex products. Around the Mediterranean, pottery remained prevalent and appears to have been traded over medium-range networks, not just produced locally.

The various Germanic states in the west all had coinages that imitated existing Roman and Byzantine forms. Gold continued to be minted until the end of the 7th century in 693-94 when it was replaced by silver in the Merovingian kindgon. The basic Frankish silver coin was the denarius or denier, while the Anglo-Saxon version was called a penny. From these areas, the denier or penny spread throughout Europe from 700 to 1000 AD. Copper or bronze coins were not struck, nor were gold except in Southern Europe. No silver coins denominated in multiple units were minted.

Christianity was a major unifying factor between Eastern and Western Europe before the Arab conquests, but the conquest of North Africa sundered maritime connections between those areas. Increasingly, the Byzantine Church differed in language, practices, and liturgy from the Western Church. The Eastern Church used Greek instead of the Western Latin. Theological and political differences emerged, and by the early and middle 8th century issues such as iconoclasm, clerical marriage, and state control of the Church had widened to the extent that the cultural and religious differences were greater than the similarities. The formal break, known as the East–West Schism, came in 1054, when the papacy and the patriarchy of Constantinople clashed over papal supremacy and excommunicated each other, which led to the division of Christianity into two Churches—the Western branch became the Roman Catholic Church and the Eastern branch the Eastern Orthodox Church.

The ecclesiastical structure of the Roman Empire survived the movements and invasions in the west mostly intact, but the papacy was little regarded, and few of the Western bishops looked to the bishop of Rome for religious or political leadership. Many of the popes prior to 750 were more concerned with Byzantine affairs and Eastern theological controversies. The register, or archived copies of the letters, of Pope Gregory the Great (pope 590–604) survived, and of those more than 850 letters, the vast majority were concerned with affairs in Italy or Constantinople. The only part of Western Europe where the papacy had influence was Britain, where Gregory had sent the Gregorian mission in 597 to convert the Anglo-Saxons to Christianity. Irish missionaries were most active in Western Europe between the 5th and the 7th centuries, going first to England and Scotland and then on to the continent. Under such monks as Columba (d. 597) and Columbanus (d. 615), they founded monasteries, taught in Latin and Greek, and authored secular and religious works.

The Early Middle Ages witnessed the rise of monasticism in the West. The shape of European monasticism was determined by traditions and ideas that originated with the Desert Fathers of Egypt and Syria. Most European monasteries were of the type that focuses on community experience of the spiritual life, called cenobitism, which was pioneered by Pachomius (d. 348) in the 4th century. Monastic ideals spread from Egypt to Western Europe in the 5th and 6th centuries through hagiographical literature such as the "Life of Anthony". Benedict of Nursia (d. 547) wrote the Benedictine Rule for Western monasticism during the 6th century, detailing the administrative and spiritual responsibilities of a community of monks led by an abbot. Monks and monasteries had a deep effect on the religious and political life of the Early Middle Ages, in various cases acting as land trusts for powerful families, centres of propaganda and royal support in newly conquered regions, and bases for missions and proselytisation. They were the main and sometimes only outposts of education and literacy in a region. Many of the surviving manuscripts of the Latin classics were copied in monasteries in the Early Middle Ages. Monks were also the authors of new works, including history, theology, and other subjects, written by authors such as Bede (d. 735), a native of northern England who wrote in the late 7th and early 8th centuries.

The Frankish kingdom in northern Gaul split into kingdoms called Austrasia, Neustria, and Burgundy during the 6th and 7th centuries, all of them ruled by the Merovingian dynasty, who were descended from Clovis. The 7th century was a tumultuous period of wars between Austrasia and Neustria. Such warfare was exploited by Pippin (d. 640), the Mayor of the Palace for Austrasia who became the power behind the Austrasian throne. Later members of his family inherited the office, acting as advisers and regents. One of his descendants, Charles Martel (d. 741), won the Battle of Poitiers in 732, halting the advance of Muslim armies across the Pyrenees. Great Britain was divided into small states dominated by the kingdoms of Northumbria, Mercia, Wessex, and East Anglia which descended from the Anglo-Saxon invaders. Smaller kingdoms in present-day Wales and Scotland were still under the control of the native Britons and Picts. Ireland was divided into even smaller political units, usually known as tribal kingdoms, under the control of kings. There were perhaps as many as 150 local kings in Ireland, of varying importance.

The Carolingian dynasty, as the successors to Charles Martel are known, officially took control of the kingdoms of Austrasia and Neustria in a coup of 753 led by (r. 752–768). A contemporary chronicle claims that Pippin sought, and gained, authority for this coup from Pope (pope 752–757). Pippin's takeover was reinforced with propaganda that portrayed the Merovingians as inept or cruel rulers, exalted the accomplishments of Charles Martel, and circulated stories of the family's great piety. At the time of his death in 768, Pippin left his kingdom in the hands of his two sons, Charles (r. 768–814) and Carloman (r. 768–771). When Carloman died of natural causes, Charles blocked the succession of Carloman's young son and installed himself as the king of the united Austrasia and Neustria. Charles, more often known as Charles the Great or Charlemagne, embarked upon a programme of systematic expansion in 774 that unified a large portion of Europe, eventually controlling modern-day France, northern Italy, and Saxony. In the wars that lasted beyond 800, he rewarded allies with war booty and command over parcels of land. In 774, Charlemagne conquered the Lombards, which freed the papacy from the fear of Lombard conquest and marked the beginnings of the Papal States.

The coronation of Charlemagne as emperor on Christmas Day 800 is regarded as a turning point in medieval history, marking a return of the Western Roman Empire, since the new emperor ruled over much of the area previously controlled by the Western emperors. It also marks a change in Charlemagne's relationship with the Byzantine Empire, as the assumption of the imperial title by the Carolingians asserted their equivalence to the Byzantine state. There were several differences between the newly established Carolingian Empire and both the older Western Roman Empire and the concurrent Byzantine Empire. The Frankish lands were rural in character, with only a few small cities. Most of the people were peasants settled on small farms. Little trade existed and much of that was with the British Isles and Scandinavia, in contrast to the older Roman Empire with its trading networks centred on the Mediterranean. The empire was administered by an itinerant court that travelled with the emperor, as well as approximately 300 imperial officials called counts, who administered the counties the empire had been divided into. Clergy and local bishops served as officials, as well as the imperial officials called "missi dominici", who served as roving inspectors and troubleshooters.

Charlemagne's court in Aachen was the centre of the cultural revival sometimes referred to as the "Carolingian Renaissance". Literacy increased, as did development in the arts, architecture and jurisprudence, as well as liturgical and scriptural studies. The English monk Alcuin (d. 804) was invited to Aachen and brought the education available in the monasteries of Northumbria. Charlemagne's chancery—or writing office—made use of a new script today known as Carolingian minuscule, allowing a common writing style that advanced communication across much of Europe. Charlemagne sponsored changes in church liturgy, imposing the Roman form of church service on his domains, as well as the Gregorian chant in liturgical music for the churches. An important activity for scholars during this period was the copying, correcting, and dissemination of basic works on religious and secular topics, with the aim of encouraging learning. New works on religious topics and schoolbooks were also produced. Grammarians of the period modified the Latin language, changing it from the Classical Latin of the Roman Empire into a more flexible form to fit the needs of the Church and government. By the reign of Charlemagne, the language had so diverged from the classical Latin that it was later called Medieval Latin.

Charlemagne planned to continue the Frankish tradition of dividing his kingdom between all his heirs, but was unable to do so as only one son, Louis the Pious (r. 814–840), was still alive by 813. Just before Charlemagne died in 814, he crowned Louis as his successor. Louis's reign of 26 years was marked by numerous divisions of the empire among his sons and, after 829, civil wars between various alliances of father and sons over the control of various parts of the empire. Eventually, Louis recognised his eldest son (d. 855) as emperor and gave him Italy. Louis divided the rest of the empire between Lothair and Charles the Bald (d. 877), his youngest son. Lothair took East Francia, comprising both banks of the Rhine and eastwards, leaving Charles West Francia with the empire to the west of the Rhineland and the Alps. Louis the German (d. 876), the middle child, who had been rebellious to the last, was allowed to keep Bavaria under the suzerainty of his elder brother. The division was disputed. of Aquitaine (d. after 864), the emperor's grandson, rebelled in a contest for Aquitaine, while Louis the German tried to annex all of East Francia. Louis the Pious died in 840, with the empire still in chaos.

A three-year civil war followed his death. By the Treaty of Verdun (843), a kingdom between the Rhine and Rhone rivers was created for Lothair to go with his lands in Italy, and his imperial title was recognised. Louis the German was in control of Bavaria and the eastern lands in modern-day Germany. Charles the Bald received the western Frankish lands, comprising most of modern-day France. Charlemagne's grandsons and great-grandsons divided their kingdoms between their descendants, eventually causing all internal cohesion to be lost. In 987 the Carolingian dynasty was replaced in the western lands, with the crowning of Hugh Capet (r. 987–996) as king. In the eastern lands the dynasty had died out earlier, in 911, with the death of Louis the Child, and the selection of the unrelated Conrad I (r. 911–918) as king.

The breakup of the Carolingian Empire was accompanied by invasions, migrations, and raids by external foes. The Atlantic and northern shores were harassed by the Vikings, who also raided the British Isles and settled there as well as in Iceland. In 911, the Viking chieftain Rollo (d. c. 931) received permission from the Frankish King Charles the Simple (r. 898–922) to settle in what became Normandy. The eastern parts of the Frankish kingdoms, especially Germany and Italy, were under continual Magyar assault until the invader's defeat at the Battle of Lechfeld in 955. The breakup of the Abbasid dynasty meant that the Islamic world fragmented into smaller political states, some of which began expanding into Italy and Sicily, as well as over the Pyrenees into the southern parts of the Frankish kingdoms.

Efforts by local kings to fight the invaders led to the formation of new political entities. In Anglo-Saxon England, King Alfred the Great (r. 871–899) came to an agreement with the Viking invaders in the late 9th century, resulting in Danish settlements in Northumbria, Mercia, and parts of East Anglia. By the middle of the 10th century, Alfred's successors had conquered Northumbria, and restored English control over most of the southern part of Great Britain. In northern Britain, Kenneth MacAlpin (d. c. 860) united the Picts and the Scots into the Kingdom of Alba. In the early 10th century, the Ottonian dynasty had established itself in Germany, and was engaged in driving back the Magyars. Its efforts culminated in the coronation in 962 of (r. 936–973) as Holy Roman Emperor. In 972, he secured recognition of his title by the Byzantine Empire, which he sealed with the marriage of his son Otto II (r. 967–983) to Theophanu (d. 991), daughter of an earlier Byzantine Emperor Romanos II (r. 959–963). By the late 10th century Italy had been drawn into the Ottonian sphere after a period of instability; Otto III (r. 996–1002) spent much of his later reign in the kingdom. The western Frankish kingdom was more fragmented, and although kings remained nominally in charge, much of the political power devolved to the local lords.

Missionary efforts to Scandinavia during the 9th and 10th centuries helped strengthen the growth of kingdoms such as Sweden, Denmark, and Norway, which gained power and territory. Some kings converted to Christianity, although not all by 1000. Scandinavians also expanded and colonised throughout Europe. Besides the settlements in Ireland, England, and Normandy, further settlement took place in what became Russia and Iceland. Swedish traders and raiders ranged down the rivers of the Russian steppe, and even attempted to seize Constantinople in 860 and 907. Christian Spain, initially driven into a small section of the peninsula in the north, expanded slowly south during the 9th and 10th centuries, establishing the kingdoms of Asturias and León.

In Eastern Europe, Byzantium revived its fortunes under Emperor Basil I (r. 867–886) and his successors Leo VI (r. 886–912) and Constantine VII (r. 913–959), members of the Macedonian dynasty. Commerce revived and the emperors oversaw the extension of a uniform administration to all the provinces. The military was reorganised, which allowed the emperors John I (r. 969–976) and Basil II (r. 976–1025) to expand the frontiers of the empire on all fronts. The imperial court was the centre of a revival of classical learning, a process known as the Macedonian Renaissance. Writers such as John Geometres (fl. early 10th century) composed new hymns, poems, and other works. Missionary efforts by both Eastern and Western clergy resulted in the conversion of the Moravians, Bulgars, Bohemians, Poles, Magyars, and Slavic inhabitants of the Kievan Rus'. These conversions contributed to the founding of political states in the lands of those peoples—the states of Moravia, Bulgaria, Bohemia, Poland, Hungary, and the Kievan Rus'. Bulgaria, which was founded around 680, at its height reached from Budapest to the Black Sea and from the Dnieper River in modern Ukraine to the Adriatic Sea. By 1018, the last Bulgarian nobles had surrendered to the Byzantine Empire.

Few large stone buildings were constructed between the Constantinian basilicas of the 4th century and the 8th century, although many smaller ones were built during the 6th and 7th centuries. By the beginning of the 8th century, the Carolingian Empire revived the basilica form of architecture. One feature of the basilica is the use of a transept, or the "arms" of a cross-shaped building that are perpendicular to the long nave. Other new features of religious architecture include the crossing tower and a monumental entrance to the church, usually at the west end of the building.

Carolingian art was produced for a small group of figures around the court, and the monasteries and churches they supported. It was dominated by efforts to regain the dignity and classicism of imperial Roman and Byzantine art, but was also influenced by the Insular art of the British Isles. Insular art integrated the energy of Irish Celtic and Anglo-Saxon Germanic styles of ornament with Mediterranean forms such as the book, and established many characteristics of art for the rest of the medieval period. Surviving religious works from the Early Middle Ages are mostly illuminated manuscripts and carved ivories, originally made for metalwork that has since been melted down. Objects in precious metals were the most prestigious form of art, but almost all are lost except for a few crosses such as the Cross of Lothair, several reliquaries, and finds such as the Anglo-Saxon burial at Sutton Hoo and the hoards of Gourdon from Merovingian France, Guarrazar from Visigothic Spain and Nagyszentmiklós near Byzantine territory. There are survivals from the large brooches in fibula or penannular form that were a key piece of personal adornment for elites, including the Irish Tara Brooch. Highly decorated books were mostly Gospel Books and these have survived in larger numbers, including the Insular Book of Kells, the Book of Lindisfarne, and the imperial Codex Aureus of St. Emmeram, which is one of the few to retain its "treasure binding" of gold encrusted with jewels. Charlemagne's court seems to have been responsible for the acceptance of figurative monumental sculpture in Christian art, and by the end of the period near life-sized figures such as the Gero Cross were common in important churches.

During the later Roman Empire, the principal military developments were attempts to create an effective cavalry force as well as the continued development of highly specialised types of troops. The creation of heavily armoured cataphract-type soldiers as cavalry was an important feature of the 5th-century Roman military. The various invading tribes had differing emphases on types of soldiers—ranging from the primarily infantry Anglo-Saxon invaders of Britain to the Vandals and Visigoths who had a high proportion of cavalry in their armies. During the early invasion period, the stirrup had not been introduced into warfare, which limited the usefulness of cavalry as shock troops because it was not possible to put the full force of the horse and rider behind blows struck by the rider. The greatest change in military affairs during the invasion period was the adoption of the Hunnic composite bow in place of the earlier, and weaker, Scythian composite bow. Another development was the increasing use of longswords and the progressive replacement of scale armour by mail armour and lamellar armour.

The importance of infantry and light cavalry began to decline during the early Carolingian period, with a growing dominance of elite heavy cavalry. The use of militia-type levies of the free population declined over the Carolingian period. Although much of the Carolingian armies were mounted, a large proportion during the early period appear to have been mounted infantry, rather than true cavalry. One exception was Anglo-Saxon England, where the armies were still composed of regional levies, known as the "fyrd", which were led by the local elites. In military technology, one of the main changes was the return of the crossbow, which had been known in Roman times and reappeared as a military weapon during the last part of the Early Middle Ages. Another change was the introduction of the stirrup, which increased the effectiveness of cavalry as shock troops. A technological advance that had implications beyond the military was the horseshoe, which allowed horses to be used in rocky terrain.

The High Middle Ages was a period of tremendous expansion of population. The estimated population of Europe grew from 35 to 80 million between 1000 and 1347, although the exact causes remain unclear: improved agricultural techniques, the decline of slaveholding, a more clement climate and the lack of invasion have all been suggested. As much as 90 per cent of the European population remained rural peasants. Many were no longer settled in isolated farms but had gathered into small communities, usually known as manors or villages. These peasants were often subject to noble overlords and owed them rents and other services, in a system known as manorialism. There remained a few free peasants throughout this period and beyond, with more of them in the regions of Southern Europe than in the north. The practice of assarting, or bringing new lands into production by offering incentives to the peasants who settled them, also contributed to the expansion of population.

The open-field system of agriculture was commonly practiced in most of Europe, especially in "northwestern and central Europe". Such agricultural communities had three basic characteristics: individual peasant holdings in the form of strips of land were scattered among the different fields belonging to the manor; crops were rotated from year to year to preserve soil fertility; and common land was used for grazing livestock and other purposes. Some regions used a three-field system of crop rotation, others retained the older two-field system.

Other sections of society included the nobility, clergy, and townsmen. Nobles, both the titled nobility and simple knights, exploited the manors and the peasants, although they did not own lands outright but were granted rights to the income from a manor or other lands by an overlord through the system of feudalism. During the 11th and 12th centuries, these lands, or fiefs, came to be considered hereditary, and in most areas they were no longer divisible between all the heirs as had been the case in the early medieval period. Instead, most fiefs and lands went to the eldest son. The dominance of the nobility was built upon its control of the land, its military service as heavy cavalry, control of castles, and various immunities from taxes or other impositions. Castles, initially in wood but later in stone, began to be constructed in the 9th and 10th centuries in response to the disorder of the time, and provided protection from invaders as well as allowing lords defence from rivals. Control of castles allowed the nobles to defy kings or other overlords. Nobles were stratified; kings and the highest-ranking nobility controlled large numbers of commoners and large tracts of land, as well as other nobles. Beneath them, lesser nobles had authority over smaller areas of land and fewer people. Knights were the lowest level of nobility; they controlled but did not own land, and had to serve other nobles.

The clergy was divided into two types: the secular clergy, who lived out in the world, and the regular clergy, who lived isolated under a religious rule and usually consisted of monks. Throughout the period monks remained a very small proportion of the population, usually less than one percent. Most of the regular clergy were drawn from the nobility, the same social class that served as the recruiting ground for the upper levels of the secular clergy. The local parish priests were often drawn from the peasant class. Townsmen were in a somewhat unusual position, as they did not fit into the traditional three-fold division of society into nobles, clergy, and peasants. During the 12th and 13th centuries, the ranks of the townsmen expanded greatly as existing towns grew and new population centres were founded. But throughout the Middle Ages the population of the towns probably never exceeded 10 percent of the total population.

Jews also spread across Europe during the period. Communities were established in Germany and England in the 11th and 12th centuries, but Spanish Jews, long settled in Spain under the Muslims, came under Christian rule and increasing pressure to convert to Christianity. Most Jews were confined to the cities, as they were not allowed to own land or be peasants. Besides the Jews, there were other non-Christians on the edges of Europe—pagan Slavs in Eastern Europe and Muslims in Southern Europe.

Women in the Middle Ages were officially required to be subordinate to some male, whether their father, husband, or other kinsman. Widows, who were often allowed much control over their own lives, were still restricted legally. Women's work generally consisted of household or other domestically inclined tasks. Peasant women were usually responsible for taking care of the household, child-care, as well as gardening and animal husbandry near the house. They could supplement the household income by spinning or brewing at home. At harvest-time, they were also expected to help with field-work. Townswomen, like peasant women, were responsible for the household, and could also engage in trade. What trades were open to women varied by country and period. Noblewomen were responsible for running a household, and could occasionally be expected to handle estates in the absence of male relatives, but they were usually restricted from participation in military or government affairs. The only role open to women in the Church was that of nuns, as they were unable to become priests.

In central and northern Italy and in Flanders, the rise of towns that were to a degree self-governing stimulated economic growth and created an environment for new types of trade associations. Commercial cities on the shores of the Baltic entered into agreements known as the Hanseatic League, and the Italian Maritime republics such as Venice, Genoa, and Pisa expanded their trade throughout the Mediterranean. Great trading fairs were established and flourished in northern France during the period, allowing Italian and German merchants to trade with each other as well as local merchants. In the late 13th century new land and sea routes to the Far East were pioneered, famously described in "The Travels of Marco Polo" written by one of the traders, Marco Polo (d. 1324). Besides new trading opportunities, agricultural and technological improvements enabled an increase in crop yields, which in turn allowed the trade networks to expand. Rising trade brought new methods of dealing with money, and gold coinage was again minted in Europe, first in Italy and later in France and other countries. New forms of commercial contracts emerged, allowing risk to be shared among merchants. Accounting methods improved, partly through the use of double-entry bookkeeping; letters of credit also appeared, allowing easy transmission of money.

The High Middle Ages was the formative period in the history of the modern Western state. Kings in France, England, and Spain consolidated their power, and set up lasting governing institutions. New kingdoms such as Hungary and Poland, after their conversion to Christianity, became Central European powers. The Magyars settled Hungary around 900 under King Árpád (d. c. 907) after a series of invasions in the 9th century. The papacy, long attached to an ideology of independence from secular kings, first asserted its claim to temporal authority over the entire Christian world; the Papal Monarchy reached its apogee in the early 13th century under the pontificate of (pope 1198–1216). Northern Crusades and the advance of Christian kingdoms and military orders into previously pagan regions in the Baltic and Finnic north-east brought the forced assimilation of numerous native peoples into European culture.

During the early High Middle Ages, Germany was ruled by the Ottonian dynasty, which struggled to control the powerful dukes ruling over territorial duchies tracing back to the Migration period. In 1024, they were replaced by the Salian dynasty, who famously clashed with the papacy under Emperor (r. 1084–1105) over Church appointments as part of the Investiture Controversy. His successors continued to struggle against the papacy as well as the German nobility. A period of instability followed the death of Emperor (r. 1111–25), who died without heirs, until Barbarossa (r. 1155–90) took the imperial throne. Although he ruled effectively, the basic problems remained, and his successors continued to struggle into the 13th century. Barbarossa's grandson Frederick II (r. 1220–1250), who was also heir to the throne of Sicily through his mother, clashed repeatedly with the papacy. His court was famous for its scholars and he was often accused of heresy. He and his successors faced many difficulties, including the invasion of the Mongols into Europe in the mid-13th century. Mongols first shattered the Kievan Rus' principalities and then invaded Eastern Europe in 1241, 1259, and 1287.

Under the Capetian dynasty the French monarchy slowly began to expand its authority over the nobility, growing out of the Île-de-France to exert control over more of the country in the 11th and 12th centuries. They faced a powerful rival in the Dukes of Normandy, who in 1066 under William the Conqueror (duke 1035–1087), conquered England (r. 1066–87) and created a cross-channel empire that lasted, in various forms, throughout the rest of the Middle Ages. Normans also settled in Sicily and southern Italy, when Robert Guiscard (d. 1085) landed there in 1059 and established a duchy that later became the Kingdom of Sicily. Under the Angevin dynasty of (r. 1154–89) and his son Richard I (r. 1189–99), the kings of England ruled over England and large areas of France, brought to the family by Henry II's marriage to Eleanor of Aquitaine (d. 1204), heiress to much of southern France. Richard's younger brother John (r. 1199–1216) lost Normandy and the rest of the northern French possessions in 1204 to the French King Philip II Augustus (r. 1180–1223). This led to dissension among the English nobility, while John's financial exactions to pay for his unsuccessful attempts to regain Normandy led in 1215 to "Magna Carta", a charter that confirmed the rights and privileges of free men in England. Under (r. 1216–72), John's son, further concessions were made to the nobility, and royal power was diminished. The French monarchy continued to make gains against the nobility during the late 12th and 13th centuries, bringing more territories within the kingdom under the king's personal rule and centralising the royal administration. Under Louis IX (r. 1226–70), royal prestige rose to new heights as Louis served as a mediator for most of Europe.

In Iberia, the Christian states, which had been confined to the north-western part of the peninsula, began to push back against the Islamic states in the south, a period known as the "Reconquista". By about 1150, the Christian north had coalesced into the five major kingdoms of León, Castile, Aragon, Navarre, and Portugal. Southern Iberia remained under control of Islamic states, initially under the Caliphate of Córdoba, which broke up in 1031 into a shifting number of petty states known as "taifas", who fought with the Christians until the Almohad Caliphate re-established centralised rule over Southern Iberia in the 1170s. Christian forces advanced again in the early 13th century, culminating in the capture of Seville in 1248.

In the 11th century, the Seljuk Turks took over much of the Middle East, occupying Persia during the 1040s, Armenia in the 1060s, and Jerusalem in 1070. In 1071, the Turkish army defeated the Byzantine army at the Battle of Manzikert and captured the Byzantine Emperor Romanus IV (r. 1068–71). The Turks were then free to invade Asia Minor, which dealt a dangerous blow to the Byzantine Empire by seizing a large part of its population and its economic heartland. Although the Byzantines regrouped and recovered somewhat, they never fully regained Asia Minor and were often on the defensive. The Turks also had difficulties, losing control of Jerusalem to the Fatimids of Egypt and suffering from a series of internal civil wars. The Byzantines also faced a revived Bulgaria, which in the late 12th and 13th centuries spread throughout the Balkans.

The crusades were intended to seize Jerusalem from Muslim control. The First Crusade was proclaimed by Pope Urban II (pope 1088–99) at the Council of Clermont in 1095 in response to a request from the Byzantine Emperor Alexios I Komnenos (r. 1081–1118) for aid against further Muslim advances. Urban promised indulgence to anyone who took part. Tens of thousands of people from all levels of society mobilised across Europe and captured Jerusalem in 1099. One feature of the crusades was the pogroms against local Jews that often took place as the crusaders left their countries for the East. These were especially brutal during the First Crusade, when the Jewish communities in Cologne, Mainz, and Worms were destroyed, as well as other communities in cities between the rivers Seine and the Rhine. Another outgrowth of the crusades was the foundation of a new type of monastic order, the military orders of the Templars and Hospitallers, which fused monastic life with military service.

The crusaders consolidated their conquests into crusader states. During the 12th and 13th centuries, there were a series of conflicts between them and the surrounding Islamic states. Appeals from the crusader states to the papacy led to further crusades, such as the Third Crusade, called to try to regain Jerusalem, which had been captured by Saladin (d. 1193) in 1187. In 1203, the Fourth Crusade was diverted from the Holy Land to Constantinople, and captured the city in 1204, setting up a Latin Empire of Constantinople and greatly weakening the Byzantine Empire. The Byzantines recaptured the city in 1261, but never regained their former strength. By 1291 all the crusader states had been captured or forced from the mainland, although a titular Kingdom of Jerusalem survived on the island of Cyprus for several years afterwards.

Popes called for crusades to take place elsewhere besides the Holy Land: in Spain, southern France, and along the Baltic. The Spanish crusades became fused with the "Reconquista" of Spain from the Muslims. Although the Templars and Hospitallers took part in the Spanish crusades, similar Spanish military religious orders were founded, most of which had become part of the two main orders of Calatrava and Santiago by the beginning of the 12th century. Northern Europe also remained outside Christian influence until the 11th century or later, and became a crusading venue as part of the Northern Crusades of the 12th to 14th centuries. These crusades also spawned a military order, the Order of the Sword Brothers. Another order, the Teutonic Knights, although founded in the crusader states, focused much of its activity in the Baltic after 1225, and in 1309 moved its headquarters to Marienburg in Prussia.

During the 11th century, developments in philosophy and theology led to increased intellectual activity. There was debate between the realists and the nominalists over the concept of "universals". Philosophical discourse was stimulated by the rediscovery of Aristotle and his emphasis on empiricism and rationalism. Scholars such as Peter Abelard (d. 1142) and Peter Lombard (d. 1164) introduced Aristotelian logic into theology. In the late 11th and early 12th centuries cathedral schools spread throughout Western Europe, signalling the shift of learning from monasteries to cathedrals and towns. Cathedral schools were in turn replaced by the universities established in major European cities. Philosophy and theology fused in scholasticism, an attempt by 12th- and 13th-century scholars to reconcile authoritative texts, most notably Aristotle and the Bible. This movement tried to employ a systemic approach to truth and reason and culminated in the thought of Thomas Aquinas (d. 1274), who wrote the "Summa Theologica", or "Summary of Theology".

Chivalry and the ethos of courtly love developed in royal and noble courts. This culture was expressed in the vernacular languages rather than Latin, and comprised poems, stories, legends, and popular songs spread by troubadours, or wandering minstrels. Often the stories were written down in the "chansons de geste", or "songs of great deeds", such as "The Song of Roland" or "The Song of Hildebrand". Secular and religious histories were also produced. Geoffrey of Monmouth (d. c. 1155) composed his "Historia Regum Britanniae", a collection of stories and legends about Arthur. Other works were more clearly history, such as Otto von Freising's (d. 1158) "Gesta Friderici Imperatoris" detailing the deeds of Emperor Frederick Barbarossa, or William of Malmesbury's (d. c. 1143) "Gesta Regum" on the kings of England.

Legal studies advanced during the 12th century. Both secular law and canon law, or ecclesiastical law, were studied in the High Middle Ages. Secular law, or Roman law, was advanced greatly by the discovery of the "Corpus Juris Civilis" in the 11th century, and by 1100 Roman law was being taught at Bologna. This led to the recording and standardisation of legal codes throughout Western Europe. Canon law was also studied, and around 1140 a monk named Gratian (fl. 12th century), a teacher at Bologna, wrote what became the standard text of canon law—the "Decretum".

Among the results of the Greek and Islamic influence on this period in European history was the replacement of Roman numerals with the decimal positional number system and the invention of algebra, which allowed more advanced mathematics. Astronomy advanced following the translation of Ptolemy's "Almagest" from Greek into Latin in the late 12th century. Medicine was also studied, especially in southern Italy, where Islamic medicine influenced the school at Salerno.

In the 12th and 13th centuries, Europe experienced economic growth and innovations in methods of production. Major technological advances included the invention of the windmill, the first mechanical clocks, the manufacture of distilled spirits, and the use of the astrolabe. Concave spectacles were invented around 1286 by an unknown Italian artisan, probably working in or near Pisa.

The development of a three-field rotation system for planting crops increased the usage of land from one half in use each year under the old two-field system to two-thirds under the new system, with a consequent increase in production. The development of the heavy plough allowed heavier soils to be farmed more efficiently, aided by the spread of the horse collar, which led to the use of draught horses in place of oxen. Horses are faster than oxen and require less pasture, factors that aided the implementation of the three-field system. Legumes – such as peas, beans, or lentils – were grown more widely as crops, in addition to the usual cereal crops of wheat, oats, barley, and rye.

The construction of cathedrals and castles advanced building technology, leading to the development of large stone buildings. Ancillary structures included new town halls, houses, bridges, and tithe barns. Shipbuilding improved with the use of the rib and plank method rather than the old Roman system of mortise and tenon. Other improvements to ships included the use of lateen sails and the stern-post rudder, both of which increased the speed at which ships could be sailed.

In military affairs, the use of infantry with specialised roles increased. Along with the still-dominant heavy cavalry, armies often included mounted and infantry crossbowmen, as well as sappers and engineers. Crossbows, which had been known in Late Antiquity, increased in use partly because of the increase in siege warfare in the 10th and 11th centuries. The increasing use of crossbows during the 12th and 13th centuries led to the use of closed-face helmets, heavy body armour, as well as horse armour. Gunpowder was known in Europe by the mid-13th century with a recorded use in European warfare by the English against the Scots in 1304, although it was merely used as an explosive and not as a weapon. Cannon were being used for sieges in the 1320s, and hand-held guns were in use by the 1360s.

In the 10th century the establishment of churches and monasteries led to the development of stone architecture that elaborated vernacular Roman forms, from which the term "Romanesque" is derived. Where available, Roman brick and stone buildings were recycled for their materials. From the tentative beginnings known as the First Romanesque, the style flourished and spread across Europe in a remarkably homogeneous form. Just before 1000 there was a great wave of building stone churches all over Europe. Romanesque buildings have massive stone walls, openings topped by semi-circular arches, small windows, and, particularly in France, arched stone vaults. The large portal with coloured sculpture in high relief became a central feature of façades, especially in France, and the capitals of columns were often carved with narrative scenes of imaginative monsters and animals. According to art historian C. R. Dodwell, "virtually all the churches in the West were decorated with wall-paintings", of which few survive. Simultaneous with the development in church architecture, the distinctive European form of the castle was developed and became crucial to politics and warfare.

Romanesque art, especially metalwork, was at its most sophisticated in Mosan art, in which distinct artistic personalities including Nicholas of Verdun (d. 1205) become apparent, and an almost classical style is seen in works such as a font at Liège, contrasting with the writhing animals of the exactly contemporary Gloucester Candlestick. Large illuminated bibles and psalters were the typical forms of luxury manuscripts, and wall-painting flourished in churches, often following a scheme with a "Last Judgement" on the west wall, a Christ in Majesty at the east end, and narrative biblical scenes down the nave, or in the best surviving example, at Saint-Savin-sur-Gartempe, on the barrel-vaulted roof.

From the early 12th century, French builders developed the Gothic style, marked by the use of rib vaults, pointed arches, flying buttresses, and large stained glass windows. It was used mainly in churches and cathedrals and continued in use until the 16th century in much of Europe. Classic examples of Gothic architecture include Chartres Cathedral and Reims Cathedral in France as well as Salisbury Cathedral in England. Stained glass became a crucial element in the design of churches, which continued to use extensive wall-paintings, now almost all lost.

During this period the practice of manuscript illumination gradually passed from monasteries to lay workshops, so that according to Janetta Benton "by 1300 most monks bought their books in shops", and the book of hours developed as a form of devotional book for lay-people. Metalwork continued to be the most prestigious form of art, with Limoges enamel a popular and relatively affordable option for objects such as reliquaries and crosses. In Italy the innovations of Cimabue and Duccio, followed by the Trecento master Giotto (d. 1337), greatly increased the sophistication and status of panel painting and fresco. Increasing prosperity during the 12th century resulted in greater production of secular art; many carved ivory objects such as gaming-pieces, combs, and small religious figures have survived.

Monastic reform became an important issue during the 11th century, as elites began to worry that monks were not adhering to the rules binding them to a strictly religious life. Cluny Abbey, founded in the Mâcon region of France in 909, was established as part of the Cluniac Reforms, a larger movement of monastic reform in response to this fear. Cluny quickly established a reputation for austerity and rigour. It sought to maintain a high quality of spiritual life by placing itself under the protection of the papacy and by electing its own abbot without interference from laymen, thus maintaining economic and political independence from local lords.

Monastic reform inspired change in the secular Church. The ideals upon which it was based were brought to the papacy by Pope Leo IX (pope 1049–1054), and provided the ideology of clerical independence that led to the Investiture Controversy in the late 11th century. This involved Pope Gregory VII (pope 1073–85) and Emperor Henry IV, who initially clashed over episcopal appointments, a dispute that turned into a battle over the ideas of investiture, clerical marriage, and simony. The emperor saw the protection of the Church as one of his responsibilities as well as wanting to preserve the right to appoint his own choices as bishops within his lands, but the papacy insisted on the Church's independence from secular lords. These issues remained unresolved after the compromise of 1122 known as the Concordat of Worms. The dispute represents a significant stage in the creation of a papal monarchy separate from and equal to lay authorities. It also had the permanent consequence of empowering German princes at the expense of the German emperors.

The High Middle Ages was a period of great religious movements. Besides the Crusades and monastic reforms, people sought to participate in new forms of religious life. New monastic orders were founded, including the Carthusians and the Cistercians. The latter, in particular, expanded rapidly in their early years under the guidance of Bernard of Clairvaux (d. 1153). These new orders were formed in response to the feeling of the laity that Benedictine monasticism no longer met the needs of the laymen, who along with those wishing to enter the religious life wanted a return to the simpler hermetical monasticism of early Christianity, or to live an Apostolic life. Religious pilgrimages were also encouraged. Old pilgrimage sites such as Rome, Jerusalem, and Compostela received increasing numbers of visitors, and new sites such as Monte Gargano and Bari rose to prominence.

In the 13th century mendicant orders—the Franciscans and the Dominicans—who swore vows of poverty and earned their living by begging, were approved by the papacy. Religious groups such as the Waldensians and the Humiliati also attempted to return to the life of early Christianity in the middle 12th and early 13th centuries, another heretical movement condemned by the papacy. Others joined the Cathars, another movement condemned as heretical by the papacy. In 1209, a crusade was preached against the Cathars, the Albigensian Crusade, which in combination with the medieval Inquisition, eliminated them.

The first years of the 14th century were marked by famines, culminating in the Great Famine of 1315–17. The causes of the Great Famine included the slow transition from the Medieval Warm Period to the Little Ice Age, which left the population vulnerable when bad weather caused crop failures. The years 1313–14 and 1317–21 were excessively rainy throughout Europe, resulting in widespread crop failures. The climate change—which resulted in a declining average annual temperature for Europe during the 14th century—was accompanied by an economic downturn.

These troubles were followed in 1347 by the Black Death, a pandemic that spread throughout Europe during the following three years. The death toll was probably about 35 million people in Europe, about one-third of the population. Towns were especially hard-hit because of their crowded conditions. Large areas of land were left sparsely inhabited, and in some places fields were left unworked. Wages rose as landlords sought to entice the reduced number of available workers to their fields. Further problems were lower rents and lower demand for food, both of which cut into agricultural income. Urban workers also felt that they had a right to greater earnings, and popular uprisings broke out across Europe. Among the uprisings were the "jacquerie" in France, the Peasants' Revolt in England, and revolts in the cities of Florence in Italy and Ghent and Bruges in Flanders. The trauma of the plague led to an increased piety throughout Europe, manifested by the foundation of new charities, the self-mortification of the flagellants, and the scapegoating of Jews. Conditions were further unsettled by the return of the plague throughout the rest of the 14th century; it continued to strike Europe periodically during the rest of the Middle Ages.

Society throughout Europe was disturbed by the dislocations caused by the Black Death. Lands that had been marginally productive were abandoned, as the survivors were able to acquire more fertile areas. Although serfdom declined in Western Europe it became more common in Eastern Europe, as landlords imposed it on those of their tenants who had previously been free. Most peasants in Western Europe managed to change the work they had previously owed to their landlords into cash rents. The percentage of serfs amongst the peasantry declined from a high of 90 to closer to 50 percent by the end of the period. Landlords also became more conscious of common interests with other landholders, and they joined together to extort privileges from their governments. Partly at the urging of landlords, governments attempted to legislate a return to the economic conditions that existed before the Black Death. Non-clergy became increasingly literate, and urban populations began to imitate the nobility's interest in chivalry.

Jewish communities were expelled from England in 1290 and from France in 1306. Although some were allowed back into France, most were not, and many Jews emigrated eastwards, and Hungary. The Jews were expelled from Spain in 1492, and dispersed to Turkey, France, Italy, and Holland. The rise of banking in Italy during the 13th century continued throughout the 14th century, fuelled partly by the increasing warfare of the period and the needs of the papacy to move money between kingdoms. Many banking firms loaned money to royalty, at great risk, as some were bankrupted when kings defaulted on their loans.

Strong, royalty-based nation states rose throughout Europe in the Late Middle Ages, particularly in England, France, and the Christian kingdoms of the Iberian Peninsula: Aragon, Castile, and Portugal. The long conflicts of the period strengthened royal control over their kingdoms and were extremely hard on the peasantry. Kings profited from warfare that extended royal legislation and increased the lands they directly controlled. Paying for the wars required that methods of taxation become more effective and efficient, and the rate of taxation often increased. The requirement to obtain the consent of taxpayers allowed representative bodies such as the English Parliament and the French Estates General to gain power and authority.

Throughout the 14th century, French kings sought to expand their influence at the expense of the territorial holdings of the nobility. They ran into difficulties when attempting to confiscate the holdings of the English kings in southern France, leading to the Hundred Years' War, waged from 1337 to 1453. Early in the war the English under Edward III (r. 1327–77) and his son Edward, the Black Prince (d. 1376), won the battles of Crécy and Poitiers, captured the city of Calais, and won control of much of France. The resulting stresses almost caused the disintegration of the French kingdom during the early years of the war. In the early 15th century, France again came close to dissolving, but in the late 1420s the military successes of Joan of Arc (d. 1431) led to the victory of the French and the capture of the last English possessions in southern France in 1453. The price was high, as the population of France at the end of the Wars was likely half what it had been at the start of the conflict. Conversely, the Wars had a positive effect on English national identity, doing much to fuse the various local identities into a national English ideal. The conflict with France also helped create a national culture in England separate from French culture, which had previously been the dominant influence. The dominance of the English longbow began during early stages of the Hundred Years' War, and cannon appeared on the battlefield at Crécy in 1346.

In modern-day Germany, the Holy Roman Empire continued to rule, but the elective nature of the imperial crown meant there was no enduring dynasty around which a strong state could form. Further east, the kingdoms of Poland, Hungary, and Bohemia grew powerful. In Iberia, the Christian kingdoms continued to gain land from the Muslim kingdoms of the peninsula; Portugal concentrated on expanding overseas during the 15th century, while the other kingdoms were riven by difficulties over royal succession and other concerns. After losing the Hundred Years' War, England went on to suffer a long civil war known as the Wars of the Roses, which lasted into the 1490s and only ended when Henry Tudor (r. 1485–1509 as Henry VII) became king and consolidated power with his victory over Richard III (r. 1483–85) at Bosworth in 1485. In Scandinavia, Margaret I of Denmark (r. in Denmark 1387–1412) consolidated Norway, Denmark, and Sweden in the Union of Kalmar, which continued until 1523. The major power around the Baltic Sea was the Hanseatic League, a commercial confederation of city-states that traded from Western Europe to Russia. Scotland emerged from English domination under Robert the Bruce (r. 1306–29), who secured papal recognition of his kingship in 1328.

Although the Palaeologi emperors recaptured Constantinople from the Western Europeans in 1261, they were never able to regain control of much of the former imperial lands. They usually controlled only a small section of the Balkan Peninsula near Constantinople, the city itself, and some coastal lands on the Black Sea and around the Aegean Sea. The former Byzantine lands in the Balkans were divided between the new Kingdom of Serbia, the Second Bulgarian Empire and the city-state of Venice. The power of the Byzantine emperors was threatened by a new Turkish tribe, the Ottomans, who established themselves in Anatolia in the 13th century and steadily expanded throughout the 14th century. The Ottomans expanded into Europe, reducing Bulgaria to a vassal state by 1366 and taking over Serbia after its defeat at the Battle of Kosovo in 1389. Western Europeans rallied to the plight of the Christians in the Balkans and declared a new crusade in 1396; a great army was sent to the Balkans, where it was defeated at the Battle of Nicopolis. Constantinople was finally captured by the Ottomans in 1453.

During the tumultuous 14th century, disputes within the leadership of the Church led to the Avignon Papacy of 1309–76, also called the "Babylonian Captivity of the Papacy" (a reference to the Babylonian captivity of the Jews), and then to the Great Schism, lasting from 1378 to 1418, when there were two and later three rival popes, each supported by several states. Ecclesiastical officials convened at the Council of Constance in 1414, and in the following year the council deposed one of the rival popes, leaving only two claimants. Further depositions followed, and in November 1417, the council elected Martin V (pope 1417–31) as pope.

Besides the schism, the Western Church was riven by theological controversies, some of which turned into heresies. John Wycliffe (d. 1384), an English theologian, was condemned as a heretic in 1415 for teaching that the laity should have access to the text of the Bible as well as for holding views on the Eucharist that were contrary to Church doctrine. Wycliffe's teachings influenced two of the major heretical movements of the later Middle Ages: Lollardy in England and Hussitism in Bohemia. The Bohemian movement initiated with the teaching of Jan Hus, who was burned at the stake in 1415, after being condemned as a heretic by the Council of Constance. The Hussite Church, although the target of a crusade, survived beyond the Middle Ages. Other heresies were manufactured, such as the accusations against the Knights Templar that resulted in their suppression in 1312, and the division of their great wealth between the French King Philip IV (r. 1285–1314) and the Hospitallers.

The papacy further refined the practice in the Mass in the Late Middle Ages, holding that the clergy alone was allowed to partake of the wine in the Eucharist. This further distanced the secular laity from the clergy. The laity continued the practices of pilgrimages, veneration of relics, and belief in the power of the Devil. Mystics such as Meister Eckhart (d. 1327) and Thomas à Kempis (d. 1471) wrote works that taught the laity to focus on their inner spiritual life, which laid the groundwork for the Protestant Reformation. Besides mysticism, belief in witches and witchcraft became widespread, and by the late 15th century the Church had begun to lend credence to populist fears of witchcraft with its condemnation of witches in 1484, and the publication in 1486 of the "Malleus Maleficarum", the most popular handbook for witch-hunters.

During the Later Middle Ages, theologians such as John Duns Scotus (d. 1308) and William of Ockham (d. c. 1348) led a reaction against intellectualist scholasticism, objecting to the application of reason to faith. Their efforts undermined the prevailing Platonic idea of universals. Ockham's insistence that reason operates independently of faith allowed science to be separated from theology and philosophy. Legal studies were marked by the steady advance of Roman law into areas of jurisprudence previously governed by customary law. The lone exception to this trend was in England, where the common law remained pre-eminent. Other countries codified their laws; legal codes were promulgated in Castile, Poland, and Lithuania.

Education remained mostly focused on the training of future clergy. The basic learning of the letters and numbers remained the province of the family or a village priest, but the secondary subjects of the trivium—grammar, rhetoric, logic—were studied in cathedral schools or in schools provided by cities. Commercial secondary schools spread, and some Italian towns had more than one such enterprise. Universities also spread throughout Europe in the 14th and 15th centuries. Lay literacy rates rose, but were still low; one estimate gave a literacy rate of 10 per cent of males and 1 per cent of females in 1500.

The publication of vernacular literature increased, with Dante (d. 1321), Petrarch (d. 1374) and Giovanni Boccaccio (d. 1375) in 14th-century Italy, Geoffrey Chaucer (d. 1400) and William Langland (d. c. 1386) in England, and François Villon (d. 1464) and Christine de Pizan (d. c. 1430) in France. Much literature remained religious in character, and although a great deal of it continued to be written in Latin, a new demand developed for saints' lives and other devotional tracts in the vernacular languages. This was fed by the growth of the "Devotio Moderna" movement, most prominently in the formation of the Brethren of the Common Life, but also in the works of German mystics such as Meister Eckhart and Johannes Tauler (d. 1361). Theatre also developed in the guise of miracle plays put on by the Church. At the end of the period, the development of the printing press in about 1450 led to the establishment of publishing houses throughout Europe by 1500.

In the early 15th century, the countries of the Iberian Peninsula began to sponsor exploration beyond the boundaries of Europe. Prince Henry the Navigator of Portugal (d. 1460) sent expeditions that discovered the Canary Islands, the Azores, and Cape Verde during his lifetime. After his death, exploration continued; Bartolomeu Dias (d. 1500) went around the Cape of Good Hope in 1486, and Vasco da Gama (d. 1524) sailed around Africa to India in 1498. The combined Spanish monarchies of Castile and Aragon sponsored the voyage of exploration by Christopher Columbus (d. 1506) in 1492 that discovered the Americas. The English crown under Henry VII sponsored the voyage of John Cabot (d. 1498) in 1497, which landed on Cape Breton Island.

One of the major developments in the military sphere during the Late Middle Ages was the increased use of infantry and light cavalry. The English also employed longbowmen, but other countries were unable to create similar forces with the same success. Armour continued to advance, spurred by the increasing power of crossbows, and plate armour was developed to protect soldiers from crossbows as well as the hand-held guns that were developed. Pole arms reached new prominence with the development of the Flemish and Swiss infantry armed with pikes and other long spears.

In agriculture, the increased usage of sheep with long-fibred wool allowed a stronger thread to be spun. In addition, the spinning wheel replaced the traditional distaff for spinning wool, tripling production. A less technological refinement that still greatly affected daily life was the use of buttons as closures for garments, which allowed for better fitting without having to lace clothing on the wearer. Windmills were refined with the creation of the tower mill, allowing the upper part of the windmill to be spun around to face the direction from which the wind was blowing. The blast furnace appeared around 1350 in Sweden, increasing the quantity of iron produced and improving its quality. The first patent law in 1447 in Venice protected the rights of inventors to their inventions.

The Late Middle Ages in Europe as a whole correspond to the Trecento and Early Renaissance cultural periods in Italy. Northern Europe and Spain continued to use Gothic styles, which became increasingly elaborate in the 15th century, until almost the end of the period. International Gothic was a courtly style that reached much of Europe in the decades around 1400, producing masterpieces such as the Très Riches Heures du Duc de Berry. All over Europe secular art continued to increase in quantity and quality, and in the 15th century the mercantile classes of Italy and Flanders became important patrons, commissioning small portraits of themselves in oils as well as a growing range of luxury items such as jewellery, ivory caskets, cassone chests, and maiolica pottery. These objects also included the Hispano-Moresque ware produced by mostly Mudéjar potters in Spain. Although royalty owned huge collections of plate, little survives except for the Royal Gold Cup. Italian silk manufacture developed, so that Western churches and elites no longer needed to rely on imports from Byzantium or the Islamic world. In France and Flanders tapestry weaving of sets like "The Lady and the Unicorn" became a major luxury industry.

The large external sculptural schemes of Early Gothic churches gave way to more sculpture inside the building, as tombs became more elaborate and other features such as pulpits were sometimes lavishly carved, as in the Pulpit by Giovanni Pisano in Sant'Andrea. Painted or carved wooden relief altarpieces became common, especially as churches created many side-chapels. Early Netherlandish painting by artists such as Jan van Eyck (d. 1441) and Rogier van der Weyden (d. 1464) rivalled that of Italy, as did northern illuminated manuscripts, which in the 15th century began to be collected on a large scale by secular elites, who also commissioned secular books, especially histories. From about 1450 printed books rapidly became popular, though still expensive. There were around 30,000 different editions of incunabula, or works printed before 1500, by which time illuminated manuscripts were commissioned only by royalty and a few others. Very small woodcuts, nearly all religious, were affordable even by peasants in parts of Northern Europe from the middle of the 15th century. More expensive engravings supplied a wealthier market with a variety of images.

The medieval period is frequently caricatured as a "time of ignorance and superstition" that placed "the word of religious authorities over personal experience and rational activity." This is a legacy from both the Renaissance and Enlightenment when scholars favourably contrasted their intellectual cultures with those of the medieval period. Renaissance scholars saw the Middle Ages as a period of decline from the high culture and civilisation of the Classical world. Enlightenment scholars saw reason as superior to faith, and thus viewed the Middle Ages as a time of ignorance and superstition.

Others argue that reason was generally held in high regard during the Middle Ages. Science historian Edward Grant writes, "If revolutionary rational thoughts were expressed [in the 18th century], they were only made possible because of the long medieval tradition that established the use of reason as one of the most important of human activities". Also, contrary to common belief, David Lindberg writes, "the late medieval scholar rarely experienced the coercive power of the Church and would have regarded himself as free (particularly in the natural sciences) to follow reason and observation wherever they led".

The caricature of the period is also reflected in some more specific notions. One misconception, first propagated in the 19th century and still very common, is that all people in the Middle Ages believed that the Earth was flat. This is untrue, as lecturers in the medieval universities commonly argued that evidence showed the Earth was a sphere. Lindberg and Ronald Numbers, another scholar of the period, state that there "was scarcely a Christian scholar of the Middle Ages who did not acknowledge [Earth's] sphericity and even know its approximate circumference". Other misconceptions such as "the Church prohibited autopsies and dissections during the Middle Ages", "the rise of Christianity killed off ancient science", or "the medieval Christian Church suppressed the growth of natural philosophy", are all cited by Numbers as examples of widely popular myths that still pass as historical truth, although they are not supported by historical research.



</doc>
<doc id="18837" url="https://en.wikipedia.org/wiki?curid=18837" title="Median">
Median

In statistics and probability theory, a median is a value separating the higher half from the lower half of a data sample, a population or a probability distribution. For a data set, it may be thought of as "the middle" value. For example,
the basic advantage of the median in describing data compared to the mean (often simply described as the "average") is that it is not skewed so much by a small proportion of extremely large or small values, and so it may give a better idea of a "typical" value. For example, in understanding statistics like household income or assets, which vary greatly, the mean may be skewed by a small number of extremely high or low values. Median income, for example, may be a better way to suggest what a "typical" income is.
Because of this, the median is of central importance in robust statistics, as it is the most resistant statistic, having a breakdown point of 50%: so long as no more than half the data are contaminated, the median will not give an arbitrarily large or small result.
The median of a finite list of numbers is the "middle" number, when those numbers are listed in order from smallest to greatest.

If there is an odd number of numbers, the middle one is picked. For example, consider the list of numbers

This list contains seven numbers. The median is the fourth of them, which is 6.

If there is an even number of observations, then there is no single middle value; the median is then usually defined to be the mean of the two middle values. For example, in the data set

the median is the mean of the middle two numbers: this is formula_1, which is formula_2. (In more technical terms, this interprets the median as the fully trimmed mid-range). With this convention, the median can be described in a caseless formula, as follows:

where formula_4 is an ordered list of formula_5 numbers, and formula_6 and formula_7 denote the floor and ceiling functions, respectively.

Formally, a median of a population is any value such that at most half of the population is less than the proposed median and at most half is greater than the proposed median. As seen above, medians may not be unique. If each set contains less than half the population, then some of the population is exactly equal to the unique median.

The median is well-defined for any ordered (one-dimensional) data, and is independent of any distance metric. The median can thus be applied to classes which are ranked but not numerical (e.g. working out a median grade when students are graded from A to F), although the result might be halfway between classes if there is an even number of cases.

A geometric median, on the other hand, is defined in any number of dimensions. A related concept, in which the outcome is forced to correspond to a member of the sample, is the medoid.

There is no widely accepted standard notation for the median, but some authors represent the median of a variable "x" either as "x͂" or as "μ" sometimes also "M". In any of these cases, the use of these or other symbols for the median needs to be explicitly defined when they are introduced.

The median is a special case of other ways of summarising the typical values associated with a statistical distribution: it is the 2nd quartile, 5th decile, and 50th percentile.

The median can be used as a measure of location when one attaches reduced importance to extreme values, typically because a distribution is skewed, extreme values are not known, or outliers are untrustworthy, i.e., may be measurement/transcription errors.

For example, consider the multiset

The median is 2 in this case, (as is the mode), and it might be seen as a better indication of the center than the arithmetic mean of 4, which is larger than all-but-one of the values. However, the widely cited empirical relationship that the mean is shifted "further into the tail" of a distribution than the median is not generally true. At most, one can say that the two statistics cannot be "too far" apart; see below.

As a median is based on the middle data in a set, it is not necessary to know the value of extreme results in order to calculate it. For example, in a psychology test investigating the time needed to solve a problem, if a small number of people failed to solve the problem at all in the given time a median can still be calculated.

Because the median is simple to understand and easy to calculate, while also a robust approximation to the mean, the median is a popular summary statistic in descriptive statistics. In this context, there are several choices for a measure of variability: the range, the interquartile range, the mean absolute deviation, and the median absolute deviation.

For practical purposes, different measures of location and dispersion are often compared on the basis of how well the corresponding population values can be estimated from a sample of data. The median, estimated using the sample median, has good properties in this regard. While it is not usually optimal if a given population distribution is assumed, its properties are always reasonably good. For example, a comparison of the efficiency of candidate estimators shows that the sample mean is more statistically efficient when — and only when — data is uncontaminated by data from heavy-tailed distributions or from mixtures of distributions. Even then, the median has a 64% efficiency compared to the minimum-variance mean (for large normal samples), which is to say the variance of the median will be ~50% greater than the variance of the mean.

For any real-valued probability distribution with cumulative distribution function "F", a median is defined as any real number "m" that satisfies the inequalities

An equivalent phrasing uses a random variable "X" distributed according to "F":

Note that this definition does not require "X" to have an absolutely continuous distribution (which has a probability density function "ƒ"), nor does it require a discrete one. In the former case, the inequalities can be upgraded to equality: a median satisfies

Any probability distribution on R has at least one median, but in pathological cases there may be more than one median: if "F" is constant 1/2 on an interval (so that "ƒ"=0 there), then any value of that interval is a median.

The medians of certain types of distributions can be easily calculated from their parameters; furthermore, they exist even for some distributions lacking a well-defined mean, such as the Cauchy distribution:

The "mean absolute error" of a real variable "c" with respect to the random variable "X" is
Provided that the probability distribution of "X" is such that the above expectation exists, then "m" is a median of "X" if and only if "m" is a minimizer of the mean absolute error with respect to "X". In particular, "m" is a sample median if and only if "m" minimizes the arithmetic mean of the absolute deviations.

More generally, a median is defined as a minimum of
as discussed below in the section on multivariate medians (specifically, the spatial median).

This optimization-based definition of the median is useful in statistical data-analysis, for example, in "k"-medians clustering.

If the distribution has finite variance, then the distance between the median formula_13 and the mean formula_14 is bounded by one standard deviation.

This bound was proved by Mallows, who used Jensen's inequality twice, as follows. Using |·| for the absolute value, we have

The first and third inequalities come from Jensen's inequality applied to the absolute-value function and the square function, which are each convex. The second inequality comes from the fact that a median minimizes the absolute deviation function formula_16.

Mallows' proof can be generalized to obtain a multivariate version of the inequality simply by replacing the absolute value with a norm:

where "m" is a spatial median, that is, a minimizer of the function formula_18 The spatial median is unique when the data-set's dimension is two or more.

An alternative proof uses the one-sided Chebyshev inequality; it appears in an inequality on location and scale parameters. This formula also follows directly from Cantelli's inequality.

For the case of unimodal distributions, one can achieve a sharper bound on the distance between the median and the mean:

A similar relation holds between the median and the mode:

Jensen's inequality states that for any random variable "X" with a finite expectation "E"["X"] and for any convex function "f"

This inequality generalizes to the median as well. We say a function is a C function if, for any "t",

is a closed interval (allowing the degenerate cases of a single point or an empty set). Every C function is convex, but the reverse does not hold. If "f" is a C function, then

If the medians are not unique, the statement holds for the corresponding suprema.

Even though comparison-sorting "n" items requires operations, selection algorithms can compute the th-smallest of items with only operations. This includes the median, which is the th order statistic (or for an even number of samples, the arithmetic mean of the two middle order statistics).

Selection algorithms still have the downside of requiring memory, that is, they need to have the full sample (or a linear-sized portion of it) in memory. Because this, as well as the linear time requirement, can be prohibitive, several estimation procedures for the median have been developed. A simple one is the median of three rule, which estimates the median as the median of a three-element subsample; this is commonly used as a subroutine in the quicksort sorting algorithm, which uses an estimate of its input's median. A more robust estimator is Tukey's "ninther", which is the median of three rule applied with limited recursion: if is the sample laid out as an array, and

then

The "remedian" is an estimator for the median that requires linear time but sub-linear memory, operating in a single pass over the sample.

The distributions of both the sample mean and the sample median were determined by Laplace. The distribution of the sample median from a population with a density function formula_24 is asymptotically normal with mean formula_25 and variance

where formula_25 is the median of formula_24 and formula_5 is the sample size. A modern proof follows below. Laplace's result is now understood as a special case of the asymptotic distribution of arbitrary quantiles.

For normal samples, the density is formula_30, thus for large samples the variance of the median equals formula_31 (See also section #Efficiency below.)

We take the sample size to be an odd number formula_32 and assume our variable continuous; the formula for the case of discrete variables is given below in . The sample can be summarized as "below median", "at median", and "above median", which corresponds to a trinomial distribution with probabilities formula_33, formula_34 and formula_35. For a continuous variable, the probability of multiple sample values being exactly equal to the median is 0, so one can calculate the density of at the point formula_36 directly from the trinomial distribution:

Now we introduce the beta function. For integer arguments formula_38 and formula_39, this can be expressed as formula_40. Also, recall that formula_41. Using these relationships and setting both formula_38 and formula_39 equal to formula_44 allows the last expression to be written as

Hence the density function of the median is a symmetric beta distribution pushed forward by formula_46. Its mean, as we would expect, is 0.5 and its variance is formula_47. By the chain rule, the corresponding variance of the sample median is

The additional 2 is negligible in the limit.

In practice, the functions formula_49 and formula_50 are often not known or assumed. However, they can be estimated from an observed frequency distribution. In this section, we give an example. Consider the following table, representing a sample of 3,800 (discrete-valued) observations:

Because the observations are discrete-valued, constructing the exact distribution of the median is not an immediate translation of the above expression for formula_51; one may (and typically does) have multiple instances of the median in one's sample. So we must sum over all these possibilities:

Here, "i" is the number of points strictly less than the median and "k" the number strictly greater.

Using these preliminaries, it is possible to investigate the effect of sample size on the standard errors of the mean and median. The observed mean is 3.16, the observed raw median is 3 and the observed interpolated median is 3.174. The following table gives some comparison statistics.
The expected value of the median falls slightly as sample size increases while, as would be expected, the standard errors of both the median and the mean are proportionate to the inverse square root of the sample size. The asymptotic approximation errs on the side of caution by overestimating the standard error.

The value of formula_53—the asymptotic value of formula_54 where formula_55 is the population median—has been studied by several authors. The standard "delete one" jackknife method produces inconsistent results. An alternative—the "delete k" method—where formula_56 grows with the sample size has been shown to be asymptotically consistent. This method may be computationally expensive for large data sets. A bootstrap estimate is known to be consistent, but converges very slowly (order of formula_57). Other methods have been proposed but their behavior may differ between large and small samples.

The efficiency of the sample median, measured as the ratio of the variance of the mean to the variance of the median, depends on the sample size and on the underlying population distribution. For a sample of size formula_58 from the normal distribution, the efficiency for large N is

The efficiency tends to formula_60 as formula_61 tends to infinity.

In other words, the relative variance of the median will be formula_62, or 57% greater than the variance of the mean – the relative standard error of the median will be formula_63, or 25% greater than the standard error of the mean, formula_64 (see also section #Sampling distribution above.).

For univariate distributions that are "symmetric" about one median, the Hodges–Lehmann estimator is a robust and highly efficient estimator of the population median.

If data are represented by a statistical model specifying a particular family of probability distributions, then estimates of the median can be obtained by fitting that family of probability distributions to the data and calculating the theoretical median of the fitted distribution. Pareto interpolation is an application of this when the population is assumed to have a Pareto distribution.

Previously, this article discussed the univariate median, when the sample or population had one-dimension. When the dimension is two or higher, there are multiple concepts that extend the definition of the univariate median; each such multivariate median agrees with the univariate median when the dimension is exactly one.

The marginal median is defined for vectors defined with respect to a fixed set of coordinates. A marginal median is defined to be the vector whose components are univariate medians. The marginal median is easy to compute, and its properties were studied by Puri and Sen.

The geometric median of a discrete set of sample points formula_65 in a Euclidean space is the point minimizing the sum of distances to the sample points.

In contrast to the marginal median, the geometric median is equivariant with respect to Euclidean similarity transformations such as translations and rotations.

An alternative generalization of the median in higher dimensions is the centerpoint.

When dealing with a discrete variable, it is sometimes useful to regard the observed values as being midpoints of underlying continuous intervals. An example of this is a Likert scale, on which opinions or preferences are expressed on a scale with a set number of possible responses. If the scale consists of the positive integers, an observation of 3 might be regarded as representing the interval from 2.50 to 3.50. It is possible to estimate the median of the underlying variable. If, say, 22% of the observations are of value 2 or below and 55.0% are of 3 or below (so 33% have the value 3), then the median formula_67 is 3 since the median is the smallest value of formula_68 for which formula_69 is greater than a half. But the interpolated median is somewhere between 2.50 and 3.50. First we add half of the interval width formula_70 to the median to get the upper bound of the median interval. Then we subtract that proportion of the interval width which equals the proportion of the 33% which lies above the 50% mark. In other words, we split up the interval width pro rata to the numbers of observations. In this case, the 33% is split into 28% below the median and 5% above it so we subtract 5/33 of the interval width from the upper bound of 3.50 to give an interpolated median of 3.35. More formally, if the values formula_71 are known, the interpolated median can be calculated from

Alternatively, if in an observed sample there are formula_73 scores above the median category, formula_74 scores in it and formula_75 scores below it then the interpolated median is given by

For univariate distributions that are "symmetric" about one median, the Hodges–Lehmann estimator is a robust and highly efficient estimator of the population median; for non-symmetric distributions, the Hodges–Lehmann estimator is a robust and highly efficient estimator of the population "pseudo-median", which is the median of a symmetrized distribution and which is close to the population median. The Hodges–Lehmann estimator has been generalized to multivariate distributions.

The Theil–Sen estimator is a method for robust linear regression based on finding medians of slopes.

In the context of image processing of monochrome raster images there is a type of noise, known as the salt and pepper noise, when each pixel independently becomes black (with some small probability) or white (with some small probability), and is unchanged otherwise (with the probability close to 1). An image constructed of median values of neighborhoods (like 3×3 square) can effectively reduce noise in this case.

In cluster analysis, the k-medians clustering algorithm provides a way of defining clusters, in which the criterion of maximising the distance between cluster-means that is used in k-means clustering, is replaced by maximising the distance between cluster-medians.

This is a method of robust regression. The idea dates back to Wald in 1940 who suggested dividing a set of bivariate data into two halves depending on the value of the independent parameter formula_4: a left half with values less than the median and a right half with values greater than the median. He suggested taking the means of the dependent formula_78 and independent formula_4 variables of the left and the right halves and estimating the slope of the line joining these two points. The line could then be adjusted to fit the majority of the points in the data set.

Nair and Shrivastava in 1942 suggested a similar idea but instead advocated dividing the sample into three equal parts before calculating the means of the subsamples. Brown and Mood in 1951 proposed the idea of using the medians of two subsamples rather the means. Tukey combined these ideas and recommended dividing the sample into three equal size subsamples and estimating the line based on the medians of the subsamples.

Any "mean"-unbiased estimator minimizes the risk (expected loss) with respect to the squared-error loss function, as observed by Gauss. A "median"-unbiased estimator minimizes the risk with respect to the absolute-deviation loss function, as observed by Laplace. Other loss functions are used in statistical theory, particularly in robust statistics.

The theory of median-unbiased estimators was revived by George W. Brown in 1947:

Further properties of median-unbiased estimators have been reported. Median-unbiased estimators are invariant under one-to-one transformations.

There are methods of constructing median-unbiased estimators that are optimal (in a sense analogous to the minimum-variance property for mean-unbiased estimators). Such constructions exist for probability distributions having monotone likelihood-functions. One such procedure is an analogue of the Rao–Blackwell procedure for mean-unbiased estimators: The procedure holds for a smaller class of probability distributions than does the Rao—Blackwell procedure but for a larger class of loss functions.

Scientific researchers in the ancient near east appear not to have used summary statistics altogether, instead choosing values that offered maximal consistency with a broader theory that integrated a wide variety of phenomena. Within the Mediterranean (and, later, European) scholarly community, statistics like the mean are fundamentally a medieval and early modern development. (The history of the median outside Europe and its predecessors remains relatively unstudied.)

The idea of the median appeared in the 13th century in the Talmud, in order to fairly analyze divergent appraisals. However, the concept did not spread to the broader scientific community.

Instead, the closest ancestor of the modern median is the mid-range, invented by Al-Biruni. Transmission of Al-Biruni's work to later scholars is unclear. Al-Biruni applied his technique to assaying metals, but, after he published his work, most assayers still adopted the most unfavorable value from their results, lest they appear to cheat. However, increased navigation at sea during the Age of Discovery meant that ship's navigators increasingly had to attempt to determine latitude in unfavorable weather against hostile shores, leading to renewed interest in summary statistics. Whether rediscovered or independently invented, the mid-range is recommended to nautical navigators in Harriot's "Instructions for Raleigh's Voyage to Guiana, 1595".

The idea of the median may have first appeared in Edward Wright's 1599 book "Certaine Errors in Navigation" on a section about compass navigation. Wright was reluctant to discard measured values, and may have felt that the median — incorporating a greater proportion of the dataset than the mid-range — was more likely to be correct. However, Wright did not give examples of his technique's use, making it hard to verify that he described the modern notion of median. The median (in the context of probability) certainly appeared in the correspondence of Christiaan Huygens, but as an example of a statistic that was inappropriate for actuarial practice.

The earliest recommendation of the median dates to 1757, when Roger Joseph Boscovich developed a regression method based on the "L" norm and therefore implicitly on the median. In 1774, Laplace made this desire explicit: he suggested the median be used as the standard estimator of the value of a posterior PDF. The specific criterion was to minimize the expected magnitude of the error; formula_80 where formula_81 is the estimate and formula_82 is the true value. To this end, Laplace determined the distributions of both the sample mean and the sample median in the early 1800s. However, a decade later, Gauss and Legendre developed the least squares method, which minimizes formula_83 to obtain the mean. Within the context of regression, Gauss and Legendre's innovation offers vastly easier computation. Consequently, Laplaces' proposal was generally rejected until the rise of computing devices 150 years later (and is still a relatively uncommon algorithm).

Antoine Augustin Cournot in 1843 was the first to use the term "median" ("valeur médiane") for the value that divides a probability distribution into two equal halves. Gustav Theodor Fechner used the median ("Centralwerth") in sociological and psychological phenomena. It had earlier been used only in astronomy and related fields. Gustav Fechner popularized the median into the formal analysis of data, although it had been used previously by Laplace, and the median appeared in a textbook by F. Y. Edgeworth. Francis Galton used the English term "median" in 1881, having earlier used the terms "middle-most value" in 1869, and the "medium" in 1880.

Statisticians encouraged the use of medians intensely throughout the 19th century for its intuitive clarity and ease of manual computation. However, the notion of median does not lend itself to the theory of higher moments as well as the arithmetic mean does, and is much harder to compute by computer. As a result, the median was steadily supplanted as a notion of generic average by the arithmetic mean during the 20th century. 




</doc>
<doc id="18838" url="https://en.wikipedia.org/wiki?curid=18838" title="Mammal">
Mammal

Mammals (from Latin "mamma" "breast") are vertebrate animals constituting the class Mammalia (), and characterized by the presence of mammary glands which in females produce milk for feeding (nursing) their young, a neocortex (a region of the brain), fur or hair, and three middle ear bones. These characteristics distinguish them from reptiles and birds, from which they diverged in the late Carboniferous, approximately 300 million years ago. Around 6,400 extant species of mammals have been described. The largest orders are the rodents, bats and Eulipotyphla (hedgehogs, moles, shrews, and others). The next three are the Primates (apes including humans, monkeys, and others), the Cetartiodactyla (cetaceans and even-toed ungulates), and the Carnivora (cats, dogs, seals, and others).

In terms of cladistics, which reflects evolutionary history, mammals are the only living members of the Synapsida; this clade, together with Sauropsida (reptiles and birds), constitutes the larger Amniota clade. The early synapsid mammalian ancestors were sphenacodont pelycosaurs, a group that included the non-mammalian "Dimetrodon". At the end of the Carboniferous period around 300 million years ago, this group diverged from the sauropsid line that led to today's reptiles and birds. The line following the stem group Sphenacodontia split into several diverse groups of non-mammalian synapsids—sometimes incorrectly referred to as mammal-like reptiles—before giving rise to Therapsida in the Early Permian period. The modern mammalian orders arose in the Paleogene and Neogene periods of the Cenozoic era, after the extinction of non-avian dinosaurs, and have been the dominant terrestrial animal group from 66 million years ago to the present.

The basic body type is quadruped, and most mammals use their four extremities for terrestrial locomotion; but in some, the extremities are adapted for life at sea, in the air, in trees, underground, or on two legs. Mammals range in size from the bumblebee bat to the blue whale—possibly the largest animal to have ever lived. Maximum lifespan varies from two years for the shrew to 211 years for the bowhead whale. All modern mammals give birth to live young, except the five species of monotremes, which are egg-laying mammals. The most species-rich group of mammals, the cohort called placentals, have a placenta, which enables the feeding of the fetus during gestation.

Most mammals are intelligent, with some possessing large brains, self-awareness, and tool use. Mammals can communicate and vocalize in several ways, including the production of ultrasound, scent-marking, alarm signals, singing, and echolocation. Mammals can organize themselves into fission-fusion societies, harems, and hierarchies—but can also be solitary and territorial. Most mammals are polygynous, but some can be monogamous or polyandrous.

Domestication of many types of mammals by humans played a major role in the Neolithic revolution, and resulted in farming replacing hunting and gathering as the primary source of food for humans. This led to a major restructuring of human societies from nomadic to sedentary, with more co-operation among larger and larger groups, and ultimately the development of the first civilizations. Domesticated mammals provided, and continue to provide, power for transport and agriculture, as well as food (meat and dairy products), fur, and leather. Mammals are also hunted and raced for sport, and are used as model organisms in science. Mammals have been depicted in art since Palaeolithic times, and appear in literature, film, mythology, and religion. Decline in numbers and extinction of many mammals is primarily driven by human poaching and habitat destruction, primarily deforestation.

Mammal classification has been through several iterations since Carl Linnaeus initially defined the class. No classification system is universally accepted; McKenna & Bell (1997) and Wilson & Reader (2005) provide useful recent compendiums. George Gaylord Simpson's "Principles of Classification and a Classification of Mammals" (AMNH "Bulletin" v. 85, 1945) provides systematics of mammal origins and relationships that were universally taught until the end of the 20th century. Since Simpson's classification, the paleontological record has been recalibrated, and the intervening years have seen much debate and progress concerning the theoretical underpinnings of systematization itself, partly through the new concept of cladistics. Though field work gradually made Simpson's classification outdated, it remains the closest thing to an official classification of mammals.

Most mammals, including the six most species-rich orders, belong to the placental group. The three largest orders in numbers of species are Rodentia: mice, rats, porcupines, beavers, capybaras and other gnawing mammals; Chiroptera: bats; and Soricomorpha: shrews, moles and solenodons. The next three biggest orders, depending on the biological classification scheme used, are the Primates including the apes, monkeys and lemurs; the Cetartiodactyla including whales and even-toed ungulates; and the Carnivora which includes cats, dogs, weasels, bears, seals and allies. According to "Mammal Species of the World", 5,416 species were identified in 2006. These were grouped into 1,229 genera, 153 families and 29 orders. In 2008, the International Union for Conservation of Nature (IUCN) completed a five-year Global Mammal Assessment for its IUCN Red List, which counted 5,488 species. According to research published in the "Journal of Mammalogy" in 2018, the number of recognized mammal species is 6,495 including 96 recently extinct.

The word "mammal" is modern, from the scientific name "Mammalia" coined by Carl Linnaeus in 1758, derived from the Latin "mamma" ("teat, pap"). In an influential 1988 paper, Timothy Rowe defined Mammalia phylogenetically as the crown group of mammals, the clade consisting of the most recent common ancestor of living monotremes (echidnas and platypuses) and therian mammals (marsupials and placentals) and all descendants of that ancestor. Since this ancestor lived in the Jurassic period, Rowe's definition excludes all animals from the earlier Triassic, despite the fact that Triassic fossils in the Haramiyida have been referred to the Mammalia since the mid-19th century. If Mammalia is considered as the crown group, its origin can be roughly dated as the first known appearance of animals more closely related to some extant mammals than to others. "Ambondro" is more closely related to monotremes than to therian mammals while "Amphilestes" and "Amphitherium" are more closely related to the therians; as fossils of all three genera are dated about in the Middle Jurassic, this is a reasonable estimate for the appearance of the crown group.

T. S. Kemp has provided a more traditional definition: "synapsids that possess a dentary–squamosal jaw articulation and occlusion between upper and lower molars with a transverse component to the movement" or, equivalently in Kemp's view, the clade originating with the last common ancestor of "Sinoconodon" and living mammals. The earliest known synapsid satisfying Kemp's definitions is "Tikitherium", dated , so the appearance of mammals in this broader sense can be given this Late Triassic date.

In 1997, the mammals were comprehensively revised by Malcolm C. McKenna and Susan K. Bell, which has resulted in the McKenna/Bell classification. Their 1997 book, "Classification of Mammals above the Species Level", is a comprehensive work on the systematics, relationships and occurrences of all mammal taxa, living and extinct, down through the rank of genus, though molecular genetic data challenge several of the higher level groupings. The authors worked together as paleontologists at the American Museum of Natural History, New York. McKenna inherited the project from Simpson and, with Bell, constructed a completely updated hierarchical system, covering living and extinct taxa that reflects the historical genealogy of Mammalia.

In the following list, extinct groups are labelled with a dagger (†).

Class Mammalia

As of the early 21st century, molecular studies based on DNA analysis have suggested new relationships among mammal families. Most of these findings have been independently validated by retrotransposon presence/absence data. Classification systems based on molecular studies reveal three major groups or lineages of placental mammals—Afrotheria, Xenarthra and Boreoeutheria—which diverged in the Cretaceous. The relationships between these three lineages is contentious, and all three possible hypotheses have been proposed with respect to which group is basal. These hypotheses are Atlantogenata (basal Boreoeutheria), Epitheria (basal Xenarthra) and Exafroplacentalia (basal Afrotheria). Boreoeutheria in turn contains two major lineages—Euarchontoglires and Laurasiatheria.

Estimates for the divergence times between these three placental groups range from 105 to 120 million years ago, depending on the type of DNA used (such as nuclear or mitochondrial) and varying interpretations of paleogeographic data.

The cladogram above is based on Tarver "et al". (2016)

Group I: Superorder Afrotheria
Group II: Superorder Xenarthra
Group III: Magnaorder Boreoeutheria

Synapsida, a clade that contains mammals and their extinct relatives, originated during the Pennsylvanian subperiod (~323 million to ~300 million years ago), when they split from reptilian and avian lineages. Crown group mammals evolved from earlier mammaliaforms during the Early Jurassic. The cladogram takes Mammalia to be the crown group.

The first fully terrestrial vertebrates were amniotes. Like their amphibious tetrapod predecessors, they had lungs and limbs. Amniotic eggs, however, have internal membranes that allow the developing embryo to breathe but keep water in. Hence, amniotes can lay eggs on dry land, while amphibians generally need to lay their eggs in water.

The first amniotes apparently arose in the Pennsylvanian subperiod of the Carboniferous. They descended from earlier reptiliomorph amphibious tetrapods, which lived on land that was already inhabited by insects and other invertebrates as well as ferns, mosses and other plants. Within a few million years, two important amniote lineages became distinct: the synapsids, which would later include the common ancestor of the mammals; and the sauropsids, which now include turtles, lizards, snakes, crocodilians and dinosaurs (including birds). Synapsids have a single hole (temporal fenestra) low on each side of the skull. One synapsid group, the pelycosaurs, included the largest and fiercest animals of the early Permian. Nonmammalian synapsids are sometimes (inaccurately) called "mammal-like reptiles".

Therapsids, a group of synapsids, descended from pelycosaurs in the Middle Permian, about 265 million years ago, and became the dominant land vertebrates. They differ from basal eupelycosaurs in several features of the skull and jaws, including: larger skulls and incisors which are equal in size in therapsids, but not for eupelycosaurs. The therapsid lineage leading to mammals went through a series of stages, beginning with animals that were very similar to their pelycosaur ancestors and ending with probainognathian cynodonts, some of which could easily be mistaken for mammals. Those stages were characterized by:

The Permian–Triassic extinction event about 252 million years ago, which was a prolonged event due to the accumulation of several extinction pulses, ended the dominance of carnivorous therapsids. In the early Triassic, most medium to large land carnivore niches were taken over by archosaurs which, over an extended period (35 million years), came to include the crocodylomorphs, the pterosaurs and the dinosaurs; however, large cynodonts like "Trucidocynodon" and traversodontids still occupied large sized carnivorous and herbivorous niches respectively. By the Jurassic, the dinosaurs had come to dominate the large terrestrial herbivore niches as well.

The first mammals (in Kemp's sense) appeared in the Late Triassic epoch (about 225 million years ago), 40 million years after the first therapsids. They expanded out of their nocturnal insectivore niche from the mid-Jurassic onwards; The Jurassic "Castorocauda", for example, was a close relative of true mammals that had adaptations for swimming, digging and catching fish. Most, if not all, are thought to have remained nocturnal (the nocturnal bottleneck), accounting for much of the typical mammalian traits. The majority of the mammal species that existed in the Mesozoic Era were multituberculates, eutriconodonts and spalacotheriids. The earliest known metatherian is "Sinodelphys", found in 125 million-year-old Early Cretaceous shale in China's northeastern Liaoning Province. The fossil is nearly complete and includes tufts of fur and imprints of soft tissues.

The oldest known fossil among the Eutheria ("true beasts") is the small shrewlike "Juramaia sinensis", or "Jurassic mother from China", dated to 160 million years ago in the late Jurassic. A later eutherian relative, "Eomaia", dated to 125 million years ago in the early Cretaceous, possessed some features in common with the marsupials but not with the placentals, evidence that these features were present in the last common ancestor of the two groups but were later lost in the placental lineage. In particular, the epipubic bones extend forwards from the pelvis. These are not found in any modern placental, but they are found in marsupials, monotremes, other nontherian mammals and "Ukhaatherium", an early Cretaceous animal in the eutherian order Asioryctitheria. This also applies to the multituberculates. They are apparently an ancestral feature, which subsequently disappeared in the placental lineage. These epipubic bones seem to function by stiffening the muscles during locomotion, reducing the amount of space being presented, which placentals require to contain their fetus during gestation periods. A narrow pelvic outlet indicates that the young were very small at birth and therefore pregnancy was short, as in modern marsupials. This suggests that the placenta was a later development.

One of the earliest known monotremes was "Teinolophos", which lived about 120 million years ago in Australia. Monotremes have some features which may be inherited from the original amniotes such as the same orifice to urinate, defecate and reproduce (cloaca)—as lizards and birds also do— and they lay eggs which are leathery and uncalcified.

"Hadrocodium", whose fossils date from approximately 195 million years ago, in the early Jurassic, provides the first clear evidence of a jaw joint formed solely by the squamosal and dentary bones; there is no space in the jaw for the articular, a bone involved in the jaws of all early synapsids.
The earliest clear evidence of hair or fur is in fossils of "Castorocauda" and "Megaconus", from 164 million years ago in the mid-Jurassic. In the 1950s, it was suggested that the foramina (passages) in the maxillae and premaxillae (bones in the front of the upper jaw) of cynodonts were channels which supplied blood vessels and nerves to vibrissae (whiskers) and so were evidence of hair or fur; it was soon pointed out, however, that foramina do not necessarily show that an animal had vibrissae, as the modern lizard "Tupinambis" has foramina that are almost identical to those found in the nonmammalian cynodont "Thrinaxodon". Popular sources, nevertheless, continue to attribute whiskers to "Thrinaxodon". Studies on Permian coprolites suggest that non-mammalian synapsids of the epoch already had fur, setting the evolution of hairs possibly as far back as dicynodonts.

When endothermy first appeared in the evolution of mammals is uncertain, though it is generally agreed to have first evolved in non-mammalian therapsids. Modern monotremes have lower body temperatures and more variable metabolic rates than marsupials and placentals, but there is evidence that some of their ancestors, perhaps including ancestors of the therians, may have had body temperatures like those of modern therians. Likewise, some modern therians like afrotheres and xenarthrans have secondarily developed lower body temperatures.

The evolution of erect limbs in mammals is incomplete—living and fossil monotremes have sprawling limbs. The parasagittal (nonsprawling) limb posture appeared sometime in the late Jurassic or early Cretaceous; it is found in the eutherian "Eomaia" and the metatherian "Sinodelphys", both dated to 125 million years ago. Epipubic bones, a feature that strongly influenced the reproduction of most mammal clades, are first found in Tritylodontidae, suggesting that it is a synapomorphy between them and mammaliformes. They are omnipresent in non-placental mammaliformes, though "Megazostrodon" and "Erythrotherium" appear to have lacked them.

It has been suggested that the original function of lactation (milk production) was to keep eggs moist. Much of the argument is based on monotremes, the egg-laying mammals.

Therian mammals took over the medium- to large-sized ecological niches in the Cenozoic, after the Cretaceous–Paleogene extinction event approximately 66 million years ago emptied ecological space once filled by non-avian dinosaurs and other groups of reptiles, as well as various other mammal groups, and underwent an exponential increase in body size (megafauna). Then mammals diversified very quickly; both birds and mammals show an exponential rise in diversity. For example, the earliest known bat dates from about 50 million years ago, only 16 million years after the extinction of the non-avian dinosaurs.

Molecular phylogenetic studies initially suggested that most placental orders diverged about 100 to 85 million years ago and that modern families appeared in the period from the late Eocene through the Miocene. However, no placental fossils have been found from before the end of the Cretaceous. The earliest undisputed fossils of placentals comes from the early Paleocene, after the extinction of the non-avian dinosaurs. In particular, scientists have identified an early Paleocene animal named "Protungulatum donnae" as one of the first placental mammals. however it has been reclassified as a non-placental eutherian. Recalibrations of genetic and morphological diversity rates have suggested a Late Cretaceous origin for placentals, and a Paleocene origin for most modern clades.

The earliest known ancestor of primates is "Archicebus achilles" from around 55 million years ago. This tiny primate weighed 20–30 grams (0.7–1.1 ounce) and could fit within a human palm.

Living mammal species can be identified by the presence of sweat glands, including those that are specialized to produce milk to nourish their young. In classifying fossils, however, other features must be used, since soft tissue glands and many other features are not visible in fossils.

Many traits shared by all living mammals appeared among the earliest members of the group:

For the most part, these characteristics were not present in the Triassic ancestors of the mammals. Nearly all mammaliaforms possess an epipubic bone, the exception being modern placentals.

The majority of mammals have seven cervical vertebrae (bones in the neck), including bats, giraffes, whales and humans. The exceptions are the manatee and the two-toed sloth, which have just six, and the three-toed sloth which has nine cervical vertebrae. All mammalian brains possess a neocortex, a brain region unique to mammals. Placental mammals have a corpus callosum, unlike monotremes and marsupials.

The lungs of mammals are spongy and honeycombed. Breathing is mainly achieved with the diaphragm, which divides the thorax from the abdominal cavity, forming a dome convex to the thorax. Contraction of the diaphragm flattens the dome, increasing the volume of the lung cavity. Air enters through the oral and nasal cavities, and travels through the larynx, trachea and bronchi, and expands the alveoli. Relaxing the diaphragm has the opposite effect, decreasing the volume of the lung cavity, causing air to be pushed out of the lungs. During exercise, the abdominal wall contracts, increasing pressure on the diaphragm, which forces air out quicker and more forcefully. The rib cage is able to expand and contract the chest cavity through the action of other respiratory muscles. Consequently, air is sucked into or expelled out of the lungs, always moving down its pressure gradient. This type of lung is known as a bellows lung due to its resemblance to blacksmith bellows.

The mammalian heart has four chambers, two upper atria, the receiving chambers, and two lower ventricles, the discharging chambers. The heart has four valves, which separate its chambers and ensures blood flows in the correct direction through the heart (preventing backflow). After gas exchange in the pulmonary capillaries (blood vessels in the lungs), oxygen-rich blood returns to the left atrium via one of the four pulmonary veins. Blood flows nearly continuously back into the atrium, which acts as the receiving chamber, and from here through an opening into the left ventricle. Most blood flows passively into the heart while both the atria and ventricles are relaxed, but toward the end of the ventricular relaxation period, the left atrium will contract, pumping blood into the ventricle. The heart also requires nutrients and oxygen found in blood like other muscles, and is supplied via coronary arteries.

The integumentary system (skin) is made up of three layers: the outermost epidermis, the dermis and the hypodermis. The epidermis is typically 10 to 30 cells thick; its main function is to provide a waterproof layer. Its outermost cells are constantly lost; its bottommost cells are constantly dividing and pushing upward. The middle layer, the dermis, is 15 to 40 times thicker than the epidermis. The dermis is made up of many components, such as bony structures and blood vessels. The hypodermis is made up of adipose tissue, which stores lipids and provides cushioning and insulation. The thickness of this layer varies widely from species to species; marine mammals require a thick hypodermis (blubber) for insulation, and right whales have the thickest blubber at . Although other animals have features such as whiskers, feathers, setae, or cilia that superficially resemble it, no animals other than mammals have hair. It is a definitive characteristic of the class. Though some mammals have very little, careful examination reveals the characteristic, often in obscure parts of their bodies.
Herbivores have developed a diverse range of physical structures to facilitate the consumption of plant material. To break up intact plant tissues, mammals have developed teeth structures that reflect their feeding preferences. For instance, frugivores (animals that feed primarily on fruit) and herbivores that feed on soft foliage have low-crowned teeth specialized for grinding foliage and seeds. Grazing animals that tend to eat hard, silica-rich grasses, have high-crowned teeth, which are capable of grinding tough plant tissues and do not wear down as quickly as low-crowned teeth. Most carnivorous mammals have carnassialiforme teeth (of varying length depending on diet), long canines and similar tooth replacement patterns.

The stomach of Artiodactyls is divided into four sections: the rumen, the reticulum, the omasum and the abomasum (only ruminants have a rumen). After the plant material is consumed, it is mixed with saliva in the rumen and reticulum and separates into solid and liquid material. The solids lump together to form a bolus (or cud), and is regurgitated. When the bolus enters the mouth, the fluid is squeezed out with the tongue and swallowed again. Ingested food passes to the rumen and reticulum where cellulytic microbes (bacteria, protozoa and fungi) produce cellulase, which is needed to break down the cellulose in plants. Perissodactyls, in contrast to the ruminants, store digested food that has left the stomach in an enlarged cecum, where it is fermented by bacteria. Carnivora have a simple stomach adapted to digest primarily meat, as compared to the elaborate digestive systems of herbivorous animals, which are necessary to break down tough, complex plant fibers. The caecum is either absent or short and simple, and the large intestine is not sacculated or much wider than the small intestine.

The mammalian excretory system involves many components. Like most other land animals, mammals are ureotelic, and convert ammonia into urea, which is done by the liver as part of the urea cycle. Bilirubin, a waste product derived from blood cells, is passed through bile and urine with the help of enzymes excreted by the liver. The passing of bilirubin via bile through the intestinal tract gives mammalian feces a distinctive brown coloration. Distinctive features of the mammalian kidney include the presence of the renal pelvis and renal pyramids, and of a clearly distinguishable cortex and medulla, which is due to the presence of elongated loops of Henle. Only the mammalian kidney has a bean shape, although there are some exceptions, such as the multilobed reniculate kidneys of pinnipeds, cetaceans and bears. Most adult placental mammals have no remaining trace of the cloaca. In the embryo, the embryonic cloaca divides into a posterior region that becomes part of the anus, and an anterior region that has different fates depending on the sex of the individual: in females, it develops into the vestibule that receives the urethra and vagina, while in males it forms the entirety of the penile urethra. However, the tenrecs, golden moles, and some shrews retain a cloaca as adults. In marsupials, the genital tract is separate from the anus, but a trace of the original cloaca does remain externally. Monotremes, which translates from Greek into "single hole", have a true cloaca.

As in all other tetrapods, mammals have a larynx that can quickly open and close to produce sounds, and a supralaryngeal vocal tract which filters this sound. The lungs and surrounding musculature provide the air stream and pressure required to phonate. The larynx controls the pitch and volume of sound, but the strength the lungs exert to exhale also contributes to volume. More primitive mammals, such as the echidna, can only hiss, as sound is achieved solely through exhaling through a partially closed larynx. Other mammals phonate using vocal folds, as opposed to the vocal cords seen in birds and reptiles. The movement or tenseness of the vocal folds can result in many sounds such as purring and screaming. Mammals can change the position of the larynx, allowing them to breathe through the nose while swallowing through the mouth, and to form both oral and nasal sounds; nasal sounds, such as a dog whine, are generally soft sounds, and oral sounds, such as a dog bark, are generally loud.

Some mammals have a large larynx and thus a low-pitched voice, namely the hammer-headed bat ("Hypsignathus monstrosus") where the larynx can take up the entirety of the thoracic cavity while pushing the lungs, heart, and trachea into the abdomen. Large vocal pads can also lower the pitch, as in the low-pitched roars of big cats. The production of infrasound is possible in some mammals such as the African elephant ("Loxodonta" spp.) and baleen whales. Small mammals with small larynxes have the ability to produce ultrasound, which can be detected by modifications to the middle ear and cochlea. Ultrasound is inaudible to birds and reptiles, which might have been important during the Mesozoic, when birds and reptiles were the dominant predators. This private channel is used by some rodents in, for example, mother-to-pup communication, and by bats when echolocating. Toothed whales also use echolocation, but, as opposed to the vocal membrane that extends upward from the vocal folds, they have a melon to manipulate sounds. Some mammals, namely the primates, have air sacs attached to the larynx, which may function to lower the resonances or increase the volume of sound.

The vocal production system is controlled by the cranial nerve nuclei in the brain, and supplied by the recurrent laryngeal nerve and the superior laryngeal nerve, branches of the vagus nerve. The vocal tract is supplied by the hypoglossal nerve and facial nerves. Electrical stimulation of the periaqueductal gray (PEG) region of the mammalian midbrain elicit vocalizations. The ability to learn new vocalizations is only exemplified in humans, seals, cetaceans, elephants and possibly bats; in humans, this is the result of a direct connection between the motor cortex, which controls movement, and the motor neurons in the spinal cord.

The primary function of the fur of mammals is thermoregulation. Others include protection, sensory purposes, waterproofing, and camouflage. Different types of fur serve different purposes:

Hair length is not a factor in thermoregulation: for example, some tropical mammals such as sloths have the same length of fur length as some arctic mammals but with less insulation; and, conversely, other tropical mammals with short hair have the same insulating value as arctic mammals. The denseness of fur can increase an animal's insulation value, and arctic mammals especially have dense fur; for example, the musk ox has guard hairs measuring as well as a dense underfur, which forms an airtight coat, allowing them to survive in temperatures of . Some desert mammals, such as camels, use dense fur to prevent solar heat from reaching their skin, allowing the animal to stay cool; a camel's fur may reach in the summer, but the skin stays at . Aquatic mammals, conversely, trap air in their fur to conserve heat by keeping the skin dry.

Mammalian coats are colored for a variety of reasons, the major selective pressures including camouflage, sexual selection, communication, and thermoregulation. Coloration in both the hair and skin of mammals is mainly determined by the type and amount of melanin; eumelanins for brown and black colors and pheomelanin for a range of yellow to reddish-brown colors, giving mammals an earth tone. Some mammals, like the mandrill, have more vibrant colors due to structural coloration. Many sloths appear green because their fur hosts green algae; this may be a symbiotic relation that affords camouflage to the sloths.

Camouflage is a powerful influence in a large number of mammals, as it helps to conceal individuals from predators or prey. In arctic and subarctic mammals such as the arctic fox ("Alopex lagopus"), collared lemming ("Dicrostonyx groenlandicus"), stoat ("Mustela erminea"), and snowshoe hare ("Lepus americanus"), seasonal color change between brown in summer and white in winter is driven largely by camouflage. Some arboreal mammals, notably primates and marsupials, have shades of violet, green, or blue skin on parts of their bodies, indicating some distinct advantage in their largely arboreal habitat due to convergent evolution.

Aposematism, warning off possible predators, is the most likely explanation of the black-and-white pelage of many mammals which are able to defend themselves, such as in the foul-smelling skunk and the powerful and aggressive honey badger. Coat color is sometimes sexually dimorphic, as in many primate species. Differences in female and male coat color may indicate nutrition and hormone levels, important in mate selection. Coat color may influence the ability to retain heat, depending on how much light is reflected. Mammals with a darker colored coat can absorb more heat from solar radiation, and stay warmer, and some smaller mammals, such as voles, have darker fur in the winter. The white, pigmentless fur of arctic mammals, such as the polar bear, may reflect more solar radiation directly onto the skin. The dazzling black-and-white striping of zebras appear to provide some protection from biting flies.

In male placentals, the penis is used both for urination and copulation. Depending on the species, an erection may be fueled by blood flow into vascular, spongy tissue or by muscular action. A penis may be contained in a prepuce when not erect, and some placentals also have a penis bone (baculum). Marsupials typically have forked penises, while the echidna penis generally has four heads with only two functioning. The testes of most mammals descend into the scrotum which is typically posterior to the penis but is often anterior in marsupials. Female mammals generally have a clitoris, labia majora and labia minora on the outside, while the internal system contains paired oviducts, 1-2 uteri, 1-2 cervices and a vagina. Marsupials have two lateral vaginas and a medial vagina. The "vagina" of monotremes is better understood as a "urogenital sinus". The uterine systems of placental mammals can vary between a duplex, were there are two uteri and cervices which open into the vagina, a bipartite, were two uterine horns have a single cervix that connects to the vagina, a bicornuate, which consists where two uterine horns that are connected distally but separate medially creating a Y-shape, and a simplex, which has a single uterus.
The ancestral condition for mammal reproduction is the birthing of relatively undeveloped, either through direct vivipary or a short period as soft-shelled eggs. This is likely due to the fact that the torso could not expand due to the presence of epipubic bones. The oldest demonstration of this reproductive style is with "Kayentatherium", which produced undeveloped perinates, but at much higher litter sizes than any modern mammal, 38 specimens. Most modern mammals are viviparous, giving birth to live young. However, the five species of monotreme, the platypus and the four species of echidna, lay eggs. The monotremes have a sex determination system different from that of most other mammals. In particular, the sex chromosomes of a platypus are more like those of a chicken than those of a therian mammal.

Viviparous mammals are in the subclass Theria; those living today are in the marsupial and placental infraclasses. Marsupials have a short gestation period, typically shorter than its estrous cycle and gives birth to an undeveloped newborn that then undergoes further development; in many species, this takes place within a pouch-like sac, the marsupium, located in the front of the mother's abdomen. This is the plesiomorphic condition among viviparous mammals; the presence of epipubic bones in all non-placental mammals prevents the expansion of the torso needed for full pregnancy. Even non-placental eutherians probably reproduced this way. The placentals give birth to relatively complete and developed young, usually after long gestation periods. They get their name from the placenta, which connects the developing fetus to the uterine wall to allow nutrient uptake. In placental mammals, the epipubic is either completely lost or converted into the baculum; allowing the torso to be able to expand and thus birth developed offspring.

The mammary glands of mammals are specialized to produce milk, the primary source of nutrition for newborns. The monotremes branched early from other mammals and do not have the nipples seen in most mammals, but they do have mammary glands. The young lick the milk from a mammary patch on the mother's belly. Compared to placental mammals, the milk of marsupials changes greatly in both production rate and in nutrient composition, due to the underdeveloped young. In addition, the mammary glands have more autonomy allowing them to supply separate milks to young at different development stages. Lactose is the main sugar in placental mammal milk while monotreme and marsupial milk is dominated by oligosaccharides. Weaning is the process in which a mammal becomes less dependent on their mother's milk and more on solid food.

Nearly all mammals are endothermic ("warm-blooded"). Most mammals also have hair to help keep them warm. Like birds, mammals can forage or hunt in weather and climates too cold for ectothermic ("cold-blooded") reptiles and insects. Endothermy requires plenty of food energy, so mammals eat more food per unit of body weight than most reptiles. Small insectivorous mammals eat prodigious amounts for their size. A rare exception, the naked mole-rat produces little metabolic heat, so it is considered an operational poikilotherm. Birds are also endothermic, so endothermy is not unique to mammals.

Among mammals, species maximum lifespan varies significantly (for example the shrew has a lifespan of two years, whereas the oldest bowhead whale is recorded to be 211 years). Although the underlying basis for these lifespan differences is still uncertain, numerous studies indicate that the ability to repair DNA damage is an important determinant of mammalian lifespan. In a 1974 study by Hart and Setlow, it was found that DNA excision repair capability increased systematically with species lifespan among seven mammalian species. Species lifespan was observed to be robustly correlated with the capacity to recognize DNA double-strand breaks as well as the level of the DNA repair protein Ku80. In a study of the cells from sixteen mammalian species, genes employed in DNA repair were found to be up-regulated in the longer-lived species. The cellular level of the DNA repair enzyme poly ADP ribose polymerase was found to correlate with species lifespan in a study of 13 mammalian species. Three additional studies of a variety of mammalian species also reported a correlation between species lifespan and DNA repair capability.

Most vertebrates—the amphibians, the reptiles and some mammals such as humans and bears—are plantigrade, walking on the whole of the underside of the foot. Many mammals, such as cats and dogs, are digitigrade, walking on their toes, the greater stride length allowing more speed. Digitigrade mammals are also often adept at quiet movement. Some animals such as horses are unguligrade, walking on the tips of their toes. This even further increases their stride length and thus their speed. A few mammals, namely the great apes, are also known to walk on their knuckles, at least for their front legs. Giant anteaters and platypuses are also knuckle-walkers. Some mammals are bipeds, using only two limbs for locomotion, which can be seen in, for example, humans and the great apes. Bipedal species have a larger field of vision than quadrupeds, conserve more energy and have the ability to manipulate objects with their hands, which aids in foraging. Instead of walking, some bipeds hop, such as kangaroos and kangaroo rats.

Animals will use different gaits for different speeds, terrain and situations. For example, horses show four natural gaits, the slowest horse gait is the walk, then there are three faster gaits which, from slowest to fastest, are the trot, the canter and the gallop. Animals may also have unusual gaits that are used occasionally, such as for moving sideways or backwards. For example, the main human gaits are bipedal walking and running, but they employ many other gaits occasionally, including a four-legged crawl in tight spaces. Mammals show a vast range of gaits, the order that they place and lift their appendages in locomotion. Gaits can be grouped into categories according to their patterns of support sequence. For quadrupeds, there are three main categories: walking gaits, running gaits and leaping gaits. Walking is the most common gait, where some feet are on the ground at any given time, and found in almost all legged animals. Running is considered to occur when at some points in the stride all feet are off the ground in a moment of suspension.

Arboreal animals frequently have elongated limbs that help them cross gaps, reach fruit or other resources, test the firmness of support ahead and, in some cases, to brachiate (swing between trees). Many arboreal species, such as tree porcupines, silky anteaters, spider monkeys, and possums, use prehensile tails to grasp branches. In the spider monkey, the tip of the tail has either a bare patch or adhesive pad, which provides increased friction. Claws can be used to interact with rough substrates and reorient the direction of forces the animal applies. This is what allows squirrels to climb tree trunks that are so large to be essentially flat from the perspective of such a small animal. However, claws can interfere with an animal's ability to grasp very small branches, as they may wrap too far around and prick the animal's own paw. Frictional gripping is used by primates, relying upon hairless fingertips. Squeezing the branch between the fingertips generates frictional force that holds the animal's hand to the branch. However, this type of grip depends upon the angle of the frictional force, thus upon the diameter of the branch, with larger branches resulting in reduced gripping ability. To control descent, especially down large diameter branches, some arboreal animals such as squirrels have evolved highly mobile ankle joints that permit rotating the foot into a 'reversed' posture. This allows the claws to hook into the rough surface of the bark, opposing the force of gravity. Small size provides many advantages to arboreal species: such as increasing the relative size of branches to the animal, lower center of mass, increased stability, lower mass (allowing movement on smaller branches) and the ability to move through more cluttered habitat. Size relating to weight affects gliding animals such as the sugar glider. Some species of primate, bat and all species of sloth achieve passive stability by hanging beneath the branch. Both pitching and tipping become irrelevant, as the only method of failure would be losing their grip.

Bats are the only mammals that can truly fly. They fly through the air at a constant speed by moving their wings up and down (usually with some fore-aft movement as well). Because the animal is in motion, there is some airflow relative to its body which, combined with the velocity of the wings, generates a faster airflow moving over the wing. This generates a lift force vector pointing forwards and upwards, and a drag force vector pointing rearwards and upwards. The upwards components of these counteract gravity, keeping the body in the air, while the forward component provides thrust to counteract both the drag from the wing and from the body as a whole.

The wings of bats are much thinner and consist of more bones than those of birds, allowing bats to maneuver more accurately and fly with more lift and less drag. By folding the wings inwards towards their body on the upstroke, they use 35% less energy during flight than birds. The membranes are delicate, ripping easily; however, the tissue of the bat's membrane is able to regrow, such that small tears can heal quickly. The surface of their wings is equipped with touch-sensitive receptors on small bumps called Merkel cells, also found on human fingertips. These sensitive areas are different in bats, as each bump has a tiny hair in the center, making it even more sensitive and allowing the bat to detect and collect information about the air flowing over its wings, and to fly more efficiently by changing the shape of its wings in response.

A fossorial (from Latin "fossor", meaning "digger") is an animal adapted to digging which lives primarily, but not solely, underground. Some examples are badgers, and naked mole-rats. Many rodent species are also considered fossorial because they live in burrows for most but not all of the day. Species that live exclusively underground are subterranean, and those with limited adaptations to a fossorial lifestyle sub-fossorial. Some organisms are fossorial to aid in temperature regulation while others use the underground habitat for protection from predators or for food storage.

Fossorial mammals have a fusiform body, thickest at the shoulders and tapering off at the tail and nose. Unable to see in the dark burrows, most have degenerated eyes, but degeneration varies between species; pocket gophers, for example, are only semi-fossorial and have very small yet functional eyes, in the fully fossorial marsupial mole the eyes are degenerated and useless, talpa moles have vestigial eyes and the cape golden mole has a layer of skin covering the eyes. External ears flaps are also very small or absent. Truly fossorial mammals have short, stout legs as strength is more important than speed to a burrowing mammal, but semi-fossorial mammals have cursorial legs. The front paws are broad and have strong claws to help in loosening dirt while excavating burrows, and the back paws have webbing, as well as claws, which aids in throwing loosened dirt backwards. Most have large incisors to prevent dirt from flying into their mouth.

Many fossorial mammals such as shrews, hedgehogs, and moles were classified under the now obsolete order Insectivora.

Fully aquatic mammals, the cetaceans and sirenians, have lost their legs and have a tail fin to propel themselves through the water. Flipper movement is continuous. Whales swim by moving their tail fin and lower body up and down, propelling themselves through vertical movement, while their flippers are mainly used for steering. Their skeletal anatomy allows them to be fast swimmers. Most species have a dorsal fin to prevent themselves from turning upside-down in the water. The flukes of sirenians are raised up and down in long strokes to move the animal forward, and can be twisted to turn. The forelimbs are paddle-like flippers which aid in turning and slowing.

Semi-aquatic mammals, like pinnipeds, have two pairs of flippers on the front and back, the fore-flippers and hind-flippers. The elbows and ankles are enclosed within the body. Pinnipeds have several adaptions for reducing drag. In addition to their streamlined bodies, they have smooth networks of muscle bundles in their skin that may increase laminar flow and make it easier for them to slip through water. They also lack arrector pili, so their fur can be streamlined as they swim. They rely on their fore-flippers for locomotion in a wing-like manner similar to penguins and sea turtles. Fore-flipper movement is not continuous, and the animal glides between each stroke. Compared to terrestrial carnivorans, the fore-limbs are reduced in length, which gives the locomotor muscles at the shoulder and elbow joints greater mechanical advantage; the hind-flippers serve as stabilizers. Other semi-aquatic mammals include beavers, hippopotamuses, otters and platypuses. Hippos are very large semi-aquatic mammals, and their barrel-shaped bodies have graviportal skeletal structures, adapted to carrying their enormous weight, and their specific gravity allows them to sink and move along the bottom of a river.

Many mammals communicate by vocalizing. Vocal communication serves many purposes, including in mating rituals, as warning calls, to indicate food sources, and for social purposes. Males often call during mating rituals to ward off other males and to attract females, as in the roaring of lions and red deer. The songs of the humpback whale may be signals to females; they have different dialects in different regions of the ocean. Social vocalizations include the territorial calls of gibbons, and the use of frequency in greater spear-nosed bats to distinguish between groups. The vervet monkey gives a distinct alarm call for each of at least four different predators, and the reactions of other monkeys vary according to the call. For example, if an alarm call signals a python, the monkeys climb into the trees, whereas the eagle alarm causes monkeys to seek a hiding place on the ground. Prairie dogs similarly have complex calls that signal the type, size, and speed of an approaching predator. Elephants communicate socially with a variety of sounds including snorting, screaming, trumpeting, roaring and rumbling. Some of the rumbling calls are infrasonic, below the hearing range of humans, and can be heard by other elephants up to away at still times near sunrise and sunset.
Mammals signal by a variety of means. Many give visual anti-predator signals, as when deer and gazelle stot, honestly indicating their fit condition and their ability to escape, or when white-tailed deer and other prey mammals flag with conspicuous tail markings when alarmed, informing the predator that it has been detected. Many mammals make use of scent-marking, sometimes possibly to help defend territory, but probably with a range of functions both within and between species. Microbats and toothed whales including oceanic dolphins vocalize both socially and in echolocation.

To maintain a high constant body temperature is energy expensive—mammals therefore need a nutritious and plentiful diet. While the earliest mammals were probably predators, different species have since adapted to meet their dietary requirements in a variety of ways. Some eat other animals—this is a carnivorous diet (and includes insectivorous diets). Other mammals, called herbivores, eat plants, which contain complex carbohydrates such as cellulose. An herbivorous diet includes subtypes such as granivory (seed eating), folivory (leaf eating), frugivory (fruit eating), nectarivory (nectar eating), gummivory (gum eating) and mycophagy (fungus eating). The digestive tract of an herbivore is host to bacteria that ferment these complex substances, and make them available for digestion, which are either housed in the multichambered stomach or in a large cecum. Some mammals are coprophagous, consuming feces to absorb the nutrients not digested when the food was first ingested. An omnivore eats both prey and plants. Carnivorous mammals have a simple digestive tract because the proteins, lipids and minerals found in meat require little in the way of specialized digestion. Exceptions to this include baleen whales who also house gut flora in a multi-chambered stomach, like terrestrial herbivores.

The size of an animal is also a factor in determining diet type (Allen's rule). Since small mammals have a high ratio of heat-losing surface area to heat-generating volume, they tend to have high energy requirements and a high metabolic rate. Mammals that weigh less than about are mostly insectivorous because they cannot tolerate the slow, complex digestive process of an herbivore. Larger animals, on the other hand, generate more heat and less of this heat is lost. They can therefore tolerate either a slower collection process (carnivores that feed on larger vertebrates) or a slower digestive process (herbivores). Furthermore, mammals that weigh more than usually cannot collect enough insects during their waking hours to sustain themselves. The only large insectivorous mammals are those that feed on huge colonies of insects (ants or termites).
Some mammals are omnivores and display varying degrees of carnivory and herbivory, generally leaning in favor of one more than the other. Since plants and meat are digested differently, there is a preference for one over the other, as in bears where some species may be mostly carnivorous and others mostly herbivorous. They are grouped into three categories: mesocarnivory (50–70% meat), hypercarnivory (70% and greater of meat), and hypocarnivory (50% or less of meat). The dentition of hypocarnivores consists of dull, triangular carnassial teeth meant for grinding food. Hypercarnivores, however, have conical teeth and sharp carnassials meant for slashing, and in some cases strong jaws for bone-crushing, as in the case of hyenas, allowing them to consume bones; some extinct groups, notably the Machairodontinae, had saber-shaped canines.

Some physiological carnivores consume plant matter and some physiological herbivores consume meat. From a behavioral aspect, this would make them omnivores, but from the physiological standpoint, this may be due to zoopharmacognosy. Physiologically, animals must be able to obtain both energy and nutrients from plant and animal materials to be considered omnivorous. Thus, such animals are still able to be classified as carnivores and herbivores when they are just obtaining nutrients from materials originating from sources that do not seemingly complement their classification. For example, it is well documented that some ungulates such as giraffes, camels, and cattle, will gnaw on bones to consume particular minerals and nutrients. Also, cats, which are generally regarded as obligate carnivores, occasionally eat grass to regurgitate indigestible material (such as hairballs), aid with hemoglobin production, and as a laxative.

Many mammals, in the absence of sufficient food requirements in an environment, suppress their metabolism and conserve energy in a process known as hibernation. In the period preceding hibernation, larger mammals, such as bears, become polyphagic to increase fat stores, whereas smaller mammals prefer to collect and stash food. The slowing of the metabolism is accompanied by a decreased heart and respiratory rate, as well as a drop in internal temperatures, which can be around ambient temperature in some cases. For example, the internal temperatures of hibernating arctic ground squirrels can drop to , however the head and neck always stay above . A few mammals in hot environments aestivate in times of drought or extreme heat, namely the fat-tailed dwarf lemur ("Cheirogaleus medius").

In intelligent mammals, such as primates, the cerebrum is larger relative to the rest of the brain. Intelligence itself is not easy to define, but indications of intelligence include the ability to learn, matched with behavioral flexibility. Rats, for example, are considered to be highly intelligent, as they can learn and perform new tasks, an ability that may be important when they first colonize a fresh habitat. In some mammals, food gathering appears to be related to intelligence: a deer feeding on plants has a brain smaller than a cat, which must think to outwit its prey.

Tool use by animals may indicate different levels of learning and cognition. The sea otter uses rocks as essential and regular parts of its foraging behaviour (smashing abalone from rocks or breaking open shells), with some populations spending 21% of their time making tools. Other tool use, such as chimpanzees using twigs to "fish" for termites, may be developed by watching others use tools and may even be a true example of animal teaching. Tools may even be used in solving puzzles in which the animal appears to experience a "Eureka moment". Other mammals that do not use tools, such as dogs, can also experience a Eureka moment.

Brain size was previously considered a major indicator of the intelligence of an animal. Since most of the brain is used for maintaining bodily functions, greater ratios of brain to body mass may increase the amount of brain mass available for more complex cognitive tasks. Allometric analysis indicates that mammalian brain size scales at approximately the or exponent of the body mass. Comparison of a particular animal's brain size with the expected brain size based on such allometric analysis provides an encephalisation quotient that can be used as another indication of animal intelligence. Sperm whales have the largest brain mass of any animal on earth, averaging and in mature males.

Self-awareness appears to be a sign of abstract thinking. Self-awareness, although not well-defined, is believed to be a precursor to more advanced processes such as metacognitive reasoning. The traditional method for measuring this is the mirror test, which determines if an animal possesses the ability of self-recognition. Mammals that have passed the mirror test include Asian elephants (some pass, some do not); chimpanzees; bonobos; orangutans; humans, from 18 months (mirror stage); bottlenose dolphins killer whales; and false killer whales.

 
Eusociality is the highest level of social organization. These societies have an overlap of adult generations, the division of reproductive labor and cooperative caring of young. Usually insects, such as bees, ants and termites, have eusocial behavior, but it is demonstrated in two rodent species: the naked mole-rat and the Damaraland mole-rat.

Presociality is when animals exhibit more than just sexual interactions with members of the same species, but fall short of qualifying as eusocial. That is, presocial animals can display communal living, cooperative care of young, or primitive division of reproductive labor, but they do not display all of the three essential traits of eusocial animals. Humans and some species of Callitrichidae (marmosets and tamarins) are unique among primates in their degree of cooperative care of young. Harry Harlow set up an experiment with rhesus monkeys, presocial primates, in 1958; the results from this study showed that social encounters are necessary in order for the young monkeys to develop both mentally and sexually.

A fission-fusion society is a society that changes frequently in its size and composition, making up a permanent social group called the "parent group". Permanent social networks consist of all individual members of a community and often varies to track changes in their environment. In a fission–fusion society, the main parent group can fracture (fission) into smaller stable subgroups or individuals to adapt to environmental or social circumstances. For example, a number of males may break off from the main group in order to hunt or forage for food during the day, but at night they may return to join (fusion) the primary group to share food and partake in other activities. Many mammals exhibit this, such as primates (for example orangutans and spider monkeys), elephants, spotted hyenas, lions, and dolphins.

Solitary animals defend a territory and avoid social interactions with the members of its species, except during breeding season. This is to avoid resource competition, as two individuals of the same species would occupy the same niche, and to prevent depletion of food. A solitary animal, while foraging, can also be less conspicuous to predators or prey.

In a hierarchy, individuals are either dominant or submissive. A despotic hierarchy is where one individual is dominant while the others are submissive, as in wolves and lemurs, and a pecking order is a linear ranking of individuals where there is a top individual and a bottom individual. Pecking orders may also be ranked by sex, where the lowest individual of a sex has a higher ranking than the top individual of the other sex, as in hyenas. Dominant individuals, or alphas, have a high chance of reproductive success, especially in harems where one or a few males (resident males) have exclusive breeding rights to females in a group. Non-resident males can also be accepted in harems, but some species, such as the common vampire bat ("Desmodus rotundus"), may be more strict.

Some mammals are perfectly monogamous, meaning that they mate for life and take no other partners (even after the original mate's death), as with wolves, Eurasian beavers, and otters. There are three types of polygamy: either one or multiple dominant males have breeding rights (polygyny), multiple males that females mate with (polyandry), or multiple males have exclusive relations with multiple females (polygynandry). It is much more common for polygynous mating to happen, which, excluding leks, are estimated to occur in up to 90% of mammals. Lek mating occurs when males congregate around females and try to attract them with various courtship displays and vocalizations, as in harbor seals.

All higher mammals (excluding monotremes) share two major adaptations for care of the young: live birth and lactation. These imply a group-wide choice of a degree of parental care. They may build nests and dig burrows to raise their young in, or feed and guard them often for a prolonged period of time. Many mammals are K-selected, and invest more time and energy into their young than do r-selected animals. When two animals mate, they both share an interest in the success of the offspring, though often to different extremes. Mammalian females exhibit some degree of maternal aggression, another example of parental care, which may be targeted against other females of the species or the young of other females; however, some mammals may "aunt" the infants of other females, and care for them. Mammalian males may play a role in child rearing, as with tenrecs, however this varies species to species, even within the same genus. For example, the males of the southern pig-tailed macaque ("Macaca nemestrina") do not participate in child care, whereas the males of the Japanese macaque ("M. fuscata") do.

Non-human mammals play a wide variety of roles in human culture. They are the most popular of pets, with tens of millions of dogs, cats and other animals including rabbits and mice kept by families around the world. Mammals such as mammoths, horses and deer are among the earliest subjects of art, being found in Upper Paleolithic cave paintings such as at Lascaux. Major artists such as Albrecht Dürer, George Stubbs and Edwin Landseer are known for their portraits of mammals. Many species of mammals have been hunted for sport and for food; deer and wild boar are especially popular as game animals. Mammals such as horses and dogs are widely raced for sport, often combined with betting on the outcome. There is a tension between the role of animals as companions to humans, and their existence as individuals with rights of their own. Mammals further play a wide variety of roles in literature, film, mythology, and religion.

Domestic mammals form a large part of the livestock raised for meat across the world. They include (2009) around 1.4 billion cattle, 1 billion sheep, 1 billion domestic pigs, and (1985) over 700 million rabbits. Working domestic animals including cattle and horses have been used for work and transport from the origins of agriculture, their numbers declining with the arrival of mechanised transport and agricultural machinery. In 2004 they still provided some 80% of the power for the mainly small farms in the third world, and some 20% of the world's transport, again mainly in rural areas. In mountainous regions unsuitable for wheeled vehicles, pack animals continue to transport goods.
Mammal skins provide leather for shoes, clothing and upholstery. Wool from mammals including sheep, goats and alpacas has been used for centuries for clothing. Mammals serve a major role in science as experimental animals, both in fundamental biological research, such as in genetics, and in the development of new medicines, which must be tested exhaustively to demonstrate their safety. Millions of mammals, especially mice and rats, are used in experiments each year. A knockout mouse is a genetically modified mouse with an inactivated gene, replaced or disrupted with an artificial piece of DNA. They enable the study of sequenced genes whose functions are unknown. A small percentage of the mammals are non-human primates, used in research for their similarity to humans.

Charles Darwin, Jared Diamond and others have noted the importance of domesticated mammals in the Neolithic development of agriculture and of civilization, causing farmers to replace hunter-gatherers around the world. This transition from hunting and gathering to herding flocks and growing crops was a major step in human history. The new agricultural economies, based on domesticated mammals, caused "radical restructuring of human societies, worldwide alterations in biodiversity, and significant changes in the Earth's landforms and its atmosphere... momentous outcomes".

Hybrids are offspring resulting from the breeding of two genetically distinct individuals, which usually will result in a high degree of heterozygosity, though hybrid and heterozygous are not synonymous. The deliberate or accidental hybridizing of two or more species of closely related animals through captive breeding is a human activity which has been in existence for millennia and has grown for economic purposes. Hybrids between different subspecies within a species (such as between the Bengal tiger and Siberian tiger) are known as intra-specific hybrids. Hybrids between different species within the same genus (such as between lions and tigers) are known as interspecific hybrids or crosses. Hybrids between different genera (such as between sheep and goats) are known as intergeneric hybrids. Natural hybrids will occur in hybrid zones, where two populations of species within the same genera or species living in the same or adjacent areas will interbreed with each other. Some hybrids have been recognized as species, such as the red wolf (though this is controversial).

Artificial selection, the deliberate selective breeding of domestic animals, is being used to breed back recently extinct animals in an attempt to achieve an animal breed with a phenotype that resembles that extinct wildtype ancestor. A breeding-back (intraspecific) hybrid may be very similar to the extinct wildtype in appearance, ecological niche and to some extent genetics, but the initial gene pool of that wild type is lost forever with its extinction. As a result, bred-back breeds are at best vague look-alikes of extinct wildtypes, as Heck cattle are of the aurochs.

Purebred wild species evolved to a specific ecology can be threatened with extinction through the process of genetic pollution, the uncontrolled hybridization, introgression genetic swamping which leads to homogenization or out-competition from the heterosic hybrid species. When new populations are imported or selectively bred by people, or when habitat modification brings previously isolated species into contact, extinction in some species, especially rare varieties, is possible. Interbreeding can swamp the rarer gene pool and create hybrids, depleting the purebred gene pool. For example, the endangered wild water buffalo is most threatened with extinction by genetic pollution from the domestic water buffalo. Such extinctions are not always apparent from a morphological standpoint. Some degree of gene flow is a normal evolutionary process, nevertheless, hybridization threatens the existence of rare species.

The loss of species from ecological communities, defaunation, is primarily driven by human activity. This has resulted in empty forests, ecological communities depleted of large vertebrates. In the Quaternary extinction event, the mass die-off of megafaunal variety coincided with the appearance of humans, suggesting a human influence. One hypothesis is that humans hunted large mammals, such as the woolly mammoth, into extinction. The 2019 "Global Assessment Report on Biodiversity and Ecosystem Services" by IPBES states that the total biomass of wild mammals has declined by 82 percent since the beginning of human civilization.

Various species are predicted to become extinct in the near future, among them the rhinoceros, primates, pangolins, and giraffes. Hunting alone threatens hundreds of mammalian species around the world. Scientists claim that the growing demand for meat is contributing to biodiversity loss as this is a significant driver of deforestation and habitat destruction; species-rich habitats, such as significant portions of the Amazon rainforest, are being converted to agricultural land for meat production. According to the World Wildlife Fund's 2016 Living Planet Index, global wildlife populations have declined 58% since 1970, primarily due to habitat destruction, over-hunting and pollution. They project that if current trends continue, 67% of wildlife could disappear by 2020. Another influence is over-hunting and poaching, which can reduce the overall population of game animals, especially those located near villages, as in the case of peccaries. The effects of poaching can especially be seen in the ivory trade with African elephants. Marine mammals are at risk from entanglement from fishing gear, notably cetaceans, with discard mortalities ranging from 65,000 to 86,000 individuals annually.

Attention is being given to endangered species globally, notably through the Convention on Biological Diversity, otherwise known as the Rio Accord, which includes 189 signatory countries that are focused on identifying endangered species and habitats. Another notable conservation organization is the IUCN, which has a membership of over 1,200 governmental and non-governmental organizations.

Recent extinctions can be directly attributed to human influences. The IUCN characterizes 'recent' extinction as those that have occurred past the cut-off point of 1500, and around 80 mammal species have gone extinct since that time and 2015. Some species, such as the Père David's deer are extinct in the wild, and survive solely in captive populations. Other species, such as the Florida panther, are ecologically extinct, surviving in such low numbers that they essentially have no impact on the ecosystem. Other populations are only locally extinct (extirpated), still existing elsewhere, but reduced in distribution, as with the extinction of gray whales in the Atlantic.



</doc>
<doc id="18839" url="https://en.wikipedia.org/wiki?curid=18839" title="Music">
Music

Music is an art form, and cultural activity, whose medium is sound. General definitions of music include common elements such as pitch (which governs melody and harmony), rhythm (and its associated concepts tempo, meter, and articulation), dynamics (loudness and softness), and the sonic qualities of timbre and texture (which are sometimes termed the "color" of a musical sound). Different styles or types of music may emphasize, de-emphasize or omit some of these elements. Music is performed with a vast range of instruments and vocal techniques ranging from singing to rapping; there are solely instrumental pieces, solely vocal pieces (such as songs without instrumental accompaniment) and pieces that combine singing and instruments. The word derives from Greek μουσική ("mousike"; "art of the Muses").
See glossary of musical terminology.

In its most general form, the activities describing music as an art form or cultural activity include the creation of works of music (songs, tunes, symphonies, and so on), the criticism of music, the study of the history of music, and the aesthetic examination of music. Ancient Greek and Indian philosophers defined music in two parts: melodies, as tones ordered horizontally, and harmonies as tones ordered vertically. Common sayings such as "the harmony of the spheres" and "it is music to my ears" point to the notion that music is often ordered and pleasant to listen to. However, 20th-century composer John Cage thought that any sound can be music, saying, for example, "There is no noise, only sound."

The creation, performance, significance, and even the definition of music vary according to culture and social context. Indeed, throughout history, some new forms or styles of music have been criticized as "not being music", including Beethoven's "Grosse Fuge" string quartet in 1825, early jazz in the beginning of the 1900s and hardcore punk in the 1980s. There are many types of music, including popular music, traditional music, art music, music written for religious ceremonies and work songs such as chanteys. Music ranges from strictly organized compositions—such as Classical music symphonies from the 1700s and 1800s—through to spontaneously played improvisational music such as jazz, and avant-garde styles of chance-based contemporary music from the 20th and 21st centuries.

Music can be divided into genres (e.g., country music) and genres can be further divided into subgenres (e.g., country blues and pop country are two of the many country subgenres), although the dividing lines and relationships between music genres are often subtle, sometimes open to personal interpretation, and occasionally controversial. For example, it can be hard to draw the line between some early 1980s hard rock and heavy metal. Within the arts, music may be classified as a performing art, a fine art or as an auditory art. Music may be played or sung and heard live at a rock concert or orchestra performance, heard live as part of a dramatic work (a music theater show or opera), or it may be recorded and listened to on a radio, MP3 player, CD player, smartphone or as film score or TV show.

In many cultures, music is an important part of people's way of life, as it plays a key role in religious rituals, rite of passage ceremonies (e.g., graduation and marriage), social activities (e.g., dancing) and cultural activities ranging from amateur karaoke singing to playing in an amateur funk band or singing in a community choir. People may make music as a hobby, like a teen playing cello in a youth orchestra, or work as a professional musician or singer. The music industry includes the individuals who create new songs and musical pieces (such as songwriters and composers), individuals who perform music (which include orchestra, jazz band and rock band musicians, singers and conductors), individuals who record music (music producers and sound engineers), individuals who organize concert tours, and individuals who sell recordings, sheet music, and scores to customers. Even once a song or piece has been performed, music critics, music journalists, and music scholars may assess and evaluate the piece and its performance.

The word, 'music' is derived from Greek μουσική ("mousike"; "art of the Muses"). In Greek mythology, the nine Muses were the goddesses who inspired literature, science, and the arts and who were the source of the knowledge embodied in the poetry, song-lyrics, and myths in the Greek culture. According to the "Online Etymological Dictionary", the term "music" is derived from "mid-13c., musike, from Old French "musique" (12c.) and directly from Latin "musica" "the art of music," also including poetry (also [the] source of Spanish "música", Italian "musica", Old High German "mosica", German "Musik", Dutch "muziek", Danish "musik")." This is derived from the "...Greek "mousike (techne)" "(art) of the Muses," from fem. of mousikos "pertaining to the Muses," from Mousa "Muse" (see muse (n.)). Modern spelling [dates] from [the] 1630s. In classical Greece, [the term "music" refers to] any art in which the Muses presided, but especially music and lyric poetry."

Music is composed and performed for many purposes, ranging from aesthetic pleasure, religious or ceremonial purposes, or as an entertainment product for the marketplace. When music was only available through sheet music scores, such as during the Classical and Romantic eras, music lovers would buy the sheet music of their favourite pieces and songs so that they could perform them at home on the piano. With the advent of the phonograph, records of popular songs, rather than sheet music became the dominant way that music lovers would enjoy their favourite songs. With the advent of home tape recorders in the 1980s and digital music in the 1990s, music lovers could make tapes or playlists of their favourite songs and take them with them on a portable cassette player or MP3 player. Some music lovers create mix tapes of their favourite songs, which serve as a "self-portrait, a gesture of friendship, prescription for an ideal party... [and] an environment consisting solely of what is most ardently loved."

Amateur musicians can compose or perform music for their own pleasure, and derive their income elsewhere. Professional musicians are employed by a range of institutions and organisations, including armed forces (in marching bands, concert bands and popular music groups), churches and synagogues, symphony orchestras, broadcasting or film production companies, and music schools. Professional musicians sometimes work as freelancers or session musicians, seeking contracts and engagements in a variety of settings. There are often many links between amateur and professional musicians. Beginning amateur musicians take lessons with professional musicians. In community settings, advanced amateur musicians perform with professional musicians in a variety of ensembles such as community concert bands and community orchestras.

A distinction is often made between music performed for a live audience and music that is performed in a studio so that it can be recorded and distributed through the music retail system or the broadcasting system. However, there are also many cases where a live performance in front of an audience is also recorded and distributed. Live concert recordings are popular in both classical music and in popular music forms such as rock, where illegally taped live concerts are prized by music lovers. In the jam band scene, live, improvised jam sessions are preferred to studio recordings.

"Composition" is the act or practice of creating a song, an instrumental music piece, a work with both singing and instruments, or another type of music. In many cultures, including Western classical music, the act of composing also includes the creation of music notation, such as a sheet music "score", which is then performed by the composer or by other singers or musicians. In popular music and traditional music, the act of composing, which is typically called songwriting, may involve the creation of a basic outline of the song, called the lead sheet, which sets out the melody, lyrics and chord progression. In classical music, the composer typically orchestrates his or her own compositions, but in musical theatre and in pop music, songwriters may hire an arranger to do the orchestration. In some cases, a songwriter may not use notation at all, and instead compose the song in her mind and then play or record it from memory. In jazz and popular music, notable recordings by influential performers are given the weight that written scores play in classical music.

Even when music is notated relatively precisely, as in classical music, there are many decisions that a performer has to make, because notation does not specify all of the elements of music precisely. The process of deciding how to perform music that has been previously composed and notated is termed "interpretation". Different performers' interpretations of the same work of music can vary widely, in terms of the tempos that are chosen and the playing or singing style or phrasing of the melodies. Composers and songwriters who present their own music are interpreting their songs, just as much as those who perform the music of others. The standard body of choices and techniques present at a given time and a given place is referred to as performance practice, whereas interpretation is generally used to mean the individual choices of a performer.

Although a musical composition often uses musical notation and has a single author, this is not always the case. A work of music can have multiple composers, which often occurs in popular music when a band collaborates to write a song, or in musical theatre, when one person writes the melodies, a second person writes the lyrics, and a third person orchestrates the songs. In some styles of music, such as the blues, a composer/songwriter may create, perform and record new songs or pieces without ever writing them down in music notation. A piece of music can also be composed with words, images, or computer programs that explain or notate how the singer or musician should create musical sounds. Examples range from avant-garde music that uses graphic notation, to text compositions such as "Aus den sieben Tagen", to computer programs that select sounds for musical pieces. Music that makes heavy use of randomness and chance is called aleatoric music, and is associated with contemporary composers active in the 20th century, such as John Cage, Morton Feldman, and Witold Lutosławski. A more commonly known example of chance-based music is the sound of wind chimes jingling in a breeze.

The study of composition has traditionally been dominated by examination of methods and practice of Western classical music, but the definition of composition is broad enough to include the creation of popular music and traditional music songs and instrumental pieces as well as spontaneously improvised works like those of free jazz performers and African percussionists such as Ewe drummers.

In the 2000s, music notation typically means the written expression of music notes and rhythms on paper using symbols. When music is written down, the pitches and rhythm of the music, such as the notes of a melody, are notated. Music notation also often provides instructions on how to perform the music. For example, the sheet music for a song may state that the song is a "slow blues" or a "fast swing", which indicates the tempo and the genre. To read music notation, a person must have an understanding of music theory, harmony and the performance practice associated with a particular song or piece's genre.

Written notation varies with style and period of music. In the 2000s, notated music is produced as sheet music or, for individuals with computer scorewriter programs, as an image on a computer screen. In ancient times, music notation was put onto stone or clay tablets. To perform music from notation, a singer or instrumentalist requires an understanding of the rhythmic and pitch elements embodied in the symbols and the performance practice that is associated with a piece of music or a genre. In genres requiring musical improvisation, the performer often plays from music where only the chord changes and form of the song are written, requiring the performer to have a great understanding of the music's structure, harmony and the styles of a particular genre (e.g., jazz or country music).

In Western art music, the most common types of written notation are scores, which include all the music parts of an ensemble piece, and parts, which are the music notation for the individual performers or singers. In popular music, jazz, and blues, the standard musical notation is the lead sheet, which notates the melody, chords, lyrics (if it is a vocal piece), and structure of the music. Fake books are also used in jazz; they may consist of lead sheets or simply chord charts, which permit rhythm section members to improvise an accompaniment part to jazz songs. Scores and parts are also used in popular music and jazz, particularly in large ensembles such as jazz "big bands." In popular music, guitarists and electric bass players often read music notated in tablature (often abbreviated as "tab"), which indicates the location of the notes to be played on the instrument using a diagram of the guitar or bass fingerboard. Tablature was also used in the Baroque era to notate music for the lute, a stringed, fretted instrument.

Musical improvisation is the creation of spontaneous music, often within (or based on) a pre-existing harmonic framework or chord progression. Improvisers use the notes of the chord, various scales that are associated with each chord, and chromatic ornaments and passing tones which may be neither chord tones not from the typical scales associated with a chord. Musical improvisation can be done with or without preparation. Improvisation is a major part of some types of music, such as blues, jazz, and jazz fusion, in which instrumental performers improvise solos, melody lines and accompaniment parts. 

In the Western art music tradition, improvisation was an important skill during the Baroque era and during the Classical era. In the Baroque era, performers improvised ornaments, and basso continuo keyboard players improvised chord voicings based on figured bass notation. As well, the top soloists were expected to be able to improvise pieces such as preludes. In the Classical era, solo performers and singers improvised virtuoso cadenzas during concerts.

However, in the 20th and early 21st century, as "common practice" Western art music performance became institutionalized in symphony orchestras, opera houses and ballets, improvisation has played a smaller role, as more and more music was notated in scores and parts for musicians to play. At the same time, some 20th and 21st century art music composers have increasingly included improvisation in their creative work. In Indian classical music, improvisation is a core component and an essential criterion of performances.

Music theory encompasses the nature and mechanics of music. It often involves identifying patterns that govern composers' techniques and examining the language and notation of music. In a grand sense, music theory distills and analyzes the parameters or elements of music – rhythm, harmony (harmonic function), melody, structure, form, and texture. Broadly, music theory may include any statement, belief, or conception of or about music. People who study these properties are known as music theorists, and they typically work as professors in colleges, universities, and music conservatories. Some have applied acoustics, human physiology, and psychology to the explanation of how and why music is perceived. Music theorists publish their research in music theory journals and university press books.

Music has many different fundamentals or elements. Depending on the definition of "element" being used, these can include: pitch, beat or pulse, tempo, rhythm, melody, harmony, texture, style, allocation of voices, timbre or color, dynamics, expression, articulation, form and structure. The elements of music feature prominently in the music curriculums of Australia, UK and US. All three curriculums identify pitch, dynamics, timbre and texture as elements, but the other identified elements of music are far from universally agreed. Below is a list of the three official versions of the "elements of music":

In relation to the UK curriculum, in 2013 the term: "appropriate musical notations" was added to their list of elements and the title of the list was changed from the "elements of music" to the "inter-related dimensions of music". The inter-related dimensions of music are listed as: pitch, duration, dynamics, tempo, timbre, texture, structure and appropriate musical notations.

The phrase "the elements of music" is used in a number of different contexts. The two most common contexts can be differentiated by describing them as the "rudimentary elements of music" and the "perceptual elements of music".

In the 1800s, the phrases "the elements of music" and "the rudiments of music" were used interchangeably. The elements described in these documents refer to aspects of music that are needed in order to become a musician, Recent writers such as Espie Estrella seem to be using the phrase "elements of music" in a similar manner. A definition which most accurately reflects this usage is: "the rudimentary principles of an art, science, etc.: the elements of grammar." The UK's curriculum switch to the "inter-related dimensions of music" seems to be a move back to using the rudimentary elements of music.

Since the emergence of the study of psychoacoustics in the 1930s, most lists of elements of music have related more to how we "hear" music than how we learn to play it or study it. C.E. Seashore, in his book "Psychology of Music", identified four "psychological attributes of sound". These were: "pitch, loudness, time, and timbre" (p. 3). He did not call them the "elements of music" but referred to them as "elemental components" (p. 2). Nonetheless these elemental components link precisely with four of the most common musical elements: "Pitch" and "timbre" match exactly, "loudness" links with dynamics and "time" links with the time-based elements of rhythm, duration and tempo. This usage of the phrase "the elements of music" links more closely with "Webster's New 20th Century Dictionary" definition of an element as: "a substance which cannot be divided into a simpler form by known methods" and educational institutions' lists of elements generally align with this definition as well.

Although writers of lists of "rudimentary elements of music" can vary their lists depending on their personal (or institutional) priorities, the perceptual elements of music should consist of an established (or proven) list of discrete elements which can be independently manipulated to achieve an intended musical effect. It seems at this stage that there is still research to be done in this area. 

A slightly different way of approaching the identification of the elements of music, is to identify the "elements of sound" as: pitch, duration, loudness, timbre, sonic texture and spatial location, and then to define the "elements of music" as: sound, structure, and artistic intent.

Pitch is an aspect of a sound that we can hear, reflecting whether one musical sound, note or tone is "higher" or "lower" than another musical sound, note or tone. We can talk about the highness or lowness of pitch in the more general sense, such as the way a listener hears a piercingly high piccolo note or whistling tone as higher in pitch than a deep thump of a bass drum. We also talk about pitch in the precise sense associated with musical melodies, basslines and chords. Precise pitch can only be determined in sounds that have a frequency that is clear and stable enough to distinguish from noise. For example, it is much easier for listeners to discern the pitch of a single note played on a piano than to try to discern the pitch of a crash cymbal that is struck.
A melody (also called a "tune") is a series of pitches (notes) sounding in succession (one after the other), often in a rising and falling pattern. The notes of a melody are typically created using pitch systems such as scales or modes. Melodies also often contain notes from the chords used in the song. The melodies in simple folk songs and traditional songs may use only the notes of a single scale, the scale associated with the tonic note or key of a given song. For example, a folk song in the key of C (also referred to as C major) may have a melody that uses only the notes of the C major scale (the individual notes C, D, E, F, G, A, B and C; these are the "white notes" on a piano keyboard. On the other hand, Bebop-era jazz from the 1940s and contemporary music from the 20th and 21st centuries may use melodies with many chromatic notes (i.e., notes in addition to the notes of the major scale; on a piano, a chromatic scale would include all the notes on the keyboard, including the "white notes" and "black notes" and unusual scales, such as the whole tone scale (a whole tone scale in the key of C would contain the notes C, D, E, F, G and A). A low, deep musical line played by bass instruments such as double bass, electric bass or tuba is called a bassline.

Harmony refers to the "vertical" sounds of pitches in music, which means pitches that are played or sung together at the same time to create a chord. Usually this means the notes are played at the same time, although harmony may also be implied by a melody that outlines a harmonic structure (i.e., by using melody notes that are played one after the other, outlining the notes of a chord). In music written using the system of major-minor tonality ("keys"), which includes most classical music written from 1600 to 1900 and most Western pop, rock and traditional music, the key of a piece determines the scale used, which centres around the "home note" or tonic of the key. Simple classical pieces and many pop and traditional music songs are written so that all the music is in a single key. More complex Classical, pop and traditional music songs and pieces may have two keys (and in some cases three or more keys). Classical music from the Romantic era (written from about 1820–1900) often contains multiple keys, as does jazz, especially Bebop jazz from the 1940s, in which the key or "home note" of a song may change every four bars or even every two bars.

Rhythm is the arrangement of sounds and silences in time. Meter animates time in regular pulse groupings, called measures or bars, which in Western classical, popular and traditional music often group notes in sets of two (e.g., 2/4 time), three (e.g., 3/4 time, also known as Waltz time, or 3/8 time), or four (e.g., 4/4 time). Meters are made easier to hear because songs and pieces often (but not always) place an emphasis on the first beat of each grouping. Notable exceptions exist, such as the backbeat used in much Western pop and rock, in which a song that uses a measure that consists of four beats (called 4/4 time or common time) will have accents on beats two and four, which are typically performed by the drummer on the snare drum, a loud and distinctive-sounding percussion instrument. In pop and rock, the rhythm parts of a song are played by the rhythm section, which includes chord-playing instruments (e.g., electric guitar, acoustic guitar, piano, or other keyboard instruments), a bass instrument (typically electric bass or for some styles such as jazz and bluegrass, double bass) and a drum kit player.

Musical texture is the overall sound of a piece of music or song. The texture of a piece or song is determined by how the melodic, rhythmic, and harmonic materials are combined in a composition, thus determining the overall nature of the sound in a piece. Texture is often described in regard to the density, or thickness, and range, or width, between lowest and highest pitches, in relative terms as well as more specifically distinguished according to the number of voices, or parts, and the relationship between these voices (see common types below). For example, a thick texture contains many 'layers' of instruments. One of these layers could be a string section, or another brass. The thickness also is affected by the amount and the richness of the instruments. Texture is commonly described according to the number of and relationship between parts or lines of music:

Music that contains a large number of independent parts (e.g., a double concerto accompanied by 100 orchestral instruments with many interweaving melodic lines) is generally said to have a "thicker" or "denser" texture than a work with few parts (e.g., a solo flute melody accompanied by a single cello).

Timbre, sometimes called "color" or "tone color" is the quality or sound of a voice or instrument. Timbre is what makes a particular musical sound different from another, even when they have the same pitch and loudness. For example, a 440 Hz A note sounds different when it is played on oboe, piano, violin or electric guitar. Even if different players of the same instrument play the same note, their notes might sound different due to differences in instrumental technique (e.g., different embouchures), different types of accessories (e.g., mouthpieces for brass players, reeds for oboe and bassoon players) or strings made out of different materials for string players (e.g., gut strings versus steel strings). Even two instrumentalists playing the same note on the same instrument (one after the other) may sound different due to different ways of playing the instrument (e.g., two string players might hold the bow differently).

The physical characteristics of sound that determine the perception of timbre include the spectrum, envelope and overtones of a note or musical sound. For electric instruments developed in the 20th century, such as electric guitar, electric bass and electric piano, the performer can also change the tone by adjusting equalizer controls, tone controls on the instrument, and by using electronic effects units such as distortion pedals. The tone of the electric Hammond organ is controlled by adjusting drawbars.

Expressive qualities are those elements in music that create change in music without changing the main pitches or substantially changing the rhythms of the melody and its accompaniment. Performers, including singers and instrumentalists, can add musical expression to a song or piece by adding phrasing, by adding effects such as vibrato (with voice and some instruments, such as guitar, violin, brass instruments and woodwinds), dynamics (the loudness or softness of piece or a section of it), tempo fluctuations (e.g., ritardando or accelerando, which are, respectively slowing down and speeding up the tempo), by adding pauses or fermatas on a cadence, and by changing the articulation of the notes (e.g., making notes more pronounced or accented, by making notes more legato, which means smoothly connected, or by making notes shorter).

Expression is achieved through the manipulation of pitch (such as inflection, vibrato, slides etc.), volume (dynamics, accent, tremolo etc.), duration (tempo fluctuations, rhythmic changes, changing note duration such as with legato and staccato, etc.), timbre (e.g. changing vocal timbre from a light to a resonant voice) and sometimes even texture (e.g. doubling the bass note for a richer effect in a piano piece). Expression therefore can be seen as a manipulation of all elements in order to convey "an indication of mood, spirit, character etc." and as such cannot be included as a unique perceptual element of music, although it can be considered an important rudimentary element of music.

In music, form describes the overall structure or plan of a song or piece of music, and it describes the layout of a composition as divided into sections. In the early 20th century, Tin Pan Alley songs and Broadway musical songs were often in AABA 32 bar form, in which the A sections repeated the same eight bar melody (with variation) and the B section provided a contrasting melody or harmony for eight bars. From the 1960s onward, Western pop and rock songs are often in verse-chorus form, which is based around a sequence of verse and chorus ("refrain") sections, with new lyrics for most verses and repeating lyrics for the choruses. Popular music often makes use of strophic form, sometimes in conjunction with the twelve bar blues.

In the tenth edition of "The Oxford Companion to Music", Percy Scholes defines musical form as "a series of strategies designed to find a successful mean between the opposite extremes of unrelieved repetition and unrelieved alteration." Examples of common forms of Western music include the fugue, the invention, sonata-allegro, canon, strophic, theme and variations, and rondo.

Scholes states that European classical music had only six stand-alone forms: simple binary, simple ternary, compound binary, rondo, air with variations, and fugue (although musicologist Alfred Mann emphasized that the fugue is primarily a method of composition that has sometimes taken on certain structural conventions.)

Where a piece cannot readily be broken down into sectional units (though it might borrow some form from a poem, story or programme), it is said to be through-composed. Such is often the case with a fantasia, prelude, rhapsody, etude (or study), symphonic poem, Bagatelle, impromptu, etc. Professor Charles Keil classified forms and formal detail as "sectional, developmental, or variational."








Some styles of music place an emphasis on certain of these fundamentals, while others place less emphasis on certain elements. To give one example, while Bebop-era jazz makes use of very complex chords, including altered dominants and challenging chord progressions, with chords changing two or more times per bar and keys changing several times in a tune, funk places most of its emphasis on rhythm and groove, with entire songs based around a vamp on a single chord. While Romantic era classical music from the mid- to late-1800s makes great use of dramatic changes of dynamics, from whispering pianissimo sections to thunderous fortissimo sections, some entire Baroque dance suites for harpsichord from the early 1700s may use a single dynamic. To give another example, while some art music pieces, such as symphonies are very long, some pop songs are just a few minutes long.
Prehistoric music can only be theorized based on findings from paleolithic archaeology sites. Flutes are often discovered, carved from bones in which lateral holes have been pierced; these are thought to have been blown at one end like the Japanese shakuhachi. The Divje Babe flute, carved from a cave bear femur, is thought to be at least 40,000 years old. Instruments such as the seven-holed flute and various types of stringed instruments, such as the Ravanahatha, have been recovered from the Indus Valley Civilization archaeological sites. India has one of the oldest musical traditions in the world—references to Indian classical music ("marga") are found in the Vedas, ancient scriptures of the Hindu tradition. The earliest and largest collection of prehistoric musical instruments was found in China and dates back to between 7000 and 6600 BC. The "Hurrian Hymn to Nikkal", found on clay tablets that date back to approximately 1400 BC, is the oldest surviving notated work of music.

The ancient Egyptians credited one of their gods, Thoth, with the invention of music, with Osiris in turn used as part of his effort to civilize the world. The earliest material and representational evidence of Egyptian musical instruments dates to the Predynastic period, but the evidence is more securely attested in the Old Kingdom when harps, flutes and double clarinets were played. Percussion instruments, lyres and lutes were added to orchestras by the Middle Kingdom. Cymbals frequently accompanied music and dance, much as they still do in Egypt today. Egyptian folk music, including the traditional Sufi "dhikr" rituals, are the closest contemporary music genre to ancient Egyptian music, having preserved many of its features, rhythms and instruments.

Asian music covers a vast swath of music cultures surveyed in the articles on Arabia, Central Asia, East Asia, South Asia, and Southeast Asia. Several have traditions reaching into antiquity.

Indian classical music is one of the oldest musical traditions in the world. The Indus Valley civilization has sculptures that show dance and old musical instruments, like the seven holed flute. Various types of stringed instruments and drums have been recovered from Harappa and Mohenjo Daro by excavations carried out by Sir Mortimer Wheeler. The Rigveda has elements of present Indian music, with a musical notation to denote the metre and the mode of chanting. Indian classical music (marga) is monophonic, and based on a single melody line or raga rhythmically organized through talas. "Silappadhikaram" by Ilango Adigal provides information about how new scales can be formed by modal shifting of the tonic from an existing scale. Present day Hindi music was influenced by Persian traditional music and Afghan Mughals. Carnatic music, popular in the southern states, is largely devotional; the majority of the songs are addressed to the Hindu deities. There are also many songs emphasising love and other social issues.

Chinese classical music, the traditional art or court music of China, has a history stretching over around three thousand years. It has its own unique systems of musical notation, as well as musical tuning and pitch, musical instruments and styles or musical genres. Chinese music is pentatonic-diatonic, having a scale of twelve notes to an octave (5 + 7 = 12) as does European-influenced music.

Knowledge of the biblical period is mostly from literary references in the Bible and post-biblical sources. Religion and music historian Herbert Lockyer, Jr. writes that "music, both vocal and instrumental, was well cultivated among the Hebrews, the New Testament Christians, and the Christian church through the centuries." He adds that "a look at the Old Testament reveals how God's ancient people were devoted to the study and practice of music, which holds a unique place in the historical and prophetic books, as well as the Psalter."

Music and theatre scholars studying the history and anthropology of Semitic and early Judeo-Christian culture have discovered common links in theatrical and musical activity between the classical cultures of the Hebrews and those of later Greeks and Romans. The common area of performance is found in a "social phenomenon called litany," a form of prayer consisting of a series of invocations or supplications. The "Journal of Religion and Theatre" notes that among the earliest forms of litany, "Hebrew litany was accompanied by a rich musical tradition:"

Genesis 4.21 indicated that Jubal is the "father of all such as handle the harp and pipe", the Pentateuch is nearly silent about the practice and instruction of music in the early life of Israel". In I Samuel 10, there are more depictions of "large choirs and orchestras". These large ensembles could only be run with extensive rehearsals. This had led some scholars to theorize that the prophet Samuel led a public music school to a wide range of students.

Music was an important part of social and cultural life in ancient Greece. Musicians and singers played a prominent role in Greek theater. Mixed-gender choruses performed for entertainment, celebration, and spiritual ceremonies. Instruments included the double-reed "aulos" and a plucked string instrument, the "lyre", principally the special kind called a "kithara". Music was an important part of education, and boys were taught music starting at age six. Greek musical literacy created a flowering of music development. Greek music theory included the Greek musical modes, that eventually became the basis for Western religious and classical music. Later, influences from the Roman Empire, Eastern Europe, and the Byzantine Empire changed Greek music. The Seikilos epitaph is the oldest surviving example of a complete musical composition, including musical notation, from anywhere in the world. The oldest surviving work written on the subject of music theory is "Harmonika Stoicheia" by Aristoxenus.

The medieval era (476 to 1400), which took place during the Middle Ages, started with the introduction of monophonic (single melodic line) chanting into Roman Catholic Church services. Musical notation was used since Ancient times in Greek culture, but in the Middle Ages, notation was first introduced by the Catholic church so that the chant melodies could be written down, to facilitate the use of the same melodies for religious music across the entire Catholic empire. The only European Medieval repertory that has been found in written form from before 800 is the monophonic liturgical plainsong chant of the Roman Catholic Church, the central tradition of which was called Gregorian chant. Alongside these traditions of sacred and church music there existed a vibrant tradition of secular song (non-religious songs). Examples of composers from this period are Léonin, Pérotin, Guillaume de Machaut, and Walther von der Vogelweide.

Renaissance music (c. 1400 to 1600) was more focused on secular (non-religious) themes, such as courtly love. Around 1450, the printing press was invented, which made printed sheet music much less expensive and easier to mass-produce (prior to the invention of the printing press, all notated music was hand-copied). The increased availability of sheet music helped to spread musical styles more quickly and across a larger area. Musicians and singers often worked for the church, courts and towns. Church choirs grew in size, and the church remained an important patron of music. By the middle of the 15th century, composers wrote richly polyphonic sacred music, in which different melody lines were interwoven simultaneously. Prominent composers from this era include Guillaume Dufay, Giovanni Pierluigi da Palestrina, Thomas Morley, and Orlande de Lassus. As musical activity shifted from the church to the aristocratic courts, kings, queens and princes competed for the finest composers. Many leading important composers came from the Netherlands, Belgium, and northern France. They are called the Franco-Flemish composers. They held important positions throughout Europe, especially in Italy. Other countries with vibrant musical activity included Germany, England, and Spain.

The Baroque era of music took place from 1600 to 1750, as the Baroque artistic style flourished across Europe; and during this time, music expanded in its range and complexity. Baroque music began when the first operas (dramatic solo vocal music accompanied by orchestra) were written. During the Baroque era, polyphonic contrapuntal music, in which multiple, simultaneous independent melody lines were used, remained important (counterpoint was important in the vocal music of the Medieval era). German Baroque composers wrote for small ensembles including strings, brass, and woodwinds, as well as for choirs and keyboard instruments such as pipe organ, harpsichord, and clavichord. During this period several major music forms were defined that lasted into later periods when they were expanded and evolved further, including the fugue, the invention, the sonata, and the concerto. The late Baroque style was polyphonically complex and richly ornamented. Important composers from the Baroque era include Johann Sebastian Bach ("Cello suites"), George Frideric Handel ("Messiah"), Georg Philipp Telemann and Antonio Lucio Vivaldi ("The Four Seasons").

The music of the Classical period (1730 to 1820) aimed to imitate what were seen as the key elements of the art and philosophy of Ancient Greece and Rome: the ideals of balance, proportion and disciplined expression. (Note: the music from the Classical period should not be confused with Classical music in general, a term which refers to Western art music from the 5th century to the 2000s, which includes the Classical period as one of a number of periods). Music from the Classical period has a lighter, clearer and considerably simpler texture than the Baroque music which preceded it. The main style was homophony, where a prominent melody and a subordinate chordal accompaniment part are clearly distinct. Classical instrumental melodies tended to be almost voicelike and singable. New genres were developed, and the fortepiano, the forerunner to the modern piano, replaced the Baroque era harpsichord and pipe organ as the main keyboard instrument (though pipe organ continued to be used in sacred music, such as Masses).

Importance was given to instrumental music. It was dominated by further development of musical forms initially defined in the Baroque period: the sonata, the concerto, and the symphony. Others main kinds were the trio, string quartet, serenade and divertimento. The sonata was the most important and developed form. Although Baroque composers also wrote sonatas, the Classical style of sonata is completely distinct. All of the main instrumental forms of the Classical era, from string quartets to symphonies and concertos, were based on the structure of the sonata. The instruments used chamber music and orchestra became more standardized. In place of the basso continuo group of the Baroque era, which consisted of harpsichord, organ or lute along with a number of bass instruments selected at the discretion of the group leader (e.g., viol, cello, theorbo, serpent), Classical chamber groups used specified, standardized instruments (e.g., a string quartet would be performed by two violins, a viola and a cello). The Baroque era improvised chord-playing of the continuo keyboardist or lute player was gradually phased out between 1750 and 1800.

One of the most important changes made in the Classical period was the development of public concerts. The aristocracy still played a significant role in the sponsorship of concerts and compositions, but it was now possible for composers to survive without being permanent employees of queens or princes. The increasing popularity of classical music led to a growth in the number and types of orchestras. The expansion of orchestral concerts necessitated the building of large public performance spaces. Symphonic music including symphonies, musical accompaniment to ballet and mixed vocal/instrumental genres such as opera and oratorio became more popular.

The best known composers of Classicism are Carl Philipp Emanuel Bach, Christoph Willibald Gluck, Johann Christian Bach, Joseph Haydn, Wolfgang Amadeus Mozart, Ludwig van Beethoven and Franz Schubert. Beethoven and Schubert are also considered to be composers in the later part of the Classical era, as it began to move towards Romanticism.

Romantic music (c. 1810 to 1900) from the 19th century had many elements in common with the Romantic styles in literature and painting of the era. Romanticism was an artistic, literary, and intellectual movement was characterized by its emphasis on emotion and individualism as well as glorification of all the past and nature. Romantic music expanded beyond the rigid styles and forms of the Classical era into more passionate, dramatic expressive pieces and songs. Romantic composers such as Wagner and Brahms attempted to increase emotional expression and power in their music to describe deeper truths or human feelings. With symphonic tone poems, composers tried to tell stories and evoke images or landscapes using instrumental music. Some composers promoted nationalistic pride with patriotic orchestral music inspired by folk music. The emotional and expressive qualities of music came to take precedence over tradition.

Romantic composers grew in idiosyncrasy, and went further in the syncretism of exploring different art-forms in a musical context, (such as literature), history (historical figures and legends), or nature itself. Romantic love or longing was a prevalent theme in many works composed during this period. In some cases the formal structures from the classical period continued to be used (e.g., the sonata form used in string quartets and symphonies), but these forms were expanded and altered. In many cases, new approaches were explored for existing genres, forms, and functions. Also, new forms were created that were deemed better suited to the new subject matter. Composers continued to develop opera and ballet music, exploring new styles and themes.

In the years after 1800, the music developed by Ludwig van Beethoven and Franz Schubert introduced a more dramatic, expressive style. In Beethoven's case, short motifs, developed organically, came to replace melody as the most significant compositional unit (an example is the distinctive four note figure used in his Fifth Symphony). Later Romantic composers such as Pyotr Ilyich Tchaikovsky, Antonín Dvořák, and Gustav Mahler used more unusual chords and more dissonance to create dramatic tension. They generated complex and often much longer musical works. During the late Romantic period, composers explored dramatic chromatic alterations of tonality, such as extended chords and altered chords, which created new sound "colours". The late 19th century saw a dramatic expansion in the size of the orchestra, and the industrial revolution helped to create better instruments, creating a more powerful sound. Public concerts became an important part of well-to-do urban society. It also saw a new diversity in theatre music, including operetta, and musical comedy and other forms of musical theatre.

In the 19th century, one of the key ways that new compositions became known to the public was by the sales of sheet music, which middle class amateur music lovers would perform at home on their piano or other common instruments, such as violin. With 20th-century music, the invention of new electric technologies such as radio broadcasting and the mass market availability of gramophone records meant that sound recordings of songs and pieces heard by listeners (either on the radio or on their record player) became the main way to learn about new songs and pieces. There was a vast increase in music listening as the radio gained popularity and phonographs were used to replay and distribute music, because whereas in the 19th century, the focus on sheet music restricted access to new music to the middle class and upper-class people who could read music and who owned pianos and instruments, in the 20th century, anyone with a radio or record player could hear operas, symphonies and big bands right in their own living room. This allowed lower-income people, who would never be able to afford an opera or symphony concert ticket to hear this music. It also meant that people could hear music from different parts of the country, or even different parts of the world, even if they could not afford to travel to these locations. This helped to spread musical styles.

The focus of art music in the 20th century was characterized by exploration of new rhythms, styles, and sounds. The horrors of World War I influenced many of the arts, including music, and some composers began exploring darker, harsher sounds. Traditional music styles such as jazz and folk music were used by composers as a source of ideas for classical music. Igor Stravinsky, Arnold Schoenberg, and John Cage were all influential composers in 20th-century art music. The invention of sound recording and the ability to edit music gave rise to new subgenre of classical music, including the acousmatic and Musique concrète schools of electronic composition. Sound recording was also a major influence on the development of popular music genres, because it enabled recordings of songs and bands to be widely distributed. The introduction of the multitrack recording system had a major influence on rock music, because it could do much more than record a band's performance. Using a multitrack system, a band and their music producer could overdub many layers of instrument tracks and vocals, creating new sounds that would not be possible in a live performance.

Jazz evolved and became an important genre of music over the course of the 20th century, and during the second half of that century, rock music did the same. Jazz is an American musical artform that originated in the beginning of the 20th century in African American communities in the Southern United States from a confluence of African and European music traditions. The style's West African pedigree is evident in its use of blue notes, improvisation, polyrhythms, syncopation, and the swung note.

Rock music is a genre of popular music that developed in the 1960s from 1950s rock and roll, rockabilly, blues, and country music. The sound of rock often revolves around the electric guitar or acoustic guitar, and it uses a strong back beat laid down by a rhythm section. Along with the guitar or keyboards, saxophone and blues-style harmonica are used as soloing instruments. In its "purest form", it "has three chords, a strong, insistent back beat, and a catchy melody". The traditional rhythm section for popular music is rhythm guitar, electric bass guitar, drums. Some bands also have keyboard instruments such as organ, piano, or, since the 1970s, analog synthesizers. In the 1980s, pop musicians began using digital synthesizers, such as the DX-7 synthesizer, electronic drum machines such as the TR-808 and synth bass devices (such as the TB-303) or synth bass keyboards. In the 1990s, an increasingly large range of computerized hardware musical devices and instruments and software (e.g., digital audio workstations) were used. In the 2020s, soft synths and computer music apps make it possible for bedroom producers to create and record some types of music, such as electronic dance music in their own home, adding sampled and digital instruments and editing the recording digitally. In the 1990s, some bands in genres such as nu metal began including DJs in their bands. DJs create music by manipulating recorded music on record players or CD players, using a DJ mixer.

Performance is the physical expression of music, which occurs when a song is sung or when a piano piece, electric guitar melody, symphony, drum beat or other musical part is played by musicians. In classical music, a musical work is written in music notation by a composer and then it is performed once the composer is satisfied with its structure and instrumentation. However, as it gets performed, the interpretation of a song or piece can evolve and change. In classical music, instrumental performers, singers or conductors may gradually make changes to the phrasing or tempo of a piece. In popular and traditional music, the performers have a lot more freedom to make changes to the form of a song or piece. As such, in popular and traditional music styles, even when a band plays a cover song, they can make changes to it such as adding a guitar solo to or inserting an introduction.

A performance can either be planned out and rehearsed (practiced)—which is the norm in classical music, with jazz big bands and many popular music styles–or improvised over a chord progression (a sequence of chords), which is the norm in small jazz and blues groups. Rehearsals of orchestras, concert bands and choirs are led by a conductor. Rock, blues and jazz bands are usually led by the bandleader. A rehearsal is a structured repetition of a song or piece by the performers until it can be sung and/or played correctly and, if it is a song or piece for more than one musician, until the parts are together from a rhythmic and tuning perspective. Improvisation is the creation of a musical idea–a melody or other musical line–created on the spot, often based on scales or pre-existing melodic riffs.

Many cultures have strong traditions of solo performance (in which one singer or instrumentalist performs), such as in Indian classical music, and in the Western art-music tradition. Other cultures, such as in Bali, include strong traditions of group performance. All cultures include a mixture of both, and performance may range from improvised solo playing to highly planned and organised performances such as the modern classical concert, religious processions, classical music festivals or music competitions. Chamber music, which is music for a small ensemble with only a few of each type of instrument, is often seen as more intimate than large symphonic works.

Many types of music, such as traditional blues and folk music were not written down in sheet music; instead, they were originally preserved in the memory of performers, and the songs were handed down orally, from one musician or singer to another, or aurally, in which a performer learns a song "by ear". When the composer of a song or piece is no longer known, this music is often classified as "traditional" or as a "folk song". Different musical traditions have different attitudes towards how and where to make changes to the original source material, from quite strict, to those that demand improvisation or modification to the music. A culture's history and stories may also be passed on by ear through song.

In music, an ornament consists of added notes that provide decoration to a melody, bassline or other musical part. The detail included explicitly in the music notation varies between genres and historical periods. In general, art music notation from the 17th through the 19th centuries required performers to have a great deal of contextual knowledge about performing styles. For example, in the 17th and 18th centuries, music notated for solo performers typically indicated a simple, unadorned melody. Performers were expected to know how to add stylistically appropriate ornaments to add interest to the music, such as trills and turns. Different styles of music use different ornaments. A Baroque flute player might add mordents, which are short notes that are played before the main melody note, either above or below the main melody note. A blues guitarist playing electric guitar might use string bending to add expression; a heavy metal guitar player might use hammer-ons and pull-offs.

In the 19th century, art music for solo performers may give a general instruction such as to perform the music expressively, without describing in detail how the performer should do this. The performer was expected to know how to use tempo changes, accentuation, and pauses (among other devices) to obtain this "expressive" performance style. In the 20th century, art music notation often became more explicit and used a range of markings and annotations to indicate to performers how they should play or sing the piece. In popular music and traditional music styles, performers are expected to know what types of ornaments are stylistically appropriate for a given song or piece, and performers typically add them in an improvised fashion. One exception is note-for-note solos, in which some players precisely recreate a famous version of a solo, such as a guitar solo.

Philosophy of music is a subfield of philosophy. The philosophy of music is the study of fundamental questions regarding music. The philosophical study of music has many connections with philosophical questions in metaphysics and aesthetics.
Some basic questions in the philosophy of music are:


In ancient times, such as with the Ancient Greeks, the aesthetics of music explored the mathematical and cosmological dimensions of rhythmic and harmonic organization. In the 18th century, focus shifted to the experience of hearing music, and thus to questions about its beauty and human enjoyment ("plaisir" and "jouissance") of music. The origin of this philosophic shift is sometimes attributed to Alexander Gottlieb Baumgarten in the 18th century, followed by Immanuel Kant. Through their writing, the ancient term 'aesthetics', meaning sensory perception, received its present-day connotation. In the 2000s, philosophers have tended to emphasize issues besides beauty and enjoyment. For example, music's capacity to express emotion has been a central issue.

In the 20th century, important contributions were made by Peter Kivy, Jerrold Levinson, Roger Scruton, and Stephen Davies. However, many musicians, music critics, and other non-philosophers have contributed to the aesthetics of music. In the 19th century, a significant debate arose between Eduard Hanslick, a music critic and musicologist, and composer Richard Wagner regarding whether music can express meaning. Harry Partch and some other musicologists, such as Kyle Gann, have studied and tried to popularize microtonal music and the usage of alternate musical scales. Also many modern composers like La Monte Young, Rhys Chatham and Glenn Branca paid much attention to a scale called just intonation.

It is often thought that music has the ability to affect our emotions, intellect, and psychology; it can assuage our loneliness or incite our passions. The philosopher Plato suggests in "The Republic" that music has a direct effect on the soul. Therefore, he proposes that in the ideal regime music would be closely regulated by the state (Book VII).

There has been a strong tendency in the aesthetics of music to emphasize the paramount importance of compositional structure; however, other issues concerning the aesthetics of music include lyricism, harmony, hypnotism, emotiveness, temporal dynamics, resonance, playfulness, and color (see also musical development).

Modern music psychology aims to explain and understand musical behavior and experience. Research in this field and its subfields are primarily empirical; their knowledge tends to advance on the basis of interpretations of data collected by systematic observation of and interaction with human participants. In addition to its focus on fundamental perceptions and cognitive processes, music psychology is a field of research with practical relevance for many areas, including music performance, composition, education, criticism, and therapy, as well as investigations of human aptitude, skill, intelligence, creativity, and social behavior.

Cognitive neuroscience of music is the scientific study of brain-based mechanisms involved in the cognitive processes underlying music. These behaviours include music listening, performing, composing, reading, writing, and ancillary activities. It also is increasingly concerned with the brain basis for musical aesthetics and musical emotion. The field is distinguished by its reliance on direct observations of the brain, using such techniques as functional magnetic resonance imaging (fMRI), transcranial magnetic stimulation (TMS), magnetoencephalography (MEG), electroencephalography (EEG), and positron emission tomography (PET).

Cognitive musicology is a branch of cognitive science concerned with computationally modeling musical knowledge with the goal of understanding both music and cognition. The use of computer models provides an exacting, interactive medium in which to formulate and test theories and has roots in artificial intelligence and cognitive science.

This interdisciplinary field investigates topics such as the parallels between language and music in the brain. Biologically inspired models of computation are often included in research, such as neural networks and evolutionary programs. This field seeks to model how musical knowledge is represented, stored, perceived, performed, and generated. By using a well-structured computer environment, the systematic structures of these cognitive phenomena can be investigated.

Psychoacoustics is the scientific study of sound perception. More specifically, it is the branch of science studying the psychological and physiological responses associated with sound (including speech and music). It can be further categorized as a branch of psychophysics.

Evolutionary musicology concerns the "origins of music, the question of animal song, selection pressures underlying music evolution", and "music evolution and human evolution". It seeks to understand music perception and activity in the context of evolutionary theory. Charles Darwin speculated that music may have held an adaptive advantage and functioned as a protolanguage, a view which has spawned several competing theories of music evolution. An alternate view sees music as a by-product of linguistic evolution; a type of "auditory cheesecake" that pleases the senses without providing any adaptive function. This view has been directly countered by numerous music researchers.

An individual's culture or ethnicity plays a role in their music cognition, including their preferences, emotional reaction, and musical memory. Musical preferences are biased toward culturally familiar musical traditions beginning in infancy, and adults' classification of the emotion of a musical piece depends on both culturally specific and universal structural features. Additionally, individuals' musical memory abilities are greater for culturally familiar music than for culturally unfamiliar music.

Many ethnographic studies demonstrate that music is a participatory, community-based activity. Music is experienced by individuals in a range of social settings ranging from being alone to attending a large concert, forming a music community, which cannot be understood as a function of individual will or accident; it includes both commercial and non-commercial participants with a shared set of common values. Musical performances take different forms in different cultures and socioeconomic milieus. In Europe and North America, there is often a divide between what types of music are viewed as a "high culture" and "low culture." "High culture" types of music typically include Western art music such as Baroque, Classical, Romantic, and modern-era symphonies, concertos, and solo works, and are typically heard in formal concerts in concert halls and churches, with the audience sitting quietly in seats.

Other types of music—including, but not limited to, jazz, blues, soul, and country—are often performed in bars, nightclubs, and theatres, where the audience may be able to drink, dance, and express themselves by cheering. Until the later 20th century, the division between "high" and "low" musical forms was widely accepted as a valid distinction that separated out better quality, more advanced "art music" from the popular styles of music heard in bars and dance halls.

However, in the 1980s and 1990s, musicologists studying this perceived divide between "high" and "low" musical genres argued that this distinction is not based on the musical value or quality of the different types of music. Rather, they argued that this distinction was based largely on the socioeconomics standing or social class of the performers or audience of the different types of music. For example, whereas the audience for Classical symphony concerts typically have above-average incomes, the audience for a rap concert in an inner-city area may have below-average incomes. Even though the performers, audience, or venue where non-"art" music is performed may have a lower socioeconomic status, the music that is performed, such as blues, rap, punk, funk, or ska may be very complex and sophisticated.

When composers introduce styles of music that break with convention, there can be a strong resistance from academic music experts and popular culture. Late-period Beethoven string quartets, Stravinsky ballet scores, serialism, bebop-era jazz, hip hop, punk rock, and electronica have all been considered non-music by some critics when they were first introduced. Such themes are examined in the sociology of music. The sociological study of music, sometimes called sociomusicology, is often pursued in departments of sociology, media studies, or music, and is closely related to the field of ethnomusicology.

Women have played a major role in music throughout history, as composers, songwriters, instrumental performers, singers, conductors, music scholars, music educators, music critics/music journalists and other musical professions. As well, it describes music movements, events and genres related to women, women's issues and feminism. In the 2010s, while women comprise a significant proportion of popular music and classical music singers, and a significant proportion of songwriters (many of them being singer-songwriters), there are few women record producers, rock critics and rock instrumentalists. Although there have been a huge number of women composers in classical music, from the Medieval period to the present day, women composers are significantly underrepresented in the commonly performed classical music repertoire, music history textbooks and music encyclopedias; for example, in the "Concise Oxford History of Music", Clara Schumann is one of the only female composers who is mentioned.

Women comprise a significant proportion of instrumental soloists in classical music and the percentage of women in orchestras is increasing. A 2015 article on concerto soloists in major Canadian orchestras, however, indicated that 84% of the soloists with the Orchestre Symphonique de Montreal were men. In 2012, women still made up just 6% of the top-ranked Vienna Philharmonic orchestra. Women are less common as instrumental players in popular music genres such as rock and heavy metal, although there have been a number of notable female instrumentalists and all-female bands. Women are particularly underrepresented in extreme metal genres. In the 1960s pop-music scene, "[l]ike most aspects of the...music business, [in the 1960s,] songwriting was a male-dominated field. Though there were plenty of female singers on the radio, women ...were primarily seen as consumers:... Singing was sometimes an acceptable pastime for a girl, but playing an instrument, writing songs, or producing records simply wasn't done." Young women "...were not socialized to see themselves as people who create [music]."

Women are also underrepresented in orchestral conducting, music criticism/music journalism, music producing, and sound engineering. While women were discouraged from composing in the 19th century, and there are few women musicologists, women became involved in music education "...to such a degree that women dominated [this field] during the later half of the 19th century and well into the 20th century."

According to Jessica Duchen, a music writer for London's "The Independent", women musicians in classical music are "...too often judged for their appearances, rather than their talent" and they face pressure "...to look sexy onstage and in photos." Duchen states that while "[t]here are women musicians who refuse to play on their looks...the ones who do tend to be more materially successful."
According to the UK's Radio 3 editor, Edwina Wolstencroft, the music industry has long been open to having women in performance or entertainment roles, but women are much less likely to have positions of authority, such as being the leader of an orchestra. In popular music, while there are many women singers recording songs, there are very few women behind the audio console acting as music producers, the individuals who direct and manage the recording process. One of the most recorded artists is Asha Bhosle, an Indian singer best known as a playback singer in Hindi cinema.

The music that composers and songwriters make can be heard through several media; the most traditional way is to hear it live, in the presence of the musicians (or as one of the musicians), in an outdoor or indoor space such as an amphitheatre, concert hall, cabaret room, theatre, pub, or coffeehouse. Since the 20th century, live music can also be broadcast over the radio, television or the Internet, or recorded and listened to on a CD player or Mp3 player. 

Some musical styles focus on producing songs and pieces for a live performance, while others focus on producing a recording that mixes together sounds that were never played "live." Even in essentially live styles such as rock, recording engineers often use the ability to edit, splice and mix to produce recordings that may be considered "better" than the actual live performance. For example, some singers record themselves singing a melody and then record multiple harmony parts using overdubbing, creating a sound that would be impossible to do live.

Technology has had an influence on music since prehistoric times, when cave people used simple tools to bore holes into bone flutes 41,000 years ago. Technology continued to influence music throughout the history of music, as it enabled new instruments and music notation reproduction systems to be used, with one of the watershed moments in music notation being the invention of the printing press in the 1400s, which meant music scores no longer had to be hand copied. In the 19th century, music technology led to the development of a more powerful, louder piano and led to the development of new valves brass instruments. 

In the early 20th century (in the late 1920s), as talking pictures emerged in the early 20th century, with their prerecorded musical tracks, an increasing number of moviehouse orchestra musicians found themselves out of work. During the 1920s, live musical performances by orchestras, pianists, and theater organists were common at first-run theaters. With the coming of the talking motion pictures, those featured performances were largely eliminated. The American Federation of Musicians (AFM) took out newspaper advertisements protesting the replacement of live musicians with mechanical playing devices. One 1929 ad that appeared in the "Pittsburgh Press" features an image of a can labeled "Canned Music / Big Noise Brand / Guaranteed to Produce No Intellectual or Emotional Reaction Whatever"

Since legislation introduced to help protect performers, composers, publishers and producers, including the Audio Home Recording Act of 1992 in the United States, and the 1979 revised Berne Convention for the Protection of Literary and Artistic Works in the United Kingdom, recordings and live performances have also become more accessible through computers, devices and Internet in a form that is commonly known as Music-On-Demand.

In many cultures, there is less distinction between performing and listening to music, since virtually everyone is involved in some sort of musical activity, often in a communal setting. In industrialized countries, listening to music through a recorded form, such as sound recording on record or radio became more common than experiencing live performance, roughly in the middle of the 20th century. By the 1980s, watching music videos was a popular way to listen to music, while also seeing the performers.

Sometimes, live performances incorporate prerecorded sounds. For example, a disc jockey uses disc records for scratching, and some 20th-century works have a solo for an instrument or voice that is performed along with music that is prerecorded onto a tape. Some pop bands use recorded backing tracks. Computers and many keyboards can be programmed to produce and play Musical Instrument Digital Interface (MIDI) music. Audiences can also "become" performers by participating in karaoke, an activity of Japanese origin centered on a device that plays voice-eliminated versions of well-known songs. Most karaoke machines also have video screens that show lyrics to songs being performed; performers can follow the lyrics as they sing over the instrumental tracks.

The advent of the Internet and widespread high-speed broadband access has transformed the experience of music, partly through the increased ease of access to recordings of music via streaming video and vastly increased choice of music for consumers. Chris Anderson, in his book "", suggests that while the traditional economic model of supply and demand describes scarcity, the Internet retail model is based on abundance. Digital storage costs are low, so a company can afford to make its whole recording inventory available online, giving customers as much choice as possible. It has thus become economically viable to offer music recordings that very few people are interested in. Consumers' growing awareness of their increased choice results in a closer association between listening tastes and social identity, and the creation of thousands of niche markets.

Another effect of the Internet arose with online communities and social media websites like YouTube and Facebook, a social networking service. These sites make it easier for aspiring singers and amateur bands to distribute videos of their songs, connect with other musicians, and gain audience interest. Professional musicians also use YouTube as a free publisher of promotional material. YouTube users, for example, no longer only download and listen to MP3s, but also actively create their own. According to Don Tapscott and Anthony D. Williams, in their book "Wikinomics", there has been a shift from a traditional consumer role to what they call a "prosumer" role, a consumer who both creates content and consumes. Manifestations of this in music include the production of mashes, remixes, and music videos by fans.

The music industry refers to the businesses connected with the creation and sale of music. It consists of songwriters and composers who create new songs and musical pieces, music producers and sound engineers who record songs and pieces, record labels and publishers that distribute recorded music products and sheet music internationally and that often control the rights to those products. Some music labels are "independent," while others are subsidiaries of larger corporate entities or international media groups. In the 2000s, the increasing popularity of listening to music as digital music files on MP3 players, iPods, or computers, and of trading music on file sharing websites or buying it online in the form of digital files had a major impact on the traditional music business. Many smaller independent CD stores went out of business as music buyers decreased their purchases of CDs, and many labels had lower CD sales. Some companies did well with the change to a digital format, though, such as Apple's iTunes, an online music store that sells digital files of songs over the Internet.

In spite of some international copyright treaties, determining which music is in the public domain is complicated by of national copyright laws that may be applicable. US copyright law formerly protected printed music published after 1923 for 28 years and with renewal for another 28 years, but the Copyright Act of 1976 made renewal automatic, and the Digital Millennium Copyright Act changed the calculation of the copyright term to 70 years after the death of the creator. Recorded sound falls under mechanical licensing, often covered by a confusing patchwork of state laws; most cover versions are licensed through the Harry Fox Agency. Performance rights may be obtained by either performers or the performance venue; the two major organizations for licensing are BMI and ASCAP. Two online sources for public domain music are IMSLP (International Music Score Library Project) and Choral Public Domain Library (CPDL).

The incorporation of some music or singing training into general education from preschool to post secondary education is common in North America and Europe. Involvement in playing and singing music is thought to teach basic skills such as concentration, counting, listening, and cooperation while also promoting understanding of language, improving the ability to recall information, and creating an environment more conducive to learning in other areas. In elementary schools, children often learn to play instruments such as the recorder, sing in small choirs, and learn about the history of Western art music and traditional music. Some elementary school children also learn about popular music styles. In religious schools, children sing hymns and other religious music. In secondary schools (and less commonly in elementary schools), students may have the opportunity to perform in some types of musical ensembles, such as choirs (a group of singers), marching bands, concert bands, jazz bands, or orchestras. In some school systems, music lessons on how to play instruments may be provided. Some students also take private music lessons after school with a singing teacher or instrument teacher. Amateur musicians typically learn basic musical rudiments (e.g., learning about musical notation for musical scales and rhythms) and beginner- to intermediate-level singing or instrument-playing techniques.

At the university level, students in most arts and humanities programs can receive credit for taking a few music courses, which typically take the form of an overview course on the history of music, or a music appreciation course that focuses on listening to music and learning about different musical styles. In addition, most North American and European universities have some types of musical ensembles that students in arts and humanities are able to participate in, such as choirs, marching bands, concert bands, or orchestras. The study of Western art music is increasingly common outside of North America and Europe, such as the Indonesian Institute of the Arts in Yogyakarta, Indonesia, or the classical music programs that are available in Asian countries such as South Korea, Japan, and China. At the same time, Western universities and colleges are widening their curriculum to include music of non-Western cultures, such as the music of Africa or Bali (e.g. Gamelan music).

Individuals aiming to become professional musicians, singers, composers, songwriters, music teachers and practitioners of other music-related professions such as music history professors, sound engineers, and so on study in specialized post-secondary programs offered by colleges, universities and music conservatories. Some institutions that train individuals for careers in music offer training in a wide range of professions, as is the case with many of the top U.S. universities, which offer degrees in music performance (including singing and playing instruments), music history, music theory, music composition, music education (for individuals aiming to become elementary or high school music teachers) and, in some cases, conducting. On the other hand, some small colleges may only offer training in a single profession (e.g., sound recording).

While most university and conservatory music programs focus on training students in classical music, there are a number of universities and colleges that train musicians for careers as jazz or popular music musicians and composers, with notable U.S. examples including the Manhattan School of Music and the Berklee College of Music. Two important schools in Canada which offer professional jazz training are McGill University and Humber College. Individuals aiming at careers in some types of music, such as heavy metal music, country music or blues are less likely to become professionals by completing degrees or diplomas in colleges or universities. Instead, they typically learn about their style of music by singing and/or playing in many bands (often beginning in amateur bands, cover bands and tribute bands), studying recordings available on CD, DVD and the Internet and working with already-established professionals in their style of music, either through informal mentoring or regular music lessons. Since the 2000s, the increasing popularity and availability of Internet forums and YouTube "how-to" videos have enabled many singers and musicians from metal, blues and similar genres to improve their skills. Many pop, rock and country singers train informally with vocal coaches and singing teachers.

Undergraduate university degrees in music, including the Bachelor of Music, the Bachelor of Music Education, and the Bachelor of Arts (with a major in music) typically take about four years to complete. These degrees provide students with a grounding in music theory and music history, and many students also study an instrument or learn singing technique as part of their program. Graduates of undergraduate music programs can seek employment or go on to further study in music graduate programs. Bachelor's degree graduates are also eligible to apply to some graduate programs and professional schools outside of music (e.g., public administration, business administration, library science, and, in some jurisdictions, teacher's college, law school or medical school).

Graduate music degrees include the Master of Music, the Master of Arts (in musicology, music theory or another music field), the Doctor of Philosophy (Ph.D.) (e.g., in musicology or music theory), and more recently, the Doctor of Musical Arts, or DMA. The Master of Music degree, which takes one to two years to complete, is typically awarded to students studying the performance of an instrument, education, voice (singing) or composition. The Master of Arts degree, which takes one to two years to complete and often requires a thesis, is typically awarded to students studying musicology, music history, music theory or ethnomusicology.

The PhD, which is required for students who want to work as university professors in musicology, music history, or music theory, takes three to five years of study after the master's degree, during which time the student will complete advanced courses and undertake research for a dissertation. The DMA is a relatively new degree that was created to provide a credential for professional performers or composers that want to work as university professors in musical performance or composition. The DMA takes three to five years after a master's degree, and includes advanced courses, projects, and performances. In Medieval times, the study of music was one of the Quadrivium of the seven Liberal Arts and considered vital to higher learning. Within the quantitative Quadrivium, music, or more accurately harmonics, was the study of rational proportions.

Musicology, the academic study of the subject of music, is studied in universities and music conservatories. The earliest definitions from the 19th century defined three sub-disciplines of musicology: systematic musicology, historical musicology, and comparative musicology or ethnomusicology. In 2010-era scholarship, one is more likely to encounter a division of the discipline into music theory, music history, and ethnomusicology. Research in musicology has often been enriched by cross-disciplinary work, for example in the field of psychoacoustics. The study of music of non-Western cultures, and the cultural study of music, is called ethnomusicology. Students can pursue the undergraduate study of musicology, ethnomusicology, music history, and music theory through several different types of degrees, including bachelor's degrees, master's degrees and PhD degrees.

Music theory is the study of music, generally in a highly technical manner outside of other disciplines. More broadly it refers to any study of music, usually related in some form with compositional concerns, and may include mathematics, physics, and anthropology. What is most commonly taught in beginning music theory classes are guidelines to write in the style of the common practice period, or tonal music. Theory, even of music of the common practice period, may take many other forms. Musical set theory is the application of mathematical set theory to music, first applied to atonal music. "Speculative music theory", contrasted with "analytic music theory", is devoted to the analysis and synthesis of music materials, for example tuning systems, generally as preparation for composition.

Zoomusicology is the study of the music of non-human animals, or the musical aspects of sounds produced by non-human animals. As George Herzog (1941) asked, "do animals have music?" François-Bernard Mâche's "Musique, mythe, nature, ou les Dauphins d'Arion" (1983), a study of "ornitho-musicology" using a technique of Nicolas Ruwet's "Langage, musique, poésie" (1972) paradigmatic segmentation analysis, shows that bird songs are organised according to a repetition-transformation principle. Jean-Jacques Nattiez (1990), argues that "in the last analysis, it is a human being who decides what is and is not musical, even when the sound is not of human origin. If we acknowledge that sound is not organised and conceptualised (that is, made to form music) merely by its producer, but by the mind that perceives it, then music is uniquely human."

In the West, much of the history of music that is taught deals with the Western civilization's art music, which is known as classical music. The history of music in non-Western cultures ("world music" or the field of "ethnomusicology"), which typically covers music from
Africa and Asia is also taught in Western universities. This includes the documented classical traditions of Asian countries outside the influence of Western Europe, as well as the folk or indigenous music of various other cultures. Popular or folk styles of music in non-Western countries varied widely from culture to culture, and from period to period. Different cultures emphasised different instruments, techniques, singing styles and uses for music. Music has been used for entertainment, ceremonies, rituals, religious purposes and for practical and artistic communication. Non-Western music has also been used for propaganda purposes, as was the case with Chinese opera during the Cultural Revolution.

There is a host of music classifications for non-Western music, many of which are caught up in the argument over the definition of music. Among the largest of these is the division between classical music (or "art" music), and popular music (or commercial music – including non-Western styles of rock, country, and pop music-related styles). Some genres do not fit neatly into one of these "big two" classifications, (such as folk music, world music, or jazz-related music).

As world cultures have come into greater global contact, their indigenous musical styles have often merged with other styles, which produces new styles. For example, the United States bluegrass style contains elements from Anglo-Irish, Scottish, Irish, German and African instrumental and vocal traditions, which were able to fuse in the United States' multi-ethnic "melting pot" society. Some types of world music contain a mixture of non-Western indigenous styles with Western pop music elements. Genres of music are determined as much by tradition and presentation as by the actual music. Some works, like George Gershwin's "Rhapsody in Blue", are claimed by both jazz and classical music, while Gershwin's "Porgy and Bess" and Leonard Bernstein's "West Side Story" are claimed by both opera and the Broadway musical tradition. Many current music festivals for non-Western music include bands and singers from a particular musical genre, such as world music.

Indian music, for example, is one of the oldest and longest living types of music, and is still widely heard and performed in South Asia, as well as internationally (especially since the 1960s). Indian music has mainly three forms of classical music, Hindustani, Carnatic, and Dhrupad styles. It has also a large repertoire of styles, which involve only percussion music such as the talavadya performances famous in South India.

Music therapy is an interpersonal process in which a trained therapist uses music and all of its facets—physical, emotional, mental, social, aesthetic, and spiritual—to help clients to improve or maintain their health. In some instances, the client's needs are addressed directly through music; in others they are addressed through the relationships that develop between the client and therapist. Music therapy is used with individuals of all ages and with a variety of conditions, including: psychiatric disorders, medical problems, physical disabilities, sensory impairments, developmental disabilities, substance abuse issues, communication disorders, interpersonal problems, and aging. It is also used to improve learning, build self-esteem, reduce stress, support physical exercise, and facilitate a host of other health-related activities. Music therapists may encourage clients to sing, play instruments, create songs, or do other musical activities.

One of the earliest mentions of music therapy was in Al-Farabi's (c. 872–950) treatise "Meanings of the Intellect", which described the therapeutic effects of music on the soul. Music has long been used to help people deal with their emotions. In the 17th century, the scholar Robert Burton's "The Anatomy of Melancholy" argued that music and dance were critical in treating mental illness, especially melancholia. He noted that music has an "excellent power ...to expel many other diseases" and he called it "a sovereign remedy against despair and melancholy." He pointed out that in Antiquity, Canus, a Rhodian fiddler, used music to "make a melancholy man merry, ...a lover more enamoured, a religious man more devout." In the Ottoman Empire, mental illnesses were treated with music. In November 2006, Dr. Michael J. Crawford and his colleagues also found that music therapy helped schizophrenic patients.

Albert Einstein had a lifelong love of music (particularly the works of Bach and Mozart), once stating that life without playing music would be inconceivable to him. In some interviews Einstein even attributed much of his scientific intuition to music, with his son Hans recounting that "whenever he felt that he had come to the end of the road or into a difficult situation in his work, he would take refuge in music, and that would usually resolve all his difficulties." Something in the music, according to Michele and Robert Root-Bernstein in "Psychology Today", "would guide his thoughts in new and creative directions." It has been said that Einstein considered Mozart's music to reveal a universal harmony that Einstein believed existed in the universe, "as if the great Wolfgang Amadeus did not 'create' his beautifully clear music at all, but simply discovered it already made. This perspective parallels, remarkably, Einstein’s views on the ultimate simplicity of nature and its explanation and statement via essentially simple mathematical expressions." A review suggests that music may be effective for improving subjective sleep quality in adults with insomnia symptoms. Music is also being used in clinical rehabilitation of cognitive and motor disorders.





</doc>
<doc id="18842" url="https://en.wikipedia.org/wiki?curid=18842" title="Mode">
Mode

Mode ( meaning "manner, tune, measure, due measure, rhythm, melody") may refer to:











</doc>
<doc id="18845" url="https://en.wikipedia.org/wiki?curid=18845" title="Mouse">
Mouse

A mouse, plural mice, is a small rodent characteristically having a pointed snout, small rounded ears, a body-length scaly tail, and a high breeding rate. The best known mouse species is the common house mouse ("Mus musculus"). It is also a popular pet. In some places, certain kinds of field mice are locally common. They are known to invade homes for food and shelter.

Species of mice are mostly classified in Rodentia, and are present throughout the order. Typical mice are classified in the genus Mus.

Mice are typically distinguished from rats by their size. Generally, when someone discovers a smaller muroid rodent, its common name includes the term "mouse", while if it is larger, the name includes the term "rat". Common terms "rat" and "mouse" are not taxonomically specific. Scientifically, the term "mouse" is not confined to members of "Mus" for example, but also applies to species from other genera such as the deer mouse, "Peromyscus".

Domestic mice sold as pets often differ substantially in size from the common house mouse. This is attributable both to breeding and to different conditions in the wild. The best-known strain, the white lab mouse, has more uniform traits that are appropriate to its use in research.

Cats, wild dogs, foxes, birds of prey, snakes and even certain kinds of arthropods have been known to prey heavily upon mice. Nevertheless, because of its remarkable adaptability to almost any environment, the mouse is one of the most successful mammalian genera living on Earth today.

Mice, in certain contexts, can be considered vermin which are a major source of crop damage, causing structural damage and spreading diseases through their parasites and feces. In North America, breathing dust that has come in contact with mouse excrement has been linked to hantavirus, which may lead to hantavirus pulmonary syndrome (HPS).

Primarily nocturnal animals, mice compensate for their poor eyesight with a keen sense of hearing, and rely especially on their sense of smell to locate food and avoid predators.

Mice build long intricate burrows in the wild. These typically have long entrances and are equipped with escape tunnels or routes. In at least one species, the architectural design of a burrow is a genetic trait.

The most common mice are murines, in the same clade as common rats. They are murids, along with gerbils and other close relatives. 

Mice are common experimental animals in laboratory research of biology and psychology fields primarily because they are mammals, and also because they share a high degree of homology with humans. They are the most commonly used mammalian model organism, more common than rats. The mouse genome has been sequenced, and virtually all mouse genes have human homologs. The mouse has approximately 2.7 billion base pairs and 20 pairs of chromosomes.
They can also be manipulated in ways that are illegal with humans, although animal rights activists often object. A knockout mouse is a genetically modified mouse that has had one or more of its genes made inoperable through a gene knockout.

Reasons for common selection of mice are that they are small and inexpensive, have a widely varied diet, are easily maintained, and can reproduce quickly. Several generations of mice can be observed in a relatively short time. Mice are generally very docile if raised from birth and given sufficient human contact. However, certain strains have been known to be quite temperamental. Mice and rats have the same organs in the same places, with the difference of size.

Researchers at the Max Planck Institute of Neurobiology have confirmed that mice have a range of facial expressions. They used machine vision to spot familiar human emotions like pleasure, disgust, nausea, pain, and fear.

Many people buy mice as companion pets. They can be playful, loving and can grow used to being handled. Like pet rats, pet mice should not be left unsupervised outside as they have many natural predators, including (but not limited to) birds, snakes, lizards, cats, and dogs. Male mice tend to have a stronger odor than the females. However, mice are careful groomers and as pets they never need bathing. Well looked-after mice can make ideal pets. Some common mouse care products are:

In nature, mice are largely herbivores, consuming any kind of fruit or grain from plants. However, mice adapt well to urban areas and are known for eating almost all types of food scraps. In captivity, mice are commonly fed commercial pelleted mouse diet. These diets are nutritionally complete, but they still need a large variety of vegetables.

Mice do not have a special appetite for cheese. They will only eat cheese for lack of better options.

Mice are a staple in the diet of many small carnivores. Humans have eaten mice since prehistoric times. In Victorian Britain, fried mice were still given to children as a folk remedy for bed-wetting; while Jared Diamond reports creamed mice being used in England as a dietary supplement during WW II rationing. Mice are a delicacy throughout eastern Zambia and northern Malawi, where they are a seasonal source of protein. Field rat is a popular food in Vietnam and neighboring countries. In many countries, however, mouse is no longer a food item.

Prescribed cures in Ancient Egypt included mice as medicine. In Ancient Egypt, when infants were ill, mice were eaten as treatment by their mothers. It was believed that mouse eating by the mother would help heal the baby who was ill.

In various countries mice are used as food for pets such as snakes, lizards, frogs, tarantulas and birds of prey, and many pet stores carry mice for this purpose.

Common terms used to refer to different ages/sizes of mice when sold for pet food are "pinkies", "fuzzies", "crawlers", "hoppers", and "adults". Pinkies are newborn mice that have not yet grown fur; fuzzies have some fur but are not very mobile; hoppers have a full coat of hair and are fully mobile but are smaller than adult mice. Mice without fur are easier for the animal to consume; however, mice with fur may be more convincing as animal feed. These terms are also used to refer to the various growth stages of rats (see Fancy rat).




</doc>
<doc id="18847" url="https://en.wikipedia.org/wiki?curid=18847" title="Multics">
Multics

Multics ("Multiplexed Information and Computing Service") was an influential early time-sharing operating system based on the concept of a single-level memory. Multics "has influenced all modern operating systems since, from microcomputers to mainframes."

Initial planning and development for Multics started in 1964, in Cambridge, Massachusetts. Originally it was a cooperative project led by MIT (Project MAC with Fernando Corbató) along with General Electric and Bell Labs. It was developed on the GE 645 computer, which was specially designed for it; the first one was delivered to MIT in January, 1967.

Multics was conceived as a commercial product for General Electric, and became one for Honeywell, albeit not very successfully. Due to its many novel and valuable ideas, Multics had a significant impact on computer science despite its faults.

Multics had numerous features intended to ensure high availability so that it would support a computing utility similar to the telephone and electricity utilities. Modular hardware structure and software architecture were used to achieve this. The system could grow in size by simply adding more of the appropriate resource, be it computing power, main memory, or disk storage. Separate access control lists on every file provided flexible information sharing, but complete privacy when needed. Multics had a number of standard mechanisms to allow engineers to analyze the performance of the system, as well as a number of adaptive performance optimization mechanisms.

Multics implemented a single-level store for data access, discarding the clear distinction between files (called "segments" in Multics) and "process memory". The memory of a process consisted solely of segments that were mapped into its address space. To read or write to them, the process simply used normal central processing unit (CPU) instructions, and the operating system took care of making sure that all the modifications were saved to disk. In POSIX terminology, it was as if every file were codice_1ed; however, in Multics there was no concept of "process memory", separate from the memory used to hold mapped-in files, as Unix has. "All" memory in the system was part of "some" segment, which appeared in the file system; this included the temporary scratch memory of the process, its kernel stack, etc.

One disadvantage of this was that the size of segments was limited to 256 kilowords, just over 1 MiB. This was due to the particular hardware architecture of the machines on which Multics ran, having a 36-bit word size and index registers (used to address within segments) of half that size (18 bits). Extra code had to be used to work on files larger than this, called multisegment files. In the days when one megabyte of memory was prohibitively expensive, and before large databases and later huge bitmap graphics, this limit was rarely encountered.

Another major new idea of Multics was dynamic linking, in which a running process could request that other segments be added to its address space, segments which could contain code that it could then execute. This allowed applications to automatically use the latest version of any external routine they called, since those routines were kept in other segments, which were dynamically linked only when a process first tried to begin execution in them. Since different processes could use different search rules, different users could end up using different versions of external routines automatically. Equally importantly, with the appropriate settings on the Multics security facilities, the code in the other segment could then gain access to data structures maintained in a different process.

Thus, to interact with an application running in part as a daemon (in another process), a user's process simply performed a normal procedure-call instruction to a code segment to which it had dynamically linked (a code segment that implemented some operation associated with the daemon). The code in that segment could then modify data maintained and used in the daemon. When the action necessary to commence the request was completed, a simple procedure return instruction returned control of the user's process to the user's code.

Multics also supported extremely aggressive on-line reconfiguration: central processing units, memory banks, disk drives, etc. could be added and removed while the system continued operating. At the MIT system, where most early software development was done, it was common practice to split the multiprocessor system into two separate systems during off-hours by incrementally removing enough components to form a second working system, leaving the rest still running the original logged-in users. System software development testing could be done on the second system, then the components of the second system were added back to the main user system, without ever having shut it down. Multics supported multiple CPUs; it was one of the earliest multiprocessor systems.

Multics was the first major operating system to be designed as a secure system from the outset. Despite this, early versions of Multics were broken into repeatedly. This led to further work that made the system much more secure and prefigured modern security engineering techniques. Break-ins became very rare once the second-generation hardware base was adopted; it had hardware support for ring-oriented security, a multilevel refinement of the concept of master mode.

Multics was the first operating system to provide a hierarchical file system, and file names could be of almost arbitrary length and syntax. A given file or directory could have multiple names (typically a long and short form), and symbolic links between directories were also supported. Multics was the first to use the now-standard concept of per-process stacks in the kernel, with a separate stack for each security ring. It was also the first to have a command processor implemented as ordinary user code – an idea later used in the Unix shell. It was also one of the first written in a high-level language (Multics PL/I), after the Burroughs MCP system written in ALGOL.

In 1964, Multics was developed initially for the GE-645 mainframe, a 36-bit system. GE's computer business, including Multics, was taken over by Honeywell in 1970; around 1973, Multics was supported on the Honeywell 6180 machines, which included security improvements including hardware support for protection rings.

Bell Labs pulled out of the project in 1969; some of the people who had worked on it there went on to create the Unix system. Multics development continued at MIT and General Electric.

Honeywell continued system development until 1985. About 80 multimillion-dollar sites were installed, at universities, industry, and government sites. The French university system had several installations in the early 1980s. After Honeywell stopped supporting Multics, users migrated to other systems like Unix.

In 1985, Multics was issued certification as a B2 level secure operating system using the Trusted Computer System Evaluation Criteria from the National Computer Security Center (NCSC) a division of the NSA, the first operating system evaluated to this level.

Multics was distributed from 1975 to 2000 by Groupe Bull in Europe, and by Bull HN Information Systems Inc. in the United States. In 2006, Bull SAS open sourced Multics versions MR10.2, MR11.0, MR12.0, MR12.1, MR12.2, MR12.3, MR12.4 & MR12.5.

The last known Multics installation running natively on Honeywell hardware was shut down on October 30, 2000, at the Canadian Department of National Defence in Halifax, Nova Scotia, Canada.

In 2006 Bull HN released the source code for MR12.5, the final 1992 Multics release, to MIT. Most of the system is now available as open-source software with the exception of some optional pieces such as TCP/IP.

In 2014 Multics was successfully run on current hardware using a emulator. The 1.0 release of the emulator is now available. Release 12.6f of Multics accompanies the 1.0 release of the emulator, and adds a few new features, including command line recall and editing using the video system.

The following is a list of commands for common computing tasks that are supported by the Multics command-line interface.
Peter H. Salus, author of a book covering Unix's early years, stated one position: "With Multics they tried to have a much more versatile and flexible operating system, and it failed miserably". This position, however, has been widely discredited in the computing community because many of Multics' technical innovations are used in modern commercial computing systems.

The permanently resident kernel of Multics, a system derided in its day as being too large and complex, was only 135 KB of code. In comparison, a Linux system in 2007 might have occupied 18 MB. The first MIT GE-645 had 512 kilowords of memory (2 MiB), a truly enormous amount at the time, and the kernel used only a moderate portion of Multics main memory.

The entire system, including the operating system and the complex PL/1 compiler, user commands, and subroutine libraries, consisted of about 1500 source modules. These averaged roughly 200 lines of source code each, and compiled to produce a total of roughly 4.5 MiB of procedure code, which was fairly large by the standards of the day.

Multics compilers generally optimised more for code density than CPU performance, for example using small sub-routines called "operators" for short standard code sequences, which makes comparison of object code size with modern systems less useful. High code density was a good optimisation choice for Multics as a multi-user system with expensive main memory.

The design and features of Multics greatly influenced the Unix operating system, which was originally written by two Multics programmers, Ken Thompson and Dennis Ritchie. Superficial influence of Multics on Unix is evident in many areas, including the naming of some commands. But the internal design philosophy was quite different, focusing on keeping the system small and simple, and so correcting some deficiencies of Multics because of its high resource demands on the limited computer hardware of the time.

The name "Unix" (originally "Unics") is itself a pun on "Multics". The "U" in Unix is rumored to stand for "uniplexed" as opposed to the "multiplexed" of Multics, further underscoring the designers' rejections of Multics' complexity in favor of a more straightforward and workable approach for smaller computers. (Garfinkel and Abelson cite an alternative origin: Peter Neumann at Bell Labs, watching a demonstration of the prototype, suggested the pun name UNICS – pronounced "eunuchs" – as a "castrated Multics", although Dennis Ritchie is said to have denied this.)

Ken Thompson, in a transcribed 2007 interview with Peter Seibel refers to Multics as "overdesigned and overbuilt and over everything. It was close to unusable. They [Massachusetts Institute of Technology] still claim it's a monstrous success, but it just clearly wasn't". On the influence of Multics on Unix, Thompson stated that "the things that I liked enough (about Multics) to actually take were the hierarchical file system and the shell — a separate process that you can replace with some other process".

The Prime Computer operating system, PRIMOS, was referred to as "Multics in a shoebox" by William Poduska, a founder of the company. Poduska later moved on to found Apollo Computer, whose AEGIS and later Domain/OS operating systems, sometimes called "Multics in a matchbox", extended the Multics design to a heavily networked graphics workstation environment.

The Stratus VOS operating system of Stratus Computer (now Stratus Technologies) was very strongly influenced by Multics, and both its external user interface and internal structure bear many close resemblances to the older project. The high-reliability, availability, and security features of Multics were extended in Stratus VOS to support a new line of fault tolerant computer systems supporting secure, reliable transaction processing. Stratus VOS is the most directly-related descendant of Multics still in active development and production usage today.

The protection architecture of Multics, restricting the ability of code at one level of the system to access resources at another, was adopted as the basis for the security features of ICL's VME operating system.


The literature contains a large number of papers about Multics, and various components of it; a fairly complete list is available at the Multics Bibliography page. The most important and/or informative ones are listed below.





</doc>
<doc id="18849" url="https://en.wikipedia.org/wiki?curid=18849" title="Marxist film theory">
Marxist film theory

Marxist film theory is one of the oldest forms of film theory. 

Sergei Eisenstein and many other Soviet filmmakers in the 1920s expressed ideas of Marxism through film. In fact, the Hegelian dialectic was considered best displayed in film editing through the Kuleshov Experiment and the development of montage.

While this structuralist approach to Marxism and filmmaking was used, the more vociferous complaint that the Russian filmmakers had was with the narrative structure of the cinema of the United States.

Eisenstein's solution was to shun narrative structure by eliminating the individual protagonist and tell stories where the action is moved by the group and the story is told through a clash of one image against the next (whether in composition, motion, or idea) so that the audience is never lulled into believing that they are watching something that has not been worked over.

Eisenstein himself, however, was accused by the Soviet authorities under Joseph Stalin of "formalist error", of highlighting form as a thing of beauty instead of portraying the worker nobly.

French Marxist film makers, such as Jean-Luc Godard, would employ radical editing and choice of subject matter as well as subversive parody to heighten class consciousness and promote Marxist ideas.

Situationist film maker Guy Debord, author of "The Society of the Spectacle", began his film "In girum imus nocte et consumimur igni" [Wandering around in the night we are consumed by fire] with a radical critique of the spectator who goes to the cinema to forget about their dispossessed daily life.

Situationist film makers produced a number of important films, where the only contribution by the situationist film cooperative was the sound-track. In "Can dialectics break bricks?" (1973), a Chinese Kung Fu film was transformed by redubbing into an epistle on state capitalism and Proletarian revolution. The intellectual technique of using capitalism's own structures against itself is known as détournement. 

Marxist film theory has developed from these precise and historical beginnings and is now sometimes viewed in a wider way to refer to any power relationships or structures within a moving image text.




</doc>
<doc id="18851" url="https://en.wikipedia.org/wiki?curid=18851" title="Mars (disambiguation)">
Mars (disambiguation)

Mars is a planet in the Solar System.

Mars also commonly refers to:

Mars may also refer to:























</doc>
<doc id="18852" url="https://en.wikipedia.org/wiki?curid=18852" title="Morpheme">
Morpheme

A morpheme is the smallest meaningful unit in a language. A morpheme is not identical to a word. The main difference between them is that a morpheme sometimes does not stand alone, but a word, by definition, always stands alone. The linguistics field of study dedicated to morphemes is called morphology. When a morpheme stands by itself, it is considered as a root because it has a meaning of its own (such as the morpheme "cat"). When it depends on another morpheme to express an idea, it is an affix because it has a grammatical function (such as the "–s" in "cats" to indicate that it is plural). Every word is composed of one or more morphemes.

Every morpheme can be classified as either free or bound. Since the categories are mutually exclusive, a given morpheme will belong to exactly one of them.

Bound morphemes can be further classified as derivational or inflectional morphemes. The main difference between derivational morphemes and inflectional morphemes is their function for words.



Allomorphs are variants of a morpheme that differ in pronunciation but are semantically identical. For example, the English plural marker "-(e)s" of regular nouns can be pronounced ("bats"), , ("bugs"), or , ("buses"), depending on the final sound of the noun's plural form.

Generally, these types of morphemes have no visible changes. For instance, "sheep" is both the singular and the plural form. The intended meaning is thus derived from the Co-occurrence determiner (in this case, "some-" or "a-").

Content morphemes express a concrete meaning or "content", and function morphemes have more of a grammatical role. For example, the morphemes "fast" and "sad" can be considered content morphemes. On the other hand, the suffix "-ed" is a function morpheme since it has the grammatical function of indicating past tense.

Both categories may seem very clear and intuitive, but the idea behind them is occasionally harder to grasp since they overlap with each other. Examples of ambiguous situations are the preposition "over" and the determiner "your", which seem to have concrete meanings but are considered function morphemes since their role is to connect ideas grammatically. Here is a general rule to determine the category of a morpheme:


Roots are composed of only one morpheme, while stems can be composed of more than one morpheme. Any additional affixes are considered morphemes. For example, in the word "quirkiness", the root is "quirk", but the stem is "quirky", which has two morphemes.

Moreover, some pairs of affixes have the same phonological form but have a different meaning. For example, the suffix "–er" can be either derivative (e.g. "sell" ⇒ "seller") or inflectional (e.g. "small" ⇒ "smaller"). Such morphemes are called homophonous.

Some words might seem to be composed of multiple morphemes but are not. Therefore, not only form but also meaning must be considered when identifying morphemes. For example, the word "relate" might seem to be composed of two morphemes, "re-" (prefix) and the word "late", but it is not. Those morphemes have no relationship with the definitions relevant to the word like "to feel sympathy," "to narrate," or "to be connected by blood or marriage."

Furthermore, the length of a word does not determine whether or not it has multiple morphemes. The word "Madagascar" is long and might seem to have morphemes like "mad", "gas", and "car", but it does not. Conversely, some short words have multiple morphemes (e.g. "dogs" = "dog" + "s").

Morphological icons are images, patterns or symbols that relate to a specific morpheme. For children with dyslexia, it has been shown to be an effective way of building up a word. The word 'inviting' as an example is made up of two commonly used morphemes, 'in-' and '-ing'. A morphological icon for 'in-' could be an arrow going into a cup, and '-ing' could be an arrow going forward to symbolise that something is in action (as in "being, running, fishing").

The concept of combining visual aid icons with morpheme teaching methods was pioneered from the mid 1980s by Neville Brown. He founded the Maple Hayes school for dyslexia in 1981, where he later improved the method alongside his son, Daryl Brown. The school's curriculum uses morphological icons as a learning aid. 

In natural language processing for Japanese, Chinese, and other languages, morphological analysis is the process of segmenting a sentence into a row of morphemes. Morphological analysis is closely related to part-of-speech tagging, but word segmentation is required for these languages because word boundaries are not indicated by blank spaces.

The purpose of morphological analysis is to determine the minimal units of meaning in a language or morphemes by using comparisons of similar forms: for example, comparing forms such as "She is walking" and "They are walking," rather than comparing either with something completely different like "You are reading." Thus, the forms can be effectively broken down into parts and the different morphemes can be distinguished. 

Similarly, both meaning and form are equally important for the identification of morphemes. For instance, an agent morpheme is an affix like "-er" that transforms a verb into a noun (e.g. "teach" → "teacher"). On the other hand, "–er" can also be a comparative morpheme that changes an adjective into another degree of the same adjective (eg.. "small" → "smaller"). Although the form is the same, the meaning of both morphemes is different. Also, the opposite can occur, with the meaning being the same but the form being different.

In generative grammar, the definition of a morpheme depends heavily on whether syntactic trees have morphemes as leaves or features as leaves.

Given the definition of a morpheme as "the smallest meaningful unit," nanosyntax aims to account for idioms in which an entire syntactic tree often contributes "the smallest meaningful unit." An example idiom is "Don't let the cat out of the bag." Here, the idiom is composed of "let the cat out of the bag." This might be considered a semantic morpheme that is itself composed of many syntactic morphemes. Other cases of the "smallest meaningful unit" being longer than a word include some collocations such as "in view of" and "business intelligence", in which the words together have a specific meaning.

The definition of morphemes also plays a significant role in the interfaces of generative grammar in the following theoretical constructs:





</doc>
<doc id="18856" url="https://en.wikipedia.org/wiki?curid=18856" title="MTV">
MTV

MTV (originally an initialism of Music Television) is an American multinational cable channel, launched on August 1, 1981. Based in New York City, it serves as the flagship property of the ViacomCBS Domestic Media Networks division of ViacomCBS, also headquartered in New York City.

MTV was originally first tested on December 1, 1977 as Sight on Sound, but was officially launched in 1981, and originally aired music videos as guided by television personalities known as "video jockeys" (VJs), but in the years since its inception, the network significantly toned down its focus on music in favor of original reality programming targeting teenagers and young adults.

MTV has spawned numerous sister channels in the U.S. and affiliated channels internationally, some of which have gone independent, with approximately 90.6 million American households in the United States receiving the channel as of January 2016.

Several earlier concepts for music video-based television programming had been around since the early 1960s. The Beatles had used music videos to promote their records starting in the mid-1960s. The creative use of music videos within their 1964 film "A Hard Day's Night," particularly the performance of the song "Can't Buy Me Love", led MTV later to honor the film's director Richard Lester with an award for "basically inventing the music video".

In his book "The Mason Williams FCC Rapport", author Mason Williams states that he pitched an idea to CBS for a television program that featured "video-radio", where disc jockeys would play avant-garde art pieces set to music. CBS rejected the idea, but Williams premiered his own musical composition "Classical Gas" on the "Smothers Brothers Comedy Hour", where he was head writer. In 1970, Philadelphia-based disc jockey Bob Whitney created "The Now Explosion", a television series filmed in Atlanta and broadcast in syndication to other local television stations throughout the United States. The series featured promotional clips from various popular artists, but was canceled by its distributor in 1971. Several music programs originating outside of the US, including Australia's "Countdown" and the United Kingdom's "Top of the Pops", which had initially aired music videos in lieu of performances from artists who were not available to perform live, began to feature them regularly by the mid-1970s.

In 1974, Gary Van Haas, vice president of Televak Corporation, introduced a concept to distribute a music video channel to record stores across the United States, and promoted the channel, named Music Video TV, to distributors and retailers in a May 1974 issue of "Billboard". The channel, which featured video disc jockeys, signed a deal with US Cable in 1978 to expand its audience from retail to cable television. The service was no longer active by the time MTV launched in 1981.

In 1977, Warner Cable, a division of Warner Communications and the precursor of Warner-Amex Satellite Entertainment launched the first two-way interactive cable television system named QUBE in Columbus, Ohio. The QUBE system offered many specialized channels. One of these specialized channels was "Sight on Sound", a music channel that featured concert footage and music-oriented television programs. With the interactive QUBE service, viewers could vote for their favorite songs and artists. MTV was not the only channel run by Warner-Amex to be first tested before its launch; sister channel Nickelodeon was also first tested as Channel C-3, which only broadcasts the series "Pinwheel", before the channel was officially launched in 1979.

The original programming format of MTV was created by media executive Robert W. Pittman, who later became president and chief executive officer (CEO) of MTV Networks. Pittman had test-driven the music format by producing and hosting a 15-minute show, "Album Tracks", on New York City television station WNBC-TV in the late 1970s.

Pittman's boss Warner-Amex executive vice president John Lack had shepherded "PopClips", a television series created by former Monkee-turned solo artist Michael Nesmith, whose attention had turned to the music video format in the late 1970s. The inspiration for "PopClips" came from a similar program on New Zealand's TVNZ network named "Radio with Pictures", which premiered in 1976. The concept itself had been in the works since 1966, when major record companies began supplying the creator of New Zealand Broadcasting Corporation with promotional music clips to play on the air at no charge. Few artists made the long trip to New Zealand to appear live.

On Saturday, August 1, 1981, at 12:01 AMEastern Time, MTV was officially launched with the words "Ladies and gentlemen, rock and roll," spoken by John Lack and played over footage of the first Space Shuttle launch countdown of "Columbia" (which took place earlier that year) and of the launch of Apollo 11. Those words were immediately followed by the original MTV theme song, a crushing rock tune composed by Jonathan Elias and John Petersen, playing over the American flag changed to show MTV's logo changing into different textures and designs. MTV producers Alan Goodman and Fred Seibert used this public domain footage as a concept; Seibert said that they had originally planned to use Neil Armstrong's "One small step" quote, but lawyers said that Armstrong owned his name and likeness and that he had refused, so the quote was replaced with a beeping sound. A shortened version of the shuttle launch ID ran at the top of every hour in different forms, from MTV's first day until it was pulled in early 1986 in the wake of the "Challenger" disaster.

The first music video shown on MTV was The Buggles' "Video Killed the Radio Star", originally available only to homes in New Jersey. This was followed by the video for Pat Benatar's "You Better Run". The screen went black infrequently when an employee at MTV inserted a tape into a VCR. MTV's lower third graphics that appeared near the beginning and end of music videos eventually used the recognizable Kabel typeface for about 25 years, but these graphics varied on MTV's first day of broadcast; they were set in a different typeface and included information such as the year and record label name.

As programming chief, Robert W. Pittman recruited and managed a team for the launch that included Tom Freston (who succeeded Pittman as CEO of MTV Networks), Fred Seibert, John Sykes, Carolyn Baker (original head of talent and acquisition), Marshall Cohen (original head of research), Gail Sparrow (of talent and acquisition), Sue Steinberg (executive producer), Julian Goldberg, Steve Lawrence, Geoff Bolton; studio producers and MTV News writers/associate producers Liz Nealon, Nancy LaPook and Robin Zorn; Steve Casey (creator of the name "MTV" and its first program director), Marcy Brafman, Ronald E. "Buzz" Brindle, and Robert Morton. Kenneth M. Miller is credited as being the first technical director to officially launch MTV from its New York City-based network operations facility.

MTV's effect was immediate in areas where the new music video channel was carried. Within two months, record stores in areas where MTV was available were selling music that local radio stations were not playing, such as Men at Work, Bow Wow Wow and the Human League. MTV sparked the Second British Invasion, with UK acts, who had been accustomed to using music videos for half a decade (some of which appeared on the BBC's "Top of the Pops"), featuring heavily on the channel.

MTV targeted an audience between the ages of 12 to 34. However, according to MTV's self conducted research over 50% of its audience was between 12 and 24. Furthermore, this particular group watched MTV for an average of thirty minutes to two hours a day.

The original purpose of MTV was to be "music television", playing music videos 24 hours a day and seven days a week, guided by on-air personalities known as VJs, or video jockeys. The original slogans of the channel were "You'll never look at music the same way again", and "On cable. In stereo."

MTV's earliest format was modeled after AOR (album-oriented rock) radio; MTV underwent a transition to copy a full Top 40 station in 1984. Fresh-faced young men and women were hired to host the channel's programming and to introduce music videos that were being played. The term VJ was coined, which was a play on the initialism DJ (disc jockey). Many VJs eventually became celebrities in their own right. The original five MTV VJs in 1981 were Nina Blackwood, Mark Goodman, Alan Hunter, J.J. Jackson and Martha Quinn. Initially popular New York DJ Meg Griffin was going to be a VJ, but decided against it at the last minute. The VJs were hired to fit certain demographics the channel was trying to obtain (Goodman was the affable every man, Hunter the popular jock, Jackson the hip radio veteran, Blackwood the bombshell vixen and Quinn the girl next door). Due to the uncertainty of how successful the channel would be, the VJs were told not to buy permanent residencies and to keep their second jobs.

The VJs recorded intro and outro voiceovers before broadcast, along with music news, interviews, concert dates and promotions. These segments appeared to air live and debut across the MTV program schedule 24 hours a day and seven days a week, although the segments themselves were pre-taped within a regular work week at MTV's studios.

The earlier music videos that made up the volume of MTV's programming in the 1980s were promotional videos (or "promos", a term that originated in the United Kingdom) that record companies had commissioned for international use or concert clips from any available sources.

Rock bands and performers of the 1980s who had airplay on MTV ranged from new wave to hard rock or heavy metal bands such as Adam Ant, Bryan Adams, The Pretenders, Blondie, Eurythmics, Tom Petty and the Heartbreakers, Culture Club, Mötley Crüe, Split Enz, Prince, Ultravox, Duran Duran, Van Halen, Bon Jovi, Ratt, Def Leppard, Metallica, Guns N' Roses, The Police, and The Cars. The channel also rotated the music videos of "Weird Al" Yankovic, who made a career out of parodying other artists' videos. MTV also aired several specials by "Weird Al" in the 1980s and 1990s under the title "Al TV".

MTV also played classic rock acts from the 1980s and earlier decades, including David Bowie, Dire Straits (whose 1985 song and video "Money for Nothing" both referenced MTV and also included the slogan "I want my MTV" in its lyrics), Journey, Rush, Linda Ronstadt, Genesis, Billy Squier, Aerosmith, The Rolling Stones, Eric Clapton, The Moody Blues, John Mellencamp, Daryl Hall & John Oates, Billy Joel, Robert Palmer, Rod Stewart, The Who, Peter Gabriel, and ZZ Top; newly solo acts such as Robert Plant, Phil Collins, Paul McCartney, David Lee Roth, and Pete Townshend; supergroup acts such as Asia, The Power Station, Yes, The Firm, and Traveling Wilburys, as well as forgotten acts such as Michael Stanley Band, Shoes, Blotto, Ph.D., Rockpile, Bootcamp, Silicon Teens and Taxxi. The hard rock band Kiss publicly appeared without their trademark makeup for the first time on MTV in 1983. The first country-music video aired on MTV was "Angel of the Morning" by Juice Newton, which first aired on MTV's premiere date. (Newton's video was the third video by a solo female artist to air on MTV, after Pat Benatar and Carly Simon.)

During the earlier days of the channel, MTV occasionally let other stars take over the channel within an hour as "guest VJs". These guests included musicians such as Adam Ant, Billy Idol, Phil Collins, Simon LeBon and Nick Rhodes of Duran Duran, Tina Turner; and comedians such as Eddie Murphy, Martin Short, Dan Aykroyd, and Steven Wright; as they chose their favorite music videos.

The 1983 film "Flashdance" was the first film in which its promoters excerpted musical segments from it and supplied them to MTV as music videos, which the channel then aired in regular rotation.

In addition to bringing lesser-known artists into view, MTV was instrumental in adding to the booming '80s dance wave. Videos' budgets increased, and artists began to add fully choreographed dance sections. Michael Jackson's music became synonymous with dance. In addition to learning the lyrics, fans also learned his choreography so they could dance along. Madonna capitalized on dance in her videos, using classically trained jazz and breakdancers. Along with extensive costuming and makeup, Duran Duran used tribal elements, pulled from Dunham technique, in "The Wild Boys", and Kate Bush used a modern dance duet in "Running Up That Hill". MTV brought more than music into public view, it added to the ever-growing resurgence of dance in the early 1980s that has carried through to today.

In 1984, more record companies and artists began making video clips for their music than in the past, realizing the popularity of MTV and the growing medium. In keeping with the influx of videos, MTV announced changes to its playlists in the November 3, 1984, issue of "Billboard" magazine, that would take effect the following week. The playlist categories would be expanded to seven, from three (light, medium, heavy); including New, Light, Breakout, Medium, Active, Heavy and Power. This would ensure artists with hit records on the charts would be get the exposure they deserved, with Medium being a home for the established hits still on the climb up to the top 10; and Heavy being a home for the big hitswithout the bells and whistlesjust the exposure they commanded.

In 1985, MTV spearheaded a safe-sex initiative as a response to the AIDS epidemic that continues to influence sexual health currently. In this light, MTV pushed teens to pay more attention to safe-sex because they were most likely more willing to hear this message from MTV than their parents. This showed that MTV was not always influencing youth negatively. Even though in other aspects, MTV was provocative, they had this campaign to showcase their positive influence on youths and safe sexa campaign that still is alive today: "Its Your Sex Life".

During MTV's first few years on the air, very few black artists were included in rotation on the channel. The select few who were in MTV's rotation were Michael Jackson, Prince, Eddy Grant, Donna Summer, Joan Armatrading, Musical Youth, and Herbie Hancock. The very first people of color to perform on MTV was the British band The Specials, which featured an integrated line-up of white and black musicians and vocalists. The Specials' video "Rat Race" was played as the 58th video on the station's first day of broadcasting.

MTV refused other black artists' videos, such as Rick James' "Super Freak", because they did not fit the channel's carefully selected album-oriented rock format at the time. The exclusion enraged James; he publicly advocated the addition of more black artists' videos on the channel. Rock legend David Bowie also questioned MTV's lack of black artists during an on-air interview with VJ Mark Goodman in 1983. MTV's original head of talent and acquisition, Carolyn B. Baker, who was black, had questioned why the definition of music had to be so narrow, as had a few others outside the network. "The party line at MTV was that we weren't playing black music because of the 'research, said Baker years later. "But the research was based on ignorance ... we were young, we were cutting edge. We didn't have to be on the cutting edge of racism." Nevertheless, it was Baker who had personally rejected Rick James' video for "Super Freak" "because there were half-naked women in it, and it was a piece of crap. As a black woman, I did not want that representing my people as the first black video on MTV."

The network's director of music programming, Buzz Brindle, told an interviewer in 2006, "MTV was originally designed to be a rock music channel. It was difficult for MTV to find African American artists whose music fit the channel's format that leaned toward rock at the outset." Writers Craig Marks and Rob Tannenbaum noted that the channel "aired videos by plenty of white artists who didn't play rock." Andrew Goodwin later wrote, "[MTV] denied racism, on the grounds that it merely followed the rules of the rock business." MTV senior executive vice president Les Garland complained decades later, "The worst thing was that 'racism' bullshit... there were hardly any videos being made by black artists. Record companies weren't funding them. "They" never got charged with racism." However, critics of that defense pointed out that record companies were not funding videos for black artists because they knew that they would have difficulty persuading MTV to play them.

Before 1983, Michael Jackson also struggled to receive airtime on MTV. To resolve the struggle and finally "break the color barrier", the president of CBS Records at the time, Walter Yetnikoff, denounced MTV in a strong, profane statement, threatening to take away MTV's ability to play any of the record label's music videos. However, Les Garland, then acquisitions head, said he decided to air Jackson's "Billie Jean" video without pressure from CBS. This was contradicted by CBS head of Business Affairs David Benjamin in Vanity Fair.

According to "The Austin Chronicle", Jackson's video for the song "Billie Jean" was "the video that broke the color barrier, even though the channel itself was responsible for erecting that barrier in the first place." But change was not immediate. "Billie Jean" was not added to MTV's "medium rotation" playlist (two to three airings per day) until after it had already reached #1 on the "Billboard" Hot 100 chart. In the final week of March, it was in "heavy rotation", one week before the MTV debut of Jackson's "Beat It" video. Prince's "Little Red Corvette" joined both videos in heavy rotation at the end of April. At the beginning of June, "Electric Avenue" by Eddy Grant joined "Billie Jean", which was still in heavy rotation until mid-June. At the end of August, "She Works Hard for the Money" by Donna Summer was in heavy rotation on the channel. Herbie Hancock's "Rockit" and Lionel Richie's "All Night Long" was placed in heavy rotation at the end of October and the beginning of November respectively. In final week of November, Donna Summer's "Unconditional Love" was in heavy rotation. When Jackson's elaborate video for "Thriller" was released late in the year, which raised the ambition bar for what a video could be, the network's support for it was total; subsequently, more pop and R&B videos were played on MTV.

Following Jackson's and Prince's breakthroughs on MTV, Rick James did several interviews where he brushed off the accomplishment as tokenism, saying in a 1983 interview that was featured in an episode of "" on James, that "any black artist that [had] their video played on MTV should pull their [videos] off MTV."

Regardless of the timeline, many black artists had their videos played in "heavy" rotation the following year (1984). Along with Herbie Hancock, Prince, Donna Summer, other black artists such as Billy Ocean, Stevie Wonder, Tina Turner, Lionel Richie, Ray Parker Jr, Rockwell, The Pointer Sisters, The Jacksons, Sheila E and Deniece Williams all had videos played in heavy rotation on MTV.

Eventually, videos from the emerging genre of rap and hip hop also began to enter rotation on MTV. A majority of the rap artists appearing on MTV in the mid-1980s such as Run-DMC, The Fat Boys, Whodini, LL Cool J, and the Beastie Boys were from the East Coast.

In 1984, the channel produced its first "MTV Video Music Awards" show, or VMAs. The first award show, in 1984, was punctuated by a live performance by Madonna of "Like A Virgin". The statuettes that are handed out at the "Video Music Awards" are of the MTV moonman, the channel's original image from its first broadcast in 1981. Presently, the "Video Music Awards" are MTV's most watched annual event.

MTV began its annual "Spring Break" coverage in 1986, setting up temporary operations in Daytona Beach, Florida, for a week in March, broadcasting live eight hours per day. "Spring break is a youth culture event", MTV's vice president Doug Herzog said at the time. "We wanted to be part of it for that reason. It makes good sense for us to come down and go live from the center of it, because obviously the people there are the kinds of people who watch MTV." The channel's coverage featured numerous live performances from artists and bands on location. The annual tradition continued into the 2000s, when it became de-emphasized and handed off to mtvU, the spin-off channel of MTV targeted at college campuses.

The channel later expanded its beach-themed events to the summer, dedicating most of each summer season to broadcasting live from a beach house at different locations away from New York City, eventually leading to channel-wide branding throughout the summer in the 1990s and early 2000s such as "Motel California", "Summer Share", "Isle of MTV", "SoCal Summer", "Summer in the Keys", and "Shore Thing". MTV VJs hosted blocks of music videos, interview artists and bands, and introduced live performances and other programs from the beach house location each summer. In the 2000s, as the channel reduced its airtime for music videos and eliminated much of its in-house programming, its annual summer-long events came to an end.

MTV also held week-long music events that took over the presentation of the channel. Examples from the 1990s and 2000s include "All Access Week", a week in the summer dedicated to live concerts and festivals; "Spankin' New Music Week", a week in the fall dedicated to brand new music videos; and week-long specials that culminated in a particular live event, such as "Wanna be a VJ" and the "Video Music Awards".

At the end of each year, MTV takes advantage of its home location in New York City to broadcast live coverage on New Year's Eve in Times Square. Several live music performances are featured alongside interviews with artists and bands that were influential throughout the year. For many years from the 1980s to the 2000s, the channel upheld a tradition of having a band perform a cover song at midnight immediately following the beginning of the new year.

Throughout its history, MTV has covered global benefit concert series live. For most of July 13, 1985, MTV showed the Live Aid concerts, held in London and Philadelphia and organized by Bob Geldof and Midge Ure to raise funds for famine relief in Ethiopia. While the ABC network showed only selected highlights during primetime, MTV broadcast 16 hours of coverage.

Along with VH1, MTV broadcast the Live 8 concerts, a series of concerts set in the G8 states and South Africa, on July 2, 2005. Live 8 preceded the 31st G8 summit and the 20th anniversary of Live Aid. MTV drew heavy criticism for its coverage of Live 8. The network cut to commercials, VJ commentary, or other performances during performances. Complaints surfaced on the Internet over MTV interrupting the reunion of Pink Floyd. In response, MTV president Van Toeffler stated that he wanted to broadcast highlights from every venue of Live 8 on MTV and VH1, and clarified that network hosts talked over performances only in transition to commercials, informative segments or other musical performances. Toeffler acknowledged that "MTV should not have placed such a high priority on showing so many acts, at the expense of airing complete sets by key artists." He also blamed the Pink Floyd interruption on a mandatory cable affiliate break. MTV averaged 1.4million viewers for its original July 2 broadcast of Live 8. Consequently, MTV and VH1 aired five hours of uninterrupted Live 8 coverage on July 9, with each channel airing other blocks of artists.

MTV had debuted "Dial MTV" in 1986, a daily top 10 music video countdown show for which viewers could call the toll-free telephone number 1-800-DIAL-MTV to request a music video. The show was replaced by "MTV Most Wanted" in 1991, which ran until 1996, and later saw a spiritual successor in "Total Request Live". The phone number remained in use for video requests until 2006.

1986 also brought the departures of three of the five original VJs, as J.J. Jackson moved back to Los Angeles and returned to radio, while Nina Blackwood moved on to pursue new roles in television. Martha Quinn's contract wasn't renewed in late 1986 and she departed the network. She was brought back in early 1989 and stayed until 1992. Downtown Julie Brown was hired as the first new VJ as a replacement. In mid-1987, Alan Hunter and Mark Goodman ceased being full-time MTV veejays.

Also in 1986, the channel introduced "120 Minutes", a show that featured low-rotation, alternative rock and other "underground" videos for the next 14 years on MTV and three additional years on sister channel MTV2. The program then became known as "Subterranean" on MTV2. Eight years later, on July 31, 2011, "120 Minutes" was resurrected with Matt Pinfield taking over hosting duties once again and airing monthly on MTV2.

Another late night music video show was added in 1987, "Headbangers Ball", which featured heavy metal music and news. Before its abrupt cancellation in 1995, it featured several hosts including Riki Rachtman and Adam Curry. A weekly block of music videos with the name "Headbangers Ball" aired from 2003 to 2011 on sister channel MTV2, before spending an additional two years as a web-only series on MTV2's website, until "Headbangers Ball" was discontinued once again in 2013. Mark Goodman and Alan Hunter departed the network in 1987.

In 1988, MTV debuted "Yo! MTV Raps", a hip hop/rap formatted program. MTV progressively increased its airing of hit rappers by way of this program, such as MC Hammer, Vanilla Ice, LL Cool J, Queen Latifah, Salt-n-Pepa, Tone Loc, Naughty By Nature, MC Lyte, and Sir-Mix-A-Lot. The channel also played R&B artists such as Janet Jackson, New Edition, En Vogue, Bell Biv Devoe, SWV, Tony! Toni! Toné!, TLC, New Kids on the Block, and Boyz II Men. The program continued until August 1995; it was renamed to simply "Yo!" and aired as a one-hour program from 1995 to 1999. The concept was reintroduced as "Direct Effect" in 2000, which became "Sucker Free" in 2006 and was cancelled in 2008, after briefly celebrating the 20th anniversary of "Yo! MTV Raps" throughout the months of April and May 2008. Despite its cancellation on MTV, a weekly countdown of hip hop videos known as "Sucker Free" still airs on MTV2 through the present day.

In 1989, MTV began to premiere music-based specials such as "MTV Unplugged", an acoustic performance show, which has featured dozens of acts as its guests and has remained active in numerous iterations on various platforms for over 20 years.

To further cater to the growing success of R&B, MTV introduced the weekly "Fade to Black" in the summer of 1991, which was hosted by Al B. Sure!. The show was reformatted into the better known "MTV Jams" the following year, which incorporated mainstream hip-hop into the playlist. Bill Bellamy became the new and ongoing host. The show became so successful it spawned its own Most Wanted spinoff titled "Most Wanted Jams".

In 1985, Viacom bought Warner-Amex Satellite Entertainment, which owned MTV and Nickelodeon, renaming the company MTV Networks and beginning this expansion. Before 1987, MTV featured almost exclusively music videos, but as time passed, they introduced a variety of other shows, including some that were originally intended for other channels.

Non-music video programming began in the late 1980s, with the introduction of a music news show "The Week in Rock", which was also the beginning of MTV's news division, MTV News. Around this time, MTV also introduced a fashion news show, "House of Style"; a dance show, "Club MTV"; and a game show, "Remote Control". Programs like these did not feature music videos, but they were still largely based around the world of music.

Following the success of the "MTV Video Music Awards", in an effort to branch out from music into movies and broader pop culture, MTV started the "MTV Movie & TV Awards" in 1992, which continues presently. MTV also created an award show for Europe after the success of the "Video Music Awards". The "MTV Europe Music Awards", or the EMAs, were created in 1994, ten years after the debut of the VMAs.

These new shows were just the beginning of new genres of shows to make an impact on MTV. As the format of the network continued to evolve, more genres of shows began to appear.

Nirvana led a sweeping transition into the rise of alternative rock and grunge music on MTV in 1991, with their video for "Smells Like Teen Spirit". By late 1991 going into 1992, MTV began frequently airing videos from their heavily promoted "Buzz Bin", such as Nirvana, Pearl Jam, Alice in Chains, Soundgarden, Nine Inch Nails, Tori Amos, PM Dawn, Arrested Development, Björk, and Gin Blossoms. MTV increased rotation of its weekly alternative music program "120 Minutes" and added the daily "Alternative Nation" to play videos of these and other underground music acts. Subsequently, grunge and alternative rock had a rise in mainstream cultures, while 1980s-style glam bands and traditional rockers were phased out, with a few exceptions such as Aerosmith and Tom Petty. Older acts such as R.E.M. and U2 remained relevant by making their music more experimental or contemporary. MTV also played many hard rock acts such as Pantera, Death and other death/heavy metal acts at the time period, which saw rotation especially in "Headbangers Ball", a program equipped for the genre.

Over the next few years, more hit alternative rock acts were on heavy rotation, such as Stone Temple Pilots, Soul Asylum, Rage Against the Machine, Marilyn Manson, Tool, Beck, Therapy?, Radiohead, and The Smashing Pumpkins. Other hit acts such as Weezer, Collective Soul, Blind Melon, The Cranberries, Bush, and Silverchair followed in the next couple of years. Alternative bands that appeared on "Beavis and Butt-Head" included White Zombie. Also at this time, MTV began promoting new pop punk acts, most successfully Green Day and The Offspring, and ska punk acts such as No Doubt, The Mighty Mighty Bosstones, and Sublime. Alternative singers that were more pop-oriented were also added to the rotation with success, such as Alanis Morissette, Jewel, Fiona Apple, and Sarah McLachlan.

In the early-mid 1990s, MTV added gangsta rappers with a less pop-friendly sound to its rotation, such as Tupac Shakur, The Notorious B.I.G., the Wu-Tang Clan, Ice Cube, Warren G, Ice-T, Dr. Dre, Nas, and Snoop Doggy Dogg. In 1992, Snoop Dogg's G funk single "Nuthin' but a "G" Thang" became a crossover hit, with its humorous "House Party"-influenced video becoming an MTV staple despite the network's historic orientation towards rock music.

By 1997, MTV focused heavily on introducing electronica acts into the mainstream, adding them to its musical rotation, including The Prodigy, The Chemical Brothers, Moby, Aphex Twin, Pendulum, Daft Punk, The Crystal Method, and Fatboy Slim. Some musicians who proceeded to experiment with electronica were still played on MTV including Madonna, U2, David Bowie, Radiohead, and Smashing Pumpkins. That year, MTV also attempted to introduce neo-swing bands, but they were not met with much success.

To accompany the new sounds that were appearing on MTV, a new form of music videos came about: more artistic, experimental, and technically-accomplished than those of the 1980s. Several noted film directors got their start creating music videos. After pressure from the Music Video Production Association, MTV began listing the names of the videos' directors at the bottom of the credits by December 1992. As a result, MTV's viewers became familiar with the names of Spike Jonze, Michel Gondry, David Fincher, Mary Lambert, Samuel Bayer, Matt Mahurin, Mark Romanek, Jonathan Dayton and Valerie Faris, Anton Corbijn, Mark Pellington, Tarsem, Hype Williams, Jake Scott, Jonathan Glazer, Marcus Nispel, F. Gary Gray, Jim Yukich, Russell Mulcahy, Steve Barron, Marty Callner, and Michael Bay, among others.

As the PBS series "Frontline" explored, MTV was a driving force that catapulted music videos to a mainstream audience, turning music videos into an art form as well as a marketing machine that became beneficial to artists. Danny Goldberg, chairman and CEO of Artemis Records, said the following about the art of music videos: "I know when I worked with Nirvana, Kurt Cobain cared as much about the videos as he did about the records. He wrote the scripts for them, he was in the editing room, and they were part of his art. And I think they stand up as part of his art, and I think that's true of the great artists today. Not every artist is a great artist and not every video is a good video, but in general having it available as a tool, to me, adds to the business. And I wish there had been music videos in the heyday of The Beatles, and The Rolling Stones. I think they would've added to their creative contribution, not subtracted from it."

In the early-mid 1990s, MTV debuted its first reality shows, "Real World" and "Road Rules". Building on the success of "The Real World" and "Road Rules", MTV placed a stronger focus on reality shows and related series at this time. The first round of these shows came with game shows such as "Singled Out", reality-based comedy shows such as "Buzzkill", and late-night talk shows such as "The Jon Stewart Show" and "Loveline".

In a continuing bid to become a more diverse network focusing on youth and culture as well as music, MTV added animated shows to its lineup in the early 1990s. The animation showcase "Liquid Television" (a co-production between BBC and MTV produced in San Francisco by Colossal Pictures) was one of the channel's first programs to focus on the medium. In addition to airing original shows created specifically for MTV, the channel also occasionally aired episodes of original cartoon series produced by sister channel Nickelodeon ("Nicktoons") in the early 1990s.

MTV has a history of cartoons with mature themes including "Beavis and Butt-Head", "Æon Flux", and "The Brothers Grunt". Although the channel has gone on to debut many other animated shows, few of MTV's other cartoon series have been renewed for additional seasons, regardless of their reception.

MTV has a long history of airing both comedy and drama programs with scripted or improvised premises. Examples from the early-mid 1990s include sketch-based comedies such as "Just Say Julie", "The Ben Stiller Show", "The State", and "The Jenny McCarthy Show".

In late 1997, MTV began shifting more progressively towards teen pop music, inspired by the overseas success of the Spice Girls, the Backstreet Boys, and NSYNC in Europe. Between 1998 and 1999, MTV's musical content composed heavily of videos of boy bands such as the Backstreet Boys and NSYNC, girl groups such as the Spice Girls, and teen pop "princesses" such as Britney Spears, Christina Aguilera, Lynda Thomas, Mandy Moore, and Jessica Simpson. Airplay of rock, electronica, and alternative acts was reduced.

Bling-era hip hop played in heavy rotation on MTV at this time, through the likes of Puff Daddy, Mase, Jermaine Dupri, Master P, DMX, Busta Rhymes, Lil' Kim, Jay-Z, Missy Elliott, Lauryn Hill, Eminem, 50 Cent, Jadakiss, The Game, Kanye West, Foxy Brown, Ja Rule, Timbaland, and their associates. In contrast to the previous gangsta rap era, this era of hip hop had a more polished sound and materialist subject matter. R&B was also heavily represented with acts such as Aaliyah, Janet Jackson, Destiny's Child, 702, Monica, and Brandy.

Beginning in late 1997, MTV progressively reduced its airing of rock music videos, leading to the slogan among skeptics, "Rock is dead." The facts that at the time rock music fans were less materialistic, and bought less music based on television suggestion, were cited as reasons that MTV abandoned its once core music. MTV instead devoted its musical airtime mostly to pop and hip hop/R&B music. All rock-centric shows were eliminated and the rock-related categories of the "Video Music Awards" were pared down to one.

From this time until 2004, MTV made some periodic efforts to reintroduce pop rock music videos to the channel. Pop punk band Blink-182 received regular airtime on MTV at this time, due in large part to their "All the Small Things" video that made fun of the boy bands that MTV was airing at the time. Meanwhile, some rock bands that were not receiving MTV support, such as Korn and Creed, continued to sell albums. Then, upon the release of Korn's rap rock album "Follow the Leader", MTV began playing their videos "Got the Life" and "Freak on a Leash".

A band promoted by Korn, Limp Bizkit, received airtime for their cover of George Michael's "Faith", which became a hit. Subsequently, MTV began airing more rap/rock hybrid acts, such as Limp Bizkit and Kid Rock. Some rock acts with more comical videos, such as Rob Zombie, Red Hot Chili Peppers and Foo Fighters, also received airtime.

In the fall of 1999, MTV announced a special "Return of the Rock" weekend, in which new rock acts received airtime, after which a compilation album was released. System of a Down, Staind, Godsmack, Green Day, Incubus, Papa Roach, P.O.D., Sevendust, Powerman 5000, Slipknot, Kittie, Static X, and CKY were among the featured bands. These bands received some airtime on MTV and more so on MTV2, though both channels gave emphasis to the rock/rap acts.

By 2000, Linkin Park, Sum 41, Jimmy Eat World, Mudvayne, Cold, At the Drive-In, Alien Ant Farm, and other acts were added to the musical rotation. MTV also launched subscription channel MTVX to play rock music videos exclusively, an experiment that lasted until 2002. A daily music video program on MTV that carried the name "Return of the Rock" ran through early 2001, replaced by a successor, "All Things Rock", from 2002 until 2004.

In 1997, MTV introduced its new studios in Times Square. MTV created four shows in the late 1990s that centered on music videos: "MTV Live", "Total Request", "Say What?", and"12 Angry Viewers". A year later, in 1998, MTV merged "Total Request" and "MTV Live" into a live daily top 10 countdown show, "Total Request Live", which became known as "TRL." The original host was Carson Daly. The show included a live studio audience and was filmed in a windowed studio that allowed crowds to look in. According to Nielsen, the average audience for the show was at its highest in 1999 and continued with strong numbers through 2001. The program played the top ten pop, rock, R&B, and hip hop music videos, and featured live interviews with artists and celebrities. In 2003, Carson Daly left MTV and "TRL" to focus on his late night talk show on NBC. The series came to an end with a special finale episode, "Total Finale Live", which aired November 16, 2008, and featured hosts and guests that previously appeared on the show.

From 1998 to 2003, MTV also aired several other music video programs from its studios. These programs included "Say What? Karaoke", a game show hosted by Dave Holmes. In the early 2000s MTV aired "VJ for a Day", hosted by Ray Munns. MTV also aired "Hot Zone", hosted by Ananda Lewis, which featured pop music videos during the midday time period. Other programs at the time included "Sucker Free", "BeatSuite", and blocks of music videos hosted by VJs simply called "Music Television".

Around 1999 through 2001, as MTV aired fewer music videos throughout the day, it regularly aired compilation specials from its then 20-year history to look back on its roots. An all-encompassing special, "MTV Uncensored", premiered in 1999 and was later released as a book.

MTV celebrated its 20th anniversary on August 1, 2001, beginning with a 12-hour long retrospective called "MTV20: Buggles to Bizkit", which featured over 100 classic videos played chronologically, hosted by various VJs in reproductions of MTV's old studios. The day of programming culminated in a three-hour celebratory live event called "MTV20: Live and Almost Legal", which was hosted by Carson Daly and featured numerous guests from MTV's history, including the original VJs from 1981. Various other related "MTV20" specials aired in the months surrounding the event.

Janet Jackson became the inaugural honoree of the "MTV Icon" award, "an annual recognition of artists who have made significant contributions to music, music video and pop culture while tremendously impacting the MTV generation." Subsequent recipients included Aerosmith, Metallica, and The Cure.

Five years later, on August 1, 2006, MTV celebrated its 25th anniversary. On their website, MTV.com, visitors could watch the very first hour of MTV, including airing the original promos and commercials from Mountain Dew, Atari, Chewels gum, and Jovan. Videos were also shown from The Buggles, Pat Benatar, Rod Stewart, and others. The introduction of the first five VJs was also shown. Additionally, MTV.com put together a "yearbook" consisting of the greatest videos of each year from 1981 to 2006. MTV itself only mentioned the anniversary once on "TRL".

Although MTV reached its 30th year of broadcasting in 2011, the channel itself passed over this milestone in favor of its current programming schedule. The channel instead aired its 30th anniversary celebrations on its sister networks MTV2 and VH1 Classic. Nathaniel Brown, senior vice president of communications for MTV, confirmed that there were no plans for an on-air MTV celebration similar to the channel's 20th anniversary. Brown explained, "MTV as a brand doesn't age with our viewers. We are really focused on our current viewers, and our feeling was that our anniversary wasn't something that would be meaningful to them, many of whom weren't even alive in 1981."

From 1995 to 2000, MTV played 36.5% fewer music videos. MTV president Van Toeffler stated: "Clearly, the novelty of just showing music videos has worn off. It's required us to reinvent ourselves to a contemporary audience." Despite targeted efforts to play certain types of music videos in limited rotation, MTV greatly reduced its overall rotation of music videos by the mid-2000s. While music videos were featured on MTV up to eight hours per day in 2000, the year 2008 saw an average of just three hours of music videos per day on MTV. The rise of social media and websites like YouTube as a convenient outlet for the promotion and viewing of music videos signaled this reduction.

As the decade progressed, MTV continued to play some music videos instead of relegating them exclusively to its sister channels, but nevertheless began to air the videos only in the early morning hours or in a condensed form on "Total Request Live". As a result of these programming changes, Justin Timberlake implored MTV to "play more damn videos!" while giving an acceptance speech at the 2007 MTV Video Music Awards. Despite the challenge from Timberlake, MTV continued to decrease its total rotation time for music videos in 2007, and the channel eliminated its long-running special tags for music videos such as "Buzzworthy" (for under-represented artists), "Breakthrough" (for visually stunning videos), and "Spankin' New" (for brand new videos). Additionally, the historic Kabel typeface, which MTV displayed at the beginning and end of all music videos since 1981, was phased out in favor of larger text and less information about the video's record label and director. The classic font can still be seen in "prechyroned" versions of old videos on sister network MTV Classic, which had their title information recorded onto the same tape as the video itself.

MTV's next round of reality shows came in the late 1990s to early 2000s, as the channel shifted its focus to prank/comedic shows such as "The Tom Green Show", "Jackass", and "Wildboyz", and game shows such as "The Challenge" (aka "Real World/Road Rules Challenge"), "The Blame Game", "webRIOT", and "Say What? Karaoke". A year later, in 2000, "MTV's Fear" became the first scare-based/supernatural reality show and the first reality show in which contestants filmed themselves. MTV continued to experiment with late night talk shows in the early 2000s with relatively short-lived programs such as "Kathy's So-Called Reality", starring Kathy Griffin; and "The Tom Green Show".

Some of the reality shows on the network also followed the lives of musicians. "The Osbournes", a reality show based on the everyday life of Black Sabbath frontman Ozzy Osbourne, his wife Sharon, and two of their children, Jack and Kelly, premiered on MTV in 2002. The show went on to become one of the network's biggest-ever successes and was also recognized for the Osbourne family members' heavy use of profanity, which MTV censored for broadcast. It also kick-started a musical career for Kelly Osbourne, while Sharon Osbourne went on to host her own self-titled talk show on US television. Production ended on "The Osbournes" in November 2004. In the fall of 2004, Ozzy Osbourne's reality show "Battle for Ozzfest" aired; the show hosted competitions between bands vying to play as part of Ozzfest, a yearly heavy metal music tour across the United States hosted by Osbourne.

In 2003, MTV added "Punk'd", a project by Ashton Kutcher to play pranks on various celebrities, and in 2004 they added "Pimp My Ride", a show about adding aesthetic and functional modifications to cars and other vehicles. Another show was "", a reality series that followed the lives of pop singers Jessica Simpson and Nick Lachey, a music celebrity couple. It began in 2003 and ran for four seasons, ending in early 2005; the couple later divorced. The success of "Newlyweds" was followed in June 2004 by "The Ashlee Simpson Show", which documented the beginnings of the music career of Ashlee Simpson, Jessica Simpson's younger sister.

In 2005 and 2006, MTV continued its focus on reality shows, with the debuts of shows such as "8th & Ocean", "", "Next", "The Hills", "Two-A-Days", "My Super Sweet 16", "Parental Control", and "Viva La Bam", featuring Bam Margera.

In 2007, MTV aired the reality show "A Shot at Love with Tila Tequila", chronicling MySpace sensation Tila Tequila's journey to find a companion. Her bisexuality played into the seriesboth male and female contestants were vying for loveand was the subject of criticism. It was the #2 show airing on MTV at that time, behind "The Hills". A spin-off series from "A Shot at Love", titled "That's Amoré!", followed a similar pursuit from previous "A Shot at Love" contestant Domenico Nesci.

MTV also welcomed Paris Hilton to its lineup in October 2008, with the launch of her new reality series, "Paris Hilton's My New BFF". In 2009, MTV aired Snoop Dogg's second program with the channel, "Dogg After Dark", and the show "College Life", based at the University of Wisconsin–Madison.

MTV continued to produce cartoons with mature themes at this time including "Celebrity Deathmatch", "Undergrads", "Clone High", and "Daria".

MTV continued to air both comedy and drama programs with scripted or improvised premises at this time. Examples from the late 1990s and early-mid 2000s include sketch-based comedies such as "The Lyricist Lounge Show" and "Doggy Fizzle Televizzle", as well as soap operas such as "Undressed" and "Spyder Games".

Prior to its finale in 2008, MTV's main source of music videos was "Total Request Live", airing four times per week, featuring short clips of music videos along with VJs and guests. MTV was experimenting at the time with new ideas for music programs to replace the purpose of "TRL" but with a new format.

In mid-2008, MTV premiered new music video programming blocks called "FNMTV" and a weekly special event called "FNMTV Premieres", hosted from Los Angeles by Pete Wentz of the band Fall Out Boy, which was designed to premiere new music videos and have viewers provide instantaneous feedback.

The "FNMTV Premieres" event ended before the 2008 MTV Video Music Awards in September. With the exception of a holiday themed episode in December 2008 and an unrelated "Spring Break" special in March 2009 with the same title, "FNMTV Premieres" never returned to the channel's regular program schedule, leaving MTV without any music video programs hosted by VJs for the first time in its history.
Music video programming returned to MTV in March 2009 as "AMTV", an early morning block of music videos that originally aired from 3am to 9am on most weekdays. It was renamed "Music Feed" in 2013 with a reduced schedule. Unlike the "FNMTV" block that preceded it, "Music Feed" featured many full-length music videos, including some older videos that had been out of regular rotation for many years on MTV. It also featured music news updates, interviews, and performances. For many years, "Music Feed" was the only program on MTV's main channel that was dedicated to music videos.

During the rest of the day, MTV used to play excerpts from music videos in split screen format during the closing credits of most programs, along with the address of a website to encourage the viewer to watch the full video online. MTV positioned its website, MTV.com, as its primary destination for music videos, but this strategy was abandoned.

In late 2009, MTV shifted its focus back to "Real World"-style reality programming with the premiere of "Jersey Shore", a program that brought high ratings to the channel and also caused controversy due to some of its content.

With backlash towards what some consider too much superficial content on the network, a 2009 "New York Times" article also stated the intention of MTV to shift its focus towards more socially conscious media, which the article labels "MTV for the Obama era." Shows in that vein included "T.I.'s Road to Redemption" and Fonzworth Bentley's finishing school show "From G's to Gents".

The channel also aired a new show around this time titled "16 and Pregnant", which documented the lives of teenagers expecting babies. This had a follow-up show after the first season titled "Teen Mom", which follows some of the teens through the first stages with their newborns.

MTV found further success with "The Buried Life", a program about four friends traveling across the country to check off a list of "100 things to do before I die" and helping others along the way. Another recent reality program is MTV's "Hired", which follows the employment interviewing process; candidates meet with career coach Ryan Kahn from "Dream Careers" and at the end of each episode one candidate lands the job of their dreams. In 2012, "Punk'd" returned with a revolving door of new hosts per episode. Meanwhile, spin-offs from "Jersey Shore" such as "The Pauly D Project" and "Snooki & JWoww" were produced. MTV announced plans to re-enter the late-night comedy space in 2012, with "Nikki & Sara Live", an unscripted series by comedians Nikki Glaser and Sara Schaefer. The program initially aired weekly from MTV's studios in Times Square.

MTV again resurrected the long-running series "MTV Unplugged" in 2009 with performances from acts such as Adele and Paramore. However, unlike past "Unplugged" specials, these new recordings usually only aired in their entirety on MTV's website, MTV.com. Nevertheless, short clips of the specials were shown on MTV during the "AMTV" block of music videos in the early morning hours. On June 12, 2011, MTV aired a traditional television premiere of a new installment of "MTV Unplugged" instead of a web debut. The featured artist was rapper Lil Wayne and the show debuted both on MTV and MTV2. The channel followed up with a similar television premiere of "MTV Unplugged" with Florence and the Machine on April 8, 2012.

MTV launched "10 on Top" in May 2010 with little promotion throughout its run, a weekly program airing on Saturdays and hosted by Lenay Dunn, that counted down the top 10 most trending and talked about topics of the week (generally focused on entertainment). Dunn also appeared in segments between MTV's shows throughout the day as a recognizable personality and face of the channel in the absence of traditional VJs aside from its MTV News correspondents.

The animated series "Beavis and Butt-head" returned to MTV in October 2011, with new episodes. As with the original version of the series that ran from 1993 to 1997, the modern-day "Beavis and Butt-head" featured segments in which its main characters watch and satirize music videos, as well as the newer addition of reality TV shows such as "Jersey Shore".

In 2012, MTV debuted "Clubland", which previously existed as an hour of EDM videos during the "AMTV" video block. The show had no host, but most editorial content was pushed online by the show's Tumblr and other social media outlets like Facebook and Twitter.

MTV launched a new talk show based on hip hop music on April 12, 2012, called "Hip Hop POV", hosted by Amanda Seales, Bu Thiam, Charlamagne, Devi Dev, and Sowmya Krishnamurthy. The show featured hosted commentary on the headlines in hip hop culture, providing opinions on new music, granting insider access to major events, and including artist interviews. "Hip Hip POV" lasted several episodes before going on hiatus. The show was supposed to return in Fall 2012, but was moved to MTV2 instead, where it was rebranded and merged with "Sucker Free Countdown". The new show debuted as "The Week in Jams" on October 28, 2012.

MTV launched a live talk show, "It's On with Alexa Chung", on June 15, 2009. The host of the program, Alexa Chung, was described as a "younger, more Web 2.0" version of Jimmy Fallon. Although it was filmed in the same Times Square studio where "TRL" used to be broadcast, the network stated that "the only thing the two shows have in common is the studio location." "It's On" was cancelled in December of the same year, which again eliminated the only live in-studio programming from MTV's schedule, just one year after "TRL" was also cancelled.

Shortly after Michael Jackson died on June 25, 2009, the channel aired several hours of Jackson's music videos, accompanied by live news specials featuring reactions from MTV personalities and other celebrities. The temporary shift in MTV's programming culminated the following week with the channel's live coverage of Jackson's memorial service. MTV aired similar one-hour live specials with music videos and news updates following the death of Whitney Houston on February 11, 2012, and the death of Adam Yauch of the Beastie Boys on May 4, 2012.

The channel tried its hand again at live programming with the premiere of a half-hour program called "The Seven" in September 2010. The program counted down seven entertainment-related stories of interest to viewers (and included some interview segments among them), having aired weekdays at 5pm with a weekend wrap-up at 10 am ET. Shortly after its debut, the show was slightly retooled as it dropped co-host Julie Alexandria but kept fellow co-host Kevin Manno; the Saturday recap show was eliminated as well. "The Seven" was cancelled on June 13, 2011. Manno's only assignment at MTV post-"Seven" was conducting an interview with a band which only aired on MTV.com. Manno is no longer employed with MTV and has since appeared as an occasional correspondent on the LXTV-produced NBC series "1st Look".

Presently, MTV airs sporadic live specials called "MTV First". The short program, produced by MTV News, debuted in early 2011 and continues to air typically once every couple of weeks on any given weekday. The specials usually begin at 7:53 pm. ET, led by one of MTV News' correspondents who will conduct a live interview with a featured artist or actor who has come to MTV to premiere a music video or movie trailer. MTV starts its next scheduled program at 8:00 pm, while the interview and chat with fans continues on MTV.com for another 30 to 60 minutes. Since its debut in 2011, "MTV First" has featured high-profile acts such as Lady Gaga, Katy Perry, Usher, and Justin Bieber. In the absence of daily live programs such as "TRL", "It's On with Alexa Chung", and "The Seven" to facilitate such segments, the channel now uses "MTV First" as its newest approach to present music video premieres and bring viewers from its main television channel to its website for real-time interaction with artists and celebrities.

In April 2016, then-appointed MTV president Sean Atkins announced plans to restore music programming to the channel. On April 21, 2016, MTV announced that new "Unplugged" episodes will begin airing, as well as a new weekly performance series called "Wonderland". On that same day, immediately after the death of Prince, MTV interrupted its usual programming to air Prince's music videos. In July 2017, it was announced that "TRL" would be returning to the network on October 2, 2017. As of 2019, the show currently airs on Saturday mornings as "TRL Top 10". Beginning in the mid-2010s and continuing after, MTV shifted to heavy marathons of two to three shows, including "Ridiculousness" for most hours of programming.

In September 2009, the channel aired "Popzilla", which showcased and imitated celebrities in an animated form. MTV again reintroduced animated programming to its lineup with the return of "Beavis and Butt-Head" in 2011 after 14 years off the air, alongside brand new animated program "Good Vibes". In January 2016, MTV returned to animation with "Greatest Party Story Ever".

The channel expanded its programming focus in late 2000s and early 2010s to include more scripted programs. The resurgence of scripted programming on MTV saw the introduction of comedy shows such as "Awkward." and "The Hard Times of RJ Berger", and dramas such as "Skins" and "Teen Wolf". In June 2012, MTV confirmed that it would develop a series based on the "Scream" franchise. The series is now in its third season. On June 24, 2019, it was announced that "Scream" would be moving to VH1 ahead of the premiere of the third season.

As MTV expanded, music videos and VJ-guided programming were no longer the centerpiece of its programming. Today, MTV's programming covers a wide variety of genres and formats aimed primarily at Generation Z adolescents and young adults. In addition to its original programming, MTV has also aired original and syndicated programs from Viacom-owned siblings and third-party networks.

MTV is also a producer of films aimed at young adults through its production label, MTV Films, and has aired both its own theatrically-released films and original made-for-television movies from MTV Studios in addition to acquired films.

In 2010, a study by the Gay and Lesbian Alliance Against Defamation found that of 207.5 hours of prime time programming on MTV, 42% included content reflecting the lives of gay, bisexual and transgender people. This was the highest in the industry and the highest percentage ever.

In 2018, MTV launched a new production unit under the "MTV Studios" name focused on producing new versions of MTV's library shows.

MTV's now-iconic logo was designed in 1981 by Manhattan Design (a collective formed by Frank Olinsky, Pat Gorman and Patty Rogoff) under the guidance of original creative director Fred Seibert. The block letter "M" was sketched by Rogoff, with the scribbled word "TV" spraypainted by Olinksky. The primary variant of MTV's logo at the time had the "M" in yellow and the "TV" in red. But unlike most television networks' logos at the time, the logo was constantly branded with different colors, patterns and images on a variety of station IDs. Examples include 1988's ID "Adam And Eve", where the "M" is an apple and the snake is the "TV". And for 1984's ID "Art History", the logo is shown in different art styles. The only constant aspects of MTV's logo at the time were its general shape and proportions, with everything else being dynamic.

MTV launched on August 1, 1981, with an extended network ID featuring the first landing on the moon (with still images acquired directly from NASA), which was a concept of Seibert's executed by Buzz Potamkin and Perpetual Motion Pictures. The ID then cut to the American flag planted on the moon's surface changed to show the MTV logo on it, which rapidly changed into different colors and patterns several times per second as the network's original guitar-driven jingle was played for the first time. After MTV's launch, the "moon landing" ID was edited to show only its ending, and was shown at the top of every hour until early 1986, when the ID was scrapped in light of the Space Shuttle Challenger disaster. The ID ran "more than 75,000 times each year (48 times each day), at the top and bottom of every hour every day" according to Seibert.

From the late 1990s to the early 2000s, MTV updated its on-air appearance at the beginning of every year and each summer, creating a consistent brand across all of its music-related shows. This style of channel-wide branding came to an end as MTV drastically reduced its number of music-related shows in the early to mid 2000s. Around this time, MTV introduced a static and single color digital on-screen graphic to be shown during all of its programming.

Starting with the premiere of the short-lived program "FNMTV" in 2008, MTV started using a revised and chopped down version of its original logo during most of its on-air programming. It became MTV's official logo on February 8, 2010 and officially debuted on its website. The channel's full name "Music Television" was officially dropped, with the revised logo largely the same as the original logo, but without the initialism, the bottom of the "M" being cropped and the "V" in "TV" being branched off. This change was most likely made to reflect MTV's more prominent focus on reality and comedy programming and less on music-related programming. However, much like the original logo, the new logo was designed to be filled in with a seemingly unlimited variety of images. It is used worldwide, but not everywhere existentially. The new logo was first used on MTV Films logo with the 2010 film "Jackass 3D". MTV's rebranding was overseen by Popkern.

On June 25, 2015, MTV International rebranded its on-air look with a new vaporwave and seapunk-inspired graphics package. It included a series of new station IDs featuring 3D renderings of objects and people, much akin to vaporwave and seapunk "aesthetics". Many have derided MTV's choice of rebranding, insisting that the artistic style was centered on denouncing corporate capitalism (many aesthetic pieces heavily incorporate corporate logos of the 1970s, 80s and 90s, which coincidentally include MTV's original logo) rather than being embraced by major corporations like MTV. Many have also suggested that MTV made an attempt to be relevant in the modern entertainment world with the rebrand. In addition to this, the rebrand was made on exactly the same day that the social media site Tumblr introduced Tumblr TV, an animated GIF viewer which featured branding inspired by MTV's original 1980s on-air look. Tumblr has been cited as a prominent location of aesthetic art, and thus many have suggested MTV and Tumblr "switched identities". The rebrand also incorporated a modified version of MTV's classic "I Want My MTV!" slogan, changed to read "I Am My MTV". "Vice" has suggested that the slogan change represents "the current generation's movement towards self-examination, identity politics and apparent narcissism." MTV also introduced MTV Bump, a website that allows Instagram and Vine users to submit videos to be aired during commercial breaks, as well as MTV Canvas, an online program where users submit custom IDs to also be aired during commercial breaks.

The channel's iconic "I want my MTV!" advertising campaign was launched in 1982. It was first developed by George Lois and was based on a cereal commercial from the 1950s with the slogan "I want my Maypo!" that Lois adapted unsuccessfully from the original created by animator John Hubley.

Lois's first pitch to the network was roundly rejected when Lois insisted that rock stars like Mick Jagger should be crying when they said the tag line, not unlike his failed 'Maypo' revamp. His associate, and Seibert mentor Dale Pon took over the campaign, strategically and creatively, and was able to get the campaign greenlit when he laughed the tears out of the spots. From then on –with the exception of the closely logos on the first round of commercials– Pon was the primary creative force.

All the commercials were produced by Buzz Potamkin and his new company Buzzco Productions, directed first by Thomas Schlamme and Alan Goodman and eventually by Candy Kugel.

The campaign featured popular artists and celebrities, including Pete Townshend, Pat Benatar, Adam Ant, David Bowie, The Police, Kiss, Culture Club, Billy Idol, Hall & Oates, Cyndi Lauper, Madonna, Lionel Richie, Ric Ocasek, John Mellencamp, Peter Wolf, Joe Elliot, Stevie Nicks, Rick Springfield, and Mick Jagger, interacting with the MTV logo on-air and encouraging viewers to call their pay television providers and request that MTV be added to their local channel lineups. Eventually, the slogan became so ubiquitous that it made an appearance as a lyric sung by Sting on the Dire Straits song "Money for Nothing", whose music video aired in regular rotation on MTV when it was first released in 1985.

The channel has been a target of criticism by various groups about programming choices, social issues, political correctness, sensitivity, censorship, and a perceived negative social influence on young people. Portions of the content of MTV's programs and productions have come under controversy in the general news media and among social groups that have taken offense. Some within the music industry criticized what they saw as MTV's homogenization of rock 'n' roll, including the punk band the Dead Kennedys, whose song "M.T.V.Get Off the Air" was released on their 1985 album "Frankenchrist", just as MTV's influence over the music industry was being solidified. MTV was also the major influence on the growth of music videos during the 1980s.

HBO also had a 30-minute program of music videos called "Video Jukebox", that first aired around the time of MTV's launch and lasted until late 1986. Also around this time, HBO, as well as other premium channels such as Cinemax, Showtime and The Movie Channel, occasionally played one or a few music videos between movies.

SuperStation WTBS launched "Night Tracks" on June 3, 1983, with up to 14 hours of music video airplay each late night weekend by 1985. Its most noticeable difference was that black artists that MTV initially ignored received airplay. The program ran until the end of May 1992.

A few markets also launched music-only channels including Las Vegas' KVMY (channel 21), which debuted in the summer of 1984 as KRLR-TV and branded as "Vusic 21". The first video played on that channel was "Video Killed the Radio Star", following in the footsteps of MTV.

Shortly after TBS began "Night Tracks", NBC launched a music video program called "Friday Night Videos", which was considered network television's answer to MTV. Later renamed simply "Friday Night", the program ran from 1983 to 2002. ABC's contribution to the music video program genre in 1984, "ABC Rocks", was far less successful, lasting only a year.

TBS founder Ted Turner started the Cable Music Channel in 1984, designed to play a broader mix of music videos than MTV's rock format allowed. But after one month as a money-losing venture, Turner sold it to MTV, who redeveloped the channel into VH1.

Shortly after its launch, The Disney Channel aired a program called "D-TV", a play on the MTV acronym. The program used music cuts, both from current and past artists. Instead of music videos, the program used clips of various vintage Disney cartoons and animated films to go with the songs. The program aired in multiple formats, sometimes between shows, sometimes as its own program, and other times as one-off specials. The specials tended to air both on The Disney Channel and NBC. The program aired at various times between 1984 and 1999. In 2009, Disney Channel revived the "D-TV" concept with a new series of short-form segments called "Re-Micks".

MTV has edited a number of music videos to remove references to drugs, sex, violence, weapons, racism, homophobia, and/or advertising. Many music videos aired on the channel were either censored, moved to late-night rotation, or banned entirely from the channel.

In the 1980s, parent media watchdog groups such as the Parents Music Resource Center (PMRC) criticized MTV over certain music videos that were claimed to have explicit imagery of satanism. As a result, MTV developed a strict policy on refusal to air videos that may depict Satanism or anti-religious themes. This policy led MTV to ban music videos such as "Jesus Christ Pose" by Soundgarden in 1991 and "Megalomaniac" by Incubus in 2004; however, the controversial band Marilyn Manson was among the most popular rock bands on MTV during the late 1990s and early 2000s.

On September 28, 2016, on an AfterBuzz TV live stream, Scout Durwood said that MTV had a "no appropriation policy" that forbid her from wearing her hair in cornrows in an episode of "Mary + Jane". She said, "I wanted to cornrow my hair, and they were like, 'That's racist.'"

During the 1989 MTV Video Music Awards ceremony, comedian Andrew Dice Clay did his usual "adult nursery rhymes" routine (which he had done in his stand-up acts), after which the network executives imposed a lifetime ban. Billy Idol's music video for the song "Cradle of Love" originally had scenes from Clay's film "The Adventures of Ford Fairlane" when it was originally aired; scenes from the film were later excised. During the 2011 MTV Video Music Awards, Clay was in attendance where he confirmed that the channel lifted the ban.

In the wake of controversy that involved a child burning down his house after allegedly watching "Beavis and Butt-head", MTV moved the show from its original 7p.m. time slot to an 11p.m. time slot. Also, Beavis' tendency to flick a lighter and yell "fire" was removed from new episodes, and controversial scenes were removed from existing episodes before their rebroadcast. Some extensive edits were noted by series creator Mike Judge after compiling his , saying that "some of those episodes may not even exist actually in their original form."

A pilot for a show called "Dude, This Sucks" was canceled after teens attending a taping at the Snow Summit Ski Resort in January 2001 were sprayed with liquidized fecal matter by a group known as "The Shower Rangers". The teens later sued, with MTV later apologizing and ordering the segment's removal.

After Viacom's purchase of CBS, MTV was selected to produce the Super Bowl XXXV halftime show in 2001, airing on CBS and featuring Britney Spears, NSYNC, and Aerosmith. Due to its success, MTV was invited back to produce another halftime show in 2004; this sparked a nationwide debate and controversy that drastically changed Super Bowl halftime shows, MTV's programming, and radio censorship.

When CBS aired Super Bowl XXXVIII in 2004, MTV was again chosen to produce the halftime show, with performances by such artists as Nelly, P. Diddy, Janet Jackson, and Justin Timberlake. The show became controversial, however, after Timberlake tore off part of Jackson's outfit while performing "Rock Your Body" with her, revealing her right breast. All involved parties apologized for the incident, and Timberlake referred to the incident as a "wardrobe malfunction".

Michael Powell, former chairman of the Federal Communications Commission, ordered an investigation the day after broadcast. In the weeks following the halftime show, MTV censored much of its programming. Several music videos, including "This Love" and "I Miss You", were edited for sexual content. In September 2004, the FCC ruled that the halftime show was indecent and fined CBS $550,000. The FCC upheld it in 2006, but federal judges reversed the fine in 2008.

Timberlake and Jackson's controversial event gave way to a "wave of self-censorship on American television unrivaled since the McCarthy era". After the sudden event, names surfaced such as nipplegate, Janet moment, and boobgate, and this spread politically, furthering the discussion into the 2004 presidential election surrounding "moral values" and "media decency".

The Christian right organization American Family Association has also criticized MTV from perceptions of negative moral influence, describing MTV as promoting a "pro-sex, anti-family, pro-choice, drug culture".

In 2005, the Parents Television Council (PTC) released a study titled "MTV Smut Peddlers", which sought to expose excessive sexual, profane, and violent content on the channel, based on MTV's spring break programming from 2004. Jeanette Kedas, an MTV network executive, called the PTC report "unfair and inaccurate" and "underestimating young people's intellect and level of sophistication", while L. Brent Bozell III, then-president of the PTC, stated: "the incessant sleaze on MTV presents the most compelling case yet for consumer cable choice", referring to the practice of pay television companies to allow consumers to pay for channels "à la carte".

In April 2008, PTC released "The Rap on Rap", a study covering hip-hop and R&B music videos rotated on programs "106 & Park" and "Rap City", both shown on BET, and "Sucker Free" on MTV. PTC urged advertisers to withdraw sponsorship of those programs, whose videos PTC stated targeted children and teenagers containing adult content.

MTV received significant criticism from Italian American organizations for "Jersey Shore", which premiered in 2009. The controversy was due in large part to the manner in which MTV marketed the show, as it liberally used the word "guido" to describe the cast members. The word "guido" is generally regarded as an ethnic slur when referring to Italians and Italian Americans. One promotion stated that the show was to follow, "eight of the hottest, tannest, craziest Guidos," while yet another advertisement stated, ""Jersey Shore" exposes one of the tri-state area's most misunderstood species ... the GUIDO. Yes, they really do exist! Our Guidos and Guidettes will move into the ultimate beach house rental and indulge in everything the Seaside Heights, New Jersey scene has to offer."

Prior to the series debut, Unico National formally requested that MTV cancel the show. In a formal letter, the company called the show a "direct, deliberate and disgraceful attack on Italian Americans." Unico National President Andre DiMino said in a statement, "MTV has festooned the 'bordello-like' house set with Italian flags and red, white and green maps of New Jersey while every other cutaway shot is of Italian signs and symbols. They are blatantly as well as subliminally bashing Italian Americans with every technique possible." Around this time, other Italian organizations joined the fight, including the NIAF and the Order Sons of Italy in America.

MTV responded by issuing a press release which stated in part, "The Italian American cast takes pride in their ethnicity. We understand that this show is not intended for every audience and depicts just one aspect of youth culture." Following the calls for the show's removal, several sponsors requested that their ads not be aired during the show. These sponsors included Dell, Domino's Pizza, and American Family Insurance. Despite the loss of certain advertisers, MTV did not cancel the show. Moreover, the show saw its audience increase from its premiere in 2009, and continued to place as MTV's top-rated programs during "Jersey Shore's" six-season run, ending in 2012.

In December 2016, MTV online published a social justice-oriented New Year's resolution-themed video directed towards white men. The video caused widespread outrage online, including video responses from well-known online personas, and was deleted from MTV's YouTube channel. The video was then reuploaded to their channel, with MTV claiming the new video contained "updated graphical elements". The new video quickly received over 10,000 dislikes and fewer than 100 likes from only 20,000 views, and MTV deleted the video for a second time.

In addition to its regular programming, MTV has a long history of promoting social, political, and environmental activism in young people. The channel's vehicles for this activism have been "Choose or Lose", encompassing political causes and encouraging viewers to vote in elections; "Fight For Your Rights", encompassing anti-violence and anti-discrimination causes; "think MTV"; and "MTV Act" and "Power of 12", the newest umbrellas for MTV's social activism.

In 1992, MTV started a pro-democracy campaign called "Choose or Lose", to encourage over 20 million people to register to vote, and the channel hosted a town hall forum for then-candidate Bill Clinton.

In recent years, other politically diverse programs on MTV have included "True Life", which documents people's lives and problems, and MTV News specials, which center on very current events in both the music industry and the world. One special show covered the 2004 US presidential election, airing programs focused on the issues and opinions of young people, including a program where viewers could ask questions of Senator John Kerry. MTV worked with P. Diddy's "Citizen Change" campaign, designed to encourage young people to vote.

Additionally, MTV aired a documentary covering a trip by the musical group Sum 41 to the Democratic Republic of the Congo, documenting the conflict there. The group ended up being caught in the midst of an attack outside of the hotel and were subsequently flown out of the country.

The channel also began showing presidential campaign commercials for the first time during the 2008 US presidential election. This has led to criticism, with Jonah Goldberg opining that "MTV serves as the Democrats' main youth outreach program."

MTV is aligned with Rock the Vote, a campaign to motivate young adults to register and vote.

In 2012, MTV launched "MTV Act" and "Power of 12", its current social activism campaigns. "MTV Act" focuses on a wide array of social issues, while "Power of 12" was a replacement for MTV's "Choose or Lose" and focused on the 2012 US presidential election.

In 2016, MTV continued its pro-democracy campaign with "Elect This", an issue-oriented look at the 2016 election targeting Millennials. Original content under the "Elect This" umbrella includes "Infographica," short animations summarizing MTV News polls; "Robo-Roundtable," a digital series hosted by animatronic robots; "The Racket," a multi-weekly digital series; and "The Stakes," a weekly political podcast.

Since its launch in 1981, the brand "MTV" has expanded to include many additional properties beyond the original MTV channel, including a variety of sister channels in the US, dozens of affiliated channels around the world, and an Internet presence through MTV.com and related websites.

MTV operates a group of channels under MTV Networksa name that continues to be used for the individual units of the now ViacomCBS Domestic Media Networks, a division of corporate parent ViacomCBS. In 1985, MTV saw the introduction of its first true sister channel, VH1, which was originally an acronym for "Video Hits One" and was designed to play adult contemporary music videos. Today, VH1 is aimed at celebrity and popular culture programming which include many reality shows. Another sister channel, CMT, targets the country music and southern culture market.

The advent of satellite television and digital cable brought MTV greater channel diversity, including its current sister channels MTV2 and MTV Tr3́s (now Tr3́s), which initially played music videos exclusively but now focus on other programming. MTV also formerly broadcast MTVU on campuses at various universities until 2018, when the MTV Networks on Campus division was sold, and the channel remained as a digital cable channel only. MTV used to also have MTV Hits and MTVX channels until these were converted into NickMusic and MTV Jams, respectively. MTV Jams later was rebranded as BET Jams.

In the 2000s, MTV launched MTV HD, a 1080i high definition simulcast feed of MTV. Until Viacom's main master control was upgraded in 2013, only the network's original series after 2010 (with some pre-2010 content) are broadcast in high definition, while music videos, despite being among the first television works to convert to high definition presentation in the early 2000s, were presented in 4:3 standard definition, forcing them into a windowboxing type of presentation; since that time all music videos are presented in HD, and are framed to their director's preference. "Jersey Shore", despite being shot with widescreen HD cameras, was also presented with SD windowboxing (though the 2018 "" revival is in full HD). The vast majority of providers carry MTV HD.

MTV Networks also operates MTV Live, a high-definition channel that features original HD music programming and HD versions of music related programs from MTV, VH1 and CMT. The channel was launched in January 2006 as MHD (Music: High Definition). The channel was officially rebranded as MTV Live on February 1, 2016.

In 2005 and 2006, MTV launched a series of channels for Asian Americans. The first channel was MTV Desi, launched in July 2005, dedicated toward South-Asian Americans. Next was MTV Chi, in December 2005, which catered to Chinese Americans. The third was MTV K, launched in June 2006 and targeted toward Korean Americans. Each of these channels featured music videos and shows from MTV's international affiliates as well as original US programming, promos, and packaging. All three of these channels ceased broadcasting on April 30, 2007.

On August 1, 2016, the 35th anniversary of the original MTV's launch, VH1 Classic was rebranded as MTV Classic. The channel's programming focused on classic music videos and programming (including notable episodes of "MTV Unplugged" and "VH1 Storytellers"), but skews more towards the 1980s, 1990s and 2000s. The network aired encores of 2000s MTV series such as "Beavis and Butt-Head" and "". The network's relaunch included a broadcast of MTV's first hour on the air, which was also simulcast on MTV and online via Facebook live streaming. MTV Classic only retained three original VH1 Classic programs, which were "That Metal Show", "Metal Evolution," and "Behind the Music Remastered", although repeats of current and former VH1 programs such as "Pop-Up Video" and "VH1 Storytellers" remained on the schedule. However, the rebranded MTV Classic had few viewers, and declined quickly to become the least-watched English-language subscription network rated by Nielsen at the end of 2016. At the start of 2017, it was reprogrammed into an all-video network.

In the late 1980s, before the World Wide Web, MTV VJ Adam Curry began experimenting on the Internet. He registered the then-unclaimed domain name "MTV.com" in 1993 with the idea of being MTV's unofficial new voice on the Internet. Although this move was sanctioned by his supervisors at MTV Networks at the time, when Curry left to start his own web-portal design and hosting company, MTV subsequently sued him for the domain name, which led to an out-of-court settlement.

The service hosted at the domain name was originally branded "MTV Online" during MTV's first few years of control over it in the mid-1990s. It served as a counterpart to the America Online portal for MTV content, which existed at AOL keyword MTV until approximately the end of the 1990s. After this time, the website became known as simply "MTV.com" and served as the Internet hub for all MTV and MTV News content.

MTV.com experimented with entirely video-based layouts between 2005 and 2007. The experiment began in April 2005 as "MTV Overdrive", a streaming video service that supplemented the regular MTV.com website. Shortly after the 2006 MTV Video Music Awards, which were streamed on MTV.com and heavily utilized the "MTV Overdrive" features, MTV introduced a massive change for MTV.com, transforming the entire site into a Flash video-based entity. Much of users' feedback about the Flash-based site was negative, demonstrating a dissatisfaction with videos that played automatically, commercials that could not be skipped or stopped, and the slower speed of the entire website. The experiment ended in February 2006 as MTV.com reverted to a traditional HTML-based website design with embedded video clips, in the style of YouTube and some other video-based websites.

From 2006 to 2007, MTV operated an online channel, MTV International, targeted to the broad international market. The purpose of the online channel was to air commercial-free music videos once the television channels started concentrating on shows unrelated to music videos or music-related programming.

The channel responded to the rise of the Internet as the new central place to watch music videos in October 2008 by launching MTV Music (later called MTV Hive), a website that featured thousands of music videos from MTV and VH1's video libraries, dating back to the earliest videos from 1981.

A newly created division of the company, MTV New Media, announced in 2008 that it would produce its own original web series, in an attempt to create a bridge between old and new media. The programming is available to viewers via personal computers, cell phones, iPods, and other digital devices.

In the summer of 2012, MTV launched a music discovery web site called the MTV Artists Platform (also known as Artists.MTV). MTV stated, "While technology has made it way easier for artists to produce and distribute their own music on their own terms, it hasn't made it any simpler to find a way to cut through all the Internet noise and speak directly to all of their potential fans. The summer launch of the platform is an attempt to help music junkies and musicians close the gap by providing a one-stop place where fans can listen to and buy music and purchase concert tickets and merchandise."

Today, MTV.com remains the official website of MTV, and it expands on the channel's broadcasts by bringing additional content to its viewers. The site's features include an online version of MTV News, podcasts, a commercial streaming service, movie features, profiles and interviews with recording artists and from MTV's television programs.

MTV Networks has launched numerous native-language regional variants of MTV-branded channels to countries around the world.





</doc>
<doc id="18857" url="https://en.wikipedia.org/wiki?curid=18857" title="Mustelidae">
Mustelidae

The Mustelidae (; from Latin "mustela", weasel) are a family of carnivorous mammals, including weasels, badgers, otters, ferrets, martens, minks, and wolverines, among others. Mustelids () are a diverse group and form the largest family in the order Carnivora, suborder Caniformia. Mustelidae comprises about 56–60 species across eight subfamilies.

Mustelids vary greatly in size and behaviour. The least weasel can be under a foot in length, while the giant otter of Amazonian South America can measure up to and sea otters can exceed in weight. The wolverine can crush bones as thick as the femur of a moose to get at the marrow, and has been seen attempting to drive bears away from their kills. The sea otter uses rocks to break open shellfish to eat. Martens are largely arboreal, while the European badger digs extensive tunnel networks, called setts. Some mustelids have been domesticated: the ferret and the tayra are kept as pets (although the tayra requires a Dangerous Wild Animals licence in the UK), or as working animals for hunting or vermin control. Others have been important in the fur trade—the mink is often raised for its fur.

As well as being one of the most species-rich families in the order Carnivora, the family Mustelidae is one of the oldest. Mustelid-like forms first appeared about 40 million years ago, roughly coinciding with the appearance of rodents. The common ancestor of modern mustelids appeared about 18 million years ago.

Within a large range of variation, the mustelids exhibit some common characteristics. They are typically small animals with elongated bodies, short legs, short, round ears, and thick fur. Most mustelids are solitary, nocturnal animals, and are active year-round.

With the exception of the sea otter, they have anal scent glands that produce a strong-smelling secretion the animals use for sexual signaling and for marking territory.
Most mustelid reproduction involves embryonic diapause. The embryo does not immediately implant in the uterus, but remains dormant for some time. No development takes place as long as the embryo remains unattached to the uterine lining. As a result, the normal gestation period is extended, sometimes up to a year. This allows the young to be born under more favorable environmental conditions. Reproduction has a large energy cost and it is to a female's benefit to have available food and mild weather. The young are more likely to survive if birth occurs after previous offspring have been weaned.

Mustelids are predominantly carnivorous, although some eat vegetable matter at times. While not all mustelids share an identical dentition, they all possess teeth adapted for eating flesh, including the presence of shearing carnassials. With variation between species, the most common dental formula is .

The fisher, tayra and martens are partially arboreal, while badgers are fossorial. A number of mustelids have aquatic lifestyles, ranging from semiaquatic minks and river otters to the fully aquatic sea otter. The sea otter is one of the few non-primate mammals known to use a tool while foraging. It uses "anvil" stones to crack open the shellfish that form a significant part of its diet. It is a "keystone species", keeping its prey populations in balance so some do not outcompete the others and destroy the kelp in which they live.

The black-footed ferret is entirely dependent on another keystone species, the prairie dog. A family of four ferrets eats 250 prairie dogs in a year; this requires a stable population of prairie dogs from an area of some .

Skunks were formerly included as a subfamily of the mustelids, but are now regarded as a separate family (Mephitidae). Mongooses bear a striking resemblance to many mustelids, but belong to a distinctly different suborder—the Feliformia (all those carnivores sharing more recent origins with the cats) and not the Caniformia (those sharing more recent origins with the dogs). Because mongooses and mustelids occupy similar ecological niches, convergent evolution has led to similarity in form and behavior.

The oldest known mustelid from North America is "Corumictis wolsani" from the early and late Oligocene (early and late Arikareean, Ar1–Ar3) of Oregon. Middle Oligocene "Mustelictis" from Europe might be a mustelid as well. Other early fossils of the mustelids were dated at the end of the Oligocene to the beginning of the Miocene. It is not clear which of these forms are Mustelidae ancestors and which should be considered the first mustelids.

The fossil record indicates that mustelids appeared in the late Oligocene period (33 mya) in Eurasia and migrated to every continent except Antarctica and Australia (all the continents that were connected during or since the early Miocene). They reached the Americas via the Bering land bridge.

Several mustelids, including the mink, the sable (a type of marten) and the stoat (ermine), boast exquisite and valuable furs, and have been accordingly hunted since prehistoric times. From the early Middle Ages, the trade in furs was of great economic importance for northern and eastern European nations with large native populations of fur-bearing mustelids, and was a major economic impetus behind Russian expansion into Siberia and French and English expansion in North America. In recent centuries, fur farming, notably of mink, has also become widespread and provides the majority of the fur brought to market.

One species, the sea mink ("Neovison macrodon") of New England and Canada, was driven to extinction by fur trappers. Its appearance and habits are almost unknown today because no complete specimens can be found and no systematic contemporary studies were conducted.

The sea otter, which has the densest fur of any animal, narrowly escaped the fate of the sea mink. The discovery of large populations in the North Pacific was the major economic driving force behind Russian expansion into Kamchatka, the Aleutian Islands, and Alaska, as well as a cause for conflict with Japan and foreign hunters in the Kuril Islands. Together with widespread hunting in California and British Columbia, the species was brought to the brink of extinction until an international moratorium came into effect in 1911.

Today, some mustelids are threatened for other reasons. Sea otters are vulnerable to oil spills and the indirect effects of overfishing; the black-footed ferret, a relative of the European polecat, suffers from the loss of American prairie; and wolverine populations are slowly declining because of habitat destruction and persecution. The rare European mink "Mustela lutreola" is one of the most endangered mustelid species.

One mustelid, the ferret, has been domesticated and is a fairly common pet.

The 56 living mustelids are classified into eight subfamilies in 22 genera.
Subfamily Taxidiinae

Subfamily Mellivorinae

Subfamily Melinae

Subfamily Helictidinae

Subfamily Guloninae

Subfamily Ictonychinae

Subfamily Lutrinae (otters)

Subfamily Mustelinae

Fossil mustelids
Extinct genera of the family Mustelidae include:


Multigene phylogenies constructed by Koepfli et al. (2008) and Law et al. (2018) found that Mustelidae comprises eight subfamilies. The early mustelids appear to have undergone two rapid bursts of diversification in Eurasia, with the resulting species only spreading to other continents later.

Mustelid species diversity is often attributed to an adaptive radiation coinciding with the Mid-Miocene Climate Transition. Contrary to expectations, Law et al. (2018) found no evidence for rapid bursts of lineage diversification at the origin of Mustelidae, and further analyses of lineage diversification rates using molecular and fossil-based methods did not find associations between rates of lineage diversification and Mid-Miocene Climate Transition as previously hypothesized.




</doc>
<doc id="18858" url="https://en.wikipedia.org/wiki?curid=18858" title="Maryland">
Maryland

Maryland ( ) is a state in the Mid-Atlantic region of the Southeastern United States, bordering Virginia, West Virginia, and the District of Columbia to its south and west; Pennsylvania to its north; and Delaware and the Atlantic Ocean to its east. The state's largest city is Baltimore, and its capital is Annapolis. Among its occasional nicknames are "Old Line State", the "Free State", and the "Chesapeake Bay State". It is named after the English queen Henrietta Maria, known in England as Queen Mary, who was the wife of King Charles I.

Sixteen of Maryland's twenty-three counties, as well as the city of Baltimore, border the tidal waters of the Chesapeake Bay estuary and its many tributaries, which combined total more than 4,000 miles of shoreline. Although one of the smallest states in the U.S., it features a variety of climates and topographical features that have earned it the moniker of "America in Miniature". In a similar vein, Maryland's geography, culture, and history combine elements of the Mid-Atlantic, Northeastern, and South Atlantic regions of the country.

Before its coastline was explored by Europeans in the 16th century, Maryland was inhabited by several groups of Native Americans, mostly by the Algonquin, and to a lesser degree by the Iroquois and Sioux. As one of the original Thirteen Colonies of Great Britain, Maryland was founded by George Calvert, a Catholic convert who sought to provide a religious haven for Catholics persecuted in England. In 1632, Charles I of England granted Calvert a colonial charter, naming the colony after his wife, Queen Mary (Henrietta Maria of France). Unlike the Pilgrims and Puritans, who rejected Catholicism in their settlements, Calvert envisioned a colony where people of different religious sects would coexist under the principle of toleration. Accordingly, in 1649 the Maryland General Assembly passed an Act Concerning Religion, which enshrined this principle by penalizing anyone who "reproached" a fellow Marylander based on religious affiliation. Nevertheless, religious strife was common in the early years, and Catholics remained a minority, albeit in greater numbers than in any other English colony.

Maryland's early settlements and population centers clustered around rivers and other waterways that empty into the Chesapeake Bay. Its economy was heavily plantation-based, centered mostly on the cultivation of tobacco. The need for cheap labor led to a rapid expansion of indentured servants, penal labor, and African slaves. In 1760, Maryland's current boundaries took form following the settlement of a long-running border dispute with Pennsylvania. Maryland was an active participant in the events leading up to the American Revolution, and by 1776 its delegates signed the Declaration of Independence. Many of its citizens subsequently played key political and military roles in the war. In 1790, the state ceded land for the establishment of the U.S. capital of Washington, D.C.

Although then a slave state, Maryland remained in the Union during the American Civil War, its strategic location giving it a significant role in the conflict. After the war, Maryland took part in the Industrial Revolution, driven by its seaports, railroad networks, and mass immigration from Europe. Since the Second World War, the state's population has grown rapidly, to approximately six million residents, and it is among the most densely populated U.S. states. , Maryland had the highest median household income of any state, owing in large part to its close proximity to Washington, D.C. and a highly diversified economy spanning manufacturing, services, higher education, and biotechnology. The state's central role in U.S. history is reflected by its hosting of some of the highest numbers of historic landmarks per capita.

Maryland has an area of and is comparable in overall area with Belgium []. It is the 42nd largest and 9th smallest state and is closest in size to the state of Hawaii [], the next smaller state. The next larger state, its neighbor West Virginia, is almost twice the size of Maryland [].

Maryland possesses a variety of topography within its borders, contributing to its nickname "America in Miniature". It ranges from sandy dunes dotted with seagrass in the east, to low marshlands teeming with wildlife and large bald cypress near the Chesapeake Bay, to gently rolling hills of oak forests in the Piedmont Region, and pine groves in the Maryland mountains to the west.

Maryland is bounded on its north by Pennsylvania, on its west by West Virginia, on its east by Delaware and the Atlantic Ocean, and on its south, across the Potomac River, by West Virginia and Virginia. The mid-portion of this border is interrupted by District of Columbia, which sits on land that was originally part of Montgomery and Prince George's counties and including the town of Georgetown, Maryland. This land was ceded to the United States Federal Government in 1790 to form the District of Columbia. (The Commonwealth of Virginia gave land south of the Potomac, including the town of Alexandria, Virginia, however Virginia retroceded its portion in 1846). The Chesapeake Bay nearly bisects the state and the counties east of the bay are known collectively as the "Eastern Shore".

Most of the state's waterways are part of the Chesapeake Bay watershed, with the exceptions of a tiny portion of extreme western Garrett County (drained by the Youghiogheny River as part of the watershed of the Mississippi River), the eastern half of Worcester County (which drains into Maryland's Atlantic coastal bays), and a small portion of the state's northeast corner (which drains into the Delaware River watershed). So prominent is the Chesapeake in Maryland's geography and economic life that there has been periodic agitation to change the state's official nickname to the "Bay State", a nickname that has been used by Massachusetts for decades.

The highest point in Maryland, with an elevation of , is Hoye Crest on Backbone Mountain, in the southwest corner of Garrett County, near the border with West Virginia, and near the headwaters of the North Branch of the Potomac River. Close to the small town of Hancock, in western Maryland, about two-thirds of the way across the state, there are between its borders. This geographical curiosity makes Maryland the narrowest state, bordered by the Mason–Dixon line to the north, and the northwards-arching Potomac River to the south.

Portions of Maryland are included in various official and unofficial geographic regions. For example, the Delmarva Peninsula is composed of the Eastern Shore counties of Maryland, the entire state of Delaware, and the two counties that make up the Eastern Shore of Virginia, whereas the westernmost counties of Maryland are considered part of Appalachia. Much of the Baltimore–Washington corridor lies just south of the Piedmont in the Coastal Plain, though it straddles the border between the two regions.

Earthquakes in Maryland are infrequent and small due to the state's distance from seismic/earthquake zones. The M5.8 Virginia earthquake in 2011 was felt moderately throughout Maryland. Buildings in the state are not well-designed for earthquakes and can suffer damage easily.

Maryland has no natural lakes, mostly due to the lack of glacial history in the area. All lakes in the state today were constructed, mostly via dams. Buckel's Bog is believed by geologists to have been a remnant of a former natural lake.

Maryland has shale formations containing natural gas, where fracking is theoretically possible.

As is typical of states on the East Coast, Maryland's plant life is abundant and healthy. A modest volume of annual precipitation helps to support many types of plants, including seagrass and various reeds at the smaller end of the spectrum to the gigantic Wye Oak, a huge example of white oak, the state tree, which can grow in excess of tall.

Middle Atlantic coastal forests, typical of the southeastern Atlantic coastal plain, grow around Chesapeake Bay and on the Delmarva Peninsula. Moving west, a mixture of Northeastern coastal forests and Southeastern mixed forests cover the central part of the state. The Appalachian Mountains of western Maryland are home to Appalachian-Blue Ridge forests. These give way to Appalachian mixed mesophytic forests near the West Virginia border.

Many foreign species are cultivated in the state, some as ornamentals, others as novelty species. Included among these are the crape myrtle, Italian cypress, southern magnolia, live oak in the warmer parts of the state, and even hardy palm trees in the warmer central and eastern parts of the state. USDA plant hardiness zones in the state range from Zones 5and6 in the extreme western part of the state to Zone7 in the central part, and Zone8 around the southern part of the coast, the bay area, and parts of metropolitan Baltimore. Invasive plant species, such as kudzu, tree of heaven, multiflora rose, and Japanese stiltgrass, stifle growth of endemic plant life. Maryland's state flower, the black-eyed susan, grows in abundance in wild flower groups throughout the state.

The state harbors a great number of white-tailed deer, especially in the woody and mountainous west of the state, and overpopulation can become a problem. Mammals can be found ranging from the mountains in the west to the central areas and include black bears, bobcats, foxes, coyotes, raccoons, and otters.

There is a population of rare wild (feral) horses found on Assateague Island. They are believed to be descended from horses who escaped from Spanish galleon shipwrecks. Every year during the last week of July, they are captured and swim across a shallow bay for sale at Chincoteague, Virginia, a conservation technique which ensures the tiny island is not overrun by the horses. The ponies and their sale were popularized by the children's book, "Misty of Chincoteague."

The purebred Chesapeake Bay Retriever dog was bred specifically for water sports, hunting and search and rescue in the Chesapeake area. In 1878 the Chesapeake Bay Retriever was the first individual retriever breed recognized by the American Kennel Club. and was later adopted by the University of Maryland, Baltimore County as their mascot.

Maryland's reptile and amphibian population includes the diamondback terrapin turtle, which was adopted as the mascot of University of Maryland, College Park, as well as the threatened Eastern box turtle. The state is part of the territory of the Baltimore oriole, which is the official state bird and mascot of the MLB team the Baltimore Orioles. Aside from the oriole, 435 other species of birds have been reported from Maryland.

The state insect is the Baltimore checkerspot butterfly, although it is not as common in Maryland as it is in the southern edge of its range.

Maryland joined with neighboring states during the end of the 20th century to improve the health of the Chesapeake Bay. The bay's aquatic life and seafood industry have been threatened by development and by fertilizer and livestock waste entering the bay.

In 2007, Forbes.com rated Maryland as the fifth "Greenest" state in the country behind three of the Pacific States and Vermont. Maryland ranks 40th in total energy consumption nationwide, and it managed less toxic waste per capita than all but six states in 2005. In April 2007 Maryland joined the Regional Greenhouse Gas Initiative (RGGI)—a regional initiative formed by all the Northeastern states, Washington D.C., and three Canadian provinces to reduce greenhouse gas emissions. In March 2017, Maryland became the first state with proven gas reserves to ban fracking by passing a law against it. Vermont has such a law, but no shale gas, and New York has such a ban, though it was made by executive order.

Maryland has a wide array of climates, due to local variances in elevation, proximity to water, and protection from colder weather due to downslope winds.

The eastern half of Maryland—which includes the cities of Ocean City, Salisbury, Annapolis, and the southern and eastern suburbs of Washington, D.C. and Baltimore—lies on the Atlantic Coastal Plain, with flat topography and sandy or muddy soil. This region has a humid subtropical climate (Köppen "Cfa"), with hot, humid summers and a short, mild to cool winter; it falls under USDA Hardiness zone 8a.

The Piedmont region—which includes northern and western greater Baltimore, Westminster, Gaithersburg, Frederick, and Hagerstown—has average seasonal snowfall totals generally exceeding and, as part of USDA Hardiness zones 7b and 7a, temperatures below are less rare. From the Cumberland Valley on westward, the climate begins to transition to a humid continental climate (Köppen "Dfa").

In western Maryland, the higher elevations of Allegany and Garrett counties—including the cities of Cumberland, Frostburg, and Oakland—display more characteristics of the humid continental zone, due in part to elevation. They fall under USDA Hardiness zones 6b and below.

Precipitation in the state is characteristic of the East Coast. Annual rainfall ranges from with more in higher elevations. Nearly every part of Maryland receives per month of rain. Average annual snowfall varies from in the coastal areas to over in the western mountains of the state.

Because of its location near the Atlantic Coast, Maryland is somewhat vulnerable to tropical cyclones, although the Delmarva Peninsula and the outer banks of North Carolina provide a large buffer, such that strikes from major hurricanes (category3 or above) occur infrequently. More often, Maryland gets the remnants of a tropical system which has already come ashore and released most of its energy. Maryland averages around 30–40 days of thunderstorms a year, and averages around six tornado strikes annually.

George Calvert, 1st Lord Baltimore (1579–1632), sought a charter from King Charles I for the territory between Massachusetts to the north and Virginia to the immediate south.
After the first Lord Baltimore died in April 1632, the charter was granted to his son, Cecilius Calvert, 2nd Baron Baltimore (1605–1675), on June 20, 1632. Officially, the new "Maryland Colony" was named in honor of Henrietta Maria of France, wife of Charles I of England. George Calvert initially proposed the name "Crescentia", the land of growth or increase, but "the King proposed Terra Mariae [Mary Land], which was concluded on and Inserted in the bill."

The original capital of Maryland was St. Mary's City, on the north shore of the Potomac River, and the county surrounding it, the first erected/created in the province, was first called Augusta Carolina, after the King, and later named St. Mary's County.

Lord Baltimore's first settlers arrived in the new colony in March 1634, with his younger brother Leonard Calvert (1606–1647), as first provincial Governor of Maryland. They made their first permanent settlement at St. Mary's City in what is now St. Mary's County. They purchased the site from the paramount chief of the region, who was eager to establish trade. St. Mary's became the first capital of Maryland, and remained so for 60 years until 1695. More settlers soon followed. Their tobacco crops were successful and quickly made the new colony profitable. However, given the incidence of malaria, yellow fever and typhoid, life expectancy in Maryland was about 10 years less than in New England.

Maryland was founded for the purpose of providing religious toleration of England's Roman Catholic minority.
Although Maryland was the most heavily Catholic of the England mainland colonies, this religious group was still in the minority, consisting of less than 10% of the total population.

In 1642 a number of Puritans left Virginia for Maryland and founded Providence (now called Annapolis) on the western shore of the upper Chesapeake Bay. A dispute with traders from Virginia over Kent Island in the Chesapeake led to armed conflict. In 1644 William Claiborne, a Puritan, seized Kent Island while his associate, the pro-Parliament Puritan Richard Ingle, took over St. Mary's. Both used religion as a tool to gain popular support. The two years from 1644 to 1646 when Claiborne and his Puritan associates held sway were known as "The Plundering Time". They captured Jesuit priests, imprisoned them, then sent them back to England.

In 1646 Leonard Calvert returned with troops, recaptured St. Mary's City, and restored order. The House of Delegates passed the "Act concerning Religion" in 1649 granting religious liberty to all Trinitarian Christians.

In 1650 the Puritans revolted against the proprietary government. "Protestants swept the Catholics out of the legislature... and religious strife returned." The Puritans set up a new government prohibiting both Roman Catholicism and Anglicanism. The Puritan revolutionary government persecuted Maryland Catholics during its reign, known as the "plundering time". Mobs burned down all the original Catholic churches of southern Maryland. The Puritan rule lasted until 1658 when the Calvert family and Lord Baltimore regained proprietary control and re-enacted the Toleration Act.

After England's "Glorious Revolution" of 1688, Maryland outlawed Catholicism. In 1704, the Maryland General Assembly prohibited Catholics from operating schools, limited the corporate ownership of property to hamper religious orders from expanding or supporting themselves, and encouraged the conversion of Catholic children. The celebration of the Catholic sacraments was also officially restricted. This state of affairs lasted until after the American Revolutionary War (1775–1783). Wealthy Catholic planters built chapels on their land to practice their religion in relative secrecy.

Into the 18th century, individual priests and lay leaders claimed Maryland farms belonging to the Jesuits as personal property and bequeathed them in order to evade the legal restrictions on religious organizations' owning property.

The royal charter granted Maryland the land north of the Potomac River up to the 40th parallel. A problem arose when Charles II granted a charter for Pennsylvania. The grant defined Pennsylvania's southern border as identical to Maryland's northern border, the 40th parallel. But the grant indicated that Charles II and William Penn assumed the 40th parallel would pass close to New Castle, Delaware when it falls north of Philadelphia, the site of which Penn had already selected for his colony's capital city. Negotiations ensued after the problem was discovered in 1681.

A compromise proposed by Charles II in 1682 was undermined by Penn's receiving the additional grant of what is now Delaware. Penn successfully argued that the Maryland charter entitled Lord Baltimore only to unsettled lands, and Dutch settlement in Delaware predated his charter. The dispute remained unresolved for nearly a century, carried on by the descendants of William Penn and Lord Baltimore — the Calvert family, which controlled Maryland, and the Penn family, which controlled Pennsylvania.

The border dispute with Pennsylvania led to Cresap's War in the 1730s. Hostilities erupted in 1730 and escalated through the first half of the decade, culminating in the deployment of military forces by Maryland in 1736 and by Pennsylvania in 1737. The armed phase of the conflict ended in May 1738 with the intervention of King George II, who compelled the negotiation of a cease-fire. A provisional agreement had been established in 1732.

Negotiations continued until a final agreement was signed in 1760. The agreement defined the border between Maryland and Pennsylvania as the line of latitude now known as the Mason–Dixon line. Maryland's border with Delaware was based on a Transpeninsular Line and the Twelve-Mile Circle around New Castle.

Most of the English colonists arrived in Maryland as indentured servants, and had to serve a several years' term as laborers to pay for their passage. In the early years, the line between indentured servants and African slaves or laborers was fluid, and white and black laborers commonly lived and worked together, and formed unions. Mixed-race children born to white mothers were considered free by the principle of "partus sequitur ventrem", by which children took the social status of their mothers, a principle of slave law that was adopted throughout the colonies, following Virginia in 1662. During the colonial era, families of free people of color were formed most often by unions of white women and African men.

Many of the free black families migrated to Delaware, where land was cheaper. As the flow of indentured laborers to the colony decreased with improving economic conditions in England, planters in Maryland imported thousands more slaves and racial caste lines hardened. The economy's growth and prosperity was based on slave labor, devoted first to the production of tobacco as the commodity crop.

Maryland was one of the thirteen colonies that revolted against British rule in the American Revolution. Near the end of the American Revolutionary War (1775–1783), on February 2, 1781, Maryland became the last and 13th state to approve the ratification of the Articles of Confederation and Perpetual Union, first proposed in 1776 and adopted by the Second Continental Congress in 1778, which brought into being the United States as a united, sovereign and national state. It also became the seventh state admitted to the Union after ratifying the new federal Constitution in 1788. In December 1790, Maryland donated land selected by first President George Washington to the federal government for the creation of the new national capital of Washington, D.C. The land was provided along the north shore of the Potomac River from Montgomery and Prince George's counties, as well as from Fairfax County and Alexandria on the south shore of the Potomac in Virginia; however, the land donated by the Commonwealth of Virginia was later returned to that state by the District of Columbia retrocession in 1846.

Influenced by a changing economy, revolutionary ideals, and preaching by ministers, numerous planters in Maryland freed their slaves in the 20 years after the Revolutionary War. Across the Upper South the free black population increased from less than 1% before the war to 14% by 1810. Abolitionist Harriet Tubman was born a slave during this time in Dorchester County, Maryland.

During the War of 1812, the British military attempted to capture Baltimore, which was protected by Fort McHenry. During this bombardment the song "Star Spangled Banner" was written by Francis Scott Key; it was later adopted as the national anthem.

The National Road (U.S. Hwy 40 today) was authorized in 1817 and ran from Baltimore to St. Louis—the first federal highway. The Baltimore and Ohio Railroad (B&O) was the first chartered railroad in the United States. It opened its first section of track for regular operation in 1830 between Baltimore and Ellicott City, and in 1852 it became the first rail line to reach the Ohio River from the eastern seaboard.

The state remained with the Union during the Civil War, due in significant part to demographics and Federal intervention. The 1860 census, held shortly before the outbreak of the civil war, showed that 49% of Maryland's African Americans were free blacks.
Governor Thomas Holliday Hicks suspended the state legislature, and to help ensure the election of a new pro-union governor and legislature, President Abraham Lincoln had a number of its pro-slavery politicians arrested, including the Mayor of Baltimore, George William Brown; suspended several civil liberties, including "habeas corpus"; and ordered artillery placed on Federal Hill overlooking Baltimore. Historians debate the constitutionality of these wartime actions, and the suspension of civil liberties was later deemed illegal by the U.S. Supreme Court.

In April 1861 Federal units and state regiments were attacked as they marched through Baltimore, sparking the Baltimore riot of 1861, the first bloodshed in the Civil War. Of the 115,000 men from Maryland who joined the military during the Civil War, 85,000, or 77%, joined the Union army, while the remainder joined the Confederate Army. The largest and most significant battle in the state was the Battle of Antietam on September 17, 1862, near Sharpsburg. Although a tactical draw, the battle was considered a strategic Union victory and a turning point of the war.

A new state constitution in 1864 abolished slavery and Maryland was first recognized as a "Free State" in that context. Following passage of constitutional amendments that granted voting rights to freedmen, in 1867 the state extended suffrage to non-white males.

The Democratic Party rapidly regained power in the state from Republicans. Democrats replaced the Constitution of 1864 with the Constitution of 1867. Following the end of Reconstruction in 1877, Democrats devised means of disfranchising blacks, initially by physical intimidation and voter fraud, later by constitutional amendments and laws. Blacks and immigrants, however, resisted Democratic Party disfranchisement efforts in the state. Maryland blacks were part of a biracial Republican coalition elected to state government in 1896–1904 and comprised 20% of the electorate.

Compared to some other states, blacks were better established both before and after the civil war. Nearly half the black population was free before the war, and some had accumulated property. Half the population lived in cities. Literacy was high among blacks and, as Democrats crafted means to exclude them, suffrage campaigns helped reach blacks and teach them how to resist. Whites did impose racial segregation in public facilities and Jim Crow laws, which effectively lasted until passage of federal civil rights legislation in the mid-1960s.

Baltimore grew significantly during the Industrial Revolution, due in large part to its seaport and good railroad connections, attracting European immigrant labor. Many manufacturing businesses were established in the Baltimore area after the Civil War. Baltimore businessmen, including Johns Hopkins, Enoch Pratt, George Peabody, and Henry Walters, founded notable city institutions that bear their names, including a university, library, music school and art museum.

Cumberland was Maryland's second-largest city in the 19th century. Nearby supplies of natural resources along with railroads fostered its growth into a major manufacturing center.

The Progressive Era of the late 19th and early 20th centuries brought political reforms. In a series of laws passed between 1892 and 1908, reformers worked for standard state-issued ballots (rather than those distributed and marked by the parties); obtained closed voting booths to prevent party workers from "assisting" voters; initiated primary elections to keep party bosses from selecting candidates; and had candidates listed without party symbols, which discouraged the illiterate from participating. These measures worked against ill-educated whites and blacks. Blacks resisted such efforts, with suffrage groups conducting voter education.
Blacks defeated three efforts to disfranchise them, making alliances with immigrants to resist various Democratic campaigns. Disfranchising bills in 1905, 1907, and 1911 were rebuffed, in large part because of black opposition. Blacks comprised 20% of the electorate and immigrants comprised 15%, and the legislature had difficulty devising requirements against blacks that did not also disadvantage immigrants.

The Progressive Era also brought reforms in working conditions for Maryland's labor force. In 1902 the state regulated conditions in mines; outlawed child laborers under the age of 12; mandated compulsory school attendance; and enacted the nation's first workers' compensation law. The workers' compensation law was overturned in the courts, but was redrafted and finally enacted in 1910.

The Great Baltimore Fire of 1904 burned for more than 30 hours, destroying 1,526 buildings and spanning 70 city blocks. More than 1,231 firefighters worked to bring the blaze under control.

With the nation's entry into World War I in 1917, new military bases such as Camp Meade, the Aberdeen Proving Ground, and the Edgewood Arsenal were established. Existing facilities, including Fort McHenry, were greatly expanded.

After Georgia congressman William D. Upshaw criticized Maryland openly in 1923 for not passing Prohibition laws, "Baltimore Sun" editor Hamilton Owens coined the "Free State" nickname for Maryland in that context, which was popularized by H. L. Mencken in a series of newspaper editorials.

Maryland's urban and rural communities had different experiences during the Great Depression. The "Bonus Army" marched through the state in 1932 on its way to Washington, D.C. Maryland instituted its first ever income tax in 1937 to generate revenue for schools and welfare.

Passenger and freight steamboat service, once important throughout Chesapeake Bay and its many tributary rivers, ended in 1962.

Baltimore was a major war production center during World War II. The biggest operations were Bethlehem Steel's Fairfield Yard, which built Liberty ships; and Glenn Martin, an aircraft manufacturer.

Maryland experienced population growth following World War II. Beginning in the 1960s, as suburban growth took hold around Washington DC and Baltimore, the state began to take on a more mid-Atlantic culture as opposed to the traditionally Southern and Tidewater culture that previously dominated most of the state. Agricultural tracts gave way to residential communities, some of them carefully planned such as Columbia, St. Charles, and Montgomery Village. Concurrently the Interstate Highway System was built throughout the state, most notably I-95, I-695, and the Capital Beltway, altering travel patterns. In 1952 the eastern and western halves of Maryland were linked for the first time by the Chesapeake Bay Bridge, which replaced a nearby ferry service.

Maryland's regions experienced economic changes following WWII. Heavy manufacturing declined in Baltimore. In Maryland's four westernmost counties, industrial, railroad, and coal mining jobs declined. On the lower Eastern Shore, family farms were bought up by major concerns and large-scale poultry farms and vegetable farming became prevalent. In Southern Maryland, tobacco farming nearly vanished due to suburban development and a state tobacco buy-out program in the 1990s.

In an effort to reverse depopulation due to the loss of working-class industries, Baltimore initiated urban renewal projects in the 1960s with Charles Center and the Baltimore World Trade Center. Some resulted in the break-up of intact residential neighborhoods, producing social volatility, and some older residential areas around the harbor have had units renovated and have become popular with new populations.

The United States Census Bureau estimates that the population of Maryland was 6,045,680 on July 1, 2019, a 4.71% increase since the 2010 United States Census and an increase of 2,962, from the prior year. This includes a natural increase since the last census of 269,166 (464,251 births minus 275,093 deaths) and an increase due to net migration of 116,713 people into the state. Immigration from outside the United States resulted in a net increase of 129,730 people, and migration within the country produced a net loss of 13,017 people.

The center of population of Maryland is located on the county line between Anne Arundel County and Howard County, in the unincorporated community of Jessup.

Maryland's history as a border state has led it to exhibit characteristics of both the Northern and the Southern regions of the United States. Generally, rural Western Maryland between the West Virginian Panhandle and Pennsylvania has an Appalachian culture; the Southern and Eastern Shore regions of Maryland embody a Southern culture,
while densely populated Central Maryland—radiating outward from Baltimore and Washington, D.C.—has more in common with that of the Northeast.
The U.S. Census Bureau designates Maryland as one of the South Atlantic States, but it is commonly associated with the Mid-Atlantic States and/or Northeastern United States by other federal agencies, the media, and some residents.

As of 2011, 58.0 percent of Maryland's population younger than age1 were minority background.

"Note: Births in table don't add up, because Hispanics are counted both by their ethnicity and by their race, giving a higher overall number."


Spanish (including Spanish Creole) is the second most spoken language in Maryland, after English. The third and fourth most spoken languages are French (including Patois and Cajun) and Chinese. Other commonly spoken languages include various African languages, Korean, German, Tagalog, Russian, Vietnamese, Italian, various Asian languages, Persian, Hindi and other Indic languages, Greek and Arabic.

Most of the population of Maryland lives in the central region of the state, in the Baltimore metropolitan area and Washington metropolitan area, both of which are part of the Baltimore–Washington metropolitan area.
The majority of Maryland's population is concentrated in the cities and suburbs surrounding Washington, D.C., as well as in and around Maryland's most populous city, Baltimore. Historically, these and many other Maryland cities developed along the Fall Line, the line along which rivers, brooks, and streams are interrupted by rapids and/or waterfalls. Maryland's capital city, Annapolis, is one exception to this pattern, since it lies along the banks of the Severn River, close to where it empties into the Chesapeake Bay.

The Eastern Shore is less populous and more rural, as are the counties of western Maryland. The two westernmost counties of Maryland, Allegany and Garrett, are mountainous and sparsely populated, resembling West Virginia and Appalachia more than they do the rest of the state. Both eastern and western Maryland are, however, dotted with cities of regional importance, such as Ocean City, Princess Anne, and Salisbury on the Eastern Shore and Cumberland, Frostburg, and Hancock in Western Maryland.
Southern Maryland is still somewhat rural, but suburbanization from Washington, D.C. has encroached significantly since the 1960s; important local population centers include Lexington Park, Prince Frederick, and Waldorf.

In 1970 the Census Bureau reported Maryland's population as 17.8 percent African-American and 80.4 percent non-Hispanic White.

African Americans form a sizable portion of the state's population, nearly 30 percent in 2010. Most are descendants of people transported to the area as slaves from West Africa, and many are of mixed race, including European and Native American ancestry. Concentrations of African Americans live in Baltimore City, Prince George's County, a suburb of Washington, D.C., where many work; Charles County, western parts of Baltimore County, and the southern Eastern Shore. New residents of African descent include 20th-century and later immigrants from Nigeria, particularly of the Igbo and Yoruba tribes. Maryland also hosts populations from other African and Caribbean nations. Many immigrants from the Horn of Africa have settled in Maryland, with large communities existing in the suburbs of Washington, D.C. (particularly Montgomery County and Prince George's County) and the city of Baltimore. The Greater Washington area has the largest population of Ethiopians outside of Africa. The Ethiopian community of Greater DC was historically based in Washington D.C.'s Adams Morgan and Shaw neighborhoods, but as the community has grown, many Ethiopians have settled in Silver Spring. The Washington, D.C. metropolitan area is also home to large Eritrean and Somali communities.

The top reported ancestries by Maryland residents are: German (15%), Irish (11%), English (8%), American (7%), Italian (6%), and Polish (3%).

Irish American populations can be found throughout the Baltimore area, and the Northern and Eastern suburbs of Washington D.C. in Maryland (descendants of those who moved out to the suburbs of Washington's once predominantly Irish neighborhoods), as well as Western Maryland, where Irish immigrant laborers helped to build the B & O Railroad. Smaller but much older Irish populations can be found in Southern Maryland, with some roots dating as far back as the early Maryland colony. This population, however, still remains culturally very active and yearly festivals are held.

A large percentage of the population of the Eastern Shore and Southern Maryland are descendants of British American ancestry. The Eastern Shore was settled by Protestants, chiefly Methodist and the southern counties were initially settled by English Catholics. Western and northern Maryland have large German-American populations. More recent European immigrants of the late 19th and early 20th century settled first in Baltimore, attracted to its industrial jobs. Many of their ethnic Italian, Polish, Czech, Lithuanian, and Greek descendants still live in the area.

Large ethnic minorities include Eastern Europeans such as Croatians, Belarusians, Russians and Ukrainians. The shares of European immigrants born in Eastern Europe increased significantly between 1990 and 2010. Following the dissolution of the Soviet Union, Yugoslavia, and Czechoslovakia, many immigrants from Eastern Europe came to the United States—12 percent of whom currently reside in Maryland.

Hispanic immigrants of the later 20th century have settled in Aspen Hill, Hyattsville/Langley Park, Glenmont/Wheaton, Bladensburg, Riverdale Park, Gaithersburg, as well as Highlandtown and Greektown in East Baltimore. Salvadorans are the largest Hispanic group in Maryland. Other Hispanic groups with significant populations in the state include Mexicans and Puerto Ricans and Hondurans. Though the Salvadoran population is more concentrated in the area around Washington, D.C., and the Puerto Rican population is more concentrated in the Baltimore area, all other major Hispanic groups in the state are evenly dispersed between these two areas. Maryland has one of the most diverse Hispanic populations in the country, with significant populations from various Caribbean and Central American nations.

Asian Americans are concentrated in the suburban counties surrounding Washington, D.C. and in Howard County, with Korean American and Taiwanese American communities in Rockville, Gaithersburg, and Germantown and a Filipino American community in Fort Washington. Numerous Indian Americans live across the state, especially in central Maryland.

Attracting educated Asians and Africans to the professional jobs in the region, Maryland has the fifth-largest proportions of racial minorities in the country.

In 2006 645,744 were counted as foreign born, which represents mainly people from Latin America and Asia. About four percent are undocumented immigrants. Maryland also has a large Korean American population. In fact, 1.7 percent are Korean, while as a whole, almost 6.0 percent are Asian.

According to The Williams Institute's analysis of the 2010 U.S. Census, 12,538 same-sex couples are living in Maryland, representing 5.8 same-sex couples per 1,000 households.

As of 2019, non-Hispanic white Americans were 50.0% of Maryland's population (White Americans, including White Hispanics, were 58.5%), making Maryland on the verge of becoming a majority minority state. 50.0% of Maryland's population is non-white and/or Hispanic/Latino, the highest percentage of any state on the East Coast and the highest percentage after the majority minority states of Hawaii, New Mexico, Texas, California and Nevada. Non-Hispanic White Americans in Maryland, the majority as of 2017, are expected to become the plurality ethnic group within five years of 2015. After Nevada in 2016, Maryland is projected to be the next state to become majority minority due to growing African-American, Asian and Latino populations. By 2031, minorities are projected to become the majority of voting eligible residents of Maryland.

Maryland has been historically prominent to American Catholic tradition because the English colony of Maryland was intended by George Calvert as a haven for English Catholics. Baltimore was the seat of the first Catholic bishop in the U.S. (1789), and Emmitsburg was the home and burial place of the first American-born citizen to be canonized, St. Elizabeth Ann Seton. Georgetown University, the first Catholic University, was founded in 1789 in what was then part of Maryland. The Basilica of the National Shrine of the Assumption of the Virgin Mary in Baltimore was the first Roman Catholic cathedral built in the United States, and the Archbishop of Baltimore is, albeit without formal primacy, the United States' quasi-primate, and often a cardinal. Among the immigrants of the 19th and 20th century from eastern and southern Europe were many Catholics.

Despite its historic relevance to the Catholic Church in the United States, the percentage of Catholics in the state of Maryland is below the national average of 20%. Demographically, both Protestants and those identifying no religion are more numerous than Catholics.

According to Pew research 69 percent of Maryland's population identifies as Christian. The largest religious groups in Maryland as of 2010 were the Catholic Church with 837,338 adherents in Maryland, followed by non-denominational Evangelical Protestants with 298,921 members, and the United Methodist Church with 238,774. The Southern Baptist Convention has 150,345 members. Amish/Mennonite communities are found in St. Mary's, Garrett, and Cecil counties. Judaism is the largest non-Christian religion in Maryland with 241,000 adherents, or four percent of the total population. Jews are numerous throughout Montgomery County and in Pikesville and Owings Mills northwest of Baltimore. An estimated 81,500 Jewish Americans live in Montgomery County, constituting approximately 10% of the total population. The Seventh-day Adventist Church's world headquarters and Ahmadiyya Muslims' national headquarters are located in Silver Spring, just outside the District of Columbia.

The Bureau of Economic Analysis estimates that Maryland's gross state product in 2016 was $382.4 billion. However, Maryland has been using Genuine Progress Indicator, an indicator of well-being, to guide the state's development, rather than relying only on growth indicators like GDP. According to the U.S. Census Bureau, Maryland households are currently the wealthiest in the country, with a 2013 median household income of $72,483 which puts it ahead of New Jersey and Connecticut, which are second and third respectively. Two of Maryland's counties, Howard and Montgomery, are the second and eleventh wealthiest counties in the nation respectively. Maryland has the most millionaires per capita in 2013, with a ratio of 7.7 percent. Also, the state's poverty rate of 7.8 percent is the lowest in the country. per capita personal income in 2006 was $43,500, fifth in the nation. As of February 2018, the state's unemployment rate was 4.2 percent.

Maryland's economy benefits from the state's close proximity to the federal government in Washington, D.C. with an emphasis on technical and administrative tasks for the defense/aerospace industry and bio-research laboratories, as well as staffing of satellite government headquarters in the suburban or exurban Baltimore/Washington area. Ft. Meade serves as the headquarters of the Defense Information Systems Agency, United States Cyber Command, and the National Security Agency/Central Security Service. In addition, a number of educational and medical research institutions are located in the state. In fact, the various components of The Johns Hopkins University and its medical research facilities are now the largest single employer in the Baltimore area. Altogether, white collar technical and administrative workers comprise 25 percent of Maryland's labor force, attributable in part to nearby Maryland being a part of the Washington Metro Area where the federal government office employment is relatively high.

Manufacturing, while large in dollar value, is highly diversified with no sub-sector contributing over 20 percent of the total. Typical forms of manufacturing include electronics, computer equipment, and chemicals. The once mighty primary metals sub-sector, which at one time included what was then the largest steel factory in the world at Sparrows Point, still exists, but is pressed with foreign competition, bankruptcies, and mergers. During World War II the Glenn Martin Company (now part of Lockheed Martin) airplane factory employed some 40,000 people.

Mining other than construction materials is virtually limited to coal, which is located in the mountainous western part of the state. The brownstone quarries in the east, which gave Baltimore and Washington much of their characteristic architecture in the mid-19th century, were once a predominant natural resource. Historically, there used to be small gold-mining operations in Maryland, some near Washington, but these no longer exist.

One major service activity is transportation, centered on the Port of Baltimore and its related rail and trucking access. The port ranked 17th in the U.S. by tonnage in 2008. Although the port handles a wide variety of products, the most typical imports are raw materials and bulk commodities, such as iron ore, petroleum, sugar, and fertilizers, often distributed to the relatively close manufacturing centers of the inland Midwest via good overland transportation. The port also receives several different brands of imported motor vehicles and is the number one auto port in the U.S.

Baltimore City is the eighth largest port in the nation, and was at the center of the February 2006 controversy over the Dubai Ports World deal because it was considered to be of such strategic importance. The state as a whole is heavily industrialized, with a booming economy and influential technology centers. Its computer industries are some of the most sophisticated in the United States, and the federal government has invested heavily in the area. Maryland is home to several large military bases and scores of high level government jobs.

The Chesapeake and Delaware Canal is a canal on the Eastern Shore that connects the waters of the Delaware River with those of the Chesapeake Bay, and in particular with the Port of Baltimore, carrying 40 percent of the port's ship traffic.

Maryland has a large food-production sector. A large component of this is commercial fishing, centered in the Chesapeake Bay, but also including activity off the short Atlantic seacoast. The largest catches by species are the blue crab, oysters, striped bass, and menhaden. The Bay also has overwintering waterfowl in its wildlife refuges. The waterfowl support a tourism sector of sportsmen.
Maryland has large areas of fertile agricultural land in its coastal and Piedmont zones, though this land use is being encroached upon by urbanization. Agriculture is oriented to dairy farming (especially in foothill and piedmont areas) for nearby large city milksheads plus specialty perishable horticulture crops, such as cucumbers, watermelons, sweet corn, tomatoes, muskmelons, squash, and peas (Source:USDA Crop Profiles). In addition, the southern counties of the western shoreline of Chesapeake Bay are warm enough to support a tobacco cash crop zone, which has existed since early Colonial times but declined greatly after a state government buyout in the 1990s. There is also a large automated chicken-farming sector in the state's southeastern part; Salisbury is home to Perdue Farms. Maryland's food-processing plants are the most significant type of manufacturing by value in the state.

Maryland is a major center for life sciences research and development. With more than 400 biotechnology companies located there, Maryland is the fourth-largest nexus in this field in the United States.

Institutions and government agencies with an interest in research and development located in Maryland include the Johns Hopkins University, the Johns Hopkins Applied Physics Laboratory, more than one campus of the University System of Maryland, Goddard Space Flight Center, the United States Census Bureau, the National Institutes of Health (NIH), the National Institute of Standards and Technology (NIST), the National Institute of Mental Health (NIMH), the Walter Reed National Military Medical Center, the federal Food and Drug Administration (FDA), the Howard Hughes Medical Institute, the Celera Genomics company, the J. Craig Venter Institute (JCVI), and AstraZeneca (formerly MedImmune).

Maryland is home to defense contractor Emergent BioSolutions, which manufactures and provides an anthrax vaccine to U.S. government military personnel.

Tourism is popular in Maryland, with tourists visiting the city of Baltimore, the beaches of the Eastern Shore, and the nature of western Maryland, as well as many passing through on the way to Washington, D.C. Baltimore attractions include the Harborplace, the Baltimore Aquarium, Fort McHenry, as well as the Camden Yards baseball stadium.
Ocean City on the Atlantic Coast has been a popular beach destination in summer, particularly since the Chesapeake Bay Bridge was built in 1952 connecting the Eastern Shore to the more populated Maryland cities. The state capital of Annapolis offers sites such as the state capitol building, the historic district, and the waterfront.
Maryland also has several sites of interest to military history, given Maryland's role in the American Civil War and in the War of 1812. Other attractions include the historic and picturesque towns along the Chesapeake Bay, such as Saint Mary's, Maryland's first colonial settlement and original capital.

As of 2017, the top two health insurers including all types of insurance were CareFirst BlueCross BlueShield with 47% market share followed by UnitedHealth Group at 15%.

Maryland has experimented with healthcare payment reforms, notably beginning in the 1970s with an all-payer rate setting program regulated by the Health Services Cost Review Commission. In 2014, it switched to a global budget revenue system, whereby hospitals receive a capitated payment to care for their population.

The Maryland Department of Transportation oversees most transportation in the state through its various administration-level agencies. The independent Maryland Transportation Authority maintains and operates the state's eight toll facilities.

Maryland's Interstate highways include of Interstate 95 (I-95), which enters the northeast portion of the state, travels through Baltimore, and becomes part of the eastern section of the Capital Beltway to the Woodrow Wilson Bridge. I-68 travels , connecting the western portions of the state to I-70 at the small town of Hancock. I-70 enters from Pennsylvania north of Hancock and continues east for to Baltimore, connecting Hagerstown and Frederick along the way.

I-83 has in Maryland and connects Baltimore to southern central Pennsylvania (Harrisburg and York, Pennsylvania). Maryland also has an portion of I-81 that travels through the state near Hagerstown. I-97, fully contained within Anne Arundel County and the shortest () one- or two-digit interstate highway in the contiguous US, connects the Baltimore area to the Annapolis area.

There are also several auxiliary Interstate highways in Maryland. Among them are two beltways encircling the major cities of the region: I-695, the McKeldin (Baltimore) Beltway, which encircles Baltimore; and a portion of I-495, the Capital Beltway, which encircles Washington, D.C. I-270, which connects the Frederick area with Northern Virginia and the District of Columbia through major suburbs to the northwest of Washington, is a major commuter route and is as wide as fourteen lanes at points. I-895, also known as the Harbor Tunnel Thruway, provides an alternate route to I-95 across the Baltimore Harbor.

Both I-270 and the Capital Beltway were extremely congested; however, the Intercounty Connector (ICC; MD 200) has alleviated some of the congestion over time. Construction of the ICC was a major part of the campaign platform of former Governor Robert Ehrlich, who was in office from 2003 until 2007, and of Governor Martin O'Malley, who succeeded him. I-595, which is an unsigned highway concurrent with US 50/US 301, is the longest unsigned interstate in the country and connects Prince George's County and Washington D.C. with Annapolis and the Eastern Shore via the Chesapeake Bay Bridge.

Maryland also has a state highway system that contains routes numbered from 2through 999, however most of the higher-numbered routes are either unsigned or are relatively short. Major state highways include Routes 2 (Governor Ritchie Highway/Solomons Island Road/Southern Maryland Blvd.), 4 (Pennsylvania Avenue/Southern Maryland Blvd./Patuxent Beach Road/St. Andrew's Church Road), 5 (Branch Avenue/Leonardtown Road/Point Lookout Road), 32, 45 (York Road), 97 (Georgia Avenue), 100 (Paul T. Pitcher Memorial Highway), 210 (Indian Head Highway), 235 (Three Notch Road), 295 (Baltimore-Washington Parkway), 355 (Wisconsin Avenue/Rockville Pike/Frederick Road), 404 (Queen Anne Highway/ Shore Highway), and 650 (New Hampshire Avenue).

Maryland's largest airport is Baltimore-Washington International Thurgood Marshall Airport, more commonly referred to as BWI. The airport is named for the Baltimore-born Thurgood Marshall, the first African-American Supreme Court justice. The only other airports with commercial service are at Hagerstown and Salisbury.

The Maryland suburbs of Washington, D.C. are also served by the other two airports in the region, Ronald Reagan Washington National Airport and Dulles International Airport, both in Northern Virginia. The College Park Airport is the nation's oldest, founded in 1909, and is still used. Wilbur Wright trained military aviators at this location.

Amtrak trains, including the high speed Acela Express serve Baltimore's Penn Station, BWI Airport, New Carrollton, and Aberdeen along the Washington D.C. to Boston Northeast Corridor. In addition, train service is provided to Rockville and Cumberland by Amtrak's Washington, D.C., to Chicago Capitol Limited.
The WMATA's Metrorail rapid transit and Metrobus local bus systems (the 2nd and 6th busiest in the nation of their respective modes) provide service in Montgomery and Prince George's counties and connect them to Washington D.C., with the express Metrobus "Route B30" serving BWI Airport. The Maryland Transit Administration (often abbreviated as "MTA Maryland"), a state agency part of the Maryland Department of Transportation also provides transit services within the state. Headquartered in Baltimore, MTA's transit services are largely focused on central Maryland, as well as some portions of the Eastern Shore and Southern MD. Baltimore's Light RailLink and Metro SubwayLink systems serve its densely populated inner-city and the surrounding suburbs. The MTA also serves the city and its suburbs with its local bus service (the 9th largest system in the nation). The MTA's Commuter Bus system provides express coach service on longer routes connecting Washington D.C. and Baltimore to parts of Central and Southern MD as well as the Eastern Shore. The commuter rail service, known as MARC, operates three lines which all terminate at Washington Union Station and provide service to Baltimore's Penn and Camden stations, Perryville, Frederick, and Martinsburg, WV. In addition, many suburban counties operate their own local bus systems which connect to and complement the larger MTA and WMATA/Metro services.

Freight rail transport is handled principally by two Class I railroads, as well as several smaller regional and local carriers. CSX Transportation has more extensive trackage throughout the state, with , followed by Norfolk Southern Railway. Major rail yards are located in Baltimore and Cumberland, with an intermodal terminal (rail, truck and marine) in Baltimore.

The government of Maryland is conducted according to the state constitution. The government of Maryland, like the other 49 state governments, has exclusive authority over matters that lie entirely within the state's borders, except as limited by the Constitution of the United States.

Power in Maryland is divided among three branches of government: executive, legislative, and judicial. The Maryland General Assembly is composed of the Maryland House of Delegates and the Maryland Senate. Maryland's governor is unique in the United States as the office is vested with significant authority in budgeting. The legislature may not increase the governor's proposed budget expenditures. Unlike many other states, significant autonomy is granted to many of Maryland's counties.

Most of the business of government is conducted in Annapolis, the state capital. Elections for governor and most statewide offices, as well as most county elections, are held in midterm-election years (even-numbered years not divisible by four).

The judicial branch of state government consists of one united District Court of Maryland that sits in every county and Baltimore City, as well as 24 Circuit Courts sitting in each County and Baltimore City, the latter being courts of general jurisdiction for all civil disputes over $30,000, all equitable jurisdiction and major criminal proceedings. The intermediate appellate court is known as the Court of Special Appeals and the state supreme court is the Court of Appeals. The appearance of the judges of the Maryland Court of Appeals is unique; Maryland is the only state whose judges wear red robes.

Maryland imposes five income tax brackets, ranging from 2to 6.25 percent of personal income. The city of Baltimore and Maryland's 23 counties levy local "piggyback" income taxes at rates between 1.25 and 3.2 percent of Maryland taxable income. Local officials set the rates and the revenue is returned to the local governments quarterly. The top income tax bracket of 9.45 percent is the fifth highest combined state and local income tax rates in the country, behind New York City's 11.35 percent, California's 10.3 percent, Rhode Island's 9.9 percent, and Vermont's 9.5 percent.

Maryland's state sales tax is six percent. All real property in Maryland is subject to the property tax. Generally, properties that are owned and used by religious, charitable, or educational organizations or property owned by the federal, state or local governments are exempt. Property tax rates vary widely. No restrictions or limitations on property taxes are imposed by the state, meaning cities and counties can set tax rates at the level they deem necessary to fund governmental services.

Since before the Civil War, Maryland's elections have been largely controlled by the Democrats, which account for 54.9% of all registered voters as of May 2017.

State elections are dominated by Baltimore and the populous suburban counties bordering Washington, D.C. and Baltimore: Montgomery, Prince George's, Anne Arundel, and Baltimore counties. As of July 2017, sixty-six percent of the state's population resides in these six jurisdictions, most of which contain large, traditionally Democratic voting bloc(s): African Americans in Baltimore City and Prince George's, federal employees in Prince George's, Anne Arundel, and Montgomery, and postgraduates in Montgomery. The remainder of the state, particularly Western Maryland and the Eastern Shore, is more supportive of Republicans. One of Maryland's best known political figures is a Republican—former governor Spiro Agnew, who pled no contest to tax evasion and resigned in 1973.

In 1980, Maryland was one of six states to vote for Jimmy Carter. In 1992, Bill Clinton fared better in Maryland than any other state except his home state of Arkansas. In 1996, Maryland was Clinton's sixth best; in 2000, Maryland ranked fourth for Gore; and in 2004, John Kerry showed his fifth-best performance in Maryland. In 2008, Barack Obama won the state's 10 electoral votes with 61.9 percent of the vote to John McCain's 36.5 percent.

In 2002, former Governor Robert Ehrlich was the first Republican to be elected to that office in four decades, and after one term lost his seat to Baltimore Mayor and Democrat Martin O'Malley. Ehrlich ran again for governor in 2010, losing again to O'Malley.

The 2006 election brought no change in the pattern of Democratic dominance. After Democratic Senator Paul Sarbanes announced that he was retiring, Democratic Congressman Benjamin Cardin defeated Republican Lieutenant Governor Michael S. Steele, with 55 percent of the vote, against Steele's 44 percent.

While Republicans usually win more counties, by piling up large margins in the west and east, they are also usually swamped by the more densely populated and heavily Democratic Baltimore–Washington axis. In 2008, for instance, McCain won 17 counties to Obama's six; Obama also carried Baltimore City. While McCain won most of the western and eastern counties by margins of 2-to-1 or more, he was almost completely shut out in the larger counties surrounding Baltimore and Washington; every large county except Anne Arundel went for Obama.

From 2007 to 2011, U.S. Congressman Steny Hoyer (MD-5), a Democrat, was elected as Majority Leader for the 110th Congress and 111th Congress of the House of Representatives, serving in that post again starting in 2019. In addition, Hoyer served as House Minority Whip from 2003 to 2006 and 2012 to 2018. His district covers parts of Anne Arundel and Prince George's counties, in addition to all of Charles, Calvert and St. Mary's counties in southern Maryland.

In 2010, Republicans won control of most counties. The Democratic Party remained in control of eight county governments including Baltimore.

In 2014, Larry Hogan, a Republican, was elected Governor of Maryland. Hogan is the second Republican to become the Governor of Maryland after Spiro Agnew, who resigned in 1969 to become vice president. In 2018, Hogan was reelected to a second term of office.

In February 2010, Attorney General Doug Gansler issued an opinion stating that Maryland law should honor same-sex marriages from out of state. At the time, the state Supreme Court wrote a decision upholding marriage discrimination.

On March 1, 2012, Maryland Governor Martin O'Malley signed the freedom to marry bill into law after it passed in the state legislature. Immediately after, opponents of same-sex marriage began collecting signatures to overturn the law. The law was scheduled to face a referendum, as Question 6, in the November 2012 election.

In May 2012, Maryland's Court of Appeals ruled that the state will recognize marriages of same-sex couples who married out-of-state, no matter the outcome of the November election.

Voters voted 52% to 48% for Question 6 on November 6, 2012. Same-sex couples began marrying in Maryland on January 1, 2013.

A large majority (57%) of Maryland voters said they would vote to uphold the freedom to marry at the ballot in November 2012, with 37% saying they would vote against marriage for all couples. This is consistent with a January 2011 Gonzales Research & Marketing Strategies poll showing 51% support for marriage in the state.

A well known newspaper is "The Baltimore Sun".

The most populous areas are served by either Baltimore or Washington, D.C. broadcast stations. The Eastern Shore is served primarily by broadcast media based around the Delmarva Peninsula; the northeastern section receives both Baltimore and Philadelphia stations. Garrett County, which is mountainous, is served by stations from Pittsburgh, and requires cable or satellite for reception. Maryland is served by state-wide PBS member station Maryland Public Television (MPT).

Education Week ranked Maryland #1 in its nationwide 2009–2013 Quality Counts reports. The College Board's 9th Annual AP Report to the Nation also ranked Maryland first. Primary and secondary education in Maryland is overseen by the Maryland State Department of Education, which is headquartered in Baltimore. The highest educational official in the state is the State Superintendent of Schools, who is appointed by the State Board of Education to a four-year term of office. The Maryland General Assembly has given the Superintendent and State Board autonomy to make educationally related decisions, limiting its own influence on the day-to-day functions of public education. Each county and county-equivalent in Maryland has a local Board of Education charged with running the public schools in that particular jurisdiction.

The budget for education was $5.5 billion in 2009, representing about 40 percent of the state's general fund.

Maryland has a broad range of private primary and secondary schools. Many of these are affiliated with various religious sects, including parochial schools of the Catholic Church, Quaker schools, Seventh-day Adventist schools, and Jewish schools. In 2003, Maryland law was changed to allow for the creation of publicly funded charter schools, although the charter schools must be approved by their local Board of Education and are not exempt from state laws on education, including collective bargaining laws.

In 2008, the state led the entire country in the percentage of students passing Advanced Placement examinations. 23.4 percent of students earned passing grades on the AP tests given in May 2008. This marks the first year that Maryland earned this honor. Three Maryland high schools (in Montgomery County) were ranked among the top 100 in the country by US News in 2009, based in large part on AP test scores.

Maryland has several historic and renowned private colleges and universities, the most prominent of which is Johns Hopkins University, founded in 1876 with a grant from Baltimore entrepreneur Johns Hopkins.

The first public university in the state is the University of Maryland, Baltimore, which was founded in 1807 and contains the University of Maryland's only public academic health, human services, and one of two law centers (the other being the University of Baltimore School of Law). Seven professional and graduate schools train the majority of the state's physicians, nurses, dentists, lawyers, social workers, and pharmacists. The flagship university and largest undergraduate institution in Maryland is the University of Maryland, College Park which was founded as the Maryland Agricultural College in 1856 and became a public land grant college in 1864. Towson University, founded in 1866, is the state's second largest university.

In 1974, Maryland, along with seven other states, mainly in the South, submitted plans to desegregate its state universities; Maryland's plans were approved by the U.S. Department of Health, Education and Welfare.

Baltimore is home to the University of Maryland, Baltimore County and the Maryland Institute College of Art. The majority of public universities in the state (Bowie State University, Coppin State University, Frostburg State University, Salisbury University and the University of Maryland-Eastern Shore) are affiliated with the University System of Maryland. Two state-funded institutions, Morgan State University and St. Mary's College of Maryland, as well as two federally funded institutions, the Uniformed Services University of the Health Sciences and the United States Naval Academy, are not affiliated with the University System of Maryland. The University of Maryland Global Campus is the largest public university in Maryland and one of the largest distance-learning institutions in the world.

St. John's College in Annapolis and Washington College in Chestertown, both private institutions, are the oldest colleges in the state and among the oldest in the country. Other private institutions include Mount St. Mary's University, McDaniel College (formerly known as Western Maryland College), Hood College, Stevenson University (formerly known as Villa Julie College), Loyola University Maryland, and Goucher College, among others.

Maryland's 24 public library systems deliver public education for everyone in the state of Maryland through a curriculum that comprises three pillars: Self-Directed Education (books and materials in all formats, e-resources), Research Assistance & Instruction (individualized research assistance, classes for students of all ages), and Instructive & Enlightening Experiences (e.g., Summer Reading Clubs, author events).

Maryland's library systems include, in part:

Many of the library systems have established formalized partnerships with other educational institutions in their counties and regions.

With two major metropolitan areas, Maryland has a number of major and minor professional sports franchises. Two National Football League teams play in Maryland, the Baltimore Ravens in Baltimore and the Washington Football Team in Landover. The Baltimore Colts represented the NFL in Baltimore from 1953 to 1983 before moving to Indianapolis.

The Baltimore Orioles are the state's Major League Baseball franchise. The National Hockey League's Washington Capitals and the National Basketball Association's Washington Wizards formerly played in Maryland, until the construction of an arena in Washington, D.C. in 1997 (now known as Capital One Arena).

Maryland enjoys considerable historical repute for the talented sports players of its past, including Cal Ripken Jr. and Babe Ruth. In 2012, "The Baltimore Sun" published a list of Maryland's top ten athletes in the state's history. The list includes Babe Ruth, Cal Ripken Jr, Johnny Unitas, Brooks Robinson, Frank Robinson, Ray Lewis, Michael Phelps, Jimmie Foxx, Jim Parker, and Wes Unseld.

Other professional sports franchises in the state include five affiliated minor league baseball teams, one independent league baseball team, the Baltimore Blast indoor soccer team, two indoor football teams, three low-level outdoor soccer teams, and the Chesapeake Bayhawks of Major League Lacrosse. Maryland is also home to one of the three races in horse racing's annual Triple Crown, the Preakness Stakes, which is run every spring at Pimlico Race Course in Baltimore.

The Congressional Country Club has hosted three golf tournaments for the U.S. Open and a PGA Championship.

The official state sport of Maryland, since 1962, is jousting; the official team sport since 2004 is lacrosse. The National Lacrosse Hall of Fame is located on the Johns Hopkins University campus in Baltimore. In 2008, intending to promote physical fitness for all ages, walking became the official state exercise. Maryland is the first state with an official state exercise.





</doc>
<doc id="18859" url="https://en.wikipedia.org/wiki?curid=18859" title="Michigan">
Michigan

Michigan () is a state in the Great Lakes and Midwestern regions of the United States. Its name comes from the Ojibwe word "mishigami", meaning "large water" or "large lake". With a population of approximately 10 million, Michigan is the tenth most populous of the 50 U.S. states, the 11th most extensive by area, and the largest by area east of the Mississippi River. Its capital is Lansing, and its largest city is Detroit. Metro Detroit is among the nation's most populous and largest metropolitan economies.

Michigan is the only state to consist of two peninsulas. The Lower Peninsula is shaped like a mitten. The Upper Peninsula (often called "the U.P.") is separated from the Lower Peninsula by the Straits of Mackinac, a channel that joins Lake Huron to Lake Michigan. The Mackinac Bridge connects the peninsulas. Michigan has the longest freshwater coastline of any political subdivision in the world, being bordered by four of the five Great Lakes, plus Lake Saint Clair. It also has 64,980 inland lakes and ponds.

The area was first occupied by a succession of Native American tribes over thousands of years. Inhabited by Natives, Métis, and French explorers in the 17th century, it was claimed as part of New France colony. After France's defeat in the French and Indian War in 1762, the region came under British rule. Britain ceded the territory to the newly independent United States after Britain's defeat in the American Revolutionary War. The area was part of the larger Northwest Territory until 1800, when western Michigan became part of the Indiana Territory. Michigan Territory was formed in 1805, but some of the northern border with Canada was not agreed upon until after the War of 1812. Michigan was admitted into the Union in 1837 as the 26th state, a free one. It soon became an important center of industry and trade in the Great Lakes region and a popular émigré destination in the late 19th and early 20th centuries; immigration from many European countries to Michigan was also the busiest at that time, especially for those who emigrated from Finland, Macedonia and the Netherlands.

Although Michigan developed a diverse economy, it is widely known as the center of the U.S. automotive industry, which developed as a major economic force in the early 20th century. It is home to the country's three major automobile companies (whose headquarters are all in Metro Detroit). While sparsely populated, the Upper Peninsula is important for tourism due to its abundance of natural resources, while the Lower Peninsula is a center of manufacturing, forestry, agriculture, services, and high-tech industry.

When the first European explorers arrived, the most populous tribes were Algonquian peoples, which include the Anishinaabe groups of Ojibwe (referred to as "Chippewa" in the United States), Odaawaa/Odawa (Ottawa), and the Boodewaadamii/Bodéwadmi (Potawatomi). The three nations co-existed peacefully as part of a loose confederation called the Council of Three Fires. The Ojibwe, whose numbers are estimated to have been between 25,000 and 35,000, were the largest.

The Ojibwe were established in Michigan's Upper Peninsula and northern and central Michigan, and also inhabited Ontario and southern Manitoba, Canada; and northern Wisconsin, and northern and north-central Minnesota. The Ottawa lived primarily south of the Straits of Mackinac in northern, western and southern Michigan, but also in southern Ontario, northern Ohio and eastern Wisconsin. The Potawatomi were in southern and western Michigan, in addition to northern and central Indiana, northern Illinois, southern Wisconsin, and southern Ontario. Other Algonquian tribes in Michigan, in the south and east, were the Mascouten, the Menominee, the Miami, the Sac (or Sauk), and the Meskwaki (Fox). The Wyandot were an Iroquoian-speaking people in this area; they were historically known as the Huron by the French.

French "voyageurs" and "coureurs des bois" explored and settled in Michigan in the 17th century. The first Europeans to reach what became Michigan were those of Étienne Brûlé's expedition in 1622. The first permanent European settlement was founded in 1668 on the site where Père Jacques Marquette established Sault Ste. Marie, Michigan, as a base for Catholic missions. Missionaries in 1671–75 founded outlying stations at Saint Ignace and Marquette. Jesuit missionaries were well received by the area's Indian populations, with few difficulties or hostilities. In 1679, Robert Cavelier, Sieur de la Salle built Fort Miami at present-day St. Joseph. In 1691, the French established a trading post and Fort St. Joseph along the St. Joseph River at the present-day city of Niles.

In 1701, French explorer and army officer Antoine de la Mothe Cadillac founded Fort Pontchartrain du Détroit or "Fort Pontchartrain on-the-Strait" on the strait, known as the Detroit River, between lakes Saint Clair and Erie. Cadillac had convinced King Louis XIV's chief minister, Louis Phélypeaux, Comte de Pontchartrain, that a permanent community there would strengthen French control over the upper Great Lakes and discourage British aspirations.

The hundred soldiers and workers who accompanied Cadillac built a fort enclosing one arpent (about , the equivalent of just under per side) and named it Fort Pontchartrain. Cadillac's wife, Marie Thérèse Guyon, soon moved to Detroit, becoming one of the first European women to settle in what was considered the wilderness of Michigan. The town quickly became a major fur-trading and shipping post. The "Église de Saint-Anne" (Church of Saint Ann) was founded the same year. While the original building does not survive, the congregation remains active. Cadillac later departed to serve as the French governor of Louisiana from 1710 to 1716. French attempts to consolidate the fur trade led to the Fox Wars involving the Meskwaki (Fox) and their allies versus the French and their Native allies.

At the same time, the French strengthened Fort Michilimackinac at the Straits of Mackinac to better control their lucrative fur-trading empire. By the mid-18th century, the French also occupied forts at present-day Niles and Sault Ste. Marie, though most of the rest of the region remained unsettled by Europeans. France offered free land to attract families to Detroit, which grew to 800 people in 1765, and was the largest city between Montreal and New Orleans. French settlers also established small farms south of the Detroit River opposite the fort, near a Jesuit mission and Huron village.
From 1660 until the end of French rule, Michigan was part of the Royal Province of New France. In 1760, Montreal fell to the British forces ending the French and Indian War (1754–1763). Under the 1763 Treaty of Paris, Michigan and the rest of New France east of the Mississippi River passed to Great Britain. After the Quebec Act was passed in 1774, Michigan became part of the British Province of Quebec. By 1778, Detroit's population was up to 2,144 and it was the third-largest city in Quebec.

During the American Revolutionary War, Detroit was an important British supply center. Most of the inhabitants were French-Canadians or Native Americans, many of whom had been allied with the French because of long trading ties. Because of imprecise cartography and unclear language defining the boundaries in the 1783 Treaty of Paris, the British retained control of Detroit and Michigan after the American Revolution. When Quebec split into Lower and Upper Canada in 1791, Michigan was part of Kent County, Upper Canada. It held its first democratic elections in August 1792 to send delegates to the new provincial parliament at Newark (now Niagara-on-the-Lake).

Under terms negotiated in the 1794 Jay Treaty, Britain withdrew from Detroit and Michilimackinac in 1796. It retained control of territory east and south of the Detroit River, which are now included in Ontario, Canada. Questions remained over the boundary for many years, and the United States did not have uncontested control of the Upper Peninsula and Drummond Island until 1818 and 1847, respectively.

During the War of 1812, the United States forces at Fort Detroit surrendered Michigan Territory (effectively consisting of Detroit and the surrounding area) after a nearly bloodless siege in 1812. A US attempt to retake Detroit resulted in a severe American defeat in the River Raisin Massacre. This battle, still ranked as the bloodiest ever fought in the state, had the highest number of American casualties of any battle in the war.

Michigan was recaptured by the Americans in 1813 after the Battle of Lake Erie. They used Michigan as a base to launch an invasion of Canada, which culminated in the Battle of the Thames. But the more northern areas of Michigan were held by the British until the peace treaty restored the old boundaries. A number of forts, including Fort Wayne, were built by the United States in Michigan during the 19th century out of fears of renewed fighting with Britain.
The population grew slowly until the opening in 1825 of the Erie Canal through the Mohawk Valley in New York, connecting the Great Lakes to the Hudson River and New York City. The new route attracted a large influx of settlers to the Michigan territory. They worked as farmers, lumbermen, shipbuilders, and merchants and shipped out grain, lumber, and iron ore. By the 1830s, Michigan had 80,000 residents, more than enough to apply and qualify for statehood.

A Constitutional Convention of Assent, led by Gershom Mott Williams, was held to lead the territory to statehood.
In October 1835 the people approved the Constitution of 1835, thereby forming a state government, although Congressional recognition was delayed pending resolution of a boundary dispute with Ohio known as the Toledo War. Congress awarded the "Toledo Strip" to Ohio. Michigan received the western part of the Upper Peninsula as a concession and formally entered the Union as a free state on January 26, 1837. The Upper Peninsula proved to be a rich source of lumber, iron, and copper. Michigan led the nation in lumber production from the 1850s to the 1880s. Railroads became a major engine of growth from the 1850s onward, with Detroit the chief hub.
A second wave of French-Canadian immigrants settled in Michigan during the late 19th to early 20th century, working in lumbering areas in counties on the Lake Huron side of the Lower Peninsula, such as the Saginaw Valley, Alpena, and Cheboygan counties, as well as throughout the Upper Peninsula, with large concentrations in Escanaba and the Keweenaw Peninsula. This was also a period of development of the gypsum industry in Alabaster, Michigan, which became nationally prominent.

The first statewide meeting of the Republican Party took place July 6, 1854, in Jackson, Michigan, where the party adopted its platform. The state was heavily Republican until the 1930s. Michigan made a significant contribution to the Union in the American Civil War and sent more than forty regiments of volunteers to the federal armies.

Modernizers and boosters set up systems for public education, including founding the University of Michigan (1817, moved to Ann Arbor in 1837) for a classical academic education; and Michigan State Normal School (1849), now Eastern Michigan University, for the training of teachers. It adopted this model from the German educational system. In 1899, Michigan State became the first normal college in the nation to offer a four-year curriculum. Michigan Agricultural College (1855), now Michigan State University in East Lansing, was founded as the pioneer land-grant college, a model for those authorized under the Morrill Act (1862). Many private colleges were founded as well, and the smaller cities established high schools late in the century.

Michigan's economy underwent a transformation at the turn of the 20th century. Many individuals, including Ransom E. Olds, John and Horace Dodge, Henry Leland, David Dunbar Buick, Henry Joy, Charles King, and Henry Ford, provided the concentration of engineering know-how and technological enthusiasm to develop the automotive industry. Ford's development of the moving assembly line in Highland Park marked a new era in transportation. Like the steamship and railroad, mass production of automobiles was a far-reaching development. More than the forms of public transportation, the affordable automobile transformed private life. Automobile production became the major industry of Detroit and Michigan, and permanently altered the socio-economic life of the United States and much of the world.

With the growth, the auto industry created jobs in Detroit that attracted immigrants from Europe and migrants from across the United States, including both blacks and whites from the rural South. By 1920, Detroit was the fourth-largest city in the US. Residential housing was in short supply, and it took years for the market to catch up with the population boom. By the 1930s, so many immigrants had arrived that more than 30 languages were spoken in the public schools, and ethnic communities celebrated in annual heritage festivals. Over the years immigrants and migrants contributed greatly to Detroit's diverse urban culture, including popular music trends. The influential Motown Sound of the 1960s was led by a variety of individual singers and groups.
Grand Rapids, the second-largest city in Michigan, is also an important center of manufacturing. Since 1838, the city has been noted for its furniture industry. In the 21st century, it is home to five of the world's leading office furniture companies. Grand Rapids is home to a number of major companies including Steelcase, Amway, and Meijer. Grand Rapids is also an important center for GE Aviation Systems.

Michigan held its first United States presidential primary election in 1910. With its rapid growth in industry, it was an important center of industry-wide union organizing, such as the rise of the United Auto Workers.

In 1920 WWJ (AM) in Detroit became the first radio station in the United States to regularly broadcast commercial programs. Throughout that decade, some of the country's largest and most ornate skyscrapers were built in the city. Particularly noteworthy are the Fisher Building, Cadillac Place, and the Guardian Building, each of which has been designated as a National Historic Landmark (NHL).

In 1927 a school bombing took place in Clinton County. The Bath School disaster, perpetrated by an adult man, resulted in the deaths of 38 schoolchildren and constitutes the deadliest mass murder in a school in U.S. history.

Michigan converted much of its manufacturing to satisfy defense needs during World War II; it manufactured 10.9 percent of the United States military armaments produced during the war, ranking second (behind New York) among the 48 states.

Detroit continued to expand through the 1950s, at one point doubling its population in a decade. After World War II, housing was developed in suburban areas outside city cores to meet demand for residences. The federal government subsidized the construction of interstate highways, which were intended to strengthen military access, but also allowed commuters and business traffic to travel the region more easily. Since 1960, modern advances in the auto industry have led to increased automation, high-tech industry, and increased suburban growth.

Michigan is the leading auto-producing state in the US, with the industry primarily located throughout the Midwestern United States; Ontario, Canada; and the Southern United States. With almost ten million residents, Michigan is a large and influential state, ranking tenth in population among the fifty states. Detroit is the centrally located metropolitan area of the Great Lakes Megalopolis and the second-largest metropolitan area in the U.S. (after Chicago) linking the Great Lakes system.
The Metro Detroit area in Southeast Michigan is the state's largest metropolitan area (roughly 50% of the population resides there) and the eleventh largest in the United States. The Grand Rapids metropolitan area in Western Michigan is the state's fastest-growing metro area, with more than 1.3 million residents . Metro Detroit receives more than 15 million visitors each year. Michigan has many popular tourist destinations, including areas such as Frankenmuth in The Thumb, and Traverse City on the Grand Traverse Bay in Northern Michigan. Tourists spend about $17 billion annually in Michigan supporting 193,000 jobs.

Michigan typically ranks third or fourth in overall Research & development (R&D) expenditures in the US. The state's leading research institutions include the University of Michigan, Michigan State University, and Wayne State University, which are important partners in the state's economy and the state's University Research Corridor. Michigan's public universities attract more than $1.5 B in research and development grants each year. Agriculture also serves a significant role, making the state a leading grower of fruit in the US, including blueberries, cherries, apples, grapes, and peaches.

The 2020 coronavirus pandemic in Michigan saw the state suffer one of the largest death tolls in the United States with the metro Detroit area particularly hard hit.

Michigan is governed as a republic, with three branches of government: the executive branch consisting of the Governor of Michigan and the other independently elected constitutional officers; the legislative branch consisting of the House of Representatives and Senate; and the judicial branch. The Michigan Constitution allows for the direct participation of the electorate by statutory initiative and referendum, recall, and constitutional initiative and referral (Article II, § 9, defined as "the power to propose laws and to enact and reject laws, called the initiative, and the power to approve or reject laws enacted by the legislature, called the referendum. The power of initiative extends only to laws which the legislature may enact under this constitution"). Lansing is the state capital and is home to all three branches of state government.

The governor and the other state constitutional officers serve four-year terms and may be re-elected only once. The current governor is Gretchen Whitmer. Michigan has two official Governor's Residences; one is in Lansing, and the other is at Mackinac Island. The other constitutionally elected executive officers are the lieutenant governor, who is elected on a joint ticket with the governor, the secretary of state, and the attorney general. The lieutenant governor presides over the Senate (voting only in case of a tie) and is also a member of the cabinet. The secretary of state is the chief elections officer and is charged with running many licensure programs including motor vehicles, all of which are done through the branch offices of the secretary of state.

The Michigan Legislature consists of a 38-member Senate and 110-member House of Representatives. Members of both houses of the legislature are elected through first past the post elections by single-member electoral districts of near-equal population that often have boundaries which coincide with county and municipal lines. Senators serve four-year terms concurrent to those of the governor, while representatives serve two-year terms. The Michigan State Capitol was dedicated in 1879 and has hosted the executive and legislative branches of the state ever since.

The Michigan judiciary consists of two courts with primary jurisdiction (the Circuit Courts and the District Courts), one intermediate level appellate court (the Michigan Court of Appeals), and the Michigan Supreme Court. There are several administrative courts and specialized courts. District courts are trial courts of limited jurisdiction, handling most traffic violations, small claims, misdemeanors, and civil suits where the amount contended is below $25,000. District courts are often responsible for handling the preliminary examination and for setting bail in felony cases. District court judges are elected to terms of six years. In a few locations, municipal courts have been retained to the exclusion of the establishment of district courts. There are 57 circuit courts in the State of Michigan, which have original jurisdiction over all civil suits where the amount contended in the case exceeds $25,000 and all criminal cases involving felonies. Circuit courts are also the only trial courts in the State of Michigan which possess the power to issue equitable remedies. Circuit courts have appellate jurisdiction from district and municipal courts, as well as from decisions and decrees of state agencies. Most counties have their own circuit court, but sparsely populated counties often share them. Circuit court judges are elected to terms of six years. State appellate court judges are elected to terms of six years, but vacancies are filled by an appointment by the governor. There are four divisions of the Court of Appeals in Detroit, Grand Rapids, Lansing, and Marquette. Cases are heard by the Court of Appeals by panels of three judges, who examine the application of the law and not the facts of the case unless there has been grievous error pertaining to questions of fact. The Michigan Supreme Court consists of seven members who are elected on non-partisan ballots for staggered eight-year terms. The Supreme Court has original jurisdiction only in narrow circumstances but holds appellate jurisdiction over the entire state judicial system.

Michigan has had four constitutions, the first of which was ratified on October5 and 6, 1835. There were also constitutions from 1850 and 1908, in addition to the current constitution from 1963. The current document has a preamble, 11 articles, and one section consisting of a schedule and temporary provisions. Michigan, like every U.S. state except Louisiana, has a common law legal system.

Michigan voters commonly elect candidates from both major parties, and it is generally regarded as a "swing" state which can be won by either Democratic or Republican presidential candidates. Governors since the 1970s have alternated between the two parties, and statewide offices including attorney general, secretary of state, and senator have been held by members of both parties in varying proportion. The Republican Party holds a majority in both the House and Senate of the Michigan Legislature. The state's Congressional delegation is commonly split, with one party or the other typically holding a narrow majority.

Republican strongholds of the state include rural areas of Western and Northern Michigan, the Upper Peninsula, the suburbs around Grand Rapids, and Livingston County. Areas of Democratic strength include the cities of Detroit, Ann Arbor, Lansing, Flint, urban Grand Rapids, and Muskegon. Much of suburban Detroit—which includes parts of Oakland, Macomb, and Wayne counties—is politically competitive between the two parties.

Historically, the first county-level meeting of the Republican Party took place in Jackson on July 6, 1854, and the party thereafter dominated Michigan until the Great Depression. In the 1912 election, Michigan was one of the six states to support progressive Republican and third-party candidate Theodore Roosevelt for president after he lost the Republican nomination to William Howard Taft.

Michigan remained fairly reliably Republican at the presidential level for much of the 20th century. It was part of Greater New England, the northern tier of states settled chiefly by migrants from New England who carried their culture with them. The state was one of only a handful to back Wendell Willkie over Franklin Roosevelt in 1940, and supported Thomas E. Dewey in his losing bid against Harry S. Truman in 1948. Michigan went to the Democrats in presidential elections during the 1960s and voted for the Republican candidate in every election from 1972 to 1988. Between 1992 and 2012 it supported the Democrats; early on in 2016, it was pegged as a swing state, and was narrowly won by the G.O.P. candidate, Donald Trump.

Michigan was the home of Gerald Ford, the 38th President of the United States. Born in Nebraska, he moved as an infant to Grand Rapids. The Gerald R. Ford Museum is in Grand Rapids, and the Gerald R. Ford Presidential Library is on the campus of his alma mater, the University of Michigan in Ann Arbor.

In 1846, Michigan became the first state in the Union as well as the first government in the world to abolish the death penalty. Historian David Chardavoyne has suggested the movement to abolish capital punishment in Michigan grew as a result of enmity toward the state's neighbor, Canada. Under British rule, it made public executions a regular practice.

Michigan has recognized and performed same-sex marriages since June 26, 2015, following the Supreme Court ruling in "Obergefell v. Hodges". Previously, such unions were prohibited under a 2004 state constitutional amendment.

Michigan has approved plans to expand Medicaid coverage in 2014 to adults with incomes up to 133% of the federal poverty level (approximately $15,500 for a single adult in 2014).

In 2018, the state electorate passed proposals to create an independent redistricting commission, and to legalize the recreational use of marijuana.

State government is decentralized among three tiers—statewide, county and township. Counties are administrative divisions of the state, and townships are administrative divisions of a county. Both of them exercise state government authority, localized to meet the particular needs of their jurisdictions, as provided by state law. There are 83 counties in Michigan.

Cities, state universities, and villages are vested with home rule powers of varying degrees. Home rule cities can generally do anything not prohibited by law. The fifteen state universities have broad power and can do anything within the parameters of their status as educational institutions that is not prohibited by the state constitution. Villages, by contrast, have limited home rule and are not completely autonomous from the county and township in which they are located.

There are two types of township in Michigan: "general law" township and "charter". Charter township status was created by the Legislature in 1947 and grants additional powers and stream-lined administration in order to provide greater protection against annexation by a city. , there were 127 charter townships in Michigan. In general, charter townships have many of the same powers as a city but without the same level of obligations. For example, a charter township can have its own fire department, water and sewer department, police department, and so on—just like a city—but it is not "required" to have those things, whereas cities "must" provide those services. Charter townships can opt to use county-wide services instead, such as deputies from the county sheriff's office instead of a home-based force of ordinance officers.

Michigan consists of two peninsulas separated by the Straits of Mackinac. The 45th parallel north runs through the state, marked by highway signs and the Polar-Equator Trail—along a line including Mission Point Light near Traverse City, the towns of Gaylord and Alpena in the Lower Peninsula and Menominee in the Upper Peninsula. With the exception of two tiny areas drained by the Mississippi River by way of the Wisconsin River in the Upper Peninsula and by way of the Kankakee-Illinois River in the Lower Peninsula, Michigan is drained by the Great Lakes-St. Lawrence watershed and is the only state with the majority of its land thus drained. No point in the state is more than from a natural water source or more than from a Great Lakes shoreline.
The Great Lakes that border Michigan from east to west are Lake Erie, Lake Huron, Lake Michigan and Lake Superior. The state is bounded on the south by the states of Ohio and Indiana, sharing land and water boundaries with both. Michigan's western boundaries are almost entirely water boundaries, from south to north, with Illinois and Wisconsin in Lake Michigan; then a land boundary with Wisconsin and the Upper Peninsula, that is principally demarcated by the Menominee and Montreal Rivers; then water boundaries again, in Lake Superior, with Wisconsin and Minnesota to the west, capped around by the Canadian province of Ontario to the north and east.
The heavily forested Upper Peninsula is relatively mountainous in the west. The Porcupine Mountains, which are part of one of the oldest mountain chains in the world, rise to an altitude of almost above sea level and form the watershed between the streams flowing into Lake Superior and Lake Michigan. The surface on either side of this range is rugged. The state's highest point, in the Huron Mountains northwest of Marquette, is Mount Arvon at . The peninsula is as large as Connecticut, Delaware, Massachusetts, and Rhode Island combined but has fewer than 330,000 inhabitants. They are sometimes called "Yoopers" (from "U.P.'ers"), and their speech (the "Yooper dialect") has been heavily influenced by the numerous Scandinavian and Canadian immigrants who settled the area during the lumbering and mining boom of the late 19th century.
The Lower Peninsula is shaped like a mitten and many residents hold up a hand to depict where they are from. It is long from north to south and from east to west and occupies nearly two-thirds of the state's land area. The surface of the peninsula is generally level, broken by conical hills and glacial moraines usually not more than a few hundred feet tall. It is divided by a low water divide running north and south. The larger portion of the state is on the west of this and gradually slopes toward Lake Michigan. The highest point in the Lower Peninsula is either Briar Hill at , or one of several points nearby in the vicinity of Cadillac. The lowest point is the surface of Lake Erie at .

The geographic orientation of Michigan's peninsulas makes for a long distance between the ends of the state. Ironwood, in the far western Upper Peninsula, lies by highway from Lambertville in the Lower Peninsula's southeastern corner. The geographic isolation of the Upper Peninsula from Michigan's political and population centers makes the U.P. culturally and economically distinct. Occasionally U.P. residents have called for secession from Michigan and establishment as a new state to be called "Superior".

A feature of Michigan that gives it the distinct shape of a mitten is the Thumb. This peninsula projects out into Lake Huron and the Saginaw Bay. The geography of the Thumb is mainly flat with a few rolling hills. Other peninsulas of Michigan include the Keweenaw Peninsula, making up the Copper Country region of the state. The Leelanau Peninsula lies in the Northern Lower Michigan region. "See Also Michigan Regions"
Numerous lakes and marshes mark both peninsulas, and the coast is much indented. Keweenaw Bay, Whitefish Bay, and the Big and Little Bays De Noc are the principal indentations on the Upper Peninsula. The Grand and Little Traverse, Thunder, and Saginaw bays indent the Lower Peninsula. Michigan has the second longest shoreline of any state—, including of island shoreline.
The state has numerous large islands, the principal ones being the North Manitou and South Manitou, Beaver, and Fox groups in Lake Michigan; Isle Royale and Grande Isle in Lake Superior; Marquette, Bois Blanc, and Mackinac islands in Lake Huron; and Neebish, Sugar, and Drummond islands in St. Mary's River. Michigan has about 150 lighthouses, the most of any U.S. state. The first lighthouses in Michigan were built between 1818 and 1822. They were built to project light at night and to serve as a landmark during the day to safely guide the passenger ships and freighters traveling the Great Lakes. See Lighthouses in the United States.

The state's rivers are generally small, short and shallow, and few are navigable. The principal ones include the Detroit River, St. Marys River, and St. Clair River which connect the Great Lakes; the Au Sable, Cheboygan, and Saginaw, which flow into Lake Huron; the Ontonagon, and Tahquamenon, which flow into Lake Superior; and the St. Joseph, Kalamazoo, Grand, Muskegon, Manistee, and Escanaba, which flow into Lake Michigan. The state has 11,037 inland lakes—totaling of inland water—in addition to of Great Lakes waters. No point in Michigan is more than from an inland lake or more than from one of the Great Lakes.

The state is home to several areas maintained by the National Park Service including: Isle Royale National Park, in Lake Superior, about southeast of Thunder Bay, Ontario. Other national protected areas in the state include: Keweenaw National Historical Park, Pictured Rocks National Lakeshore, Sleeping Bear Dunes National Lakeshore, Huron National Forest, Manistee National Forest, Hiawatha National Forest, Ottawa National Forest and Father Marquette National Memorial. The largest section of the North Country National Scenic Trail passes through Michigan.

With 78 state parks, 19 state recreation areas, and six state forests, Michigan has the largest state park and state forest system of any state. These parks and forests include Holland State Park, Mackinac Island State Park, Au Sable State Forest, and Mackinaw State Forest.

Michigan has a continental climate, although there are two distinct regions. The southern and central parts of the Lower Peninsula (south of Saginaw Bay and from the Grand Rapids area southward) have a warmer climate (Köppen climate classification "Dfa") with hot summers and cold winters. The northern part of Lower Peninsula and the entire Upper Peninsula has a more severe climate (Köppen "Dfb"), with warm, but shorter summers and longer, cold to very cold winters. Some parts of the state average high temperatures below freezing from December through February, and into early March in the far northern parts. During the winter through the middle of February, the state is frequently subjected to heavy lake-effect snow. The state averages from of precipitation annually; however, some areas in the northern lower peninsula and the upper peninsula average almost of snowfall per year. Michigan's highest recorded temperature is at Mio on July 13, 1936, and the coldest recorded temperature is at Vanderbilt on February 9, 1934.

The state averages 30 days of thunderstorm activity per year. These can be severe, especially in the southern part of the state. The state averages 17 tornadoes per year, which are more common in the state's extreme southern section. Portions of the southern border have been almost as vulnerable historically as states further west and in Tornado Alley. For this reason, many communities in the very southern portions of the state have tornado sirens to warn residents of approaching tornadoes. Farther north, in Central Michigan, Northern Michigan, and the Upper Peninsula, tornadoes are rare.

The geological formation of the state is greatly varied, with the Michigan Basin being the most major formation. Primary boulders are found over the entire surface of the Upper Peninsula (being principally of primitive origin), while Secondary deposits cover the entire Lower Peninsula. The Upper Peninsula exhibits Lower Silurian sandstones, limestones, copper and iron bearing rocks, corresponding to the Huronian system of Canada. The central portion of the Lower Peninsula contains coal measures and rocks of the Pennsylvanian period. Devonian and sub-Carboniferous deposits are scattered over the entire state.

Michigan rarely experiences earthquakes, thus far mostly smaller ones that do not cause significant damage. A 4.6-magnitude earthquake struck in August 1947. More recently, a 4.2-magnitude earthquake occurred on Saturday, May 2, 2015, shortly after noon, about five miles south of Galesburg, Michigan (9 miles southeast of Kalamazoo) in central Michigan, about 140 miles west of Detroit, according to the Colorado-based U.S. Geological Survey's National Earthquake Information Center. No major damage or injuries were reported, according to Governor Rick Snyder's office.

The United States Census Bureau estimates the population of Michigan was 9,986,857 on July 1, 2019, an increase of 1.04% from 9,883,635 recorded at the 2010 United States Census. This includes an natural increase since the last census of 177,254 people (i.e., 1,043,213 births minus 865,959 deaths) and an decrease due to net migration of 73,053 people. Immigration resulted in a net increase of 193,031 people, and migration from within the U.S. resulted in a net decrease of 266,084 people.

The center of population of Michigan is in Shiawassee County, in the southeastern corner of the civil township of Bennington, which is northwest of the village of Morrice.

As of the 2010 American Community Survey for the U.S. Census, the state had a foreign-born population of 592,212, or 6.0% of the total. Michigan has the largest Dutch, Finnish, and Macedonian populations in the United States.

The 2010 Census reported:

In the same year Hispanics or Latinos (of any race) made up 4.4% of the population.

The large majority of Michigan's population is Caucasian. Americans of European descent live throughout Michigan and most of Metro Detroit. Large European American groups include those of German, British, Irish, Polish and Belgian ancestry. People of Scandinavian descent, and those of Finnish ancestry, have a notable presence in the Upper Peninsula. Western Michigan is known for the Dutch heritage of many residents (the highest concentration of any state), especially in Holland and metropolitan Grand Rapids.

African-Americans, who came to Detroit and other northern cities in the Great Migration of the early 20th century, form a majority of the population of the city of Detroit and of other cities, including Flint and Benton Harbor.

, almost 8,000 Hmong people lived in the State of Michigan, about double their 1999 presence in the state. most lived in northeastern Detroit, but they had been increasingly moving to Pontiac and Warren. By 2015 the number of Hmong in the Detroit city limits had significantly declined. Lansing hosts a statewide Hmong New Year Festival. The Hmong community also had a prominent portrayal in the 2008 film "Gran Torino", which was set in Detroit.

, 80% of Michigan's Japanese population lived in the counties of Macomb, Oakland, Washtenaw, and Wayne in the Detroit and Ann Arbor areas. , the largest Japanese national population is in Novi, with 2,666 Japanese residents, and the next largest populations are respectively in Ann Arbor, West Bloomfield Township, Farmington Hills, and Battle Creek. The state has 481 Japanese employment facilities providing 35,554 local jobs. 391 of them are in Southeast Michigan, providing 20,816 jobs, and the 90 in other regions in the state provide 14,738 jobs. The Japanese Direct Investment Survey of the Consulate-General of Japan, Detroit stated more than 2,208 additional Japanese residents were employed in the State of Michigan , than in 2011. During the 1990s the Japanese population of Michigan experienced an increase, and many Japanese people with children moved to particular areas for their proximity to Japanese grocery stores and high-performing schools.

A person from Michigan is called a Michigander or Michiganian; also at times, but rarely, a "Michiganite". Residents of the Upper Peninsula are sometimes referred to as "Yoopers" (a phonetic pronunciation of "U.P.ers"), and they sometimes refer to those from the Lower Peninsula as "trolls" because they live below the bridge (see Three Billy Goats Gruff).

, 34.3% of Michigan's children under the age of one belonged to racial or ethnic minority groups, meaning they had at least one parent who was not non-Hispanic white.

"Note: Percentages in the table can exceed 100% as Hispanics are counted both by their ethnicity and by their race."


, 91.11% (8,507,947) of Michigan residents age five and older spoke only English at home, while 2.93% (273,981) spoke Spanish, 1.04% (97,559) Arabic, 0.44% (41,189) German, 0.36% (33,648) Chinese (which includes Mandarin), 0.31% (28,891) French, 0.29% (27,019) Polish, and Syriac languages (such as Modern Aramaic and Northeastern Neo-Aramaic) was spoken as a main language by 0.25% (23,420) of the population over the age of five. In total, 8.89% (830,281) of Michigan's population age five and older spoke a mother language other than English.

The Roman Catholic Church has six dioceses and one archdiocese in Michigan; Gaylord, Grand Rapids, Kalamazoo, Lansing, Marquette, Saginaw and Detroit. The Roman Catholic Church is the largest denomination by number of adherents, according to the Association of Religion Data Archives (ARDA) 2010 survey, with 1,717,296 adherents. The Roman Catholic Church was the only organized religion in Michigan until the 19th century, reflecting the territory's French colonial roots. Detroit's Saint Anne's parish, established in 1701 by Antoine de la Mothe Cadillac, is the second-oldest Roman Catholic parish in the United States. On March 8, 1833, the Holy See formally established a diocese in the Michigan territory, which included all of Michigan, Wisconsin, Minnesota, and the Dakotas east of the Mississippi River. When Michigan became a state in 1837, the boundary of the Diocese of Detroit was redrawn to coincide with that of the State; the other dioceses were later carved out from the Diocese of Detroit but remain part of the Ecclesiastical Province of Detroit.

In 2010, the largest Protestant denominations were the United Methodist Church with 228,521 adherents; followed by the Lutheran Church–Missouri Synod with 219,618, and the Evangelical Lutheran Church in America with 120,598 adherents. The Christian Reformed Church in North America had almost 100,000 members and more than 230 congregations in Michigan. The Reformed Church in America had 76,000 members and 154 congregations in the state. In the same survey, Jewish adherents in the state of Michigan were estimated at 44,382, and Muslims at 120,351. The Lutheran Church was introduced by German and Scandinavian immigrants; Lutheranism is the second largest religious denomination in the state. The first Jewish synagogue in the state was Temple Beth El, founded by twelve German Jewish families in Detroit in 1850. In West Michigan, Dutch immigrants fled from the specter of religious persecution and famine in the Netherlands around 1850 and settled in and around what is now Holland, Michigan, establishing a "colony" on American soil that fervently held onto Calvinist doctrine that established a significant presence of Reformed churches. Islam was introduced by immigrants from the Near East during the 20th century. Michigan is home to the largest mosque in North America, the Islamic Center of America in Dearborn. Battle Creek, Michigan, is also the birthplace of the Seventh-day Adventist Church, which was founded on May 21, 1863.

In 2017, 3,859,949 people in Michigan were employed at 222,553 establishments, according to the U.S. Census Bureau.

The U.S. Bureau of Economic Analysis estimated Michigan's Q3 2018 gross state product to be $538 billion, ranking 14th out of the 50 states. According to the Bureau of Labor Statistics, , the state's seasonally adjusted unemployment rate was estimated at 4.0%.

Products and services include automobiles, food products, information technology, aerospace, military equipment, furniture, and mining of copper and iron ore. Michigan is the third leading grower of Christmas trees with of land dedicated to Christmas tree farming. The beverage Vernors was invented in Michigan in 1866, sharing the title of oldest soft drink with Hires Root Beer. Faygo was founded in Detroit on November 4, 1907. Two of the top four pizza chains were founded in Michigan and are headquartered there: Domino's Pizza by Tom Monaghan and Little Caesars Pizza by Mike Ilitch. Michigan became the 24th right-to-work state in U.S. in 2012.

Since 2009, GM, Ford and Chrysler have managed a significant reorganization of their benefit funds structure after a volatile stock market which followed the September 11 attacks and early 2000s recession impacted their respective U.S. pension and benefit funds (OPEB). General Motors, Ford, and Chrysler reached agreements with the United Auto Workers Union to transfer the liabilities for their respective health care and benefit funds to a 501(c)(9) Voluntary Employee Beneficiary Association (VEBA). Manufacturing in the state grew 6.6% from 2001 to 2006, but the high speculative price of oil became a factor for the U.S. auto industry during the economic crisis of 2008 impacting industry revenues. In 2009, GM and Chrysler emerged from Chapter 11 restructurings with financing provided in part by the U.S. and Canadian governments. GM began its initial public offering (IPO) of stock in 2010. For 2010, the Big Three domestic automakers have reported significant profits indicating the beginning of rebound.

, Michigan ranked fourth in the U.S. in high tech employment with 568,000 high tech workers, which includes 70,000 in the automotive industry. Michigan typically ranks third or fourth in overall research and development (R&D) expenditures in the United States. Its research and development, which includes automotive, comprises a higher percentage of the state's overall gross domestic product than for any other U.S. state. The state is an important source of engineering job opportunities. The domestic auto industry accounts directly and indirectly for one of every ten jobs in the U.S.

Michigan was second in the U.S. in 2004 for new corporate facilities and expansions. From 1997 to 2004, Michigan was the only state to top the 10,000 mark for the number of major new developments; however, the effects of the late 2000s recession have slowed the state's economy. In 2008, Michigan placed third in a site selection survey among the states for luring new business which measured capital investment and new job creation per one million population. In August 2009, Michigan and Detroit's auto industry received $1.36 B in grants from the U.S. Department of Energy for the manufacture of electric vehicle technologies which is expected to generate 6,800 immediate jobs and employ 40,000 in the state by 2020. From 2007 to 2009, Michigan ranked 3rd in the U.S. for new corporate facilities and expansions.

As leading research institutions, the University of Michigan, Michigan State University, and Wayne State University are important partners in the state's economy and its University Research Corridor. Michigan's public universities attract more than $1.5 B in research and development grants each year. The National Superconducting Cyclotron Laboratory is at Michigan State University. Michigan's workforce is well-educated and highly skilled, making it attractive to companies. It has the third highest number of engineering graduates nationally.

Detroit Metropolitan Airport is one of the nation's most recently expanded and modernized airports with six major runways, and large aircraft maintenance facilities capable of servicing and repairing a Boeing 747 and is a major hub for Delta Air Lines. Michigan's schools and colleges rank among the nation's best. The state has maintained its early commitment to public education. The state's infrastructure gives it a competitive edge; Michigan has 38 deep water ports. In 2007, Bank of America announced that it would commit $25 billion to community development in Michigan following its acquisition of LaSalle Bank in Troy.

Michigan led the nation in job creation improvement in 2010.

Michigan's personal income tax is a flat rate of 4.25%. In addition, 22 cities impose income taxes; rates are set at 1% for residents and 0.5% for non-residents in all but four cities. Michigan's state sales tax is 6%, though items such as food and medication are exempted. Property taxes are assessed on the local level, but every property owner's local assessment contributes six mills (a rate of $6 per $1000 of property value) to the statutory State Education Tax. Property taxes are appealable to local boards of review and need the approval of the local electorate to exceed millage rates prescribed by state law and local charters. In 2011, the state repealed its business tax and replaced it with a 6% corporate income tax which substantially reduced taxes on business. Article IX of the Constitution of the State of Michigan also provides limitations on how much the state can tax.

A 6% use tax is levied on goods purchased outside the state (that are brought in and used in state), at parity with the sales tax. The use tax applies to internet sales/purchases from outside Michigan and is equivalent to the sales tax.

A wide variety of commodity crops, fruits, and vegetables are grown in Michigan, making it second only to California among U.S. states in the diversity of its agriculture. The state has 54,800 farms utilizing of land which sold $6.49 billion worth of products in 2010. The most valuable agricultural product is milk. Leading crops include corn, soybeans, flowers, wheat, sugar beets, and potatoes. Livestock in the state included 78,000 sheep, a million cattle, a million hogs, and more than three million chickens. Livestock products accounted for 38% of the value of agricultural products while crops accounted for the majority.

Michigan is a leading grower of fruit in the U.S., including blueberries, tart cherries, apples, grapes, and peaches. Plums, pears, and strawberries are also grown in Michigan. These fruits are mainly grown in West Michigan due to the moderating effect of Lake Michigan on the climate. There is also significant fruit production, especially cherries, but also grapes, apples, and other fruits, in Northwest Michigan along Lake Michigan. Michigan produces wines, beers and a multitude of processed food products. Kellogg's cereal is based in Battle Creek, Michigan and processes many locally grown foods. Thornapple Valley, Ball Park Franks, Koegel Meat Company, and Hebrew National sausage companies are all based in Michigan.

Michigan is home to very fertile land in the Saginaw Valley and Thumb areas. Products grown there include corn, sugar beets, navy beans, and soybeans. Sugar beet harvesting usually begins the first of October. It takes the sugar factories about five months to process the 3.7 million tons of sugarbeets into 485,000 tons of pure, white sugar. Michigan's largest sugar refiner, Michigan Sugar Company is the largest east of the Mississippi River and the fourth largest in the nation. Michigan sugar brand names are Pioneer Sugar and the newly incorporated Big Chief Sugar. Potatoes are grown in Northern Michigan, and corn is dominant in Central Michigan. Alfalfa, cucumbers, and asparagus are also grown.

Michigan's tourists spend $17.2 billion per year in the state, supporting 193,000 tourism jobs. Michigan's tourism website ranks among the busiest in the nation. Destinations draw vacationers, hunters, and nature enthusiasts from across the United States and Canada. Michigan is fifty percent forest land, much of it quite remote. The forests, lakes and thousands of miles of beaches are top attractions. Event tourism draws large numbers to occasions like the Tulip Time Festival and the National Cherry Festival.
In 2006, the Michigan State Board of Education mandated all public schools in the state hold their first day of school after the Labor Day holiday, in accordance with the new Post Labor Day School law. A survey found 70% of all tourism business comes directly from Michigan residents, and the Michigan Hotel, Motel, & Resort Association claimed the shorter summer in between school years cut into the annual tourism season in the state.

Tourism in metropolitan Detroit draws visitors to leading attractions, especially The Henry Ford, the Detroit Institute of Arts, the Detroit Zoo, and to sports in Detroit. Other museums include the Detroit Historical Museum, the Charles H. Wright Museum of African American History, museums in the Cranbrook Educational Community, and the Arab American National Museum. The metro area offers four major casinos, MGM Grand Detroit, Greektown, Motor City, and Caesars Windsor in Windsor, Ontario, Canada; moreover, Detroit is the largest American city and metropolitan region to offer casino resorts.
Hunting and fishing are significant industries in the state. Charter boats are based in many Great Lakes cities to fish for salmon, trout, walleye, and perch. Michigan ranks first in the nation in licensed hunters (over one million) who contribute $2 billion annually to its economy. More than three-quarters of a million hunters participate in white-tailed deer season alone. Many school districts in rural areas of Michigan cancel school on the opening day of firearm deer season, because of attendance concerns.

Michigan's Department of Natural Resources manages the largest dedicated state forest system in the nation. The forest products industry and recreational users contribute $12 billion and 200,000 associated jobs annually to the state's economy. Public hiking and hunting access has also been secured in extensive commercial forests. The state has the highest number of golf courses and registered snowmobiles in the nation.

The state has numerous historical markers, which can themselves become the center of a tour. The Great Lakes Circle Tour is a designated scenic road system connecting all of the Great Lakes and the St. Lawrence River.

With its position in relation to the Great Lakes and the countless ships that have foundered over the many years they have been used as a transport route for people and bulk cargo, Michigan is a world-class scuba diving destination. The Michigan Underwater Preserves are 11 underwater areas where wrecks are protected for the benefit of sport divers.

Michigan has nine international road crossings with Ontario, Canada:

The Gordie Howe International Bridge, a second international bridge between Detroit and Windsor, is under construction. It is expected to be completed in 2024.

Michigan is served by four Class I railroads: the Canadian National Railway, the Canadian Pacific Railway, CSX Transportation, and the Norfolk Southern Railway. These are augmented by several dozen short line railroads. The vast majority of rail service in Michigan is devoted to freight, with Amtrak and various scenic railroads the exceptions.
Amtrak passenger rail services the state, connecting many southern and western Michigan cities to Chicago, Illinois. There are plans for commuter rail for Detroit and its suburbs (see SEMCOG Commuter Rail).


The Detroit Metropolitan Airport in the western suburb of Romulus, was in 2010 the 16th busiest airfield in North America measured by passenger traffic. The Gerald R. Ford International Airport in Grand Rapids is the next busiest airport in the state, served by eight airlines to 23 destinations. Flint Bishop International Airport is the third largest airport in the state, served by four airlines to several primary hubs. Cherry Capital Airport is in Traverse City. Alpena County Regional Airport services Alpena and the northeastern lower peninsula. MBS International Airport serves Midland, Bay City and Saginaw. Smaller regional and local airports are located throughout the state including on several islands.

Other economically significant cities include:

Half the wealthiest communities in the state are in Oakland County, just north of Detroit. Another wealthy community is just east of the city, in Grosse Pointe. Only three of these cities are outside of Metro Detroit. The city of Detroit, with a per capita income of $14,717, ranks 517th on the list of Michigan locations by per capita income. Benton Harbor is the poorest city in Michigan, with a per capita income of $8,965, while Barton Hills is the richest with a per capita income of $110,683.

Michigan's education system provides services to 1.6 million K-12 students in public schools. More than 124,000 students attend private schools and an uncounted number are home-schooled under certain legal requirements. The public school system has a $14.5 billion budget in 2008–2009. Michigan has a number of public universities spread throughout the state and numerous private colleges as well. Michigan State University has the eighth largest campus population of any U.S. school. Seven of the state's universities (Central Michigan University, University of Michigan, Michigan State University, Michigan Technological University, Oakland University, Wayne State University, and Western Michigan University) are classified as research universities by the Carnegie Foundation.

From 2009 to 2019 over 200 private schools in Michigan closed, partly due to competition from charter schools.

Michigan music is known for three music trends: early punk rock, Motown/soul music and techno music. Michigan musicians include Bill Haley & His Comets, The Supremes, The Marvelettes, The Temptations, The Four Tops, Stevie Wonder, Marvin Gaye "The Prince of Soul", Smokey Robinson and the Miracles, Aretha Franklin, Mary Wells, Tommy James and the Shondells, ? and the Mysterians, Al Green, The Spinners, Grand Funk Railroad, The Stooges, the MC5, The Knack, Madonna "The Queen of Pop", Bob Seger, Ray Parker Jr., Aaliyah, Eminem, Kid Rock, Jack White and Meg White (The White Stripes), Big Sean, Alice Cooper, and Del Shannon.

Major theaters in Michigan include the Fox Theatre, Music Hall, Gem Theatre, Masonic Temple Theatre, the Detroit Opera House, Fisher Theatre, The Fillmore Detroit, Saint Andrew's Hall, Majestic Theater, and Orchestra Hall.

The Nederlander Organization, the largest controller of Broadway productions in New York City, originated in Detroit. Detroit Symphony Orchestra

Motown Motion Picture Studios with produces movies in Detroit and the surrounding area based at the Pontiac Centerpoint Business Campus.

Michigan's major-league sports teams include: Detroit Tigers baseball team, Detroit Lions football team, Detroit Red Wings ice hockey team, and the Detroit Pistons men's basketball team. All of Michigan's major league teams play in the Metro Detroit area.

The Pistons played at Detroit's Cobo Arena until 1978 and at the Pontiac Silverdome until 1988 when they moved into The Palace of Auburn Hills. In 2017, the team moved to the newly built Little Caesars Arena in downtown Detroit. The Detroit Lions played at Tiger Stadium in Detroit until 1974, then moved to the Pontiac Silverdome where they played for 27 years between 1975 and 2002 before moving to Ford Field in Detroit in 2002. The Detroit Tigers played at Tiger Stadium (formerly known as Navin Field and Briggs Stadium) from 1912 to 1999. In 2000 they moved to Comerica Park. The Red Wings played at Olympia Stadium before moving to Joe Louis Arena in 1979. They later moved to Little Caesars Arena to join the Pistons as tenants in 2017. Professional hockey got its start in Houghton, when the Portage Lakers were formed.

The Michigan International Speedway is the site of NASCAR races and Detroit was formerly the site of a Formula One World Championship Grand Prix race. From 1959 to 1961, Detroit Dragway hosted the NHRA's U.S. Nationals. Michigan is home to one of the major canoeing marathons: the Au Sable River Canoe Marathon. The Port Huron to Mackinac Boat Race is also a favorite.

Twenty-time Grand Slam champion Serena Williams was born in Saginaw. The 2011 World Champion for Women's Artistic Gymnastics, Jordyn Wieber is from DeWitt. Wieber was also a member of the gold medal team at the London Olympics in 2012.

Collegiate sports in Michigan are popular in addition to professional sports. The state's two largest athletic programs are the Michigan Wolverines and Michigan State Spartans, which play in the NCAA Big Ten Conference. Michigan Stadium in Ann Arbor, home to the Michigan Wolverines football team, is the largest stadium in the Western Hemisphere and the second-largest stadium worldwide behind Rungrado May Day Stadium in Pyongyang, North Korea.

The Michigan High School Athletic Association features around 300,000 participants.

Michigan is traditionally known as "The Wolverine State", and the University of Michigan takes the wolverine as its mascot. The association is well and long established: for example, many Detroiters volunteered to fight during the American Civil War and George Armstrong Custer, who led the Michigan Brigade, called them the "Wolverines". The origins of this association are obscure; it may derive from a busy trade in wolverine furs in Sault Ste. Marie in the 18th century or may recall a disparagement intended to compare early settlers in Michigan with the vicious mammal. Wolverines are, however, extremely rare in Michigan. A sighting in February 2004 near Ubly was the first confirmed sighting in Michigan in 200 years. The animal was found dead in 2010.




</doc>
<doc id="18862" url="https://en.wikipedia.org/wiki?curid=18862" title="Minimum wage">
Minimum wage

A minimum wage is the lowest remuneration that employers can legally pay their workers—the price floor below which workers may not sell their labor. Most countries had introduced minimum wage legislation by the end of the 20th century.

Supply and demand models suggest that there may be welfare and employment losses from minimum wages. However, if the labor market is in a state of monopsony (with only one employer available who is hiring), minimum wages can increase the efficiency of the market. There is debate about the full effects of minimum wages.

The movement for minimum wages was first motivated as a way to stop the exploitation of workers in sweatshops, by employers who were thought to have unfair bargaining power over them. Over time, minimum wages came to be seen as a way to help lower-income families. Modern national laws enforcing compulsory union membership which prescribed minimum wages for their members were first passed in New Zealand and Australia in the 1890s.

Although minimum wage laws are in effect in many jurisdictions, differences of opinion exist about the benefits and drawbacks of a minimum wage. Supporters of the minimum wage say it increases the standard of living of workers, reduces poverty, reduces inequality, and boosts morale. In contrast, opponents of the minimum wage say it increases poverty, increases unemployment because some low-wage workers "will be unable to find work...[and] will be pushed into the ranks of the unemployed" and is damaging to businesses, because excessively high minimum wages require businesses to raise the prices of their product or service to accommodate the extra expense of paying a higher wage.

Modern minimum wage laws trace their origin to the Ordinance of Labourers (1349), which was a decree by King Edward III that set a" maximum wage" for laborers in medieval England. King Edward III, who was a wealthy landowner, was dependent, like his lords, on serfs to work the land. In the autumn of 1348, the Black Plague reached England and decimated the population. The severe shortage of labor caused wages to soar and encouraged King Edward III to set a wage ceiling. Subsequent amendments to the ordinance, such as the Statute of Labourers (1351), increased the penalties for paying a wage above the set rates.

While the laws governing wages initially set a ceiling on compensation, they were eventually used to set a living wage. An amendment to the Statute of Labourers in 1389 effectively fixed wages to the price of food. As time passed, the Justice of the Peace, who was charged with setting the maximum wage, also began to set formal minimum wages. The practice was eventually formalized with the passage of the Act Fixing a Minimum Wage in 1604 by King James I for workers in the textile industry.

By the early 19th century, the Statutes of Labourers was repealed as increasingly capitalistic England embraced "laissez-faire" policies which disfavored regulations of wages (whether upper or lower limits). The subsequent 19th century saw significant labor unrest affect many industrial nations. As trade unions were decriminalized during the century, attempts to control wages through collective agreement were made. However, this meant that a uniform minimum wage was not possible. In "Principles of Political Economy" in 1848, John Stuart Mill argued that because of the collective action problems that workers faced in organisation, it was a justified departure from "laissez-faire" policies (or freedom of contract) to regulate people's wages and hours by the law.

It was not until the 1890s that the first modern legislative attempts to regulate minimum wages were seen in New Zealand and Australia. The movement for a minimum wage was initially focused on stopping sweatshop labor and controlling the proliferation of sweatshops in manufacturing industries. The sweatshops employed large numbers of women and young workers, paying them what were considered to be substandard wages. The sweatshop owners were thought to have unfair bargaining power over their employees, and a minimum wage was proposed as a means to make them pay fairly. Over time, the focus changed to helping people, especially families, become more self-sufficient.

The first modern national minimum wages were enacted by the government recognition of unions which in turn established minimum wage policy among their members, as in New Zealand in 1894, followed by Australia in 1896 and the United Kingdom in 1909. In the United States, statutory minimum wages were first introduced nationally in 1938, and they were reintroduced and expanded in the United Kingdom in 1998. There is now legislation or binding collective bargaining regarding minimum wage in more than 90 percent of all countries. In the European Union, 22 member states out of 28 currently have national minimum wages. Other countries, such as Sweden, Finland, Denmark, Switzerland, Austria, and Italy, have no minimum wage laws, but rely on employer groups and trade unions to set minimum earnings through collective bargaining.

Minimum wage rates vary greatly across many different jurisdictions, not only in setting a particular amount of money—for example $7.25 per hour ($14,500 per year) under certain US state laws (or $2.13 for employees who receive tips, which is known as the tipped minimum wage), $11.00 in the US state of Washington, or £8.72 (for those aged 25+) in the United Kingdom—but also in terms of which pay period (for example Russia and China set monthly minimum wages) or the scope of coverage. Currently the United States federal minimum wage is $7.25 per hour. However, some states do not recognize the minimum wage law, such as Louisiana and Tennessee. Other states operate below the federal minimum wage such as Georgia and Wyoming. Some jurisdictions allow employers to count tips given to their workers as credit towards the minimum wage levels. India was one of the first developing countries to introduce minimum wage policy in its law in 1948. However, it is rarely implemented, even by contractors of government agencies. In Mumbai, as of 2017, the minimum wage was Rs. 348/day.
India also has one of the most complicated systems with more than 1,200 minimum wage rates depending on the geographical region.

Customs and extra-legal pressures from governments or labor unions can produce a "de facto" minimum wage. So can international public opinion, by pressuring multinational companies to pay Third World workers wages usually found in more industrialized countries. The latter situation in Southeast Asia and Latin America was publicized in the 2000s, but it existed with companies in West Africa in the middle of the 20th century.

Among the indicators that might be used to establish an initial minimum wage rate are ones that minimize the loss of jobs while preserving international competitiveness. Among these are general economic conditions as measured by real and nominal gross domestic product; inflation; labor supply and demand; wage levels, distribution and differentials; employment terms; productivity growth; labor costs; business operating costs; the number and trend of bankruptcies; economic freedom rankings; standards of living and the prevailing average wage rate.

In the business sector, concerns include the expected increased cost of doing business, threats to profitability, rising levels of unemployment (and subsequent higher government expenditure on welfare benefits raising tax rates), and the possible knock-on effects to the wages of more experienced workers who might already be earning the new statutory minimum wage, or slightly more. Among workers and their representatives, political considerations weigh in as labor leaders seek to win support by demanding the highest possible rate. Other concerns include purchasing power, inflation indexing and standardized working hours.

In the United States, the minimum wage have been set under the Fair Labor Standards Act of 1938. According to the Economic Policy Institute, the minimum wage in the United States would have been $18.28 in 2013 if the minimum wage had kept pace with labor productivity. To adjust for increased rates of worker productivity in the United States, raising the minimum wage to $22 (or more) an hour has been presented.

According to the supply and demand model of the labor market shown in many economics textbooks, increasing the minimum wage decreases the employment of minimum-wage workers. One such textbook states:
A firm's cost is an increasing function of the wage rate. The higher the wage rate, the fewer hours an employer will demand of employees. This is because, as the wage rate rises, it becomes more expensive for firms to hire workers and so firms hire fewer workers (or hire them for fewer hours). The demand of labor curve is therefore shown as a line moving down and to the right. Since higher wages increase the quantity supplied, the supply of labor curve is upward sloping, and is shown as a line moving up and to the right. If no minimum wage is in place, wages will adjust until quantity of labor demanded is equal to quantity supplied, reaching equilibrium, where the supply and demand curves intersect. Minimum wage behaves as a classical price floor on labor. Standard theory says that, if set above the equilibrium price, more labor will be willing to be provided by workers than will be demanded by employers, creating a surplus of labor, i.e. unemployment. The economic model of markets predicts the same of other commodities (like milk and wheat, for example): Artificially raising the price of the commodity tends to cause an increase in quantity supplied and a decrease in quantity demanded. The result is a surplus of the commodity. When there is a wheat surplus, the government buys it. Since the government does not hire surplus labor, the labor surplus takes the form of unemployment, which tends to be higher with minimum wage laws than without them.

The supply and demand model implies that by mandating a price floor above the equilibrium wage, minimum wage laws will cause unemployment. This is because a greater number of people are willing to work at the higher wage while a smaller number of jobs will be available at the higher wage. Companies can be more selective in those whom they employ thus the least skilled and least experienced will typically be excluded. An imposition or increase of a minimum wage will generally only affect employment in the low-skill labor market, as the equilibrium wage is already at or below the minimum wage, whereas in higher skill labor markets the equilibrium wage is too high for a change in minimum wage to affect employment.

The supply and demand model predicts that raising the minimum wage helps workers whose wages are raised, and hurts people who are not hired (or lose their jobs) when companies cut back on employment. But proponents of the minimum wage hold that the situation is much more complicated than the model can account for. One complicating factor is possible monopsony in the labor market, whereby the individual employer has some market power in determining wages paid. Thus it is at least theoretically possible that the minimum wage may boost employment. Though single employer market power is unlikely to exist in most labor markets in the sense of the traditional 'company town,' asymmetric information, imperfect mobility, and the personal element of the labor transaction give some degree of wage-setting power to most firms.

Modern economic theory predicts that although an excessive minimum wage may raise unemployment as it fixes a price above most demand for labor, a minimum wage at a more reasonable level can increase employment, and enhance growth and efficiency. This is because labor markets are monopsonistic and workers persistently lack bargaining power. When poorer workers have more to spend it stimulates effective aggregate demand for goods and services.

The argument that a minimum wage decreases employment is based on a simple supply and demand model of the labor market. A number of economists (for example Pierangelo Garegnani, Robert L. Vienneau, and Arrigo Opocher & Ian Steedman), building on the work of Piero Sraffa, argue that that model, even given all its assumptions, is logically incoherent. Michael Anyadike-Danes and Wynne Godley argue, based on simulation results, that little of the empirical work done with the textbook model constitutes a potentially falsifiable theory, and consequently empirical evidence hardly exists for that model. Graham White argues, partially on the basis of Sraffianism, that the policy of increased labor market flexibility, including the reduction of minimum wages, does not have an "intellectually coherent" argument in economic theory.
Gary Fields, Professor of Labor Economics and Economics at Cornell University, argues that the standard textbook model for the minimum wage is ambiguous, and that the standard theoretical arguments incorrectly measure only a one-sector market. Fields says a two-sector market, where "the self-employed, service workers, and farm workers are typically excluded from minimum-wage coverage... [and with] one sector with minimum-wage coverage and the other without it [and possible mobility between the two]," is the basis for better analysis. Through this model, Fields shows the typical theoretical argument to be ambiguous and says "the predictions derived from the textbook model definitely do not carry over to the two-sector case. Therefore, since a non-covered sector exists nearly everywhere, the predictions of the textbook model simply cannot be relied on."

An alternate view of the labor market has low-wage labor markets characterized as monopsonistic competition wherein buyers (employers) have significantly more market power than do sellers (workers). This monopsony could be a result of intentional collusion between employers, or naturalistic factors such as segmented markets, search costs, information costs, imperfect mobility and the personal element of labor markets. In such a case a simple supply and demand graph would not yield the quantity of labor clearing and the wage rate. This is because while the upward sloping aggregate labor supply would remain unchanged, instead of using the upward labor supply curve shown in a supply and demand diagram, monopsonistic employers would use a steeper upward sloping curve corresponding to marginal expenditures to yield the intersection with the supply curve resulting in a wage rate lower than would be the case under competition. Also, the amount of labor sold would also be lower than the competitive optimal allocation.

Such a case is a type of market failure and results in workers being paid less than their marginal value. Under the monopsonistic assumption, an appropriately set minimum wage could increase both wages and employment, with the optimal level being equal to the marginal product of labor. This view emphasizes the role of minimum wages as a market regulation policy akin to antitrust policies, as opposed to an illusory "free lunch" for low-wage workers.

Another reason minimum wage may not affect employment in certain industries is that the demand for the product the employees produce is highly inelastic. For example, if management is forced to increase wages, management can pass on the increase in wage to consumers in the form of higher prices. Since demand for the product is highly inelastic, consumers continue to buy the product at the higher price and so the manager is not forced to lay off workers. Economist Paul Krugman argues this explanation neglects to explain why the firm was not charging this higher price absent the minimum wage.

Three other possible reasons minimum wages do not affect employment were suggested by Alan Blinder: higher wages may reduce turnover, and hence training costs; raising the minimum wage may "render moot" the potential problem of recruiting workers at a higher wage than current workers; and minimum wage workers might represent such a small proportion of a business's cost that the increase is too small to matter. He admits that he does not know if these are correct, but argues that "the list demonstrates that one can accept the new empirical findings and still be a card-carrying economist."

The following mathematical models are more quantitative in orientation, and highlight some of the difficulties in determining the impact of the minimum wage on labor market outcomes. Specifically, these models focus on labor markets with frictions.

Assume that the decision to participate in the labor market results from a trade-off between being an unemployed job seeker and not participating at all. All individuals whose expected utility outside the labor market is less than the expected utility of an unemployed person formula_1 decide to participate in the labor market. In the basic search and matching model, the expected utility of unemployed persons formula_1 and that of employed persons formula_3 are defined by:

formula_4Let formula_5 be the wage, formula_6 the interest rate, formula_7 the instantaneous income of unemployed persons, formula_8 the exogenous job destruction rate, formula_9 the labor market tightness, and formula_10 the job finding rate. The profits formula_11 and formula_12 expected from a filled job and a vacant one are:formula_13where formula_14 is the cost of a vacant job and formula_15 is the productivity. When the "free entry condition" formula_16 is satisfied, these two equalities yield the following relationship between the wage formula_5 and labor market tightness formula_9:

formula_19If formula_5 represents a minimum wage that applies to all workers, this equation completely determines the equilibrium value of the labor market tightness formula_9. There are two conditions associated with the matching function:formula_22This implies that formula_9 is a decreasing function of the minimum wage formula_5, and so is the job finding rate formula_25. A hike in the minimum wage degrades the profitability of a job, so firms post fewer vacancies and the job finding rate falls off. Now let's rewrite formula_26 to be:formula_27Using the relationship between the wage and labor market tightness to eliminate the wage from the last equation gives us:
formula_28 If we maximize formula_26 in this equation, with respect to the labor market tightness, we find that:formula_30where formula_31 is the elasticity of the matching function:formula_32This result shows that the expected utility of unemployed workers is maximized when the minimum wage is set at a level that corresponds to the wage level of the decentralized economy in which the bargaining power parameter is equal to the elasticity formula_31.  The level of the negotiated wage is formula_34.

If formula_35, then an increase in the minimum wage increases participation "and" the unemployment rate, with an ambiguous impact on employment. When the bargaining power of workers is less than formula_31, an increases in the minimum wage improves the welfare of the unemployed - this suggests that minimum wage hikes can improve labor market efficiency, at least up to the point when bargaining power equals formula_31. On the other hand, if formula_38, any increases in the minimum wage entails a decline in labor market participation and an increase in unemployment.

In the model just presented, we found that the minimum wage always increases unemployment. This result does not necessarily hold when the search effort of workers in endogenous.

Consider a model where the intensity of the job search is designated by the scalar formula_39, which can be interpreted as the amount of time and/or intensity of the effort devoted to search. Assume that the arrival rate of job offers is formula_40 and that the wage distribution is degenerated to a single wage formula_5. Denote formula_42 to be the cost arising from the search effort, with formula_43. Then the discounted utilities are given by:formula_44Therefore, the optimal search effort is such that the marginal cost of performing the search is equation to the marginal return:formula_45This implies that the optimal search effort increases as the difference between the expected utility of the job holder and the expected utility of the job seeker grows. In fact, this difference actually grows with the wage. To see this, take the difference of the two discounted utilities to find:formula_46Then differentiating with respect to formula_5 and rearranging gives us:formula_48where formula_49 is the optimal search effort. This implies that a wage increase drives up job search effort and, therefore, the job finding rate. Additionally, the unemployment rate formula_50 at equilibrium is given by:formula_51A hike in the wage, which increases the search effort and the job finding rate, decreases the unemployment rate. So it is possible that a hike in the minimum wage "may", by boosting the search effort of job seekers, boost employment. Taken in sum with the previous section, the minimum wage in labor markets with frictions can improve employment and decrease the unemployment rate when it is sufficiently low. However, a high minimum wage is detrimental to employment and increases the unemployment rate.

Economists disagree as to the measurable impact of minimum wages in practice. This disagreement usually takes the form of competing empirical tests of the elasticities of supply and demand in labor markets and the degree to which markets differ from the efficiency that models of perfect competition predict.

Economists have done empirical studies on different aspects of the minimum wage, including:
Until the mid-1990s, a general consensus existed among economists, both conservative and liberal, that the minimum wage reduced employment, especially among younger and low-skill workers. In addition to the basic supply-demand intuition, there were a number of empirical studies that supported this view. For example, Gramlich (1976) found that many of the benefits went to higher income families, and that teenagers were made worse off by the unemployment associated with the minimum wage.

Brown et al. (1983) noted that time series studies to that point had found that for a 10 percent increase in the minimum wage, there was a decrease in teenage employment of 1–3 percent. However, the studies found wider variation, from 0 to over 3 percent, in their estimates for the effect on teenage unemployment (teenagers without a job and looking for one). In contrast to the simple supply and demand diagram, it was commonly found that teenagers withdrew from the labor force in response to the minimum wage, which produced the possibility of equal reductions in the supply as well as the demand for labor at a higher minimum wage and hence no impact on the unemployment rate. Using a variety of specifications of the employment and unemployment equations (using ordinary least squares vs. generalized least squares regression procedures, and linear vs. logarithmic specifications), they found that a 10 percent increase in the minimum wage caused a 1 percent decrease in teenage employment, and no change in the teenage unemployment rate. The study also found a small, but statistically significant, increase in unemployment for adults aged 20–24.

Wellington (1991) updated Brown et al.'s research with data through 1986 to provide new estimates encompassing a period when the real (i.e., inflation-adjusted) value of the minimum wage was declining, because it had not increased since 1981. She found that a 10% increase in the minimum wage decreased the absolute teenage employment by 0.6%, with no effect on the teen or young adult unemployment rates.

Some research suggests that the unemployment effects of small minimum wage increases are dominated by other factors. In Florida, where voters approved an increase in 2004, a follow-up comprehensive study after the increase confirmed a strong economy with increased employment above previous years in Florida and better than in the US as a whole. When it comes to on-the-job training, some believe the increase in wages is taken out of training expenses. A 2001 empirical study found that there is "no evidence that minimum wages reduce training, and little evidence that they tend to increase training."

Some empirical studies have tried to ascertain the benefits of a minimum wage beyond employment effects. In an analysis of census data, Joseph Sabia and Robert Nielson found no statistically significant evidence that minimum wage increases helped reduce financial, housing, health, or food insecurity. This study was undertaken by the Employment Policies Institute, a think tank funded by the food, beverage and hospitality industries. In 2012, Michael Reich published an economic analysis that suggested that a proposed minimum wage hike in San Diego might stimulate the city's economy by about $190 million.

"The Economist" wrote in December 2013: "A minimum wage, providing it is not set too high, could thus boost pay with no ill effects on jobs...America's federal minimum wage, at 38% of median income, is one of the rich world's lowest. Some studies find no harm to employment from federal or state minimum wages, others see a small one, but none finds any serious damage. ... High minimum wages, however, particularly in rigid labour markets, do appear to hit employment. France has the rich world’s highest wage floor, at more than 60% of the median for adults and a far bigger fraction of the typical wage for the young. This helps explain why France also has shockingly high rates of youth unemployment: 26% for 15- to 24-year-olds."

The restaurant industry is commonly studied because of its high number of minimum wage workers. A 2018 study from the Center on Wage and Employment Dynamics at the University of California, Berkeley focusing on food services showed that minimum wage increases in Washington, Chicago, Seattle, San Francisco, Oakland, and San Jose gave workers higher pay without hampering job growth. A 2017 study of restaurants in the San Francisco Bay Area examined the period 2008-2016 and the effect that a minimum wage increase had on the probability of restaurants going out of business, and broke out results based on the restaurant's rating on the review site Yelp. The study found no effect for 5-star (highest rated) restaurants (regardless of the expensiveness of the cuisine) but those with increasingly lower ratings were increasingly likely to go out of business (for example a 14% increase at 3.5 stars for a $1 per hour minimum wage increase). It also noted that the Yelp star rating was correlated with likelihood of minority ownership and minority customer base. Importantly, it noted that restaurants below 4 star in rating were proportionally more likely to hire low-skilled workers. The minimum wage increases during this period did not prevent growth in the industry overall – the number of restaurants in San Francisco went from 3,600 in 2012 to 7,600 in 2016. An August 2019 study from The New School's Center for New York City Affairs and the think tank National Employment Law Project, which advocates for raising the minimum wage, found that the restaurant industry in New York City has been "thriving" following an increase in the minimum wage to $15 an hour.

A 2019 study in the "Quarterly Journal of Economics" found that minimum wage increases did not have an impact on the overall number of low-wage jobs in the five years subsequent to the wage increase. However, it did find disemployment in 'tradeable' sectors, defined as those sectors most reliant on entry level or low skilled labor.

In another study, which shared authors with the above, published in the "American Economic Review" found that a large and persistent increase in the minimum wage in Hungary produced some disemployment with the large majority of additional cost being passed on to consumers. The authors also found that firms began substituting capital for labor over time.

In 1992, the minimum wage in New Jersey increased from $4.25 to $5.05 per hour (an 18.8% increase), while in the adjacent state of Pennsylvania it remained at $4.25. David Card and Alan Krueger gathered information on fast food restaurants in New Jersey and eastern Pennsylvania in an attempt to see what effect this increase had on employment within New Jersey. A basic supply and demand model predicts that relative employment should have decreased in New Jersey. Card and Krueger surveyed employers before the April 1992 New Jersey increase, and again in November–December 1992, asking managers for data on the full-time equivalent staff level of their restaurants both times. Based on data from the employers' responses, the authors concluded that the increase in the minimum wage slightly increased employment in the New Jersey restaurants.

Card and Krueger expanded on this initial article in their 1995 book "Myth and Measurement: The New Economics of the Minimum Wage". They argued that the negative employment effects of minimum wage laws are minimal if not non-existent. For example, they look at the 1992 increase in New Jersey's minimum wage, the 1988 rise in California's minimum wage, and the 1990–91 increases in the federal minimum wage. In addition to their own findings, they reanalyzed earlier studies with updated data, generally finding that the older results of a negative employment effect did not hold up in the larger datasets.

In 1996, David Neumark and William Wascher reexamined Card and Krueger's result using administrative payroll records from a sample of large fast food restaurant chains, and reported that minimum wage increases were followed by decreases in employment. An assessment of data collected and analyzed by Neumark and Wascher did not initially contradict the Card and Krueger results, but in a later edited version they found a four percent decrease in employment, and reported that "the estimated disemployment effects in the payroll data are often statistically significant at the 5- or 10-percent level although there are some estimators and subsamples that yield insignificant—although almost always negative" employment effects. Neumark and Wascher's conclusions were subsequently rebutted in a 2000 paper by Card and Krueger. A 2011 paper has reconciled the difference between Card and Krueger's survey data and Neumark and Wascher's payroll-based data. The paper shows that both datasets evidence conditional employment effects that are positive for small restaurants, but are negative for large fast-food restaurants. A 2014 analysis based on panel data found that the minimum wage reduces employment among teenagers.

In 1996 and 1997, the federal minimum wage was increased from $4.25 to $5.15, thereby increasing the minimum wage by $0.90 in Pennsylvania but by just $0.10 in New Jersey; this allowed for an examination of the effects of minimum wage increases in the same area, subsequent to the 1992 change studied by Card and Krueger. A study by Hoffman and Trace found the result anticipated by traditional theory: a detrimental effect on employment.

Further application of the methodology used by Card and Krueger by other researchers yielded results similar to their original findings, across additional data sets. A 2010 study by three economists (Arindrajit Dube of the University of Massachusetts Amherst, William Lester of the University of North Carolina at Chapel Hill, and Michael Reich of the University of California, Berkeley), compared adjacent counties in different states where the minimum wage had been raised in one of the states. They analyzed employment trends for several categories of low-wage workers from 1990 to 2006 and found that increases in minimum wages had no negative effects on low-wage employment and successfully increased the income of workers in food services and retail employment, as well as the narrower category of workers in restaurants.

However, a 2011 study by Baskaya and Rubinstein of Brown University found that at the federal level, "a rise in minimum wage have ["sic"] an instantaneous impact on wage rates and a corresponding negative impact on employment", stating, "Minimum wage increases boost teenage wage rates and reduce teenage employment." Another 2011 study by Sen, Rybczynski, and Van De Waal found that "a 10% increase in the minimum wage is significantly correlated with a 3−5% drop in teen employment." A 2012 study by Sabia, Hansen, and Burkhauser found that "minimum wage increases can have substantial adverse labor demand effects for low-skilled individuals", with the largest effects on those aged 16 to 24.

A 2013 study by Meer and West concluded that "the minimum wage reduces net job growth, primarily through its effect on job creation by expanding establishments ... most pronounced for younger workers and in industries with a higher proportion of low-wage workers." This study by Meer and West was later critiqued for its trends of assumption in the context of narrowly defined low-wage groups. The authors replied to the critiques and released additional data which addressed the criticism of their methodology, but did not resolve the issue of whether their data showed a causal relationship. A 2019 paper published in the "Quarterly Journal of Economics" by Cengiz, Dube, Lindner and Zipperer argues that the job losses found using a Meer and West type methodology "tend to be driven by an unrealistically large drop in the number of jobs at the upper tail of the wage distribution, which is unlikely to be a causal effect of the minimum wage." Another 2013 study by Suzana Laporšek of the University of Primorska, on youth unemployment in Europe claimed there was "a negative, statistically significant impact of minimum wage on youth employment." A 2013 study by labor economists Tony Fang and Carl Lin which studied minimum wages and employment in China, found that "minimum wage changes have significant adverse effects on employment in the Eastern and Central regions of China, and result in disemployment for females, young adults, and low-skilled workers".

A 2017 study found that in Seattle, increasing the minimum wage to $13 per hour lowered income of low-wage workers by $125 per month, due to the resulting reduction in hours worked, as industries made changes to make their businesses less labor intensive. The authors argue that previous research that found no negative effects on hours worked are flawed because they only look at select industries, or only look at teenagers, instead of entire economies.

Finally, a study by Overstreet in 2019 examined increases to the minimum wage in Arizona. Utilizing data spanning from 1976 to 2017, Overstreet found that a 1% increase in the minimum wage was significantly correlated with a 1.13% increase in per capita income in Arizona. This study could show that smaller increases in minimum wage may not distort labor market as significantly as larger increases experienced in other cities and states. Thus, the small increases experienced in Arizona may have actually led to a slight increase in economic growth.

In 2019, economists from Georgia Tech published a study that found a strong correlation between incresases to the minimum wage and detectable harm to the financial conditions of small businesses, including a higher rate of bankruptcy, lower hiring rates, lower credit scores, and higher interest payments. The researchers noted that these small businesses were also correlated with minority ownership and minority customer bases.

In July 2019, the Congressional Budget Office published the impact on proposed national $15/hour legislation. It noted that workers who retained full employment would see a modest improvement in take home pay offset by a small decrease in working conditions and non-pecuniary benefits. However, this benefit is offset by three primary factors; the reduction in hours worked, the reduction in total employment, and the increased cost of goods and services. Those factors result in a decrease of about $33 Billion in total income and nearly 1.7-3.7 million lost jobs in the first three years (the CBO also noted this figure increases over time).

In response to an April 2016 Council of Economic Advisers (CEA) report advocating the raising of the minimum wage to deter crime, economists used data from the 1998-2016 Uniform Crime Reports (UCR), National Incident-Based Reporting System (NIBRS), and National Longitudinal Study of Youth (NLSY) to assess the impact of the minimum wage on crime. They found that increasing the minimum wage resulted in increased property crime arrests among those ages 16-to-24. They estimated that an increase of the Federal minimum wage to $15/hour would "generate criminal externality costs of nearly $2.4 billion."

Economists in Denmark, relying on a discontinuity in wage rates when a worker turns 18, found that employment fell by 33% and total hours fell by 45% when the minimum wage law was in effect.

Several researchers have conducted statistical meta-analyses of the employment effects of the minimum wage. In 1995, Card and Krueger analyzed 14 earlier time-series studies on minimum wages and concluded that there was clear evidence of publication bias (in favor of studies that found a statistically significant negative employment effect). They point out that later studies, which had more data and lower standard errors, did not show the expected increase in t-statistic (almost all the studies had a t-statistic of about two, just above the level of statistical significance at the .05 level). Though a serious methodological indictment, opponents of the minimum wage largely ignored this issue; as Thomas Leonard noted, "The silence is fairly deafening."

In 2005, T.D. Stanley showed that Card and Krueger's results could signify either publication bias or the absence of a minimum wage effect. However, using a different methodology, Stanley concluded that there is evidence of publication bias and that correction of this bias shows no relationship between the minimum wage and unemployment. In 2008, Hristos Doucouliagos and T.D. Stanley conducted a similar meta-analysis of 64 U.S. studies on disemployment effects and concluded that Card and Krueger's initial claim of publication bias is still correct. Moreover, they concluded, "Once this publication selection is corrected, little or no evidence of a negative association between minimum wages and employment remains." In 2013, a meta-analysis of 16 UK studies found no significant effects on employment attributable to the minimum wage. a 2007 meta-analyses by David Neumark of 96 studies found a consistent, but not always statistically significant, negative effect on employment from increases in the minimum wage.

Minimum wage laws affect workers in most low-paid fields of employment and have usually been judged against the criterion of reducing poverty. Minimum wage laws receive less support from economists than from the general public. Despite decades of experience and economic research, debates about the costs and benefits of minimum wages continue today.

Various groups have great ideological, political, financial, and emotional investments in issues surrounding minimum wage laws. For example, agencies that administer the laws have a vested interest in showing that "their" laws do not create unemployment, as do labor unions whose members' finances are protected by minimum wage laws. On the other side of the issue, low-wage employers such as restaurants finance the Employment Policies Institute, which has released numerous studies opposing the minimum wage. The presence of these powerful groups and factors means that the debate on the issue is not always based on dispassionate analysis. Additionally, it is extraordinarily difficult to separate the effects of minimum wage from all the other variables that affect employment.

The following table summarizes the arguments made by those for and against minimum wage laws:

A widely circulated argument that the minimum wage was ineffective at reducing poverty was provided by George Stigler in 1949:
In 2006, the International Labour Organization (ILO) argued that the minimum wage could not be directly linked to unemployment in countries that have suffered job losses. In April 2010, the Organisation for Economic Co-operation and Development (OECD) released a report arguing that countries could alleviate teen unemployment by "lowering the cost of employing low-skilled youth" through a sub-minimum training wage. A study of U.S. states showed that businesses' annual and average payrolls grow faster and employment grew at a faster rate in states with a minimum wage. The study showed a correlation, but did not claim to prove causation.

Although strongly opposed by both the business community and the Conservative Party when introduced in the UK in 1999, the Conservatives reversed their opposition in 2000. Accounts differ as to the effects of the minimum wage. The Centre for Economic Performance found no discernible impact on employment levels from the wage increases, while the Low Pay Commission found that employers had reduced their rate of hiring and employee hours employed, and found ways to cause current workers to be more productive (especially service companies). The Institute for the Study of Labor found prices in the minimum wage sector rose significantly faster than prices in non-minimum wage sectors, in the four years following the implementation of the minimum wage. Neither trade unions nor employer organizations contest the minimum wage, although the latter had especially done so heavily until 1999.

In 2014, supporters of minimum wage cited a study that found that job creation within the United States is faster in states that raised their minimum wages. In 2014, supporters of minimum wage cited news organizations who reported the state with the highest minimum-wage garnered more job creation than the rest of the United States.

In 2014, in Seattle, Washington, liberal and progressive business owners who had supported the city's new $15 minimum wage said they might hold off on expanding their businesses and thus creating new jobs, due to the uncertain timescale of the wage increase implementation. However, subsequently at least two of the business owners quoted did expand.

The dollar value of the minimum wage loses purchasing power over time due to inflation. Minimum wage laws, for instance proposals to index the minimum wage to average wages, have the potential to keep the dollar value of the minimum wage relevant and predictable.

With regard to the economic effects of introducing minimum wage legislation in Germany in January 2015, recent developments have shown that the feared increase in unemployment has not materialized, however, in some economic sectors and regions of the country, it came to a decline in job opportunities particularly for temporary and part-time workers, and some low-wage jobs have disappeared entirely. Because of this overall positive development, the Deutsche Bundesbank revised its opinion, and ascertained that “the impact of the introduction of the minimum wage on the total volume of work appears to be very limited in the present business cycle”.

A 2019 study published in the American Journal of Preventive Medicine showed that in the United States, those states which have implemented a higher minimum wage saw a decline in the growth of suicide rates. The researchers say that for every one dollar increase, the annual suicide growth rate fell by 1.9%. The study covers all 50 states for the years 2006 to 2016.

According to a 1978 article in the "American Economic Review", 90% of the economists surveyed agreed that the minimum wage increases unemployment among low-skilled workers. By 1992 the survey found 79% of economists in agreement with that statement, and by 2000, 46% were in full agreement with the statement and 28% agreed with provisos (74% total). The authors of the 2000 study also reweighted data from a 1990 sample to show that at that time 62% of academic economists agreed with the statement above, while 20% agreed with provisos and 18% disagreed. They state that the reduction on consensus on this question is "likely" due to the Card and Krueger research and subsequent debate.

A similar survey in 2006 by Robert Whaples polled PhD members of the American Economic Association (AEA). Whaples found that 47% respondents wanted the minimum wage eliminated, 38% supported an increase, 14% wanted it kept at the current level, and 1% wanted it decreased. Another survey in 2007 conducted by the University of New Hampshire Survey Center found that 73% of labor economists surveyed in the United States believed 150% of the then-current minimum wage would result in employment losses and 68% believed a mandated minimum wage would cause an increase in hiring of workers with greater skills. 31% felt that no hiring changes would result.

Surveys of labor economists have found a sharp split on the minimum wage. Fuchs et al. (1998) polled labor economists at the top 40 research universities in the United States on a variety of questions in the summer of 1996. Their 65 respondents were nearly evenly divided when asked if the minimum wage should be increased. They argued that the different policy views were not related to views on whether raising the minimum wage would reduce teen employment (the median economist said there would be a reduction of 1%), but on value differences such as income redistribution. Daniel B. Klein and Stewart Dompe conclude, on the basis of previous surveys, "the average level of support for the minimum wage is somewhat higher among labor economists than among AEA members."

In 2007, Klein and Dompe conducted a non-anonymous survey of supporters of the minimum wage who had signed the "Raise the Minimum Wage" statement published by the Economic Policy Institute. 95 of the 605 signatories responded. They found that a majority signed on the grounds that it transferred income from employers to workers, or equalized bargaining power between them in the labor market. In addition, a majority considered disemployment to be a moderate potential drawback to the increase they supported.

In 2013, a diverse group of 37 economics professors was surveyed on their view of the minimum wage's impact on employment. 34% of respondents agreed with the statement, "Raising the federal minimum wage to $9 per hour would make it noticeably harder for low-skilled workers to find employment." 32% disagreed and the remaining respondents were uncertain or had no opinion on the question. 47% agreed with the statement, "The distortionary costs of raising the federal minimum wage to $9 per hour and indexing it to inflation are sufficiently small compared with the benefits to low-skilled workers who can find employment that this would be a desirable policy", while 11% disagreed.

Economists and other political commentators have proposed alternatives to the minimum wage. They argue that these alternatives may address the issue of poverty better than a minimum wage, as it would benefit a broader population of low wage earners, not cause any unemployment, and distribute the costs widely rather than concentrating it on employers of low wage workers.

A basic income (or negative income tax - NIT) is a system of social security that periodically provides each citizen with a sum of money that is sufficient to live on frugally. Supporters of the basic-income idea argue that recipients of the basic income would have considerably more bargaining power when negotiating a wage with an employer, as there would be no risk of destitution for not taking the employment. As a result, jobseekers could spend more time looking for a more appropriate or satisfying job, or they could wait until a higher-paying job appeared. Alternatively, they could spend more time increasing their skills (via education and training), which would make them more suitable for higher-paying jobs, as well as provide numerous other benefits. Experiments on Basic Income and NIT in Canada and the USA show that people spent more time studying while the program was running.

Proponents argue that a basic income that is based on a broad tax base would be more economically efficient than a minimum wage, as the minimum wage effectively imposes a high marginal tax on employers, causing losses in efficiency.

A guaranteed minimum income is another proposed system of social welfare provision. It is similar to a basic income or negative income tax system, except that it is normally conditional and subject to a means test. Some proposals also stipulate a willingness to participate in the labor market, or a willingness to perform community services.

A refundable tax credit is a mechanism whereby the tax system can reduce the tax owed by a household to below zero, and result in a net payment to the taxpayer beyond their own payments into the tax system. Examples of refundable tax credits include the earned income tax credit and the additional child tax credit in the US, and working tax credits and child tax credits in the UK. Such a system is slightly different from a negative income tax, in that the refundable tax credit is usually only paid to households that have earned at least some income. This policy is more targeted against poverty than the minimum wage, because it avoids subsidizing low-income workers who are supported by high-income households (for example, teenagers still living with their parents).

In the United States, earned income tax credit rates, also known as EITC or EIC, vary by state—some are refundable while other states do not allow a refundable tax credit. The federal EITC program has been expanded by a number of presidents including Jimmy Carter, Ronald Reagan, George H.W. Bush, and Bill Clinton. In 1986, President Reagan described the EITC as "the best anti poverty, the best pro-family, the best job creation measure to come out of Congress." The ability of the earned income tax credit to deliver larger monetary benefits to the poor workers than an increase in the minimum wage and at a lower cost to society was documented in a 2007 report by the Congressional Budget Office.

The Adam Smith Institute prefers cutting taxes on the poor and middle class instead of raising wages as an alternative to the minimum wage.

Italy, Sweden, Norway, Finland, and Denmark are examples of developed nations where there is no minimum wage that is required by legislation. Such nations, particularly the Nordics, have very high union participation rates. Instead, minimum wage standards in different sectors are set by collective bargaining.

Some economists such as Scott Sumner and Edmund Phelps advocate a wage subsidy program. A wage subsidy is a payment made by a government for work people do. It is based either on an hourly basis or by income earned. Advocates argue that the primary deficiencies of the EITC and the minimum wage are best avoided by a wage subsidy. However, the wage subsidy in the United States suffers from a lack of political support from either major political party.

Providing education or funding apprenticeships or technical training can provide a bridge for low skilled workers to move into wages above a minimum wage. For example, Germany has adopted a state funded apprenticeship program that combines on-the-job and classroom training. Having more skills makes workers more valuable and more productive, but having a high minimum wage for low-skill jobs reduces the incentive to seek education and training. Moving some workers to higher-paying jobs will decrease the supply of workers willing to accept low-skill jobs, increasing the market wage for those low skilled jobs (assuming a stable labor market). However, in that solution the wage will still not increase above the marginal return for the role and will likely promote automation or business closure.

In January 2014, seven Nobel economists—Kenneth Arrow, Peter Diamond, Eric Maskin, Thomas Schelling, Robert Solow, Michael Spence, and Joseph Stiglitz—and 600 other economists wrote a letter to the US Congress and the US President urging that, by 2016, the US government should raise the minimum wage to $10.10. They endorsed the Minimum Wage Fairness Act which was introduced by US Senator Tom Harkin in 2013. U.S. Senator Bernie Sanders introduced a bill in 2015 that would raise the minimum wage to $15, and in his 2016 campaign for president ran on a platform of increasing it. Although Sanders did not become the nominee, the Democratic National Committee adopted his $15 minimum wage push in their 2016 party platform.

In late March 2016, Governor of California Jerry Brown reached a deal to raise the minimum wage to $15 by 2022 for big businesses and 2023 for smaller businesses.

In contrast, the relatively high minimum wage in Puerto Rico has been blamed by various politicians and commentators as a highly significant factor in the Puerto Rican government-debt crisis. One study concluded that "Employers are disinclined to hire workers because the US federal minimum wage is very high relative to the local average".

, unions were exempt from recent minimum wage increases in Chicago, Illinois, SeaTac, Washington, and Milwaukee County, Wisconsin, as well as the California cities of Los Angeles, San Francisco, Long Beach, San Jose, Richmond, and Oakland.

Scholars found in 2019 that, in America, "Between 1990 and 2015, raising the minimum wage by $1 in each state might have saved more than 27,000 lives, according to a report published this week in the "Journal of Epidemiology & Community Health." An increase of $2 in each state's minimum wage could have prevented more than 57,000 suicides." The researchers stated, "The effect of a US$1 increase in the minimum wage ranged from a 3.4% decrease (95% CI 0.4 to 6.4) to a 5.9% decrease (95% CI 1.4 to 10.2) in the suicide rate among adults aged 18–64 years with a high school education or less. We detected significant effect modification by unemployment rate, with the largest effects of minimum wage on reducing suicides observed at higher unemployment levels." They concluded, "Minimum wage increases appear to reduce the suicide rate among those with a high school education or less, and may reduce disparities between socioeconomic groups. Effects appear greatest during periods of high unemployment."




</doc>
<doc id="18864" url="https://en.wikipedia.org/wiki?curid=18864" title="Mullet">
Mullet

Mullet may refer to:







</doc>
<doc id="18866" url="https://en.wikipedia.org/wiki?curid=18866" title="Macbeth">
Macbeth

Macbeth (; full title The Tragedy of Macbeth) is a tragedy by William Shakespeare; it is thought to have been first performed in 1606. It dramatises the damaging physical and psychological effects of political ambition on those who seek power for its own sake. Of all the plays that Shakespeare wrote during the reign of James I, who was patron of Shakespeare's acting company, "Macbeth" most clearly reflects the playwright's relationship with his sovereign. It was first published in the Folio of 1623, possibly from a prompt book, and is Shakespeare's shortest tragedy.

A brave Scottish general named Macbeth receives a prophecy from a trio of witches that one day he will become King of Scotland. Consumed by ambition and spurred to action by his wife, Macbeth murders King Duncan and takes the Scottish throne for himself. He is then wracked with guilt and paranoia. Forced to commit more and more murders to protect himself from enmity and suspicion, he soon becomes a tyrannical ruler. The bloodbath and consequent civil war swiftly take Macbeth and Lady Macbeth into the realms of madness and death.

Shakespeare's source for the story is the account of Macbeth, King of Scotland, Macduff, and Duncan in "Holinshed's Chronicles" (1587), a history of England, Scotland, and Ireland familiar to Shakespeare and his contemporaries, although the events in the play differ extensively from the history of the real Macbeth. The events of the tragedy are usually associated with the execution of Henry Garnet for complicity in the Gunpowder Plot of 1605.

In the backstage world of theatre, some believe that the play is cursed, and will not mention its title aloud, referring to it instead as "The Scottish Play". Over the course of many centuries, the play has attracted some of the most renowned actors to the roles of Macbeth and Lady Macbeth. It has been adapted to film, television, opera, novels, comics, and other media.

Three witches resolve to meet Macbeth after the battle. Victory is reported to King Duncan, who, impressed by reports of Macbeth's conduct, transfers the rebellious Thane of Cawdor's title to him. The witches meet Macbeth and Banquo, prophesy to each in turn, and vanish. Ross and Angus arrive with news that accords with that same "prophetic greeting" (1.3.78), and Macbeth muses much on "the swelling act / Of the imperial theme" (1.3.130-31) before suggesting they all move on. They meet the king, who names Malcolm (his son) heir, and says he will stay with Macbeth. Macbeth sets off ahead to tell his wife. She has a letter from Macbeth, and she also muses on the prospect of him becoming king, as prophesied. But she is interrupted, first by a messenger, then by Macbeth. She advises the latter that the king must be "provided for" (1.5.67). The king arrives and is kindly received. Macbeth steps out from dining with the king, worried about consequences of killing him. His wife allays his anxiety, and he is "settled" (1.7.80).

At night, Banquo broaches the witches to Macbeth. Macbeth says he "think[s] not of them" (2.1.22), but would discuss the business at a suitable time. Macbeth's wife is anxious, waiting for Macbeth to kill Duncan, until he arrives, extremely distressed, saying he has "done the deed" (2.2.15). He has brought the daggers which he should have planted with the drugged grooms, and refuses to go back. His wife takes them, stressing the importance of smearing the grooms with blood. Macduff and Lennox are met at the gate, and led to the king's door. Macduff goes in, while Macbeth and Lennox discuss the "unruly" (2.3.54) night. Macduff reappears, proclaiming horror. Macbeth and Lennox go in, and Macduff raises an alarm to which all convene. "Those of his chamber" (2.3.102) are blamed, and killed by Macbeth, whose "violent love / Outran the pauser, reason" (2.3.111-12). All agree to meet and discuss, besides Malcolm and Donalbain (Duncan's two sons) who flee. Ross discusses the situation with an old man, until Macduff arrives with news that Malcolm and Donalbain are fled, held in suspicion, and that Macbeth is named king.

Banquo muses on the situation, and is met by Macbeth and his wife, now king and queen, who invite him to a feast. Macbeth, having achieved what was prophesied for himself, is unhappy with Banquo's share of it (that he would "get kings, though [himself] be none" (1.3.67)), so meets two murderers. They agree to kill Banquo and Fleance (Banquo's son). Macbeth and his wife discuss Banquo. Three murderers kill Banquo, but Fleance escapes. Macbeth, having had such news, sees Banquo's ghost sitting in his place at the feast. Reacting, Macbeth "displace[s] the mirth" (3.4.107), then tells his wife he will visit the witches tomorrow. Lennox questions a lord about Macduff. 

Macbeth meets the witches. They conjure apparitions to address his concerns, and vanish. Macbeth learns that Macduff is in England. Having just been warned to "Beware Macduff" (4.1.70), he resolves to seize Macduff's property, and slaughter his family. Ross, with Macduff's wife, discusses Macduff's flight, then leaves. A messenger enters, advises Macduff's wife to flee, then does so himself. Murderers enter, and kill her son. She flees, "crying 'Murder'" (SD 4.2.87). Malcolm goads Macduff to a "noble passion" (4.3.114), thereby convincing himself of his "good truth and honour" (4.3.117), then tells him that he has soldiers ready. Ross arrives, and they share news, including that of Macduff's slaughtered family.

A doctor and a gentlewoman observe Macbeth's wife walking and talking, though apparently asleep. Military action against Macbeth begins, with marches to meet those of Malcolm. Macbeth dismisses the news, quoting the apparitions. The doctor tells him that he cannot cure his wife. The forces against Macbeth all meet, then move on against him. Macbeth continues to scoff at the threat, until he is told that his wife is dead. He muses on futility and what the apparitions told him. Macduff seeks Macbeth in the field, while Malcolm takes the castle. Macduff finds Macbeth, who is still confident, until he realises that he has misinterpreted the apparitions. His nihilism peaks, but he will not yield. Macduff takes Macbeth's head to Malcolm, hailing the latter King of Scotland. Malcolm distributes titles.

A principal source comes from the "Daemonologie" of King James published in 1597 which included a news pamphlet titled "Newes from Scotland" that detailed the famous North Berwick Witch Trials of 1590. The publication of "Daemonologie" came just a few years before the tragedy of "Macbeth" with the themes and setting in a direct and comparative contrast with King James' personal experiences with witchcraft. Not only had this trial taken place in Scotland, the witches involved were recorded to have also conducted rituals with the same mannerisms as the three witches. One of the evidenced passages is referenced when the witches involved in the trial confessed to attempt the use of witchcraft to raise a tempest and sabotage the very boat King James and his queen were on board during their return trip from Denmark. This was significant as one ship sailing with King James' fleet actually sank in the storm. The three witches discuss the raising of winds at sea in the opening lines of Act 1 Scene 3.

"Macbeth" has been compared to Shakespeare's "Antony and Cleopatra." As characters, both Antony and Macbeth seek a new world, even at the cost of the old one. Both fight for a throne and have a 'nemesis' to face to achieve that throne. For Antony, the nemesis is Octavius; for Macbeth, it is Banquo. At one point Macbeth even compares himself to Antony, saying "under Banquo / My Genius is rebuk'd, as it is said / Mark Antony's was by Caesar." Lastly, both plays contain powerful and manipulative female figures: Cleopatra and Lady Macbeth.

Shakespeare borrowed the story from several tales in "Holinshed's Chronicles", a popular history of the British Isles well known to Shakespeare and his contemporaries. In "Chronicles", a man named Donwald finds several of his family put to death by his king, King Duff, for dealing with witches. After being pressured by his wife, he and four of his servants kill the King in his own house. In "Chronicles", Macbeth is portrayed as struggling to support the kingdom in the face of King Duncan's ineptitude. He and Banquo meet the three witches, who make exactly the same prophecies as in Shakespeare's version. Macbeth and Banquo then together plot the murder of Duncan, at Lady Macbeth's urging. Macbeth has a long, ten-year reign before eventually being overthrown by Macduff and Malcolm. The parallels between the two versions are clear. However, some scholars think that George Buchanan's "Rerum Scoticarum Historia" matches Shakespeare's version more closely. Buchanan's work was available in Latin in Shakespeare's day.

No medieval account of the reign of Macbeth mentions the Weird Sisters, Banquo, or Lady Macbeth, and with the exception of the latter none actually existed. The characters of Banquo, the Weird Sisters, and Lady Macbeth were first mentioned in 1527 by a Scottish historian Hector Boece in his book "Historia Gentis Scotorum" ("History of the Scottish People") who wanted to denigrate Macbeth in order to strengthen the claim of the House of Stewart to the Scottish throne. Boece portrayed Banquo as an ancestor of the Stewart kings of Scotland, adding in a "prophecy" that the descendants of Banquo would be the rightful kings of Scotland while the Weird Sisters served to give a picture of King Macbeth as gaining the throne via dark supernatural forces. Macbeth did have a wife, but it is not clear if she was as power-hungry and ambitious as Boece portrayed her, which served his purpose of having even Macbeth realise he lacked a proper claim to the throne, and only took it at the urging of his wife. Holinshed accepted Boece's version of Macbeth's reign at face value and included it in his "Chronicles". Shakespeare saw the dramatic possibilities in the story as related by Holinshed, and used it as the basis for the play.

No other version of the story has Macbeth kill the king in Macbeth's own castle. Scholars have seen this change of Shakespeare's as adding to the darkness of Macbeth's crime as the worst violation of hospitality. Versions of the story that were common at the time had Duncan being killed in an ambush at Inverness, not in a castle. Shakespeare conflated the story of Donwald and King Duff in what was a significant change to the story.

Shakespeare made another important change. In "Chronicles", Banquo is an accomplice in Macbeth's murder of King Duncan, and plays an important part in ensuring that Macbeth, not Malcolm, takes the throne in the coup that follows. In Shakespeare's day, Banquo was thought to be an ancestor of the Stuart King James I. (In the 19th century it was established that Banquo is an unhistorical character, the Stuarts are actually descended from a Breton family which migrated to Scotland slightly later than Macbeth's time.) The Banquo portrayed in earlier sources is significantly different from the Banquo created by Shakespeare. Critics have proposed several reasons for this change. First, to portray the king's ancestor as a murderer would have been risky. Other authors of the time who wrote about Banquo, such as Jean de Schelandre in his "Stuartide", also changed history by portraying Banquo as a noble man, not a murderer, probably for the same reasons. Second, Shakespeare may have altered Banquo's character simply because there was no dramatic need for another accomplice to the murder; there was, however, a need to give a dramatic contrast to Macbeth—a role which many scholars argue is filled by Banquo.

Other scholars maintain that a strong argument can be made for associating the tragedy with the Gunpowder Plot of 1605. As presented by Harold Bloom in 2008: "[S]cholars cite the existence of several topical references in "Macbeth" to the events of that year, namely the execution of the Father Henry Garnett for his alleged complicity in the Gunpowder Plot of 1605, as referenced in the porter's scene." Those arrested for their role in the Gunpowder Plot refused to give direct answers to the questions posed to them by their interrogators, which reflected the influence of the Jesuit practice of equivocation. Shakespeare, by having Macbeth say that demons "palter...in a double sense" and "keep the promise to our ear/And break it to our hope", confirmed James's belief that equivocation was a "wicked" practice, which reflected in turn the "wickedness" of the Catholic Church. Garnett had in his possession "A Treatise on Equivocation", and in the play the Weird Sisters often engage in equivocation, for instance telling Macbeth that he could never be overthrown until "Great Birnan wood to high Dunsinane hill/Shall Come". Macbeth interprets the prophecy as meaning never, but in fact, the Three Sisters refer only to branches of the trees of Great Birnan coming to Dunsinane hill.

"Macbeth" cannot be dated precisely but is usually taken as contemporaneous to the other canonical tragedies ("Hamlet", "Othello", and "King Lear"). While some scholars have placed the original writing of the play as early as 1599, most believe that the play is unlikely to have been composed earlier than 1603 as the play is widely seen to celebrate King James' ancestors and the Stuart accession to the throne in 1603 (James believed himself to be descended from Banquo), suggesting that the parade of eight kings—which the witches show Macbeth in a vision in Act IV—is a compliment to King James. Many scholars think the play was written in 1606 in the aftermath of the Gunpowder Plot, citing possible internal allusions to the 1605 plot and its ensuing trials. In fact, there are a great number of allusions and possible pieces of evidence alluding to the Plot, and, for this reason, a great many critics agree that "Macbeth" was written in the year 1606. Lady Macbeth's instructions to her husband, "Look like the innocent flower, but be the serpent under't" (1.5.74–75), may be an allusion to a medal that was struck in 1605 to commemorate King James' escape that depicted a serpent hiding among lilies and roses.

Particularly, the Porter's speech (2.3.1–21) in which he welcomes an "equivocator", a farmer, and a tailor to hell (2.3.8–13), has been argued to be an allusion to the 28 March 1606 trial and execution on 3 May 1606 of the Jesuit Henry Garnet, who used the alias "Farmer", with "equivocator" referring to Garnet's defence of "equivocation". The porter says that the equivocator "committed treason enough for God's sake" (2.3.9–10), which specifically connects equivocation and treason and ties it to the Jesuit belief that equivocation was only lawful when used "for God's sake", strengthening the allusion to Garnet. The porter goes on to say that the equivocator "yet could not equivocate to heaven" (2.3.10–11), echoing grim jokes that were current on the eve of Garnet's execution: i.e. that Garnet would be "hanged without equivocation" and at his execution he was asked "not to equivocate with his last breath." The "English tailor" the porter admits to hell (2.3.13), has been seen as an allusion to Hugh Griffin, a tailor who was questioned by the Archbishop of Canterbury on 27 November and 3 December 1607 for the part he played in Garnet's "miraculous straw", an infamous head of straw that was stained with Garnet's blood that had congealed into a form resembling Garnet's portrait, which was hailed by Catholics as a miracle. The tailor Griffin became notorious and the subject of verses published with his portrait on the title page.

When James became king of England, a feeling of uncertainty settled over the nation. James was a Scottish king and the son of Mary, Queen of Scots, a staunch Catholic and English traitor. In the words of critic Robert Crawford, ""Macbeth" was a play for a post-Elizabethan England facing up to what it might mean to have a Scottish king. England seems comparatively benign, while its northern neighbour is mired in a bloody, monarch-killing past. ... "Macbeth" may have been set in medieval Scotland, but it was filled with material of interest to England and England's ruler." Critics argue that the content of the play is clearly a message to James, the new Scottish King of England. Likewise, the critic Andrew Hadfield noted the contrast the play draws between the saintly King Edward the Confessor of England who has the power of the royal touch to cure scrofula and whose realm is portrayed as peaceful and prosperous vs. the bloody chaos of Scotland. James in his 1598 book "The Trew Law of Free Monarchies" had asserted that kings are always right, if not just, and his subjects owe him total loyalty at all times, writing that even if a king is a tyrant, his subjects must never rebel and just endure his tyranny for their own good. James had argued that the tyranny was preferable to the problems caused by rebellion which were even worse; Shakespeare by contrast in "Macbeth" argued for the right of the subjects to overthrow a tyrant king, in what appeared to be an implied criticism of James's theories if applied to England. Hadfield also noted a curious aspect of the play in that it implies that primogeniture is the norm in Scotland, but Duncan has to nominate his son Malcolm to be his successor while Macbeth is accepted without protest by the Scottish lairds as their king despite being an usurper. Hadfield argued this aspect of the play with the thanes apparently choosing their king was a reference to the Stuart claim to the English throne, and the attempts of the English parliament to block the succession of James's Catholic mother, Mary, Queen of Scots, from succeeding to the English throne. Hadfield argued that Shakespeare implied that James was indeed the rightful king of England, but owned his throne not to divine favour as James would have it, but rather due to the willingness of the English Parliament to accept the Protestant son of the Catholic Mary, Queen of Scots, as their king.

Garry Wills provides further evidence that "Macbeth" is a Gunpowder Play (a type of play that emerged immediately following the events of the Gunpowder Plot). He points out that every Gunpowder Play contains "a necromancy scene, regicide attempted or completed, references to equivocation, scenes that test loyalty by use of deceptive language, and a character who sees through plots—along with a vocabulary similar to the Plot in its immediate aftermath (words like "train, blow, vault") and an ironic recoil of the Plot upon the Plotters (who fall into the pit they dug)."

The play utilizes a few key words that the audience at the time would recognize as allusions to the Plot. In one sermon in 1605, Lancelot Andrewes stated, regarding the failure of the Plotters on God's day, "Be they fair or foul, glad or sad (as the poet calleth Him) the great Diespiter, 'the Father of days' hath made them both." Shakespeare begins the play by using the words "fair" and "foul" in the first speeches of the witches and Macbeth. In the words of Jonathan Gil Harris, the play expresses the "horror unleashed by a supposedly loyal subject who seeks to kill a king and the treasonous role of equivocation. The play even echoes certain keywords from the scandal—the 'vault' beneath the House of Parliament in which Guy Fawkes stored thirty kegs of gunpowder and the 'blow' about which one of the conspirators had secretly warned a relative who planned to attend the House of Parliament on 5 November...Even though the Plot is never alluded to directly, its presence is everywhere in the play, like a pervasive odor."
Scholars also cite an entertainment seen by King James at Oxford in the summer of 1605 that featured three "sibyls" like the weird sisters; Kermode surmises that Shakespeare could have heard about this and alluded to it with the weird sisters. However, A. R. Braunmuller in the New Cambridge edition finds the 1605–06 arguments inconclusive, and argues only for an earliest date of 1603.

One suggested allusion supporting a date in late 1606 is the first witch's dialogue about a sailor's wife: "'Aroint thee, witch!' the rump-fed ronyon cries./Her husband's to Aleppo gone, master o' the "Tiger"" (1.3.6–7). This has been thought to allude to the "Tiger", a ship that returned to England 27 June 1606 after a disastrous voyage in which many of the crew were killed by pirates. A few lines later the witch speaks of the sailor, "He shall live a man forbid:/Weary se'nnights nine times nine" (1.3.21–22). The real ship was at sea 567 days, the product of 7x9x9, which has been taken as a confirmation of the allusion, which if correct, confirms that the witch scenes were either written or amended later than July 1606.

The play is not considered to have been written any later than 1607, since, as Kermode notes, there are "fairly clear allusions to the play in 1607." One notable reference is in Francis Beaumont's "Knight of the Burning Pestle", first performed in 1607. The following lines (Act V, Scene 1, 24–30) are, according to scholars, a clear allusion to the scene in which Banquo's ghost haunts Macbeth at the dinner table:

<poem>
When thou art at thy table with thy friends,
Merry in heart, and filled with swelling wine,
I'll come in midst of all thy pride and mirth,
Invisible to all men but thyself,
And whisper such a sad tale in thine ear
Shall make thee let the cup fall from thy hand,
And stand as mute and pale as death itself.</poem>

"Macbeth" was first printed in the First Folio of 1623 and the Folio is the only source for the text. Some scholars contend that the Folio text was abridged and rearranged from an earlier manuscript or prompt book. Often cited as interpolation are stage cues for two songs, whose lyrics are not included in the Folio but are included in Thomas Middleton's play "The Witch", which was written between the accepted date for "Macbeth" (1606) and the printing of the Folio. Many scholars believe these songs were editorially inserted into the Folio, though whether they were Middleton's songs or preexisting songs is not certain. It is also widely believed that the character of Hecate, as well as some lines of the First Witch (4.1 124–31), were not part of Shakespeare's original play but were added by the Folio editors and possibly written by Middleton, though "there is no completely objective proof" of such interpolation.

The 'reconstructive movement' was concerned with the recreation of Elizabethan acting conditions, and would eventually lead to the creation of Shakespeare's Globe and similar replicas. One of the movement's offshoots was in the reconstruction of Elizabethan pronunciation: for example Bernard Miles' 1951 "Macbeth", for which linguists from University College London were employed to create a transcript of the play in Elizabethan English, then an audio recording of that transcription, from which the actors, in turn, learned their lines.

The pronunciation of many words evolves over time. In Shakespeare's day, for example, "heath" was pronounced as "heth" ("or a slightly elongated 'e' as in the modern 'get'"), so it rhymed with "Macbeth" in the sentences by the Witches at the beginning of the play:

Second Witch: Upon the heath.Third Witch: There to meet with Macbeth.

A scholar of antique pronunciation writes, ""Heath" would have made a close (if not exact) rhyme with the "-eth" of "Macbeth", which was pronounced with a short 'i' as in 'it'."

In the theatre programme notes, "much was made of how OP [Original Pronunciation] performance reintroduces lost rhymes such as the final couplet: 'So thanks to all at once, and each to one, / Whom we invite to see us crowned at Scone'" (5.11.40–41) where 'one' sounds like 'own'. The Witches, the play's great purveyors of rhyme, benefited most in this regard. So, 'babe' (4.1.30) sounded like 'bab' and rhymed with 'drab' (4.1.31)..."

Eoin Price wrote, "I found the OP rendition of Banquo's brilliant question 'Or have we eaten on the insane root / That takes the raison prisoner?' unduly amusing"; and he adds,

"Macbeth" is an anomaly among Shakespeare's tragedies in certain critical ways. It is short: more than a thousand lines shorter than "Othello" and "King Lear", and only slightly more than half as long as "Hamlet". This brevity has suggested to many critics that the received version is based on a heavily cut source, perhaps a prompt-book for a particular performance. This would reflect other Shakespearean plays existing in both Quarto and the Folio, where the Quarto versions are usually longer than the Folio versions. "Macbeth" was first printed in the First Folio, but has no Quarto version – if there were a Quarto, it would probably be longer than the Folio version. That brevity has also been connected to other unusual features: the fast pace of the first act, which has seemed to be "stripped for action"; the comparative flatness of the characters other than Macbeth; and the oddness of Macbeth himself compared with other Shakespearean tragic heroes. A. C. Bradley, in considering this question, concluded the play "always was an extremely short one", noting the witch scenes and battle scenes would have taken up some time in performance, remarking, "I do not think that, in reading, we "feel" Macbeth to be short: certainly we are astonished when we hear it is about half as long as "Hamlet". Perhaps in the Shakespearean theatre too it seemed to occupy a longer time than the clock recorded."

At least since the days of Alexander Pope and Samuel Johnson, analysis of the play has centred on the question of Macbeth's ambition, commonly seen as so dominant a trait that it defines the character. Johnson asserted that Macbeth, though esteemed for his military bravery, is wholly reviled.

This opinion recurs in critical literature, and, according to Caroline Spurgeon, is supported by Shakespeare himself, who apparently intended to degrade his hero by vesting him with clothes unsuited to him and to make Macbeth look ridiculous by several nimisms he applies: His garments seem either too big or too small for him – as his ambition is too big and his character too small for his new and unrightful role as king. When he feels as if "dressed in borrowed robes", after his new title as Thane of Cawdor, prophesied by the witches, has been confirmed by Ross (I, 3, ll. 108–09), Banquo comments: "New honours come upon him,<br>Like our strange garments, cleave not to their mould,<br>But with the aid of use" (I, 3, ll. 145–46). And, at the end, when the tyrant is at bay at Dunsinane, Caithness sees him as a man trying in vain to fasten a large garment on him with too small a belt: "He cannot buckle his distemper'd cause <br> Within the belt of rule" (V, 2, ll. 14–15) while Angus, in a similar nimism, sums up what everybody thinks ever since Macbeth's accession to power: "now does he feel his title <br> Hang loose about him, like a giant's robe <br> upon a dwarfish thief" (V, 2, ll. 18–20).

Like Richard III, but without that character's perversely appealing exuberance, Macbeth wades through blood until his inevitable fall. As Kenneth Muir writes, "Macbeth has not a predisposition to murder; he has merely an inordinate ambition that makes murder itself seem to be a lesser evil than failure to achieve the crown." Some critics, such as E. E. Stoll, explain this characterisation as a holdover from Senecan or medieval tradition. Shakespeare's audience, in this view, expected villains to be wholly bad, and Senecan style, far from prohibiting a villainous protagonist, all but demanded it.

Yet for other critics, it has not been so easy to resolve the question of Macbeth's motivation. Robert Bridges, for instance, perceived a paradox: a character able to express such convincing horror before Duncan's murder would likely be incapable of committing the crime. For many critics, Macbeth's motivations in the first act appear vague and insufficient. John Dover Wilson hypothesised that Shakespeare's original text had an extra scene or scenes where husband and wife discussed their plans. This interpretation is not fully provable; however, the motivating role of ambition for Macbeth is universally recognised. The evil actions motivated by his ambition seem to trap him in a cycle of increasing evil, as Macbeth himself recognises: "I am in blood<br>Stepp'd in so far that, should I wade no more,<br> Returning were as tedious as go o'er" (III, 4, ll. 134-36).

While working on Russian translations of Shakespeare's works, Boris Pasternak compared Macbeth to Raskolnikov, the protagonist of "Crime and Punishment" by Fyodor Dostoevsky. Pasternak argues that "neither Macbeth or Raskolnikov is a born criminal or a villain by nature. They are turned into criminals by faulty rationalizations, by deductions from false premises." He goes on to argue that Lady Macbeth is "feminine ... one of those active, insistent wives" who becomes her husband's "executive, more resolute and consistent than he is himself." According to Pasternak, she is only helping Macbeth carry out his own wishes, to her own detriment.

The disastrous consequences of Macbeth's ambition are not limited to him. Almost from the moment of the murder, the play depicts Scotland as a land shaken by inversions of the natural order. Shakespeare may have intended a reference to the great chain of being, although the play's images of disorder are mostly not specific enough to support detailed intellectual readings. He may also have intended an elaborate compliment to James's belief in the divine right of kings, although this hypothesis, outlined at greatest length by Henry N. Paul, is not universally accepted. As in "Julius Caesar", though, perturbations in the political sphere are echoed and even amplified by events in the material world. Among the most often depicted of the inversions of the natural order is sleep. Macbeth's announcement that he has "murdered sleep" is figuratively mirrored in Lady Macbeth's sleepwalking.

"Macbeth"s generally accepted indebtedness to medieval tragedy is often seen as significant in the play's treatment of moral order. Glynne Wickham connects the play, through the Porter, to a mystery play on the harrowing of hell. Howard Felperin argues that the play has a more complex attitude toward "orthodox Christian tragedy" than is often admitted; he sees a kinship between the play and the tyrant plays within the medieval liturgical drama.

The theme of androgyny is often seen as a special aspect of the theme of disorder. Inversion of normative gender roles is most famously associated with the witches and with Lady Macbeth as she appears in the first act. Whatever Shakespeare's degree of sympathy with such inversions, the play ends with a thorough return to normative gender values. Some feminist psychoanalytic critics, such as Janet Adelman, have connected the play's treatment of gender roles to its larger theme of inverted natural order. In this light, Macbeth is punished for his violation of the moral order by being removed from the cycles of nature (which are figured as female); nature itself (as embodied in the movement of Birnam Wood) is part of the restoration of moral order.

Critics in the early twentieth century reacted against what they saw as an excessive dependence on the study of character in criticism of the play. This dependence, though most closely associated with Andrew Cecil Bradley, is clear as early as the time of Mary Cowden Clarke, who offered precise, if fanciful, accounts of the predramatic lives of Shakespeare's female leads. She suggested, for instance, that the child Lady Macbeth refers to in the first act died during a foolish military action.

In the play, the Three Witches represent darkness, chaos, and conflict, while their role is as agents and witnesses. Their presence communicates treason and impending doom. During Shakespeare's day, witches were seen as worse than rebels, "the most notorious traytor and rebell that can be." They were not only political traitors, but spiritual traitors as well. Much of the confusion that springs from them comes from their ability to straddle the play's borders between reality and the supernatural. They are so deeply entrenched in both worlds that it is unclear whether they control fate, or whether they are merely its agents. They defy logic, not being subject to the rules of the real world. The witches' lines in the first act: "Fair is foul, and foul is fair: Hover through the fog and filthy air" are often said to set the tone for the rest of the play by establishing a sense of confusion. Indeed, the play is filled with situations where evil is depicted as good, while good is rendered evil. The line "Double, double toil and trouble," communicates the witches' intent clearly: they seek only trouble for the mortals around them. The witches' spells are remarkably similar to the spells of the witch Medusa in Anthony Munday's play "Fidele and Fortunio" published in 1584, and Shakespeare may have been influenced by these.

While the witches do not tell Macbeth directly to kill King Duncan, they use a subtle form of temptation when they tell Macbeth that he is destined to be king. By placing this thought in his mind, they effectively guide him on the path to his own destruction. This follows the pattern of temptation used at the time of Shakespeare. First, they argued, a thought is put in a man's mind, then the person may either indulge in the thought or reject it. Macbeth indulges in it, while Banquo rejects.

According to J. A. Bryant Jr., "Macbeth" also makes use of Biblical parallels, notably between King Duncan's murder and the murder of Christ:

While many today would say that any misfortune surrounding a production is mere coincidence, actors and others in the theatre industry often consider it bad luck to mention "Macbeth" by name while inside a theatre, and sometimes refer to it indirectly, for example as "The Scottish Play", or "MacBee", or when referring to the character and not the play, "Mr. and Mrs. M", or "The Scottish King".

This is because Shakespeare (or the play's revisers) are said to have used the spells of real witches in his text, purportedly angering the witches and causing them to curse the play. Thus, to say the name of the play inside a theatre is believed to doom the production to failure, and perhaps cause physical injury or death to cast members. There are stories of accidents, misfortunes and even deaths taking place during runs of "Macbeth".

According to the actor Sir Donald Sinden, in his Sky Arts TV series "Great West End Theatres", contrary to popular myth, Shakespeare's tragedy Macbeth is not the unluckiest play as superstition likes to portray it. Exactly the opposite! The origin of the unfortunate moniker dates back to repertory theatre days when each town and village had at least one theatre to entertain the public. If a play was not doing well, it would invariably get 'pulled' and replaced with a sure-fire audience pleaser – Macbeth guaranteed full-houses. So when the weekly theatre newspaper, "The Stage" was published, listing what was on in each theatre in the country, it was instantly noticed what shows had "not" worked the previous week, as they had been replaced by a definite crowd-pleaser. More actors have died during performances of Hamlet than in the "Scottish play" as the profession still calls it. It is forbidden to quote from it backstage as this could cause the current play to collapse and have to be replaced, causing possible unemployment.

Several methods exist to dispel the curse, depending on the actor. One, attributed to Michael York, is to immediately leave the building the stage is in with the person who uttered the name, walk around it three times, spit over their left shoulders, say an obscenity then wait to be invited back into the building. A related practice is to spin around three times as fast as possible on the spot, sometimes accompanied by spitting over their shoulder, and uttering an obscenity. Another popular "ritual" is to leave the room, knock three times, be invited in, and then quote a line from "Hamlet". Yet another is to recite lines from "The Merchant of Venice", thought to be a lucky play.

The only eyewitness account of "Macbeth" in Shakespeare's lifetime was recorded by Simon Forman, who saw a performance at the Globe on 20 April 1610. Scholars have noted discrepancies between Forman's account and the play as it appears in the Folio. For example, he makes no mention of the apparition scene, or of Hecate, of the man not of woman born, or of Birnam Wood. However, Clark observes that Forman's accounts were often inaccurate and incomplete (for instance omitting the statue scene from "The Winter's Tale") and his interest did not seem to be in "giving full accounts of the productions."

As mentioned above, the Folio text is thought by some to be an alteration of the original play. This has led to the theory that the play as we know it from the Folio was an adaptation for indoor performance at the Blackfriars Theatre (which was operated by the King's Men from 1608) – and even speculation that it represents a specific performance before King James. The play contains more musical cues than any other play in the canon as well as a significant use of sound effects.

All theatres were closed down by the Puritan government on 6 September 1642. Upon the restoration of the monarchy in 1660, two patent companies (the King's Company and the Duke's Company) were established, and the existing theatrical repertoire divided between them. Sir William Davenant, founder of the Duke's Company, adapted Shakespeare's play to the tastes of the new era, and his version would dominate on stage for around eighty years. Among the changes he made were the expansion of the role of the witches, introducing new songs, dances and 'flying', and the expansion of the role of Lady Macduff as a foil to Lady Macbeth. There were, however, performances outside the patent companies: among the evasions of the Duke's Company's monopoly was a puppet version of "Macbeth".

"Macbeth" was a favourite of the seventeenth-century diarist Samuel Pepys, who saw the play on 5 November 1664 ("admirably acted"), 28 December 1666 ("most excellently acted"), ten days later on 7 January 1667 ("though I saw it lately, yet [it] appears a most excellent play in all respects"), on 19 April 1667 ("one of the best plays for a stage ... that ever I saw"), again on 16 October 1667 ("was vexed to see Young, who is but a bad actor at best, act Macbeth in the room of Betterton, who, poor man! is sick"), and again three weeks later on 6 November 1667 ("[at] "Macbeth", which we still like mightily"), yet again on 12 August 1668 ("saw "Macbeth", to our great content"), and finally on 21 December 1668, on which date the king and court were also present in the audience.

The first professional performances of "Macbeth" in North America were probably those of The Hallam Company.

In 1744, David Garrick revived the play, abandoning Davenant's version and instead advertising it "as written by Shakespeare". In fact this claim was largely false: he retained much of Davenant's more popular business for the witches, and himself wrote a lengthy death speech for Macbeth. And he cut more than 10% of Shakespeare's play, including the drunken porter, the murder of Lady Macduff's son, and Malcolm's testing of Macduff. Hannah Pritchard was his greatest stage partner, having her premiere as his Lady Macbeth in 1747. He would later drop the play from his repertoire upon her retirement from the stage. Mrs. Pritchard was the first actress to achieve acclaim in the role of Lady Macbeth – at least partly due to the removal of Davenant's material, which made irrelevant moral contrasts with Lady Macduff. Garrick's portrayal focused on the inner life of the character, endowing him with an innocence vacillating between good and evil, and betrayed by outside influences. He portrayed a man capable of observing himself, as if a part of him remained untouched by what he had done, the play moulding him into a man of sensibility, rather than him descending into a tyrant.

John Philip Kemble first played Macbeth in 1778. Although usually regarded as the antithesis of Garrick, Kemble nevertheless refined aspects of Garrick's portrayal into his own. However it was the "towering and majestic" Sarah Siddons (Kemble's sister) who became a legend in the role of Lady Macbeth. In contrast to Hannah Pritchard's savage, demonic portrayal, Siddons' Lady Macbeth, while terrifying, was nevertheless – in the scenes in which she expresses her regret and remorse – tenderly human. And in portraying her actions as done out of love for her husband, Siddons deflected from him some of the moral responsibility for the play's carnage. Audiences seem to have found the sleepwalking scene particularly mesmerising: Hazlitt said of it that "all her gestures were involuntary and mechanical ... She glided on and off the stage almost like an apparition."

In 1794, Kemble dispensed with the ghost of Banquo altogether, allowing the audience to see Macbeth's reaction as his wife and guests see it, and relying upon the fact that the play was so well known that his audience would already be aware that a ghost enters at that point.

Ferdinand Fleck, notable as the first German actor to present Shakespeare's tragic roles in their fullness, played Macbeth at the Berlin National Theatre from 1787. Unlike his English counterparts, he portrayed the character as achieving his stature after the murder of Duncan, growing in presence and confidence: thereby enabling stark contrasts, such as in the banquet scene, which he ended babbling like a child.

Performances outside the patent theatres were instrumental in bringing the monopoly to an end. Robert Elliston, for example, produced a popular adaptation of "Macbeth" in 1809 at the Royal Circus described in its publicity as "this matchless piece of pantomimic and choral performance", which circumvented the illegality of speaking Shakespeare's words through mimed action, singing, and doggerel verse written by J. C. Cross.

In 1809, in an unsuccessful attempt to take Covent Garden upmarket, Kemble installed private boxes, increasing admission prices to pay for the improvements. The inaugural run at the newly renovated theatre was "Macbeth", which was disrupted for over two months with cries of "Old prices!" and "No private boxes!" until Kemble capitulated to the protestors' demands.

Edmund Kean at Drury Lane gave a psychological portrayal of the central character, with a common touch, but was ultimately unsuccessful in the role. However he did pave the way for the most acclaimed performance of the nineteenth century, that of William Charles Macready. Macready played the role over a 30-year period, firstly at Covent Garden in 1820 and finally in his retirement performance. Although his playing evolved over the years, it was noted throughout for the tension between the idealistic aspects and the weaker, venal aspects of Macbeth's character. His staging was full of spectacle, including several elaborate royal processions.

In 1843 the Theatres Regulation Act finally brought the patent companies' monopoly to an end. From that time until the end of the Victorian era, London theatre was dominated by the actor-managers, and the style of presentation was "pictorial" – proscenium stages filled with spectacular stage-pictures, often featuring complex scenery, large casts in elaborate costumes, and frequent use of tableaux vivant. Charles Kean (son of Edmund), at London's Princess's Theatre from 1850 to 1859, took an antiquarian view of Shakespeare performance, setting his "Macbeth" in a historically accurate eleventh-century Scotland. His leading lady, Ellen Tree, created a sense of the character's inner life: "The Times" critic saying "The countenance which she assumed ... when luring on Macbeth in his course of crime, was actually appalling in intensity, as if it denoted a hunger after guilt." At the same time, special effects were becoming popular: for example in Samuel Phelps' "Macbeth" the witches performed behind green gauze, enabling them to appear and disappear using stage lighting.

In 1849, rival performances of the play sparked the Astor Place riot in Manhattan. The popular American actor Edwin Forrest, whose Macbeth was said to be like "the ferocious chief of a barbarous tribe" played the central role at the Broadway Theatre to popular acclaim, while the "cerebral and patrician" English actor Macready, playing the same role at the Astor Place Opera House, suffered constant heckling. The existing enmity between the two men (Forrest had openly hissed Macready at a recent performance of "Hamlet" in Britain) was taken up by Forrest's supporters – formed from the working class and lower middle class and anti-British agitators, keen to attack the upper-class pro-British patrons of the Opera House and the colonially-minded Macready. Nevertheless, Macready performed the role again three days later to a packed house while an angry mob gathered outside. The militia tasked with controlling the situation fired into the mob. In total, 31 rioters were killed and over 100 injured.

Charlotte Cushman is unique among nineteenth century interpreters of Shakespeare in achieving stardom in roles of both genders. Her New York debut was as Lady Macbeth in 1836, and she would later be admired in London in the same role in the mid-1840s. Helen Faucit was considered the embodiment of early-Victorian notions of femininity. But for this reason she largely failed when she eventually played Lady Macbeth in 1864: her serious attempt to embody the coarser aspects of Lady Macbeth's character jarred harshly with her public image. Adelaide Ristori, the great Italian actress, brought her Lady Macbeth to London in 1863 in Italian, and again in 1873 in an English translation cut in such a way as to be, in effect, Lady Macbeth's tragedy.

Henry Irving was the most successful of the late-Victorian actor-managers, but his "Macbeth" failed to curry favour with audiences. His desire for psychological credibility reduced certain aspects of the role: He described Macbeth as a brave soldier but a moral coward, and played him untroubled by conscience – clearly already contemplating the murder of Duncan before his encounter with the witches. Irving's leading lady was Ellen Terry, but her Lady Macbeth was unsuccessful with the public, for whom a century of performances influenced by Sarah Siddons had created expectations at odds with Terry's conception of the role.

Late nineteenth-century European Macbeths aimed for heroic stature, but at the expense of subtlety: Tommaso Salvini in Italy and Adalbert Matkowsky in Germany were said to inspire awe, but elicited little pity.

Two developments changed the nature of "Macbeth" performance in the 20th century: first, developments in the craft of acting itself, especially the ideas of Stanislavski and Brecht; and second, the rise of the dictator as a political icon. The latter has not always assisted the performance: it is difficult to sympathise with a Macbeth based on Hitler, Stalin, or Idi Amin.

Barry Jackson, at the Birmingham Repertory Theatre in 1923, was the first of the 20th-century directors to costume "Macbeth" in modern dress.

In 1936, a decade before his film adaptation of the play, Orson Welles directed "Macbeth" for the Negro Theatre Unit of the Federal Theatre Project at the Lafayette Theatre in Harlem, using black actors and setting the action in Haiti: with drums and Voodoo rituals to establish the Witches scenes. The production, dubbed "The Voodoo Macbeth", proved inflammatory in the aftermath of the Harlem riots, accused of making fun of black culture and as "a campaign to burlesque negroes" until Welles persuaded crowds that his use of black actors and voodoo made important cultural statements.
A performance which is frequently referenced as an example of the play's curse was the outdoor production directed by Burgess Meredith in 1953 in the British colony of Bermuda, starring Charlton Heston. Using the imposing spectacle of Fort St. Catherine as a key element of the set, the production was plagued by a host of mishaps, including Charlton Heston being burned when his tights caught fire.

The critical consensus is that there have been three great Macbeths on the English-speaking stage in the 20th century, all of them commencing at Stratford-upon-Avon: Laurence Olivier in 1955, Ian McKellen in 1976 and Antony Sher in 1999. Olivier's portrayal (directed by Glen Byam Shaw, with Vivien Leigh as Lady Macbeth) was immediately hailed as a masterpiece. Kenneth Tynan expressed the view that it succeeded because Olivier built the role to a climax at the end of the play, whereas most actors spend all they have in the first two acts.

The play caused grave difficulties for the Royal Shakespeare Company, especially at the (then) Shakespeare Memorial Theatre. Peter Hall's 1967 production was (in Michael Billington's words) "an acknowledged disaster" with the use of real leaves from Birnham Wood getting unsolicited first-night laughs, and Trevor Nunn's 1974 production was (Billington again) "an over-elaborate religious spectacle".

But Nunn achieved success for the RSC in his 1976 production at the intimate Other Place, with Ian McKellen and Judi Dench in the central roles. A small cast worked within a simple circle, and McKellen's Macbeth had nothing noble or likeable about him, being a manipulator in a world of manipulative characters. They were a young couple, physically passionate, "not monsters but recognisable human beings", but their relationship atrophied as the action progressed.

The RSC again achieved critical success in Gregory Doran's 1999 production at The Swan, with Antony Sher and Harriet Walter in the central roles, once again demonstrating the suitability of the play for smaller venues. Doran's witches spoke their lines to a theatre in absolute darkness, and the opening visual image was the entrance of Macbeth and Banquo in the berets and fatigues of modern warfare, carried on the shoulders of triumphant troops. In contrast to Nunn, Doran presented a world in which king Duncan and his soldiers were ultimately benign and honest, heightening the deviance of Macbeth (who seems genuinely surprised by the witches' prophesies) and Lady Macbeth in plotting to kill the king. The play said little about politics, instead powerfully presenting its central characters' psychological collapse.

"Macbeth" returned to the RSC in 2018, when Christopher Eccleston played the title role, with Niamh Cusack as his wife, Lady Macbeth. The play later transferred to the Barbican in London.

In Soviet-controlled Prague in 1977, faced with the illegality of working in theatres, Pavel Kohout adapted "Macbeth" into a 75-minute abridgement for five actors, suitable for "bringing a show in a suitcase to people's homes".

Spectacle was unfashionable in Western theatre throughout the 20th century. In East Asia, however, spectacular productions have achieved great success, including Yukio Ninagawa's 1980 production with Masane Tsukayama as Macbeth, set in the 16th century Japanese Civil War. The same director's tour of London in 1987 was widely praised by critics, even though (like most of their audience) they were unable to understand the significance of Macbeth's gestures, the huge Buddhist altar dominating the set, or the petals falling from the cherry trees.

Xu Xiaozhong's 1980 Central Academy of Drama production in Beijing made every effort to be unpolitical (necessary in the aftermath of the Cultural Revolution): yet audiences still perceived correspondences between the central character (whom the director had actually modelled on Louis Napoleon) and Mao Zedong. Shakespeare has often been adapted to indigenous theatre traditions, for example the "Kunju Macbeth" of Huang Zuolin performed at the inaugural Chinese Shakespeare Festival of 1986. Similarly, B. V. Karanth's "Barnam Vana" of 1979 had adapted "Macbeth" to the Yakshagana tradition of Karnataka, India. In 1997, Lokendra Arambam created "Stage of Blood", merging a range of martial arts, dance and gymnastic styles from Manipur, performed in Imphal and in England. The stage was literally a raft on a lake.

"Throne of Blood" (蜘蛛巣城 Kumonosu-jō, "Spider Web Castle") is a 1957 Japanese samurai film co-written and directed by Akira Kurosawa. The film transposes Macbeth from Medieval Scotland to feudal Japan, with stylistic elements drawn from Noh drama. Kurosawa was a fan of the play and planned his own adaptation for several years, postponing it after learning of Orson Welles' Macbeth (1948). The film won two Mainichi Film Awards.

The play has been translated and performed in various languages in different parts of the world, and "Media Artists" was the first to stage its Punjabi adaptation in India. The adaptation by Balram and the play directed by Samuel John have been universally acknowledged as a milestone in Punjabi theatre. The unique attempt involved trained theatre experts and the actors taken from a rural background in Punjab. Punjabi folk music imbued the play with the native ethos as the Scottish setting of Shakespeare's play was transposed into a Punjabi milieu.

In September, 2018, "Macbeth" was faithfully adapted into a fully illustrated Manga edition, by Manga Classics, an imprint of UDON Entertainment.


All references to "Macbeth", unless otherwise specified, are taken from the Arden Shakespeare, second series edition edited by Kenneth Muir. Under their referencing system, III.I.55 means act 3, scene 1, line 55. All references to other Shakespeare plays are to The Oxford Shakespeare "Complete Works of Shakespeare" edited by Stanley Wells and Gary Taylor.



</doc>
<doc id="18870" url="https://en.wikipedia.org/wiki?curid=18870" title="Minor Threat">
Minor Threat

Minor Threat was an American hardcore punk band, formed in 1980 in Washington, D.C. by vocalist Ian MacKaye and drummer Jeff Nelson. MacKaye and Nelson had played in several other bands together, and recruited bassist Brian Baker and guitarist Lyle Preslar to form Minor Threat. They added a fifth member, Steve Hansgen, in 1982, playing bass, while Baker switched to second guitar. 

The band was relatively short-lived, disbanding after only four years together, but had a strong influence on the punk scene, both stylistically and in establishing a "do it yourself" ethic for music distribution and concert promotion. Minor Threat's song "Straight Edge" became the eventual basis of the straight edge movement, which emphasized a lifestyle without alcohol or other drugs, or promiscuous sex. AllMusic described Minor Threat's music as "iconic" and noted that their groundbreaking music "has held up better than [that of] most of their contemporaries."

Along with the fellow Washington, D.C. hardcore band Bad Brains and California band Black Flag, Minor Threat set the standard for many hardcore punk bands in the 1980s and 1990s. All of Minor Threat's recordings were released on MacKaye's and Nelson's own label, Dischord Records. The "Minor Threat" EP and their only full-length studio album "Out of Step" have received a number of accolades and are cited as landmarks of the hardcore punk genre.

Prior to forming Minor Threat in 1980, vocalist Ian MacKaye and drummer Jeff Nelson had played bass and drums respectively in the Teen Idles while attending Wilson High School. During their two-year career within the flourishing Washington D.C. hardcore punk scene, the Teen Idles had gained a following of around one hundred fans (a sizable amount at the time), and were seen as only second within the scene to the contemporary Bad Brains. MacKaye and Nelson were strong believers in the DIY mentality and an independent, underground music scene. After the breakup of the Teen Idles, they used the money earned through the band to create Dischord Records, an independent record label that would host the releases of the Teen Idles, Minor Threat, and numerous other D.C. punk bands.

Eager to start a new band after the Teen Idles, MacKaye and Nelson recruited guitarist Lyle Preslar and bassist Brian Baker. They played their first performance in December 1980 to fifty people in a basement, opening for Bad Brains, The Untouchables, Black Market Baby and S.O.A., all D.C. bands.

The band's first 7" EPs, "Minor Threat" and "In My Eyes", were released in 1981. The group became popular regionally and toured the east coast and Midwest.

"Straight Edge," a song from the band's first EP, helped to inspire the straight edge movement. The lyrics of the song relay MacKaye's first-person perspective of his personal choice of abstinence from alcohol and other drugs, a novel ideology for rock musicians which initially found a small but dedicated following. Other prominent groups that subsequently advocated the straight edge stance include SS Decontrol and 7 Seconds. Although the original song was not written as a manifesto or a "set of rules," many later bands inspired by this idea used it as such, and over the years since its release, the song and the term "straight edge" became the zeitgeist for an entire subculture, and indeed the basis for a paradigm shift that has persisted and grown consistently throughout the world. The term comes as the point of the story -- he doesn't want to do drugs or drink, so therefore the writer has an edge over those who do -- a straight edge.

"Out of Step", A Minor Threat song from their second EP, further demonstrates the said belief: "Don't smoke/Don't drink/Don't fuck/At least I can fucking think/I can't keep up/I'm out of step with the world." The "I" in the lyrics was usually only implied, mainly because it did not quite fit the rhythm of the song. Some of the other members of Minor Threat, Jeff Nelson in particular, took exception to what they saw as MacKaye's imperious attitude on the song. The song was later re-recorded, and the updated version of the song on the 1983 album "Out of Step", which is slower so the first-person use of "I" would be clearer, included a bridge where MacKaye explains his personal beliefs, explaining that his ideals, which at the time were not yet known as what became a collective philosophy, or in fact, known as "straight edge," "is not a set of rules; I'm not telling you what to do. All I'm saying is there are three things, that are like so important to the whole world that I don't happen to find much importance in, whether it's fucking, or whether it's playing golf, because of that, I feel... I can't keep up... (full chorus)". 

Minor Threat's song "Guilty of Being White" led to some accusations of racism, but MacKaye has strongly denied such intentions and said that some listeners misinterpreted his words. He claims that his experiences attending Wilson High School, whose student population was 70 percent black, inspired the song. There, many students bullied MacKaye and his friends. In an interview, MacKaye stated that he was offended that some perceived racist overtones in the lyrics, saying, "To me, at the time and now, it seemed clear it's an anti-racist song. Of course, it didn't occur to me at the time I wrote it that anybody outside of my twenty or thirty friends who I was singing to would ever have to actually ponder the lyrics or even consider them." Thrash metal band Slayer later covered the song, with the last iteration of the lyric "guilty of being white" changed to "guilty of being right."

In the time between the release of the band's second seven-inch EP and the "Out of Step" record, the band briefly split when guitarist Lyle Preslar moved to Illinois to attend college for a semester at Northwestern University. Preslar was a member of Big Black for a few tempestuous rehearsals. During that period, MacKaye and Nelson put together a studio-only project called Skewbald/Grand Union; in a reflection of the slowly increasing disagreements between the two musicians, they were unable to decide on one name. The group recorded three untitled songs, which would be released posthumously as Dischord's 50th release. During Minor Threat's inactive period, Brian Baker also briefly played guitar for Government Issue and appeared on the "Make an Effort" EP.

In March 1982, at the urging of Bad Brains' H.R., Preslar left college to reform Minor Threat. The reunited band featured an expanded lineup: Steve Hansgen joined as the band's bassist and Baker switched to second guitar.

Some in Minor Threat, particularly drummer Jeff Nelson, took exception to what they saw as MacKaye's imperious attitude on the song "Out of Step." When the song was re-recorded for the LP "Out of Step," MacKaye clearly sang "I don't drink/smoke/fuck" (as was the intent of his words all along). The band also inserted an overdubbed spoken section into the instrumental break before the last chorus with MacKaye stating, "This is not a set of rules, I'm not telling you what to do..." Recording engineer Don Zientara had inadvertently recorded an argument between drummer Nelson and lyricist/singer MacKaye that captured the message perfectly, so this was used. According to Mark Andersen and Mark Jenkins' "Dance of Days: Two Decades of Punk in the Nation's Capital", this argument was over exactly what would be said in the message that Nelson wanted MacKaye to record, stating essentially what he said without knowing it was being recorded. An ideological door had already been opened, however, and by 1983, some straight-edge punks, such as followers of the band SS Decontrol, were swatting beers out of people's hands at clubs.

Minor Threat broke up in 1983. A contributing factor was disagreement over musical direction. MacKaye was allegedly skipping rehearsal sessions towards the end of the band's career, and he wrote the lyrics to the songs on the "Salad Days" EP in the studio. That was quite a contrast with the earlier recordings, as he had written and co-written the music for much of the band's early material. Minor Threat, which had returned to being a four-piece group with the departure of Hansgen, played its final show on September 23, 1983, at the Lansburgh Cultural Center in Washington, D.C., sharing the bill with go-go band Trouble Funk, and Austin, Texas punk funk act the Big Boys. In a meaningful way, Minor Threat ended their final set with "Last Song", a tune whose name was also the original title of the band's song "Salad Days".

Following the breakup, MacKaye stated that he did not "check out" on hardcore, but in fact hardcore "checked out." Explaining this, he stated that at a 1984 Minutemen show, a fan struck MacKaye's younger brother Alec in the face, and he punched the fan back, then realizing that the violence was "stupid," and that he saw his role in the stupidity. MacKaye claimed that immediately after this he decided to leave the hardcore scene.

In March 1984, six months after the band broke up, the EPs "Minor Threat" and "In My Eyes" were compiled together and re-released as the "Minor Threat" album.

MacKaye went on to found Embrace with former members of the Faith, Egg Hunt with Jeff Nelson, and later Fugazi and the Evens, as well as collaborating on Pailhead.

Baker went on to play in Junkyard, the Meatmen, Dag Nasty and Government Issue. Since 1994, Baker has been a member of Bad Religion.

Preslar was briefly a member of Glenn Danzig's Samhain, and his playing appears on a few songs on the band's first record. He joined The Meatmen in 1984, along with fellow Minor Threat member Brian Baker. He later ran Caroline Records, signing and working with (among others) Peter Gabriel, Ben Folds, Chemical Brothers, and Idaho, and ran marketing for Sire Records. He graduated from Rutgers University School of Law and lives in New Jersey.

Nelson played less-frantic alternative rock with Three and The High-Back Chairs before retiring from live performance. He runs his own label, Adult Swim Records, distributed by Dischord, and is a graphic artist and a political activist in Toledo, Ohio. The band's own Dischord Records released material by many bands from the Washington, D.C., area, such as Government Issue, Void, Scream, Fugazi, Artificial Peace, Rites of Spring, Gray Matter, and Dag Nasty, and has become a respected independent record label.

Hansgen formed Second Wind with Rich Moore, a former Minor Threat roadie and drummer for the Untouchables. In 1992, he worked as a producer on the first Tool EP "Opiate".

In 2005, a mock-up of the cover of Minor Threat's first EP (also used on the "Minor Threat" LP and "Complete Discography" CD) was copied by athletic footwear manufacturer Nike for use on a promotional poster for a skateboarding tour called "Major Threat". Nike also altered Minor Threat's logo (designed by Jeff Nelson) for the same campaign, as well as featuring Nike shoes in the new picture, rather than the combat boots worn by Ian MacKaye's younger brother Alec on the original.

MacKaye issued a press statement condemning Nike's actions and said that he would discuss legal options with the other members of the band. Meanwhile, fans, at the encouragement of Dischord, organized a letter-writing campaign protesting Nike's infringement. On June 27, 2005, Nike issued a statement apologizing to Minor Threat, Dischord Records, and their fans for the "Major Threat" campaign and said that all promotional artwork (print and digital) that they could acquire were destroyed.

On October 29, 2005, Fox played the first few seconds of Minor Threat's "Salad Days" during an NFL broadcast. Use of the song was not cleared by Dischord Records or any of the members of Minor Threat. Fox claimed that the clip was too short to have violated any copyrights.

In 2007, Brooklyn-based company Wheelhouse Pickles marketed a pepper sauce named "Minor Threat Sauce". Requesting only that the original label design (which was based on the "Bottled Violence" artwork) be amended, Ian MacKaye gave the product his endorsement. A small mention of this was made in music magazine "Revolver", where MacKaye commented "I don't really like hot sauce but I like the Minor Threat stuff".

In 2013, Minor Threat shirts began appearing in Urban Outfitters stores. Ian MacKaye confirmed that the shirts were officially licensed. Having spent what he described as "a complete waste of time" trying to track down bootlegged Minor Threat merchandise, MacKaye and Dischord made arrangements with a merchandise company in California to manage licensing of the band's shirts, as well as working to ensure that bootleg manufacturers of the shirts were curtailed. In comments that appeared in "Rolling Stone", MacKaye called it "absurd" for the shirts to be sold for $28 but concluded that "my time is better spent doing other things" than dealing with shirts. Dischord had previously taken action against Forever 21 in 2009 for marketing unlicensed Minor Threat shirts.








</doc>
<doc id="18875" url="https://en.wikipedia.org/wiki?curid=18875" title="Mental event">
Mental event

A mental event is any event that happens within the mind of a conscious individual. Examples include thoughts, feelings, decisions, dreams, and realizations.

Some believe that mental events are not limited to human thought but can be associated with animals and artificial intelligence as well. Whether mental events are identical to complex physical events, or whether such an identity even makes sense, is central to the mind-body problem.

Some state that the mental and the physical are the very same property which cause any event(s). This view is known as substance monism. An opposing view is substance dualism, which claims that the mental and physical are fundamentally different and can exist independently. A third approach is Donald Davidson's "anomalous monism". The Philosophy of Action states that every action is caused by prior thoughts or feelings, and understanding those mental events would in turn explain behavior.

Physicalism, a form of substance monism, states that everything that exists is either physical or depends on that which is physical. The existence of mental events has been used by philosophers as an argument against physicalism. For example, in his 1974 paper "What Is it Like to Be a Bat?", Thomas Nagel argues that physicalist theories of mind cannot explain an organism's subjective experience because they cannot account for its mental events.





</doc>
<doc id="18878" url="https://en.wikipedia.org/wiki?curid=18878" title="Monopoly">
Monopoly

A monopoly (from Greek and ) exists when a specific person or enterprise is the only supplier of a particular commodity. This contrasts with a monopsony which relates to a single entity's control of a market to purchase a good or service, and with oligopoly which consists of a few sellers dominating a market. Monopolies are thus characterized by a lack of economic competition to produce the good or service, a lack of viable substitute goods, and the possibility of a high monopoly price well above the seller's marginal cost that leads to a high monopoly profit. The verb "monopolise" or "monopolize" refers to the "process" by which a company gains the ability to raise prices or exclude competitors. In economics, a monopoly is a single seller. In law, a monopoly is a business entity that has significant market power, that is, the power to charge overly high prices. Although monopolies may be big businesses, size is not a characteristic of a monopoly. A small business may still have the power to raise prices in a small industry (or market).

A monopoly may also have monopsony control of a sector of a market. Likewise, a monopoly should be distinguished from a cartel (a form of oligopoly), in which several providers act together to coordinate services, prices or sale of goods. Monopolies, monopsonies and oligopolies are all situations in which one or a few entities have market power and therefore interact with their customers (monopoly or oligopoly), or suppliers (monopsony) in ways that distort the market.

Monopolies can be established by a government, form naturally, or form by integration. In many jurisdictions, competition laws restrict monopolies due to government concerns over potential adverse effects. Holding a dominant position or a monopoly in a market is often not illegal in itself, however certain categories of behavior can be considered abusive and therefore incur legal sanctions when business is dominant. A government-granted monopoly or "legal monopoly", by contrast, is sanctioned by the state, often to provide an incentive to invest in a risky venture or enrich a domestic interest group. Patents, copyrights, and trademarks are sometimes used as examples of government-granted monopolies. The government may also reserve the venture for itself, thus forming a government monopoly, for example with a state-owned company.

Monopolies may be naturally occurring due to limited competition because the industry is resource intensive and requires substantial costs to operate (e.g., certain railroad systems).
In economics, the idea of monopoly is important in the study of management structures, which directly concerns normative aspects of economic competition, and provides the basis for topics such as industrial organization and economics of regulation. There are four basic types of market structures in traditional economic analysis: perfect competition, monopolistic competition, oligopoly and monopoly. A monopoly is a structure in which a single supplier produces and sells a given product or service. If there is a single seller in a certain market and there are no close substitutes for the product, then the market structure is that of a "pure monopoly". Sometimes, there are many sellers in an industry and/or there exist many close substitutes for the goods being produced, but nevertheless companies retain some market power. This is termed monopolistic competition, whereas in oligopoly the companies interact strategically.

In general, the main results from this theory compares the price-fixing methods across market structures, analyze the effect of a certain structure on welfare, and vary technological/demand assumptions in order to assess the consequences for an abstract model of society. Most economic textbooks follow the practice of carefully explaining the "perfect competition" model, mainly because this helps to understand "departures" from it (the so-called "imperfect competition" models).

The boundaries of what constitutes a market and what does not are relevant distinctions to make in economic analysis. In a general equilibrium context, a good is a specific concept including geographical and time-related characteristics. Most studies of market structure relax a little their definition of a good, allowing for more flexibility in the identification of substitute goods.

A monopoly has these five characteristics:

Monopolies derive their market power from barriers to entry – circumstances that prevent or greatly impede a potential competitor's ability to compete in a market. There are three major types of barriers to entry: economic, legal and deliberate.
In addition to barriers to entry and competition, barriers to exit may be a source of market power. Barriers to exit are market conditions that make it difficult or expensive for a company to end its involvement with a market. High liquidation costs are a primary barrier to exiting. Market exit and shutdown are sometimes separate events. The decision whether to shut down or operate is not affected by exit barriers. A company will shut down if price falls below minimum average variable costs.

While monopoly and perfect competition mark the extremes of market structures there is some similarity. The cost functions are the same. Both monopolies and perfectly competitive (PC) companies minimize cost and maximize profit. The shutdown decisions are the same. Both are assumed to have perfectly competitive factors markets. There are distinctions, some of the most important distinctions are as follows:

The most significant distinction between a PC company and a monopoly is that the monopoly has a downward-sloping demand curve rather than the "perceived" perfectly elastic curve of the PC company. Practically all the variations mentioned above relate to this fact. If there is a downward-sloping demand curve then by necessity there is a distinct marginal revenue curve. The implications of this fact are best made manifest with a linear demand curve. Assume that the inverse demand curve is of the form x = a − by. Then the total revenue curve is TR = ay − by and the marginal revenue curve is thus MR = a − 2by. From this several things are evident. First the marginal revenue curve has the same y intercept as the inverse demand curve. Second the slope of the marginal revenue curve is twice that of the inverse demand curve. Third the x intercept of the marginal revenue curve is half that of the inverse demand curve. What is not quite so evident is that the marginal revenue curve is below the inverse demand curve at all points. Since all companies maximise profits by equating MR and MC it must be the case that at the profit-maximizing quantity MR and MC are less than price, which further implies that a monopoly produces less quantity at a higher price than if the market were perfectly competitive.

The fact that a monopoly has a downward-sloping demand curve means that the relationship between total revenue and output for a monopoly is much different than that of competitive companies. Total revenue equals price times quantity. A competitive company has a perfectly elastic demand curve meaning that total revenue is proportional to output. Thus the total revenue curve for a competitive company is a ray with a slope equal to the market price. A competitive company can sell all the output it desires at the market price. For a monopoly to increase sales it must reduce price. Thus the total revenue curve for a monopoly is a parabola that begins at the origin and reaches a maximum value then continuously decreases until total revenue is again zero. Total revenue has its maximum value when the slope of the total revenue function is zero. The slope of the total revenue function is marginal revenue. So the revenue maximizing quantity and price occur when MR = 0. For example, assume that the monopoly's demand function is P = 50 − 2Q. The total revenue function would be TR = 50Q − 2Q and marginal revenue would be 50 − 4Q. Setting marginal revenue equal to zero we have

So the revenue maximizing quantity for the monopoly is 12.5 units and the revenue maximizing price is 25.

A company with a monopoly does not experience price pressure from competitors, although it may experience pricing pressure from potential competition. If a company increases prices too much, then others may enter the market if they are able to provide the same good, or a substitute, at a lesser price. The idea that monopolies in markets with easy entry need not be regulated against is known as the "revolution in monopoly theory".

A monopolist can extract only one premium, and getting into complementary markets does not pay. That is, the total profits a monopolist could earn if it sought to leverage its monopoly in one market by monopolizing a complementary market are equal to the extra profits it could earn anyway by charging more for the monopoly product itself. However, the one monopoly profit theorem is not true if customers in the monopoly good are stranded or poorly informed, or if the tied good has high fixed costs.

A pure monopoly has the same economic rationality of perfectly competitive companies, i.e. to optimise a profit function given some constraints. By the assumptions of increasing marginal costs, exogenous inputs' prices, and control concentrated on a single agent or entrepreneur, the optimal decision is to equate the marginal cost and marginal revenue of production. Nonetheless, a pure monopoly can – unlike a competitive company – alter the market price for its own convenience: a decrease of production results in a higher price. In the economics' jargon, it is said that pure monopolies have "a downward-sloping demand". An important consequence of such behaviour is worth noticing: typically a monopoly selects a higher price and lesser quantity of output than a price-taking company; again, less is available at a higher price.

A monopoly chooses that price that maximizes the difference between total revenue and total cost. The basic markup rule (as measured by the Lerner index) can be expressed as
formula_4,
where formula_5 is the price elasticity of demand the firm faces. The markup rules indicate that the ratio between profit margin and the price is inversely proportional to the price elasticity of demand. The implication of the rule is that the more elastic the demand for the product the less pricing power the monopoly has.

Market power is the ability to increase the product's price above marginal cost without losing all customers. Perfectly competitive (PC) companies have zero market power when it comes to setting prices. All companies of a PC market are price takers. The price is set by the interaction of demand and supply at the market or aggregate level. Individual companies simply take the price determined by the market and produce that quantity of output that maximizes the company's profits. If a PC company attempted to increase prices above the market level all its customers would abandon the company and purchase at the market price from other companies. A monopoly has considerable although not unlimited market power. A monopoly has the power to set prices or quantities although not both. A monopoly is a price maker. The monopoly is the market and prices are set by the monopolist based on their circumstances and not the interaction of demand and supply. The two primary factors determining monopoly market power are the company's demand curve and its cost structure.

Market power is the ability to affect the terms and conditions of exchange so that the price of a product is set by a single company (price is not imposed by the market as in perfect competition). Although a monopoly's market power is great it is still limited by the demand side of the market. A monopoly has a negatively sloped demand curve, not a perfectly inelastic curve. Consequently, any price increase will result in the loss of some customers.

Price discrimination allows a monopolist to increase its profit by charging higher prices for identical goods to those who are willing or able to pay more. For example, most economic textbooks cost more in the United States than in developing countries like Ethiopia. In this case, the publisher is using its government-granted copyright monopoly to price discriminate between the generally wealthier American economics students and the generally poorer Ethiopian economics students. Similarly, most patented medications cost more in the U.S. than in other countries with a (presumed) poorer customer base. Typically, a high general price is listed, and various market segments get varying discounts. This is an example of framing to make the process of charging some people higher prices more socially acceptable. Perfect price discrimination would allow the monopolist to charge each customer the exact maximum amount they would be willing to pay. This would allow the monopolist to extract all the consumer surplus of the market. While such perfect price discrimination is a theoretical construct, advances in information technology and micromarketing may bring it closer to the realm of possibility.

It is very important to realize that partial price discrimination can cause some customers who are inappropriately pooled with high price customers to be excluded from the market. For example, a poor student in the U.S. might be excluded from purchasing an economics textbook at the U.S. price, which the student may have been able to purchase at the Ethiopian price'. Similarly, a wealthy student in Ethiopia may be able to or willing to buy at the U.S. price, though naturally would hide such a fact from the monopolist so as to pay the reduced third world price. These are deadweight losses and decrease a monopolist's profits. As such, monopolists have substantial economic interest in improving their market information and "market segmenting".

There is important information for one to remember when considering the monopoly model diagram (and its associated conclusions) displayed here. The result that monopoly prices are higher, and production output lesser, than a competitive company follow from a requirement that the monopoly not charge different prices for different customers. That is, the monopoly is restricted from engaging in price discrimination (this is termed first degree price discrimination, such that all customers are charged the same amount). If the monopoly were permitted to charge individualised prices (this is termed third degree price discrimination), the quantity produced, and the price charged to the "marginal" customer, would be identical to that of a competitive company, thus eliminating the deadweight loss; however, all gains from trade (social welfare) would accrue to the monopolist and none to the consumer. In essence, every consumer would be indifferent between (1) going completely without the product or service and (2) being able to purchase it from the monopolist.

As long as the price elasticity of demand for most customers is less than one in absolute value, it is advantageous for a company to increase its prices: it receives more money for fewer goods. With a price increase, price elasticity tends to increase, and in the optimum case above it will be greater than one for most customers.

A company maximizes profit by selling where marginal revenue equals marginal cost. A company that does not engage in price discrimination will charge the profit maximizing price, P*, to all its customers. In such circumstances there are customers who would be willing to pay a higher price than P* and those who will not pay P* but would buy at a lower price. A price discrimination strategy is to charge less price sensitive buyers a higher price and the more price sensitive buyers a lower price. Thus additional revenue is generated from two sources. The basic problem is to identify customers by their willingness to pay.

The purpose of price discrimination is to transfer consumer surplus to the producer. Consumer surplus is the difference between the value of a good to a consumer and the price the consumer must pay in the market to purchase it. Price discrimination is not limited to monopolies.

Market power is a company's ability to increase prices without losing all its customers. Any company that has market power can engage in price discrimination. Perfect competition is the only market form in which price discrimination would be impossible (a perfectly competitive company has a perfectly elastic demand curve and has zero market power).

There are three forms of price discrimination. First degree price discrimination charges each consumer the maximum price the consumer is willing to pay. Second degree price discrimination involves quantity discounts. Third degree price discrimination involves grouping consumers according to willingness to pay as measured by their price elasticities of demand and charging each group a different price. Third degree price discrimination is the most prevalent type.

There are three conditions that must be present for a company to engage in successful price discrimination. First, the company must have market power. Second, the company must be able to sort customers according to their willingness to pay for the good. Third, the firm must be able to prevent resell.

A company must have some degree of market power to practice price discrimination. Without market power a company cannot charge more than the market price. Any market structure characterized by a downward sloping demand curve has market power – monopoly, monopolistic competition and oligopoly. The only market structure that has no market power is perfect competition.

A company wishing to practice price discrimination must be able to prevent middlemen or brokers from acquiring the consumer surplus for themselves. The company accomplishes this by preventing or limiting resale. Many methods are used to prevent resale. For instance, persons are required to show photographic identification and a boarding pass before boarding an airplane. Most travelers assume that this practice is strictly a matter of security. However, a primary purpose in requesting photographic identification is to confirm that the ticket purchaser is the person about to board the airplane and not someone who has repurchased the ticket from a discount buyer.

The inability to prevent resale is the largest obstacle to successful price discrimination. Companies have however developed numerous methods to prevent resale. For example, universities require that students show identification before entering sporting events. Governments may make it illegal to resale tickets or products. In Boston, Red Sox baseball tickets can only be resold legally to the team.

The three basic forms of price discrimination are first, second and third degree price discrimination. In "first degree price discrimination" the company charges the maximum price each customer is willing to pay. The maximum price a consumer is willing to pay for a unit of the good is the reservation price. Thus for each unit the seller tries to set the price equal to the consumer's reservation price. Direct information about a consumer's willingness to pay is rarely available. Sellers tend to rely on secondary information such as where a person lives (postal codes); for example, catalog retailers can use mail high-priced catalogs to high-income postal codes. First degree price discrimination most frequently occurs in regard to professional services or in transactions involving direct buyer/seller negotiations. For example, an accountant who has prepared a consumer's tax return has information that can be used to charge customers based on an estimate of their ability to pay.

In "second degree price discrimination" or quantity discrimination customers are charged different prices based on how much they buy. There is a single price schedule for all consumers but the prices vary depending on the quantity of the good bought. The theory of second degree price discrimination is a consumer is willing to buy only a certain quantity of a good at a given price. Companies know that consumer's willingness to buy decreases as more units are purchased. The task for the seller is to identify these price points and to reduce the price once one is reached in the hope that a reduced price will trigger additional purchases from the consumer. For example, sell in unit blocks rather than individual units.

In "third degree price discrimination" or multi-market price discrimination the seller divides the consumers into different groups according to their willingness to pay as measured by their price elasticity of demand. Each group of consumers effectively becomes a separate market with its own demand curve and marginal revenue curve. The firm then attempts to maximize profits in each segment by equating MR and MC, Generally the company charges a higher price to the group with a more price inelastic demand and a relatively lesser price to the group with a more elastic demand. Examples of third degree price discrimination abound. Airlines charge higher prices to business travelers than to vacation travelers. The reasoning is that the demand curve for a vacation traveler is relatively elastic while the demand curve for a business traveler is relatively inelastic. Any determinant of price elasticity of demand can be used to segment markets. For example, seniors have a more elastic demand for movies than do young adults because they generally have more free time. Thus theaters will offer discount tickets to seniors.

Assume that by a uniform pricing system the monopolist would sell five units at a price of $10 per unit. Assume that his marginal cost is $5 per unit. Total revenue would be $50, total costs would be $25 and profits would be $25. If the monopolist practiced price discrimination he would sell the first unit for $50 the second unit for $40 and so on. Total revenue would be $150, his total cost would be $25 and his profit would be $125.00. Several things are worth noting. The monopolist acquires all the consumer surplus and eliminates practically all the deadweight loss because he is willing to sell to anyone who is willing to pay at least the marginal cost. Thus the price discrimination promotes efficiency. Secondly, by the pricing scheme price = average revenue and equals marginal revenue. That is the monopolist behaving like a perfectly competitive company. Thirdly, the discriminating monopolist produces a larger quantity than the monopolist operating by a uniform pricing scheme.
Successful price discrimination requires that companies separate consumers according to their willingness to buy. Determining a customer's willingness to buy a good is difficult. Asking consumers directly is fruitless: consumers don't know, and to the extent they do they are reluctant to share that information with marketers. The two main methods for determining willingness to buy are observation of personal characteristics and consumer actions. As noted information about where a person lives (postal codes), how the person dresses, what kind of car he or she drives, occupation, and income and spending patterns can be helpful in classifying.

According to the standard model, in which a monopolist sets a single price for all consumers, the monopolist will sell a lesser quantity of goods at a higher price than would companies by perfect competition. Because the monopolist ultimately forgoes transactions with consumers who value the product or service more than its price, monopoly pricing creates a deadweight loss referring to potential gains that went neither to the monopolist nor to consumers. Given the presence of this deadweight loss, the combined surplus (or wealth) for the monopolist and consumers is necessarily less than the total surplus obtained by consumers by perfect competition. Where efficiency is defined by the total gains from trade, the monopoly setting is less efficient than perfect competition.

It is often argued that monopolies tend to become less efficient and less innovative over time, becoming "complacent", because they do not have to be efficient or innovative to compete in the marketplace. Sometimes this very loss of psychological efficiency can increase a potential competitor's value enough to overcome market entry barriers, or provide incentive for research and investment into new alternatives. The theory of contestable markets argues that in some circumstances (private) monopolies are forced to behave "as if" there were competition because of the risk of losing their monopoly to new entrants. This is likely to happen when a market's barriers to entry are low. It might also be because of the availability in the longer term of substitutes in other markets. For example, a canal monopoly, while worth a great deal during the late 18th century United Kingdom, was worth much less during the late 19th century because of the introduction of railways as a substitute.

Contrary to common misconception, monopolists do not try to sell items for the highest possible price, nor do they try to maximize profit per unit, but rather they try to maximize total profit.

A natural monopoly is an organization that experiences increasing returns to scale over the relevant range of output and relatively high fixed costs. A natural monopoly occurs where the average cost of production "declines throughout the relevant range of product demand". The relevant range of product demand is where the average cost curve is below the demand curve. When this situation occurs, it is always cheaper for one large company to supply the market than multiple smaller companies; in fact, absent government intervention in such markets, will naturally evolve into a monopoly. An early market entrant that takes advantage of the cost structure and can expand rapidly can exclude smaller companies from entering and can drive or buy out other companies. A natural monopoly suffers from the same inefficiencies as any other monopoly. Left to its own devices, a profit-seeking natural monopoly will produce where marginal revenue equals marginal costs. Regulation of natural monopolies is problematic. Fragmenting such monopolies is by definition inefficient. The most frequently used methods dealing with natural monopolies are government regulations and public ownership. Government regulation generally consists of regulatory commissions charged with the principal duty of setting prices.

To reduce prices and increase output, regulators often use average cost pricing. By average cost pricing, the price and quantity are determined by the intersection of the average cost curve and the demand curve. This pricing scheme eliminates any positive economic profits since price equals average cost. Average-cost pricing is not perfect. Regulators must estimate average costs. Companies have a reduced incentive to lower costs. Regulation of this type has not been limited to natural monopolies. Average-cost pricing does also have some disadvantages. By setting price equal to the intersection of the demand curve and the average total cost curve, the firm's output is allocatively inefficient as the price is less than the marginal cost (which is the output quantity for a perfectly competitive and allocatively efficient market).

A government-granted monopoly (also called a ""de jure" monopoly") is a form of "coercive monopoly", in which a government grants exclusive privilege to a private individual or company to be the sole provider of a commodity. Monopoly may be granted explicitly, as when potential competitors are excluded from the market by a specific law, or implicitly, such as when the requirements of an administrative regulation can only be fulfilled by a single market player, or through some other legal or procedural mechanism, such as patents, trademarks, and copyright.

A monopolist should shut down when price is less than average variable cost for every output level – in other words where the demand curve is entirely below the average variable cost curve. Under these circumstances at the profit maximum level of output (MR = MC) average revenue would be less than average variable costs and the monopolists would be better off shutting down in the short term.

In an unregulated market, monopolies can potentially be ended by new competition, breakaway businesses, or consumers seeking alternatives. In a regulated market, a government will often either regulate the monopoly, convert it into a publicly owned monopoly environment, or forcibly fragment it (see Antitrust law and trust busting). Public utilities, often being naturally efficient with only one operator and therefore less susceptible to efficient breakup, are often strongly regulated or publicly owned. American Telephone & Telegraph (AT&T) and Standard Oil are often cited as examples of the breakup of a private monopoly by government. The Bell System, later AT&T, was protected from competition first by the Kingsbury Commitment, and later by a series of agreements between AT&T and the Federal Government. In 1984, decades after having been granted monopoly power by force of law, AT&T was broken up into various components, MCI, Sprint, who were able to compete effectively in the long distance phone market.

The law regulating dominance in the European Union is governed by Article 102 of the "Treaty on the Functioning of the European Union" which aims at enhancing the consumer's welfare and also the efficiency of allocation of resources by protecting competition on the downstream market. The existence of a very high market share does not always mean consumers are paying excessive prices since the threat of new entrants to the market can restrain a high-market-share company's price increases. Competition law does not make merely having a monopoly illegal, but rather abusing the power a monopoly may confer, for instance through exclusionary practices (i.e. pricing high just because you are the only one around.) It may also be noted that it is illegal to try to obtain a monopoly, by practices of buying out the competition, or equal practices. If one occurs naturally, such as a competitor going out of business, or lack of competition, it is not illegal until such time as the monopoly holder abuses the power.

First it is necessary to determine whether a company is dominant, or whether it behaves "to an appreciable extent independently of its competitors, customers and ultimately of its consumer". Establishing dominance is a two-stage test. The first thing to consider is market definition which is one of the crucial factors of the test. It includes relevant product market and relevant geographic market.

As the definition of the market is of a matter of interchangeability, if the goods or services are regarded as interchangeable then they are within the same product market. For example, in the case of "United Brands v Commission", it was argued in this case that bananas and other fresh fruit were in the same product market and later on dominance was found because the special features of the banana made it could only be interchangeable with other fresh fruits in a limited extent and other and is only exposed to their competition in a way that is hardly perceptible. The demand substitutability of the goods and services will help in defining the product market and it can be access by the ‘hypothetical monopolist’ test or the ‘SSNIP’ test .

It is necessary to define it because some goods can only be supplied within a narrow area due to technical, practical or legal reasons and this may help to indicate which undertakings impose a competitive constraint on the other undertakings in question. Since some goods are too expensive to transport where it might not be economic to sell them to distant markets in relation to their value, therefore the cost of transporting is a crucial factor here. Other factors might be legal controls which restricts an undertaking in a Member States from exporting goods or services to another.

Market definition may be difficult to measure but is important because if it is defined too broadly, the undertaking may be more likely to be found dominant and if it is defined too narrowly, the less likely that it will be found dominant.

As with collusive conduct, market shares are determined with reference to the particular market in which the company and product in question is sold. It does not in itself determine whether an undertaking is dominant but work as an indicator of the states of the existing competition within the market. The Herfindahl-Hirschman Index (HHI) is sometimes used to assess how competitive an industry is. It sums up the squares of the individual market shares of all of the competitors within the market. The lower the total, the less concentrated the market and the higher the total, the more concentrated the market. In the US, the merger guidelines state that a post-merger HHI below 1000 is viewed as not concentrated while HHIs above that will provoke further review.

By European Union law, very large market shares raise a presumption that a company is dominant, which may be rebuttable. A market share of 100% may be very rare but it is still possible to be found and in fact it has been identified in some cases, for instance the "AAMS v Commission" case. Undertakings possessing market share that is lower than 100% but over 90% had also been found dominant, for example, Microsoft v Commission case. In the AKZO v Commission case, the undertaking is presumed to be dominant if it has a market share of 50%. There are also findings of dominance that are below a market share of 50%, for instance, United Brands v Commission, it only possessed a market share of 40% to 45% and still to be found dominant with other factors. The lowest yet market share of a company considered "dominant" in the EU was 39.7%.If a company has a dominant position, then there is a special responsibility not to allow its conduct to impair competition on the common market however these will all falls away if it is not dominant.

When considering whether an undertaking is dominant, it involves a combination of factors. Each of them cannot be taken separately as if they are, they will not be as determinative as they are when they are combined together. Also, in cases where an undertaking has previously been found dominant, it is still necessary to redefine the market and make a whole new analysis of the conditions of competition based on the available evidence at the appropriate time.

According to the Guidance, there are three more issues that must be examined. They are actual competitors that relates to the market position of the dominant undertaking and its competitors, potential competitors that concerns the expansion and entry and lastly the countervailing buyer power.

Market share may be a valuable source of information regarding the market structure and the market position when it comes to accessing it. The dynamics of the market and the extent to which the goods and services differentiated are relevant in this area.

It concerns with the competition that would come from other undertakings which are not yet operating in the market but will enter it in the future. So, market shares may not be useful in accessing the competitive pressure that is exerted on an undertaking in this area. The potential entry by new firms and expansions by an undertaking must be taken into account, therefore the barriers to entry and barriers to expansion is an important factor here.

Competitive constraints may not always come from actual or potential competitors. Sometimes, it may also come from powerful customers who have sufficient bargaining strength which come from its size or its commercial significance for a dominant firm.

There are three main types of abuses which are exploitative abuse, exclusionary abuse and single market abuse.

It arises when a monopolist has such significant market power that it can restrict its output while increasing the price above the competitive level without losing customers. This type is less concerned by the Commission than other types.

This is most concerned about by the Commissions because it is capable of causing long- term consumer damage and is more likely to prevent the development of competition. An example of it is exclusive dealing agreements.

It arises when a dominant undertaking carrying out excess pricing which would not only have an exploitative effect but also prevent parallel imports and limits intra- brand competition.


Despite wide agreement that the above constitute abusive practices, there is some debate about whether there needs to be a causal connection between the dominant position of a company and its actual abusive conduct. Furthermore, there has been some consideration of what happens when a company merely attempts to abuse its dominant position.

To provide a more specific example, economic and philosophical scholar Adam Smith cites that trade to the East India Company has, for the most part, been subjected to an exclusive company such as that of the English or Dutch. Monopolies such as these are generally established against the nation in which they arose out of. The profound economist goes on to state how there are two types of monopolies. The first type of monopoly is one which tends to always attract to the particular trade where the monopoly was conceived, a greater proportion of the stock of the society than what would go to that trade originally. The second type of monopoly tends to occasionally attract stock towards the particular trade where it was conceived, and sometimes repel it from that trade depending on varying circumstances. Rich countries tended to repel while poorer countries were attracted to this. For example, The Dutch company would dispose of any excess goods not taken to the market in order to preserve their monopoly while the English sold more goods for better prices. Both of these tendencies were extremely destructive as can be seen in Adam Smith's writings.

The term "monopoly" first appears in Aristotle's "Politics". Aristotle describes Thales of Miletus's cornering of the market in olive presses as a monopoly ("μονοπώλιον"). Another early reference to the concept of “monopoly” in a commercial sense appears in tractate Demai of the Mishna (2nd century C.E.), regarding the purchasing of agricultural goods from a dealer who has a monopoly on the produce (chapter 5; 4). The meaning and understanding of the English word 'monopoly' has changed over the years.

Vending of common salt (sodium chloride) was historically a natural monopoly. Until recently, a combination of strong sunshine and low humidity or an extension of peat marshes was necessary for producing salt from the sea, the most plentiful source. Changing sea levels periodically caused salt "famines" and communities were forced to depend upon those who controlled the scarce inland mines and salt springs, which were often in hostile areas (e.g. the Sahara desert) requiring well-organised security for transport, storage, and distribution.

The Salt Commission was a legal monopoly in China. Formed in 758, the Commission controlled salt production and sales in order to raise tax revenue for the Tang Dynasty.

The "Gabelle" was a notoriously high tax levied upon salt in the Kingdom of France. The much-hated levy had a role in the beginning of the French Revolution, when strict legal controls specified who was allowed to sell and distribute salt. First instituted in 1286, the Gabelle was not permanently abolished until 1945.

Robin Gollan argues in "The Coalminers of New South Wales" that anti-competitive practices developed in the coal industry of Australia's Newcastle as a result of the business cycle. The monopoly was generated by formal meetings of the local management of coal companies agreeing to fix a minimum price for sale at dock. This collusion was known as "The Vend". The Vend ended and was reformed repeatedly during the late 19th century, ending by recession in the business cycle. "The Vend" was able to maintain its monopoly due to trade union assistance, and material advantages (primarily coal geography). During the early 20th century, as a result of comparable monopolistic practices in the Australian coastal shipping business, the Vend developed as an informal and illegal collusion between the steamship owners and the coal industry, eventually resulting in the High Court case Adelaide Steamship Co. Ltd v. R. & AG.

Standard Oil was an American oil producing, transporting, refining, and marketing company. Established in 1870, it became the largest oil refiner in the world. John D. Rockefeller was a founder, chairman and major shareholder. The company was an innovator in the development of the business trust. The Standard Oil trust streamlined production and logistics, lowered costs, and undercut competitors. "Trust-busting" critics accused Standard Oil of using aggressive pricing to destroy competitors and form a monopoly that threatened consumers. Its controversial history as one of the world's first and largest multinational corporations ended in 1911, when the United States Supreme Court ruled that Standard was an illegal monopoly. The Standard Oil trust was dissolved into 33 smaller companies; two of its surviving "child" companies are ExxonMobil and the Chevron Corporation.

U.S. Steel has been accused of being a monopoly. J. P. Morgan and Elbert H. Gary founded U.S. Steel in 1901 by combining Andrew Carnegie's Carnegie Steel Company with Gary's Federal Steel Company and William Henry "Judge" Moore's National Steel Company. At one time, U.S. Steel was the largest steel producer and largest corporation in the world. In its first full year of operation, U.S. Steel made 67 percent of all the steel produced in the United States. However, U.S. Steel's share of the expanding market slipped to 50 percent by 1911, and antitrust prosecution that year failed.

De Beers settled charges of price fixing in the diamond trade in the 2000s. De Beers is well known for its monopoloid practices throughout the 20th century, whereby it used its dominant position to manipulate the international diamond market. The company used several methods to exercise this control over the market. Firstly, it convinced independent producers to join its single channel monopoly, it flooded the market with diamonds similar to those of producers who refused to join the cartel, and lastly, it purchased and stockpiled diamonds produced by other manufacturers in order to control prices through limiting supply.

In 2000, the De Beers business model changed due to factors such as the decision by producers in Russia, Canada and Australia to distribute diamonds outside the De Beers channel, as well as rising awareness of blood diamonds that forced De Beers to "avoid the risk of bad publicity" by limiting sales to its own mined products. De Beers' market share by value fell from as high as 90% in the 1980s to less than 40% in 2012, having resulted in a more fragmented diamond market with more transparency and greater liquidity.

In November 2011 the Oppenheimer family announced its intention to sell the entirety of its 40% stake in De Beers to Anglo American plc thereby increasing Anglo American's ownership of the company to 85%.[30] The transaction was worth £3.2 billion ($5.1 billion) in cash and ended the Oppenheimer dynasty's 80-year ownership of De Beers.

A public utility (or simply "utility") is an organization or company that maintains the infrastructure for a public service or provides a set of services for public consumption. Common examples of utilities are electricity, natural gas, water, sewage, cable television, and telephone. In the United States, public utilities are often natural monopolies because the infrastructure required to produce and deliver a product such as electricity or water is very expensive to build and maintain.

Western Union was criticized as a "price gouging" monopoly in the late 19th century. American Telephone & Telegraph was a telecommunications giant. AT&T was broken up in 1984. In the case of Telecom New Zealand, local loop unbundling was enforced by central government.

Telkom is a semi-privatised, part state-owned South African telecommunications company. Deutsche Telekom is a former state monopoly, still partially state owned. Deutsche Telekom currently monopolizes high-speed VDSL broadband network. The Long Island Power Authority (LIPA) provided electric service to over 1.1 million customers in Nassau and Suffolk counties of New York, and the Rockaway Peninsula in Queens.

The Comcast Corporation is the largest mass media and communications company in the world by revenue. It is the largest cable company and home Internet service provider in the United States, and the nation's third largest home telephone service provider. Comcast has a monopoly in Boston, Philadelphia, and many other small towns across the US.

The United Aircraft and Transport Corporation was an aircraft manufacturer holding company that was forced to divest itself of airlines in 1934.

Iarnród Éireann, the Irish Railway authority, is a current monopoly as Ireland does not have the size for more companies.

The Long Island Rail Road (LIRR) was founded in 1834, and since the mid-1800s has provided train service between Long Island and New York City. In the 1870s, LIRR became the sole railroad in that area through a series of acquisitions and consolidations. In 2013, the LIRR's commuter rail system is the busiest commuter railroad in North America, serving nearly 335,000 passengers daily.

Dutch East India Company was created as a legal trading monopoly in 1602. The "Vereenigde Oost-Indische Compagnie" enjoyed huge profits from its spice monopoly through most of the 17th century.

The British East India Company was created as a legal trading monopoly in 1600. The East India Company was formed for pursuing trade with the East Indies but ended up trading mainly with the Indian subcontinent, North-West Frontier Province, and Balochistan. The Company traded in basic commodities, which included cotton, silk, indigo dye, salt, saltpetre, tea and opium.

Major League Baseball survived U.S. antitrust litigation in 1922, though its special status is still in dispute as of 2009.

The National Football League survived antitrust lawsuit in the 1960s but was convicted of being an illegal monopoly in the 1980s.


According to professor Milton Friedman, laws against monopolies cause more harm than good, but unnecessary monopolies should be countered by removing tariffs and other regulation that upholds monopolies.

However, professor Steve H. Hanke believes that although private monopolies are more efficient than public ones, often by a factor of two, sometimes private natural monopolies, such as local water distribution, should be regulated (not prohibited) by, e.g., price auctions.

Thomas DiLorenzo asserts, however, that during the early days of utility companies where there was little regulation, there were no natural monopolies and there was competition. Only when companies realized that they could gain power through government did monopolies begin to form.

Baten, Bianchi and Moser find historical evidence that monopolies which are protected by patent laws may have adverse effects on the creation of innovation in an economy. They argue that under certain circumstances, compulsory licensing – which allows governments to license patents without the consent of patent-owners – may be effective in promoting invention by increasing the threat of competition in fields with low pre-existing levels of competition.


</doc>
<doc id="18879" url="https://en.wikipedia.org/wiki?curid=18879" title="Massachusetts Institute of Technology">
Massachusetts Institute of Technology

Massachusetts Institute of Technology (MIT) is a private research university in Cambridge, Massachusetts. The institute is a land-grant, sea-grant, and space-grant university, with an urban campus that extends more than a mile (1.6 km) alongside the Charles River. The institute also encompasses a number of major off-campus facilities such as the MIT Lincoln Laboratory, the Bates Center, and the Haystack Observatory, as well as affiliated laboratories such as the Broad and Whitehead Institutes. Founded in 1861 in response to the increasing industrialization of the United States, MIT adopted a European polytechnic university model and stressed laboratory instruction in applied science and engineering. It has since played a key role in the development of many aspects of modern science, engineering, mathematics, and technology, and is widely known for its innovation and academic strength.

, 96 Nobel laureates, 26 Turing Award winners, and 8 Fields Medalists have been affiliated with MIT as alumni, faculty members, or researchers. In addition, 58 National Medal of Science recipients, 29 National Medals of Technology and Innovation recipients, 50 MacArthur Fellows, 73 Marshall Scholars, 48 Rhodes Scholars, 41 astronauts, and 16 Chief Scientists of the U.S. Air Force have been affiliated with MIT. The school also has a strong entrepreneurial culture. MIT is a member of the Association of American Universities (AAU).

In 1859, a proposal was submitted to the Massachusetts General Court to use newly filled lands in Back Bay, Boston for a "Conservatory of Art and Science", but the proposal failed. A charter for the incorporation of the Massachusetts Institute of Technology, proposed by William Barton Rogers, was signed by John Albion Andrew, the governor of Massachusetts, on April 10, 1861.

Rogers, a professor from the University of Virginia, wanted to establish an institution to address rapid scientific and technological advances. He did not wish to found a professional school, but a combination with elements of both professional and liberal education, proposing that:

The true and only practicable object of a polytechnic school is, as I conceive, the teaching, not of the minute details and manipulations of the arts, which can be done only in the workshop, but the inculcation of those scientific principles which form the basis and explanation of them, and along with this, a full and methodical review of all their leading processes and operations in connection with physical laws.

The Rogers Plan reflected the German research university model, emphasizing an independent faculty engaged in research, as well as instruction oriented around seminars and laboratories.<ref name="Angulo https://archive.org/details/williambartonrog00angu/page/155 155–156"></ref>

Two days after MIT was chartered, the first battle of the Civil War broke out. After a long delay through the war years, MIT's first classes were held in the Mercantile Building in Boston in 1865. The new institute was founded as part of the Morrill Land-Grant Colleges Act to fund institutions "to promote the liberal and practical education of the industrial classes" and was a land-grant school. In 1863 under the same act, the Commonwealth of Massachusetts founded the Massachusetts Agricultural College, which developed as the University of Massachusetts Amherst. In 1866, the proceeds from land sales went toward new buildings in the Back Bay.

MIT was informally called "Boston Tech". The institute adopted the European polytechnic university model and emphasized laboratory instruction from an early date. Despite chronic financial problems, the institute saw growth in the last two decades of the 19th century under President Francis Amasa Walker. Programs in electrical, chemical, marine, and sanitary engineering were introduced, new buildings were built, and the size of the student body increased to more than one thousand.

The curriculum drifted to a vocational emphasis, with less focus on theoretical science. The fledgling school still suffered from chronic financial shortages which diverted the attention of the MIT leadership. During these "Boston Tech" years, MIT faculty and alumni rebuffed Harvard University president (and former MIT faculty) Charles W. Eliot's repeated attempts to merge MIT with Harvard College's Lawrence Scientific School. There would be at least six attempts to absorb MIT into Harvard. In its cramped Back Bay location, MIT could not afford to expand its overcrowded facilities, driving a desperate search for a new campus and funding. Eventually, the MIT Corporation approved a formal agreement to merge with Harvard, over the vehement objections of MIT faculty, students, and alumni. However, a 1917 decision by the Massachusetts Supreme Judicial Court effectively put an end to the merger scheme.
In 1916, the MIT administration and the MIT charter crossed the Charles River on the ceremonial barge "Bucentaur" built for the occasion, to signify MIT's move to a spacious new campus largely consisting of filled land on a tract along the Cambridge side of the Charles River. The neoclassical "New Technology" campus was designed by William W. Bosworth and had been funded largely by anonymous donations from a mysterious "Mr. Smith", starting in 1912. In January 1920, the donor was revealed to be the industrialist George Eastman of Rochester, New York, who had invented methods of film production and processing, and founded Eastman Kodak. Between 1912 and 1920, Eastman donated $20 million ($ million in 2015 dollars) in cash and Kodak stock to MIT.

In 1931 Stanislav Shumovsky enrolled. He later communicated much technical information on aviation to the Soviet Union. Other MIT spies sent other secrets about American technology, eventually including atomic bomb secrets.

In the 1930s, President Karl Taylor Compton and Vice-President (effectively Provost) Vannevar Bush emphasized the importance of pure sciences like physics and chemistry and reduced the vocational practice required in shops and drafting studios. The Compton reforms "renewed confidence in the ability of the Institute to develop leadership in science as well as in engineering". Unlike Ivy League schools, MIT catered more to middle-class families, and depended more on tuition than on endowments or grants for its funding. The school was elected to the Association of American Universities in 1934.

Still, as late as 1949, the Lewis Committee lamented in its report on the state of education at MIT that "the Institute is widely conceived as basically a vocational school", a "partly unjustified" perception the committee sought to change. The report comprehensively reviewed the undergraduate curriculum, recommended offering a broader education, and warned against letting engineering and government-sponsored research detract from the sciences and humanities. The School of Humanities, Arts, and Social Sciences and the MIT Sloan School of Management were formed in 1950 to compete with the powerful Schools of Science and Engineering. Previously marginalized faculties in the areas of economics, management, political science, and linguistics emerged into cohesive and assertive departments by attracting respected professors and launching competitive graduate programs. The School of Humanities, Arts, and Social Sciences continued to develop under the successive terms of the more humanistically oriented presidents Howard W. Johnson and Jerome Wiesner between 1966 and 1980.

MIT's involvement in military science surged during World War II. In 1941, Vannevar Bush was appointed head of the federal Office of Scientific Research and Development and directed funding to only a select group of universities, including MIT. Engineers and scientists from across the country gathered at MIT's Radiation Laboratory, established in 1940 to assist the British military in developing microwave radar. The work done there significantly affected both the war and subsequent research in the area. Other defense projects included gyroscope-based and other complex control systems for gunsight, bombsight, and inertial navigation under Charles Stark Draper's Instrumentation Laboratory; the development of a digital computer for flight simulations under Project Whirlwind; and high-speed and high-altitude photography under Harold Edgerton. By the end of the war, MIT became the nation's largest wartime R&D contractor (attracting some criticism of Bush), employing nearly 4000 in the Radiation Laboratory alone and receiving in excess of $100 million ($ billion in 2015 dollars) before 1946. Work on defense projects continued even after then. Post-war government-sponsored research at MIT included SAGE and guidance systems for ballistic missiles and Project Apollo.

These activities affected MIT profoundly. A 1949 report noted the lack of "any great slackening in the pace of life at the Institute" to match the return to peacetime, remembering the "academic tranquility of the prewar years", though acknowledging the significant contributions of military research to the increased emphasis on graduate education and rapid growth of personnel and facilities. The faculty doubled and the graduate student body quintupled during the terms of Karl Taylor Compton, president of MIT between 1930 and 1948; James Rhyne Killian, president from 1948 to 1957; and Julius Adams Stratton, chancellor from 1952 to 1957, whose institution-building strategies shaped the expanding university. By the 1950s, MIT no longer simply benefited the industries with which it had worked for three decades, and it had developed closer working relationships with new patrons, philanthropic foundations and the federal government.

In late 1960s and early 1970s, student and faculty activists protested against the Vietnam War and MIT's defense research. In this period MIT's various departments were researching helicopters, smart bombs and counterinsurgency techniques for the war in Vietnam as well as guidance systems for nuclear missiles. The Union of Concerned Scientists was founded on March 4, 1969 during a meeting of faculty members and students seeking to shift the emphasis on military research toward environmental and social problems. MIT ultimately divested itself from the Instrumentation Laboratory and moved all classified research off-campus to the MIT Lincoln Laboratory facility in 1973 in response to the protests. The student body, faculty, and administration remained comparatively unpolarized during what was a tumultuous time for many other universities. Johnson was seen to be highly successful in leading his institution to "greater strength and unity" after these times of turmoil. However six MIT students were sentenced to prison terms at this time and some former student leaders, such as Michael Albert and George Katsiaficas, are still indignant about MIT's role in military research and its suppression of these protests. (Richard Leacock's film, "November Actions", records some of these tumultuous events.)

In the 1980s, there was more controversy at MIT over its involvement in SDI (space weaponry) and CBW (chemical and biological warfare) research. More recently, MIT's research for the military has included work on robots, drones and 'battle suits'.

MIT has kept pace with and helped to advance the digital age. In addition to developing the predecessors to modern computing and networking technologies, students, staff, and faculty members at Project MAC, the Artificial Intelligence Laboratory, and the Tech Model Railroad Club wrote some of the earliest interactive computer video games like "Spacewar!" and created much of modern hacker slang and culture. Several major computer-related organizations have originated at MIT since the 1980s: Richard Stallman's GNU Project and the subsequent Free Software Foundation were founded in the mid-1980s at the AI Lab; the MIT Media Lab was founded in 1985 by Nicholas Negroponte and Jerome Wiesner to promote research into novel uses of computer technology; the World Wide Web Consortium standards organization was founded at the Laboratory for Computer Science in 1994 by Tim Berners-Lee; the OpenCourseWare project has made course materials for over 2,000 MIT classes available online free of charge since 2002; and the One Laptop per Child initiative to expand computer education and connectivity to children worldwide was launched in 2005.

MIT was named a sea-grant college in 1976 to support its programs in oceanography and marine sciences and was named a space-grant college in 1989 to support its aeronautics and astronautics programs. Despite diminishing government financial support over the past quarter century, MIT launched several successful development campaigns to significantly expand the campus: new dormitories and athletics buildings on west campus; the Tang Center for Management Education; several buildings in the northeast corner of campus supporting research into biology, brain and cognitive sciences, genomics, biotechnology, and cancer research; and a number of new "backlot" buildings on Vassar Street including the Stata Center. Construction on campus in the 2000s included expansions of the Media Lab, the Sloan School's eastern campus, and graduate residences in the northwest. In 2006, President Hockfield launched the MIT Energy Research Council to investigate the interdisciplinary challenges posed by increasing global energy consumption.

In 2001, inspired by the open source and open access movements, MIT launched OpenCourseWare to make the lecture notes, problem sets, syllabi, exams, and lectures from the great majority of its courses available online for no charge, though without any formal accreditation for coursework completed. While the cost of supporting and hosting the project is high, OCW expanded in 2005 to include other universities as a part of the OpenCourseWare Consortium, which currently includes more than 250 academic institutions with content available in at least six languages. In 2011, MIT announced it would offer formal certification (but not credits or degrees) to online participants completing coursework in its "MITx" program, for a modest fee. The "edX" online platform supporting MITx was initially developed in partnership with Harvard and its analogous "Harvardx" initiative. The courseware platform is open source, and other universities have already joined and added their own course content. In March 2009 the MIT faculty adopted an open-access policy to make its scholarship publicly accessible online.

MIT has its own police force. Three days after the Boston Marathon bombing of April 2013, MIT Police patrol officer Sean Collier was fatally shot by the suspects Dzhokhar and Tamerlan Tsarnaev, setting off a violent manhunt that shut down the campus and much of the Boston metropolitan area for a day. One week later, Collier's memorial service was attended by more than 10,000 people, in a ceremony hosted by the MIT community with thousands of police officers from the New England region and Canada. On November 25, 2013, MIT announced the creation of the Collier Medal, to be awarded annually to "an individual or group that embodies the character and qualities that Officer Collier exhibited as a member of the MIT community and in all aspects of his life". The announcement further stated that "Future recipients of the award will include those whose contributions exceed the boundaries of their profession, those who have contributed to building bridges across the community, and those who consistently and selflessly perform acts of kindness".

In September 2017, the school announced the creation of an artificial intelligence research lab called the MIT-IBM Watson AI Lab. IBM will spend $240 million over the next decade, and the lab will be staffed by MIT and IBM scientists. In October 2018 MIT announced that it would open a new Schwarzman College of Computing dedicated to the study of artificial intelligence, named after lead donor and The Blackstone Group CEO Stephen Schwarzman. The focus of the new college is to study not just AI, but interdisciplinary AI education, and how AI can be used in fields as diverse as history and biology. The cost of buildings and new faculty for the new college is expected to be $1 billion upon completion.

Over the course of 20 years, MIT received approximately $800,000 via foundations controlled by Jeffrey Epstein, convicted sex offender charged with the sex trafficking and sexual abuse of minors. All of those gifts went either to the MIT Media Lab or to Professor Seth Lloyd. Both Lloyd and former Media Lab Director Joi Ito have made public statements apologizing to Jeffrey Epstein's victims and others for judgments made over a series of years. Ito resigned from his position as Media Lab Director and professor, but Lloyd did not. Lloyd has been placed on temporary leave and there have been several protests from the MIT community urging for his resignation.

MIT's campus in the city of Cambridge spans approximately a mile along the north side of the Charles River basin. The campus is divided roughly in half by Massachusetts Avenue, with most dormitories and student life facilities to the west and most academic buildings to the east. The bridge closest to MIT is the Harvard Bridge, which is known for being marked off in a non-standard unit of length – the smoot.

The Kendall/MIT MBTA Red Line station is located on the northeastern edge of the campus, in Kendall Square. The Cambridge neighborhoods surrounding MIT are a mixture of high tech companies occupying both modern office and rehabilitated industrial buildings, as well as socio-economically diverse residential neighborhoods. In early 2016, MIT presented its updated Kendall Square Initiative to the City of Cambridge, with plans for mixed-use educational, retail, residential, startup incubator, and office space in a dense high-rise transit-oriented development plan. The MIT Museum will eventually be moved immediately adjacent to a Kendall Square subway entrance, joining the List Visual Arts Center on the eastern end of the campus.

Each building at MIT has a number (possibly preceded by a "W", "N", "E", or "NW") designation and most have a name as well. Typically, academic and office buildings are referred to primarily by number while residence halls are referred to by name. The organization of building numbers roughly corresponds to the order in which the buildings were built and their location relative (north, west, and east) to the original center cluster of Maclaurin buildings. Many of the buildings are connected above ground as well as through an extensive network of tunnels, providing protection from the Cambridge weather as well as a venue for roof and tunnel hacking.

MIT's on-campus nuclear reactor is one of the most powerful university-based nuclear reactors in the United States. The prominence of the reactor's containment building in a densely populated area has been controversial, but MIT maintains that it is well-secured. In 1999 Bill Gates donated US$20 million to MIT for the construction of a computer laboratory named the "William H. Gates Building", and designed by architect Frank Gehry. While Microsoft had previously given financial support to the institution, this was the first personal donation received from Gates.

MIT Nano, also known as Building 12, is an interdisciplinary facility for nanoscale research. Its cleanroom and research space, visible through expansive glass facades, is the largest research facility of its kind in the nation. With a cost of US$400 million, it is also one of the costliest buildings on campus. The facility also provides state-of-the-art nanoimaging capabilities with vibration damped imaging and metrology suites sitting atop a slab of concrete underground.

Other notable campus facilities include a pressurized wind tunnel for testing aerodynamic research, a towing tank for testing ship and ocean structure designs, and Alcator C-Mod, the largest fusion device operated by any university. MIT's campus-wide wireless network was completed in the fall of 2005 and consists of nearly 3,000 access points covering of campus.

In 2001, the Environmental Protection Agency sued MIT for violating the Clean Water Act and the Clean Air Act with regard to its hazardous waste storage and disposal procedures. MIT settled the suit by paying a $155,000 fine and launching three environmental projects. In connection with capital campaigns to expand the campus, the Institute has also extensively renovated existing buildings to improve their energy efficiency. MIT has also taken steps to reduce its environmental impact by running alternative fuel campus shuttles, subsidizing public transportation passes, and building a low-emission cogeneration plant that serves most of the campus electricity, heating, and cooling requirements.

The MIT Police with state and local authorities, in the 2009–2011 period, have investigated reports of 12 forcible sex offenses, 6 robberies, 3 aggravated assaults, 164 burglaries, 1 case of arson, and 4 cases of motor vehicle theft on campus; affecting a community of around 22,000 students and employees.

MIT has substantial commercial real estate holdings in Cambridge on which it pays property taxes, plus an additional voluntary payment in lieu of taxes (PILOT) on academic buildings which are legally tax-exempt. , it is the largest taxpayer in the city, contributing approximately 14% of the city's annual revenues. Holdings include Technology Square, parts of Kendall Square, and many properties in Cambridgeport and Area 4 neighboring the educational buildings. The land is held for investment purposes and potential long-term expansion.

MIT's School of Architecture, now the School of Architecture and Planning, was the first in the United States, and it has a history of commissioning progressive buildings. The first buildings constructed on the Cambridge campus, completed in 1916, are sometimes called the "Maclaurin buildings" after Institute president Richard Maclaurin who oversaw their construction. Designed by William Welles Bosworth, these imposing buildings were built of reinforced concrete, a first for a non-industrial – much less university – building in the US. Bosworth's design was influenced by the City Beautiful Movement of the early 1900s and features the Pantheon-esque Great Dome housing the Barker Engineering Library. The Great Dome overlooks Killian Court, where graduation ceremonies are held each year. The friezes of the limestone-clad buildings around Killian Court are engraved with the names of important scientists and philosophers. The spacious Building 7 atrium at 77 Massachusetts Avenue is regarded as the entrance to the Infinite Corridor and the rest of the campus.

Alvar Aalto's Baker House (1947), Eero Saarinen's MIT Chapel and Kresge Auditorium (1955), and I.M. Pei's Green, Dreyfus, Landau, and Wiesner buildings represent high forms of post-war modernist architecture. More recent buildings like Frank Gehry's Stata Center (2004), Steven Holl's Simmons Hall (2002), Charles Correa's Building 46 (2005), and Fumihiko Maki's Media Lab Extension (2009) stand out among the Boston area's classical architecture and serve as examples of contemporary campus "starchitecture". These buildings have not always been well received; in 2010, "The Princeton Review" included MIT in a list of twenty schools whose campuses are "tiny, unsightly, or both".

Undergraduates are guaranteed four-year housing in one of MIT's 10 undergraduate dormitories. Those living on campus can receive support and mentoring from live-in graduate student tutors, resident advisors, and faculty housemasters. Because housing assignments are made based on the preferences of the students themselves, diverse social atmospheres can be sustained in different living groups; for example, according to the "Yale Daily News" staff's "The Insider's Guide to the Colleges, 2010", "The split between East Campus and West Campus is a significant characteristic of MIT. East Campus has gained a reputation as a thriving counterculture." MIT also has 5 dormitories for single graduate students and 2 apartment buildings on campus for married student families.

MIT has an active Greek and co-op housing system, including thirty-six fraternities, sororities, and independent living groups (FSILGs). , 98% of all undergraduates lived in MIT-affiliated housing; 54% of the men participated in fraternities and 20% of the women were involved in sororities. Most FSILGs are located across the river in Back Bay near where MIT was founded, and there is also a cluster of fraternities on MIT's West Campus that face the Charles River Basin. After the 1997 alcohol-related death of Scott Krueger, a new pledge at the Phi Gamma Delta fraternity, MIT required all freshmen to live in the dormitory system starting in 2002. Because FSILGs had previously housed as many as 300 freshmen off-campus, the new policy could not be implemented until Simmons Hall opened in that year.

In 2013–2014, MIT abruptly closed and then demolished undergrad dorm Bexley Hall, citing extensive water damage that made repairs infeasible. In 2017, MIT shut down Senior House after a century of service as an undergrad dorm. That year, MIT administrators released data showing just 60% of Senior House residents had graduated in four years. Campus-wide, the four-year graduation rate is 84% (the cumulative graduation rate is significantly higher).

MIT is chartered as a non-profit organization and is owned and governed by a privately appointed board of trustees known as the MIT Corporation. The current board consists of 43 members elected to five-year terms, 25 life members who vote until their 75th birthday, 3 elected officers (President, Treasurer, and Secretary), and 4 "ex officio" members (the president of the alumni association, the Governor of Massachusetts, the Massachusetts Secretary of Education, and the Chief Justice of the Massachusetts Supreme Judicial Court). The board is chaired by Robert Millard, a co-founder of L-3 Communications Holdings. The Corporation approves the budget, new programs, degrees and faculty appointments, and elects the President to serve as the chief executive officer of the university and preside over the Institute's faculty. MIT's endowment and other financial assets are managed through a subsidiary called MIT Investment Management Company (MITIMCo). Valued at $16.4 billion in 2018, MIT's endowment was then the sixth-largest among American colleges and universities.

MIT has five schools (Science, Engineering, Architecture and Planning, Management, and Humanities, Arts, and Social Sciences) and one college (Schwarzman College of Computing), but no schools of law or medicine. While faculty committees assert substantial control over many areas of MIT's curriculum, research, student life, and administrative affairs, the chair of each of MIT's 32 academic departments reports to the dean of that department's school, who in turn reports to the Provost under the President. The current president is L. Rafael Reif, who formerly served as provost under President Susan Hockfield, the first woman to hold the post.

MIT is a large, highly residential, research university with a majority of enrollments in graduate and professional programs. The university has been accredited by the New England Association of Schools and Colleges since 1929. MIT operates on a 4–1–4 academic calendar with the fall semester beginning after Labor Day and ending in mid-December, a 4-week "Independent Activities Period" in the month of January, and the spring semester commencing in early February and ceasing in late May.

MIT students refer to both their majors and classes using numbers or acronyms alone. Departments and their corresponding majors are numbered in the approximate order of their foundation; for example, Civil and Environmental Engineering is , while Linguistics and Philosophy is . Students majoring in Electrical Engineering and Computer Science (EECS), the most popular department, collectively identify themselves as "Course 6". MIT students use a combination of the department's course number and the number assigned to the class to identify their subjects; for instance, the introductory calculus-based classical mechanics course is simply "8.01" at MIT.

The four-year, full-time undergraduate program maintains a balance between professional majors and those in the arts and sciences, and has been dubbed "most selective" by "U.S. News", admitting few transfer students and 6.7% of its applicants in the 2017–2018 admissions cycle. MIT offers 44 undergraduate degrees across its five schools. In the 2017–2018 academic year, 1,045 bachelor of science degrees (abbreviated "SB") were granted, the only type of undergraduate degree MIT now awards. In the 2011 fall term, among students who had designated a major, the School of Engineering was the most popular division, enrolling 63% of students in its 19 degree programs, followed by the School of Science (29%), School of Humanities, Arts, & Social Sciences (3.7%), Sloan School of Management (3.3%), and School of Architecture and Planning (2%). The largest undergraduate degree programs were in Electrical Engineering and Computer Science (), Computer Science and Engineering (), Mechanical Engineering (), Physics (), and Mathematics ().

All undergraduates are required to complete a core curriculum called the General Institute Requirements (GIRs). The Science Requirement, generally completed during freshman year as prerequisites for classes in science and engineering majors, comprises two semesters of physics, two semesters of calculus, one semester of chemistry, and one semester of biology. There is a Laboratory Requirement, usually satisfied by an appropriate class in a course major. The Humanities, Arts, and Social Sciences (HASS) Requirement consists of eight semesters of classes in the humanities, arts, and social sciences, including at least one semester from each division as well as the courses required for a designated concentration in a HASS division. Under the Communication Requirement, two of the HASS classes, plus two of the classes taken in the designated major must be "communication-intensive", including "substantial instruction and practice in oral presentation". Finally, all students are required to complete a swimming test; non-varsity athletes must also take four quarters of physical education classes.

Most classes rely on a combination of lectures, recitations led by associate professors or graduate students, weekly problem sets ("p-sets"), and periodic quizzes or tests. While the pace and difficulty of MIT coursework has been compared to "drinking from a fire hose", the freshmen retention rate at MIT is similar to other research universities. The "pass/no-record" grading system relieves some pressure for first-year undergraduates. For each class taken in the fall term, freshmen transcripts will either report only that the class was passed, or otherwise not have any record of it. In the spring term, passing grades (A, B, C) appear on the transcript while non-passing grades are again not recorded. (Grading had previously been "pass/no record" all freshman year, but was amended for the Class of 2006 to prevent students from gaming the system by completing required major classes in their freshman year.) Also, freshmen may choose to join alternative learning communities, such as Experimental Study Group, Concourse, or Terrascope.

In 1969, Margaret MacVicar founded the Undergraduate Research Opportunities Program (UROP) to enable undergraduates to collaborate directly with faculty members and researchers. Students join or initiate research projects ("UROPs") for academic credit, pay, or on a volunteer basis through postings on the UROP website or by contacting faculty members directly. A substantial majority of undergraduates participate. Students often become published, file patent applications, and/or launch start-up companies based upon their experience in UROPs.

In 1970, the then-Dean of Institute Relations, Benson R. Snyder, published "The Hidden Curriculum," arguing that education at MIT was often slighted in favor of following a set of unwritten expectations and that graduating with good grades was more often the product of figuring out the system rather than a solid education. The successful student, according to Snyder, was the one who was able to discern which of the formal requirements were to be ignored in favor of which unstated norms. For example, organized student groups had compiled "course bibles"—collections of problem-set and examination questions and answers for later students to use as references. This sort of gamesmanship, Snyder argued, hindered development of a creative intellect and contributed to student discontent and unrest.

MIT's graduate program has high coexistence with the undergraduate program, and many courses are taken by qualified students at both levels. MIT offers a comprehensive doctoral program with degrees in the humanities, social sciences, and STEM fields as well as professional degrees. The Institute offers graduate programs leading to academic degrees such as the Master of Science (which is abbreviated as SM at MIT), various Engineer's Degrees, Doctor of Philosophy (PhD), and Doctor of Science (ScD) and interdisciplinary graduate programs such as the MD-PhD (with Harvard Medical School) and a joint program in oceanography with Woods Hole Oceanographic Institution.

Admission to graduate programs is decentralized; applicants apply directly to the department or degree program. More than 90% of doctoral students are supported by fellowships, research assistantships (RAs), or teaching assistantships (TAs).

MIT awarded 1,547 master's degrees and 609 doctoral degrees in the academic year 2010–11. In the 2011 fall term, the School of Engineering was the most popular academic division, enrolling 45.0% of graduate students, followed by the Sloan School of Management (19%), School of Science (16.9%), School of Architecture and Planning (9.2%), Whitaker College of Health Sciences (5.1%), and School of Humanities, Arts, and Social Sciences (4.7%). The largest graduate degree programs were the Sloan MBA, Electrical Engineering and Computer Science, and Mechanical Engineering.

MIT also places among the top five in many overall rankings of universities (see right) and rankings based on students' revealed preferences. For several years, "U.S. News & World Report", the QS World University Rankings, and the Academic Ranking of World Universities have ranked MIT's School of Engineering first, as did the 1995 National Research Council report. In the same lists, MIT's strongest showings apart from in engineering are in computer science, the natural sciences, business, architecture, economics, linguistics, mathematics, and, to a lesser extent, political science and philosophy.

Times Higher Education has recognized MIT as one of the world's "six super brands" on its "World Reputation Rankings", along with Berkeley, Cambridge, Harvard, Oxford and Stanford. In 2019, it ranked 3rd among the universities around the world by SCImago Institutions Rankings. In 2017, the Times Higher Education World University Rankings rated MIT the #2 university for arts and humanities. MIT was ranked #7 in 2015 and #6 in 2017 of the Nature Index Annual Tables, which measure the largest contributors to papers published in 82 leading journals.

The university historically pioneered research and training collaborations between academia, industry and government.  In 1946, President Compton, Harvard Business School professor Georges Doriot, and Massachusetts Investor Trust chairman Merrill Grisswold founded American Research and Development Corporation, the first American venture-capital firm.  In 1948, Compton established the MIT Industrial Liaison Program. Throughout the late 1980s and early 1990s, American politicians and business leaders accused MIT and other universities of contributing to a declining economy by transferring taxpayer-funded research and technology to international – especially Japanese – firms that were competing with struggling American businesses. On the other hand, MIT's extensive collaboration with the federal government on research projects has led to several MIT leaders serving as presidential scientific advisers since 1940. MIT established a Washington Office in 1991 to continue effective lobbying for research funding and national science policy.

The US Justice Department began an investigation in 1989, and in 1991 filed an antitrust suit against MIT, the eight Ivy League colleges, and eleven other institutions for allegedly engaging in price-fixing during their annual "Overlap Meetings", which were held to prevent bidding wars over promising prospective students from consuming funds for need-based scholarships. While the Ivy League institutions settled, MIT contested the charges, arguing that the practice was not anti-competitive because it ensured the availability of aid for the greatest number of students. MIT ultimately prevailed when the Justice Department dropped the case in 1994.

MIT's proximity to Harvard University ("the other school up the river") has led to a substantial number of research collaborations such as the Harvard-MIT Division of Health Sciences and Technology and the Broad Institute. In addition, students at the two schools can cross-register for credits toward their own school's degrees without any additional fees. A cross-registration program between MIT and Wellesley College has also existed since 1969, and in 2002 the Cambridge–MIT Institute launched an undergraduate exchange program between MIT and the University of Cambridge. MIT also has a long term partnership with Imperial College London, for both student exchanges and research collaboration. More modest cross-registration programs have been established with Boston University, Brandeis University, Tufts University, Massachusetts College of Art and the School of the Museum of Fine Arts, Boston.

MIT maintains substantial research and faculty ties with independent research organizations in the Boston area, such as the Charles Stark Draper Laboratory, the Whitehead Institute for Biomedical Research, and the Woods Hole Oceanographic Institution. Ongoing international research and educational collaborations include the Amsterdam Institute for Advanced Metropolitan Solutions (AMS Institute), Singapore-MIT Alliance, MIT-Politecnico di Milano, MIT-Zaragoza International Logistics Program, and projects in other countries through the MIT International Science and Technology Initiatives (MISTI) program.

The mass-market magazine "Technology Review" is published by MIT through a subsidiary company, as is a special edition that also serves as an alumni magazine. The MIT Press is a major university press, publishing over 200 books and 30 journals annually, emphasizing science and technology as well as arts, architecture, new media, current events and social issues.

The MIT library system consists of five subject libraries: Barker (Engineering), Dewey (Economics), Hayden (Humanities and Science), Lewis (Music), and Rotch (Arts and Architecture). There are also various specialized libraries and archives. The libraries contain more than 2.9 million printed volumes, 2.4 million microforms, 49,000 print or electronic journal subscriptions, and 670 reference databases. The past decade has seen a trend of increased focus on digital over print resources in the libraries. Notable collections include the Lewis Music Library with an emphasis on 20th and 21st-century music and electronic music, the List Visual Arts Center's rotating exhibitions of contemporary art, and the Compton Gallery's cross-disciplinary exhibitions. MIT allocates a percentage of the budget for all new construction and renovation to commission and support its extensive public art and outdoor sculpture collection.

The MIT Museum was founded in 1971 and collects, preserves, and exhibits artifacts significant to the culture and history of MIT. The museum now engages in significant educational outreach programs for the general public, including the annual Cambridge Science Festival, the first celebration of this kind in the United States. Since 2005, its official mission has been, "to engage the wider community with MIT's science, technology and other areas of scholarship in ways that will best serve the nation and the world in the 21st century".

MIT was elected to the Association of American Universities in 1934 and is classified among "R1: Doctoral Universities – Very high research activity"; research expenditures totaled $952 million in 2017. The federal government was the largest source of sponsored research, with the Department of Health and Human Services granting $255.9 million, Department of Defense $97.5 million, Department of Energy $65.8 million, National Science Foundation $61.4 million, and NASA $27.4 million. MIT employs approximately 1300 researchers in addition to faculty. In 2011, MIT faculty and researchers disclosed 632 inventions, were issued 153 patents, earned $85.4 million in cash income, and received $69.6 million in royalties. Through programs like the Deshpande Center, MIT faculty leverage their research and discoveries into multi-million-dollar commercial ventures.

In electronics, magnetic core memory, radar, single electron transistors, and inertial guidance controls were invented or substantially developed by MIT researchers. Harold Eugene Edgerton was a pioneer in high speed photography and sonar. Claude E. Shannon developed much of modern information theory and discovered the application of Boolean logic to digital circuit design theory. In the domain of computer science, MIT faculty and researchers made fundamental contributions to cybernetics, artificial intelligence, computer languages, machine learning, robotics, and cryptography. At least nine Turing Award laureates and seven recipients of the Draper Prize in engineering have been or are currently associated with MIT.

Current and previous physics faculty have won eight Nobel Prizes, four Dirac Medals, and three Wolf Prizes predominantly for their contributions to subatomic and quantum theory. Members of the chemistry department have been awarded three Nobel Prizes and one Wolf Prize for the discovery of novel syntheses and methods. MIT biologists have been awarded six Nobel Prizes for their contributions to genetics, immunology, oncology, and molecular biology. Professor Eric Lander was one of the principal leaders of the Human Genome Project. Positronium atoms, synthetic penicillin, synthetic self-replicating molecules, and the genetic bases for Amyotrophic lateral sclerosis (also known as ALS or Lou Gehrig's disease) and Huntington's disease were first discovered at MIT. Jerome Lettvin transformed the study of cognitive science with his paper "What the frog's eye tells the frog's brain". Researchers developed a system to convert MRI scans into 3D printed physical models.

In the domain of humanities, arts, and social sciences, as of October 2019 MIT economists have been awarded seven Nobel Prizes and nine John Bates Clark Medals. Linguists Noam Chomsky and Morris Halle authored seminal texts on generative grammar and phonology. The MIT Media Lab, founded in 1985 within the School of Architecture and Planning and known for its unconventional research, has been home to influential researchers such as constructivist educator and Logo creator Seymour Papert.

Spanning many of the above fields, MacArthur Fellowships (the so-called "Genius Grants") have been awarded to 50 people associated with MIT. Five Pulitzer Prize–winning writers currently work at or have retired from MIT. Four current or former faculty are members of the American Academy of Arts and Letters.

Allegations of research misconduct or improprieties have received substantial press coverage. Professor David Baltimore, a Nobel Laureate, became embroiled in a misconduct investigation starting in 1986 that led to Congressional hearings in 1991. Professor Ted Postol has accused the MIT administration since 2000 of attempting to whitewash potential research misconduct at the Lincoln Lab facility involving a ballistic missile defense test, though a final investigation into the matter has not been completed. Associate Professor Luk Van Parijs was dismissed in 2005 following allegations of scientific misconduct and found guilty of the same by the United States Office of Research Integrity in 2009.

In 2019, Clarivate Analytics named 54 members of MIT's faculty to its list of "Highly Cited Researchers". That number places MIT 8th among the world's universities.



MIT alumni and faculty have founded numerous companies, some of which are shown below:


The faculty and student body place a high value on meritocracy and on technical proficiency. MIT has never awarded an honorary degree, nor does it award athletic scholarships, ad eundem degrees, or Latin honors upon graduation. However, MIT has twice awarded honorary professorships: to Winston Churchill in 1949 and Salman Rushdie in 1993.

Many upperclass students and alumni wear a large, heavy, distinctive class ring known as the "Brass Rat". Originally created in 1929, the ring's official name is the "Standard Technology Ring". The undergraduate ring design (a separate graduate student version exists as well) varies slightly from year to year to reflect the unique character of the MIT experience for that class, but always features a three-piece design, with the MIT seal and the class year each appearing on a separate face, flanking a large rectangular bezel bearing an image of a beaver. The initialism IHTFP, representing the informal school motto "I Hate This Fucking Place" and jocularly euphemized as "I Have Truly Found Paradise", "Institute Has The Finest Professors", "Institute of Hacks, Tomfoolery and Pranks", "It's Hard to Fondle Penguins", and other variations, has occasionally been featured on the ring given its historical prominence in student culture.

MIT has over 500 recognized student activity groups, including a campus radio station, "The Tech" student newspaper, an annual entrepreneurship competition, and weekly screenings of popular films by the Lecture Series Committee. Less traditional activities include the "world's largest open-shelf collection of science fiction" in English, a model railroad club, and a vibrant folk dance scene. Students, faculty, and staff are involved in over 50 educational outreach and public service programs through the MIT Museum, Edgerton Center, and MIT Public Service Center.

Fraternities and sororities provide a base of activities in addition to housing. Approximately 1,000 undergrads, 48% of men and 30% of women, participate in one of several dozen Greek Life men's, women's and co-ed chapters on the campus. 

The Independent Activities Period is a four-week-long "term" offering hundreds of optional classes, lectures, demonstrations, and other activities throughout the month of January between the Fall and Spring semesters. Some of the most popular recurring IAP activities are Autonomous Robot Design (course 6.270), Robocraft Programming (6.370), and MasLab competitions, the annual "mystery hunt", and Charm School. More than 250 students pursue externships annually at companies in the US and abroad.

Many MIT students also engage in "hacking", which encompasses both the physical exploration of areas that are generally off-limits (such as rooftops and steam tunnels), as well as elaborate practical jokes. Recent high-profile hacks have included the abduction of Caltech's cannon, reconstructing a Wright Flyer atop the Great Dome, and adorning the John Harvard statue with the Master Chief's Mjölnir Helmet.

MIT sponsors 31 varsity sports and has one of the three broadest NCAA Division III athletic programs. MIT participates in the NCAA's Division III, the New England Women's and Men's Athletic Conference, the New England Football Conference, NCAA's Division I Patriot League for women's crew, and the Collegiate Water Polo Association (CWPA) for Men's Water Polo. Men's crew competes outside the NCAA in the Eastern Association of Rowing Colleges (EARC). The intercollegiate sports teams, called the MIT Engineers won 22 Team National Championships, 42 Individual National Championships. MIT is the all-time Division III leader in producing Academic All-Americas (302) and rank second across all NCAA Divisions only behind the University of Nebraska. MIT Athletes won 13 Elite 90 awards and ranks first among NCAA Division III programs, and third among all divisions. In April 2009, budget cuts led to MIT eliminating eight of its 41 sports, including the mixed men's and women's teams in alpine skiing and pistol; separate teams for men and women in ice hockey and gymnastics; and men's programs in golf and wrestling.

MIT enrolled 4,602 undergraduates and 6,972 graduate students in 2018–2019. Women constituted 45 percent of undergraduate students. Undergraduate and graduate students came from all 50 US states as well as from 115 foreign countries.

MIT received 20,075 applications for admission to the undergraduate Class of 2024: it admitted 1,457 (7.2 percent). In 2019, 29,114 applications were received for graduate and advanced degree programs across all departments; 3,670 were admitted (12.6 percent) and 2,312 enrolled (63 percent).

The interquartile range on the SAT was 2090–2340 and 97 percent of students ranked in the top tenth of their high school graduating class. 97 percent of the Class of 2012 returned as sophomores; 82 percent of the Class of 2007 graduated within 4 years, and 93 percent (91 percent of the men and 95 percent of the women) graduated within 6 years.

Undergraduate tuition and fees total $40,732 per student and annual expenses are estimated at $52,507 . 62 percent of students received need-based financial aid in the form of scholarships and grants from federal, state, institutional, and external sources averaging $38,964 per student. Students were awarded a total of $102 million in scholarships and grants, primarily from institutional support ($84 million). The annual increase in expenses has led to a student tradition (dating back to the 1960s) of tongue-in-cheek "tuition riots".

MIT has been nominally co-educational since admitting Ellen Swallow Richards in 1870. Richards also became the first female member of MIT's faculty, specializing in sanitary chemistry. Female students remained a small minority prior to the completion of the first wing of a women's dormitory, McCormick Hall, in 1963. Between 1993 and 2009 the proportion of women rose from 34 percent to 45 percent of undergraduates and from 20 percent to 31 percent of graduate students. , women outnumbered men in Biology, Brain & Cognitive Sciences, Architecture, Urban Planning, and Biological Engineering.

A number of student deaths in the late 1990s and early 2000s resulted in considerable media attention focussing on MIT's culture and student life. After the alcohol-related death of Scott Krueger in September 1997 as a new member at the Phi Gamma Delta fraternity, MIT began requiring all freshmen to live in the dormitory system. The 2000 suicide of MIT undergraduate Elizabeth Shin drew attention to suicides at MIT and created a controversy over whether MIT had an unusually high suicide rate. In late 2001 a task force's recommended improvements in student mental health services were implemented, including expanding staff and operating hours at the mental health center. These and later cases were significant as well because they sought to prove the negligence and liability of university administrators "in loco parentis".

, MIT had 1,030 faculty members. Faculty are responsible for lecturing classes, for advising both graduate and undergraduate students, and for sitting on academic committees, as well as for conducting original research. Between 1964 and 2009 a total of seventeen faculty and staff members affiliated with MIT won Nobel Prizes (thirteen of them in the latter 25 years). As of October 2019, 37 MIT faculty members, past or present, have won Nobel Prizes, the majority in Economics or Physics.

, current faculty and teaching staff included 67 Guggenheim Fellows, 6 Fulbright Scholars, and 22 MacArthur Fellows. Faculty members who have made extraordinary contributions to their research field as well as the MIT community are granted appointments as Institute Professors for the remainder of their tenures.

A 1998 MIT study concluded that a systemic bias against female faculty existed in its School of Science, although the study's methods were controversial. Since the study, though, women have headed departments within the Schools of Science and of Engineering, and MIT has appointed several female vice-presidents, although allegations of sexism continue. Susan Hockfield, a molecular neurobiologist, served as MIT's president from 2004 to 2012 – the first woman to hold the post.

Tenure issues have vaulted MIT into the national spotlight on several occasions. The 1984 dismissal of David F. Noble (a historian of technology) became a "cause célèbre" about the extent to which academics are granted freedom of speech after he published several books and papers critical of MIT's and other research universities' reliance upon financial support from corporations and the military. Former materials-science professor Gretchen Kalonji sued MIT in 1994, alleging that she was denied tenure because of sexual discrimination. Several years later, the lawsuit was settled with undisclosed payments and the establishment of a project to encourage women and minorities to seek faculty positions. In 1997 the Massachusetts Commission Against Discrimination issued a probable-cause finding supporting UMass Boston Professor James Jennings' allegations of racial discrimination after a senior faculty search committee in the Department of Urban Studies and Planning did not offer him reciprocal tenure.

In 2006–2007, MIT's denial of tenure to African-American stem-cell scientist professor James Sherley reignited accusations of racism in the tenure process, eventually leading to a protracted public dispute with the administration, a brief hunger-strike, and the resignation of Professor Frank L. Douglas in protest. "The Boston Globe" reported on February 6, 2007: "Less than half of MIT's junior faculty members are granted tenure. After Sherley was initially denied tenure, his case was examined three times before the university established that neither racial discrimination nor conflict of interest affected the decision. Twenty-one of Sherley's colleagues later issued a statement saying that the professor was treated fairly in tenure review."

MIT faculty members have often been recruited to lead other colleges and universities. Founding faculty-member Charles W. Eliot became president of Harvard University in 1869, a post he would hold for 40 years, during which he wielded considerable influence both on American higher education and on secondary education. MIT alumnus and faculty member George Ellery Hale played a central role in the development of the California Institute of Technology (Caltech), and other faculty members have been key founders of Franklin W. Olin College of Engineering in nearby Needham, Massachusetts.

In addition, faculty members have been recruited to lead governmental agencies; for example, former professor Marcia McNutt is president of the National Academy of Sciences, urban studies professor Xavier de Souza Briggs served as the associate director of the White House Office of Management and Budget, and biology professor Eric Lander was a co-chair of the President's Council of Advisors on Science and Technology. In 2013, faculty member Ernest Moniz was nominated by President Obama and later confirmed as United States Secretary of Energy. Former professor Hans Mark served as Secretary of the Air Force from 1979 to 1981. Alumna and Institute Professor Sheila Widnall served as Secretary of the Air Force between 1993 and 1997, making her the first female Secretary of the Air Force and first woman to lead an entire branch of the US military in the Department of Defense.

, MIT was the second-largest employer in the city of Cambridge. Based on feedback from employees, MIT was ranked #7 as a place to work, among US colleges and universities . Surveys cited a "smart", "creative", "friendly" environment, noting that the work-life balance tilts towards a "strong work ethic" but complaining about "low pay" compared to an industry position.

Many of MIT's over 120,000 alumni have had considerable success in scientific research, public service, education, and business. , 39 MIT alumni have won the Nobel Prize, 47 have been selected as Rhodes Scholars, and 61 have been selected as Marshall Scholars.

Alumni in American politics and public service include former Chairman of the Federal Reserve Ben Bernanke, former MA-1 Representative John Olver, former CA-13 Representative Pete Stark, Representative Thomas Massie, former National Economic Council chairman Lawrence H. Summers, and former Council of Economic Advisors chairman Christina Romer. MIT alumni in international politics include Foreign Affairs Minister of Iran Ali Akbar Salehi, Israeli Prime Minister Benjamin Netanyahu, President of Colombia Virgilio Barco Vargas, President of the European Central Bank Mario Draghi, former Governor of the Reserve Bank of India Raghuram Rajan, former British Foreign Minister David Miliband, former Greek Prime Minister Lucas Papademos, former UN Secretary General Kofi Annan, former Iraqi Deputy Prime Minister Ahmed Chalabi, former Minister of Education and Culture of The Republic of Indonesia Yahya Muhaimin, former Jordanian Minister of Education, Higher Education and Scientific Research & former Jordanian Minister of Energy and Mineral Resources Khaled Toukan. Alumni in sports have included Olympic fencing champion Johan Harmenberg.

MIT alumni founded or co-founded many notable companies, such as Intel, McDonnell Douglas, Texas Instruments, 3Com, Qualcomm, Bose, Raytheon, Apotex, Koch Industries, Rockwell International, Genentech, Dropbox, and Campbell Soup. According to the British newspaper, "The Guardian", "a survey of living MIT alumni found that they have formed 25,800 companies, employing more than three million people including about a quarter of the workforce of Silicon Valley. Those firms collectively generate global revenues of about $1.9 trillion (£1.2 trillion) a year". If the companies founded by MIT alumni were a country, they would have the 11th highest GDP of any nation in the world.

Prominent institutions of higher education have been led by MIT alumni, including the University of California system, Harvard University, New York Institute of Technology, Johns Hopkins University, Carnegie Mellon University, Tufts University, Rochester Institute of Technology, Rhode Island School of Design (RISD), New Jersey Institute of Technology, Northeastern University, Tel Aviv University, Lahore University of Management Sciences, Rensselaer Polytechnic Institute, Tecnológico de Monterrey, Purdue University, Virginia Polytechnic Institute, KAIST, and Quaid-e-Azam University. Berklee College of Music, the largest independent college of contemporary music in the world, was founded and led by MIT alumnus Lawrence Berk for more than three decades.

More than one third of the United States' manned spaceflights have included MIT-educated astronauts, more than any university excluding the United States service academies. Of the 12 people who have been on the Moon as of 2019, four graduated from MIT (among them Apollo 11 Lunar Module Pilot Buzz Aldrin). Alumnus and former faculty member Qian Xuesen led the Chinese nuclear weapons program and was instrumental in the PRC rocket program.

Noted alumni in non-scientific fields include author Hugh Lofting, sculptor Daniel Chester French, guitarist Tom Scholz of the band Boston, the British "BBC" and "ITN" correspondent and political advisor David Walter, "The New York Times" columnist and Nobel Prize Winning economist Paul Krugman, "The Bell Curve" author Charles Murray, United States Supreme Court building architect Cass Gilbert, Pritzker Prize-winning architects I.M. Pei and Gordon Bunshaft.




</doc>
<doc id="18880" url="https://en.wikipedia.org/wiki?curid=18880" title="Monopolistic competition">
Monopolistic competition

Monopolistic competition is a type of imperfect competition such that many producers sell products that are differentiated from one another (e.g. by branding or quality) and hence are not perfect substitutes. In monopolistic competition, a firm takes the prices charged by its rivals as given and ignores the impact of its own prices on the prices of other firms. In the presence of coercive government, monopolistic competition will fall into government-granted monopoly. Unlike perfect competition, the firm maintains spare capacity. Models of monopolistic competition are often used to model industries. Textbook examples of industries with market structures similar to monopolistic competition include restaurants, cereal, clothing, shoes, and service industries in large cities. The "founding father" of the theory of monopolistic competition is Edward Hastings Chamberlin, who wrote a pioneering book on the subject, "Theory of Monopolistic Competition" (1933). Joan Robinson published a book "The Economics of Imperfect Competition" with a comparable theme of distinguishing perfect from imperfect competition.

Monopolistically competitive markets have the following characteristics:


The long-run characteristics of a monopolistically competitive market are almost the same as a perfectly competitive market. Two differences between the two are that monopolistic competition produces heterogeneous products and that monopolistic competition involves a great deal of non-price competition, which is based on subtle product differentiation. A firm making profits in the short run will nonetheless only break even in the long run because demand will decrease and average total cost will increase. This means in the long run, a monopolistically competitive firm will make zero economic profit. This illustrates the amount of influence the firm has over the market; because of brand loyalty, it can raise its prices without losing all of its customers. This means that an individual firm's demand curve is downward sloping, in contrast to perfect competition, which has a perfectly elastic demand schedule.

There are six characteristics of monopolistic competition (MC):


MC firms sell products that have real or perceived non-price differences. However, the differences are not so great as to eliminate other goods as substitutes. Technically, the cross price elasticity of demand between goods in such a market is positive. In fact, the XED would be high. MC goods are best described as close but imperfect substitutes. The goods perform the same basic functions but have differences in qualities such as type, style, quality, reputation, appearance, and location that tend to distinguish them from each other. For example, the basic function of motor vehicles is the same—to move people and objects from point to point in reasonable comfort and safety. Yet there are many different types of motor vehicles such as motor scooters, motor cycles, trucks and cars, and many variations even within these categories.

There are many firms in each MC product group and many firms on the side lines prepared to enter the market. A product group is a "collection of similar products". The fact that there are "many firms" gives each MC firm the freedom to set prices without engaging in strategic decision making regarding the prices of other firms and each firm's actions have a negligible impact on the market. For example, a firm could cut prices and increase sales without fear that its actions will prompt retaliatory responses from competitors.

How many firms will an MC market structure support at market equilibrium? The answer depends on factors such as fixed costs, economies of scale and the degree of product differentiation. For example, the higher the fixed costs, the fewer firms the market will support.

Like perfect competition, under monopolistic competition also, the firms can enter or exit freely. The firms will enter when the existing firms are making super-normal profits. With the entry of new firms, the supply would increase which would reduce the price and hence the existing firms will be left only with normal profits. Similarly, if the existing firms are sustaining losses, some of the marginal firms will exit. It will reduce the supply due to which price would rise and the existing firms will be left only with normal profit.

Each MC firm independently sets the terms of exchange for its product. The firm gives no consideration to what effect its decision may have on competitors. The theory is that any action will have such a negligible effect on the overall market demand that an MC firm can act without fear of prompting heightened competition. In other words, each firm feels free to set prices as if it were a monopoly rather than an oligopoly.

MC firms have some degree of market power. Market power means that the firm has control over the terms and conditions of exchange. An MC firm can raise its prices without losing all its customers. The firm can also lower prices without triggering a potentially ruinous price war with competitors. The source of an MC firm's market power is not barriers to entry since they are low. Rather, an MC firm has market power because it has relatively few competitors, those competitors do not engage in strategic decision making and the firms sells differentiated product. Market power also means that an MC firm faces a downward sloping demand curve. The demand curve is highly elastic although not "flat".

No sellers or buyers have complete market information, like market demand or market supply.

There are two sources of inefficiency in the MC market structure. First, at its optimum output the firm charges a price that exceeds marginal costs, The MC firm maximizes profits where marginal revenue = marginal cost. Since the MC firm's demand curve is downward sloping this means that the firm will be charging a price that exceeds marginal costs. The monopoly power possessed by a MC firm means that at its profit maximizing level of production there will be a net loss of consumer (and producer) surplus. The second source of inefficiency is the fact that MC firms operate with excess capacity. That is, the MC firm's profit maximizing output is less than the output associated with minimum average cost. Both a PC and MC firm will operate at a point where demand or price equals average cost. For a PC firm this equilibrium condition occurs where the perfectly elastic demand curve equals minimum average cost. A MC firm's demand curve is not flat but is downward sloping. Thus in the long run the demand curve will be tangential to the long run average cost curve at a point to the left of its minimum. The result is excess capacity.


Monopolistically competitive firms are inefficient, it is usually the case that the costs of regulating prices for products sold in monopolistic competition exceed the benefits of such regulation. A monopolistically competitive firm might be said to be marginally inefficient because the firm produces at an output where average total cost is not a minimum. A monopolistically competitive market is productively inefficient market structure because marginal cost is less than price in the long run. Monopolistically competitive markets are also allocatively inefficient, as the price given is higher than Marginal cost. Product differentiation increases total utility by better meeting people's wants than homogenous products in a perfectly competitive market.

Another concern is that monopolistic competition fosters advertising and the creation of brand names. Advertising induces customers into spending more on products because of the name associated with them rather than because of rational factors. Defenders of advertising dispute this, arguing that brand names can represent a guarantee of quality and that advertising helps reduce the cost to consumers of weighing the tradeoffs of numerous competing brands. There are unique information and information processing costs associated with selecting a brand in a monopolistically competitive environment. In a monopoly market, the consumer is faced with a single brand, making information gathering relatively inexpensive. In a perfectly competitive industry, the consumer is faced with many brands, but because the brands are virtually identical information gathering is also relatively inexpensive. In a monopolistically competitive market, the consumer must collect and process information on a large number of different brands to be able to select the best of them. In many cases, the cost of gathering information necessary to selecting the best brand can exceed the benefit of consuming the best brand instead of a randomly selected brand. The result is that the consumer is confused. Some brands gain prestige value and can extract an additional price for that.

Evidence suggests that consumers use information obtained from advertising not only to assess the single brand advertised, but also to infer the possible existence of brands that the consumer has, heretofore, not observed, as well as to infer consumer satisfaction with brands similar to the advertised brand.

In many markets, such as toothpaste, soap, air conditioning, smartphones and toilet paper, producers practice product differentiation by altering the physical composition of products, using special packaging, or simply claiming to have superior products based on brand images or advertising.



</doc>
<doc id="18881" url="https://en.wikipedia.org/wiki?curid=18881" title="Mathematical induction">
Mathematical induction

Mathematical induction is a mathematical proof technique. It is essentially used to prove that a statement "P"("n") holds for every natural number "n" = 0, 1, 2, 3, . . . ; that is, the overall statement is a sequence of infinitely many cases "P"(0), "P"(1), "P"(2), "P"(3), . . . . Informal metaphors help to explain this technique, such as falling dominoes or climbing a ladder:
A proof by induction consists of two cases. The first, the base case (or basis), proves the statement for "n =" 0 without assuming any knowledge of other cases. The second case, the induction step, proves that "if" the statement holds for any given case "n = k", "then" it must also hold for the next case "n" = "k" + 1. These two steps establish that the statement holds for every natural number "n". The base case does not necessarily begin with "n =" 0, but often with "n =" 1, and possibly with any fixed natural number "n = N", establishing the truth of the statement for all natural numbers "n ≥ N".

The method can be extended to prove statements about more general well-founded structures, such as trees; this generalization, known as structural induction, is used in mathematical logic and computer science. Mathematical induction in this extended sense is closely related to recursion. Mathematical induction is an inference rule used in formal proofs, and in some form is the foundation of all correctness proofs for computer programs. 

Although its name may suggest otherwise, mathematical induction should not be confused with inductive reasoning as used in philosophy (see Problem of induction). The mathematical method examines infinitely many cases to prove a general statement, but does so by a finite chain of deductive reasoning involving the variable "n", which can take infinitely many values.

In 370 BC, Plato's Parmenides may have contained an early example of an implicit inductive proof. The earliest clear use of mathematical induction (though not by that name) may be found in Euclid's proof that the number of primes is infinite. An opposite iterated technique, counting "down" rather than up, is found in the sorites paradox, where it was argued that if 1,000,000 grains of sand formed a heap, and removing one grain from a heap left it a heap, then a single grain of sand (or even no grains) forms a heap.

In India, early implicit proofs by mathematical induction appear in Bhaskara's "cyclic method", and in the "al-Fakhri" written by al-Karaji around 1000 AD, who applied it to arithmetic sequences to prove the binomial theorem and properties of Pascal's triangle.

None of these ancient mathematicians, however, explicitly stated the induction hypothesis. Another similar case (contrary to what Vacca has written, as Freudenthal carefully showed) was that of Francesco Maurolico in his "Arithmeticorum libri duo" (1575), who used the technique to prove that the sum of the first "n" odd integers is "n".

The earliest rigorous use of induction was by Gersonides (1288–1344). The first explicit formulation of the principle of induction was given by Pascal in his "Traité du triangle arithmétique" (1665). Another Frenchman, Fermat, made ample use of a related principle: indirect proof by infinite descent. 

The induction hypothesis was also employed by the Swiss Jakob Bernoulli, and from then on it became well known. The modern formal treatment of the principle came only in the 19th century, with George Boole, Augustus de Morgan, Charles Sanders Peirce, 
Giuseppe Peano, and Richard Dedekind.

The simplest and most common form of mathematical induction infers that a statement involving a natural number formula_1 (that is, an integer formula_2 or 1) holds for all values of formula_1. The proof consists of two steps:

The hypothesis in the inductive step, that the statement holds for a particular "formula_1", is called the induction hypothesis or inductive hypothesis. To prove the inductive step, one assumes the induction hypothesis for formula_1 and then uses this assumption to prove that the statement holds for formula_6.

Authors who prefer to define natural numbers to begin at 0 use that value in the base case; those who define natural numbers to begin at 1 use that value.

Mathematical induction can be used to prove the following statement "P"("n") for all natural numbers "n".

This states a general formula for the sum of the natural numbers less than or equal to a given number; in fact an infinite sequence of statements: formula_13, formula_14, formula_15, etc. 

Proposition. For any formula_16, formula_17

Proof. Let "P"("n") be the statement formula_17 We give a proof by induction on "n".

"Base case:" Show that the statement holds for the smallest natural number "n" = 0.

"P"(0) is clearly true: formula_19

"Inductive step:" Show that for any "k ≥" 0, if "P"("k") holds, then "P"("k"+1) also holds. 

Assume the induction hypothesis that for a particular "k", the single case "n = k" holds, meaning "P"("k") is true:formula_20It follows that:

Algebraically, the right hand side simplifies as:

Equating the extreme left hand and right hand sides, we deduce that:formula_23That is, the statement "P"("k+"1) also holds true, establishing the inductive step.

"Conclusion": Since both the base case and the inductive step have been proved as true, by mathematical induction the statement "P"("n") holds for every natural number "n". ∎

Induction is often used to prove inequalities. As an example, we prove that formula_24 for any real number formula_25 and natural number formula_1. 

At first glance, it may appear that a more general version, formula_27 for any "real" numbers formula_28, could be proven without induction; but the case formula_29 shows it may be false for non-integral values of formula_1. This suggests we examine the statement specifically for "natural" values of formula_1, and induction is the readiest tool.

Proposition. For any formula_32, formula_27. 

Proof. Fix an arbitrary real number formula_25, and let formula_35 be the statement formula_27. We induct on formula_1. 

"Base case:" The calculation formula_38 verifies formula_39. 

"Inductive step:" We show the implication formula_40 for any natural number formula_41. Assume the induction hypothesis: for a given value formula_42, the single case formula_43 is true. Using the angle addition formula and the triangle inequality, we deduce:

The inequality between the extreme left hand and right-hand quantities shows that formula_45 is true, which completes the inductive step. 

"Conclusion": The proposition formula_35 holds for all natural numbers formula_1. ∎

In practice, proofs by induction are often structured differently, depending on the exact nature of the property to be proven.
All variants of induction are special cases of transfinite induction; see below.

If one wishes to prove a statement, not for all natural numbers, but only for all numbers "n" greater than or equal to a certain number "b", then the proof by induction consists of:
This can be used, for example, to show that formula_51 for formula_52.

In this way, one can prove that some statement formula_35 holds for all formula_54, or even for all formula_55. This form of mathematical induction is actually a special case of the previous form, because if the statement to be proved is formula_35 then proving it with these two rules is equivalent with proving formula_57 for all natural numbers formula_1 with an induction base case formula_59.

Assume an infinite supply of 4- and 5-dollar coins. Induction can be used to prove that any whole amount of dollars greater than or equal to formula_60 can be formed by a combination of such coins. Let formula_61 denote the statement ""the amount of formula_41 dollars can be formed by a combination of 4- and 5-dollar coins"". The proof that formula_61 is true for all formula_64 can then be achieved by induction on formula_41 as follows:

"Base case": Showing that formula_61 holds for formula_67 is easy: take three 4-dollar coins.

"Induction step": Given that formula_61 holds for some value of formula_64 ("induction hypothesis"), prove that formula_70 holds, too: 

Therefore, by the principle of induction, formula_61 holds for all formula_72, and the proof is complete.

In this example, although formula_61 also holds for formula_81,
the above proof cannot be modified to replace the minimum amount of formula_60 dollar to any lower value formula_83.
For formula_84, the base case is actually false;
for formula_85, the second case in the induction step (replacing three 5- by four 4-dollar coins) will not work;
let alone for even lower formula_83.

It is sometimes desirable to prove a statement involving two natural numbers, "n" and "m", by iterating the induction process. That is, one proves a base case and an inductive step for "n", and in each of those proves a base case and an inductive step for "m". See, for example, the proof of commutativity accompanying "addition of natural numbers". More complicated arguments involving three or more counters are also possible.

The method of infinite descent is a variation of mathematical induction which was used by Pierre de Fermat. It is used to show that some statement "Q"("n") is false for all natural numbers "n". Its traditional form consists of showing that if "Q"("n") is true for some natural number "n", it also holds for some strictly smaller natural number "m". Because there are no infinite decreasing sequences of natural numbers, this situation would be impossible, thereby showing (by contradiction) that "Q"("n") cannot be true for any "n". 

The validity of this method can be verified from the usual principle of mathematical induction. Using mathematical induction on the statement "P"("n") defined as ""Q"("m") is false for all natural numbers "m" less than or equal to "n"", it follows that "P"("n") holds for all "n", which means that "Q"("n") is false for every natural number "n".

The most common form of proof by mathematical induction requires proving in the inductive step that

whereupon the induction principle "automates" "n" applications of this step in getting from "P"(0) to "P"("n"). This could be called "predecessor induction" because each step proves something about a number from something about that number's predecessor.

A variant of interest in computational complexity is "prefix induction", in which one proves the following statement in the inductive step:

or equivalently

The induction principle then "automates" log "n" applications of this inference in getting from "P"(0) to "P"("n"). In fact, it is called "prefix induction" because each step proves something about a number from something about the "prefix" of that number — as formed by truncating the low bit of its binary representation. It can also be viewed as an application of traditional induction on the length of that binary representation.

If traditional predecessor induction is interpreted computationally as an "n"-step loop, then prefix induction would correspond to a log-"n"-step loop. Because of that, proofs using prefix induction are "more feasibly constructive" than proofs using predecessor induction.

Predecessor induction can trivially simulate prefix induction on the same statement. Prefix induction can simulate predecessor induction, but only at the cost of making the statement more syntactically complex (adding a bounded universal quantifier), so the interesting results relating prefix induction to polynomial-time computation depend on excluding unbounded quantifiers entirely, and limiting the alternation of bounded universal and existential quantifiers allowed in the statement.

One can take the idea a step further: one must prove

whereupon the induction principle "automates" log log "n" applications of this inference in getting from "P"(0) to "P"("n"). This form of induction has been used, analogously, to study log-time parallel computation.

Another variant, called complete induction, course of values induction or strong induction (in contrast to which the basic form of induction is sometimes known as weak induction), makes the inductive step easier to prove by using a stronger hypothesis: one proves the statement under the assumption that "P"("n") holds for "all" natural "n" less than ; by contrast, the basic form only assumes "P"("m"). The name "strong induction" does not mean that this method can prove more than "weak induction", but merely refers to the stronger hypothesis used in the inductive step.

In fact, it can be shown that the two methods are actually equivalent, as explained below. In this form of complete induction, one still has to prove the base case, "P"(0), and it may even be necessary to prove extra-base cases such as "P"(1) before the general argument applies, as in the example below of the Fibonacci number "F".

Although the form just described requires one to prove the base case, this is unnecessary if one can prove "P"("m") (assuming "P"("n") for all lower "n") for all . This is a special case of transfinite induction as described below. In this form the base case is subsumed by the case , where "P"(0) is proved with no other "P"("n") assumed;
this case may need to be handled separately, but sometimes the same argument applies for "m" = 0 and , making the proof simpler and more elegant.
In this method, however, it is vital to ensure that the proof of "P"("m") does not implicitly assume that , e.g. by saying "choose an arbitrary ", or by assuming that a set of "m" elements has an element.

Complete induction is equivalent to ordinary mathematical induction as described above, in the sense that a proof by one method can be transformed into a proof by the other. Suppose there is a proof of "P"("n") by complete induction. Let Q("n") mean ""P"("m") holds for all "m" such that ". Then Q("n") holds for all "n" if and only if P("n") holds for all "n", and our proof of "P"("n") is easily transformed into a proof of Q("n") by (ordinary) induction. If, on the other hand, "P"("n") had been proven by ordinary induction, the proof would already effectively be one by complete induction: "P"(0) is proved in the base case, using no assumptions, and is proved in the inductive step, in which one may assume all earlier cases but need only use the case "P"("n").

Complete induction is most useful when several instances of the inductive hypothesis are required for each inductive step. For example, complete induction can be used to show that

where formula_92 is the "n"th Fibonacci number, <math display="inline">\varphi = 


</doc>
<doc id="18884" url="https://en.wikipedia.org/wiki?curid=18884" title="Matrix">
Matrix

Matrix or MATRIX may refer to:














</doc>
<doc id="18885" url="https://en.wikipedia.org/wiki?curid=18885" title="Morton Downey Jr.">
Morton Downey Jr.

Sean Morton Downey (December 9, 1932 – March 12, 2001), better known as Morton Downey Jr., was an American television talk show host of the late-1980s who pioneered the "trash TV" format on his program "The Morton Downey Jr. Show".

Downey's parents were in show business; his father, Morton Downey, was a popular singer, and his mother, Barbara Bennett, was a stage and film actress and singer and dancer. Downey did not use his legal first name (Sean) in his stage name. His aunts included Hollywood film stars Constance and Joan Bennett, from whom he was estranged, and his maternal grandfather was the celebrated matinée idol Richard Bennett. Born into a wealthy family, he was raised during the summers next door to the Kennedy compound in Hyannis Port, Massachusetts. Downey attended New York University.

He was a program director and announcer at radio station WPOP in Hartford, Connecticut in the 1950s. He went on to work as a disc jockey, sometimes using the moniker "Doc" Downey, in various markets around the U.S., including Phoenix (KRIZ), Miami (WFUN), Kansas City (KUDL), San Diego (KDEO) and Seattle (KJR). He had to resign from WFUN after drawing ire from the FCC for announcing a competing disc jockey's home phone number on the air and insulting his wife. Like his father, Downey pursued a career in music, recording in both pop and country styles. He sang on a few records and then began to write songs, several of which were popular in the 1950s and 1960s. He joined ASCAP as a result. In 1958, he recorded "Boulevard of Broken Dreams", which he sang on national television on a set that resembled a dark street with one street light. In 1981, "Green Eyed Girl" charted on the "Billboard Magazine" country chart, peaking at #95.

In the 1980s, Downey was a talk show host at KFBK-AM in Sacramento, California, where he employed his abrasive style. He was fired in 1984, and was subsequently replaced by Rush Limbaugh. Downey also had a stint on WMAQ-AM in Chicago where he unsuccessfully tried to get other on air radio personalities to submit to drug testing. Downey's largest effect on American culture came from his popular, yet short-lived, syndicated late 1980s television talk show, "The Morton Downey Jr. Show".

On January 22, 1980, Downey, a devoted pro-life activist, hosted the California State Rally for Life at the invitation of the California ProLife Council and United Students for Life. At that time, he was also running for President of the United States, as a Democrat. The United Students for Life, at California State University, Sacramento helped organize his California presidential rallies. Downey worked to help promote anti-abortion candidates in California and around the country.

Downey headed to Secaucus, New Jersey, where his highly controversial television program "The Morton Downey Jr. Show" was taped. Starting as a local program on New York-New Jersey superstation WWOR-TV in October 1987, it expanded into national syndication in early 1988. The program featured screaming matches among Downey, his guests, and audience members. Using a large silver bowl for an ashtray, he would chainsmoke during the show and blow smoke in his guests' faces. Downey's fans became known as "Loudmouths", patterned after the studio lecterns decorated with gaping cartoon mouths, from which Downey's guests would go head-to-head against each other on their respective issues.

Downey's signature phrases "pablum puking liberal" (in reference to left-liberals) and "zip it!" briefly enjoyed some popularity in the contemporary vernacular. He particularly enjoyed making his guests angry with each other, which on a few occasions resulted in physical confrontations. One such incident occurred on a 1988 show taped at the Apollo Theater, involving Al Sharpton and CORE National Chairman Roy Innis. The exchange between the two men culminated in Innis shoving Sharpton into his chair, knocking him to the floor and Downey intervening to separate the pair.

Downey briefly took his show on the road in 1989 holding concert like events across the country 

Because of the controversial format and content of the show, distributor MCA Television had problems selling the show to a number of stations and advertisers. Even Downey's affiliates, many of which were low-rated independent television stations in small to medium markets, were so fearful of advertiser and viewer backlash that they would air one or even two local disclaimers during the broadcast.

During one controversial episode Downey introduced his gay brother, Tony Downey, to his studio audience and informed them Tony was HIV positive. During the episode Downey stated he was afraid his audience would abandon him if they knew he had a gay brother, but then said he did not care.

"The Washington Post" wrote about him, "Suppose a maniac got hold of a talk show. Or need we suppose?" David Letterman said, "I'm always amazed at what people will fall for. We see this every ten or twelve years, an attempt at this, and I guess from that standpoint I don't quite understand why everybody's falling over backwards over the guy."

The success of the show made Downey a pop culture celebrity, leading to appearances on "Saturday Night Live" in 1988, WrestleMania V in 1989 in which he traded insults with Roddy Piper and Brother Love on "Piper's Pit", and later roles in movies such as "Predator 2" and "". He was also cast in several television roles, often playing tabloid TV hosts or other obnoxious media types. Downey notably starred in the "Tales from the Crypt" episode "Television Terror" which utilized several scenes shot by characters within the story, a format which became popular in horror films a decade later with the found footage genre.

In 1989, Downey released an album of songs based on his show entitled "Morton Downey Jr. Sings". The album's single, "Zip It!" (a catch-phrase from the TV show, used to quiet an irate guest), became a surprise hit on some college radio stations. Over the course of the 1988–89 television season, his TV show suffered a decline in viewership, resulting from many markets downgrading its time slot; even flagship station WWOR moved Downey's program from its original 9:00 PM slot to 11:30 PM in the fall of 1988. Beginning in January 1989, the time slot immediately following Downey's program was given to the then-new "Arsenio Hall Show". Following Hall's strong early ratings, however, the two series swapped time slots several weeks later, thus relegating Downey to 12:30 AM in the number-one television market. 

In late April 1989, he was involved in an incident in a San Francisco International Airport restroom in which he claimed to have been attacked by neo-Nazis who painted a swastika on his face and attempted to shave his head. Some inconsistencies in Downey's account (e.g., the swastika was painted in reverse, suggesting that Downey had drawn it himself in a mirror), and the failure of the police to find supportive evidence, led many to suspect the incident was a hoax and a ploy for attention. In July 1989, his show was canceled, with the owners of the show announcing that the last episode had been taped on June 30, and that no new shows would air after September 15, 1989.

At the time of its cancellation, the show was airing on a total of 70 stations across the country, and its advertisers had been reduced primarily to "direct-response" ads (such as 900 chat line and phone sex numbers). In February 1990, Downey filed for bankruptcy in the US Bankruptcy Court for the District of New Jersey.

In 1990, Downey resurfaced on CNBC with an interview program called "Showdown", which was followed by three attempted talk radio comebacks: first in 1992 on Washington, D.C. radio station WWRC; then in 1993 on Dallas radio station KGBS, where he would scream insults at his callers. He was also hired as the station's VP of Operations. The following year, he returned to CNBC with a short-lived television show, "Downey"; in one episode, Downey claimed to have had a psychic communication with O.J. Simpson's murdered ex-wife, Nicole Brown Simpson.

His third – and final – attempt at a talk radio comeback occurred in 1997 on Cleveland radio station WTAM in a late evening time slot. It marked his return to the Cleveland market, where Downey had been a host for crosstown radio station WERE in the early 1980s prior to joining KFBK. This stint came shortly after the surgery for lung cancer that removed one of his lungs. At WTAM, Downey abandoned the confrontational schtick of his TV and previous radio shows, and conducted this program in a much more conversational and jovial manner.

On August 30, 1997, Downey quit his WTAM show to focus on pursuing legal action against Howard Stern. Downey had accused Stern of spreading rumors that he had resumed his smoking habit, to which publicist Les Schecter retorted, "He hasn't picked up a cigarette." His replacement was former WERE host Rick Gilmour.

Following his death, news reports and obituaries incorrectly (according to the "Orange County Register") credited him as the composer of "Wipe Out." As of 2008, Downey's official website (and others) continue to make this claim. Prior to Downey's death, "Spin" in April 1989 had identified the "Wipe Out" authorship as a myth.

In 1984, at KFBK radio, Downey used the word "Chinaman" while telling a joke. His use of the word upset portions of the sizable Asian community in Sacramento. One Asian-American city councilman called for an apology and pressured the station for Downey's resignation. Downey refused to apologize and was forced to resign.

Downey was sued for allegedly appropriating the words and music to his theme song from two songwriters. He was sued for $40 million after bringing then-stripper Kellie Everts onto the show and calling her a "slut", a "pig", a "hooker", and a "tramp", claiming that she had venereal diseases, and banging his pelvis against hers.

In April 1988, he was arraigned on criminal charges for allegedly attacking a gay guest on his show, in a never-aired segment. In another lawsuit, he was accused of slandering a newscaster (a former colleague), and of indecently exposing himself to her and slapping her. Downey punched Stuttering John during an interview done for "The Howard Stern Show", while also shouting verbal insults at John, referring to him as an "uneducated slob". The situation then began to evolve into a brawl between the two until Downey had to be pulled off of John by security; the entire incident was caught on camera. When an "Inside Edition" camera crew approached Downey in 1989 to question him about his involvement in an alleged business scam, Downey grabbed the boom mike and struck the soundman's head with it.

In his later years, Downey expressed remorse for some of the extreme theatrics of his TV show, as well as various incidents outside the studio, including the "Inside Edition" confrontation. However, he also claimed that his show was of a higher quality and not as "sleazy" as Jerry Springer's show.

Downey was married four times and had four children from three of those marriages. With wife Helen, he had daughter Melissa; with Joan, he had daughters Tracey and Kelli; and, with fourth wife Lori, he had daughter Seanna Micaela. He and Lori met when she appeared as a dancer in a show he attended in Atlantic City. According to Terry Pluto's book, "Loose Balls", Downey was one of the owners of the New Orleans Buccaneers basketball team in the American Basketball Association in the late 1960s. Downey was also president and co-founder of the proposed World Baseball Association in 1974.

In 1998, a Golden Palm Star on the Palm Springs, California, Walk of Stars was dedicated to him.

Morton Koopa Jr., a character from the "Super Mario" series and one of the Koopalings, was named after him.

In June 1996, Downey was diagnosed with lung cancer while being treated for pneumonia, and had one of his lungs removed. His views on tobacco use changed substantially, going from a one-time member of the National Smokers Alliance to a staunch anti-smoking activist. He continued to speak against smoking until his death from lung cancer and pneumonia on March 12, 2001.

After being diagnosed with lung cancer, he commented, "I had spawned a generation of kids to think it was cool to smoke a cigarette. Kids walked up to me until a matter of weeks ago, they'd have a cigarette in their hand and they'd say, 'Hey, Mort,' or, 'Hey, Mouth, autograph my cigarette.' And I'd do it." He also blamed tobacco companies for lying to consumers about cigarettes.

Released in 2012, the documentary film "" touches upon Downey's upbringing and formative years in radio and politics before launching into the history of "The Morton Downey Jr. Show" and Downey's influence on trash TV. The film also looks at Downey's relationship with Al Sharpton and other important 80s figures, as well as Downey's role as a predecessor for commentators like Glenn Beck and Rush Limbaugh.




</doc>
<doc id="18886" url="https://en.wikipedia.org/wiki?curid=18886" title="List of male singles tennis players">
List of male singles tennis players

This is a list of top international male singles tennis players, both past and present.

It includes players who have been officially ranked among the top 25 singles players in the world during the "Open Era"; been ranked in the top 10 prior to the Open Era; have been a singles quarterfinalist or better at a Grand Slam tournament; have reached the finals of or won the season-ending event; or have been singles medalists at the Olympics.

Players who have won more than one Grand Slam singles title or have been ranked world no. 1 in singles have been put in bold font. Players who are still active on the tour have been put in "italics".



</doc>
<doc id="18887" url="https://en.wikipedia.org/wiki?curid=18887" title="Metaphilosophy">
Metaphilosophy

Metaphilosophy (sometimes called philosophy of philosophy) is "the investigation of the nature of philosophy". Its subject matter includes the aims of philosophy, the boundaries of philosophy, and its methods. Thus, while philosophy characteristically inquires into the nature of being, the reality of objects, the possibility of knowledge, the nature of truth, and so on, metaphilosophy is the self-reflective inquiry into the nature, aims, and methods of the activity that makes these kinds of inquiries, by asking what "is" philosophy itself, what sorts of questions it should ask, how it might pose and answer them, and what it can achieve in doing so. It is considered by some to be a subject prior and preparatory to philosophy, while others see it as inherently a part of philosophy, or automatically a part of philosophy while others adopt some combination of these views.

The interest in metaphilosophy led to the establishment of the journal "Metaphilosophy" in January 1970.

Many sub-disciplines of philosophy have their own branch of 'metaphilosophy', examples being meta-aesthetics, meta-epistemology, meta-ethics, and metametaphysics (meta-ontology).

Although the "term" metaphilosophy and explicit attention to metaphilosophy as a specific domain within philosophy arose in the 20th century, the topic is likely as old as philosophy itself, and can be traced back at least as far as the works of Plato and Aristotle.

Some philosophers consider metaphilosophy to be a subject apart from philosophy, above or beyond it, while others object to that idea. Timothy Williamson argues that the philosophy of philosophy is "automatically part of philosophy", as is the philosophy of anything else. Nicholas Bunnin and Jiyuan Yu write that the separation of first- from second-order study has lost popularity as philosophers find it hard to observe the distinction. As evidenced by these contrasting opinions, debate persists as to whether the evaluation of the nature of philosophy is 'second-order philosophy' or simply 'plain philosophy'.

Many philosophers have expressed doubts over the value of metaphilosophy. Among them is Gilbert Ryle: "preoccupation with questions about methods tends to distract us from prosecuting the methods themselves. We run as a rule, worse, not better, if we think a lot about our feet. So let us ... not speak of it all but just do it."

The designations "metaphilosophy" and "philosophy of philosophy" have a variety of meanings, sometimes taken to be synonyms, and sometimes seen as distinct.

Morris Lazerowitz claims to have coined the term 'metaphilosophy' around 1940 and used it in print in 1942. Lazerowitz proposed that metaphilosophy is 'the investigation of the nature of philosophy'. Earlier uses have been found in translations from the French. The term is derived from Greek word "meta" μετά ("after", "beyond", "with") and "philosophía" φιλοσοφία ("love of wisdom").

The term 'metaphilosophy' is used by Paul Moser in the sense of a 'second-order' or more fundamental undertaking than philosophy itself, in the manner suggested by Charles Griswold:

This usage was considered nonsense by Ludwig Wittgenstein, who rejected the analogy between metalanguage and a metaphilosophy. As expressed by Martin Heidegger:
Some other philosophers treat the prefix "meta" as simply meaning '"about..."', rather than as referring to a metatheoretical 'second-order' form of philosophy, among them Rescher and Double. Others, such as Williamson, prefer the term "'philosophy of philosophy"' instead of 'metaphilosophy' as it avoids the connotation of a 'second-order' discipline that looks down on philosophy, and instead denotes something that is a part of it. Joll suggests that to take metaphilosophy as 'the application of the methods of philosophy to philosophy itself' is too vague, while the view that sees metaphilosophy as a 'second-order' or more abstract discipline, outside philosophy, "is narrow and tendentious".

In the analytical tradition, the term "metaphilosophy" is mostly used to tag commenting and research on previous works as opposed to original contributions towards solving philosophical problems.

Ludwig Wittgenstein wrote about the nature of philosophical puzzles and philosophical understanding. He suggested philosophical errors arose from confusions about the nature of philosophical inquiry. In the "Philosophical Investigations", Wittgenstein wrote that there is not a metaphilosophy in the sense of a metatheory of philosophy.

C. D. Broad distinguished Critical from Speculative philosophy in his "The Subject-matter of Philosophy, and its Relations to the special Sciences," in "Introduction to Scientific Thought", 1923. Curt Ducasse, in "Philosophy as a Science", examines several views of the nature of philosophy, and concludes that philosophy has a distinct subject matter: appraisals. Ducasse's view has been among the first to be described as 'metaphilosophy'.

Henri Lefebvre in "Métaphilosophie" (1965) argued, from a Marxian standpoint, in favor of an "ontological break", as a necessary methodological approach for critical social theory (whilst criticizing Louis Althusser's "epistemological break" with subjective Marxism, which represented a fundamental theoretical tool for the school of Marxist structuralism).

Paul Moser writes that typical metaphilosophical discussion includes determining the conditions under which a claim can be said to be a philosophical one. He regards meta-ethics, the study of ethics, to be a form of metaphilosophy, as well as meta-epistemology, the study of epistemology.

Many sub-disciplines of philosophy have their own branch of 'metaphilosophy'. However, some topics within 'metaphilosophy' cut across the various subdivisions of philosophy to consider fundamentals important to all its sub-disciplines. Some of these are mentioned below.

Some philosophers (e.g. existentialists, pragmatists) think philosophy is ultimately a practical discipline that should help us lead meaningful lives by showing us who we are, how we relate to the world around us and what we should do. Others (e.g. analytic philosophers) see philosophy as a technical, formal, and entirely theoretical discipline, with goals such as "the disinterested pursuit of knowledge for its own sake". Other proposed goals of philosophy include discovering the absolutely fundamental reason of everything it investigates, making explicit the nature and significance of ordinary and scientific beliefs, and unifying and transcending the insights given by science and religion. Others proposed that philosophy is a complex discipline because it has 4 or 6 different dimensions.

Defining philosophy and its boundaries is itself problematic; Nigel Warburton has called it "notoriously difficult". There is no straightforward definition, and most interesting definitions are controversial. As Bertrand Russell wrote:

While there is some agreement that philosophy involves general or fundamental topics, there is no clear agreement about a series of demarcation issues, including:



Philosophical method (or philosophical methodology) is the study of how to do philosophy. A common view among philosophers is that philosophy is distinguished by the ways that philosophers follow in addressing philosophical questions. There is not just one method that philosophers use to answer philosophical questions.

Recently, some philosophers have cast doubt about intuition as a basic tool in philosophical inquiry, from Socrates up to contemporary philosophy of language. In "Rethinking Intuition" various thinkers discard intuition as a valid source of knowledge and thereby call into question 'a priori' philosophy. Experimental philosophy is a form of philosophical inquiry that makes at least partial use of empirical research—especially "opinion polling"—in order to address persistent philosophical questions. This is in contrast with the methods found in analytic philosophy, whereby some say a philosopher will sometimes begin by appealing to his or her intuitions on an issue and then form an argument with those intuitions as premises. However, disagreement about what experimental philosophy can accomplish is widespread and several philosophers have offered criticisms. One claim is that the empirical data gathered by experimental philosophers can have an indirect effect on philosophical questions by allowing for a better understanding of the underlying psychological processes which lead to philosophical intuitions. Some analytic philosophers like Timothy Williamson have rejected such a move against 'armchair' philosophy–i.e., philosophical inquiry that is undergirded by intuition–by construing 'intuition' (which they believe to be a misnomer) as merely referring to common cognitive faculties: If one is calling into question 'intuition', one is, they would say, harboring a skeptical attitude towards common cognitive faculties–a consequence that seems philosophically unappealing. For Williamson, instances of intuition are instances of our cognitive faculties processing counterfactuals (or subjunctive conditionals) that are specific to the thought experiment or example in question. 

A prominent question in metaphilosophy is that of whether or not philosophical progress occurs and more so, whether such progress in philosophy is even possible. It has even been disputed, most notably by Ludwig Wittgenstein, whether genuine philosophical problems actually exist. The opposite has also been claimed, for example by Karl Popper, who held that such problems do exist, that they are solvable, and that he had actually found definite solutions to some of them.

David Chalmers divides inquiry into philosophical progress in metaphilosophy into three questions.





</doc>
<doc id="18888" url="https://en.wikipedia.org/wiki?curid=18888" title="Mandolin">
Mandolin

A mandolin ( ; literally "small mandola") is a stringed musical instrument in the lute family and is usually plucked with a plectrum. It commonly has four courses of doubled metal strings tuned in unison (8 strings), although five (10 strings) and six (12 strings) course versions also exist. The courses are typically tuned in a succession of perfect fifths, with the same tuning as a violin (G3, D4, A4, E5). Also like the violin, it is the soprano member of a family that includes the mandola, octave mandolin, mandocello and mandobass.

There are many styles of mandolin, but three are common, the "Neapolitan" or "round-backed" mandolin, the "carved-top" mandolin and the "flat-backed" mandolin. The round-back has a deep bottom, constructed of strips of wood, glued together into a bowl. The carved-top or "arch-top" mandolin has a much shallower, arched back, and an arched top—both carved out of wood. The flat-backed mandolin uses thin sheets of wood for the body, braced on the inside for strength in a similar manner to a guitar. Each style of instrument has its own sound quality and is associated with particular forms of music. Neapolitan mandolins feature prominently in European classical music and traditional music. Carved-top instruments are common in American folk music and bluegrass music. Flat-backed instruments are commonly used in Irish, British and Brazilian folk music. Some modern Brazilian instruments feature an extra fifth course tuned a fifth lower than the standard fourth course.

Other mandolin varieties differ primarily in the number of strings and include four-string models (tuned in fifths) such as the Brescian and Cremonese, six-string types (tuned in fourths) such as the Milanese, Lombard and the Sicilian and 6 course instruments of 12 strings (two strings per course) such as the Genoese. There has also been a twelve-string (three strings per course) type and an instrument with sixteen-strings (four strings per course).

Much of mandolin development revolved around the soundboard (the top). Pre-mandolin instruments were quiet instruments, strung with as many as six courses of gut strings, and were plucked with the fingers or with a quill. However, modern instruments are louder—using four courses of metal strings, which exert more pressure than the gut strings. The modern soundboard is designed to withstand the pressure of metal strings that would break earlier instruments. The soundboard comes in many shapes—but generally round or teardrop-shaped, sometimes with scrolls or other projections. There are usually one or more sound holes in the soundboard, either round, oval, or shaped like a calligraphic (f-hole). A round or oval sound hole may be covered or bordered with decorative rosettes or purfling.

See: History of the mandolin.

Mandolins evolved from lute family instruments in Europe. Predecessors include the gittern and mandore or mandola in Italy during the 17th and 18th centuries. There were a variety of regional variants, but two most widespread ones were the Neapolitan mandolin and the Lombardic mandolin. The Neapolitan style has spread worldwide.

Mandolins have a body that acts as a resonator, attached to a neck. The resonating body may be shaped as a bowl () or a box (). Traditional Italian mandolins, such as the Neapolitan mandolin, meet the necked bowl description. The necked box instruments include the carved top mandolins and the flatback mandolins.

Strings run between mechanical tuning machines at the top of the neck to a tailpiece that anchors the other end of the strings. The strings are suspended over the neck and soundboard and pass over a floating bridge. The bridge is kept in contact with the soundboard by the downward pressure from the strings. The neck is either flat or has a slight radius, and is covered with a fingerboard with frets. The action of the strings on the bridge causes the soundboard to vibrate, producing sound.

Like any plucked instrument, mandolin notes decay to silence rather than sound out continuously as with a bowed note on a violin, and mandolin notes decay faster than larger stringed instruments like the guitar. This encourages the use of tremolo (rapid picking of one or more pairs of strings) to create sustained notes or chords. The mandolin's paired strings facilitate this technique: the plectrum (pick) strikes each of a pair of strings alternately, providing a more full and continuous sound than a single string would.

Various design variations and amplification techniques have been used to make mandolins comparable in volume with louder instruments and orchestras, including the creation of mandolin-banjo hybrid with the louder banjo, adding metal resonators (most notably by Dobro and the National String Instrument Corporation) to make a resonator mandolin, and amplifying electric mandolins through amplifiers.

A variety of different tunings are used. Usually, courses of 2 adjacent strings are tuned in unison. By far the most common tuning is the same as violin tuning, in scientific pitch notation G–D–A–E, or in Helmholtz pitch notation: g–d′–a′–e″.


Note that the numbers of Hz shown above assume a 440 Hz A, standard in most parts of the western world. Some players use an A up to 10 Hz above or below a 440, mainly outside the United States.

Other tunings exist, including "cross-tunings", in which the usually doubled string runs are tuned to different pitches. Additionally, guitarists may sometimes tune a mandolin to mimic a portion of the intervals on a standard guitar tuning to achieve familiar fretting patterns.

The mandolin is the soprano member of the mandolin family, as the violin is the soprano member of the violin family. Like the violin, its scale length is typically about . Modern American mandolins modelled after Gibsons have a longer scale, about . The strings in each of its double-strung courses are tuned in unison, and the courses use the same tuning as the violin: G–D–A–E.

The piccolo or sopranino mandolin is a rare member of the family, tuned one octave above the mandola and one fourth above the mandolin (C–G–D–A); the same relation as that of the piccolo (to the flute) or sopranino violin (to the violin and viola). One model was manufactured by the Lyon & Healy company under the Leland brand. A handful of contemporary luthiers build piccolo mandolins. Its scale length is typically about .

The mandola (US and Canada), termed the tenor mandola in Britain and Ireland and liola or alto mandolin in continental Europe, which is tuned to a fifth below the mandolin, in the same relationship as that of the viola to the violin, or the alto flute to the concert flute. Some also call this instrument the "alto mandola." Its scale length is typically about . It is normally tuned like a viola (fifth below the mandolin) and tenor banjo: C–G–D–A.

The octave mandolin (US and Canada), termed the octave mandola in Britain and Ireland and mandola in continental Europe, is tuned an octave below the mandolin: G–D–A–E. Its relationship to the mandolin is that of the tenor violin to the violin. Octave mandolin scale length is typically about , although instruments with scales as short as or as long as are not unknown. 

Bandol: The instrument has a variant off the coast of South America in Trinidad, where it is known as the bandol, a flat-backed instrument with four courses, the lower two strung with metal and nylon strings.

The Irish bouzouki, although not strictly a member of the mandolin family, has a resemblance and similar range to the octave mandolin. It was derived from the Greek bouzouki (a long-necked lute), constructed like a flat-backed mandolin and uses fifth-based tunings, most often G–D–A–E (an octave below the mandolin)—in which case it essentially functions as an octave mandolin. Common alternate tunings include: G–D–A–D, A–D–A–D or A–D–A–E. Although the Irish bouzouki's bass course pairs are most often tuned in unison, on some instruments one of each pair is replaced with a lighter string and tuned in octaves, in the fashion of the 12-string guitar. While occupying the same range as the octave mandolin/octave mandola, the Irish bouzouki is theoretically distinguished from the former instrument by its longer scale length, typically from , although scales as long as , which is the usual Greek bouzouki scale, are not unknown. In modern usage, however, the terms "octave mandolin" and "Irish bouzouki" are often used interchangeably to refer to the same instrument.

The modern cittern may also be loosely included in an "extended" mandolin family, based on resemblance to the flat-backed mandolins, which it predates. Its own lineage dates it back to the Renaissance. It is typically a five course (ten string) instrument having a scale length between . The instrument is most often tuned to either D–G–D–A–D or G–D–A–D–A, and is essentially an octave mandola with a fifth course at either the top or the bottom of its range. Some luthiers, such as Stefan Sobell also refer to the octave mandola or a shorter-scaled Irish bouzouki as a cittern, irrespective of whether it has four or five courses.

Other relatives of the cittern, which might also be loosely linked to the mandolins (and are sometimes tuned and played as such), include the 6-course/12-string Portuguese guitar and the 5-course/9-string waldzither.

The mandocello, which is classically tuned to an octave plus a fifth below the mandolin, in the same relationship as that of the cello to the violin: C–G–D–A. Its scale length is typically about . A typical violoncello scale is .
The mandolone was a Baroque member of the mandolin family in the bass range that was surpassed by the mandocello. Built as part of the Neapolitan mandolin family.

The Greek laouto or laghouto (long-necked lute) is similar to a mandocello, ordinarily tuned C/C–G/G–D/D–A/A with half of each pair of the lower two courses being tuned an octave high on a lighter gauge string. The body is a staved bowl, the saddle-less bridge glued to the flat face like most ouds and lutes, with mechanical tuners, steel strings, and tied gut frets. Modern laoutos, as played on Crete, have the entire lower course tuned to C, a reentrant octave above the expected low C. Its scale length is typically about .

The Algerian mandole was developed by an Italian luthier in the early 1930s, scaled up from a mandola until it reached a scale length of approximately 25-27 inches. It is a flatback instrument, with a wide neck and 4 courses (8 strings), 5 courses (10 strings) or 6 courses (12 strings). Used in music in Algeria and Morocco. The instrument can be tuned as a guitar, oud or mandocello, depending on the music it will be used to play and player preference. When tuning it as a guitar the strings will be tuned (E) (E) A A D D G G B B (E) (E). Strings in parenthesis are dropped for a five or four course instrument. Using a common Arabic oud tuning D D G G A A D D (G) (G) (C) (C). For a mandocello tuning using fifths C C G G D D A A (E) (E).

The mando-bass most frequently has 4 single strings, rather than double courses, and is typically tuned in fourths like a double bass or a bass guitar: E–A–D–G. These were made by the Gibson company in the early 20th century, but appear to have never been very common. A smaller scale four-string mandobass, usually tuned in fifths: G–D–A–E (two octaves below the mandolin), though not as resonant as the larger instrument, was often preferred by players as easier to handle and more portable. Reportedly, however, most mandolin orchestras preferred to use the ordinary double bass, rather than a specialised mandolin family instrument. Calace and other Italian makers predating Gibson also made mandolin-basses.

The relatively rare eight-string mandobass, or tremolo-bass also exists, with double courses like the rest of the mandolin family, and is tuned either G–D–A–E, two octaves lower than the mandolin, or C–G–D–A, two octaves below the mandola.

Bowlback mandolins (also known as roundbacks), are used worldwide. They are most commonly manufactured in Europe, where the long history of mandolin development has created local styles. However, Japanese luthiers also make them.

Owing to the shape and to the common construction from wood strips of alternating colors, in the United States these are sometimes colloquially referred to as the "potato bug" or "potato beetle" mandolin.

The Neapolitan style has an almond-shaped body resembling a bowl, constructed from curved strips of wood. It usually has a bent sound table, canted in two planes with the design to take the tension of the eight metal strings arranged in four courses. A hardwood fingerboard sits on top of or is flush with the sound table. Very old instruments may use wooden tuning pegs, while newer instruments tend to use geared metal tuners. The bridge is a movable length of hardwood. A pickguard is glued below the sound hole under the strings. European roundbacks commonly use a scale instead of the common on archtop Mandolins.

Intertwined with the Neapolitan style is the Roman style mandolin, which has influenced it. The Roman mandolin had a fingerboard that was more curved and narrow. The fingerboard was lengthened over the sound hole for the E strings, the high pitched strings. The shape of the back of the neck was different, less rounded with an edge, the bridge was curved making the G strings higher. The Roman mandolin had mechanical tuning gears before the Neapolitan.

Prominent Italian manufacturers include Vinaccia (Naples), Embergher (Rome) and Calace (Naples). Other modern manufacturers include Lorenzo Lippi (Milan), Hendrik van den Broek (Netherlands), Brian Dean (Canada), Salvatore Masiello and Michele Caiazza (La Bottega del Mandolino) and Ferrara, Gabriele Pandini.

In the United States, when the bowlback was being made in numbers, Lyon and Healy was a major manufacturer, especially under the "Washburn" brand. Other American manufacturers include Martin, Vega, and Larson Brothers.

In Canada, Brian Dean has manufactured instruments in Neapolitan, Roman, German and American styles but is also known for his original 'Grand Concert' design created for American virtuoso Joseph Brent.

German manufacturers include Albert & Mueller, Dietrich, Klaus Knorr, Reinhold Seiffert and Alfred Woll. The German bowlbacks use a style developed by Seiffert, with a larger and rounder body.

Japanese brands include Kunishima and Suzuki. Other Japanese manufacturers include Oona, Kawada, Noguchi, Toichiro Ishikawa, Rokutaro Nakade, Otiai Tadao, Yoshihiko Takusari, Nokuti Makoto, Watanabe, Kanou Kadama and Ochiai.

Another family of bowlback mandolins came from Milan and Lombardy. These mandolins are closer to the mandolino or mandore than other modern mandolins. They are shorter and wider than the standard Neapolitan mandolin, with a shallow back. The instruments have 6 strings, 3 wire treble-strings and 3 gut or wire-wrapped-silk bass-strings. The strings ran between the tuning pegs and a bridge that was glued to the soundboard, as a guitar's. The Lombardic mandolins were tuned g–b–e′–a′–d″–g″ (shown in Helmholtz pitch notation). A developer of the Milanese stye was Antonio Monzino (Milan) and his family who made them for 6 generations.

Samuel Adelstein described the Lombardi mandolin in 1893 as wider and shorter than the Neapolitan mandolin, with a shallower back and a shorter and wider neck, with six single strings to the regular mandolin's set of 4. The Lombardi was tuned C–D–A–E–B–G. The strings were fastened to the bridge like a guitar's. There were 20 frets, covering three octaves, with an additional 5 notes. When Adelstein wrote, there were no nylon strings, and the gut and single strings "do not vibrate so clearly and sweetly as the double steel string of the Neapolitan."

Brescian mandolins (also known as Cremonese) that have survived in museums have four gut strings instead of six and a fixed bridge. The mandolin was tuned in fifths, like the Neapolitan mandolin. In his 1805 mandolin method, "Anweisung die Mandoline von selbst zu erlernen nebst einigen Uebungsstucken von Bortolazzi", Bartolomeo Bortolazzi popularised the Cremonese mandolin, which had four single-strings and a fixed bridge, to which the strings were attached. Bortolazzi said in this book that the new wire strung mandolins were uncomfortable to play, when compared with the gut-string instruments. Also, he felt they had a "less pleasing...hard, zither-like tone" as compared to the gut string's "softer, full-singing tone."
He favored the four single strings of the Cremonese instrument, which were tuned the same as the Neapolitan.

Like the Lombardy mandolin, the Genoese mandolin was not tuned in fifths. Its 6 gut strings (or 6 courses of strings) were tuned as a guitar but one octave higher: e-a-d’-g’-b natural-e”. Like the Neapolitan and unlike the Lombardy mandolin, the Genoese does not have the bridge glued to the soundboard, but holds the bridge on with downward tension, from strings that run between the bottom and neck of the instrument. The neck was wider than the Neapolitan mandolin's neck. The peg-head is similar to the guitar's.

At the very end of the 19th century, a new style, with a carved top and back construction inspired by violin family instruments began to supplant the European-style bowl-back instruments in the United States. This new style is credited to mandolins designed and built by Orville Gibson, a Kalamazoo, Michigan luthier who founded the "Gibson Mandolin-Guitar Manufacturing Co., Limited" in 1902. Gibson mandolins evolved into two basic styles: the Florentine or F-style, which has a decorative scroll near the neck, two points on the lower body and usually a scroll carved into the headstock; and the A-style, which is pear shaped, has no points and usually has a simpler headstock.

These styles generally have either two f-shaped soundholes like a violin (F-5 and A-5), or an oval sound hole (F-4 and A-4 and lower models) directly under the strings. Much variation exists between makers working from these archetypes, and other variants have become increasingly common. Generally, in the United States, Gibson F-hole F-5 mandolins and mandolins influenced by that design are strongly associated with bluegrass, while the A-style is associated with other types of music, although it too is most often used for and associated with bluegrass. The F-5's more complicated woodwork also translates into a more expensive instrument.

Internal bracing to support the top in the F-style mandolins is usually achieved with parallel tone bars, similar to the bass bar on a violin. Some makers instead employ "X-bracing," which is two tone bars mortised together to form an X. Some luthiers now using a "modified x-bracing" that incorporates both a tone bar and X-bracing.

Numerous modern mandolin makers build instruments that largely replicate the Gibson F-5 Artist models built in the early 1920s under the supervision of Gibson acoustician Lloyd Loar. Original Loar-signed instruments are sought after and extremely valuable. Other makers from the Loar period and earlier include Lyon and Healy, Vega and Larson Brothers.

Flatback mandolins use a thin sheet of wood with bracing for the back, as a guitar uses, rather than the bowl of the bowlback or the arched back of the carved mandolins.

Like the bowlback, the flatback has a round sound hole. This has been sometimes modified to an elongated hole, called a D-hole. The body has a rounded almond shape with flat or sometimes canted soundboard.

The type was developed in Europe in the 1850s. The French and Germans called it a Portuguese mandolin, although they also developed it locally. The Germans used it in Wandervogel.

The bandolim is commonly used wherever the Spanish and Portuguese took it: in South America, in Brazil (Choro) and in the Philippines.

In the early 1970s English luthier Stefan Sobell developed a large-bodied, flat-backed mandolin with a carved soundboard, based on his own cittern design; this is often called a 'Celtic' mandolin.

American forms include the Army-Navy mandolin, the flatiron and the pancake mandolins.

The tone of the flatback is described as warm or mellow, suitable for folk music and smaller audiences. The instrument sound does not punch through the other players' sound like a carved top does.

The double top is a feature that luthiers are experimenting with in the 21st century, to get better sound. 
However, mandolinists and luthiers have been experimenting with them since at least the early 1900s.

Back in the early 1900s, mandolinist Ginislao Paris approached Luigi Embergher to build custom mandolins. The sticker inside one of the four surviving instruments indicates the build was called after him, the "Sistema Ginislao Paris"). Paris' round-back double-top mandolins use a false back below the soundboard to create a second hollow space within the instrument.

Modern mandolinists such as Joseph Brent and Avi Avital use instruments customized, either by the luthier's choice or at the request of player. Joseph Brent's mandolin, made by Brian Dean also uses what Brent calls a false back. Brent's mandolin was the luthier's solution to Brent's request for a loud mandolin in which the wood was clearly audible, with less metallic sound from the strings. The type used by Avital is variation of the flatback, with a double top that encloses a resonating chamber, sound holes on the side, and a convex back. It is made by one manufacturer in Israel, luthier Arik Kerman. Other players of Kerman mandolins include Alon Sariel, Jacob Reuven, and Tom Cohen.

Other American-made variants include the mandolinetto or Howe-Orme guitar-shaped mandolin (manufactured by the Elias Howe Company between 1897 and roughly 1920), which featured a cylindrical bulge along the top from fingerboard end to tailpiece and the Vega mando-lute (more commonly called a cylinder-back mandolin manufactured by the Vega Company between 1913 and roughly 1927), which had a similar longitudinal bulge but on the back rather than the front of the instrument.

The mandolin was given a banjo body in an 1882 patent by Benjamin Bradbury of Brooklyn and given the name "banjolin" by John Farris in 1885. Today "banjolin" describes an instrument with four strings, while the version with the four courses of double strings is called a "mandolin-banjo".

A resonator mandolin or "resophonic mandolin" is a mandolin whose sound is produced by one or more metal cones (resonators) instead of the customary wooden soundboard (mandolin top/face). Historic brands include Dobro and National.

As with almost every other contemporary string instrument, another modern variant is the electric mandolin. These mandolins can have four or five individual or double courses of strings.

They have been around since the late 1920s or early 1930s depending on the brand. They come in solid body and acoustic electric forms.

Instruments have been designed that overcome the mandolin's lack of sustain with its plucked notes. Fender released a model in 1992 with an additional string (a high a, above the e string), a tremolo bridge and extra humbucker pickup (total of two). The result was an instrument capable of playing heavy metal style guitar riffs or violin-like passages with sustained notes that can be adjusted as with an electric guitar.

See Mandolin playing traditions worldwide and History of the mandolin
The international repertoire of music for mandolin is almost unlimited, and musicians use it to play various types of music. This is especially true of violin music, since the mandolin has the same tuning as the violin. Following its invention and early development in Italy the mandolin spread throughout the European continent. The instrument was primarily used in a classical tradition with Mandolin orchestras, so called "Estudiantinas" or in Germany "Zupforchestern" appearing in many cities. Following this continental popularity of the mandolin family local traditions appeared outside Europe in the Americas and in Japan. Travelling mandolin virtuosi like Carlo Curti, Giuseppe Pettine, Raffaele Calace and Silvio Ranieri contributed to the mandolin becoming a "fad" instrument in the early 20th century. This "mandolin craze" was fading by the 1930s, but just as this practice was falling into disuse, the mandolin found a new niche in American country, old-time music, bluegrass and folk music. More recently, the Baroque and Classical mandolin repertory and styles have benefited from the raised awareness of and interest in Early music, with media attention to classical players such as Israeli Avi Avital, Italian Carlo Aonzo and American Joseph Brent.

The tradition of so-called "classical music" for the mandolin has been somewhat spotty, due to its being widely perceived as a "folk" instrument. Significant composers did write music specifically for the mandolin, but few "large" works were composed for it by the most widely regarded composers. The total number of these works is rather small in comparison to—say—those composed for violin. One result of this dearth being that there were few positions for mandolinists in regular orchestras. To fill this gap in the literature, mandolin orchestras have traditionally played many arrangements of music written for regular orchestras or other ensembles. Some players have sought out contemporary composers to solicit new works.

Furthermore, of the works that have been written for mandolin from the 18th century onward, many have been lost or forgotten. Some of these await discovery in museums and libraries and archives. One example of rediscovered 18th-century music for mandolin and ensembles with mandolins is the "Gimo collection", collected in the first half of 1762 by Jean Lefebure. Lefebure collected the music in Italy, and it was forgotten until manuscripts were rediscovered.

Vivaldi created some concertos for mandolinos and orchestra: one for 4-chord mandolino, string bass & continuous in C major, (RV 425), and one for two 5-chord mandolinos, bass strings & continuous in G major, (RV 532), and concerto for two mandolins, 2 violons "in Tromba"—2 flûtes à bec, 2 salmoe, 2 théorbes, violoncelle, cordes et basse continuein in C major (p. 16).

Beethoven composed mandolin music and enjoyed playing the mandolin. His 4 small pieces date from 1796: Sonatine WoO 43a; Adagio ma non troppo WoO 43b; Sonatine WoO 44a and Andante con Variazioni WoO 44b.

The opera "Don Giovanni" by Mozart (1787) includes mandolin parts, including the accompaniment to the famous aria "Deh vieni alla finestra", and Verdi's opera Otello calls for guzla accompaniment in the aria "Dove guardi splendono raggi", but the part is commonly performed on mandolin.

Gustav Mahler used the mandolin in his Symphony No. 7, Symphony No. 8 and Das Lied von der Erde.

Parts for mandolin are included in works by Schoenberg (Variations Op. 31), Stravinsky (Agon), Prokofiev (Romeo and Juliet) and Webern (opus Parts 10)

Some 20th century composers also used the mandolin as their instrument of choice (amongst these are: Schoenberg, Webern, Stravinsky and Prokofiev).

Among the most important European mandolin composers of the 20th century are Raffaele Calace (composer, performer and luthier) and Giuseppe Anedda (virtuoso concert pianist and professor of the first chair of the Conservatory of Italian Mandolin, Padua, 1975). Today representatives of Italian classical music and Italian classical-contemporary music include Ugo Orlandi, Carlo Aonzo, Dorina Frati, Mauro Squillante and Duilio Galfetti.

Japanese composers also produced orchestral music for mandolin in the 20th century, but these are not well known outside Japan.

Traditional mandolin orchestras remain especially popular in Japan and Germany, but also exist throughout the United States, Europe and the rest of the world. They perform works composed for mandolin family instruments, or re-orchestrations of traditional pieces. The structure of a contemporary traditional mandolin orchestra consists of: first and second mandolins, mandolas (either octave mandolas, tuned an octave below the mandolin, or tenor mandolas, tuned like the viola), mandocellos (tuned like the cello), and bass instruments (conventional string bass or, rarely, mandobasses). Smaller ensembles, such as quartets composed of two mandolins, mandola, and mandocello, may also be found.

































A duet or duo is a musical composition for two performers in which the performers have equal importance to the piece. A musical ensemble with more than two solo instruments or voices is called trio, quartet, quintet, sextet, septet, octet, etc.


































Concerto: a musical composition generally composed of three movements, in which, usually, one solo instrument (for instance, a piano, violin, cello or flute) is accompanied by an orchestra or concert band.




















Orchestral works in which the mandolin has a limited part.






















Chord dictionaries

Method and instructional guides



</doc>
<doc id="18889" url="https://en.wikipedia.org/wiki?curid=18889" title="Microphotonics">
Microphotonics

Microphotonics is a branch of technology that deals with directing light on a microscopic scale and is used in optical networking. Particularly, it refers to the branch of technology that deals with wafer-level integrated devices and systems that emit, transmit, detect, and process light along with other forms of radiant energy with photon as the quantum unit.

Microphotonics employs at least two different materials with a large differential index of refraction to squeeze the light down to a small size. Generally speaking, virtually all of microphotonics relies on Fresnel reflection to guide the light. If the photons reside mainly in the higher index material, the confinement is due to total internal reflection. If the confinement is due many distributed Fresnel reflections, the device is termed a photonic crystal. There are many different types of geometries used in microphotonics including optical waveguides, optical microcavities, and Arrayed waveguide gratings.

Photonic crystals are non-conducting materials that reflect various wavelengths of light almost perfectly. Such a crystal can be referred to as a perfect mirror. Other devices employed in microphotonics include micromirrors and photonic wire waveguides. These tools are used to "mold the flow of light", a famous phrase for describing the goal of microphotonics. The crystals serve as structures that allow the manipulation, confinement, and control of light in one, two, or three dimensions of space.

An optical microdisk, optical microtoroid, or optical microsphere uses internal reflection in a circular geometry to hold on to the photons. This type of circularly symmetric optical resonance is called a Whispering gallery mode, after Lord Rayleigh coined the term.

Microphotonics has biological applications and these can be demonstrated in the case of the "biophotonic chips", which are developed to increase efficiency in terms of "photonic yield" or the collected luminescent signal emitted by fluorescent markers used in biological chips.

Currently, microphotonics technology is also being developed to replace electronics devices and bio-compatible intracellular devices. For instance, the long-standing goal of an all-optical router would eliminate electronic bottlenecks, speeding up the network. Perfect mirrors are being developed for use in fiber optic cables.



</doc>
<doc id="18890" url="https://en.wikipedia.org/wiki?curid=18890" title="Microsoft Windows">
Microsoft Windows

Microsoft Windows, commonly referred to as Windows, is a group of several proprietary graphical operating system families, all of which are developed and marketed by Microsoft. Each family caters to a certain sector of the computing industry. Active Microsoft Windows families include Windows NT and Windows IoT; these may encompass subfamilies, e.g. Windows Server or Windows Embedded Compact (Windows CE). Defunct Microsoft Windows families include Windows 9x, Windows Mobile and Windows Phone.

Microsoft introduced an operating environment named "Windows" on November 20, 1985, as a graphical operating system shell for MS-DOS in response to the growing interest in graphical user interfaces (GUIs). Microsoft Windows came to dominate the world's personal computer (PC) market with over 90% market share, overtaking Mac OS, which had been introduced in 1984. Apple came to see Windows as an unfair encroachment on their innovation in GUI development as implemented on products such as the Lisa and Macintosh (eventually settled in court in Microsoft's favor in 1993). On PCs, Windows is still the most popular operating system. However, in 2014, Microsoft admitted losing the majority of the overall operating system market to Android, because of the massive growth in sales of Android smartphones. In 2014, the number of Windows devices sold was less than 25% that of Android devices sold. This comparison, however, may not be fully relevant, as the two operating systems traditionally target different platforms. Still, numbers for server use of Windows (that are comparable to competitors) show one third market share, similar to that for end user use. 

, the most recent version of Windows for PCs, tablets and embedded devices is Windows 10, version 2004. The most recent version for server computers is Windows Server, version 2004. A specialized version of Windows also runs on the Xbox One video game console.

Microsoft, the developer of Windows, has registered several trademarks, each of which denotes a family of Windows operating systems that target a specific sector of the computing industry. As of 2014, the following Windows families were being actively developed:


The following Windows families are no longer being developed:


The term "Windows" collectively describes any or all of several generations of Microsoft operating system products. These products are generally categorized as follows:

The history of Windows dates back to 1981 when Microsoft started work on a program called "Interface Manager". It was announced in November 1983 (after the Apple Lisa, but before the Macintosh) under the name "Windows", but Windows 1.0 was not released until November 1985. Windows 1.0 was to compete with Apple's operating system, but achieved little popularity. Windows 1.0 is not a complete operating system; rather, it extends MS-DOS. The shell of Windows 1.0 is a program known as the MS-DOS Executive. Components included Calculator, Calendar, Cardfile, Clipboard Viewer, Clock, Control Panel, Notepad, Paint, Reversi, Terminal and Write. Windows 1.0 does not allow overlapping windows. Instead all windows are tiled. Only modal dialog boxes may appear over other windows. Microsoft sold as included Windows Development libraries with the C development environment, which included numerous windows samples.

Windows 2.0 was released in December 1987, and was more popular than its predecessor. It features several improvements to the user interface and memory management. Windows 2.03 changed the OS from tiled windows to overlapping windows. The result of this change led to Apple Computer filing a suit against Microsoft alleging infringement on Apple's copyrights. Windows 2.0 also introduced more sophisticated keyboard shortcuts and could make use of expanded memory.

Windows 2.1 was released in two different versions: Windows/286 and Windows/386. Windows/386 uses the virtual 8086 mode of the Intel 80386 to multitask several DOS programs and the paged memory model to emulate expanded memory using available extended memory. Windows/286, in spite of its name, runs on both Intel 8086 and Intel 80286 processors. It runs in real mode but can make use of the high memory area. 

In addition to full Windows-packages, there were runtime-only versions that shipped with early Windows software from third parties and made it possible to run their Windows software on MS-DOS and without the full Windows feature set.

The early versions of Windows are often thought of as graphical shells, mostly because they ran on top of MS-DOS and use it for file system services. However, even the earliest Windows versions already assumed many typical operating system functions; notably, having their own executable file format and providing their own device drivers (timer, graphics, printer, mouse, keyboard and sound). Unlike MS-DOS, Windows allowed users to execute multiple graphical applications at the same time, through cooperative multitasking. Windows implemented an elaborate, segment-based, software virtual memory scheme, which allows it to run applications larger than available memory: code segments and resources are swapped in and thrown away when memory became scarce; data segments moved in memory when a given application had relinquished processor control.

Windows 3.0, released in 1990, improved the design, mostly because of virtual memory and loadable virtual device drivers (VxDs) that allow Windows to share arbitrary devices between multi-tasked DOS applications. Windows 3.0 applications can run in protected mode, which gives them access to several megabytes of memory without the obligation to participate in the software virtual memory scheme. They run inside the same address space, where the segmented memory provides a degree of protection. Windows 3.0 also featured improvements to the user interface. Microsoft rewrote critical operations from C into assembly. Windows 3.0 is the first Microsoft Windows version to achieve broad commercial success, selling 2 million copies in the first six months.

Windows 3.1, made generally available on March 1, 1992, featured a facelift. In August 1993, Windows for Workgroups, a special version with integrated peer-to-peer networking features and a version number of 3.11, was released. It was sold along with Windows 3.1. Support for Windows 3.1 ended on December 31, 2001.

Windows 3.2, released 1994, is an updated version of the Chinese version of Windows 3.1. The update was limited to this language version, as it fixed only issues related to the complex writing system of the Chinese language. Windows 3.2 was generally sold by computer manufacturers with a ten-disk version of MS-DOS that also had Simplified Chinese characters in basic output and some translated utilities.

The next major consumer-oriented release of Windows, Windows 95, was released on August 24, 1995. While still remaining MS-DOS-based, Windows 95 introduced support for native 32-bit applications, plug and play hardware, preemptive multitasking, long file names of up to 255 characters, and provided increased stability over its predecessors. Windows 95 also introduced a redesigned, object oriented user interface, replacing the previous Program Manager with the Start menu, taskbar, and Windows Explorer shell. Windows 95 was a major commercial success for Microsoft; Ina Fried of CNET remarked that "by the time Windows 95 was finally ushered off the market in 2001, it had become a fixture on computer desktops around the world." Microsoft published four OEM Service Releases (OSR) of Windows 95, each of which was roughly equivalent to a service pack. The first OSR of Windows 95 was also the first version of Windows to be bundled with Microsoft's web browser, Internet Explorer. Mainstream support for Windows 95 ended on December 31, 2000, and extended support for Windows 95 ended on December 31, 2001.

Windows 95 was followed up with the release of Windows 98 on June 25, 1998, which introduced the Windows Driver Model, support for USB composite devices, support for ACPI, hibernation, and support for multi-monitor configurations. Windows 98 also included integration with Internet Explorer 4 through Active Desktop and other aspects of the Windows Desktop Update (a series of enhancements to the Explorer shell which were also made available for Windows 95). In May 1999, Microsoft released Windows 98 Second Edition, an updated version of Windows 98. Windows 98 SE added Internet Explorer 5.0 and Windows Media Player 6.2 amongst other upgrades. Mainstream support for Windows 98 ended on June 30, 2002, and extended support for Windows 98 ended on July 11, 2006.

On September 14, 2000, Microsoft released Windows Me (Millennium Edition), the last DOS-based version of Windows. Windows Me incorporated visual interface enhancements from its Windows NT-based counterpart Windows 2000, had faster boot times than previous versions (which however, required the removal of the ability to access a real mode DOS environment, removing compatibility with some older programs), expanded multimedia functionality (including Windows Media Player 7, Windows Movie Maker, and the Windows Image Acquisition framework for retrieving images from scanners and digital cameras), additional system utilities such as System File Protection and System Restore, and updated home networking tools. However, Windows Me was faced with criticism for its speed and instability, along with hardware compatibility issues and its removal of real mode DOS support. "PC World" considered Windows Me to be one of the worst operating systems Microsoft had ever released, and the 4th worst tech product of all time.

In November 1988, a new development team within Microsoft (which included former Digital Equipment Corporation developers Dave Cutler and Mark Lucovsky) began work on a revamped version of IBM and Microsoft's OS/2 operating system known as "NT OS/2". NT OS/2 was intended to be a secure, multi-user operating system with POSIX compatibility and a modular, portable kernel with preemptive multitasking and support for multiple processor architectures. However, following the successful release of Windows 3.0, the NT development team decided to rework the project to use an extended 32-bit port of the Windows API known as Win32 instead of those of OS/2. Win32 maintained a similar structure to the Windows APIs (allowing existing Windows applications to easily be ported to the platform), but also supported the capabilities of the existing NT kernel. Following its approval by Microsoft's staff, development continued on what was now Windows NT, the first 32-bit version of Windows. However, IBM objected to the changes, and ultimately continued OS/2 development on its own.

The first release of the resulting operating system, Windows NT 3.1 (named to associate it with Windows 3.1) was released in July 1993, with versions for desktop workstations and servers. Windows NT 3.5 was released in September 1994, focusing on performance improvements and support for Novell's NetWare, and was followed up by Windows NT 3.51 in May 1995, which included additional improvements and support for the PowerPC architecture. Windows NT 4.0 was released in June 1996, introducing the redesigned interface of Windows 95 to the NT series. On February 17, 2000, Microsoft released Windows 2000, a successor to NT 4.0. The Windows NT name was dropped at this point in order to put a greater focus on the Windows brand.

The next major version of Windows NT, Windows XP, was released on October 25, 2001. The introduction of Windows XP aimed to unify the consumer-oriented Windows 9x series with the architecture introduced by Windows NT, a change which Microsoft promised would provide better performance over its DOS-based predecessors. Windows XP would also introduce a redesigned user interface (including an updated Start menu and a "task-oriented" Windows Explorer), streamlined multimedia and networking features, Internet Explorer 6, integration with Microsoft's .NET Passport services, modes to help provide compatibility with software designed for previous versions of Windows, and Remote Assistance functionality.

At retail, Windows XP was now marketed in two main editions: the "Home" edition was targeted towards consumers, while the "Professional" edition was targeted towards business environments and power users, and included additional security and networking features. Home and Professional were later accompanied by the "Media Center" edition (designed for home theater PCs, with an emphasis on support for DVD playback, TV tuner cards, DVR functionality, and remote controls), and the "Tablet PC" edition (designed for mobile devices meeting its specifications for a tablet computer, with support for stylus pen input and additional pen-enabled applications). Mainstream support for Windows XP ended on April 14, 2009. Extended support ended on April 8, 2014.

After Windows 2000, Microsoft also changed its release schedules for server operating systems; the server counterpart of Windows XP, Windows Server 2003, was released in April 2003. It was followed in December 2005, by Windows Server 2003 R2.

After a lengthy development process, Windows Vista was released on November 30, 2006, for volume licensing and January 30, 2007, for consumers. It contained a number of new features, from a redesigned shell and user interface to significant technical changes, with a particular focus on security features. It was available in a number of different editions, and has been subject to some criticism, such as drop of performance, longer boot time, criticism of new UAC, and stricter license agreement. Vista's server counterpart, Windows Server 2008 was released in early 2008.

On July 22, 2009, Windows 7 and Windows Server 2008 R2 were released as RTM (release to manufacturing) while the former was released to the public 3 months later on October 22, 2009. Unlike its predecessor, Windows Vista, which introduced a large number of new features, Windows 7 was intended to be a more focused, incremental upgrade to the Windows line, with the goal of being compatible with applications and hardware with which Windows Vista was already compatible. Windows 7 has multi-touch support, a redesigned Windows shell with an updated taskbar, a home networking system called HomeGroup, and performance improvements.
Windows 8, the successor to Windows 7, was released generally on October 26, 2012. A number of significant changes were made on Windows 8, including the introduction of a user interface based around Microsoft's Metro design language with optimizations for touch-based devices such as tablets and all-in-one PCs. These changes include the Start screen, which uses large tiles that are more convenient for touch interactions and allow for the display of continually updated information, and a new class of apps which are designed primarily for use on touch-based devices. The new Windows version required a minimum resolution of 1024×768 pixels, effectively making it unfit for netbooks with 800×600-pixel screens.

Other changes include increased integration with cloud services and other online platforms (such as social networks and Microsoft's own OneDrive (formerly SkyDrive) and Xbox Live services), the Windows Store service for software distribution, and a new variant known as Windows RT for use on devices that utilize the ARM architecture. An update to Windows 8, called Windows 8.1, was released on October 17, 2013, and includes features such as new live tile sizes, deeper OneDrive integration, and many other revisions. Windows 8 and Windows 8.1 have been subject to some criticism, such as removal of the Start menu.

On September 30, 2014, Microsoft announced Windows 10 as the successor to Windows 8.1. It was released on July 29, 2015, and addresses shortcomings in the user interface first introduced with Windows 8. Changes on PC include the return of the Start Menu, a virtual desktop system, and the ability to run Windows Store apps within windows on the desktop rather than in full-screen mode. Windows 10 is said to be available to update from qualified Windows 7 with SP1, Windows 8.1 and Windows Phone 8.1 devices from the Get Windows 10 Application (for Windows 7, Windows 8.1) or Windows Update (Windows 7).

In February 2017, Microsoft announced the migration of its Windows source code repository from Perforce to Git. This migration involved 3.5 million separate files in a 300 gigabyte repository. By May 2017, 90 percent of its engineering team was using Git, in about 8500 commits and 1760 Windows builds per day.

Multilingual support has been built into Windows since Windows 3. The language for both the keyboard and the interface can be changed through the Region and Language Control Panel. Components for all supported input languages, such as Input Method Editors, are automatically installed during Windows installation (in Windows XP and earlier, files for East Asian languages, such as Chinese, and right-to-left scripts, such as Arabic, may need to be installed separately, also from the said Control Panel). Third-party IMEs may also be installed if a user feels that the provided one is insufficient for their needs.

Interface languages for the operating system are free for download, but some languages are limited to certain editions of Windows. Language Interface Packs (LIPs) are redistributable and may be downloaded from Microsoft's Download Center and installed for any edition of Windows (XP or later) they translate most, but not all, of the Windows interface, and require a certain base language (the language which Windows originally shipped with). This is used for most languages in emerging markets. Full Language Packs, which translates the complete operating system, are only available for specific editions of Windows (Ultimate and Enterprise editions of Windows Vista and 7, and all editions of Windows 8, 8.1 and RT except Single Language). They do not require a specific base language, and are commonly used for more popular languages such as French or Chinese. These languages cannot be downloaded through the Download Center, but available as optional updates through the Windows Update service (except Windows 8).

The interface language of installed applications are not affected by changes in the Windows interface language. Availability of languages depends on the application developers themselves.

Windows 8 and Windows Server 2012 introduces a new Language Control Panel where both the interface and input languages can be simultaneously changed, and language packs, regardless of type, can be downloaded from a central location. The PC Settings app in Windows 8.1 and Windows Server 2012 R2 also includes a counterpart settings page for this. Changing the interface language also changes the language of preinstalled Windows Store apps (such as Mail, Maps and News) and certain other Microsoft-developed apps (such as Remote Desktop). The above limitations for language packs are however still in effect, except that full language packs can be installed for any edition except Single Language, which caters to emerging markets.

Windows NT included support for several different platforms before the x86-based personal computer became dominant in the professional world. Windows NT 4.0 and its predecessors supported PowerPC, DEC Alpha and MIPS R4000. (Although some these platforms implement 64-bit computing, the operating system treated them as 32-bit.) However, Windows 2000, the successor of Windows NT 4.0, dropped support for all platforms except the third generation x86 (known as IA-32) or newer in 32-bit mode. The client line of Windows NT family still runs on IA-32, although the Windows Server line has ceased supporting this platform with the release of Windows Server 2008 R2.

With the introduction of the Intel Itanium architecture (IA-64), Microsoft released new versions of Windows to support it. Itanium versions of Windows XP and Windows Server 2003 were released at the same time as their mainstream x86 counterparts. Windows XP 64-Bit Edition, released in 2005, is the last Windows client operating systems to support Itanium. Windows Server line continues to support this platform until Windows Server 2012; Windows Server 2008 R2 is the last Windows operating system to support Itanium architecture.

On April 25, 2005, Microsoft released Windows XP Professional x64 Edition and Windows Server 2003 x64 Editions to support the x86-64 (or simply x64), the eighth generation of x86 architecture. Windows Vista was the first client version of Windows NT to be released simultaneously in IA-32 and x64 editions. x64 is still supported.

An edition of Windows 8 known as Windows RT was specifically created for computers with ARM architecture and while ARM is still used for Windows smartphones with Windows 10, tablets with Windows RT will not be updated. Starting from Windows 10 Fall Creators Update and later includes support for PCs with ARM architecture.

Windows CE (officially known as "Windows Embedded Compact"), is an edition of Windows that runs on minimalistic computers, like satellite navigation systems and some mobile phones. Windows Embedded Compact is based on its own dedicated kernel, dubbed Windows CE kernel. Microsoft licenses Windows CE to OEMs and device makers. The OEMs and device makers can modify and create their own user interfaces and experiences, while Windows CE provides the technical foundation to do so.

Windows CE was used in the Dreamcast along with Sega's own proprietary OS for the console. Windows CE was the core from which Windows Mobile was derived. Its successor, Windows Phone 7, was based on components from both Windows CE 6.0 R3 and Windows CE 7.0. Windows Phone 8 however, is based on the same NT-kernel as Windows 8.

Windows Embedded Compact is not to be confused with Windows XP Embedded or Windows NT 4.0 Embedded, modular editions of Windows based on Windows NT kernel.

Xbox OS is an unofficial name given to the version of Windows that runs on the Xbox One. It is a more specific implementation with an emphasis on virtualization (using Hyper-V) as it is three operating systems running at once, consisting of the core operating system, a second implemented for games and a more Windows-like environment for applications.
Microsoft updates Xbox One's OS every month, and these updates can be downloaded from the Xbox Live service to the Xbox and subsequently installed, or by using offline recovery images downloaded via a PC. The Windows 10-based Core had replaced the Windows 8-based one in this update, and the new system is sometimes referred to as "Windows 10 on Xbox One" or "OneCore".
Xbox One's system also allows backward compatibility with Xbox 360, and the Xbox 360's system is backwards compatible with the original Xbox.

In 2017 Microsoft announced that it would start using Git, an open source version control system created by Linus Torvalds. Microsoft has previously used a proprietary version control system called "Source Depot". Microsoft had begun to integrate Git into Team Foundation Server in 2013, but Windows continued to rely on Source Depot. Because of its large, decades-long history, the Windows codebase is not especially well suited to the decentralized nature of Linux development that Git was originally created to manage. Each Git repository contains a complete history of all the files, which proved unworkable for Windows developers because cloning the repository takes several hours. Microsoft has been working on a new project called the Virtual File System for Git (VFSForGit) to address these challenges.

According to Net Applications, which tracks the use of operating systems in devices that are active on the Web, Windows was the most used operating-system family on personal computers in April 2020, with around 88% usage share. Including personal computers of all kinds (e.g., desktops, laptops, mobile devices, and game consoles), Windows OSes accounted for 35.84% of usage share in May 2020, compared to Android (highest, at 37.48%), iOS's 15.52%, and macOS's 8.61%, according to StatCounter, which tracks use of operating systems by their use in devices active on the Web. Windows is used in less than half the market not only in developing countries, but also in developed ones such as the United States, where use of Windows on desktops, on which it is the plurality operating system, has fallen to 48.46%, and the United Kingdom and Ireland. These numbers are easiest (monthly numbers) to find that track real use, but they may not mirror installed base or sales numbers (in recent years) of devices. They are consistent with server numbers in next section.

Use of the latest version Windows 10 has exceeded Windows 7 globally since early 2018.

Usage share of Windows on serversthose running a web server that is (there are also other kinds of servers) is at 30.3%.

Consumer versions of Windows were originally designed for ease-of-use on a single-user PC without a network connection, and did not have security features built in from the outset. However, Windows NT and its successors are designed for security (including on a network) and multi-user PCs, but were not initially designed with Internet security in mind as much, since, when it was first developed in the early 1990s, Internet use was less prevalent.

These design issues combined with programming errors (e.g. buffer overflows) and the popularity of Windows means that it is a frequent target of computer worm and virus writers. In June 2005, Bruce Schneier's "Counterpane Internet Security" reported that it had seen over 1,000 new viruses and worms in the previous six months. In 2005, Kaspersky Lab found around 11,000 malicious programs viruses, Trojans, back-doors, and exploits written for Windows.

Microsoft releases security patches through its Windows Update service approximately once a month (usually the second Tuesday of the month), although critical updates are made available at shorter intervals when necessary. In versions of Windows after and including Windows 2000 SP3 and Windows XP, updates can be automatically downloaded and installed if the user selects to do so. As a result, Service Pack 2 for Windows XP, as well as Service Pack 1 for Windows Server 2003, were installed by users more quickly than it otherwise might have been.

While the Windows 9x series offered the option of having profiles for multiple users, they had no concept of access privileges, and did not allow concurrent access; and so were not true multi-user operating systems. In addition, they implemented only partial memory protection. They were accordingly widely criticised for lack of security.

The Windows NT series of operating systems, by contrast, are true multi-user, and implement absolute memory protection. However, a lot of the advantages of being a true multi-user operating system were nullified by the fact that, prior to Windows Vista, the first user account created during the setup process was an administrator account, which was also the default for new accounts. Though Windows XP did have limited accounts, the majority of home users did not change to an account type with fewer rights – partially due to the number of programs which unnecessarily required administrator rights – and so most home users ran as administrator all the time.

Windows Vista changes this by introducing a privilege elevation system called User Account Control. When logging in as a standard user, a logon session is created and a token containing only the most basic privileges is assigned. In this way, the new logon session is incapable of making changes that would affect the entire system. When logging in as a user in the Administrators group, two separate tokens are assigned. The first token contains all privileges typically awarded to an administrator, and the second is a restricted token similar to what a standard user would receive. User applications, including the Windows shell, are then started with the restricted token, resulting in a reduced privilege environment even under an Administrator account. When an application requests higher privileges or "Run as administrator" is clicked, UAC will prompt for confirmation and, if consent is given (including administrator credentials if the account requesting the elevation is not a member of the administrators group), start the process using the unrestricted token.

Leaked documents published by WikiLeaks, codenamed Vault 7 and dated from 2013–2016, detail the capabilities of the CIA to perform electronic surveillance and cyber warfare, such as the ability to compromise operating systems such as Microsoft Windows.

In August 2019, computer experts reported that the BlueKeep security vulnerability, , that potentially affects older unpatched Microsoft Windows versions via the program's Remote Desktop Protocol, allowing for the possibility of remote code execution, may now include related flaws, collectively named "DejaBlue", affecting newer Windows versions (i.e., Windows 7 and all recent versions) as well. In addition, experts reported a Microsoft security vulnerability, , based on legacy code involving Microsoft CTF and ctfmon (ctfmon.exe), that affects all Windows versions from the older Windows XP version to the most recent Windows 10 versions; a patch to correct the flaw is currently available.

All Windows versions from Windows NT 3 have been based on a file system permission system referred to as AGDLP (Accounts, Global, Domain Local, Permissions) in which file permissions are applied to the file/folder in the form of a 'local group' which then has other 'global groups' as members. These global groups then hold other groups or users depending on different Windows versions used. This system varies from other vendor products such as Linux and NetWare due to the 'static' allocation of permission being applied directly to the file or folder. However using this process of AGLP/AGDLP/AGUDLP allows a small number of static permissions to be applied and allows for easy changes to the account groups without reapplying the file permissions on the files and folders.

Owing to the operating system's popularity, a number of applications have been released that aim to provide compatibility with Windows applications, either as a compatibility layer for another operating system, or as a standalone system that can run software written for Windows out of the box. These include:





</doc>
<doc id="18892" url="https://en.wikipedia.org/wiki?curid=18892" title="Mojo (African-American culture)">
Mojo (African-American culture)

Mojo , in the African-American folk belief called hoodoo, is an amulet consisting of a flannel bag containing one or more magical items. It is a "prayer in a bag", or a spell that can be carried with or on the host's body.

Alternative American names for the mojo bag include hand, mojo hand, conjure hand, lucky hand, conjure bag, trick bag, root bag, toby, jomo, and gris-gris bag.

The term "mojo" is now commonly used in the English language to mean one's personal talent or gift. For example, a person might say that they are "getting their mojo on" when trying to get the attention of a possible mate.

The most common synonym for the word mojo is "gris-gris", which literally means "fetish" or "charm"; thus a gris-gris bag is a charm bag. In the Caribbean, an almost identical African-derived bag is called a "wanga" or "oanga" bag, but that term is uncommon in the United States. The word "conjure" is an ancient alternative to "hoodoo", which is a direct variation of African-American folklore. Because of this, a conjure hand is also considered a hoodoo bag, usually made by a respected community conjure doctor.

The word "hand" in this context is defined as a combination of ingredients. The term may derive from the use of finger and hand bones from the dead in mojo bags, or from ingredients such as the lucky hand root (favored by gamblers). The latter suggests an analogy between the varied bag ingredients and the several cards that make up a hand in card games. Mojo reaches as far back as West African culture, where it is said to drive away evil spirits, keep good luck in the household, manipulate a fortune, and lure and persuade lovers. The ideology of the ancestors and the descendants of the mojo hand used this "prayer in a bag" based on their belief of spiritual inheritance, by which the omniscient forefathers of their families would provide protection and favor, especially when they used the mojo. Through this, a strong belief was placed in the idealism of whomever used mojo, creating a spiritual trust in the magic itself.

Although most Southern-style conjure bags are made of red flannel material, most seasoned conjurers use color symbolism. This practice embodies itself in the practice of hoodoo, in which green flannel is used for a money mojo, white flannel is used for a baby-blessing mojo, red flannel is used for love mojo, and so on. West Indians also use mojo bags but often use leather instead of flannel.

The contents of each bag vary directly with the aim of the conjurer. For example, a mojo carried for love-drawing will contain different ingredients than one for gambling luck or magical protection. Ingredients can include roots, herbs, animal parts, minerals, coins, crystals, good luck tokens, and carved amulets. The more personalized objects are used to add extra power because of their symbolic value.

There is a process to fixing a proper mojo. A ritual must be put in place in order to successfully prepare a mojo by being filled and awakened to life. This can be done by smoking incense and candles, or it may be breathed upon to bring it to life. Prayers may be said, and other methods may be used to accomplish this essential step. Once prepared, the mojo is "dressed" or "fed" with a liquid such as alcohol, perfume, water, or bodily fluids. The reason it is said to feed the mojo to keep it working is that it is alive with spirit. One story from the work entitled "From My People" describes a slave who went out and sought a mojo conjurer that gave him a mojo to run away from home. The story describes the slave's mojo as fixing him into many formations, and he ultimately dies because he misuses its power. Had he fixed and believed in the specific mojo himself, he might have escaped the plantation alive.

Mojos are traditionally made for an individual and so must be concealed on the person at all times. Men usually keep the trinkets hidden in the pants pocket, while women are more prone to clip it to the bra. They are also commonly pinned to clothes below the waist. Depending on the type of mojo, the hiding place will be crucial to its success, as those who make conjure bags to carry love spells sometimes specify that the mojo must be worn next to the skin. A story from the book "From My People" described the story of Moses and the task he went through to bring his people out of slavery. It described how "Hoodoo Lost his Hand", as Moses's mojo was hidden through his staff. When he turned it into a snake, the pharaoh made his soothsayers and magicians create the same effect. As a result, the Pharaoh's snake was killed by Moses's snake, and that is how Hoodoo lost his hand.



</doc>
<doc id="18894" url="https://en.wikipedia.org/wiki?curid=18894" title="Matt Groening">
Matt Groening

Matthew Abram Groening ( ; born February 15, 1954) is an American cartoonist, writer, producer, and animator. He is the creator of the comic strip "Life in Hell" (1977–2012) and the television series "The Simpsons" (1989–present), "Futurama" (1999–2003, 2008–2013), and "Disenchantment" (2018–present). "The Simpsons" is the longest-running U.S. primetime-television series in history and the longest-running U.S. animated series and sitcom.

Groening made his first professional cartoon sale of "Life in Hell" to the avant-garde "Wet" magazine in 1978. At its peak, the cartoon was carried in 250 weekly newspapers. "Life in Hell" caught the attention of James L. Brooks. In 1985, Brooks contacted Groening with the proposition of working in animation for the Fox variety show "The Tracey Ullman Show". Originally, Brooks wanted Groening to adapt his "Life in Hell" characters for the show. Fearing the loss of ownership rights, Groening decided to create something new and came up with a cartoon family, the Simpson family, and named the members after his own parents and sisters—while Bart was an anagram of the word "brat". The shorts would be spun off into their own series "The Simpsons", which has since aired episodes. In 1997, Groening and former "Simpsons" writer David X. Cohen developed "Futurama", an animated series about life in the year 3000, which premiered in 1999, running for four years on Fox, then picked up by Comedy Central for additional seasons. In 2016, Groening developed a new series for Netflix titled "Disenchantment", which premiered in August 2018.

Groening has won thirteen Primetime Emmy Awards, eleven for "The Simpsons" and two for "Futurama" as well as a British Comedy Award for "outstanding contribution to comedy" in 2004. In 2002, he won the National Cartoonist Society Reuben Award for his work on "Life in Hell". He received a star on the Hollywood Walk of Fame on February 14, 2012.

Groening was born on February 15, 1954 in Portland, Oregon, the middle of five children (older brother Mark and sister Patty were born in 1950 and 1952, while the younger sisters Lisa and Maggie in 1956 and 1958, respectively). His Norwegian American mother, Margaret Ruth (née Wiggum; March 23, 1919 – April 22, 2013), was once a teacher, and his German Canadian father, Homer Philip Groening (December 30, 1919 – March 15, 1996), was a filmmaker, advertiser, writer and cartoonist. Homer, born in Main Centre, Saskatchewan, Canada, grew up in a Mennonite, Plautdietsch (German)-speaking family.

Matt's grandfather, Abraham Groening, was a professor at Tabor College, a Mennonite Brethren liberal arts college in Hillsboro, Kansas before moving to Albany College (now known as Lewis and Clark College) in Oregon in 1930.

Groening grew up in Portland, and attended Ainsworth Elementary School and Lincoln High School. From 1972 to 1977, Groening attended The Evergreen State College in Olympia, Washington, a liberal arts school that he described as "a hippie college, with no grades or required classes, that drew every weirdo in the Northwest." He served as the editor of the campus newspaper, "The Cooper Point Journal", for which he also wrote articles and drew cartoons. He befriended fellow cartoonist Lynda Barry after discovering that she had written a fan letter to Joseph Heller, one of Groening's favorite authors, and had received a reply. Groening has credited Barry with being "probably [his] biggest inspiration." He first became interested in cartoons after watching the Disney animated film "One Hundred and One Dalmatians", and he has also cited Robert Crumb, Ernie Bushmiller, Ronald Searle, Monty Python, and Charles M. Schulz as inspirations.

In 1977, at the age of 23, Groening moved to Los Angeles to become a writer. He went through what he described as "a series of lousy jobs," including being an extra in the television movie "When Every Day Was the Fourth of July", busing tables, washing dishes at a nursing home, clerking at the Hollywood Licorice Pizza record store, landscaping in a sewage treatment plant, and chauffeuring and ghostwriting for a retired Western director.

Groening described life in Los Angeles to his friends in the form of the self-published comic book "Life in Hell", which was loosely inspired by the chapter "How to Go to Hell" in Walter Kaufmann's book "Critique of Religion and Philosophy". Groening distributed the comic book in the book corner of Licorice Pizza, a record store in which he worked. He made his first professional cartoon sale to the avant-garde "Wet" magazine in 1978. The strip, titled "Forbidden Words," appeared in the September/October issue of that year.

Groening had gained employment at the "Los Angeles Reader", a newly formed alternative newspaper, delivering papers, typesetting, editing and answering phones. He showed his cartoons to the editor, James Vowell, who was impressed and eventually gave him a spot in the paper. "Life in Hell" made its official debut as a comic strip in the "Reader" on April 25, 1980. Vowell also gave Groening his own weekly music column, "Sound Mix," in 1982. However, the column would rarely actually be about music, as he would often write about his "various enthusiasms, obsessions, pet peeves and problems" instead. In an effort to add more music to the column, he "just made stuff up," concocting and reviewing fictional bands and nonexistent records. In the following week's column, he would confess to fabricating everything in the previous column and swear that everything in the new column was true. Eventually, he was finally asked to give up the "music" column. Among the fans of the column was Harry Shearer, who would later become a voice on "The Simpsons".

"Life in Hell" became popular almost immediately. In November 1984, Deborah Caplan, Groening's then-girlfriend and co-worker at the "Reader", offered to publish "Love is Hell", a series of relationship-themed "Life in Hell" strips, in book form. Released a month later, the book was an underground success, selling 22,000 copies in its first two printings. "Work is Hell" soon followed, also published by Caplan. Soon afterward, Caplan and Groening left and put together the Life in Hell Co., which handled merchandising for "Life in Hell". Groening also started Acme Features Syndicate, which initially syndicated "Life in Hell" as well as work by Lynda Barry and John Callahan, but would eventually only syndicate "Life in Hell". At the end of its run, "Life in Hell" was carried in 250 weekly newspapers and has been anthologized in a series of books, including "School is Hell", "Childhood is Hell", "The Big Book of Hell", and "The Huge Book of Hell". Although Groening previously stated, "I'll never give up the comic strip. It's my foundation," the June 16, 2012 strip marked "Life in Hell"s conclusion. After Groening ended the strip, the Center for Cartoon Studies commissioned a poster that was presented to Groening in honor of his work. The poster contained tribute cartoons by 22 of Groening's cartoonist friends who were influenced by "Life in Hell".

"Life in Hell" caught the attention of Hollywood writer-producer and Gracie Films founder James L. Brooks, who had been shown the strip by fellow producer Polly Platt. In 1985, Brooks contacted Groening with the proposition of working in animation on an undefined future project, which would turn out to be developing a series of short animated skits, called "bumpers," for the Fox variety show "The Tracey Ullman Show". Originally, Brooks wanted Groening to adapt his "Life in Hell" characters for the show. Groening feared that he would have to give up his ownership rights, and that the show would fail and would take down his comic strip with it. Groening conceived of the idea for the Simpsons in the lobby of James L. Brooks's office and hurriedly sketched out his version of a dysfunctional family: Homer, the overweight father; Marge, the slim mother; Bart, the bratty oldest child; Lisa, the intelligent middle child; and Maggie, the baby. Groening famously named the main Simpson characters after members of his own family: his parents, Homer and Marge (Margaret or Marjorie in full), and his younger sisters, Lisa and Margaret (Maggie). Claiming that it was a bit too obvious to name a character after himself, he chose the name "Bart," an anagram of brat. However, he stresses that aside from some of the sibling rivalry, his family is nothing like the Simpsons. Groening also has an older brother and sister, Mark and Patty, and in a 1995 interview Groening divulged that Mark "is the actual inspiration for Bart."

Maggie Groening has co-written a few "Simpsons" books featuring her cartoon namesake.

The family was crudely drawn, because Groening had submitted basic sketches to the animators, assuming they would clean them up; instead, they just traced over his drawings. The entire Simpson family was designed so that they would be recognizable in silhouette. When Groening originally designed Homer, he put his own initials into the character's hairline and ear: the hairline resembled an 'M', and the right ear resembled a 'G'. Groening decided that this would be too distracting though, and redesigned the ear to look normal. He still draws the ear as a 'G' when he draws pictures of Homer for fans. Marge's distinct beehive hairstyle was inspired by "Bride of Frankenstein" and the style that Margaret Groening wore during the 1960s, although her hair was never blue. Bart's original design, which appeared in the first shorts, had spikier hair, and the spikes were of different lengths. The number was later limited to nine spikes, all of the same size. At the time Groening was primarily drawing in black and "not thinking that [Bart] would eventually be drawn in color" gave him spikes that appear to be an extension of his head. Lisa's physical features are generally not used in other characters; for example, in the later seasons, no character other than Maggie shares her hairline. While designing Lisa, Groening "couldn't be bothered to even think about girls' hair styles". When designing Lisa and Maggie, he "just gave them this kind of spiky starfish hair style, not thinking that they would eventually be drawn in color". Groening storyboarded and scripted every short (now known as "The Simpsons shorts"), which were then animated by a team including David Silverman and Wes Archer, both of whom would later become directors on the series.

The Simpsons shorts first appeared in "The Tracey Ullman Show" on April 19, 1987. Another family member, Grampa Simpson, was introduced in the later shorts. Years later, during the early seasons of "The Simpsons", when it came time to give Grampa a first name, Groening says he refused to name him after his own grandfather, Abraham Groening, leaving it to other writers to choose a name. By coincidence, they chose "Abraham", unaware that it was the name of Groening's grandfather.

Although "The Tracey Ullman Show" was not a big hit, the popularity of the shorts led to a half-hour spin-off in 1989. A team of production companies adapted "The Simpsons" into a half-hour series for the Fox Broadcasting Company. The team included what is now the Klasky Csupo animation house. James L. Brooks negotiated a provision in the contract with the Fox network that prevented Fox from interfering with the show's content. Groening said his goal in creating the show was to offer the audience an alternative to what he called "the mainstream trash" that they were watching. The half-hour series premiered on December 17, 1989 with "Simpsons Roasting on an Open Fire", a Christmas special. "Some Enchanted Evening" was the first full-length episode produced, but it did not broadcast until May 1990, as the last episode of the first season, because of animation problems.

The series quickly became a worldwide phenomenon, to the surprise of many. Groening said: "Nobody thought "The Simpsons" was going to be a big hit. It sneaked up on everybody." "The Simpsons" was co-developed by Groening, Brooks, and Sam Simon, a writer-producer with whom Brooks had worked on previous projects. Groening and Simon, however, did not get along and were often in conflict over the show; Groening once described their relationship as "very contentious." Simon eventually left the show in 1993 over creative differences.

Like the main family members, several characters from the show have names that were inspired by people, locations or films. The name "Wiggum" for police chief Chief Wiggum is Groening's mother's maiden name. The names of a few other characters were taken from major street names in Groening's hometown of Portland, Oregon, including Flanders, Lovejoy, Powell, Quimby and Kearney. Despite common fan belief that Sideshow Bob Terwilliger was named after SW Terwilliger Boulevard in Portland, he was actually named after the character Dr. Terwilliker from the film "The 5,000 Fingers of Dr. T".

Although Groening has pitched a number of spin-offs from "The Simpsons", those attempts have been unsuccessful. In 1994, Groening and other "Simpsons" producers pitched a live-action spin-off about Krusty the Clown (with Dan Castellaneta playing the lead role), but were unsuccessful in getting it off the ground. Groening has also pitched "Young Homer" and a spin-off about the non-Simpsons citizens of Springfield.

In 1995, Groening got into a major disagreement with Brooks and other "Simpsons" producers over "A Star Is Burns", a crossover episode with "The Critic", an animated show also produced by Brooks and staffed with many former "Simpsons" crew members. Groening claimed that he feared viewers would "see it as nothing but a pathetic attempt to advertise "The Critic" at the expense of "The Simpsons"," and was concerned about the possible implication that he had created or produced "The Critic". He requested his name be taken off the episode.

Groening is credited with writing or co-writing the episodes "Some Enchanted Evening", "The Telltale Head", "Colonel Homer" and "22 Short Films About Springfield", as well as "The Simpsons Movie", released in 2007. He has had several cameo appearances in the show, with a speaking role in the episode "My Big Fat Geek Wedding". He currently serves at "The Simpsons" as an executive producer and creative consultant.

After spending a few years researching science fiction, Groening got together with "Simpsons" writer/producer David X. Cohen (known as David S. Cohen at the time) in 1997 and developed "Futurama", an animated series about life in the year 3000. By the time they pitched the series to Fox in April 1998, Groening and Cohen had composed many characters and storylines; Groening claimed they had gone "overboard" in their discussions. Groening described trying to get the show on the air as "by far the worst experience of [his] grown-up life." The show premiered on March 28, 1999. Groening's writing credits for the show are for the premiere episode, "Space Pilot 3000" (co-written with Cohen), "Rebirth" (story) and "In-A-Gadda-Da-Leela" (story).

After four years on the air, the show was canceled by Fox. In a situation similar to "Family Guy", however, strong DVD sales and very stable ratings on Adult Swim brought "Futurama" back to life. When Comedy Central began negotiating for the rights to air "Futurama" reruns, Fox suggested that there was a possibility of also creating new episodes. When Comedy Central committed to sixteen new episodes, it was decided that four straight-to-DVD films – "" (2007), "" (2008), "" (2008) and "" (2009) – would be produced.

Since no new "Futurama" projects were in production, the movie "Into the Wild Green Yonder" was designed to stand as the "Futurama" series finale. However, Groening had expressed a desire to continue the "Futurama" franchise in some form, including as a theatrical film. In an interview with CNN, Groening said that "we have a great relationship with Comedy Central and we would love to do more episodes for them, but I don't know... We're having discussions and there is some enthusiasm but I can't tell if it's just me". Comedy Central commissioned an additional 26 new episodes, and began airing them in 2010. The show continued in to 2013, before Comedy Central announced in April 2013 that they would not be renewing it beyond its seventh season. The final episode aired on September 4, 2013.

On January 15, 2016, it was announced that Groening was in talks with Netflix to develop a new animated series. On July 25, 2017 the series, "Disenchantment", was ordered by Netflix. The first ten episodes premiered on the streaming service in August 2018, with the remaining ten episodes of the initial order scheduled to air in September 2019. Netflix has renewed the series for twenty additional episodes, which are expected to debut in ten-episode batches in 2020 and 2021.

Groening described the fantasy-oriented series as originating in a sketchbook full of "fantastic creatures we couldn't do on "The Simpsons"." The show's cast includes Abbi Jacobson, Eric Andre, and Nat Faxon.

In 1994, Groening formed Bongo Comics (named after the character Bongo from "Life in Hell") with Steve Vance, Cindy Vance and Bill Morrison, which publishes comic books based on "The Simpsons" and "Futurama" (including "Futurama Simpsons Infinitely Secret Crossover Crisis", a crossover between the two), as well as a few original titles. According to Groening, the goal with Bongo is to "[try] to bring humor into the fairly grim comic book market." He also formed Zongo Comics in 1995, an imprint of Bongo that published comics for more mature readers, which included three issues of Mary Fleener's "Fleener" and seven issues of his close friend Gary Panter's "Jimbo" comics.

Groening is known for his eclectic taste in music. His favorite band is Frank Zappa and The Mothers of Invention and his favorite album is "Trout Mask Replica" by Captain Beefheart (which was produced by Zappa). He guest-edited Da Capo Press's "Best Music Writing 2003" and curated a US All Tomorrow's Parties music festival in 2003. He illustrated the cover of Frank Zappa's posthumous album "" (1996). In May 2010, he curated another edition of All Tomorrow's Parties in Minehead, England. He also plays the drums in the all-author rock and roll band The Rock Bottom Remainders (although he is listed as the cowbell player), whose other members include Dave Barry, Ridley Pearson, Scott Turow, Amy Tan, James McBride, Mitch Albom, Roy Blount Jr., Stephen King, Kathi Kamen Goldmark, Sam Barry and Greg Iles. In July 2013, Groening co-authored "Hard Listening" (2013) with the rest of the Rock Bottom Remainders (published by Coliloquy, LLC).
Groening and Deborah Caplan married in 1986 and had two sons together, Homer (who goes by Will) and Abe, both of whom Groening occasionally portrays as rabbits in "Life in Hell". The couple divorced in 1999. In 2011, Groening married Argentine artist Agustina Picasso after a four-year relationship, and became stepfather to her daughter Camila Costantini. In May 2013, Picasso gave birth to Nathaniel Philip Picasso Groening, named after writer Nathanael West. She joked that "his godfather is SpongeBob's creator Stephen Hillenburg". In 2015, Groening's daughters Luna Margaret and India Mia were born. Matt is the brother-in-law of "Hey Arnold!", "Dinosaur Train" and "Ready Jet Go!" creator, Craig Bartlett, who is married to Groening's sister, Lisa. Bartlett used to appear in "Simpsons Illustrated".

On June 16, 2018, he became the father of twins for a second time when his wife gave birth to Sol Matthew and Venus Ruth, announced via Instagram.
Groening is a self-identified agnostic. He has often made campaign contributions to Democratic Party candidates. His first cousin, Laurie Monnes Anderson, is a member of the Oregon State Senate representing eastern Multnomah County.

Groening has been nominated for 41 Emmy Awards and has won thirteen, eleven for "The Simpsons" and two for "Futurama" in the "Outstanding Animated Program (for programming one hour or less)" category. Groening received the 2002 National Cartoonist Society Reuben Award, and had been nominated for the same award in 2000. He received a British Comedy Award for "outstanding contribution to comedy" in 2004. In 2007, he was ranked fourth (and highest American by birth) in a list of the "top 100 living geniuses", published by British newspaper "The Daily Telegraph".

He received the 2,459th star on the Hollywood Walk of Fame on February 14, 2012.




</doc>
<doc id="18895" url="https://en.wikipedia.org/wiki?curid=18895" title="Metaphysics">
Metaphysics

Metaphysics is the branch of philosophy that examines the fundamental nature of reality, including the relationship between mind and matter, between substance and attribute, and between potentiality and actuality. The word "metaphysics" comes from two Greek words that, together, literally mean "after or behind or among [the study of] the natural". It has been suggested that the term might have been coined by a first century AD editor who assembled various small selections of Aristotle’s works into the treatise we now know by the name "Metaphysics" ("ta meta ta phusika", 'after the "Physics" ', another of Aristotle's works).

Metaphysics studies questions related to what it is for something to exist and what types of existence there are. Metaphysics seeks to answer, in an abstract and fully general manner, the questions:

Topics of metaphysical investigation include existence, objects and their properties, space and time, cause and effect, and possibility. Metaphysics is considered one of the four main branches of philosophy, along with epistemology, logic, and ethics.

Metaphysical study is conducted using deduction from that which is known "a priori". Like foundational mathematics (which is sometimes considered a special case of metaphysics applied to the existence of number), it tries to give a coherent account of the structure of the world, capable of explaining our everyday and scientific perception of the world, and being free from contradictions. In mathematics, there are many different ways to define numbers; similarly in metaphysics there are many different ways to define objects, properties, concepts, and other entities which are claimed to make up the world. While metaphysics may, as a special case, study the entities postulated by fundamental science such as atoms and superstrings, its core topic is the set of categories such as object, property and causality which those scientific theories assume. For example: claiming that "electrons have charge" is a scientific theory; while exploring what it means for electrons to be (or at least, to be perceived as) "objects", charge to be a "property", and for both to exist in a topological entity called "space" is the task of metaphysics.

There are two broad stances about what is "the world" studied by metaphysics. The strong, classical view assumes that the objects studied by metaphysics exist independently of any observer, so that the subject is the most fundamental of all sciences. The weak, modern view assumes that the objects studied by metaphysics exist inside the mind of an observer, so the subject becomes a form of introspection and conceptual analysis. Some philosophers, notably Kant, discuss both of these "worlds" and what can be inferred about each one. Some, such as the logical positivists, and many scientists, reject the strong view of metaphysics as meaningless and unverifiable. Others reply that this criticism also applies to any type of knowledge, including hard science, which claims to describe anything other than the contents of human perception, and thus that the world of perception "is" the objective world in some sense. Metaphysics itself usually assumes that some stance has been taken on these questions and that it may proceed independently of the choice—the question of which stance to take belongs instead to another branch of philosophy, epistemology.

Ontology is the philosophical study of the nature of being, becoming, existence or reality, as well as the basic categories of being and their relations. Traditionally listed as the core of metaphysics, ontology often deals with questions concerning what entities exist and how such entities may be grouped, related within a hierarchy, and subdivided according to similarities and differences.

Identity is a fundamental metaphysical concern. Metaphysicians investigating identity are tasked with the question of what, exactly, it means for something to be identical to itself, or – more controversially – to something else. Issues of identity arise in the context of time: what does it mean for something to be itself across two moments in time? How do we account for this? Another question of identity arises when we ask what our criteria ought to be for determining identity, and how the reality of identity interfaces with linguistic expressions.

The metaphysical positions one takes on identity have far-reaching implications on issues such as the Mind–body problem, personal identity, ethics, and law.

A few ancient Greeks took extreme positions on the nature of change. Parmenides denied change altogether, while Heraclitus argued that change was ubiquitous: "No man ever steps in the same river twice."

Identity, sometimes called numerical identity, is the relation that a thing bears to itself, and which no thing bears to anything other than itself (cf. sameness).

A modern philosopher who made a lasting impact on the philosophy of identity was Leibniz, whose "Law of the Indiscernibility of Identicals" is still widely accepted today. It states that if some object "x" is identical to some object "y", then any property that "x" has, "y" will have as well.

Put formally, it states

However, it does seem that objects can change over time. If one were to look at a tree one day, and the tree later lost a leaf, it would seem that one could still be looking at that same tree. Two rival theories to account for the relationship between change and identity are perdurantism, which treats the tree as a series of tree-stages, and endurantism, which maintains that the organism—the same tree—is present at every stage in its history.

By appealing to intrinsic and extrinsic properties, endurantism finds a way to harmonize identity with change. Endurantists believe that objects persist by being strictly numerically identical over time. However, if Leibniz's Law of the Indiscernibility of Identicals is utilized to define numerical identity here, it seems that objects must be completely unchanged in order to persist. Discriminating between intrinsic properties and extrinsic properties, endurantists state that numerical identity means that, if some object "x" is identical to some object "y", then any "intrinsic" property that "x" has, "y" will have as well. Thus, if an object persists, "intrinsic" properties of it are unchanged, but "extrinsic" properties can change over time. Besides the object itself, environments and other objects can change over time; properties that relate to other objects would change even if this object does not change.

Perdurantism can harmonize identity with change in another way. In four-dimensionalism, a version of perdurantism, what persists is a four-dimensional object which does not change although three-dimensional slices of the object may differ.

Objects appear to us in space and time, while abstract entities such as classes, properties, and relations do not. How do space and time serve this function as a ground for objects? Are space and time entities themselves, of some form? Must they exist prior to objects? How exactly can they be defined? How is time related to change; must there always be something changing in order for time to exist?

Classical philosophy recognized a number of causes, including teleological future causes. In special relativity and quantum field theory the notions of space, time and causality become tangled together, with temporal orders of causations becoming dependent on who is observing them. The laws of physics are symmetrical in time, so could equally well be used to describe time as running backwards. Why then do we perceive it as flowing in one direction, the arrow of time, and as containing causation flowing in the same direction?

For that matter, can an effect precede its cause? This was the title of a 1954 paper by Michael Dummett, which sparked a discussion that continues today. Earlier, in 1947, C. S. Lewis had argued that one can meaningfully pray concerning the outcome of, e.g., a medical test while recognizing that the outcome is determined by past events: "My free act contributes to the cosmic shape." Likewise, some interpretations of quantum mechanics, dating to 1945, involve backward-in-time causal influences.

Causality is linked by many philosophers to the concept of counterfactuals. To say that A caused B means that if A had not happened then B would not have happened. This view was advanced by David Lewis in his 1973 paper "Causation". His subsequent papers further develop his theory of causation.

Causality is usually required as a foundation for philosophy of science, if science aims to understand causes and effects and make predictions about them.

Metaphysicians investigate questions about the ways the world could have been. David Lewis, in "On the Plurality of Worlds", endorsed a view called Concrete Modal realism, according to which facts about how things could have been are made true by other concrete worlds in which things are different. Other philosophers, including Gottfried Leibniz, have dealt with the idea of possible worlds as well. A necessary fact is true across all possible worlds. A possible fact is true in some possible world, even if not in the actual world. For example, it is possible that cats could have had two tails, or that any particular apple could have not existed. By contrast, certain propositions seem necessarily true, such as analytic propositions, e.g., "All bachelors are unmarried." The view that any analytic truth is necessary is not universally held among philosophers. A less controversial view is that self-identity is necessary, as it seems fundamentally incoherent to claim that any "x" is not identical to itself; this is known as the law of identity, a putative "first principle". Similarly, Aristotle describes the principle of non-contradiction: 

What is "central" and "peripheral" to metaphysics has varied over time and schools; however contemporary analytic philosophy as taught in USA and UK universities generally regards the above as "central" and the following as "applications" or "peripheral" topics; or in some cases as distinct subjects which have grown out of and depend upon metaphysics:

Metaphysical cosmology is the branch of metaphysics that deals with the world as the totality of all phenomena in space and time. Historically, it formed a major part of the subject alongside Ontology, though its role is more peripheral in contemporary philosophy. It has had a broad scope, and in many cases was founded in religion. The ancient Greeks drew no distinction between this use and their model for the cosmos. However, in modern times it addresses questions about the Universe which are beyond the scope of the physical sciences. It is distinguished from religious cosmology in that it approaches these questions using philosophical methods (e.g. dialectics).

Cosmogony deals specifically with the origin of the universe. Modern metaphysical cosmology and cosmogony try to address questions such as:

Accounting for the existence of mind in a world largely composed of matter is a metaphysical problem which is so large and important as to have become a specialized subject of study in its own right, philosophy of mind.

Substance dualism is a classical theory in which mind and body are essentially different, with the mind having some of the attributes traditionally assigned to the soul, and which creates an immediate conceptual puzzle about how the two interact. This form of substance dualism differs from the dualism of some eastern philosophical traditions (like Nyāya), which also posit a soul; for the soul, under their view, is ontologically distinct from the mind. Idealism postulates that material objects do not exist unless perceived and only as perceptions. Adherents of panpsychism, a kind of property dualism, hold that everything "has" a mental aspect, but not that everything exists "in" a mind. Neutral monism postulates that existence consists of a single substance that in itself is neither mental nor physical, but is capable of mental and physical aspects or attributesthus it implies a dual-aspect theory. For the last century, the dominant theories have been science-inspired including materialistic monism, type identity theory, token identity theory, functionalism, reductive physicalism, nonreductive physicalism, eliminative materialism, anomalous monism, property dualism, epiphenomenalism and emergence.

Determinism is the philosophical proposition that every event, including human cognition, decision and action, is causally determined by an unbroken chain of prior occurrences. It holds that nothing happens that has not already been determined. The principal consequence of the deterministic claim is that it poses a challenge to the existence of free will.

The problem of free will is the problem of whether rational agents exercise control over their own actions and decisions. Addressing this problem requires understanding the relation between freedom and causation, and determining whether the laws of nature are causally deterministic. Some philosophers, known as incompatibilists, view determinism and free will as mutually exclusive. If they believe in determinism, they will therefore believe free will to be an illusion, a position known as "Hard Determinism". Proponents range from Baruch Spinoza to Ted Honderich. Henri Bergson defended free will in his dissertation "Time and Free Will" from 1889.

Others, labeled compatibilists (or "soft determinists"), believe that the two ideas can be reconciled coherently. Adherents of this view include Thomas Hobbes and many modern philosophers such as John Martin Fischer, Gary Watson, Harry Frankfurt, and the like.

Incompatibilists who accept free will but reject determinism are called libertarians, a term not to be confused with the political sense. Robert Kane and Alvin Plantinga are modern defenders of this theory.

The earliest type of classification of social construction traces back to Plato in his dialogue Phaedrus where he claims that the biological classification system seems to carve nature at the joints. In contrast, later philosophers such as Michel Foucault and Jorge Luis Borges have challenged the capacity of natural and social classification. In his essay The Analytical Language of John Wilkins, Borges makes us imagine a certain encyclopedia where the animals are divided into (a) those that belong to the emperor; (b) embalmed ones; (c) those that are trained;... and so forth, in order to bring forward the ambiguity of natural and social kinds. According to metaphysics author Alyssa Ney: "the reason all this is interesting is that there seems to be a metaphysical difference between the Borgesian system and Plato's". The difference is not obvious but one classification attempts to carve entities up according to objective distinction while the other does not. According to Quine this notion is closely related to the notion of similarity.

There are different ways to set up the notion of number in metaphysics theories. Platonist theories postulate number as a fundamental category itself. Others consider it to be a property of an entity called a "group" comprising other entities; or to be a relation held between several groups of entities, such as "the number four is the set of all sets of four things". Many of the debates around universals are applied to the study of number, and are of particular importance due to its status as a foundation for the philosophy of mathematics and for mathematics itself.

Although metaphysics as a philosophical enterprise is highly hypothetical, it also has practical application in most other branches of philosophy, science, and now also information technology. Such areas generally assume some basic ontology (such as a system of objects, properties, classes, and space time) as well as other metaphysical stances on topics such as causality and agency, then build their own particular theories upon these.

In science, for example, some theories are based on the ontological assumption of objects with properties (such as electrons having charge) while others may reject objects completely (such as quantum field theories, where spread-out "electronness" becomes a property of space time rather than an object).

"Social" branches of philosophy such as philosophy of morality, aesthetics and philosophy of religion - which in turn give rise to practical subjects such as ethics, politics, law, and art - all require metaphysical foundations, which may be considered as branches or applications of metaphysics. For example, they may postulate the existence of basic entities such as value, beauty, and God. Then they use these postulates to make their own arguments about consequences resulting from them. When philosophers in these subjects make their foundations they are doing applied metaphysics, and may draw upon its core topics and methods to guide them, including ontology and other core and peripheral topics. As in science, the foundations chosen will in turn depend on the underlying ontology used, so philosophers in these subjects may have to dig right down to the ontological layer of metaphysics to find what is possible for their theories. For example, a contradiction obtained in a theory of God or Beauty might be due to an assumption that it is an object rather than some other kind of ontological entity.

Prior to the modern history of science, scientific questions were addressed as a part of natural philosophy. Originally, the term "science" () simply meant "knowledge". The scientific method, however, transformed natural philosophy into an empirical activity deriving from experiment, unlike the rest of philosophy. By the end of the 18th century, it had begun to be called "science" to distinguish it from other branches of philosophy. Science and philosophy have been considered separated disciplines ever since. Thereafter, metaphysics denoted philosophical enquiry of a non-empirical character into the nature of existence.

Metaphysics continues asking "why" where science leaves off. For example, any theory of fundamental physics is based on some set of axioms, which may postulate the existence of entities such as atoms, particles, forces, charges, mass, or fields. Stating such postulates is considered to be the "end" of a science theory. Metaphysics takes these postulates and explores what they mean as human concepts. For example, do all theories of physics require the existence of space and time, objects, and properties? Or can they be expressed using only objects, or only properties? Do the objects have to retain their identity over time or can they change? If they change, then are they still the same object? Can theories be reformulated by converting properties or predicates (such as "red") into entities (such as redness or redness fields) or processes ('there is some redding happening over there' appears in some human languages in place of the use of properties). Is the distinction between objects and properties fundamental to the physical world or to our perception of it?

Much recent work has been devoted to analyzing the role of metaphysics in scientific theorizing. Alexandre Koyré led this movement, declaring in his book "Metaphysics and Measurement", "It is not by following experiment, but by outstripping experiment, that the scientific mind makes progress." That metaphysical propositions can influence scientific theorizing is John Watkins' most lasting contribution to philosophy. Since 1957 "he showed the ways in which some un-testable and hence, according to Popperian ideas, non-empirical propositions can nevertheless be influential in the development of properly testable and hence scientific theories. These profound results in applied elementary logic...represented an important corrective to positivist teachings about the meaninglessness of metaphysics and of normative claims". Imre Lakatos maintained that all scientific theories have a metaphysical "hard core" essential for the generation of hypotheses and theoretical assumptions. Thus, according to Lakatos, "scientific changes are connected with vast cataclysmic metaphysical revolutions."

An example from biology of Lakatos' thesis: David Hull has argued that changes in the ontological status of the species concept have been central in the development of biological thought from Aristotle through Cuvier, Lamarck, and Darwin. Darwin's ignorance of metaphysics made it more difficult for him to respond to his critics because he could not readily grasp the ways in which their underlying metaphysical views differed from his own.

In physics, new metaphysical ideas have arisen in connection with quantum mechanics, where subatomic particles arguably do not have the same sort of individuality as the particulars with which philosophy has traditionally been concerned. Also, adherence to a deterministic metaphysics in the face of the challenge posed by the quantum-mechanical uncertainty principle led physicists such as Albert Einstein to propose alternative theories that retained determinism. A.N. Whitehead is famous for creating a process philosophy metaphysics inspired by electromagnetism and special relativity.

In chemistry, Gilbert Newton Lewis addressed the nature of motion, arguing that an electron should not be said to move when it has none of the properties of motion.

Katherine Hawley notes that the metaphysics even of a widely accepted scientific theory may be challenged if it can be argued that the metaphysical presuppositions of the theory make no contribution to its predictive success.

Metametaphysics is the branch of philosophy that is concerned with the foundations of metaphysics. A number of individuals have suggested that much or all of metaphysics should be rejected, a metametaphysical position known as metaphysical deflationism or ontological deflationism.

In the 16th century, Francis Bacon rejected scholastic metaphysics, and argued strongly for what is now called empiricism, being seen later as the father of modern empirical science. In the 18th century, David Hume took a strong position, arguing that all genuine knowledge involves either mathematics or matters of fact and that metaphysics, which goes beyond these, is worthless. He concludes his "Enquiry Concerning Human Understanding" with the statement:

If we take in our hand any volume; of divinity or school metaphysics, for instance; let us ask, "Does it contain any abstract reasoning concerning quantity or number?" No. "Does it contain any experimental reasoning concerning matter of fact and existence?" No. Commit it then to the flames: for it can contain nothing but sophistry and illusion.

Thirty-three years after Hume's "Enquiry" appeared, Immanuel Kant published his "Critique of Pure Reason". Although he followed Hume in rejecting much of previous metaphysics, he argued that there was still room for some synthetic "a priori" knowledge, concerned with matters of fact yet obtainable independent of experience. These included fundamental structures of space, time, and causality. He also argued for the freedom of the will and the existence of "things in themselves", the ultimate (but unknowable) objects of experience.

Wittgenstein introduced the concept that metaphysics could be influenced by theories of aesthetics, via logic, vis. a world composed of "atomical facts".

In the 1930s, A.J. Ayer and Rudolf Carnap endorsed Hume's position; Carnap quoted the passage above. They argued that metaphysical statements are neither true nor false but meaningless since, according to their verifiability theory of meaning, a statement is meaningful only if there can be empirical evidence for or against it. Thus, while Ayer rejected the monism of Spinoza, he avoided a commitment to pluralism, the contrary position, by holding both views to be without meaning. Carnap took a similar line with the controversy over the reality of the external world. While the logical positivism movement is now considered dead (with Ayer, a major proponent, admitting in a 1979 TV interview that "nearly all of it was false"), it has continued to influence philosophy development.

Arguing against such rejections, the Scholastic philosopher Edward Feser held that Hume's critique of metaphysics, and specifically Hume's fork, is "notoriously self-refuting". Feser argues that Hume's fork itself is not a conceptual truth and is not empirically testable.

Some living philosophers, such as Amie Thomasson, have argued that many metaphysical questions can be dissolved just by looking at the way we use words; others, such as Ted Sider, have argued that metaphysical questions are substantive, and that we can make progress toward answering them by comparing theories according to a range of theoretical virtues inspired by the sciences, such as simplicity and explanatory power.

The word "metaphysics" derives from the Greek words μετά ("metá", "after") and φυσικά ("physiká", "physics"). It was first used as the title for several of Aristotle's works, because they were usually anthologized after the works on physics in complete editions. The prefix "meta-" ("after") indicates that these works come "after" the chapters on physics. However, Aristotle himself did not call the subject of these books metaphysics: he referred to it as "first philosophy" (; ). The editor of Aristotle's works, Andronicus of Rhodes, is thought to have placed the books on first philosophy right after another work, "Physics", and called them ("tà metà tà physikà biblía") or "the books [that come] after the [books on] physics".

However, once the name was given, the commentators sought to find other reasons for its appropriateness. For instance, Thomas Aquinas understood it to refer to the chronological or pedagogical order among our philosophical studies, so that the "metaphysical sciences" would mean "those that we study after having mastered the sciences that deal with the physical world".

The term was misread by other medieval commentators, who thought it meant "the science of what is beyond the physical". Following this tradition, the prefix "meta-" has more recently been prefixed to the names of sciences to designate higher sciences dealing with ulterior and more fundamental problems: hence metamathematics, , etc.

A person who creates or develops metaphysical theories is called a "metaphysician".

Common parlance also uses the word "metaphysics" for a different referent from that of the present article, namely for beliefs in arbitrary non-physical or magical entities. For example, "Metaphysical healing" to refer to healing by means of remedies that are magical rather than scientific. This usage stemmed from the various historical schools of speculative metaphysics which operated by postulating all manner of physical, mental and spiritual entities as bases for particular metaphysical systems. Metaphysics as a subject does not preclude beliefs in such magical entities but neither does it promote them. Rather, it is the subject which provides the vocabulary and logic with which such beliefs might be analyzed and studied, for example to search for inconsistencies both within themselves and with other accepted systems such as Science.

Cognitive archeology such as analysis of cave paintings and other pre-historic art and customs suggests that a form of perennial philosophy or Shamanic metaphysics may stretch back to the birth of behavioral modernity, all around the world. Similar beliefs are found in present-day "stone age" cultures such as Australian aboriginals. Perennial philosophy postulates the existence of a spirit or concept world alongside the day-to-day world, and interactions between these worlds during dreaming and ritual, or on special days or at special places. It has been argued that perennial philosophy formed the basis for Platonism, with Plato articulating, rather than creating, much older widespread beliefs.

Bronze Age cultures such as ancient Mesopotamia and ancient Egypt (along with similarly structured but chronologically later cultures such as Mayans and Aztecs) developed belief systems based on mythology, anthropomorphic gods, mind–body dualism, and a spirit world, to explain causes and cosmology. These cultures appear to have been interested in astronomy and may have associated or identified the stars with some of these entities. In ancient Egypt, the ontological distinction between order (maat) and chaos (Isfet) seems to have been important.

The first named Greek philosopher, according to Aristotle, is Thales of Miletus, early 6th century BCE. He made use of purely physical explanations to explain the phenomena of the world rather than the mythological and divine explanations of tradition. He is thought to have posited water as the single underlying principle (or "Arche in later Aristotelian terminology) of the material world". His fellow, but younger Miletians, Anaximander and Anaximenes, also posited monistic underlying principles, namely apeiron (the indefinite or boundless) and air respectively.

Another school was the Eleatics, in southern Italy. The group was founded in the early fifth century BCE by Parmenides, and included Zeno of Elea and Melissus of Samos. Methodologically, the Eleatics were broadly rationalist, and took logical standards of clarity and necessity to be the criteria of truth. Parmenides' chief doctrine was that reality is a single unchanging and universal Being. Zeno used "reductio ad absurdum", to demonstrate the illusory nature of change and time in his paradoxes.

Heraclitus of Ephesus, in contrast, made change central, teaching that "all things flow". His philosophy, expressed in brief aphorisms, is quite cryptic. For instance, he also taught the unity of opposites.

Democritus and his teacher Leucippus, are known for formulating an atomic theory for the cosmos. They are considered forerunners of the scientific method.

Metaphysics in Chinese philosophy can be traced back to the earliest Chinese philosophical concepts from the Zhou Dynasty such as Tian (Heaven) and Yin and Yang. The fourth century BCE saw a turn towards cosmogony with the rise of Taoism (in the Daodejing and Zhuangzi) and sees the natural world as dynamic and constantly changing processes which spontaneously arise from a single immanent metaphysical source or principle (Tao). Another philosophical school which arose around this time was the School of Naturalists which saw the ultimate metaphysical principle as the Taiji, the "supreme polarity" composed of the forces of Yin and Yang which were always in a state of change seeking balance. Another concern of Chinese metaphysics, especially Taoism, is the relationship and nature of Being and non-Being (you 有 and wu 無). The Taoists held that the ultimate, the Tao, was also non-being or no-presence. Other important concepts were those of spontaneous generation or natural vitality (Ziran) and "correlative resonance" (Ganying).

After the fall of the Han Dynasty (220 CE), China saw the rise of the Neo-Taoist Xuanxue school. This school was very influential in developing the concepts of later Chinese metaphysics. Buddhist philosophy entered China (c. 1st century) and was influenced by the native Chinese metaphysical concepts to develop new theories. The native Tiantai and Huayen schools of philosophy maintained and reinterpreted the Indian theories of shunyata (emptiness, kong 空) and Buddha-nature (Fo xing 佛性) into the theory of interpenetration of phenomena. Neo-Confucians like Zhang Zai under the influence of other schools developed the concepts of "principle" (li) and vital energy (qi).

Socrates is known for his dialectic or questioning approach to philosophy rather than a positive metaphysical doctrine.

His pupil, Plato is famous for his theory of forms (which he places in the mouth of Socrates in his dialogues). Platonic realism (also considered a form of idealism) is considered to be a solution to the problem of universals; i.e., what particular objects have in common is that they share a specific Form which is universal to all others of their respective kind.

The theory has a number of other aspects:

Platonism developed into Neoplatonism, a philosophy with a monotheistic and mystical flavour that survived well into the early Christian era.

Plato's pupil Aristotle wrote widely on almost every subject, including metaphysics. His solution to the problem of universals contrasts with Plato's. Whereas Platonic Forms are existentially apparent in the visible world, Aristotelian essences dwell in particulars.

Potentiality and Actuality are principles of a dichotomy which Aristotle used throughout his philosophical works to analyze motion, causality and other issues.

The Aristotelian theory of change and causality stretches to four causes: the material, formal, efficient and final. The efficient cause corresponds to what is now known as a cause "simplicity". Final causes are explicitly teleological, a concept now regarded as controversial in science. The Matter/Form dichotomy was to become highly influential in later philosophy as the substance/essence distinction.

The opening arguments in Aristotle's "Metaphysics", Book I, revolve around the senses, knowledge, experience, theory, and wisdom. The first main focus in the Metaphysics is attempting to determine how intellect "advances from sensation through memory, experience, and art, to theoretical knowledge". Aristotle claims that eyesight provides us with the capability to recognize and remember experiences, while sound allows us to learn.

"More on Indian philosophy: Hindu philosophy"

"Sāṃkhya" is an ancient system of Indian philosophy based on a dualism involving the ultimate principles of consciousness and matter. It is described as the rationalist school of Indian philosophy. It is most related to the Yoga school of Hinduism, and its method was most influential on the development of Early Buddhism.

The Sāmkhya is an enumerationist philosophy whose epistemology accepts three of six pramanas (proofs) as the only reliable means of gaining knowledge. These include "pratyakṣa" (perception), "anumāṇa" (inference) and "śabda" ("āptavacana", word/testimony of reliable sources).

Samkhya is strongly dualist. Sāmkhya philosophy regards the universe as consisting of two realities; puruṣa (consciousness) and prakṛti (matter). Jiva (a living being) is that state in which puruṣa is bonded to prakṛti in some form. This fusion, state the Samkhya scholars, led to the emergence of "buddhi" ("spiritual awareness") and "ahaṅkāra" (ego consciousness). The universe is described by this school as one created by purusa-prakṛti entities infused with various permutations and combinations of variously enumerated elements, senses, feelings, activity and mind. During the state of imbalance, one of more constituents overwhelm the others, creating a form of bondage, particularly of the mind. The end of this imbalance, bondage is called liberation, or moksha, by the Samkhya school.

The existence of God or supreme being is not directly asserted, nor considered relevant by the Samkhya philosophers. Sāṃkhya denies the final cause of Ishvara (God). While the Samkhya school considers the Vedas as a reliable source of knowledge, it is an atheistic philosophy according to Paul Deussen and other scholars. A key difference between Samkhya and Yoga schools, state scholars, is that Yoga school accepts a "personal, yet essentially inactive, deity" or "personal god".

Samkhya is known for its theory of guṇas (qualities, innate tendencies). Guṇa, it states, are of three types: "sattva" being good, compassionate, illuminating, positive, and constructive; "rajas" is one of activity, chaotic, passion, impulsive, potentially good or bad; and "tamas" being the quality of darkness, ignorance, destructive, lethargic, negative. Everything, all life forms and human beings, state Samkhya scholars, have these three guṇas, but in different proportions. The interplay of these guṇas defines the character of someone or something, of nature and determines the progress of life. The Samkhya theory of guṇas was widely discussed, developed and refined by various schools of Indian philosophies, including Buddhism. Samkhya's philosophical treatises also influenced the development of various theories of Hindu ethics.

Realization of the nature of Self-identity is the principal object of the Vedanta system of Indian metaphysics. In the Upanishads, self-consciousness is not the first-person indexical self-awareness or the self-awareness which is self-reference without identification, and also not the self-consciousness which as a kind of desire is satisfied by another self-consciousness. It is Self-realisation; the realisation of the Self consisting of consciousness that leads all else.

The word "Self-consciousness" in the Upanishads means the knowledge about the existence and nature of Brahman. It means the consciousness of our own real being, the primary reality. Self-consciousness means Self-knowledge, the knowledge of Prajna i.e. of Prana which is Brahman. According to the Upanishads the Atman or Paramatman is phenomenally unknowable; it is the object of realisation. The Atman is unknowable in its essential nature; it is unknowable in its essential nature because it is the eternal subject who knows about everything including itself. The Atman is the knower and also the known.

Metaphysicians regard the Self either to be distinct from the Absolute or entirely identical with the Absolute. They have given form to three schools of thought – a) the "Dualistic school", b) the "Quasi-dualistic school" and c) the "Monistic school", as the result of their varying mystical experiences. Prakrti and Atman, when treated as two separate and distinct aspects form the basis of the Dualism of the Shvetashvatara Upanishad. Quasi-dualism is reflected in the Vaishnavite-monotheism of Ramanuja and the absolute Monism, in the teachings of Adi Shankara.

Self-consciousness is the Fourth state of consciousness or "Turiya", the first three being "Vaisvanara", "Taijasa" and "Prajna". These are the four states of individual consciousness.

There are three distinct stages leading to Self-realisation. The First stage is in mystically apprehending the glory of the Self within us as though we were distinct from it. The Second stage is in identifying the "I-within" with the Self, that we are in essential nature entirely identical with the pure Self. The Third stage is in realising that the Atman is Brahman, that there is no difference between the Self and the Absolute. The Fourth stage is in realising "I am the Absolute" – "Aham Brahman Asmi". The Fifth stage is in realising that Brahman is the "All" that exists, as also that which does not exist.

In Buddhist philosophy there are various metaphysical traditions that have proposed different questions about the nature of reality based on the teachings of the Buddha in the early Buddhist texts. The Buddha of the early texts does not focus on metaphysical questions but on ethical and spiritual training and in some cases, he dismisses certain metaphysical questions as unhelpful and indeterminate Avyakta, which he recommends should be set aside. The development of systematic metaphysics arose after the Buddha's death with the rise of the Abhidharma traditions. The Buddhist Abhidharma schools developed their analysis of reality based on the concept of "dharmas" which are the ultimate physical and mental events that make up experience and their relations to each other. Noa Ronkin has called their approach "phenomenological".

Later philosophical traditions include the Madhyamika school of Nagarjuna, which further developed the theory of the emptiness (shunyata) of all phenomena or dharmas which rejects any kind of substance. This has been interpreted as a form of anti-foundationalism and anti-realism which sees reality as having no ultimate essence or ground. The Yogacara school meanwhile promoted a theory called "awareness only" (vijnapti-matra) which has been interpreted as a form of Idealism or Phenomenology and denies the split between awareness itself and the objects of awareness.

Major ideas in Sufi metaphysics have surrounded the concept of weḥdah (وحدة) meaning "unity", or in Arabic توحيد tawhid. waḥdat al-wujūd literally means the "Unity of Existence" or "Unity of Being." The phrase has been translated "pantheism." Wujud (i.e. existence or presence) here refers to Allah's wujud (compare tawhid). On the other hand, waḥdat ash-shuhūd, meaning "Apparentism" or "Monotheism of Witness", holds that God and his creation are entirely separate.

"More on medieval philosophy and metaphysics: Medieval Philosophy"

Between about 1100 and 1500, philosophy as a discipline took place as part of the Catholic church's teaching system, known as scholasticism. Scholastic philosophy took place within an established
framework blending Christian theology with Aristotelian teachings. Although fundamental orthodoxies were not commonly challenged, there were nonetheless deep metaphysical disagreements, particularly over the problem of universals, which engaged Duns Scotus and Pierre Abelard. William of Ockham is remembered for his principle of ontological parsimony.

In the early modern period (17th and 18th centuries), the system-building "scope" of philosophy is often linked to the rationalist "method" of philosophy, that is the technique of deducing the nature of the world by pure reason. The scholastic concepts of substance and accident were employed.

Christian Wolff had theoretical philosophy divided into an ontology or "philosophia prima" as a general metaphysics, which arises as a preliminary to the distinction of the three "special metaphysics" on the soul, world and God: rational psychology, rational cosmology and rational theology. The three disciplines are called empirical and rational because they are independent of revelation. This scheme, which is the counterpart of religious tripartition in creature, creation, and Creator, is best known to philosophical students by Kant's treatment of it in the "Critique of Pure Reason". In the "Preface" of the 2nd edition of Kant's book, Wolff is defined "the greatest of all dogmatic philosophers."

British empiricism marked something of a reaction to rationalist and system-building metaphysics, or "speculative" metaphysics as it was pejoratively termed. The skeptic David Hume famously declared that most metaphysics should be consigned to the flames (see below). Hume was notorious among his contemporaries as one of the first philosophers to openly doubt religion, but is better known now for his critique of causality. John Stuart Mill, Thomas Reid and John Locke were less skeptical, embracing a more cautious style of metaphysics based on realism, common sense and science. Other philosophers, notably George Berkeley were led from empiricism to idealistic metaphysics.

Immanuel Kant attempted a grand synthesis and revision of the trends already mentioned: scholastic philosophy, systematic metaphysics, and skeptical empiricism, not to forget the burgeoning science of his day. As did the systems builders, he had an overarching framework in which all questions were to be addressed. Like Hume, who famously woke him from his 'dogmatic slumbers', he was suspicious of metaphysical speculation, and also places much emphasis on the limitations of the human mind.
Kant described his shift in metaphysics away from making claims about an objective noumenal world, towards exploring the subjective phenomenal world, as a Copernican Revolution, by analogy to (though opposite in direction to) Copernicus' shift from man (the subject) to the sun (an object) at the center of the universe.

Kant saw rationalist philosophers as aiming for a kind of metaphysical knowledge he defined as the "synthetic apriori"—that is knowledge that does not come from the senses (it is a priori) but is nonetheless about reality (synthetic). Inasmuch as it is about reality, it differs from abstract mathematical propositions (which he terms analytical apriori), and being apriori it is distinct from empirical, scientific knowledge (which he terms synthetic aposteriori). The only synthetic apriori knowledge we can have is of how our minds organise the data of the senses; that organising framework is space and time, which for Kant have no mind-independent existence, but nonetheless operate uniformly in all humans. Apriori knowledge of space and time is all that remains of metaphysics as traditionally conceived. There "is" a reality beyond sensory data or phenomena, which he calls the realm of noumena; however, we cannot know it as it is in itself, but only as it appears to us. He allows himself to speculate that the origins of phenomenal God, morality, and free will "might" exist in the noumenal realm, but these possibilities have to be set against its basic unknowability for humans. Although he saw himself as having disposed of metaphysics, in a sense, he has generally been regarded in retrospect as having a metaphysics of his own, and as beginning the modern analytical conception of the subject.

Nineteenth century philosophy was overwhelmingly influenced by Kant and his successors. Schopenhauer, Schelling, Fichte and Hegel all purveyed their own panoramic versions of German Idealism, Kant's own caution about metaphysical speculation, and refutation of idealism, having fallen by the wayside. The idealistic impulse continued into the early twentieth century with British idealists such as F.H. Bradley and J.M.E. McTaggart. Followers of Karl Marx took Hegel's dialectic view of history and re-fashioned it as materialism.

During the period when idealism was dominant in philosophy, science had been making great advances. The arrival of a new generation of scientifically minded philosophers led to a sharp decline in the popularity of idealism during the 1920s.

Analytic philosophy was spearheaded by Bertrand Russell and G.E. Moore. Russell and William James tried to compromise between idealism and materialism with the theory of neutral monism.

The early to mid twentieth century philosophy saw a trend to reject metaphysical questions as meaningless. The driving force behind this tendency was the philosophy of logical positivism as espoused by the Vienna Circle, which argued that the meaning of a statement was its prediction of observable results of an experiment, and thus that there is no need to postulate the existence of any objects other than these perceptual observations.

At around the same time, the American pragmatists were steering a middle course between materialism and idealism.
System-building metaphysics, with a fresh inspiration from science, was revived by A.N. Whitehead and Charles Hartshorne.

The forces that shaped analytic philosophy—the break with idealism, and the influence of science—were much less significant outside the English speaking world, although there was a shared turn toward language. Continental philosophy continued in a trajectory from post Kantianism.

The phenomenology of Husserl and others was intended as a collaborative project for the investigation of the features and structure of consciousness common to all humans, in line with Kant's basing his synthetic apriori on the uniform operation of consciousness. It was officially neutral with regards to ontology, but was nonetheless to spawn a number of metaphysical systems. Brentano's concept of intentionality would become widely influential, including on analytic philosophy.

Heidegger, author of "Being and Time", saw himself as re-focusing on Being-qua-being, introducing the novel concept of "Dasein" in the process. Classing himself an existentialist, Sartre wrote an extensive study of "Being and Nothingness".

The speculative realism movement marks a return to full blooded realism.

There are two fundamental aspects of everyday experience: change and persistence. Until recently, the Western philosophical tradition has arguably championed substance and persistence, with some notable exceptions, however. According to process thinkers, novelty, flux and accident do matter, and sometimes they constitute the ultimate reality.

In a broad sense, process metaphysics is as old as Western philosophy, with figures such as Heraclitus, Plotinus, Duns Scotus, Leibniz, David Hume, Georg Wilhelm Friedrich Hegel, Friedrich Wilhelm Joseph von Schelling, Gustav Theodor Fechner, Friedrich Adolf Trendelenburg, Charles Renouvier, Karl Marx, Ernst Mach, Friedrich Wilhelm Nietzsche, Émile Boutroux, Henri Bergson, Samuel Alexander and Nicolas Berdyaev. It seemingly remains an open question whether major "Continental" figures such as the late Martin Heidegger, Maurice Merleau-Ponty, Gilles Deleuze, Michel Foucault, or Jacques Derrida should be included.

In a strict sense, process metaphysics may be limited to the works of a few founding fathers: G.W.F. Hegel, Charles Sanders Peirce, William James, Henri Bergson, A.N. Whitehead, and John Dewey. From a European perspective, there was a very significant and early Whiteheadian influence on the works of outstanding scholars such as Émile Meyerson (1859–1933), Louis Couturat (1868–1914), Jean Wahl (1888–1974), Robin George Collingwood (1889–1943), Philippe Devaux (1902–1979), Hans Jonas (1903–1993), Dorothy M. Emmett (1904–2000), Maurice Merleau Ponty (1908–1961), Enzo Paci (1911–1976), Charlie Dunbar Broad (1887–1971), Wolfe Mays (1912–2005), Ilya Prigogine (1917–2003), Jules Vuillemin (1920–2001), Jean Ladrière (1921–2007), Gilles Deleuze (1925–1995), Wolfhart Pannenberg (1928–2014), and Reiner Wiehl (1929–2010).

While early analytic philosophy tended to reject metaphysical theorizing, under the influence of logical positivism, it was revived in the second half of the twentieth century. Philosophers such as David K. Lewis and David Armstrong developed elaborate theories on a range of topics such as universals, causation, possibility and necessity and abstract objects. However, the focus of analytic philosophy generally is away from the construction of all-encompassing systems and toward close analysis of individual ideas.

Among the developments that led to the revival of metaphysical theorizing were Quine's attack on the analytic–synthetic distinction, which was generally taken to undermine Carnap's distinction between existence questions internal to a framework and those external to it.

The philosophy of fiction, the problem of empty names, and the debate over existence's status as a property have all come of relative obscurity into the limelight, while perennial issues such as free will, possible worlds, and the philosophy of time have had new life breathed into them.

The analytic view is of metaphysics as studying phenomenal human concepts rather than making claims about the noumenal world, so its style often blurs into philosophy of language and introspective psychology. Compared to system-building, it can seem very dry, stylistically similar to computer programming, mathematics or even accountancy (as a common stated goal is to "account for" entities in the world).






</doc>
<doc id="18896" url="https://en.wikipedia.org/wiki?curid=18896" title="Human spaceflight">
Human spaceflight

Human spaceflight (also referred to as manned spaceflight or crewed spaceflight) is spaceflight with a crew or passengers aboard a spacecraft. Spacecraft carrying people may be operated directly, by human crew, or it may be either remotely operated from ground stations on Earth or be autonomous, able to carry out a specific mission with no human involvement.

The first human in space was Yuri Gagarin, who flew the Vostok 1 spacecraft, launched by the Soviet Union on 12 April 1961 as part of the Vostok program. Humans have flown to the Moon nine times from 1968 to 1972 in the United States Apollo program, and have been continuously present in space for on the International Space Station.

Russia and China have human spaceflight capability with the Soyuz program and Shenzhou program. In the United States, SpaceShipTwo reached the edge of space in 2018; this was the first crewed spaceflight from the US since the Space Shuttle retired in 2011. From 2011 to May 2020 expeditions to the International Space Station used Soyuz vehicles, which remain attached to the station to allow quick return if needed. The United States developed commercial crew transportation to facilitate domestic access to the ISS, low Earth orbit and beyond such as the Orion vehicle and the private SpaceX Starship. The first successful commercial launch from the United States was conducted in May 2020 aboard the SpaceX Crew Dragon spacecraft carrying NASA astronauts Robert Behnken and Douglas Hurley on their way to the International Space Station.

While spaceflight has typically been a government-directed activity, commercial spaceflight has gradually been taking on a greater role. The first private human spaceflight took place on 21 June 2004, when SpaceShipOne conducted a suborbital flight, and a number of non-governmental companies have been working to develop a space tourism industry. NASA has also played a role to stimulate private spaceflight through programs such as Commercial Orbital Transportation Services (COTS) and Commercial Crew Development (CCDev). With its 2011 budget proposals released in 2010, the Obama administration moved towards a model where commercial companies would supply NASA with transportation services of both people and cargo transport to low Earth orbit. The vehicles used for these services could then serve both NASA and potential commercial customers. Commercial resupply of ISS began two years after the retirement of the Shuttle, and commercial crew launches have operated since May 2020.

Human spaceflight capability was first developed during the Cold War between the United States and the Soviet Union (USSR), which developed the first intercontinental ballistic missile rockets to deliver nuclear weapons. These rockets were large enough to be adapted to carry the first artificial satellites into low Earth orbit. After the first satellites were launched in 1957 and 1958, the US worked on Project Mercury to launch men singly into orbit, while the USSR secretly pursued the Vostok program to accomplish the same thing. The USSR launched the first human in space, Yuri Gagarin, into a single orbit in Vostok 1 on a Vostok 3KA rocket, on 12 April 1961. The US launched its first astronaut, Alan Shepard, on a suborbital flight aboard "Freedom 7" on a Mercury-Redstone rocket, on 5 May 1961. Unlike Gagarin, Shepard manually controlled his spacecraft's attitude, and landed inside it. The first American in orbit was John Glenn aboard "Friendship 7", launched 20 February 1962 on a Mercury-Atlas rocket. The USSR launched five more cosmonauts in Vostok capsules, including the first woman in space, Valentina Tereshkova aboard Vostok 6 on 16 June 1963. The US launched a total of two astronauts in suborbital flight and four into orbit through 1963. The US also made two flights in the North American X-15 (90 and 91) piloted by Joseph A. Walker that exceeded the Kármán line, the internationally recognized 100 km altitude used by the FAI to denote the edge of space.

US President John F. Kennedy raised the stakes of the Space Race by setting the goal of landing a man on the Moon and returning him safely by the end of the 1960s. The US started the three-man Apollo program in 1961 to accomplish this, launched by the Saturn family of launch vehicles, and the interim two-man Project Gemini in 1962, which flew 10 missions launched by Titan II rockets in 1965 and 1966. Gemini's objective was to support Apollo by developing American orbital spaceflight experience and techniques to be used in the Moon mission.

Meanwhile, the USSR remained silent about their intentions to send humans to the Moon, and proceeded to stretch the limits of their single-pilot Vostok capsule into a two- or three-person Voskhod capsule to compete with Gemini. They were able to launch two orbital flights in 1964 and 1965 and achieved the first spacewalk, made by Alexei Leonov on Voskhod 2 on 8 March 1965. But Voskhod did not have Gemini's capability to maneuver in orbit, and the program was terminated. The US Gemini flights did not accomplish the first spacewalk but overcame the early Soviet lead by performing several spacewalks and solving the problem of astronaut fatigue caused by overcoming the lack of gravity, demonstrating up to two weeks endurance in a human spaceflight, and the first space rendezvous and dockings of spacecraft.

The US succeeded in developing the Saturn V rocket necessary to send the Apollo spacecraft to the Moon, and sent Frank Borman, James Lovell, and William Anders into 10 orbits around the Moon in Apollo 8 in December 1968. In July 1969, Apollo 11 accomplished Kennedy's goal by landing Neil Armstrong and Buzz Aldrin on the Moon 21 July and returning them safely on 24 July along with Command Module pilot Michael Collins. A total of six Apollo missions landed 12 men to walk on the Moon through 1972, half of which drove electric powered vehicles on the surface. The crew of Apollo 13, Lovell, Jack Swigert, and Fred Haise, survived a catastrophic in-flight spacecraft failure and returned to Earth safely without landing on the Moon.

Meanwhile, the USSR secretly pursued crewed lunar orbiting and landing programs. They successfully developed the three-person Soyuz spacecraft for use in the lunar programs, but failed to develop the N1 rocket necessary for a human landing, and discontinued the lunar programs in 1974. On losing the Moon race, they concentrated on the development of space stations, using the Soyuz as a ferry to take cosmonauts to and from the stations. They started with a series of Salyut sortie stations from 1971 to 1986.

After the Apollo program, the US launched the Skylab sortie space station in 1973, inhabiting it for 171 days with three crews aboard Apollo spacecraft. President Richard Nixon and Soviet Premier Leonid Brezhnev negotiated an easing of relations known as détente, an easing of Cold War tensions. As part of this, they negotiated the Apollo-Soyuz program, in which an Apollo spacecraft carrying a special docking adapter module rendezvoused and docked with Soyuz 19 in 1975. The American and Russian crews shook hands in space, but the purpose of the flight was purely diplomatic and symbolic.

Nixon appointed his Vice President Spiro Agnew to head a Space Task Group in 1969 to recommend follow-on human spaceflight programs after Apollo. The group proposed an ambitious Space Transportation System based on a reusable Space Shuttle which consisted of a winged, internally fueled orbiter stage burning liquid hydrogen, launched by a similar, but larger kerosene-fueled booster stage, each equipped with airbreathing jet engines for powered return to a runway at the Kennedy Space Center launch site. Other components of the system included a permanent modular space station, reusable space tug and nuclear interplanetary ferry, leading to a human expedition to Mars as early as 1986, or as late as 2000, depending on the level of funding allocated. However, Nixon knew the American political climate would not support Congressional funding for such an ambition, and killed proposals for all but the Shuttle, possibly to be followed by the space station. Plans for the Shuttle were scaled back to reduce development risk, cost, and time, replacing the piloted flyback booster with two reusable solid rocket boosters, and the smaller orbiter would use an expendable external propellant tank to feed its hydrogen-fueled main engines. The orbiter would have to make unpowered landings.
The two nations continued to compete rather than cooperate in space, as the US turned to developing the Space Shuttle and planning the space station, dubbed "Freedom". 
The USSR launched three Almaz military sortie stations from 1973 to 1977, disguised as Salyuts. They followed Salyut with the development of "Mir", the first modular, semi-permanent space station, the construction of which took place from 1986 to 1996. "Mir" orbited at an altitude of , at a 51.6° inclination. It was occupied for 4,592 days, and made a controlled reentry in 2001.

The Space Shuttle started flying in 1981, but the US Congress failed to approve sufficient funds to make "Freedom" a reality. A fleet of four shuttles was built: "Columbia", "Challenger", "Discovery", and "Atlantis". A fifth shuttle, "Endeavour", was built to replace "Challenger", which was destroyed in an accident during launch that killed 7 astronauts on 28 January 1986. Twenty-two Shuttle flights carried a European Space Agency sortie space station called Spacelab in the payload bay from 1983 to 1998.
The USSR copied the reusable Space Shuttle orbiter, which they called "Buran"-class orbiter or simply "Buran". It was designed to be launched into orbit by the expendable Energia rocket, and capable of robotic orbital flight and landing. Unlike the Space Shuttle, "Buran" had no main rocket engines, but like the Space Shuttle orbiter it used engines to perform its final orbital insertion. A single uncrewed orbital test flight was successfully made in November 1988. A second test flight was planned by 1993, but the program was canceled due to lack of funding and the dissolution of the Soviet Union in 1991. Two more orbiters were never completed, and the one that performed an uncrewed flight was destroyed in a hangar roof collapse in May 2002.

The dissolution of the Soviet Union in 1991 brought an end to the Cold War and opened the door to true cooperation between the US and Russia. The Soviet Soyuz and Mir programs were taken over by the Russian Federal Space Agency, now known as the Roscosmos State Corporation. The Shuttle-Mir Program included American Space Shuttles visiting the "Mir" space station, Russian cosmonauts flying on the Shuttle, and an American astronaut flying aboard a Soyuz spacecraft for long-duration expeditions aboard "Mir".

In 1993, President Bill Clinton secured Russia's cooperation in converting the planned Space Station "Freedom" into the International Space Station (ISS). Construction of the station began in 1998. The station orbits at an altitude of and an inclination of 51.65°.

The Space Shuttle was retired in 2011 after 135 orbital flights, several of which helped assemble, supply, and crew the ISS. "Columbia" was destroyed in another accident during reentry, which killed 7 astronauts on 1 February 2003.

Russia has continued cooperation though half of the International Space Station is its sole singular half.

After Russia's launch of Sputnik 1 in 1957, Chairman Mao Zedong intended to place a Chinese satellite in orbit by 1959 to celebrate the 10th anniversary of the founding of the People's Republic of China (PRC), However, China did not successfully launch its first satellite until 24 April 1970. Mao and Premier Zhou Enlai decided on 14 July 1967, that the PRC should not be left behind, and started China's own human spaceflight program. The first attempt, the Shuguang spacecraft copied from the US Gemini, was cancelled on 13 May 1972.
China later designed the Shenzhou spacecraft resembling the Russian Soyuz, and became the third nation to achieve independent human spaceflight capability by launching Yang Liwei on a 21-hour flight aboard Shenzhou 5 on 15 October 2003. China launched the Tiangong-1 space station on 29 September 2011, and two sortie missions to it: Shenzhou 9 16–29 June 2012, with China's first female astronaut Liu Yang; and Shenzhou 10, 13–26 June 2013. The station was retired on 21 March 2016 and reentered on 2 April 2018, burning up with smaller fragments impacting the ocean. Tiangong-1's successor Tiangong-2 was launched in September 2016 to then be derorbited in July 2019. Tiangong-2 hosted a crew of two (Jing Haipeng and Chen Dong) for 26 days. The Tianzhou 1 cargo spacecraft docked to the station on 22 April 2017.

The European Space Agency began development in 1987 of the Hermes shuttle spaceplane, to be launched on the Ariane 5 expendable launch vehicle and to be docks with European Columbus space station. The projects were cancelled in 1992, when it became clear that neither cost nor performance goals could be achieved. No Hermes shuttles were ever built. Columbus space station reconfigured to same name European module in International Space Station.

Japan (NASDA) began development in the 1980s of the HOPE-X experimental shuttle spaceplane, to be launched on its H-IIA expendable launch vehicle. A string of failures in 1998 led to funding reduction, and the project's cancellation in 2003 in favour of participation in International Space Station by "Kibō" Japanese Experiment Module and H-II Transfer Vehicle cargo spacecraft.

As alternative to HOPE-X NASDA in 2001 proposed Fuji manned capsuled spacecraft for independent or ISS flights but project was no adopted.

Earlier in 1993-1997 , Kawasaki Heavy Industries and Mitsubishi Heavy Industries proposed Kankoh-maru proposed vertical takeoff and landing single-stage manned of cargo reusable launch system. Later in 2005 this system was proposed for space tourism.

According to a press-release of Iraqi News Agency of 5 December 1989 about the first (and last) test of the Tammouz space launcher, Iraq intended to develop crewed space facilities by the end of the century. These plans were put to an end by the Gulf War of 1991 and the economic hard times that followed.

Under the Bush administration, the Constellation program included plans for retiring the Space Shuttle program and replacing it with the capability for spaceflight beyond low Earth orbit. In the 2011 United States federal budget, the Obama administration cancelled Constellation for being over budget and behind schedule while not innovating and investing in critical new technologies. As part of the Artemis program, NASA is developing the Orion spacecraft to be launched by the Space Launch System. Under the Commercial Crew Development plan, NASA will rely on transportation services provided by the private sector to reach low Earth orbit, such as SpaceX Dragon 2, Sierra Nevada Corporation's Dream Chaser, or Boeing Starliner. The period between the retirement of the Space Shuttle in 2011 and the first launch to space of SpaceShipTwo Flight VP-03 on 13 December 2018 is similar to the gap between the end of Apollo in 1975 and the first Space Shuttle flight in 1981, is referred to by a presidential Blue Ribbon Committee as the U.S. human spaceflight gap.

SpaceX Dragon 2 launched on May 30, 2020 with a crew of 2 US astronauts, making it the first human spaceflight of the United States since STS-135.

Since the early 2000s, a variety of private spaceflight ventures have been undertaken. Several of the companies, including Blue Origin, SpaceX, Virgin Galactic, and Sierra Nevada have explicit plans to advance human spaceflight. , all four of those companies have development programs underway to fly commercial passengers.

A commercial suborbital spacecraft aimed at the space tourism market is being developed by Virgin Galactic called SpaceshipTwo which reached space in December 2018.
Blue Origin has begun a multi-year test program of their New Shepard vehicle and carried out 11 successful uncrewed test flights in 2015–2019. Blue Origin planned to fly with humans in 2019.

SpaceX and Boeing are both developing passenger-capable orbital space capsules as of 2020, with SpaceX carrying NASA astronauts to the International Space Station onboard a Crew Dragon spacecraft launched on a Falcon 9 Block 5 launch vehicle. Boeing will be doing it with their CST-100 launched on a United Launch Alliance Atlas V launch vehicle.
Development funding for these orbital-capable technologies has been provided by a mix of government and private funds, with SpaceX providing a greater portion of total development funding for this human-carrying capability from private investment.
There have been no public announcements of commercial offerings for orbital flights from either company, although both companies are planning some flights with their own private, not NASA, astronauts on board.


Sally Ride became the first American woman in space in 1983. Eileen Collins was the first female Shuttle pilot, and with Shuttle mission STS-93 in 1999 she became the first woman to command a U.S. spacecraft.

For many years, only the USSR (later Russia) and the United States had their own astronauts. Citizens of other nations flew in space, beginning with the flight of Vladimir Remek, a Czech, on a Soviet spacecraft on 2 March 1978, in the Interkosmos programme. , citizens from 38 nations (including space tourists) have flown in space aboard Soviet, American, Russian, and Chinese spacecraft.

Human spaceflight programs have been conducted by the former Soviet Union and currently Russia, the United States, Mainland China, and by the American private spaceflight companies.

Space vehicles are spacecraft used for transportation between the Earth's surface and outer space, or between locations in outer space. The following space vehicles and spaceports are currently used for launching human spaceflights:

The following space stations are currently maintained in Earth orbit for human occupation:

Numerous private companies attempted human spaceflight programs in an effort to win the $10 million Ansari X Prize. The first private human spaceflight took place on 21 June 2004, when SpaceShipOne conducted a suborbital flight. SpaceShipOne captured the prize on 4 October 2004, when it accomplished two consecutive flights within one week.

Most of the time, the only humans in space are those aboard the ISS, whose crew of six spends up to six months at a time in low Earth orbit.

NASA and ESA use the term "human spaceflight" to refer to their programs of launching people into space. These endeavors have also been referred to as "manned space missions," though because of gender specificity this is no longer official parlance according to NASA style guides.

Under the Indian Human Spaceflight Programme, India is planning to send humans into space on its orbital vehicle Gaganyaan before August 2022. The Indian Space Research Organisation (ISRO) began work on this project in 2006. The initial objective is to carry a crew of two or three to low Earth orbit (LEO) for a 3 to 7 day flight in a spacecraft on a GSLV Mk III rocket and return them safely for a water landing at a predefined landing zone. 
On 15 August 2018, Indian Prime Minister Narendra Modi, from the rampart of the Red Fort in New Delhi, formally declared that India will independently send humans into space before the 75th anniversary of independence in 2022. In 2019, ISRO further revealed plans for a space station by 2030, followed by a crewed lunar mission. Activities are currently progressing with a focus on the development of critical technologies, infrastructure and subsystems such as the crew module (CM), environmental control and life support system (ECLSS), crew escape system, etc. The department has initiated activities to study technical and managerial issues related to crewed missions. The program envisages the development of a fully autonomous orbital vehicle capable of carrying 2 or 3 crew members to an about low Earth orbit and bringing them safely back home. Development of space food and astronaut training is underway as of 2020, although the impact of the COVID-19 pandemic could delay the first mission by many months.

NASA is developing a plan to land humans on Mars by the 2030s. The first step will begin with Artemis 1 in 2021, sending an uncrewed Orion spacecraft to a distant retrograde orbit around the Moon and return it to Earth after a 25-day mission.

Several other countries and space agencies have announced and begun human spaceflight programs utilizing natively developed equipment and technology, including Japan (JAXA), Iran (ISA) and North Korea (NADA).

Since 2008 Japan Aerospace Exploration Agency has developed the H-II Transfer Vehicle cargo spacecraft based manned spacecraft and "Kibō" Japanese Experiment Module based small space laboratory.

Iranian manned spaceship project suggests small spacecraft and space laboratory. North Korea's space program has manned spacecraft and small shuttle system even as final aims.

A number of spacecraft have been proposed over the decades that might facilitate spaceliner passenger travel. Somewhat analogous to travel by airliner after the middle of the 20th century, these vehicles are proposed to transport a large number of passengers to destinations in space, or to destinations on Earth which travel through space. To date, none of these concepts have been built, although a few vehicles that carry fewer than 10 persons are currently in the flight testing phase of their development process.

One large spaceliner concept currently in early development is the SpaceX Starship which, in addition to replacing the Falcon 9 and Falcon Heavy launch vehicles in the legacy Earth-orbit market after 2020, has been proposed by SpaceX for long-distance commercial travel on Earth. This is to transport people on point-to-point suborbital flights between two points on Earth in under one hour, also known as "Earth-to-Earth," and carrying 100+ passengers.

Small spaceplane or small capsule suborbital spacecraft have been under development for the past decade or so and, , at least one of each type are under development. Both Virgin Galactic and Blue Origin are in active development, with the SpaceShipTwo spaceplane and the New Shepard capsule, respectively. Both would carry approximately a half-dozen passengers up to space for a brief time of zero gravity before returning to the same location from where the trip began. XCOR Aerospace had been developing the Lynx single-passenger spaceplane since the 2000s but development was halted in 2017.

There are two main sources of hazard in space flight: those due to the environment of space which make it hostile to the human body, and the potential for mechanical malfunctions of the equipment required to accomplish space flight.

Planners of human spaceflight missions face a number of safety concerns.

The basic needs for breathable air and drinkable water are addressed by the life support system of the spacecraft.

Medical consequences such as possible blindness and bone loss have been associated with human space flight.

On 31 December 2012, a NASA-supported study reported that spaceflight may harm the brain of astronauts and accelerate the onset of Alzheimer's disease.

In October 2015, the NASA Office of Inspector General issued a health hazards report related to space exploration, including a human mission to Mars.

On 2 November 2017, scientists reported that significant changes in the position and structure of the brain have been found in astronauts who have taken trips in space, based on MRI studies. Astronauts who took longer space trips were associated with greater brain changes.

Researchers in 2018 reported, after detecting the presence on the International Space Station (ISS) of five "Enterobacter bugandensis" bacterial strains, none pathogenic to humans, that microorganisms on ISS should be carefully monitored to continue assuring a medically healthy environment for astronauts.

In March 2019, NASA reported that latent viruses in humans may be activated during space missions, adding possibly more risk to astronauts in future deep-space missions.

Medical data from astronauts in low Earth orbits for long periods, dating back to the 1970s, show several adverse effects of a microgravity environment: loss of bone density, decreased muscle strength and endurance, postural instability, and reductions in aerobic capacity. Over time these deconditioning effects can impair astronauts' performance or increase their risk of injury.

In a weightless environment, astronauts put almost no weight on the back muscles or leg muscles used for standing up, which causes them to weaken and get smaller. Astronauts can lose up to twenty per cent of their muscle mass on spaceflights lasting five to eleven days. The consequent loss of strength could be a serious problem in case of a landing emergency. Upon return to Earth from long-duration flights, astronauts are considerably weakened, and are not allowed to drive a car for twenty-one days.

Astronauts experiencing weightlessness will often lose their orientation, get motion sickness, and lose their sense of direction as their bodies try to get used to a weightless environment. When they get back to Earth, or any other mass with gravity, they have to readjust to the gravity and may have problems standing up, focusing their gaze, walking and turning. Importantly, those body motor disturbances after changing from different gravities only get worse the longer the exposure to little gravity. These changes will affect operational activities including approach and landing, docking, remote manipulation, and emergencies that may happen while landing. This can be a major roadblock to mission success.

In addition, after long space flight missions, male astronauts may experience severe eyesight problems. Such eyesight problems may be a major concern for future deep space flight missions, including a crewed mission to the planet Mars. Long space flights can also alter a space traveler's eye movements.

Without proper shielding, the crews of missions beyond low Earth orbit (LEO) might be at risk from high-energy protons emitted by solar flares and associated solar particle events (SPEs). Lawrence Townsend of the University of Tennessee and others have studied the overall most powerful solar storm ever recorded. The flare was seen by the British astronomer Richard Carrington in September 1859. Radiation doses astronauts would receive from a Carrington-type storm could cause acute radiation sickness and possibly even death. Another storm that could have incurred a lethal radiation dose if astronauts were outside the Earth's protective magnetosphere occurred during the Space Age, in fact, shortly after Apollo 16 landed and before Apollo 17 launched. This solar storm of August 1972 would likely at least have caused acute illness.

Another type of radiation, galactic cosmic rays, presents further challenges to human spaceflight beyond low Earth orbit.

There is also some scientific concern that extended spaceflight might slow down the body's ability to protect itself against diseases. Some of the problems are a weakened immune system and the activation of dormant viruses in the body. Radiation can cause both short and long term consequences to the bone marrow stem cells which create the blood and immune systems. Because the interior of a spacecraft is so small, a weakened immune system and more active viruses in the body can lead to a fast spread of infection.

During long missions, astronauts are isolated and confined into small spaces. Depression, cabin fever and other psychological problems may impact the crew's safety and mission success.

Astronauts may not be able to quickly return to Earth or receive medical supplies, equipment or personnel if a medical emergency occurs. The astronauts may have to rely for long periods on their limited existing resources and medical advice from the ground.

During astronauts' stay in space, they may experience mental disorders (such as post-trauma, depression, anxiety, etc.), more than for an average person. NASA spends millions of dollars on psychological treatments for astronauts and former astronauts. To date, there is no way to prevent or reduce mental problems caused by extended periods of stay in space.

Due to these mental disorders, the efficiency of their work is impaired and sometimes they are forced to send the astronauts back to Earth, which is very expensive. A Russian expedition to space in 1976 was returned to Earth after the cosmonauts reported a strong odor that caused a fear of fluid leakage, but after a thorough investigation it became clear that there was no leakage or technical malfunction.  It was concluded by NASA that the cosmonauts most likely had hallucinations of the smell, which brought many unnecessary wasted expenses.

It is possible that the mental health of astronauts can be affected by the changes in the sensory systems while in prolonged space travel.

During astronauts' spaceflight, they are in a very extreme state where there is no gravity. This given state and the fact that no change is taking place in the environment will result in the weakening of sensory input to the astronauts in all seven senses.


Space flight requires much higher velocities than ground or air transportation, which in turn requires the use of high energy density propellants for launch, and the dissipation of large amounts of energy, usually as heat, for safe reentry through the Earth's atmosphere.

Since rockets carry the potential for fire or explosive destruction, space capsules generally employ some sort of launch escape system, consisting either of a tower-mounted solid-fuel rocket to quickly carry the capsule away from the launch vehicle (employed on Mercury, Apollo, and Soyuz), or else ejection seats (employed on Vostok and Gemini) to carry astronauts out of the capsule and away for individual parachute landing. The escape tower is discarded at some point before the launch is complete, at a point where an abort can be performed using the spacecraft's engines.

Such a system is not always practical for multiple crew member vehicles (particularly spaceplanes), depending on location of egress hatch(es). When the single-hatch Vostok capsule was modified to become the 2 or 3-person Voskhod, the single-cosmonaut ejection seat could not be used, and no escape tower system was added. The two Voskhod flights in 1964 and 1965 avoided launch mishaps. The Space Shuttle carried ejection seats and escape hatches for its pilot and copilot in early flights, but these could not be used for passengers who sat below the flight deck on later flights, and so were discontinued.

There have only been two in-flight launch aborts of a crewed flight. The first occurred on Soyuz 18a on 5 April 1975. The abort occurred after the launch escape system had been jettisoned when the launch vehicle's spent second stage failed to separate before the third stage ignited. The vehicle strayed off course, and the crew separated the spacecraft and fired its engines to pull it away from the errant rocket. Both cosmonauts landed safely. The second occurred on 11 October 2018 with the launch of Soyuz MS-10. Again, both crew members survived.

In the first use of a launch escape system on a crewed flight, the planned Soyuz T-10a launch on 26 September 1983 was aborted by a launch vehicle fire 90 seconds before liftoff. Both cosmonauts aboard landed safely.

The only crew fatality during launch occurred on 28 January 1986, when the Space Shuttle "Challenger" broke apart 73 seconds after liftoff, due to failure of a solid rocket booster seal which caused separation of the booster and failure of the external fuel tank, resulting in explosion of the fuel. All seven crew members were killed.

Despite the ever-present risks related to mechanical failures while working in open space, no spacewalking astronaut has ever been lost. There is a requirement for spacewalking astronauts to use tethers and sometimes supplementary anchors. If those fail, a spacewalking astronaut would most probably float away according to relevant forces that were acting on him when breaking loose. Astronaut would possibly be spinning as kicking and flailing is of no use. At the right angle and velocity, he might even re-enter the Earth's atmosphere and burn away completely. NASA has protocols for such situations: astronauts would be wearing an emergency jetpack, which would automatically counter any tumbling to stabilize them. Then NASA's plan states that astronauts should take manual control and fly back to safety.

However, if the pack's of fuel runs out, and if there is no other astronaut in close proximity to help, or if the air lock is irreparably damaged, the outcome would certainly be fatal. At the moment, there is no spacecraft to save an astronaut floating in space as the only one with a rescue-ready air-locked compartment — the Space Shuttle — retired years ago. There's approximately a litre of water available via straw in astronaut's helmet. They would wait roughly for 7.5 hours for breathable air to run out before dying of suffocation.

The single pilot of Soyuz 1, Vladimir Komarov was killed when his capsule's parachutes failed during an emergency landing on 24 April 1967, causing the capsule to crash.

The crew of seven aboard the were killed on reentry after completing a successful mission in space on 1 February 2003. A wing leading edge reinforced carbon-carbon heat shield had been damaged by a piece of frozen external tank foam insulation which broke off and struck the wing during launch. Hot reentry gasses entered and destroyed the wing structure, leading to the breakup of the orbiter vehicle.

There are two basic choices for an artificial atmosphere: either an Earth-like mixture of oxygen in an inert gas such as nitrogen or helium, or pure oxygen, which can be used at lower than standard atmospheric pressure. A nitrogen-oxygen mixture is used in the International Space Station and Soyuz spacecraft, while low-pressure pure oxygen is commonly used in space suits for extravehicular activity.

The use of a gas mixture carries the risk of decompression sickness (commonly known as "the bends") when transitioning to or from the pure oxygen space suit environment. There have also been instances of injury and fatalities caused by suffocation in the presence of too much nitrogen and not enough oxygen.

A pure oxygen atmosphere carries the risk of fire. The original design of the Apollo spacecraft used pure oxygen at greater than atmospheric pressure prior to launch. An electrical fire started in the cabin of Apollo 1 during a ground test at Cape Kennedy Air Force Station Launch Complex 34 on 27 January 1967, and spread rapidly. The high pressure (increased even higher by the fire) prevented removal of the plug door hatch cover in time to rescue the crew. All three, Gus Grissom, Ed White, and Roger Chaffee, were killed. This led NASA to use a nitrogen/oxygen atmosphere before launch, and low pressure pure oxygen only in space.

The March 1966 Gemini 8 mission was aborted in orbit when an attitude control system thruster stuck in the on position, sending the craft into a dangerous spin which threatened the lives of Neil Armstrong and David Scott. Armstrong had to shut the control system off and use the reentry control system to stop the spin. The craft made an emergency reentry and the astronauts landed safely. The most probable cause was determined to be an electrical short due to a static electricity discharge, which caused the thruster to remain powered even when switched off. The control system was modified to put each thruster on its own isolated circuit.

The third lunar landing expedition Apollo 13 in April 1970, was aborted and the lives of the crew, James Lovell, Jack Swigert and Fred Haise, were threatened by failure of a cryogenic liquid oxygen tank en route to the Moon. The tank burst when electrical power was applied to internal stirring fans in the tank, causing the immediate loss of all of its contents, and also damaging the second tank, causing the loss of its remaining oxygen in a span of 130 minutes. This in turn caused loss of electrical power provided by fuel cells to the command spacecraft. The crew managed to return to Earth safely by using the lunar landing craft as a "life boat". The tank failure was determined to be caused by two mistakes. The tank's drain fitting had been damaged when it was dropped during factory testing. This necessitated use of its internal heaters to boil out the oxygen after a pre-launch test, which in turn damaged the fan wiring's electrical insulation because the thermostats on the heaters did not meet the required voltage rating due to a vendor miscommunication.

The crew of Soyuz 11 were killed on 30 June 1971 by a combination of mechanical malfunctions: they were asphyxiated due to cabin decompression following separation of their descent capsule from the service module. A cabin ventilation valve had been jolted open at an altitude of by the stronger than expected shock of explosive separation bolts which were designed to fire sequentially, but in fact, had fired simultaneously. The loss of pressure became fatal within about 30 seconds.

, 23 crew members have died in accidents aboard spacecraft. Over 100 others have died in accidents during activity directly related to spaceflight or testing.

The first woman to ever enter space was Valentina Tereshkova. She flew in 1963 but it was not until the 1980s that another woman entered space again. All astronauts were required to be military test pilots at the time and women were not able to enter this career, this is one reason for the delay in allowing women to join space crews. After the rule changed, Svetlana Savitskaya became the second woman to enter space, she was also from the Soviet Union. Sally Ride became the next woman to enter space and the first woman to enter space through the United States program.

Since then, eleven other countries have allowed women astronauts. Due to some slow changes in the space programs to allow women.
The first all female space walk occurred in 2018, including Christina Koch and Jessica Meir. These two women have both participated in separate space walks with NASA. The first woman to go to the moon is planned for 2024.

Despite these developments women are still underrepresented among astronauts and especially cosmonauts. Issues that block potential applicants from the programs and limit the space missions they are able to go on, are for example:

Additionally women have been treated in discriminatory ways, for example as with Sally Ride by being scrutinized more than her male counterparts and asked sexist questions by the press.





</doc>
<doc id="18899" url="https://en.wikipedia.org/wiki?curid=18899" title="Mendelevium">
Mendelevium

Mendelevium is a synthetic element with the symbol Md (formerly Mv) and atomic number 101. A metallic radioactive transuranic element in the actinide series, it is the first element by atomic number that currently cannot be produced in macroscopic quantities through neutron bombardment of lighter elements. It is the third-to-last actinide and the ninth transuranic element. It can only be produced in particle accelerators by bombarding lighter elements with charged particles. A total of seventeen mendelevium isotopes are known, the most stable being Md with a half-life of 51 days; nevertheless, the shorter-lived Md (half-life 1.17 hours) is most commonly used in chemistry because it can be produced on a larger scale.

Mendelevium was discovered by bombarding einsteinium with alpha particles in 1955, the same method still used to produce it today. It was named after Dmitri Mendeleev, father of the periodic table of the chemical elements. Using available microgram quantities of the isotope einsteinium-253, over a million mendelevium atoms may be produced each hour. The chemistry of mendelevium is typical for the late actinides, with a preponderance of the +3 oxidation state but also an accessible +2 oxidation state. All known isotopes of mendelevium have relatively short half-lives; there are currently no uses for it outside basic scientific research, and only small amounts are produced.

Mendelevium was the ninth transuranic element to be synthesized. It was first synthesized by Albert Ghiorso, Glenn T. Seaborg, Gregory Robert Choppin, Bernard G. Harvey, and team leader Stanley G. Thompson in early 1955 at the University of California, Berkeley. The team produced Md (half-life of 77 minutes) when they bombarded an Es target consisting of only a billion (10) einsteinium atoms with alpha particles (helium nuclei) in the Berkeley Radiation Laboratory's 60-inch cyclotron, thus increasing the target's atomic number by two. Md thus became the first isotope of any element to be synthesized one atom at a time. In total, seventeen mendelevium atoms were produced. This discovery was part of a program, begun in 1952, that irradiated plutonium with neutrons to transmute it into heavier actinides. This method was necessary as the previous method used to synthesize transuranic elements, neutron capture, could not work because of a lack of known beta decaying isotopes of fermium that would produce isotopes of the next element, mendelevium, and also due to the very short half-life to spontaneous fission of Fm that thus constituted a hard limit to the success of the neutron capture process.

To predict if the production of mendelevium would be possible, the team made use of a rough calculation. The number of atoms that would be produced would be approximately equal to the product of the number of atoms of target material, the target's cross section, the ion beam intensity, and the time of bombardment; this last factor was related to the half-life of the product when bombarding for a time on the order of its half-life. This gave one atom per experiment. Thus under optimum conditions, the preparation of only one atom of element 101 per experiment could be expected. This calculation demonstrated that it was feasible to go ahead with the experiment. The target material, einsteinium-253, could be produced readily from irradiating plutonium: one year of irradiation would give a billion atoms, and its three-week half-life meant that the element 101 experiments could be conducted in one week after the produced einsteinium was separated and purified to make the target. However, it was necessary to upgrade the cyclotron to obtain the needed intensity of 10 alpha particles per second; Seaborg applied for the necessary funds.
While Seaborg applied for funding, Harvey worked on the einsteinium target, while Thomson and Choppin focused on methods for chemical isolation. Choppin suggested using α-hydroxyisobutyric acid to separate the mendelevium atoms from those of the lighter actinides. The actual synthesis was done by a recoil technique, introduced by Albert Ghiorso. In this technique, the einsteinium was placed on the opposite side of the target from the beam, so that the recoiling mendelevium atoms would get enough momentum to leave the target and be caught on a catcher foil made of gold. This recoil target was made by an electroplating technique, developed by Alfred Chetham-Strode. This technique gave a very high yield, which was absolutely necessary when working with such a rare and valuable product as the einsteinium target material. The recoil target consisted of 10 atoms of Es which were deposited electrolytically on a thin gold foil. It was bombarded by 41 MeV alpha particles in the Berkeley cyclotron with a very high beam density of 6×10 particles per second over an area of 0.05 cm. The target was cooled by water or liquid helium, and the foil could be replaced.

Initial experiments were carried out in September 1954. No alpha decay was seen from mendelevium atoms; thus, Ghiorso suggested that the mendelevium had all decayed by electron capture to fermium and that the experiment should be repeated to search instead for spontaneous fission events. The repetition of the experiment happened in February 1955.
On the day of discovery, 19 February, alpha irradiation of the einsteinium target occurred in three three-hour sessions. The cyclotron was in the University of California campus, while the Radiation Laboratory was on the next hill. To deal with this situation, a complex procedure was used: Ghiorso took the catcher foils (there were three targets and three foils) from the cyclotron to Harvey, who would use aqua regia to dissolve it and pass it through an anion-exchange resin column to separate out the transuranium elements from the gold and other products. The resultant drops entered a test tube, which Choppin and Ghiorso took in a car to get to the Radiation Laboratory as soon as possible. There Thompson and Choppin used a cation-exchange resin column and the α-hydroxyisobutyric acid. The solution drops were collected on platinum disks and dried under heat lamps. The three disks were expected to contain respectively the fermium, no new elements, and the mendelevium. Finally, they were placed in their own counters, which were connected to recorders such that spontaneous fission events would be recorded as huge deflections in a graph showing the number and time of the decays. There thus was no direct detection, but by observation of spontaneous fission events arising from its electron-capture daughter Fm. The first one was identified with a "hooray" followed by a "double hooray" and a "triple hooray". The fourth one eventually officially proved the chemical identification of the 101st element, mendelevium. In total, five decays were reported up till 4 a.m. Seaborg was notified and the team left to sleep. Additional analysis and further experimentation showed the produced mendelevium isotope to have mass 256 and to decay by electron capture to fermium-256 with a half-life of 1.5 h.

Being the first of the second hundred of the chemical elements, it was decided that the element would be named "mendelevium" after the Russian chemist Dmitri Mendeleev, father of the periodic table. Because this discovery came during the Cold War, Seaborg had to request permission of the government of the United States to propose that the element be named for a Russian, but it was granted. The name "mendelevium" was accepted by the International Union of Pure and Applied Chemistry (IUPAC) in 1955 with symbol "Mv", which was changed to "Md" in the next IUPAC General Assembly (Paris, 1957).

In the periodic table, mendelevium is located to the right of the actinide fermium, to the left of the actinide nobelium, and below the lanthanide thulium. Mendelevium metal has not yet been prepared in bulk quantities, and bulk preparation is currently impossible. Nevertheless, a number of predictions and some preliminary experimental results have been done regarding its properties.

The lanthanides and actinides, in the metallic state, can exist as either divalent (such as europium and ytterbium) or trivalent (most other lanthanides) metals. The former have fds configurations, whereas the latter have fs configurations. In 1975, Johansson and Rosengren examined the measured and predicted values for the cohesive energies (enthalpies of crystallization) of the metallic lanthanides and actinides, both as divalent and trivalent metals. The conclusion was that the increased binding energy of the [Rn]5f6d7s configuration over the [Rn]5f7s configuration for mendelevium was not enough to compensate for the energy needed to promote one 5f electron to 6d, as is true also for the very late actinides: thus einsteinium, fermium, mendelevium, and nobelium were expected to be divalent metals. The increasing predominance of the divalent state well before the actinide series concludes is attributed to the relativistic stabilization of the 5f electrons, which increases with increasing atomic number. Thermochromatographic studies with trace quantities of mendelevium by Zvara and Hübener from 1976 to 1982 confirmed this prediction. In 1990, Haire and Gibson estimated mendelevium metal to have an enthalpy of sublimation between 134 and 142 kJ/mol. Divalent mendelevium metal should have a metallic radius of around . Like the other divalent late actinides (except the once again trivalent lawrencium), metallic mendelevium should assume a face-centered cubic crystal structure. Mendelevium's melting point has been estimated at 827 °C, the same value as that predicted for the neighboring element nobelium. Its density is predicted to be around .

The chemistry of mendelevium is mostly known only in solution, in which it can take on the +3 or +2 oxidation states. The +1 state has also been reported, but has not yet been confirmed.

Before mendelevium's discovery, Seaborg and Katz predicted that it should be predominantly trivalent in aqueous solution and hence should behave similarly to other tripositive lanthanides and actinides. After the synthesis of mendelevium in 1955, these predictions were confirmed, first in the observation at its discovery that it eluted just after fermium in the trivalent actinide elution sequence from a cation-exchange column of resin, and later the 1967 observation that mendelevium could form insoluble hydroxides and fluorides that coprecipitated with trivalent lanthanide salts. Cation-exchange and solvent extraction studies led to the conclusion that mendelevium was a trivalent actinide with an ionic radius somewhat smaller than that of the previous actinide, fermium. Mendelevium can form coordination complexes with 1,2-cyclohexanedinitrilotetraacetic acid (DCTA).

In reducing conditions, mendelevium(III) can be easily reduced to mendelevium(II), which is stable in aqueous solution. The standard reduction potential of the "E"°(Md→Md) couple was variously estimated in 1967 as −0.10 V or −0.20 V: later 2013 experiments established the value as . In comparison, "E"°(Md→Md) should be around −1.74 V, and "E"°(Md→Md) should be around −2.5 V. Mendelevium(II)'s elution behavior has been compared with that of strontium(II) and europium(II).

In 1973, mendelevium(I) was reported to have been produced by Russian scientists, who obtained it by reducing higher oxidation states of mendelevium with samarium(II). It was found to be stable in neutral water–ethanol solution and be homologous to caesium(I). However, later experiments found no evidence for mendelevium(I) and found that mendelevium behaved like divalent elements when reduced, not like the monovalent alkali metals. Nevertheless, the Russian team conducted further studies on the thermodynamics of cocrystallizing mendelevium with alkali metal chlorides, and concluded that mendelevium(I) had formed and could form mixed crystals with divalent elements, thus cocrystallizing with them. The status of the +1 oxidation state is still tentative.

Although "E"°(Md→Md) was predicted in 1975 to be +5.4 V, suggesting that mendelevium(III) could be oxidized to mendelevium(IV), 1967 experiments with the strong oxidizing agent sodium bismuthate were unable to oxidize mendelevium(III) to mendelevium(IV).

A mendelevium atom has 101 electrons, of which at least three (and perhaps four) can act as valence electrons. They are expected to be arranged in the configuration [Rn]5f7s (ground state term symbol F), although experimental verification of this electron configuration had not yet been made as of 2006. In forming compounds, three valence electrons may be lost, leaving behind a [Rn]5f core: this conforms to the trend set by the other actinides with their [Rn] 5f electron configurations in the tripositive state. The first ionization potential of mendelevium was measured to be at most (6.58 ± 0.07) eV in 1974, based on the assumption that the 7s electrons would ionize before the 5f ones; this value has since not yet been refined further due to mendelevium's scarcity and high radioactivity. The ionic radius of hexacoordinate Md had been preliminarily estimated in 1978 to be around 91.2 pm; 1988 calculations based on the logarithmic trend between distribution coefficients and ionic radius produced a value of 89.6 pm, as well as an enthalpy of hydration of . Md should have an ionic radius of 115 pm and hydration enthalpy −1413 kJ/mol; Md should have ionic radius 117 pm.

Seventeen isotopes of mendelevium are known, with mass numbers from 244 to 260; all are radioactive. Additionally, five nuclear isomers are known: Md, Md, Md, Md, and Md. Of these, the longest-lived isotope is Md with a half-life of 51.5 days, and the longest-lived isomer is Md with a half-life of 58.0 minutes. Nevertheless, the shorter-lived Md (half-life 1.17 hours) is more often used in chemical experimentation because it can be produced in larger quantities from alpha particle irradiation of einsteinium. After Md, the next most stable mendelevium isotopes are Md with a half-life of 31.8 days, Md with a half-life of 5.52 hours, Md with a half-life of 1.60 hours, and Md with a half-life of 1.17 hours. All of the remaining mendelevium isotopes have half-lives that are less than an hour, and the majority of these have half-lives that are less than 5 minutes.

The half-lives of mendelevium isotopes mostly increase smoothly from Md onwards, reaching a maximum at Md. Experiments and predictions suggest that the half-lives will then decrease, apart from Md with a half-life of 31.8 days, as spontaneous fission becomes the dominant decay mode due to the mutual repulsion of the protons posing a limit to the island of relative stability of long-lived nuclei in the actinide series.

Mendelevium-256, the chemically most important isotope of mendelevium, decays through electron capture 90% of the time and alpha decay 10% of the time. It is most easily detected through the spontaneous fission of its electron capture daughter fermium-256, but in the presence of other nuclides that undergo spontaneous fission, alpha decays at the characteristic energies for mendelevium-256 (7.205 and 7.139 MeV) can provide more useful identification.

The lightest mendelevium isotopes (Md to Md) are mostly produced through bombardment of bismuth targets with heavy argon ions, while slightly heavier ones (Md to Md) are produced by bombarding plutonium and americium targets with lighter ions of carbon and nitrogen. The most important and most stable isotopes are in the range from Md to Md and are produced through bombardment of einsteinium isotopes with alpha particles: einsteinium-253, -254, and -255 can all be used. Md is produced as a daughter of No, and Md can be produced in a transfer reaction between einsteinium-254 and oxygen-18. Typically, the most commonly used isotope Md is produced by bombarding either einsteinium-253 or -254 with alpha particles: einsteinium-254 is preferred when available because it has a longer half-life and therefore can be used as a target for longer. Using available microgram quantities of einsteinium, femtogram quantities of mendelevium-256 may be produced.

The recoil momentum of the produced mendelevium-256 atoms is used to bring them physically far away from the einsteinium target from which they are produced, bringing them onto a thin foil of metal (usually beryllium, aluminium, platinum, or gold) just behind the target in a vacuum. This eliminates the need for immediate chemical separation, which is both costly and prevents reusing of the expensive einsteinium target. The mendelevium atoms are then trapped in a gas atmosphere (frequently helium), and a gas jet from a small opening in the reaction chamber carries the mendelevium along. Using a long capillary tube, and including potassium chloride aerosols in the helium gas, the mendelevium atoms can be transported over tens of meters to be chemically analyzed and have their quantity determined. The mendelevium can then be separated from the foil material and other fission products by applying acid to the foil and then coprecipitating the mendelevium with lanthanum fluoride, then using a cation-exchange resin column with a 10% ethanol solution saturated with hydrochloric acid, acting as an eluant. However, if the foil is made of gold and thin enough, it is enough to simply dissolve the gold in aqua regia before separating the trivalent actinides from the gold using anion-exchange chromatography, the eluant being 6 M hydrochloric acid.

Mendelevium can finally be separated from the other trivalent actinides using selective elution from a cation-exchange resin column, the eluant being ammonia α-HIB. Using the gas-jet method often renders the first two steps unnecessary. The above procedure is the most commonly used one for the separation of transeinsteinium elements.

Another possible way to separate the trivalent actinides is via solvent extraction chromatography using bis-(2-ethylhexyl) phosphoric acid (abbreviated as HDEHP) as the stationary organic phase and nitric acid as the mobile aqueous phase. The actinide elution sequence is reversed from that of the cation-exchange resin column, so that the heavier actinides elute later. The mendelevium separated by this method has the advantage of being free of organic complexing agent compared to the resin column; the disadvantage is that mendelevium then elutes very late in the elution sequence, after fermium.

Another method to isolate mendelevium exploits the distinct elution properties of Md from those of Es and Fm. The initial steps are the same as above, and employs HDEHP for extraction chromatography, but coprecipitates the mendelevium with terbium fluoride instead of lanthanum fluoride. Then, 50 mg of chromium is added to the mendelevium to reduce it to the +2 state in 0.1 M hydrochloric acid with zinc or mercury. The solvent extraction then proceeds, and while the trivalent and tetravalent lanthanides and actinides remain on the column, mendelevium(II) does not and stays in the hydrochloric acid. It is then reoxidized to the +3 state using hydrogen peroxide and then isolated by selective elution with 2 M hydrochloric acid (to remove impurities, including chromium) and finally 6 M hydrochloric acid (to remove the mendelevium). It is also possible to use a column of cationite and zinc amalgam, using 1 M hydrochloric acid as an eluant, reducing Md(III) to Md(II) where it behaves like the alkaline earth metals. Thermochromatographic chemical isolation could be achieved using the volatile mendelevium hexafluoroacetylacetonate: the analogous fermium compound is also known and is also volatile.

Although few people come in contact with mendelevium, the International Commission on Radiological Protection has set annual exposure limits for the most stable isotope. For mendelevium-258, the ingestion limit was set at 9×10 becquerels (1 Bq is equivalent to one decay per second), and the inhalation limit at 6000 Bq.




</doc>
<doc id="18900" url="https://en.wikipedia.org/wiki?curid=18900" title="Modus ponens">
Modus ponens

In propositional logic, modus ponens () (MP), also known as modus ponendo ponens (Latin for "mode that by affirming affirms") or implication elimination, is a deductive argument form and rule of inference. It can be summarized as ""P implies Q" and "P" is true, therefore "Q" must be true."

"Modus ponens" is closely related to another valid form of argument, "modus tollens". Both have apparently similar but invalid forms such as affirming the consequent, denying the antecedent, and evidence of absence. Constructive dilemma is the disjunctive version of "modus ponens". Hypothetical syllogism is closely related to "modus ponens" and sometimes thought of as "double "modus ponens"."

The history of "modus ponens" goes back to antiquity. The first to explicitly describe the argument form "modus ponens" was Theophrastus. It, along with "modus tollens", is one of the standard patterns of inference that can be applied to derive chains of conclusions that lead to the desired goal.

The form of a "modus ponens" argument resembles a syllogism, with two premises and a conclusion:

The first premise is a conditional ("if–then") claim, namely that "P" implies "Q". The second premise is an assertion that "P", the antecedent of the conditional claim, is the case. From these two premises it can be logically concluded that "Q", the consequent of the conditional claim, must be the case as well.

An example of an argument that fits the form "modus ponens":

This argument is valid, but this has no bearing on whether any of the statements in the argument are actually true; for "modus ponens" to be a sound argument, the premises must be true for any true instances of the conclusion. An argument can be valid but nonetheless unsound if one or more premises are false; if an argument is valid "and" all the premises are true, then the argument is sound. For example, John might be going to work on Wednesday. In this case, the reasoning for John's going to work (because it is Wednesday) is unsound. The argument is only sound on Tuesdays (when John goes to work), but valid on every day of the week. A propositional argument using "modus ponens" is said to be deductive.

In single-conclusion sequent calculi, "modus ponens" is the Cut rule. The cut-elimination theorem for a calculus says that every proof involving Cut can be transformed (generally, by a constructive method) into a proof without Cut, and hence that Cut is admissible.

The Curry–Howard correspondence between proofs and programs relates "modus ponens" to function application: if "f" is a function of type "P" → "Q" and "x" is of type "P", then "f x" is of type "Q".

In artificial intelligence, "modus ponens" is often called forward chaining.

The "modus ponens" rule may be written in sequent notation as

where "P", "Q" and "P" → "Q" are statements (or propositions) in a formal language and ⊢ is a metalogical symbol meaning that "Q" is a syntactic consequence of "P" and "P" → "Q" in some logical system.

The validity of "modus ponens" in classical two-valued logic can be clearly demonstrated by use of a truth table.
In instances of "modus ponens" we assume as premises that "p" → "q" is true and "p" is true. Only one line of the truth table—the first—satisfies these two conditions ("p" and "p" → "q"). On this line, "q" is also true. Therefore, whenever "p" → "q" is true and "p" is true, "q" must also be true.

While "modus ponens" is one of the most commonly used argument forms in logic it must not be mistaken for a logical law; rather, it is one of the accepted mechanisms for the construction of deductive proofs that includes the "rule of definition" and the "rule of substitution". "Modus ponens" allows one to eliminate a conditional statement from a logical proof or argument (the antecedents) and thereby not carry these antecedents forward in an ever-lengthening string of symbols; for this reason modus ponens is sometimes called the rule of detachment or the law of detachment. Enderton, for example, observes that "modus ponens can produce shorter formulas from longer ones", and Russell observes that "the process of the inference cannot be reduced to symbols. Its sole record is the occurrence of ⊦q [the consequent] ... an inference is the dropping of a true premise; it is the dissolution of an implication".

A justification for the "trust in inference is the belief that if the two former assertions [the antecedents] are not in error, the final assertion [the consequent] is not in error". In other words: if one statement or proposition implies a second one, and the first statement or proposition is true, then the second one is also true. If "P" implies "Q" and "P" is true, then "Q" is true.

"Modus ponens" represents an instance of the Law of total probability which for a binary variable is expressed as:

formula_2,

where e.g. formula_3 denotes the probability of formula_4 and the conditional probability formula_5 generalizes the logical implication formula_6. Assume that formula_7 is equivalent to formula_4 being TRUE, and that formula_9 is equivalent to formula_4 being FALSE. It is then easy to see that formula_7 when formula_12 and formula_13. Hence, the law of total probability represents a generalization of "modus ponens".

"Modus ponens" represents an instance of the binomial deduction operator in subjective logic expressed as:

formula_14,

where formula_15 denotes the subjective opinion about formula_16 as expressed by source formula_17, and the conditional opinion formula_18 generalizes the logical implication formula_6. The deduced marginal opinion about formula_4 is denoted by formula_21. The case where formula_15 is an absolute TRUE opinion about formula_16 is equivalent to source formula_17 saying that formula_16 is TRUE, and the case where formula_15 is an absolute FALSE opinion about formula_16 is equivalent to source formula_17 saying that formula_16 is FALSE. The deduction operator formula_30 of subjective logic produces an absolute TRUE deduced opinion formula_21 when the conditional opinion formula_18 is absolute TRUE and the antecedent opinion formula_15 is absolute TRUE. Hence, subjective logic deduction represents a generalization of both "modus ponens" and the Law of total probability.

The philosopher and logician Vann McGee has argued that "modus ponens" can fail to be valid when the consequent is itself a conditional sentence. Here is an example:

The first premise seems reasonable enough, because Shakespeare is generally credited with writing "Hamlet". The second premise seems reasonable, as well, because with the set of Hamlet''s possible authors limited to just Shakespeare and Hobbes, eliminating one leaves only the other. But the conclusion, considered by itself and with the possible authors "not" limited to just Shakespeare and Hobbes, is dubious, because if Shakespeare is ruled out as Hamlet's author, there are many more plausible alternatives than Hobbes.

The general form of McGee-type counterexamples to "modus ponens" is simply formula_34, therefore formula_35; it is not essential that formula_16 be a disjunction, as in the example given. That these kinds of cases constitute failures of "modus ponens" remains a minority view among logicians, but opinions vary on how the cases should be disposed of.

In deontic logic, some examples of conditional obligation also raise the possibility of modus ponens failure. These are cases where the conditional premise describes an obligation predicated on an immoral or imprudent action, e.g., “If Doe murders his mother, he ought to do so gently,” for which the dubious unconditional conclusion would be "Doe ought to gently murder his mother." It would appear to follow that if Doe is in fact gently murdering his mother, then by modus ponens he is doing exactly what he should, unconditionally, be doing. Here again, modus ponens failure is not a popular diagnosis but is sometimes argued for.

The fallacy of affirming the consequent is a common misinterpretation of the modus ponens.





</doc>
<doc id="18901" url="https://en.wikipedia.org/wiki?curid=18901" title="Modus tollens">
Modus tollens

In propositional logic, modus tollens () (MT), also known as modus tollendo tollens (Latin for "mode that by denying denies") and denying the consequent, is a deductive argument form and a rule of inference. "Modus tollens" takes the form of "If P, then Q. Not Q. Therefore, not P." It is an application of the general truth that if a statement is true, then so is its contrapositive. The form shows that inference from "P implies Q" to "the negation of Q implies the negation of P" is a valid argument.

The history of the inference rule "modus tollens" goes back to antiquity. The first to explicitly describe the argument form "modus tollens" was Theophrastus.

"Modus tollens" is closely related to "modus ponens". There are two similar, but invalid, forms of argument: affirming the consequent and denying the antecedent. See also contraposition and proof by contrapositive.

The form of a "modus tollens" argument resembles a syllogism, with two premises and a conclusion:

The first premise is a conditional ("if-then") claim, such as "P" implies "Q". The second premise is an assertion that "Q", the consequent of the conditional claim, is not the case. From these two premises it can be logically concluded that "P", the antecedent of the conditional claim, is also not the case. 

For example:

Supposing that the premises are both true (the dog will bark if it detects an intruder, and does indeed not bark), it follows that no intruder has been detected. This is a valid argument since it is not possible for the conclusion to be false if the premises are true. (It is conceivable that there may have been an intruder that the dog did not detect, but that does not invalidate the argument; the first premise is "if the dog "detects" an intruder". The thing of importance is that the dog detects or does not detect an intruder, not whether there is one.)

Another example:

Another example: 

Every use of "modus tollens" can be converted to a use of "modus ponens" and one use of transposition to the premise which is a material implication. For example:

Likewise, every use of "modus ponens" can be converted to a use of "modus tollens" and transposition.

The "modus tollens" rule can be stated formally as:

where formula_2 stands for the statement "P implies Q". formula_3 stands for "it is not the case that Q" (or in brief "not Q"). Then, whenever "formula_2" and "formula_5" each appear by themselves as a line of a proof, then "formula_6" can validly be placed on a subsequent line. 

The "modus tollens" rule may be written in sequent notation:

where formula_8 is a metalogical symbol meaning that formula_6 is a syntactic consequence of formula_2 and formula_5 in some logical system;

or as the statement of a functional tautology or theorem of propositional logic:

where formula_13 and formula_14 are propositions expressed in some formal system;

or including assumptions:

though since the rule does not change the set of assumptions, this is not strictly necessary.

More complex rewritings involving "modus tollens" are often seen, for instance in set theory:

Also in first-order predicate logic:

Strictly speaking these are not instances of "modus tollens", but they may be derived from "modus tollens" using a few extra steps.

The validity of "modus tollens" can be clearly demonstrated through a truth table.

In instances of "modus tollens" we assume as premises that p → q is true and q is false. There is only one line of the truth table—the fourth line—which satisfies these two conditions. In this line, p is false. Therefore, in every instance in which p → q is true and q is false, p must also be false.

"Modus tollens" represents an instance of the law of total probability combined with Bayes' theorem expressed as:

formula_22,

where the conditionals formula_23 and formula_24 are obtained with (the extended form of) Bayes' theorem expressed as:

formula_25 and formula_26.

In the equations above formula_27 denotes the probability of formula_14, and formula_29 denotes the base rate (aka. prior probability) of formula_13. The conditional probability formula_31 generalizes the logical statement formula_2, i.e. in addition to assigning TRUE or FALSE we can also assign any probability to the statement. Assume that formula_33 is equivalent to formula_14 being TRUE, and that formula_35 is equivalent to formula_14 being FALSE. It is then easy to see that formula_37 when formula_38 and formula_35. This is because formula_40 so that formula_41 in the last equation. Therefore, the product terms in the first equation always have a zero factor so that formula_37 which is equivalent to formula_13 being FALSE. Hence, the law of total probability combined with Bayes' theorem represents a generalization of "modus tollens" .

"Modus tollens" represents an instance of the abduction operator in subjective logic expressed as:

formula_44,

where formula_45 denotes the subjective opinion about formula_14, and formula_47 denotes a pair of binomial conditional opinions, as expressed by source formula_48. The parameter formula_49 denotes the base rate (aka. the prior probability) of formula_13. The abduced marginal opinion on formula_13 is denoted formula_52. The conditional opinion formula_53 generalizes the logical statement formula_2, i.e. in addition to assigning TRUE or FALSE the source formula_48 can assign any subjective opinion to the statement. The case where formula_45 is an absolute TRUE opinion is equivalent to source formula_48 saying that formula_14 is TRUE, and the case where formula_45 is an absolute FALSE opinion is equivalent to source formula_48 saying that formula_14 is FALSE. The abduction operator formula_62 of subjective logic produces an absolute FALSE abduced opinion formula_63 when the conditional opinion formula_53 is absolute TRUE and the consequent opinion formula_45 is absolute FALSE. Hence, subjective logic abduction represents a generalization of both "modus tollens" and of the Law of total probability combined with Bayes' theorem .





</doc>
<doc id="18902" url="https://en.wikipedia.org/wiki?curid=18902" title="Mathematician">
Mathematician

A mathematician is someone who uses an extensive knowledge of mathematics in their work, typically to solve mathematical problems.
Mathematicians are concerned with numbers, data, quantity, structure, space, models, and change.

One of the earliest known mathematicians was Thales of Miletus (c. 624–c.546 BC); he has been hailed as the first true mathematician and the first known individual to whom a mathematical discovery has been attributed. He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries to Thales' Theorem.

The number of known mathematicians grew when Pythagoras of Samos (c. 582–c. 507 BC) established the Pythagorean School, whose doctrine it was that mathematics ruled the universe and whose motto was "All is number". It was the Pythagoreans who coined the term "mathematics", and with whom the study of mathematics for its own sake begins.

The first woman mathematician recorded by history was Hypatia of Alexandria (AD 350 - 415). She succeeded her father as Librarian at the Great Library and wrote many works on applied mathematics. Because of a political dispute, the Christian community in Alexandria punished her, presuming she was involved, by stripping her naked and scraping off her skin with clamshells (some say roofing tiles).

Science and mathematics in the Islamic world during the Middle Ages followed various models and modes of funding varied based primarily on scholars. It was extensive patronage and strong intellectual policies implemented by specific rulers that allowed scientific knowledge to develop in many areas. Funding for translation of scientific texts in other languages was ongoing throughout the reign of certain caliphs, and it turned out that certain scholars became experts in the works they translated and in turn received further support for continuing to develop certain sciences. As these sciences received wider attention from the elite, more scholars were invited and funded to study particular sciences. An example of a translator and mathematician who benefited from this type of support was al-Khawarizmi. A notable feature of many scholars working under Muslim rule in medieval times is that they were often polymaths. Examples include the work on optics, maths and astronomy of Ibn al-Haytham.

The Renaissance brought an increased emphasis on mathematics and science to Europe. During this period of transition from a mainly feudal and ecclesiastical culture to a predominantly secular one, many notable mathematicians had other occupations: Luca Pacioli (founder of accounting); Niccolò Fontana Tartaglia (notable engineer and bookkeeper); Gerolamo Cardano (earliest founder of probability and binomial expansion); Robert Recorde (physician) and François Viète (lawyer).

As time passed, many mathematicians gravitated towards universities. An emphasis on free thinking and experimentation had begun in Britain's oldest universities beginning in the seventeenth century at Oxford with the scientists Robert Hooke and Robert Boyle, and at Cambridge where Isaac Newton was Lucasian Professor of Mathematics & Physics. Moving into the 19th century, the objective of universities all across Europe evolved from teaching the “regurgitation of knowledge” to “encourag[ing] productive thinking.” In 1810, Humboldt convinced the King of Prussia to build a university in Berlin based on Friedrich Schleiermacher’s liberal ideas; the goal was to demonstrate the process of the discovery of knowledge and to teach students to “take account of fundamental laws of science in all their thinking.” Thus, seminars and laboratories started to evolve.

British universities of this period adopted some approaches familiar to the Italian and German universities, but as they already enjoyed substantial freedoms and autonomy the changes there had begun with the Age of Enlightenment, the same influences that inspired Humboldt. The Universities of Oxford and Cambridge emphasized the importance of research, arguably more authentically implementing Humboldt's idea of a university than even German universities, which were subject to state authority. Overall, science (including mathematics) became the focus of universities in the 19th and 20th centuries. Students could conduct research in seminars or laboratories and began to produce doctoral theses with more scientific content. According to Humboldt, the mission of the University of Berlin was to pursue scientific knowledge. The German university system fostered professional, bureaucratically regulated scientific research performed in well-equipped laboratories, instead of the kind of research done by private and individual scholars in Great Britain and France. In fact, Rüegg asserts that the German system is responsible for the development of the modern research university because it focused on the idea of “freedom of scientific research, teaching and study.”

Mathematicians usually cover a breadth of topics within mathematics in their undergraduate education, and then proceed to specialize in topics of their own choice at the graduate level. In some universities, a qualifying exam serves to test both the breadth and depth of a student's understanding of mathematics; the students, who pass, are permitted to work on a doctoral dissertation.

Mathematicians involved with solving problems with applications in real life are called applied mathematicians. Applied mathematicians are mathematical scientists who, with their specialized knowledge and professional methodology, approach many of the imposing problems presented in related scientific fields. With professional focus on a wide variety of problems, theoretical systems, and localized constructs, applied mathematicians work regularly in the study and formulation of mathematical models. Mathematicians and applied mathematicians are considered to be two of the STEM (science, technology, engineering, and mathematics) careers.

The discipline of applied mathematics concerns itself with mathematical methods that are typically used in science, engineering, business, and industry; thus, "applied mathematics" is a mathematical science with specialized knowledge. The term "applied mathematics" also describes the professional specialty in which mathematicians work on problems, often concrete but sometimes abstract. As professionals focused on problem solving, "applied mathematicians" look into the "formulation, study, and use of mathematical models" in science, engineering, business, and other areas of mathematical practice.

Pure mathematics is mathematics that studies entirely abstract concepts. From the eighteenth century onwards, this was a recognized category of mathematical activity, sometimes characterized as "speculative mathematics", and at variance with the trend towards meeting the needs of navigation, astronomy, physics, economics, engineering, and other applications.

Another insightful view put forth is that "pure mathematics is not necessarily applied mathematics": it is possible to study abstract entities with respect to their intrinsic nature, and not be concerned with how they manifest in the real world. Even though the pure and applied viewpoints are distinct philosophical positions, in practice there is much overlap in the activity of pure and applied mathematicians.

To develop accurate models for describing the real world, many applied mathematicians draw on tools and techniques that are often considered to be "pure" mathematics. On the other hand, many pure mathematicians draw on natural and social phenomena as inspiration for their abstract research.

Many professional mathematicians also engage in the teaching of mathematics. Duties may include:

Many careers in mathematics outside of universities involve consulting. For instance, actuaries assemble and analyze data to estimate the probability and likely cost of the occurrence of an event such as death, sickness, injury, disability, or loss of property. Actuaries also address financial questions, including those involving the level of pension contributions required to produce a certain retirement income and the way in which a company should invest resources to maximize its return on investments in light of potential risk. Using their broad knowledge, actuaries help design and price insurance policies, pension plans, and other financial strategies in a manner which will help ensure that the plans are maintained on a sound financial basis.

As another example, mathematical finance will derive and extend the mathematical or numerical models without necessarily establishing a link to financial theory, taking observed market prices as input. Mathematical consistency is required, not compatibility with economic theory. Thus, for example, while a financial economist might study the structural reasons why a company may have a certain share price, a financial mathematician may take the share price as a given, and attempt to use stochastic calculus to obtain the corresponding value of derivatives of the stock ("see: Valuation of options; Financial modeling").

According to the Dictionary of Occupational Titles occupations in mathematics include the following.


The following are quotations about mathematicians, or by mathematicians.

There is no Nobel Prize in mathematics, though sometimes mathematicians have won the Nobel Prize in a different field, such as economics. Prominent prizes in mathematics include the Abel Prize, the Chern Medal, the Fields Medal, the Gauss Prize, the Nemmers Prize, the Balzan Prize, the Crafoord Prize, the Shaw Prize, the Steele Prize, the Wolf Prize, the Schock Prize, and the Nevanlinna Prize.

The American Mathematical Society, Association for Women in Mathematics, and other mathematical societies offer several prizes aimed at increasing the representation of women and minorities in the future of mathematics.

Several well known mathematicians have written autobiographies in part to explain to a general audience what it is about mathematics that has made them want to devote their lives to its study. These provide some of the best glimpses into what it means to be a mathematician. The following list contains some works that are not autobiographies, but rather essays on mathematics and mathematicians with strong autobiographical elements.




</doc>
