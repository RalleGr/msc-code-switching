<doc id="21164" url="https://en.wikipedia.org/wiki?curid=21164" title="Drug policy of the Netherlands">
Drug policy of the Netherlands

While recreational use, possession and trade of non-medicinal drugs described by the Opium Law are all technically illegal under Dutch law, official policy since the late 20th century has been to openly tolerate all recreational use while tolerating the other two under certain circumstances. This pragmatic approach was motivated by the idea that a drug-free Dutch society is unrealistic and unattainable, and efforts would be better spent trying to minimize harm caused by recreational drug use. As a result of this gedoogbeleid (lit. "tolerance policy" or "policy of tolerance"), the Netherlands is typically seen as much more tolerant of drugs than most other countries.

Legal distinctions are made in the Opium Law between drugs with a low risk of harm and/or addiction, called soft drugs, and drugs with a high risk of harm and/or addiction, called hard drugs. Soft drugs include hash, marijuana, sleeping pills and sedatives, while hard drugs include heroin, cocaine, amphetamine, LSD and ecstasy. Policy has been to largely tolerate the sale of soft drugs while strongly suppressing the sale, circulation and use of hard drugs, effectively separating it into two markets. Establishments that have been permitted to sell soft drugs under certain circumstances are called "coffee shops". Laws established in January 2013 required visitors of coffee shops to be Dutch residents, but these laws were only applied in Zeeland, North Brabant and Limburg after much local criticism. Possession of a soft drug for personal use in quantities below a certain threshold (5 grams of cannabis or 5 cannabis plants) is tolerated, but larger quantities or possession of hard drugs may lead to prosecution. Prosecution for possession, trade and (in some rare cases) use are typically handled by the municipal government except where large-scale criminal activity is suspected.

Notably absent from toleration of drugs is its production, particularly the cultivation of cannabis. This has led to a seemingly paradoxical system where coffee shops are allowed to buy and sell soft drugs but where production is nearly always punished. Because coffee shops have to get their goods from somewhere, criticism has been raised over the years against continued prosecution of soft drug producers. It was first challenged in court in 2014 when a judge found two people guilty of producing cannabis in large quantities but refused to punish them. A significant change occurred in early 2017, when a slight majority in the House of Representatives allowed for a law to pass that would partly legalize production of cannabis. In late 2017, the newly formed coalition announced that they would seek to implement an experimental new system in certain cities where coffee shops could legally acquire cannabis from a state-appointed producer.

While the legalization of cannabis remains controversial, the introduction of heroin-assisted treatment in 1998 has been lauded for considerably improving the health and social situation of opiate-dependent patients in the Netherlands.

Large-scale dealing, production, import and export are prosecuted to the fullest extent of the law, even if it does not supply end users or "coffeeshops" with more than the allowed amounts. Exactly how coffeeshops get their supplies is rarely investigated, however. The average concentration of THC in the cannabis sold in coffeeshops has increased from 9% in 1998 to 18% in 2005. This means that less plant material has to be consumed to achieve the same effect. One of the reasons is plant breeding and use of greenhouse technology for illegal growing of cannabis in Netherlands.
The former minister of Justice Piet Hein Donner announced in June 2007 that cultivation of cannabis shall continue to be illegal.

The drug policy of the Netherlands is marked by its distinguishing between so called soft and hard drugs. An often used argument is that alcohol, which is claimed by some scientists as a hard drug, is legal and a soft drug cannot be more dangerous to society if it is controlled. This may refer to the Prohibition in the 1920s, when the U.S. government decided to ban all alcohol. Prohibition created a golden opportunity for organized crime syndicates to smuggle alcohol, and as a result the syndicates were able to gain considerable power in some major cities.
Cannabis remains a controlled substance in the Netherlands and both possession and production for personal use are still misdemeanors, punishable by fines. Coffeeshops are also technically illegal but are flourishing nonetheless. However, a policy of non-enforcement has led to a situation where reliance upon non-enforcement has become common, and because of this the courts have ruled against the government when individual cases were prosecuted.

This is because the Dutch Ministry of Justice applies a "gedoogbeleid" (tolerance policy) with regard to the category of soft drugs: an official set of guidelines telling public prosecutors under which circumstances offenders should not be prosecuted. This is a more official version of a common practice in other European countries wherein law enforcement sets priorities regarding offenses on which it is important enough to spend limited resources.

According to current "gedoogbeleid" the possession of a maximum amount of five grams cannabis for personal use is not prosecuted. Cultivation is treated in a similar way. Cultivation of 5 plants or less is usually not prosecuted when they are renounced by the cultivator.

Proponents of "gedoogbeleid" argue that such a policy practices more consistency in legal protection than without it. Opponents of the Dutch drug policy either call for full legalization, or argue that laws should penalize morally wrong or deviant behavior, whether enforceable or not. In the Dutch courts, however, it has long been determined that the institutionalized non-enforcement of statutes with well defined limits constitutes "de facto" decriminalization. The statutes are kept on the books mainly due to international pressure and in adherence with international treaties. A November 2008 poll showed that a 60% majority of the Dutch population support the legalisation of soft drugs. The same poll showed that 85% supported closing of all cannabis coffeeshops within 250 meters walking distance from schools.

Importing and exporting of any classified drug is a serious offence. The penalty can run up to 12 to 16 years if it is hard drug trade, maximum 4 years for import or export of large quantities of cannabis. It is prohibited to operate a motor vehicle while under the influence of any drug that affects driving ability to such an extent that you are unable to drive properly. (Section 8 of the 1994 Road Traffic Act section 1). The Dutch police have the right to do a drug test if they suspect influenced driving. For example, anybody involved in a traffic accident may be tested. Causing an accident that inflicts bodily harm, while under influence of any drug, is seen as a crime that may be punished by up to 3 years in prison (9 years in case of a fatal accident). Suspension of driving license is also normal in such a case (maximum 5 years). Schiphol, a large international airport near Amsterdam, has long practiced a zero tolerance policy regarding airline passengers carrying drugs. In 2006 there were 20,769 drug crimes registered by public prosecutors and 4,392 persons received an unconditional prison sentence The rate of imprisonment for drug crimes is about the same as in Sweden, which has a zero tolerance policy for drug crimes.

Despite the high priority given by the Dutch government to fighting illegal drug trafficking, the Netherlands continue to be an important transit point for drugs entering Europe. The Netherlands is a major producer and leading distributor of cannabis, heroin, cocaine, amphetamines and other synthetic drugs, and a medium consumer of illicit drugs. Despite the crackdown by Interpol on traffic and illicit manufacture of temazepam, the country has also become a major exporter of illicit temazepam of the jelly variety, trafficking it to the United Kingdom and other European nations. The government has intensified cooperation with neighbouring countries and stepped up border controls. In recent years, it also introduced so-called 100% checks and bodyscans at Schiphol Airport on incoming flights from Dutch overseas territories Aruba and Netherlands Antilles to prevent importing cocaine by means of swallowing balloons by mules.

Although drug use, as opposed to trafficking, is seen primarily as a public health issue, responsibility for drug policy is shared by both the Ministry of Health, Welfare, and Sports, and the Ministry of Justice.

The Netherlands spends more than €130 million annually on facilities for addicts, of which about fifty percent goes to drug addicts. The Netherlands has extensive demand reduction programs, reaching about ninety percent of the country's 25,000 to 28,000 hard drug users. The number of hard drug addicts has stabilized in the past few years and their average age has risen to 38 years, which is generally seen as a positive trend. Notably, the number of drug-related deaths in the country remains amongst the lowest in Europe.

On 27 November 2003, the Dutch Justice Minister Piet Hein Donner announced that his government was considering rules under which coffeeshops would only be allowed to sell soft drugs to Dutch residents in order to satisfy both European neighbors' concerns about the influx of drugs from the Netherlands, as well as those of Netherlands border town residents unhappy with the influx of "drug tourists" from elsewhere in Europe. The European Court of Justice ruled in December 2010 that Dutch authorities can ban coffeeshops from selling cannabis to foreigners. The EU court said the southern Dutch city of Maastricht was within its rights when it introduced a "weed passport" in 2005 to prevent foreigners from entering cafés that sell cannabis.

In 2010 the owner of Netherlands's largest cannabis selling coffeeshop was fined 10 million euros for breaking drug laws by keeping more than the tolerated amount of cannabis in the shop. He was also sentenced to a 16-week prison term.

Criminal investigations into more serious forms of organized crime mainly involve drugs (72%). Most of these are investigations of hard drug crime (specifically cocaine and synthetic drugs) although the number of soft drug cases is rising and currently accounts for 69% of criminal investigations.

In a study of the levels of cannabis, cocaine, MDMA, methamphetamine and other amphetamine in wastewater from 42 major cities in Europe Amsterdam came near the top of the list in every category but methamphetamine.

The Netherlands tolerates the sale of soft drugs in ‘coffee shops’. A coffee shop is an establishment where cannabis may be sold subject to certain strict conditions, but no alcoholic drinks may be sold or consumed. The Dutch government does not prosecute members of the public for possession or use of small quantities of soft drugs.

In the province of North-Brabant in the south of the Netherlands, the organized crime organizations form the main producer of MDMA, amphetamine and cannabis in Europe. Together with the proximity of the ports of Antwerp and especially Rotterdam where heroin and cocaine enter the European continent, this causes these substances to be readily available for a relative low price. Therefore, there is a large quantity drugs of a relative high purity/quality available. This means that users will not have to rely on more polluted substances with greater health risks. Together with an approach that focuses on easily accessible health care, harm reduction and prevention, this causes the medical condition of the Dutch addicts to be less severe than that of many other countries.

The Netherlands is a party to the 1961 Single Convention on Narcotic Drugs, the 1971 Convention on Psychotropic Substances, and the 1988 United Nations Convention Against Illicit Traffic in Narcotic Drugs and Psychotropic Substances. The 1961 convention prohibits cultivation and trade of naturally occurring drugs such as cannabis; the 1971 treaty bans the manufacture and trafficking of synthetic drugs such as barbiturates and amphetamines; and the 1988 convention requires states to criminalize illicit drug possession:
Subject to its constitutional principles and the basic concepts of its legal system, each Party shall adopt such measures as may be necessary to establish as a criminal offence under its domestic law, when committed intentionally, the possession, purchase or cultivation of narcotic drugs or psychotropic substances for personal consumption contrary to the provisions of the 1961 Convention, the 1961 Convention as amended or the 1971 Convention.
The International Narcotics Control Board typically interprets this provision to mean that states must prosecute drug possession offenses. The conventions clearly state that controlled substances are to be restricted to scientific and medical uses. However, Cindy Fazey, former Chief of Demand Reduction for the United Nations Drug Control Programme, believes that the treaties have enough ambiguities and loopholes to allow some room to maneuver. In her report entitled "The Mechanics and Dynamics of the UN System for International Drug Control", she notes:
Many countries have now decided not to use the full weight of criminal sanctions against people who are in possession of drugs that are for their personal consumption. The Conventions say that there must be an offence under domestic criminal law, it does not say that the law has to be enforced, or that when it is what sanctions should apply. . . . Despite such grey areas latitude is by no means unlimited. The centrality of the principle of limiting narcotic and psychotropic drugs for medical and scientific purposes leaves no room for the legal possibility of recreational use. . . . Nations may currently be pushing the boundaries of the international system, but the pursuit of any action to formally legalize non-medical and non-scientific drug use would require either treaty revision or a complete or partial withdrawal from the current regime.
The Dutch policy of keeping anti-drug laws on the books while limiting enforcement of certain offenses is carefully designed to reduce harm while still complying with the letter of international drug control treaties. This is necessary in order to avoid criticism from the International Narcotics Board, which historically has taken a dim view of any moves to relax official drug policy. In their annual report, the Board has criticised many governments, including Canada, for permitting the medicinal use of cannabis, Australia for providing injecting rooms and the United Kingdom for proposing to downgrade the classification of cannabis, which it has since done (although this change was reversed by the Home Secretary on 7 May 2008 against the advice of its own commissioned report).

The liberal drug policy of the authorities in the Netherlands especially led to problems in "border hot spots" that attracted "drug tourism" as well as trafficking and related law enforcement problems in towns like Enschede in the East and Terneuzen, Venlo, Maastricht and Heerlen in the South. In 2006, Gerd Leers, then mayor of the border city of Maastricht, on the Dutch-Belgian border, criticised the current policy as inconsistent, by recording a song with the Dutch punk rock band De Heideroosjes. By allowing possession and retail sales of cannabis, but not cultivation or wholesale, the government creates numerous problems of crime and public safety, he alleges, and therefore he would like to switch to either legalising and regulating production, or to the full repression that his party (CDA) officially advocates. The latter suggestion has widely been interpreted as rhetorical. Leers's comments have garnered support from other local authorities and put the cultivation issue back on the agenda.

In November 2008, Pieter van Geel, the leader of the CDA (Christian Democrats) in the Dutch parliament, called for a ban on the cafés where cannabis is sold. He said the practice of allowing so-called coffeeshops to operate had failed. The CDA had the support of its smaller coalition partner, the CU (ChristenUnie), but the third party in government, PvdA (Labour), opposed. The coalition agreement worked out by the three coalition parties in 2007 stated that there would be no change in the policy of tolerance. Prominent CDA member Gerd Leers spoke out against him: cannabis users who now cause no trouble would be viewed as criminals if an outright ban was to be implemented. Van Geel later said that he respected the coalition agreement and would not press for a ban during the current government's tenure.

27 "coffeeshops" selling cannabis in Rotterdam, all within 200 metres from schools, were ordered to close down by 2009. This was nearly half of the "coffeeshops" that operated within its municipality. This was due to the new policy of city mayor Ivo Opstelten and the town council. The higher levels of the active ingredient in cannabis in Netherlands create a growing opposition to the traditional Dutch view of cannabis as a relatively innocent soft drug. Supporters of "coffeeshops" state that such claims are often exaggerated and ignore the fact that higher content means a user needs to use less of the plant to get the desired effects, making it in effect safer. Dutch research has however shown that an increase of THC content also increase the occurrence of impaired psychomotor skills, particularly among younger or inexperienced cannabis smokers, who do not adapt their smoking-style to the higher THC content. Closing of "coffeeshops" is not unique to Rotterdam. Many other towns have done the same in the last 10 years.

In 2008, the municipality of Utrecht imposed a Zero Tolerance Policy to all events like the big dance party Trance Energy held in Jaarbeurs. However, such zero-tolerance policy at dance parties are now becoming common in the Netherlands and are even stricter in cities like Arnhem.

The two towns Roosendaal and Bergen op Zoom announced in October 2008 that they would start closing all "coffeeshops", each week visited by up to 25,000 French and Belgian drug tourists, with closures beginning in February 2009.

In May 2011 the Dutch government announced that tourist are to be banned from Dutch coffeeshops, starting in the southern provinces and at the end of 2011 in the rest of the country. In a letter to the parliament, the Dutch health and justice ministers said that, "In order to tackle the nuisance and criminality associated with coffeeshops and drug trafficking, the open-door policy of coffeeshops will end".

A government committee delivered in June 2011 a report about Cannabis to the Dutch government. It includes a proposal that cannabis with more than 15 percent THC should be labeled as hard drugs. Higher concentrations of THC and drug tourism have challenged the current policy and led to a re-examination of the current approach; e.g. ban of all sales of cannabis to tourists in coffeeshops from end of 2011 was proposed but currently only the border city of Maastricht has adopted the measure in order to test out its feasibility. According to the initial measure, starting in 2012, each coffeeshop was to operate like a private club with some 1,000 to 1,500 members. In order to qualify for a membership card, applicants would have to be adult Dutch citizens, membership was only to be allowed in one club.

In Amsterdam 26 coffeeshops in the De Wallen area were ordered to close their doors between 1 September 2012 and 31 August 2015.

A Dutch judge has ruled that tourists can legally be banned from entering cannabis cafés, as part of new restrictions which come into force in 2012.

A study conducted by the European Monitoring Centre of Drugs and Drug Addiction report that 25.3% of Irish people smoked cannabis at some stage in their life, and 25.7% of Dutch people have tried cannabis.

In October 2007, the prohibition of hallucinogenic or "magic mushrooms" was announced by the Dutch authorities.

On 25 April 2008, the Dutch government, backed by a majority of members of parliament, decided to ban cultivation and use of all magic mushrooms. Amsterdam mayor Job Cohen proposed a three-day cooling period in which clients would be informed three days before actually procuring the mushrooms and if they would still like to go through with it they could pick up their spores from the smart shop.
The ban has been considered a retreat from liberal drug policies. This followed a few deadly incidents mostly involving tourists. These deaths were not directly caused by the use of the drug "per se", but by deadly accidents occurring while under the influence of magic mushrooms.

As of 1 December 2008, all psychedelic mushrooms are banned. However, schlerotia (what are termed as "truffles"), mushroom spores, and active mycellium cultures remained legal and are readily available in the "smartshops", these truffles have the same effect as the "magic mushrooms", the stores in the Dutch cities that sell legal drugs, herbs and related gadgets.

The relatively recent increase in the cocaine trafficking business has been largely focused on the Caribbean area. Since early 2003, a special law court with prison facilities has been operational at Schiphol airport. Since the beginning of 2005, there has been 100% control of all flights from key countries in the Caribbean. In 2004, an average of 290 drug couriers per month were arrested, decreasing to 80 per month by early 2006.





</doc>
<doc id="21168" url="https://en.wikipedia.org/wiki?curid=21168" title="2001 in the Netherlands">
2001 in the Netherlands

This article lists some of the events that took place in the Netherlands in 2001.



















</doc>
<doc id="21170" url="https://en.wikipedia.org/wiki?curid=21170" title="Numeral system">
Numeral system

A numeral system (or system of numeration) is a writing system for expressing numbers; that is, a mathematical notation for representing numbers of a given set, using digits or other symbols in a consistent manner.

The same sequence of symbols may represent different numbers in different numeral systems. For example, "11" represents the number "eleven" in the decimal numeral system (used in common life), the number "three" in the binary numeral system (used in computers), and the number two in the unary numeral system (e.g. used in tallying scores).

The number the numeral represents is called its value.

Ideally, a numeral system will:

For example, the usual decimal representation of whole numbers gives every nonzero whole number a unique representation as a finite sequence of digits, beginning with a non-zero digit. However, when decimal representation is used for the rational or real numbers, such numbers, in general, have an infinite number of representations, for example 2.31 can also be written as 2.310, 2.3100000, 2.309999999..., etc., all of which have the same meaning except for some scientific and other contexts where greater precision is implied by a larger number of figures shown.

Numeral systems are sometimes called "number systems", but that name is ambiguous, as it could refer to different systems of numbers, such as the system of real numbers, the system of complex numbers, the system of "p"-adic numbers, etc. Such systems are, however, not the topic of this article.

The most commonly used system of numerals is the Hindu–Arabic numeral system. Two Indian mathematicians are credited with developing it. Aryabhata of Kusumapura developed the place-value notation in the 5th century and a century later Brahmagupta introduced the symbol for zero. The numeral system and the zero concept, developed by the Hindus in India, slowly spread to other surrounding regions like Arabia due to their commercial and military activities with India. The Hindu-Arabic numeral system then spread to Europe along with many other science knowledge and due to merchants trading and using a stable simple numeral system. The Western world modified them and called them the Arabic numerals, as they learned them from the Arabs. Hence the current western numeral system is the modified version of the Hindu numeral system developed in India. It also exhibits a great similarity to the Sanskrit–Devanagari notation, which is still used in India and neighbouring Nepal.

The simplest numeral system is the unary numeral system, in which every natural number is represented by a corresponding number of symbols. If the symbol / is chosen, for example, then the number seven would be represented by ///////. Tally marks represent one such system still in common use. The unary system is only useful for small numbers, although it plays an important role in theoretical computer science. Elias gamma coding, which is commonly used in data compression, expresses arbitrary-sized numbers by using unary to indicate the length of a binary numeral.

The unary notation can be abbreviated by introducing different symbols for certain new values. Very commonly, these values are powers of 10; so for instance, if / stands for one, − for ten and + for 100, then the number 304 can be compactly represented as +++ //// and the number 123 as + − − /// without any need for zero. This is called sign-value notation. The ancient Egyptian numeral system was of this type, and the Roman numeral system was a modification of this idea.

More useful still are systems which employ special abbreviations for repetitions of symbols; for example, using the first nine letters of the alphabet for these abbreviations, with A standing for "one occurrence", B "two occurrences", and so on, one could then write C+ D/ for the number 304. This system is used when writing Chinese numerals and other East Asian numerals based on Chinese. The number system of the English language is of this type ("three hundred [and] four"), as are those of other spoken languages, regardless of what written systems they have adopted. However, many languages use mixtures of bases, and other features, for instance 79 in French is "soixante dix-neuf" () and in Welsh is "pedwar ar bymtheg a thrigain" () or (somewhat archaic) "pedwar ugain namyn un" (). In English, one could say "four score less one", as in the famous Gettysburg Address representing "87 years ago" as "four score and seven years ago".

More elegant is a "positional system", also known as place-value notation. Again working in base 10, ten different digits 0, ..., 9 are used and the position of a digit is used to signify the power of ten that the digit is to be multiplied with, as in or more precisely . Zero, which is not needed in the other systems, is of crucial importance here, in order to be able to "skip" a power. The Hindu–Arabic numeral system, which originated in India and is now used throughout the world, is a positional base 10 system.

Arithmetic is much easier in positional systems than in the earlier additive ones; furthermore, additive systems need a large number of different symbols for the different powers of 10; a positional system needs only ten different symbols (assuming that it uses base 10).

The positional decimal system is presently universally used in human writing. The base 1000 is also used (albeit not universally), by grouping the digits and considering a sequence of three decimal digits as a single digit. This is the meaning of the common notation 1,000,234,567 used for very large numbers.

In computers, the main numeral systems are based on the positional system in base 2 (binary numeral system), with two binary digits, 0 and 1. Positional systems obtained by grouping binary digits by three (octal numeral system) or four (hexadecimal numeral system) are commonly used. For very large integers, bases 2 or 2 (grouping binary digits by 32 or 64, the length of the machine word) are used, as, for example, in GMP.

In certain biological systems, the unary coding system is employed. Unary numerals used in the neural circuits responsible for birdsong production. The nucleus in the brain of the songbirds that plays a part in both the learning and the production of bird song is the HVC (high vocal center). The command signals for different notes in the birdsong emanate from different points in the HVC. This coding works as space coding which is an efficient strategy for biological circuits due to its inherent simplicity and robustness.

The numerals used when writing numbers with digits or symbols can be divided into two types that might be called the arithmetic numerals (0, 1, 2, 3, 4, 5, 6, 7, 8, 9) and the geometric numerals (1, 10, 100, 1000, 10000 ...), respectively. The sign-value systems use only the geometric numerals and the positional systems use only the arithmetic numerals. A sign-value system does not need arithmetic numerals because they are made by repetition (except for the Ionic system), and a positional system does not need geometric numerals because they are made by position. However, the spoken language uses "both" arithmetic and geometric numerals.

In certain areas of computer science, a modified base "k" positional system is used, called bijective numeration, with digits 1, 2, ..., "k" (), and zero being represented by an empty string. This establishes a bijection between the set of all such digit-strings and the set of non-negative integers, avoiding the non-uniqueness caused by leading zeros. Bijective base-"k" numeration is also called "k"-adic notation, not to be confused with "p"-adic numbers. Bijective base 1 is the same as unary.

In a positional base "b" numeral system (with "b" a natural number greater than 1 known as the radix), "b" basic symbols (or digits) corresponding to the first "b" natural numbers including zero are used. To generate the rest of the numerals, the position of the symbol in the figure is used. The symbol in the last position has its own value, and as it moves to the left its value is multiplied by "b".

For example, in the decimal system (base 10), the numeral 4327 means , noting that .

In general, if "b" is the base, one writes a number in the numeral system of base "b" by expressing it in the form and writing the enumerated digits in descending order. The digits are natural numbers between 0 and , inclusive.

If a text (such as this one) discusses multiple bases, and if ambiguity exists, the base (itself represented in base 10) is added in subscript to the right of the number, like this: number. Unless specified by context, numbers without subscript are considered to be decimal.

By using a dot to divide the digits into two groups, one can also write fractions in the positional system. For example, the base 2 numeral 10.11 denotes .

In general, numbers in the base "b" system are of the form:

The numbers "b" and "b" are the weights of the corresponding digits. The position "k" is the logarithm of the corresponding weight "w", that is formula_2. The highest used position is close to the order of magnitude of the number.

The number of tally marks required in the unary numeral system for "describing the weight" would have been w. In the positional system, the number of digits required to describe it is only formula_3, for "k" ≥ 0. For example, to describe the weight 1000 then four digits are needed because formula_4. The number of digits required to "describe the position" is formula_5 (in positions 1, 10, 100... only for simplicity in the decimal example).

A number has a terminating or repeating expansion if and only if it is rational; this does not depend on the base. A number that terminates in one base may repeat in another (thus ). An irrational number stays aperiodic (with an infinite number of non-repeating digits) in all integral bases. Thus, for example in base 2, can be written as the aperiodic 11.001001000011111...

Putting overscores, , or dots, "ṅ", above the common digits is a convention used to represent repeating rational expansions. Thus:

If "b" = "p" is a prime number, one can define base-"p" numerals whose expansion to the left never stops; these are called the "p"-adic numbers.

More general is using a mixed radix notation (here written little-endian) like formula_7 for formula_8, etc.

This is used in punycode, one aspect of which is the representation of a sequence of non-negative integers of arbitrary size in the form of a sequence without delimiters, of "digits" from a collection of 36: a–z and 0–9, representing 0–25 and 26–35 respectively. A digit lower than a threshold value marks that it is the most-significant digit, hence the end of the number. The threshold value depends on the position in the number. For example, if the threshold value for the first digit is b (i.e. 1) then a (i.e. 0) marks the end of the number (it has just one digit), so in numbers of more than one digit, range is only b–9 (1–35), therefore the weight "b" is 35 instead of 36. Suppose the threshold values for the second and third digits are c (2), then the third digit has a weight 34 × 35 = 1190 and we have the following sequence:

a (0), ba (1), ca (2), .., 9a (35), bb (36), cb (37), .., 9b (70), bca (71), .., 99a (1260), bcb (1261), etc.

Unlike a regular based numeral system, there are numbers like 9b where 9 and b each represents 35; yet the representation is unique because ac and aca are not allowed – the a would terminate the number.

The flexibility in choosing threshold values allows optimization depending on the frequency of occurrence of numbers of various sizes.

The case with all threshold values equal to 1 corresponds to bijective numeration, where the zeros correspond to separators of numbers with digits which are non-zero.




</doc>
<doc id="21173" url="https://en.wikipedia.org/wiki?curid=21173" title="Natural language">
Natural language

In neuropsychology, linguistics, and the philosophy of language, a natural language or ordinary language is any language that has evolved naturally in humans through use and repetition without conscious planning or premeditation. Natural languages can take different forms, such as speech or signing. They are distinguished from constructed and formal languages such as those used to program computers or to study logic.

Though the exact definition varies between scholars, natural language can broadly be defined in contrast to artificial or constructed languages (such as computer programming languages and international auxiliary languages) and to other communication systems in nature. Examples of such communication systems include bees' waggle dance and whale song, to which researchers have found or applied the linguistic cognates of dialect and even syntax. However, classification of animal communication systems as languages is controversial.

All language varieties of world languages are natural languages, although some varieties are subject to greater degrees of published prescriptivism or language regulation than others. Thus nonstandard dialects can be viewed as a wild type in comparison with standard languages. But even an official language with a regulating academy, such as Standard French with the French Academy, is classified as a natural language (for example, in the field of natural language processing), as its prescriptive points do not make it either constructed enough to be classified as a constructed language or controlled enough to be classified as a controlled natural language.

Controlled natural languages are subsets of natural languages whose grammars and dictionaries have been restricted in order to reduce or eliminate both ambiguity and complexity (for instance, by cutting down on rarely used superlative or adverbial forms or irregular verbs). The purpose behind the development and implementation of a controlled natural language typically is to aid non-native speakers of a natural language in understanding it, or to ease computer processing of a natural language. An example of a widely used controlled natural language is Simplified English, which was originally developed for aerospace industry maintenance manuals.

Constructed international auxiliary languages such as Esperanto and Interlingua (even those that have native speakers) are not generally considered natural languages. Natural languages have been used to communicate and have evolved in a natural way, whereas Esperanto was designed by L. L. Zamenhof selecting elements from natural languages, not grown from natural fluctuations in vocabulary and syntax. Some natural languages have become naturally "standardized" by children's natural tendency to correct for illogical grammatical structures in their parents' speech, which can be seen in the development of pidgin languages into creole languages (as explained by Steven Pinker in "The Language Instinct"), but this is not the case in many languages, including constructed languages such as Esperanto, where strict rules are in place as an attempt to consciously remove such irregularities. The possible exception to this are true native speakers of such languages. More substantive basis for this designation is that the vocabulary, grammar, and orthography of Interlingua are natural; they have been standardized and presented by a linguistic research body, but they predated it and are not themselves considered a product of human invention. Most experts, however, consider Interlingua to be naturalistic rather than natural. Latino sine flexione, a second naturalistic auxiliary language, is also naturalistic in content but is no longer widely spoken.




</doc>
<doc id="21174" url="https://en.wikipedia.org/wiki?curid=21174" title="Nanook of the North">
Nanook of the North

Nanook of the North (also known as Nanook of the North: A Story Of Life and Love In the Actual Arctic) is a 1922 American silent documentary film by Robert J. Flaherty, with elements of docudrama, at a time when the concept of separating films into documentary and drama did not yet exist.

In the tradition of what would later be called salvage ethnography, Flaherty captured the struggles of the Inuk man named Nanook and his family in the Canadian Arctic. Some have criticized Flaherty for staging several sequences, but the film is generally viewed as standing "alone in its stark regard for the courage and ingenuity of its heroes."

In 1989, "Nanook of the North" was one of the first 25 films selected by the Library of Congress for preservation in the United States National Film Registry for being "culturally, historically, or aesthetically significant". 

The documentary follows the lives of an Inuk, Nanook, and his family as they travel, search for food, and trade in the Ungava Peninsula of northern Quebec, Canada. Nanook; his wife, Nyla; and their family are introduced as fearless heroes who endure rigors no other race could survive. The audience sees Nanook, often with his family, hunt a walrus, build an igloo, go about his day, and perform other tasks.

In 1910 Flaherty was hired as an explorer and prospector along the Hudson Bay for the Canadian Pacific Railway. Learning about the lands and people there, Flaherty decided to bring a camera with him on his third expedition in 1913, but knowing nothing about film, Flaherty took a three-week course on cinematography in Rochester, New York.

Using a Bell & Howell camera, a portable developing and printing machine, and some lighting equipment, Flaherty spent 1914 and 1915 shooting hours of film of Inuit life. By 1916, Flaherty had enough footage that he began test screenings and was met with wide enthusiasm. However, in 1916, Flaherty dropped a cigarette onto the original camera negative (which was highly flammable nitrate stock) and lost 30,000 feet of film. With his first attempt ruined, Flaherty decided to not only return for new footage, but also to refocus the film on one Inuit family as he felt his earlier footage was too much of travelogue. Spending four years raising money, Flaherty was eventually funded by French fur company Revillon Frères and returned to the North and shot from August 1920 to August 1921. As a main character, Flaherty chose the celebrated hunter of the Itivimuit tribe, Allakariallak. The full collaboration of the Inuit was key to Flaherty's success as the Inuit were his film crew and many of them knew his camera better than he did.

Flaherty has been criticized for deceptively portraying staged events as reality. "Nanook" was in fact named Allakariallak (). Flaherty chose this nickname because of its seeming genuineness which makes it more marketable to Euro-American audiences. The "wife" shown in the film was not really his wife. According to Charlie Nayoumealuk, who was interviewed in "Nanook Revisited" (1990), "the two women in "Nanook" - Nyla (Alice [?] Nuvalinga) and Cunayou (whose real name we do not know) were not Allakariallak's wives, but were in fact common-law wives of Flaherty." And although Allakariallak normally used a gun when hunting, Flaherty encouraged him to hunt after the fashion of his recent ancestors in order to capture the way the Inuit lived before European colonization of the Americas. Flaherty also exaggerated the peril to Inuit hunters with his claim, often repeated, that Allakariallak had died of starvation less than two years after the film was completed, whereas in fact he died at home, likely of tuberculosis.

Furthermore, it has been criticized for portraying Inuit people as subhuman Arctic beings, without technology or culture which reproduces the historical image that situates them outside modern history. It was also criticized for comparing Inuit people to animals. The film is considered to be an artifact of popular culture at the time and also a result of a historical fascination for Inuit performers in exhibitions, zoos, fairs, museums and early cinema.

Flaherty defended his work by stating, "one often has to distort a thing in order to catch its true spirit." Later filmmakers have pointed out that the only cameras available to Flaherty at the time were both large and immobile, making it impossible to effectively capture most interior shots or unstructured exterior scenes without significantly modifying the environment and subject action.

The building of the igloo is one of the most celebrated sequences in the film, but interior photography presented a problem. Building an igloo large enough for a camera to enter resulted in the dome collapsing, and when they finally succeeded in making the igloo it was too dark for photography. Instead, the images of the inside of the igloo in the film were actually shot in a special three-walled igloo for Flaherty's bulky camera so that there would be enough light for it to capture interior shots.

In the "Trade Post of the White Man" scene, Nanook and his family arrive in a kayak at the trading post and one family member after another emerge from a small kayak, akin to a clown car at the circus. Going to trade his hunt from the year, including the skins of foxes, seals, and polar bears, Nanook comes in contact with the white man and there is a funny interaction as the two cultures meet. The trader plays music on a gramophone and tries to explain how a man 'cans' his voice. Bending forward and staring at the machine, Nanook puts his ear closer as the trader cranks the mechanism again. The trader removes the record and hands it to Nanook who at first peers at it and then puts it in his mouth and bites it. The scene is meant to be a comical one as the audience laughs at the naivete of Nanook and people isolated from Western culture. In truth, the scene was entirely scripted and Allakariallak knew what a gramophone was.

It has been noted that in the 1920s, when Nanook was filmed, the Inuit had already begun integrating the use of Western clothing and were using rifles to hunt rather than harpoons, but this does not negate that the Inuit knew how to make traditional clothing from animals found in their environment, could still fashion traditional weapons and were perfectly able to make use of them if found to be preferable for a given situation.
As the first "nonfiction" work of its scale, "Nanook of the North" was ground-breaking cinema. It captured many authentic details of a culture little known to outsiders, and it was filmed in a remote location. Hailed almost unanimously by critics, the film was a box-office success in the United States and abroad. In the following years, many others would try to follow Flaherty's success with "primitive peoples" films. In 2005, film critic Roger Ebert described the film's central figure, Nanook, as "one of the most vital and unforgettable human beings ever recorded on film." In a 2014 "Sight and Sound" poll, film critics voted "Nanook of the North" the seventh-best documentary film of all time.

On review aggregator website Rotten Tomatoes, the film holds an approval rating of 100% based on 30 reviews, with an average rating of 8.68/10. The site's critics' consensus reads: "An enthralling documentary and a visual feat, "Nanook of the North" fascinates with its dramatic depiction of life in an extremely hostile environment."

At the time, few documentaries had been filmed and there was little precedent to guide Flaherty's work. Since Flaherty's time, staging, attempting to steer documentary action, or presenting re-enactment as naturally captured footage has come to be considered unethical.

In its earliest years (approx. 1895–1902), film production was dominated by actualities—short pictures of real people in real places. Robert Flaherty's great innovation was simply to combine the two forms of actuality, infusing the exotic journey with the details of indigenous work and play and life.

In 1999, "Nanook of the North" was digitally remastered and released on DVD by The Criterion Collection. It includes an interview with Flaherty's widow (and "Nanook of the North" co-editor), Frances Flaherty, photos from Flaherty's trip to the arctic, and excerpts from a TV documentary, "Flaherty and Film." In 2013, Flicker Alley released a remastered Blu-ray version that includes six other arctic films.







</doc>
<doc id="21175" url="https://en.wikipedia.org/wiki?curid=21175" title="Nitrogen">
Nitrogen

Nitrogen is the chemical element with the symbol N and atomic number 7. It was first discovered and isolated by Scottish physician Daniel Rutherford in 1772. Although Carl Wilhelm Scheele and Henry Cavendish had independently done so at about the same time, Rutherford is generally accorded the credit because his work was published first. The name "nitrogène" was suggested by French chemist Jean-Antoine-Claude Chaptal in 1790 when it was found that nitrogen was present in nitric acid and nitrates. Antoine Lavoisier suggested instead the name "azote", from the Greek ἀζωτικός "no life", as it is an asphyxiant gas; this name is instead used in many languages, such as French, Italian, Russian, Romanian and Turkish, and appears in the English names of some nitrogen compounds such as hydrazine, azides and azo compounds.

Nitrogen is the lightest member of group 15 of the periodic table, often called the pnictogens. It is a common element in the universe, estimated at about seventh in total abundance in the Milky Way and the Solar System. At standard temperature and pressure, two atoms of the element bind to form dinitrogen, a colourless and odorless diatomic gas with the formula N. Dinitrogen forms about 78% of Earth's atmosphere, making it the most abundant uncombined element. Nitrogen occurs in all organisms, primarily in amino acids (and thus proteins), in the nucleic acids (DNA and RNA) and in the energy transfer molecule adenosine triphosphate. The human body contains about 3% nitrogen by mass, the fourth most abundant element in the body after oxygen, carbon, and hydrogen. The nitrogen cycle describes movement of the element from the air, into the biosphere and organic compounds, then back into the atmosphere.

Many industrially important compounds, such as ammonia, nitric acid, organic nitrates (propellants and explosives), and cyanides, contain nitrogen. The extremely strong triple bond in elemental nitrogen (N≡N), the second strongest bond in any diatomic molecule after carbon monoxide (CO), dominates nitrogen chemistry. This causes difficulty for both organisms and industry in converting N into useful compounds, but at the same time means that burning, exploding, or decomposing nitrogen compounds to form nitrogen gas releases large amounts of often useful energy. Synthetically produced ammonia and nitrates are key industrial fertilisers, and fertiliser nitrates are key pollutants in the eutrophication of water systems.

Apart from its use in fertilisers and energy-stores, nitrogen is a constituent of organic compounds as diverse as Kevlar used in high-strength fabric and cyanoacrylate used in superglue. Nitrogen is a constituent of every major pharmacological drug class, including antibiotics. Many drugs are mimics or prodrugs of natural nitrogen-containing signal molecules: for example, the organic nitrates nitroglycerin and nitroprusside control blood pressure by metabolizing into nitric oxide. Many notable nitrogen-containing drugs, such as the natural caffeine and morphine or the synthetic amphetamines, act on receptors of animal neurotransmitters.

Nitrogen compounds have a very long history, ammonium chloride having been known to Herodotus. They were well known by the Middle Ages. Alchemists knew nitric acid as "aqua fortis" (strong water), as well as other nitrogen compounds such as ammonium salts and nitrate salts. The mixture of nitric and hydrochloric acids was known as "aqua regia" (royal water), celebrated for its ability to dissolve gold, the king of metals.

The discovery of nitrogen is attributed to the Scottish physician Daniel Rutherford in 1772, who called it "noxious air". Though he did not recognise it as an entirely different chemical substance, he clearly distinguished it from Joseph Black's "fixed air", or carbon dioxide. The fact that there was a component of air that does not support combustion was clear to Rutherford, although he was not aware that it was an element. Nitrogen was also studied at about the same time by Carl Wilhelm Scheele, Henry Cavendish, and Joseph Priestley, who referred to it as "burnt air" or "phlogisticated air". Nitrogen gas was inert enough that Antoine Lavoisier referred to it as "mephitic air" or "azote", from the Greek word (azotikos), "no life". In an atmosphere of pure nitrogen, animals died and flames were extinguished. Though Lavoisier's name was not accepted in English, since it was pointed out that almost all gases (indeed, with the sole exception of oxygen) are mephitic, it is used in many languages (French, Italian, Portuguese, Polish, Russian, Albanian, Turkish, etc.; the German "Stickstoff" similarly refers to the same characteristic, viz. "ersticken" "to choke or suffocate") and still remains in English in the common names of many nitrogen compounds, such as hydrazine and compounds of the azide ion. Finally, it led to the name "pnictogens" for the group headed by nitrogen, from the Greek πνίγειν "to choke".

The English word nitrogen (1794) entered the language from the French "nitrogène", coined in 1790 by French chemist Jean-Antoine Chaptal (1756–1832), from the French "nitre" (potassium nitrate, also called saltpeter) and the French suffix "-gène", "producing", from the Greek -γενής (-genes, "begotten"). Chaptal's meaning was that nitrogen is the essential part of nitric acid, which in turn was produced from nitre. In earlier times, niter had been confused with Egyptian "natron" (sodium carbonate) – called νίτρον (nitron) in Greek – which, despite the name, contained no nitrate.

The earliest military, industrial, and agricultural applications of nitrogen compounds used saltpeter (sodium nitrate or potassium nitrate), most notably in gunpowder, and later as fertiliser. In 1910, Lord Rayleigh discovered that an electrical discharge in nitrogen gas produced "active nitrogen", a monatomic allotrope of nitrogen. The "whirling cloud of brilliant yellow light" produced by his apparatus reacted with mercury to produce explosive mercury nitride.

For a long time, sources of nitrogen compounds were limited. Natural sources originated either from biology or deposits of nitrates produced by atmospheric reactions. Nitrogen fixation by industrial processes like the Frank–Caro process (1895–1899) and Haber–Bosch process (1908–1913) eased this shortage of nitrogen compounds, to the extent that half of global food production (see Applications) now relies on synthetic nitrogen fertilisers. At the same time, use of the Ostwald process (1902) to produce nitrates from industrial nitrogen fixation allowed the large-scale industrial production of nitrates as feedstock in the manufacture of explosives in the World Wars of the 20th century.

A nitrogen atom has seven electrons. In the ground state, they are arranged in the electron configuration 1s2s2p2p2p. It therefore has five valence electrons in the 2s and 2p orbitals, three of which (the p-electrons) are unpaired. It has one of the highest electronegativities among the elements (3.04 on the Pauling scale), exceeded only by chlorine (3.16), oxygen (3.44), and fluorine (3.98). (The light noble gases, helium, neon, and argon, would presumably also be more electronegative, and in fact are on the Allen scale.) Following periodic trends, its single-bond covalent radius of 71 pm is smaller than those of boron (84 pm) and carbon (76 pm), while it is larger than those of oxygen (66 pm) and fluorine (57 pm). The nitride anion, N, is much larger at 146 pm, similar to that of the oxide (O: 140 pm) and fluoride (F: 133 pm) anions. The first three ionisation energies of nitrogen are 1.402, 2.856, and 4.577 MJ·mol, and the sum of the fourth and fifth is 16.920 MJ·mol. Due to these very high figures, nitrogen has no simple cationic chemistry.

The lack of radial nodes in the 2p subshell is directly responsible for many of the anomalous properties of the first row of the p-block, especially in nitrogen, oxygen, and fluorine. The 2p subshell is very small and has a very similar radius to the 2s shell, facilitating orbital hybridisation. It also results in very large electrostatic forces of attraction between the nucleus and the valence electrons in the 2s and 2p shells, resulting in very high electronegativities. Hypervalency is almost unknown in the 2p elements for the same reason, because the high electronegativity makes it difficult for a small nitrogen atom to be a central atom in an electron-rich three-center four-electron bond since it would tend to attract the electrons strongly to itself. Thus, despite nitrogen's position at the head of group 15 in the periodic table, its chemistry shows huge differences from that of its heavier congeners phosphorus, arsenic, antimony, and bismuth.

Nitrogen may be usefully compared to its horizontal neighbours carbon and oxygen as well as its vertical neighbours in the pnictogen column, phosphorus, arsenic, antimony, and bismuth. Although each period 2 element from lithium to oxygen shows some similarities to the period 3 element in the next group (from magnesium to chlorine; these are known as diagonal relationships), their degree drops off abruptly past the boron–silicon pair. The similarities of nitrogen to sulfur are mostly limited to sulfur nitride ring compounds when both elements are the only ones present.

Nitrogen does not share the proclivity of carbon for catenation. Like carbon, nitrogen tends to form ionic or metallic compounds with metals. Nitrogen forms an extensive series of nitrides with carbon, including those with chain-, graphitic-, and fullerenic-like structures.

It resembles oxygen with its high electronegativity and concomitant capability for hydrogen bonding and the ability to form coordination complexes by donating its lone pairs of electrons. There are some parallels between the chemistry of ammonia NH and water HO. For example, the capacity of both compounds to be protonated to give NH and HO or deprotonated to give NH and OH, with all of these able to be isolated in solid compounds.

Nitrogen shares with both its horizontal neighbours a preference for forming multiple bonds, typically with carbon, oxygen, or other nitrogen atoms, through p–p interactions. Thus, for example, nitrogen occurs as diatomic molecules and therefore has very much lower melting (−210 °C) and boiling points (−196 °C) than the rest of its group, as the N molecules are only held together by weak van der Waals interactions and there are very few electrons available to create significant instantaneous dipoles. This is not possible for its vertical neighbours; thus, the nitrogen oxides, nitrites, nitrates, nitro-, nitroso-, azo-, and diazo-compounds, azides, cyanates, thiocyanates, and imino-derivatives find no echo with phosphorus, arsenic, antimony, or bismuth. By the same token, however, the complexity of the phosphorus oxoacids finds no echo with nitrogen. Setting aside their differences, nitrogen and phosphorus form an extensive series of compounds with one another; these have chain, ring, and cage structures.

Nitrogen has two stable isotopes: N and N. The first is much more common, making up 99.634% of natural nitrogen, and the second (which is slightly heavier) makes up the remaining 0.366%. This leads to an atomic weight of around 14.007 u. Both of these stable isotopes are produced in the CNO cycle in stars, but N is more common as its neutron capture is the rate-limiting step. N is one of the five stable odd–odd nuclides (a nuclide having an odd number of protons and neutrons); the other four are H, Li, B, and Ta.

The relative abundance of N and N is practically constant in the atmosphere but can vary elsewhere, due to natural isotopic fractionation from biological redox reactions and the evaporation of natural ammonia or nitric acid. Biologically mediated reactions (e.g., assimilation, nitrification, and denitrification) strongly control nitrogen dynamics in the soil. These reactions typically result in N enrichment of the substrate and depletion of the product.

The heavy isotope N was first discovered by S. M. Naudé in 1929, soon after heavy isotopes of the neighbouring elements oxygen and carbon were discovered. It presents one of the lowest thermal neutron capture cross-sections of all isotopes. It is frequently used in nuclear magnetic resonance (NMR) spectroscopy to determine the structures of nitrogen-containing molecules, due to its fractional nuclear spin of one-half, which offers advantages for NMR such as narrower line width. N, though also theoretically usable, has an integer nuclear spin of one and thus has a quadrupole moment that leads to wider and less useful spectra. N NMR nevertheless has complications not encountered in the more common H and C NMR spectroscopy. The low natural abundance of N (0.36%) significantly reduces sensitivity, a problem which is only exacerbated by its low gyromagnetic ratio, (only 10.14% that of H). As a result, the signal-to-noise ratio for H is about 300 times as much as that for N at the same magnetic field strength. This may be somewhat alleviated by isotopic enrichment of N by chemical exchange or fractional distillation. N-enriched compounds have the advantage that under standard conditions, they do not undergo chemical exchange of their nitrogen atoms with atmospheric nitrogen, unlike compounds with labelled hydrogen, carbon, and oxygen isotopes that must be kept away from the atmosphere. The N:N ratio is commonly used in stable isotope analysis in the fields of geochemistry, hydrology, paleoclimatology and paleoceanography, where it is called "δ"N.

Of the ten other isotopes produced synthetically, ranging from N to N, N has a half-life of ten minutes and the remaining isotopes have half-lives on the order of seconds (N and N) or milliseconds. No other nitrogen isotopes are possible as they would fall outside the nuclear drip lines, leaking out a proton or neutron. Given the half-life difference, N is the most important nitrogen radioisotope, being relatively long-lived enough to use in positron emission tomography (PET), although its half-life is still short and thus it must be produced at the venue of the PET, for example in a cyclotron via proton bombardment of O producing N and an alpha particle.

The radioisotope N is the dominant radionuclide in the coolant of pressurised water reactors or boiling water reactors during normal operation, and thus it is a sensitive and immediate indicator of leaks from the primary coolant system to the secondary steam cycle, and is the primary means of detection for such leaks. It is produced from O (in water) via an (n,p) reaction in which the O atom captures a neutron and expels a proton. It has a short half-life of about 7.1 s, but during its decay back to O produces high-energy gamma radiation (5 to 7 MeV). Because of this, access to the primary coolant piping in a pressurised water reactor must be restricted during reactor power operation.

Atomic nitrogen, also known as active nitrogen, is highly reactive, being a triradical with three unpaired electrons. Free nitrogen atoms easily react with most elements to form nitrides, and even when two free nitrogen atoms collide to produce an excited N molecule, they may release so much energy on collision with even such stable molecules as carbon dioxide and water to cause homolytic fission into radicals such as CO and O or OH and H. Atomic nitrogen is prepared by passing an electric discharge through nitrogen gas at 0.1–2 mmHg, which produces atomic nitrogen along with a peach-yellow emission that fades slowly as an afterglow for several minutes even after the discharge terminates.

Given the great reactivity of atomic nitrogen, elemental nitrogen usually occurs as molecular N, dinitrogen. This molecule is a colourless, odourless, and tasteless diamagnetic gas at standard conditions: it melts at −210 °C and boils at −196 °C. Dinitrogen is mostly unreactive at room temperature, but it will nevertheless react with lithium metal and some transition metal complexes. This is due to its bonding, which is unique among the diatomic elements at standard conditions in that it has an N≡N triple bond. Triple bonds have short bond lengths (in this case, 109.76 pm) and high dissociation energies (in this case, 945.41 kJ/mol), and are thus very strong, explaining dinitrogen's chemical inertness.

There are some theoretical indications that other nitrogen oligomers and polymers may be possible. If they could be synthesised, they may have potential applications as materials with a very high energy density, that could be used as powerful propellants or explosives. This is because they should all decompose to dinitrogen, whose N≡N triple bond (bond energy 946 kJ⋅mol) is much stronger than those of the N=N double bond (418 kJ⋅mol) or the N–N single bond (160 kJ⋅mol): indeed, the triple bond has more than thrice the energy of the single bond. (The opposite is true for the heavier pnictogens, which prefer polyatomic allotropes.) A great disadvantage is that most neutral polynitrogens are not expected to have a large barrier towards decomposition, and that the few exceptions would be even more challenging to synthesise than the long-sought but still unknown tetrahedrane. This stands in contrast to the well-characterised cationic and anionic polynitrogens azide (), pentazenium (), and pentazolide (cyclic aromatic ). Under extremely high pressures (1.1 million atm) and high temperatures (2000 K), as produced in a diamond anvil cell, nitrogen polymerises into the single-bonded cubic gauche crystal structure. This structure is similar to that of diamond, and both have extremely strong covalent bonds, resulting in its nickname "nitrogen diamond".
At atmospheric pressure, molecular nitrogen condenses (liquefies) at 77 K (−195.79 °C) and freezes at 63 K (−210.01 °C) into the beta hexagonal close-packed crystal allotropic form. Below 35.4 K (−237.6 °C) nitrogen assumes the cubic crystal allotropic form (called the alpha phase). Liquid nitrogen, a colourless fluid resembling water in appearance, but with 80.8% of the density (the density of liquid nitrogen at its boiling point is 0.808 g/mL), is a common cryogen. Solid nitrogen has many crystalline modifications. It forms a significant dynamic surface coverage on Pluto and outer moons of the Solar System such as Triton. Even at the low temperatures of solid nitrogen it is fairly volatile and can sublime to form an atmosphere, or condense back into nitrogen frost. It is very weak and flows in the form of glaciers and on Triton geysers of nitrogen gas come from the polar ice cap region.

In early 2020, researchers at the university of Bayreuth found a new allotrope, the so-called 'black nitrogen'. It has a structure similar to black phosphorus, it is created in a diamond anvil at a pressure of 140 GPa (1.4 million bars) and a temperature of 4000 °C and is only stable in such extreme conditions. It is actually transparent and in the experiment it was heated by a strong laser.

The first example of a dinitrogen complex to be discovered was [Ru(NH)(N)] (see figure at right), and soon many other such complexes were discovered. These complexes, in which a nitrogen molecule donates at least one lone pair of electrons to a central metal cation, illustrate how N might bind to the metal(s) in nitrogenase and the catalyst for the Haber process: these processes involving dinitrogen activation are vitally important in biology and in the production of fertilisers.

Dinitrogen is able to coordinate to metals in five different ways. The more well-characterised ways are the end-on M←N≡N ("η") and M←N≡N→M ("μ", bis-"η"), in which the lone pairs on the nitrogen atoms are donated to the metal cation. The less well-characterised ways involve dinitrogen donating electron pairs from the triple bond, either as a bridging ligand to two metal cations ("μ", bis-"η") or to just one ("η"). The fifth and unique method involves triple-coordination as a bridging ligand, donating all three electron pairs from the triple bond ("μ"-N). A few complexes feature multiple N ligands and some feature N bonded in multiple ways. Since N is isoelectronic with carbon monoxide (CO) and acetylene (CH), the bonding in dinitrogen complexes is closely allied to that in carbonyl compounds, although N is a weaker "σ"-donor and "π"-acceptor than CO. Theoretical studies show that "σ" donation is a more important factor allowing the formation of the M–N bond than "π" back-donation, which mostly only weakens the N–N bond, and end-on ("η") donation is more readily accomplished than side-on ("η") donation.

Today, dinitrogen complexes are known for almost all the transition metals, accounting for several hundred compounds. They are normally prepared by three methods:
Occasionally the N≡N bond may be formed directly within a metal complex, for example by directly reacting coordinated ammonia (NH) with nitrous acid (HNO), but this is not generally applicable. Most dinitrogen complexes have colours within the range white-yellow-orange-red-brown; a few exceptions are known, such as the blue [{Ti("η"-CH)}-(N)].

Nitrogen bonds to almost all the elements in the periodic table except the first three noble gases, helium, neon, and argon, and some of the very short-lived elements after bismuth, creating an immense variety of binary compounds with varying properties and applications. Many binary compounds are known: with the exception of the nitrogen hydrides, oxides, and fluorides, these are typically called nitrides. Many stoichiometric phases are usually present for most elements (e.g. MnN, MnN, MnN, MnN, MnN, and MnN for 9.2 < "x" < 25.3). They may be classified as "salt-like" (mostly ionic), covalent, "diamond-like", and metallic (or interstitial), although this classification has limitations generally stemming from the continuity of bonding types instead of the discrete and separate types that it implies. They are normally prepared by directly reacting a metal with nitrogen or ammonia (sometimes after heating), or by thermal decomposition of metal amides:
Many variants on these processes are possible.The most ionic of these nitrides are those of the alkali metals and alkaline earth metals, LiN (Na, K, Rb, and Cs do not form stable nitrides for steric reasons) and MN (M = Be, Mg, Ca, Sr, Ba). These can formally be thought of as salts of the N anion, although charge separation is not actually complete even for these highly electropositive elements. However, the alkali metal azides NaN and KN, featuring the linear anion, are well-known, as are Sr(N) and Ba(N). Azides of the B-subgroup metals (those in groups 11 through 16) are much less ionic, have more complicated structures, and detonate readily when shocked.
Many covalent binary nitrides are known. Examples include cyanogen ((CN)), triphosphorus pentanitride (PN), disulfur dinitride (SN), and tetrasulfur tetranitride (SN). The essentially covalent silicon nitride (SiN) and germanium nitride (GeN) are also known: silicon nitride in particular would make a promising ceramic if not for the difficulty of working with and sintering it. In particular, the group 13 nitrides, most of which are promising semiconductors, are isoelectronic with graphite, diamond, and silicon carbide and have similar structures: their bonding changes from covalent to partially ionic to metallic as the group is descended. In particular, since the B–N unit is isoelectronic to C–C, and carbon is essentially intermediate in size between boron and nitrogen, much of organic chemistry finds an echo in boron–nitrogen chemistry, such as in borazine ("inorganic benzene"). Nevertheless, the analogy is not exact due to the ease of nucleophilic attack at boron due to its deficiency in electrons, which is not possible in a wholly carbon-containing ring.

The largest category of nitrides are the interstitial nitrides of formulae MN, MN, and MN (although variable composition is perfectly possible), where the small nitrogen atoms are positioned in the gaps in a metallic cubic or hexagonal close-packed lattice. They are opaque, very hard, and chemically inert, melting only at very high temperatures (generally over 2500 °C). They have a metallic lustre and conduct electricity as do metals. They hydrolyse only very slowly to give ammonia or nitrogen.

The nitride anion (N) is the strongest "π" donor known amongst ligands (the second-strongest is O). Nitrido complexes are generally made by thermal decomposition of azides or by deprotonating ammonia, and they usually involve a terminal {≡N} group. The linear azide anion (), being isoelectronic with nitrous oxide, carbon dioxide, and cyanate, forms many coordination complexes. Further catenation is rare, although (isoelectronic with carbonate and nitrate) is known.

Industrially, ammonia (NH) is the most important compound of nitrogen and is prepared in larger amounts than any other compound, because it contributes significantly to the nutritional needs of terrestrial organisms by serving as a precursor to food and fertilisers. It is a colourless alkaline gas with a characteristic pungent smell. The presence of hydrogen bonding has very significant effects on ammonia, conferring on it its high melting (−78 °C) and boiling (−33 °C) points. As a liquid, it is a very good solvent with a high heat of vaporisation (enabling it to be used in vacuum flasks), that also has a low viscosity and electrical conductivity and high dielectric constant, and is less dense than water. However, the hydrogen bonding in NH is weaker than that in HO due to the lower electronegativity of nitrogen compared to oxygen and the presence of only one lone pair in NH rather than two in HO. It is a weak base in aqueous solution (p"K" 4.74); its conjugate acid is ammonium, . It can also act as an extremely weak acid, losing a proton to produce the amide anion, . It thus undergoes self-dissociation, similar to water, to produce ammonium and amide. Ammonia burns in air or oxygen, though not readily, to produce nitrogen gas; it burns in fluorine with a greenish-yellow flame to give nitrogen trifluoride. Reactions with the other nonmetals are very complex and tend to lead to a mixture of products. Ammonia reacts on heating with metals to give nitrides.

Many other binary nitrogen hydrides are known, but the most important are hydrazine (NH) and hydrogen azide (HN). Although it is not a nitrogen hydride, hydroxylamine (NHOH) is similar in properties and structure to ammonia and hydrazine as well. Hydrazine is a fuming, colourless liquid that smells similarly to ammonia. Its physical properties are very similar to those of water (melting point 2.0 °C, boiling point 113.5 °C, density 1.00 g/cm). Despite it being an endothermic compound, it is kinetically stable. It burns quickly and completely in air very exothermically to give nitrogen and water vapour. It is a very useful and versatile reducing agent and is a weaker base than ammonia. It is also commonly used as a rocket fuel.

Hydrazine is generally made by reaction of ammonia with alkaline sodium hypochlorite in the presence of gelatin or glue:
(The attacks by hydroxide and ammonia may be reversed, thus passing through the intermediate NHCl instead.) The reason for adding gelatin is that it removes metal ions such as Cu that catalyses the destruction of hydrazine by reaction with monochloramine (NHCl) to produce ammonium chloride and nitrogen.

Hydrogen azide (HN) was first produced in 1890 by the oxidation of aqueous hydrazine by nitrous acid. It is very explosive and even dilute solutions can be dangerous. It has a disagreeable and irritating smell and is a potentially lethal (but not cumulative) poison. It may be considered the conjugate acid of the azide anion, and is similarly analogous to the hydrohalic acids.

All four simple nitrogen trihalides are known. A few mixed halides and hydrohalides are known, but are mostly unstable; examples include NClF, NClF, NBrF, NFH, NFH, NClH, and NClH.

Five nitrogen fluorides are known. Nitrogen trifluoride (NF, first prepared in 1928) is a colourless and odourless gas that is thermodynamically stable, and most readily produced by the electrolysis of molten ammonium fluoride dissolved in anhydrous hydrogen fluoride. Like carbon tetrafluoride, it is not at all reactive and is stable in water or dilute aqueous acids or alkalis. Only when heated does it act as a fluorinating agent, and it reacts with copper, arsenic, antimony, and bismuth on contact at high temperatures to give tetrafluorohydrazine (NF). The cations and are also known (the latter from reacting tetrafluorohydrazine with strong fluoride-acceptors such as arsenic pentafluoride), as is ONF, which has aroused interest due to the short N–O distance implying partial double bonding and the highly polar and long N–F bond. Tetrafluorohydrazine, unlike hydrazine itself, can dissociate at room temperature and above to give the radical NF•. Fluorine azide (FN) is very explosive and thermally unstable. Dinitrogen difluoride (NF) exists as thermally interconvertible "cis" and "trans" isomers, and was first found as a product of the thermal decomposition of FN.

Nitrogen trichloride (NCl) is a dense, volatile, and explosive liquid whose physical properties are similar to those of carbon tetrachloride, although one difference is that NCl is easily hydrolysed by water while CCl is not. It was first synthesised in 1811 by Pierre Louis Dulong, who lost three fingers and an eye to its explosive tendencies. As a dilute gas it is less dangerous and is thus used industrially to bleach and sterilise flour. Nitrogen tribromide (NBr), first prepared in 1975, is a deep red, temperature-sensitive, volatile solid that is explosive even at −100 °C. Nitrogen triiodide (NI) is still more unstable and was only prepared in 1990. Its adduct with ammonia, which was known earlier, is very shock-sensitive: it can be set off by the touch of a feather, shifting air currents, or even alpha particles. For this reason, small amounts of nitrogen triiodide are sometimes synthesised as a demonstration to high school chemistry students or as an act of "chemical magic". Chlorine azide (ClN) and bromine azide (BrN) are extremely sensitive and explosive.

Two series of nitrogen oxohalides are known: the nitrosyl halides (XNO) and the nitryl halides (XNO). The first are very reactive gases that can be made by directly halogenating nitrous oxide. Nitrosyl fluoride (NOF) is colourless and a vigorous fluorinating agent. Nitrosyl chloride (NOCl) behaves in much the same way and has often been used as an ionising solvent. Nitrosyl bromide (NOBr) is red. The reactions of the nitryl halides are mostly similar: nitryl fluoride (FNO) and nitryl chloride (ClNO) are likewise reactive gases and vigorous halogenating agents.

Nitrogen forms nine molecular oxides, some of which were the first gases to be identified: NO (nitrous oxide), NO (nitric oxide), NO (dinitrogen trioxide), NO (nitrogen dioxide), NO (dinitrogen tetroxide), NO (dinitrogen pentoxide), NO (nitrosylazide), and N(NO) (trinitramide). All are thermally unstable towards decomposition to their elements. One other possible oxide that has not yet been synthesised is oxatetrazole (NO), an aromatic ring.

Nitrous oxide (NO), better known as laughing gas, is made by thermal decomposition of molten ammonium nitrate at 250 °C. This is a redox reaction and thus nitric oxide and nitrogen are also produced as byproducts. It is mostly used as a propellant and aerating agent for sprayed canned whipped cream, and was formerly commonly used as an anaesthetic. Despite appearances, it cannot be considered to be the anhydride of hyponitrous acid (HNO) because that acid is not produced by the dissolution of nitrous oxide in water. It is rather unreactive (not reacting with the halogens, the alkali metals, or ozone at room temperature, although reactivity increases upon heating) and has the unsymmetrical structure N–N–O (N≡NO↔N=N=O): above 600 °C it dissociates by breaking the weaker N–O bond.

Nitric oxide (NO) is the simplest stable molecule with an odd number of electrons. In mammals, including humans, it is an important cellular signaling molecule involved in many physiological and pathological processes. It is formed by catalytic oxidation of ammonia. It is a colourless paramagnetic gas that, being thermodynamically unstable, decomposes to nitrogen and oxygen gas at 1100–1200 °C. Its bonding is similar to that in nitrogen, but one extra electron is added to a "π"* antibonding orbital and thus the bond order has been reduced to approximately 2.5; hence dimerisation to O=N–N=O is unfavourable except below the boiling point (where the "cis" isomer is more stable) because it does not actually increase the total bond order and because the unpaired electron is delocalised across the NO molecule, granting it stability. There is also evidence for the asymmetric red dimer O=N–O=N when nitric oxide is condensed with polar molecules. It reacts with oxygen to give brown nitrogen dioxide and with halogens to give nitrosyl halides. It also reacts with transition metal compounds to give nitrosyl complexes, most of which are deeply coloured.

Blue dinitrogen trioxide (NO) is only available as a solid because it rapidly dissociates above its melting point to give nitric oxide, nitrogen dioxide (NO), and dinitrogen tetroxide (NO). The latter two compounds are somewhat difficult to study individually because of the equilibrium between them, although sometimes dinitrogen tetroxide can react by heterolytic fission to nitrosonium and nitrate in a medium with high dielectric constant. Nitrogen dioxide is an acrid, corrosive brown gas. Both compounds may be easily prepared by decomposing a dry metal nitrate. Both react with water to form nitric acid. Dinitrogen tetroxide is very useful for the preparation of anhydrous metal nitrates and nitrato complexes, and it became the storable oxidiser of choice for many rockets in both the United States and USSR by the late 1950s. This is because it is a hypergolic propellant in combination with a hydrazine-based rocket fuel and can be easily stored since it is liquid at room temperature.

The thermally unstable and very reactive dinitrogen pentoxide (NO) is the anhydride of nitric acid, and can be made from it by dehydration with phosphorus pentoxide. It is of interest for the preparation of explosives. It is a deliquescent, colourless crystalline solid that is sensitive to light. In the solid state it is ionic with structure [NO][NO]; as a gas and in solution it is molecular ON–O–NO. Hydration to nitric acid comes readily, as does analogous reaction with hydrogen peroxide giving peroxonitric acid (HOONO). It is a violent oxidising agent. Gaseous dinitrogen pentoxide decomposes as follows:

Many nitrogen oxoacids are known, though most of them are unstable as pure compounds and are known only as aqueous solution or as salts. Hyponitrous acid (HNO) is a weak diprotic acid with the structure HON=NOH (p"K" 6.9, p"K" 11.6). Acidic solutions are quite stable but above pH 4 base-catalysed decomposition occurs via [HONNO] to nitrous oxide and the hydroxide anion. Hyponitrites (involving the anion) are stable to reducing agents and more commonly act as reducing agents themselves. They are an intermediate step in the oxidation of ammonia to nitrite, which occurs in the nitrogen cycle. Hyponitrite can act as a bridging or chelating bidentate ligand.

Nitrous acid (HNO) is not known as a pure compound, but is a common component in gaseous equilibria and is an important aqueous reagent: its aqueous solutions may be made from acidifying cool aqueous nitrite (, bent) solutions, although already at room temperature disproportionation to nitrate and nitric oxide is significant. It is a weak acid with p"K" 3.35 at 18 °C. They may be titrimetrically analysed by their oxidation to nitrate by permanganate. They are readily reduced to nitrous oxide and nitric oxide by sulfur dioxide, to hyponitrous acid with tin(II), and to ammonia with hydrogen sulfide. Salts of hydrazinium react with nitrous acid to produce azides which further react to give nitrous oxide and nitrogen. Sodium nitrite is mildly toxic in concentrations above 100 mg/kg, but small amounts are often used to cure meat and as a preservative to avoid bacterial spoilage. It is also used to synthesise hydroxylamine and to diazotise primary aromatic amines as follows:

Nitrite is also a common ligand that can coordinate in five ways. The most common are nitro (bonded from the nitrogen) and nitrito (bonded from an oxygen). Nitro-nitrito isomerism is common, where the nitrito form is usually less stable.
Nitric acid (HNO) is by far the most important and the most stable of the nitrogen oxoacids. It is one of the three most used acids (the other two being sulfuric acid and hydrochloric acid) and was first discovered by the alchemists in the 13th century. It is made by catalytic oxidation of ammonia to nitric oxide, which is oxidised to nitrogen dioxide, and then dissolved in water to give concentrated nitric acid. In the United States of America, over seven million tonnes of nitric acid are produced every year, most of which is used for nitrate production for fertilisers and explosives, among other uses. Anhydrous nitric acid may be made by distilling concentrated nitric acid with phosphorus pentoxide at low pressure in glass apparatus in the dark. It can only be made in the solid state, because upon melting it spontaneously decomposes to nitrogen dioxide, and liquid nitric acid undergoes self-ionisation to a larger extent than any other covalent liquid as follows:
Two hydrates, HNO·HO and HNO·3HO, are known that can be crystallised. It is a strong acid and concentrated solutions are strong oxidising agents, though gold, platinum, rhodium, and iridium are immune to attack. A 3:1 mixture of concentrated hydrochloric acid and nitric acid, called "aqua regia", is still stronger and successfully dissolves gold and platinum, because free chlorine and nitrosyl chloride are formed and chloride anions can form strong complexes. In concentrated sulfuric acid, nitric acid is protonated to form nitronium, which can act as an electrophile for aromatic nitration:
The thermal stabilities of nitrates (involving the trigonal planar anion) depends on the basicity of the metal, and so do the products of decomposition (thermolysis), which can vary between the nitrite (for example, sodium), the oxide (potassium and lead), or even the metal itself (silver) depending on their relative stabilities. Nitrate is also a common ligand with many modes of coordination.

Finally, although orthonitric acid (HNO), which would be analogous to orthophosphoric acid, does not exist, the tetrahedral orthonitrate anion is known in its sodium and potassium salts:
These white crystalline salts are very sensitive to water vapour and carbon dioxide in the air:
Despite its limited chemistry, the orthonitrate anion is interesting from a structural point of view due to its regular tetrahedral shape and the short N–O bond lengths, implying significant polar character to the bonding.

Nitrogen is one of the most important elements in organic chemistry. Many organic functional groups involve a carbon–nitrogen bond, such as amides (RCONR), amines (RN), imines (RC(=NR)R), imides (RCO)NR, azides (RN), azo compounds (RNR), cyanates and isocyanates (ROCN or RCNO), nitrates (RONO), nitriles and isonitriles (RCN or RNC), nitrites (RONO), nitro compounds (RNO), nitroso compounds (RNO), oximes (RCR=NOH), and pyridine derivatives. C–N bonds are strongly polarised towards nitrogen. In these compounds, nitrogen is usually trivalent (though it can be tetravalent in quaternary ammonium salts, RN), with a lone pair that can confer basicity on the compound by being coordinated to a proton. This may be offset by other factors: for example, amides are not basic because the lone pair is delocalised into a double bond (though they may act as acids at very low pH, being protonated at the oxygen), and pyrrole is not acidic because the lone pair is delocalised as part of an aromatic ring. The amount of nitrogen in a chemical substance can be determined by the Kjeldahl method. In particular, nitrogen is an essential component of nucleic acids, amino acids and thus proteins, and the energy-carrying molecule adenosine triphosphate and is thus vital to all life on Earth.

Nitrogen is the most common pure element in the earth, making up 78.1% of the entire volume of the atmosphere. Despite this, it is not very abundant in Earth's crust, making up only 19 parts per million of this, on par with niobium, gallium, and lithium. The only important nitrogen minerals are nitre (potassium nitrate, saltpetre) and sodanitre (sodium nitrate, Chilean saltpetre). However, these have not been an important source of nitrates since the 1920s, when the industrial synthesis of ammonia and nitric acid became common.

Nitrogen compounds constantly interchange between the atmosphere and living organisms. Nitrogen must first be processed, or "fixed", into a plant-usable form, usually ammonia. Some nitrogen fixation is done by lightning strikes producing the nitrogen oxides, but most is done by diazotrophic bacteria through enzymes known as nitrogenases (although today industrial nitrogen fixation to ammonia is also significant). When the ammonia is taken up by plants, it is used to synthesise proteins. These plants are then digested by animals who use the nitrogen compounds to synthesise their own proteins and excrete nitrogen–bearing waste. Finally, these organisms die and decompose, undergoing bacterial and environmental oxidation and denitrification, returning free dinitrogen to the atmosphere. Industrial nitrogen fixation by the Haber process is mostly used as fertiliser, although excess nitrogen–bearing waste, when leached, leads to eutrophication of freshwater and the creation of marine dead zones, as nitrogen-driven bacterial growth depletes water oxygen to the point that all higher organisms die. Furthermore, nitrous oxide, which is produced during denitrification, attacks the atmospheric ozone layer.

Many saltwater fish manufacture large amounts of trimethylamine oxide to protect them from the high osmotic effects of their environment; conversion of this compound to dimethylamine is responsible for the early odour in unfresh saltwater fish. In animals, free radical nitric oxide (derived from an amino acid), serves as an important regulatory molecule for circulation.

Nitric oxide's rapid reaction with water in animals results in production of its metabolite nitrite. Animal metabolism of nitrogen in proteins, in general, results in excretion of urea, while animal metabolism of nucleic acids results in excretion of urea and uric acid. The characteristic odour of animal flesh decay is caused by the creation of long-chain, nitrogen-containing amines, such as putrescine and cadaverine, which are breakdown products of the amino acids ornithine and lysine, respectively, in decaying proteins.

Nitrogen gas is an industrial gas produced by the fractional distillation of liquid air, or by mechanical means using gaseous air (pressurised reverse osmosis membrane or pressure swing adsorption). Nitrogen gas generators using membranes or pressure swing adsorption (PSA) are typically more cost and energy efficient than bulk delivered nitrogen. Commercial nitrogen is often a byproduct of air-processing for industrial concentration of oxygen for steelmaking and other purposes. When supplied compressed in cylinders it is often called OFN (oxygen-free nitrogen). Commercial-grade nitrogen already contains at most 20 ppm oxygen, and specially purified grades containing at most 2 ppm oxygen and 10 ppm argon are also available.

In a chemical laboratory, it is prepared by treating an aqueous solution of ammonium chloride with sodium nitrite.

Small amounts of the impurities NO and HNO are also formed in this reaction. The impurities can be removed by passing the gas through aqueous sulfuric acid containing potassium dichromate. Very pure nitrogen can be prepared by the thermal decomposition of barium azide or sodium azide.

The applications of nitrogen compounds are naturally extremely widely varied due to the huge size of this class: hence, only applications of pure nitrogen itself will be considered here. Two-thirds of nitrogen produced by industry is sold as the gas and the remaining one-third as the liquid. The gas is mostly used as an inert atmosphere whenever the oxygen in the air would pose a fire, explosion, or oxidising hazard. Some examples include:

Nitrogen is commonly used during sample preparation in chemical analysis. It is used to concentrate and reduce the volume of liquid samples. Directing a pressurised stream of nitrogen gas perpendicular to the surface of the liquid causes the solvent to evaporate while leaving the solute(s) and un-evaporated solvent behind.

Nitrogen can be used as a replacement, or in combination with, carbon dioxide to pressurise kegs of some beers, particularly stouts and British ales, due to the smaller bubbles it produces, which makes the dispensed beer smoother and headier. A pressure-sensitive nitrogen capsule known commonly as a "widget" allows nitrogen-charged beers to be packaged in cans and bottles. Nitrogen tanks are also replacing carbon dioxide as the main power source for paintball guns. Nitrogen must be kept at higher pressure than CO, making N tanks heavier and more expensive. Nitrogen gas has become the inert gas of choice for inert gas asphyxiation, and is under consideration as a replacement for lethal injection in Oklahoma. Nitrogen gas, formed from the decomposition of sodium azide, is used for the inflation of airbags.

Liquid nitrogen is a cryogenic liquid. When insulated in proper containers such as Dewar flasks, it can be transported without much evaporative loss.
Like dry ice, the main use of liquid nitrogen is as a refrigerant. Among other things, it is used in the cryopreservation of blood, reproductive cells (sperm and egg), and other biological samples and materials. It is used in the clinical setting in cryotherapy to remove cysts and warts on the skin. It is used in cold traps for certain laboratory equipment and to cool infrared detectors or X-ray detectors. It has also been used to cool central processing units and other devices in computers that are overclocked, and that produce more heat than during normal operation. Other uses include freeze-grinding and machining materials that are soft or rubbery at room temperature, shrink-fitting and assembling engineering components, and more generally to attain very low temperatures whenever necessary (around −200 °C). Because of its low cost, liquid nitrogen is also often used when such low temperatures are not strictly necessary, such as refrigeration of food, freeze-branding livestock, freezing pipes to halt flow when valves are not present, and consolidating unstable soil by freezing whenever excavation is going on underneath.

Liquid nitrogen is extensively used in vacuum pump systems.

Although nitrogen is non-toxic, when released into an enclosed space it can displace oxygen, and therefore presents an asphyxiation hazard. This may happen with few warning symptoms, since the human carotid body is a relatively poor and slow low-oxygen (hypoxia) sensing system. An example occurred shortly before the launch of the first Space Shuttle mission on March 19, 1981, when two technicians died from asphyxiation after they walked into a space located in the Space Shuttle's mobile launcher platform that was pressurised with pure nitrogen as a precaution against fire.

When inhaled at high partial pressures (more than about 4 bar, encountered at depths below about 30 m in scuba diving), nitrogen is an anesthetic agent, causing nitrogen narcosis, a temporary state of mental impairment similar to nitrous oxide intoxication.

Nitrogen dissolves in the blood and body fats. Rapid decompression (as when divers ascend too quickly or astronauts decompress too quickly from cabin pressure to spacesuit pressure) can lead to a potentially fatal condition called decompression sickness (formerly known as caisson sickness or "the bends"), when nitrogen bubbles form in the bloodstream, nerves, joints, and other sensitive or vital areas. Bubbles from other "inert" gases (gases other than carbon dioxide and oxygen) cause the same effects, so replacement of nitrogen in breathing gases may prevent nitrogen narcosis, but does not prevent decompression sickness.

As a cryogenic liquid, liquid nitrogen can be dangerous by causing cold burns on contact, although the Leidenfrost effect provides protection for very short exposure (about one second). Ingestion of liquid nitrogen can cause severe internal damage. For example, in 2012, a young woman in England had to have her stomach removed after ingesting a cocktail made with liquid nitrogen.

Because the liquid-to-gas expansion ratio of nitrogen is 1:694 at 20 °C, a tremendous amount of force can be generated if liquid nitrogen is rapidly vaporised in an enclosed space. In an incident on January 12, 2006 at Texas A&M University, the pressure-relief devices of a tank of liquid nitrogen were malfunctioning and later sealed. As a result of the subsequent pressure buildup, the tank failed catastrophically. The force of the explosion was sufficient to propel the tank through the ceiling immediately above it, shatter a reinforced concrete beam immediately below it, and blow the walls of the laboratory 0.1–0.2 m off their foundations.

Liquid nitrogen readily evaporates to form gaseous nitrogen, and hence the precautions associated with gaseous nitrogen also apply to liquid nitrogen. For example, oxygen sensors are sometimes used as a safety precaution when working with liquid nitrogen to alert workers of gas spills into a confined space.

Vessels containing liquid nitrogen can condense oxygen from air. The liquid in such a vessel becomes increasingly enriched in oxygen (boiling point −183 °C, higher than that of nitrogen) as the nitrogen evaporates, and can cause violent oxidation of organic material.

Oxygen deficiency monitors are used to measure levels of oxygen in confined spaces and any place where nitrogen gas or liquid are stored or used. In the event of a nitrogen leak, and a decrease in oxygen to a pre-set alarm level, an oxygen deficiency monitor can be programmed to set off audible and visual alarms, thereby providing notification of the possible impending danger. Most commonly the oxygen range to alert personal is when oxygen levels get below 19.5%. OSHA specifies that a hazardous atmosphere may include one where the oxygen concentration is below 19.5% or above 23.5%.
Oxygen deficiency monitors can either be fixed, mounted to the wall and hard-wired into the building’s power supply or simply plugged into a power outlet, or a portable hand-held or wearable monitor.




</doc>
<doc id="21176" url="https://en.wikipedia.org/wiki?curid=21176" title="Nominalism">
Nominalism

In metaphysics, nominalism is a philosophical view which denies the existence of universals and abstract objects, but affirms the existence of general or abstract terms and predicates. There are at least two main versions of nominalism. One version denies the existence of universals – things that can be instantiated or exemplified by many particular things (e.g., strength, humanity). The other version specifically denies the existence of abstract objects – objects that do not exist in space and time.

Most nominalists have held that only physical particulars in space and time are real, and that universals exist only "post res", that is, subsequent to particular things. However, some versions of nominalism hold that some particulars are abstract entities (e.g., numbers), while others are concrete entities – entities that do exist in space and time (e.g., pillars, snakes, bananas).

Nominalism is primarily a position on the problem of universals, which dates back at least to Plato, and is opposed to realist philosophies, such as Platonic realism, which assert that universals do exist over and above particulars. However, the name "nominalism" emerged from debates in medieval philosophy with Roscellinus.

The term 'nominalism' stems from the Latin "nomen", "name". John Stuart Mill summarised nominalism in the apothegm "there is nothing general except names".

In philosophy of law, nominalism finds its application in what is called constitutional nominalism.

The opposite of nominalism is realism. Plato was perhaps the first writer in Western philosophy to clearly state a realist i.e. non-nominalist position:

What about someone who believes in beautiful things, but doesn't believe in the beautiful itself…? Don't you think he is living in a dream rather than a wakened state? ("Republic" 476c)

The Platonic universals corresponding to the names "bed" and "beautiful" were the Form of the Bed and the Form of the Beautiful, or the "Bed Itself" and the "Beautiful Itself". Platonic Forms were the first universals posited as such in philosophy.

Our term "universal" is due to the English translation of Aristotle's technical term "katholou" which he coined specially for the purpose of discussing the problem of universals. "Katholou" is a contraction of the phrase "kata holou", meaning "on the whole".

Aristotle famously rejected certain aspects of Plato's Theory of Forms, but he clearly rejected nominalism as well:

...'Man', and indeed every general predicate, signifies not an individual, but some quality, or quantity or relation, or something of that sort. ("Sophistical Refutations" xxii, 178b37, trans. Pickard-Cambridge)

The first philosophers to explicitly describe nominalist arguments were the Stoics, especially Chrysippus.

In medieval philosophy, the French philosopher and theologian Roscellinus (c. 1050 – c. 1125) was an early, prominent proponent of nominalism. Nominalist ideas can be found in the work of Peter Abelard and reached their flowering in William of Ockham, who was the most influential and thorough nominalist. Abelard's and Ockham's version of nominalism is sometimes called conceptualism, which presents itself as a middle way between nominalism and realism, asserting that there "is" something in common among like individuals, but that it is a concept in the mind, rather than a real entity existing independently of the mind. Ockham argued that only individuals existed and that universals were only mental ways of referring to sets of individuals. "I maintain", he wrote, "that a universal is not something real that exists in a subject... but that it has a being only as a thought-object in the mind [objectivum in anima]". As a general rule, Ockham argued against assuming any entities that were not necessary for explanations. Accordingly, he wrote, there is no reason to believe that there is an entity called "humanity" that resides inside, say, Socrates, and nothing further is explained by making this claim. This is in accord with the analytical method that has since come to be called Ockham's razor, the principle that the explanation of any phenomenon should make as few assumptions as possible. Critics argue that conceptualist approaches answer only the psychological question of universals. If the same concept is "correctly" and non-arbitrarily applied to two individuals, there must be some resemblance or shared property between the two individuals that justifies their falling under the same concept and that is just the metaphysical problem that universals were brought in to address, the starting-point of the whole problem (MacLeod & Rubenstein, 2006, §3d). If resemblances between individuals are asserted, conceptualism becomes moderate realism; if they are denied, it collapses into nominalism.

In modern philosophy, nominalism was revived by Thomas Hobbes and Pierre Gassendi.

In contemporary analytic philosophy, it has been defended by Rudolf Carnap, Nelson Goodman, H. H. Price, and D. C. Williams.

Indian philosophy encompasses various realist and nominalist traditions. Certain orthodox Hindu schools defend the realist position, notably Purva Mimamsa, Nyaya and Vaisheshika, maintaining that the referent of the word is both the individual thing perceived by the subject of knowledge and the class to which the thing belongs. According to Indian realism, both the individual and the class have objective existence, with the second underlying the former. 

Buddhists take the nominalist position, especially those of the Yogacara school; they were of the opinion that words have as referent not true objects, but only concepts produced in the intellect. These concepts are not real since they do not have efficient existence, that is, causal powers. Words, as linguistic conventions, are useful to thought and discourse, but even so, it should not be accepted that words apprehend reality as it is. 

Dignaga formulated a nominalist theory of meaning called "apoha", or "theory of exclusions". The theory seeks to explain how it is possible for words to refer to classes of objects even if no such class has an objective existence. Dignaga's thesis is that classes do not refer to positive qualities that their members share in common. On the contrary, classes are exclusions ("apoha"). As such, the "cow" class, for example, is composed of all exclusions common to individual cows: they are all non-horse, non-elephant, etc.

Among Hindu realists, this thesis was criticized for being negative.

Nominalism arose in reaction to the problem of universals, specifically accounting for the fact that some things are of the same type. For example, Fluffy and Kitzler are both cats, or, the fact that certain properties are repeatable, such as: the grass, the shirt, and Kermit the Frog are green. One wants to know by virtue of "what" are Fluffy and Kitzler both cats, and "what" makes the grass, the shirt, and Kermit green.

The Platonist answer is that all the green things are green in virtue of the existence of a universal: a single abstract thing that, in this case, is a part of all the green things. With respect to the color of the grass, the shirt and Kermit, one of their parts is identical. In this respect, the three parts are literally one. Greenness is repeatable because there is one thing that manifests itself wherever there are green things.

Nominalism denies the existence of universals. The motivation for this flows from several concerns, the first one being where they might exist. Plato famously held, on one interpretation, that there is a realm of abstract forms or universals apart from the physical world (see theory of the forms). Particular physical objects merely exemplify or instantiate the universal. But this raises the question: Where is this universal realm? One possibility is that it is outside space and time. A view sympathetic with this possibility holds that, precisely because some form is immanent in several physical objects, it must also transcend each of those physical objects; in this way, the forms are "transcendent" only insofar as they are "immanent" in many physical objects. In other words, immanence implies transcendence; they are not opposed to one another. (Nor, in this view, would there be a separate "world" or "realm" of forms that is distinct from the physical world, thus shirking much of the worry about where to locate a "universal realm".) However, naturalists assert that nothing is outside of space and time. Some Neoplatonists, such as the pagan philosopher Plotinus and the Christian philosopher Augustine, imply (anticipating conceptualism) that universals are contained within the "mind" of God. To complicate things, what is the nature of the instantiation or exemplification relation?

Conceptualists hold a position intermediate between nominalism and realism, saying that universals exist only within the mind and have no external or substantial reality.

Moderate realists hold that there is no realm in which universals exist, but rather universals are located in space and time wherever they are manifest. Now, recall that a universal, like greenness, is supposed to be a single thing. Nominalists consider it unusual that there could be a single thing that exists in multiple places simultaneously. The realist maintains that all the instances of greenness are held together by the exemplification relation, but this relation cannot be explained.

Finally, many philosophers prefer simpler ontologies populated with only the bare minimum of types of entities, or as W. V. O. Quine said "They have a taste for 'desert landscapes.'" They try to express everything that they want to explain without using universals such as "catness" or "greenness."

There are various forms of nominalism ranging from extreme to almost-realist. One extreme is predicate nominalism, which states that Fluffy and Kitzler, for example, are both cats simply because the predicate 'is a cat' applies to both of them. And this is the case for all similarity of attribute among objects. The main criticism of this view is that it does not provide a sufficient solution to the problem of universals. It fails to provide an account of what makes it the case that a group of things warrant having the same predicate applied to them.

Proponents of resemblance nominalism believe that 'cat' applies to both cats because Fluffy and Kitzler resemble an exemplar cat closely enough to be classed together with it as members of its kind, or that they differ from each other (and other cats) quite less than they differ from other things, and this warrants classing them together. Some resemblance nominalists will concede that the resemblance relation is itself a universal, but is the only universal necessary. Others argue that each resemblance relation is a particular, and is a resemblance relation simply in virtue of its resemblance to other resemblance relations. This generates an infinite regress, but many argue that it is not vicious.

Class nominalism argues that class membership forms the metaphysical backing for property relationships: two particular red balls share a property in that they are both members of classes corresponding to their properties—that of being red and being balls. A version of class nominalism that sees some classes as "natural classes" is held by Anthony Quinton.

Conceptualism is a philosophical theory that explains universality of particulars as conceptualized frameworks situated within the thinking mind. The conceptualist view approaches the metaphysical concept of universals from a perspective that denies their presence in particulars outside of the mind's perception of them.

Another form of nominalism is trope nominalism. A trope is a particular instance of a property, like the specific greenness of a shirt. One might argue that there is a primitive, objective resemblance relation that holds among like tropes. Another route is to argue that all apparent tropes are constructed out of more primitive tropes and that the most primitive tropes are the entities of complete physics. Primitive trope resemblance may thus be accounted for in terms of causal indiscernibility. Two tropes are exactly resembling if substituting one for the other would make no difference to the events in which they are taking part. Varying degrees of resemblance at the macro level can be explained by varying degrees of resemblance at the micro level, and micro-level resemblance is explained in terms of something no less robustly physical than causal power. David Armstrong, perhaps the most prominent contemporary realist, argues that such a trope-based variant of nominalism has promise, but holds that it is unable to account for the laws of nature in the way his theory of universals can.

Ian Hacking has also argued that much of what is called social constructionism of science in contemporary times is actually motivated by an unstated nominalist metaphysical view. For this reason, he claims, scientists and constructionists tend to "shout past each other".

A notion that philosophy, especially ontology and the philosophy of mathematics, should abstain from set theory owes much to the writings of Nelson Goodman (see especially Goodman 1940 and 1977), who argued that concrete and abstract entities having no parts, called "individuals" exist. Collections of individuals likewise exist, but two collections having the same individuals are the same collection. Goodman was himself drawing heavily on the work of Stanisław Leśniewski, especially his mereology, which was itself a reaction to the paradoxes associated with Cantorian set theory. Leśniewski denied the existence of the empty set and held that any singleton was identical to the individual inside it. Classes corresponding to what are held to be species or genera are concrete sums of their concrete constituting individuals. For example, the class of philosophers is nothing but the sum of all concrete, individual philosophers.

The principle of extensionality in set theory assures us that any matching pair of curly braces enclosing one or more instances of the same individuals denote the same set. Hence {"a", "b"}, {"b", "a"}, {"a", "b", "a", "b"} are all the same set. For Goodman and other proponents of mathematical nominalism, {"a", "b"} is also identical to {"a", {"b"} }, {"b", {"a", "b"} }, and any combination of matching curly braces and one or more instances of "a" and "b", as long as "a" and "b" are names of individuals and not of collections of individuals. Goodman, Richard Milton Martin, and Willard Quine all advocated reasoning about collectivities by means of a theory of "virtual sets" (see especially Quine 1969), one making possible all elementary operations on sets except that the universe of a quantified variable cannot contain any virtual sets.

In the foundations of mathematics, nominalism has come to mean doing mathematics without assuming that sets in the mathematical sense exist. In practice, this means that quantified variables may range over universes of numbers, points, primitive ordered pairs, and other abstract ontological primitives, but not over sets whose members are such individuals. To date, only a small fraction of the corpus of modern mathematics can be rederived in a nominalistic fashion.

As a category of late medieval thought, the concept of 'nominalism' has been increasingly queried. Traditionally, the fourteenth century has been regarded as the heyday of nominalism, with figures such as John Buridan and William of Ockham viewed as founding figures. However, the concept of 'nominalism' as a movement (generally contrasted with 'realism'), first emerged only in the late fourteenth century, and only gradually became widespread during the fifteenth century. The notion of two distinct ways, a "via antiqua", associated with realism, and a "via moderna", associated with nominalism, became widespread only in the later fifteenth century – a dispute which eventually dried up in the sixteenth century.

Aware that explicit thinking in terms of a divide between 'nominalism' and 'realism’ emerged only in the fifteenth century, scholars have increasingly questioned whether a fourteenth-century school of nominalism can really be said to have existed. While one might speak of family resemblances between Ockham, Buridan, Marsilius and others, there are also striking differences. More fundamentally, Robert Pasnau has questioned whether any kind of coherent body of thought that could be called 'nominalism' can be discerned in fourteenth century writing. This makes it difficult, it has been argued, to follow the twentieth century narrative which portrayed late scholastic philosophy as a dispute which emerged in the fourteenth century between the "via moderna", nominalism, and the "via antiqua", realism, with the nominalist ideas of William of Ockham foreshadowing the eventual rejection of scholasticism in the seventeenth century.

A critique of nominalist reconstructions in mathematics was undertaken by Burgess (1983) and Burgess and Rosen (1997). Burgess distinguished two types of nominalist reconstructions. Thus, "hermeneutic nominalism" is the hypothesis that science, properly interpreted, already dispenses with mathematical objects
(entities) such as numbers and sets. Meanwhile, "revolutionary nominalism" is the project of replacing current scientific theories by alternatives dispensing with mathematical objects (see Burgess, 1983, p. 96). A recent study extends the Burgessian critique to three nominalistic reconstructions: the reconstruction of analysis by Georg Cantor, Richard Dedekind, and Karl Weierstrass that dispensed with infinitesimals; the constructivist re-reconstruction of Weierstrassian analysis by Errett Bishop that dispensed with the law of excluded middle; and the hermeneutic reconstruction, by Carl Boyer, Judith Grabiner, and others, of Cauchy's foundational contribution to analysis that dispensed with Cauchy's infinitesimals.





</doc>
<doc id="21178" url="https://en.wikipedia.org/wiki?curid=21178" title="Non-cognitivism">
Non-cognitivism

Non-cognitivism is the meta-ethical view that ethical sentences do not express propositions (i.e., statements) and thus cannot be true or false (they are not truth-apt). A noncognitivist denies the cognitivist claim that "moral judgments are capable of being objectively true, because they describe some feature of the world". If moral statements cannot be true, and if one cannot know something that is not true, noncognitivism implies that moral knowledge is impossible.

Non-cognitivism entails that non-cognitive attitudes underlie moral discourse and this discourse therefore consists of non-declarative speech acts, although accepting that its surface features may consistently and efficiently work as if moral discourse were cognitive. The point of interpreting moral claims as non-declarative speech acts is to explain what moral claims mean if they are neither true nor false (as philosophies such as logical positivism entail). Utterances like "Boo to killing!" and "Don't kill" are not candidates for truth or falsity, but have non-cognitive meaning.

Emotivism, associated with A. J. Ayer, the Vienna Circle and C. L. Stevenson, suggests that ethical sentences are primarily emotional expressions of one's own attitudes and are intended to influence the actions of the listener. Under this view, "Killing is wrong" is translated as "Killing, boo!" or "I disapprove of killing."

A close cousin of emotivism, developed by R. M. Hare, is called universal prescriptivism. Prescriptivists interpret ethical statements as being universal "imperatives", prescribing behavior for all to follow. According to prescriptivism, 
phrases like "Thou shalt not murder!" or "Do not steal!" are the clearest expressions of morality, while reformulations like "Killing is wrong" tend to obscure the meaning of moral sentences.

Other forms of non-cognitivism include Simon Blackburn's quasi-realism and Allan Gibbard's norm-expressivism.

Arguments for prescriptivism focus on the "function" of normative statements.

Prescriptivists argue that factual statements and prescriptions are totally different, because of different expectations of change in cases of a clash between word and world.
In a descriptive sentence, if one premises that "red is a number" then according to the rules of English grammar said statement would be false. Since said premise describes the objects "red" and "number", anyone with an adequate understanding of English would notice the falseness of such description and the falseness of said statement. However, if the norm "thou shalt not kill!" is uttered, and this premise is negated (by the fact of a person being murdered), the speaker is not to change his sentence upon observation of this into "kill other people!", but is to reiterate the moral outrage of the act of killing. Adjusting statements based upon objective reality and adjusting reality based upon statements are contrary uses of language; that is to say, descriptive statements are a different kind of sentence to normative statements. If truth is understood according to correspondence theory, the question of the truth or falsity of sentences not contingent upon external phenomena cannot be tested (see tautologies).

Some cognitivists argue that some expressions like "courageous" have both a factual as well as a normative component which cannot be distinguished by analysis. Prescriptivists argue that according to context, either the factual or the normative component of the meaning is dominant. The sentence "Hero A behaved courageously" is wrong, if A ran away in the face of danger. But the sentence "Be brave and fight for the glory of your country!" has no truth value and cannot be falsified by someone who doesn't join the army.

Prescriptivism is also supported by the actual way of speaking. Many moral statements are de facto uttered as recommendations or commands, e.g. when parents or teachers forbid children to do wrong actions. The most famous moral ideas are prescriptions: the Ten Commandments, the command of charity, the categorical imperative, and the Golden Rule command to do or not to do something rather than state that something is or is not the case.

Prescriptivism can fit the theist idea of morality as obedience towards god. It is however different from the cognitivist supernaturalism which interprets morality as subjective will of god, while prescriptivism claims that moral rules are universal and can be found by reason alone without reference to a god.

According to Hare, prescriptivists cannot argue that amoralists are logically wrong or contradictive. Everyone can choose to follow moral commands or not. This is the human condition according to the Christian reinterpretation of the Choice of Heracles. According to prescriptivism, morality is not about knowledge (of moral facts), but about character (to choose to do the right thing). Actors cannot externalize their responsibility and freedom of will towards some moral truth in the world, virtuous people don't need to wait for some cognition to choose what's right.

Prescriptivism is also supported by imperative logic, in which there are no truth values for imperatives, and by the idea of the naturalistic fallacy: even if someone could prove the existence of an ethical property and express it in a factual statement, he could never derive any command from this statement, so the search for ethical properties is pointless.

As with other anti-realist meta-ethical theories, non-cognitivism is largely supported by the argument from queerness: ethical properties, if they existed, would be different from any other thing in the universe, since they have no observable effect on the world. People generally have a negative attitude towards murder, which presumably keeps most of us from murdering. But does the actual "wrongness" of murder play an "independent" role? Is there any evidence that there is a property of wrongness that some types of acts have? Some people might think that the strong feelings we have when we see or consider a murder provide evidence of murder's wrongness. But it is not difficult to explain these feelings without saying that "wrongness" was their cause. Thus there is no way of discerning which, if any, ethical properties exist; by Occam's razor, the simplest assumption is that none do. The non-cognitivist then asserts that, since a proposition about an ethical property would have no referent, ethical statements must be something else.

Arguments for emotivism focus on what normative statements "express" when uttered by a speaker. A person who says that killing is wrong certainly expresses her disapproval of killing. Emotivists claim that this is "all" she does, that the statement "killing is wrong" is not a truth-apt declaration, and that the burden of evidence is on the cognitivists who want to show that in addition to expressing disapproval, the claim "killing is wrong" is also true. Emotivists ask whether there really is evidence that killing is wrong. We have evidence that Jupiter has a magnetic field and that birds are oviparous, but as yet, we do not seem to have found evidence of moral properties, such as "goodness". Emotivists ask why, without such evidence, we should think there "is" such a property. Ethical intuitionists think the evidence comes not from science or reason but from our own feelings: good deeds make us feel a certain way and bad deeds make us feel very differently. But is this enough to show that there are genuinely good and bad deeds? Emotivists think not, claiming that we do not need to postulate the existence of moral "badness" or "wrongness" to explain why considering certain deeds makes us feel disapproval; that all we really observe when we introspect are feelings of disapproval. Thus the emotivist asks why not adopt the simple explanation and say that this is all there is, rather than insist that some intrinsic "badness" (of murder, for example) must be causing feelings when a simpler explanation is available.

One argument against non-cognitivism is that it ignores the external "causes" of emotional and prescriptive reactions. If someone says, "John is a good person," something about John must have inspired that reaction. If John gives to the poor, takes care of his sick grandmother, and is friendly to others, and these are what inspire the speaker to think well of him, it is plausible to say, "John is a good person because he gives to the poor, takes care of his sick grandmother, and is friendly to others." If, in turn, the speaker responds positively to the idea of giving to the poor, then some aspect of that idea must have inspired a positive response; one could argue that that aspect is also the basis of its goodness.

Another argument is the "embedding problem." Consider the following sentences:

Attempts to translate these sentences in an emotivist framework seem to fail (e.g. "She does not realize, 'Boo on eating meat!'"). Prescriptivist translations fare only slightly better ("She does not realize that she is not to eat meat"). Even the act of forming such a construction indicates some sort of cognition in the process.

According to some non-cognitivist points of view, these sentences simply assume the false premise that ethical statements are either true or false. They might be literally translated as:

These translations, however, seem divorced from the way people actually use language. A non-cognitivist would have to disagree with someone saying, "'Eating meat is wrong' is a false statement" (since "Eating meat is wrong" is not truth-apt at all), but may be tempted to agree with a person saying, "Eating meat is not wrong."

One might more constructively interpret these statements to describe the underlying emotional statement that they express, i.e.: I disapprove/do not disapprove of eating meat, I used to, he doesn't, I do and she doesn't, etc.; however, this interpretation is closer to ethical subjectivism than to non-cognitivism proper.

A similar argument against non-cognitivism is that of ethical argument. A common argument might be, "If killing an innocent human is always wrong, and all fetuses are innocent humans, then killing a fetus is always wrong." Most people would consider such an utterance to represent an analytic proposition which is true "a priori". However, if ethical statements do not represent cognitions, it seems odd to use them as premises in an argument, and even odder to assume they follow the same rules of syllogism as true propositions. However, R.M. Hare, proponent of universal prescriptivism, has argued that the rules of logic are independent of grammatical mood, and thus the same logical relations may hold between imperatives as hold between indicatives.

Many objections to non-cognitivism based on the linguistic characteristics of what purport to be moral judgments were originally raised by Peter Glassen in "The Cognitivity of Moral Judgments", published in "Mind" in January 1959, and in Glassen's follow-up article in the January 1963 issue of the same journal.




</doc>
<doc id="21179" url="https://en.wikipedia.org/wiki?curid=21179" title="North Sea">
North Sea

The North Sea is a sea of the Atlantic Ocean located between Great Britain (England and Scotland), Denmark, Norway, Germany, the Netherlands, Belgium and France. An epeiric (or "shelf") sea on the European continental shelf, it connects to the ocean through the English Channel in the south and the Norwegian Sea in the north. It is more than long and wide, with an area of .

The North Sea has long been the site of important European shipping lanes as well as a major fishery. The coast is a popular destination for recreation and tourism in bordering countries, and more recently the sea has developed into a rich source of energy resources, including fossil fuels, wind, and early efforts in wave power.

Historically, the North Sea has featured prominently in geopolitical and military affairs, particularly in Northern Europe. It was also important globally through the power northern Europeans projected worldwide during much of the Middle Ages and into the modern era. The North Sea was the centre of the Vikings' rise. Subsequently, the Hanseatic League, the Dutch Republic, and the British each sought to gain command the North Sea and thus access to the world's markets and resources. As Germany's only outlet to the ocean, the North Sea continued to be strategically important through both World Wars.

The coast of the North Sea presents a diversity of geological and geographical features. In the north, deep fjords and sheer cliffs mark the Norwegian and Scottish coastlines, whereas in the south, the coast consists primarily of sandy beaches and wide mudflats. Due to the dense population, heavy industrialization, and intense use of the sea and area surrounding it, there have been various environmental issues affecting the sea's ecosystems. Adverse environmental issues – commonly including overfishing, industrial and agricultural runoff, dredging, and dumping, among others – have led to a number of efforts to prevent degradation of the sea while still making use of its economic potential.

The North Sea is bounded by the Orkney Islands and east coast of Great Britain to the west and the northern and central European mainland to the east and south, including Norway, Denmark, Germany, the Netherlands, Belgium, and France. In the southwest, beyond the Straits of Dover, the North Sea becomes the English Channel connecting to the Atlantic Ocean. In the east, it connects to the Baltic Sea via the Skagerrak and Kattegat, narrow straits that separate Denmark from Norway and Sweden respectively. In the north it is bordered by the Shetland Islands, and connects with the Norwegian Sea, which is a marginal sea in the Arctic Ocean.

The North Sea is more than long and wide, with an area of and a volume of . Around the edges of the North Sea are sizeable islands and archipelagos, including Shetland, Orkney, and the Frisian Islands. The North Sea receives freshwater from a number of European continental watersheds, as well as the British Isles. A large part of the European drainage basin empties into the North Sea, including water from the Baltic Sea. The largest and most important rivers flowing into the North Sea are the Elbe and the Rhine – Meuse. Around 185 million people live in the catchment area of the rivers discharging into the North Sea encompassing some highly industrialized areas.

For the most part, the sea lies on the European continental shelf with a mean depth of . The only exception is the Norwegian trench, which extends parallel to the Norwegian shoreline from Oslo to an area north of Bergen. It is between wide and has a maximum depth of .

The Dogger Bank, a vast moraine, or accumulation of unconsolidated glacial debris, rises to a mere below the surface. This feature has produced the finest fishing location of the North Sea. The Long Forties and the Broad Fourteens are large areas with roughly uniform depth in fathoms, (forty fathoms and fourteen fathoms or deep respectively). These great banks and others make the North Sea particularly hazardous to navigate, which has been alleviated by the implementation of satellite navigation systems. The Devil's Hole lies east of Dundee, Scotland. The feature is a series of asymmetrical trenches between long, wide and up to deep.

Other areas which are less deep are Cleaver Bank, Fisher Bank and Noordhinder Bank.

The International Hydrographic Organization defines the limits of the North Sea as follows:

"On the Southwest." A line joining the Walde Lighthouse (France, 1°55'E) and Leathercoat Point (England, 51°10'N).

"On the Northwest." From Dunnet Head (3°22'W) in Scotland to Tor Ness (58°47'N) in the Island of Hoy, thence through this island to the Kame of Hoy (58°55'N) on to Breck Ness on Mainland (58°58'N) through this island to Costa Head (3°14'W) and to Inga Ness (59'17'N) in Westray through Westray, to Bow Head, across to Mull Head (North point of Papa Westray) and on to Seal Skerry (North point of North Ronaldsay) and thence to Horse Island (South point of the Shetland Islands).

"On the North." From the North point (Fethaland Point) of the Mainland of the Shetland Islands, across to Graveland Ness (60°39'N) in the Island of Yell, through Yell to Gloup Ness (1°04'W) and across to Spoo Ness (60°45'N) in Unst island, through Unst to Herma Ness (60°51'N), on to the SW point of the Rumblings and to Muckle Flugga () all these being included in the North Sea area; thence up the meridian of 0°53' West to the parallel of 61°00' North and eastward along this parallel to the coast of Norway, the whole of Viking Bank being thus included in the North Sea.

"On the East." The Western limit of the Skagerrak [A line joining Hanstholm () and the Naze (Lindesnes, )].

The average temperature in summer is and in the winter. The average temperatures have been trending higher since 1988, which has been attributed to climate change. Air temperatures in January range on average between and in July between . The winter months see frequent gales and storms.

The salinity averages between of water. The salinity has the highest variability where there is fresh water inflow, such as at the Rhine and Elbe estuaries, the Baltic Sea exit and along the coast of Norway.

The main pattern to the flow of water in the North Sea is an anti-clockwise rotation along the edges.

The North Sea is an arm of the Atlantic Ocean receiving the majority of ocean current from the northwest opening, and a lesser portion of warm current from the smaller opening at the English Channel. These tidal currents leave along the Norwegian coast. Surface and deep water currents may move in different directions. Low salinity surface coastal waters move offshore, and deeper, denser high salinity waters move inshore.

The North Sea located on the continental shelf has different waves from those in deep ocean water. The wave speeds are diminished and the wave amplitudes are increased. In the North Sea there are two amphidromic systems and a third incomplete amphidromic system. In the North Sea the average tide difference in wave amplitude is between zero and .

The Kelvin tide of the Atlantic Ocean is a semidiurnal wave that travels northward. Some of the energy from this wave travels through the English Channel into the North Sea. The wave continues to travel northward in the Atlantic Ocean, and once past the northern tip of Great Britain, the Kelvin wave turns east and south and once again enters the North Sea.

The eastern and western coasts of the North Sea are jagged, formed by glaciers during the ice ages. The coastlines along the southernmost part are covered with the remains of deposited glacial sediment. The Norwegian mountains plunge into the sea creating deep fjords and archipelagos. South of Stavanger, the coast softens, the islands become fewer. The eastern Scottish coast is similar, though less severe than Norway. From north east of England, the cliffs become lower and are composed of less resistant moraine, which erodes more easily, so that the coasts have more rounded contours. In the Netherlands, Belgium and in East Anglia the littoral is low and marshy. The east coast and south-east of the North Sea (Wadden Sea) have coastlines that are mainly sandy and straight owing to longshore drift, particularly along Belgium and Denmark.

The southern coastal areas were originally amphibious flood plains and swampy land. In areas especially vulnerable to storm surges, people settled behind elevated levees and on natural areas of high ground such as spits and geestland. As early as 500 BC, people were constructing artificial dwelling hills higher than the prevailing flood levels. It was only around the beginning of the High Middle Ages, in 1200 AD, that inhabitants began to connect single ring dikes into a dike line along the entire coast, thereby turning amphibious regions between the land and the sea into permanent solid ground.

The modern form of the dikes supplemented by overflow and lateral diversion channels, began to appear in the 17th and 18th centuries, built in the Netherlands. The North Sea Floods of 1953 and 1962 were the impetus for further raising of the dikes as well as the shortening of the coast line so as to present as little surface area as possible to the punishment of the sea and the storms. Currently, 27% of the Netherlands is below sea level protected by dikes, dunes, and beach flats.

Coastal management today consists of several levels. The dike slope reduces the energy of the incoming sea, so that the dike itself does not receive the full impact. Dikes that lie directly on the sea are especially reinforced. The dikes have, over the years, been repeatedly raised, sometimes up to and have been made flatter to better reduce wave erosion. Where the dunes are sufficient to protect the land behind them from the sea, these dunes are planted with beach grass ("Ammophila arenaria") to protect them from erosion by wind, water, and foot traffic.

Storm surges threaten, in particular, the coasts of the Netherlands, Belgium, Germany, and Denmark and low lying areas of eastern England particularly around The Wash and Fens.
Storm surges are caused by changes in barometric pressure combined with strong wind created wave action.

The first recorded storm tide flood was the "Julianenflut", on 17 February 1164. In its wake, the Jadebusen, (a bay on the coast of Germany), began to form.
A storm tide in 1228 is recorded to have killed more than 100,000 people. In 1362, the Second Marcellus Flood, also known as the "Grote Manndrenke", hit the entire southern coast of the North Sea. Chronicles of the time again record more than 100,000 deaths, large parts of the coast were lost permanently to the sea, including the now legendary lost city of Rungholt.
In the 20th century, the North Sea flood of 1953 flooded several nations' coasts and cost more than 2,000 lives.
315 citizens of Hamburg died in the North Sea flood of 1962.

Though rare, the North Sea has been the site of a number of historically documented tsunamis. The Storegga Slides were a series of underwater landslides, in which a piece of the Norwegian continental shelf slid into the Norwegian Sea. The immense landslips occurred between 8150 BCE and 6000 BCE, and caused a tsunami up to high that swept through the North Sea, having the greatest effect on Scotland and the Faeroe Islands.
The Dover Straits earthquake of 1580 is among the first recorded earthquakes in the North Sea measuring between 5.6 and 5.9 on the Richter scale. This event caused extensive damage in Calais both through its tremors and possibly triggered a tsunami, though this has never been confirmed. The theory is a vast underwater landslide in the English Channel was triggered by the earthquake, which in turn caused a tsunami. The tsunami triggered by the 1755 Lisbon earthquake reached Holland, although the waves had lost their destructive power. The largest earthquake ever recorded in the United Kingdom was the 1931 Dogger Bank earthquake, which measured 6.1 on the Richter magnitude scale and caused a small tsunami that flooded parts of the British coast.

Shallow epicontinental seas like the current North Sea have since long existed on the European continental shelf. The rifting that formed the northern part of the Atlantic Ocean during the Jurassic and Cretaceous periods, from about , caused tectonic uplift in the British Isles. Since then, a shallow sea has almost continuously existed between the uplands of the Fennoscandian Shield and the British Isles. This precursor of the current North Sea has grown and shrunk with the rise and fall of the eustatic sea level during geologic time. Sometimes it was connected with other shallow seas, such as the sea above the Paris Basin to the south-west, the Paratethys Sea to the south-east, or the Tethys Ocean to the south.

During the Late Cretaceous, about , all of modern mainland Europe except for Scandinavia was a scattering of islands. By the Early Oligocene, , the emergence of Western and Central Europe had almost completely separated the North Sea from the Tethys Ocean, which gradually shrank to become the Mediterranean as Southern Europe and South West Asia became dry land. The North Sea was cut off from the English Channel by a narrow land bridge until that was breached by at least two catastrophic floods between 450,000 and 180,000 years ago. Since the start of the Quaternary period about , the eustatic sea level has fallen during each glacial period and then risen again. Every time the ice sheet reached its greatest extent, the North Sea became almost completely dry. The present-day coastline formed after the Last Glacial Maximum when the sea began to flood the European continental shelf.

In 2006 a bone fragment was found while drilling for oil in the North Sea. Analysis indicated that it was a Plateosaurus from 199 to 216 million years ago. This was the deepest dinosaur fossil ever found and the first find for Norway.

Copepods and other zooplankton are plentiful in the North Sea. These tiny organisms are crucial elements of the food chain supporting many species of fish. Over 230 species of fish live in the North Sea. Cod, haddock, whiting, saithe, plaice, sole, mackerel, herring, pouting, sprat, and sandeel are all very common and are fished commercially. Due to the various depths of the North Sea trenches and differences in salinity, temperature, and water movement, some fish such as blue-mouth redfish and rabbitfish reside only in small areas of the North Sea.

Crustaceans are also commonly found throughout the sea. Norway lobster, deep-water prawns, and brown shrimp are all commercially fished, but other species of lobster, shrimp, oyster, mussels and clams all live in the North Sea. Recently non-indigenous species have become established including the Pacific oyster and Atlantic jackknife clam.

The coasts of the North Sea are home to nature reserves including the Ythan Estuary, Fowlsheugh Nature Preserve, and Farne Islands in the UK and the Wadden Sea National Parks in Denmark, Germany and the Netherlands. These locations provide breeding habitat for dozens of bird species. Tens of millions of birds make use of the North Sea for breeding, feeding, or migratory stopovers every year. Populations of black-legged kittiwakes, Atlantic puffins, northern gannets, northern fulmars, and species of petrels, seaducks, loons (divers), cormorants, gulls, auks, and terns, and many other seabirds make these coasts popular for birdwatching.

The North Sea is also home to marine mammals. Common seals, and harbour porpoises can be found along the coasts, at marine installations, and on islands. The very northern North Sea islands such as the Shetland Islands are occasionally home to a larger variety of pinnipeds including bearded, harp, hooded and ringed seals, and even walrus. North Sea cetaceans include various porpoise, dolphin and whale species.

Plant species in the North Sea include species of wrack, among them bladder wrack, knotted wrack, and serrated wrack. Algae, macroalgal, and kelp, such as oarweed and laminaria hyperboria, and species of maerl are found as well. Eelgrass, formerly common in the entirety of the Wadden Sea, was nearly wiped out in the 20th century by a disease. Similarly, sea grass used to coat huge tracts of ocean floor, but have been damaged by trawling and dredging have diminished its habitat and prevented its return. Invasive Japanese seaweed has spread along the shores of the sea clogging harbours and inlets and has become a nuisance.

Due to the heavy human populations and high level of industrialization along its shores, the wildlife of the North Sea has suffered from pollution, overhunting, and overfishing. Flamingos and pelicans were once found along the southern shores of the North Sea, but became extinct over the 2nd millennium. Walruses frequented the Orkney Islands through the mid-16th century, as both Sable Island and Orkney Islands lay within its normal range. Gray whales also resided in the North Sea but were driven to extinction in the Atlantic in the 17th century Other species have dramatically declined in population, though they are still found. North Atlantic right whales, sturgeon, shad, rays, skates, salmon, and other species were common in the North Sea until the 20th century, when numbers declined due to overfishing.
Other factors like the introduction of non-indigenous species, industrial and agricultural pollution, trawling and dredging, human-induced eutrophication, construction on coastal breeding and feeding grounds, sand and gravel extraction, offshore construction, and heavy shipping traffic have also contributed to the decline.
For example, a resident killer whale pod was lost in the 1960s, presumably due to the peak in PCB pollution in this time period.

The OSPAR commission manages the OSPAR convention to counteract the harmful effects of human activity on wildlife in the North Sea, preserve endangered species, and provide environmental protection. All North Sea border states are signatories of the MARPOL 73/78 Accords, which preserve the marine environment by preventing pollution from ships.
Germany, Denmark, and the Netherlands also have a trilateral agreement for the protection of the Wadden Sea, or mudflats, which run along the coasts of the three countries on the southern edge of the North Sea.

The North Sea has had various names through history. One of the earliest recorded names was "Septentrionalis Oceanus", or "Northern Ocean," which was cited by Pliny. The name "North Sea" probably came into English, however, via the Dutch "Noordzee", who named it thus either in contrast with the Zuiderzee ("South Sea"), located south of Frisia, or because the sea is generally to the north of the Netherlands. Before the adoption of "North Sea," the names used in English, in American English in particular, were "German Sea" or "German Ocean", referred to the Latin names "Mare Germanicum" and "Oceanus Germanicus", and these persisted in use until the First World War.

Other common names in use for long periods were the Latin terms "Mare Frisicum", as well as the English equivalent, "Frisian Sea".

The modern names of the sea in the other local languages are: ("West Sea") or "Nordsøen" , , , , , , , Northern Frisian: "Weestsiie" ("West Sea"), , , , , and Zeeuws: "Noôrdzeê".

North Sea has provided waterway access for commerce and conquest. Many areas have access to the North Sea because of its long coastline and the European rivers that empty into it. The British Isles had been protected from invasion by the North Sea waters until the Roman conquest of Britain in 43 CE. The Romans established organised ports, which increased shipping, and began sustained trade. When the Romans abandoned Britain in 410, the Germanic Angles, Saxons, and Jutes began the next great migration across the North Sea during the Migration Period. They made successive invasions of the island.

The Viking Age began in 793 with the attack on Lindisfarne; for the next quarter-millennium the Vikings ruled the North Sea. In their superior longships, they raided, traded, and established colonies and outposts along the coasts of the sea. From the Middle Ages through the 15th century, the northern European coastal ports exported domestic goods, dyes, linen, salt, metal goods and wine. The Scandinavian and Baltic areas shipped grain, fish, naval necessities, and timber. In turn the North Sea countries imported high-grade cloths, spices, and fruits from the Mediterranean region. Commerce during this era was mainly conducted by maritime trade due to underdeveloped roadways.

In the 13th century the Hanseatic League, though centred on the Baltic Sea, started to control most of the trade through important members and outposts on the North Sea. The League lost its dominance in the 16th century, as neighbouring states took control of former Hanseatic cities and outposts. Their internal conflict prevented effective cooperation and defence. As the League lost control of its maritime cities, new trade routes emerged that provided Europe with Asian, American, and African goods.

The 17th century Dutch Golden Age during which Dutch herring, cod and whale fisheries reached an all time high saw Dutch power at its zenith. Important overseas colonies, a vast merchant marine, powerful navy and large profits made the Dutch the main challengers to an ambitious England. This rivalry led to the first three Anglo-Dutch Wars between 1652 and 1673, which ended with Dutch victories. After the Glorious Revolution in 1688, the Dutch prince William ascended to the English throne. With unified leadership, commercial, military, and political power began to shift from Amsterdam to London.
The British did not face a challenge to their dominance of the North Sea until the 20th century.

Tensions in the North Sea were again heightened in 1904 by the Dogger Bank incident. During the Russo-Japanese War, several ships of the Russian Baltic Fleet, which was on its way to the Far East, mistook British fishing boats for Japanese ships and fired on them, and then upon each other, near the Dogger Bank, nearly causing Britain to enter the war on the side of Japan.

During the First World War, Great Britain's Grand Fleet and Germany's Kaiserliche Marine faced each other in the North Sea, which became the main theatre of the war for surface action. Britain's larger fleet and North Sea Mine Barrage were able to establish an effective blockade for most of the war, which restricted the Central Powers' access to many crucial resources. Major battles included the Battle of Heligoland Bight, the Battle of the Dogger Bank, and the Battle of Jutland.
World War I also brought the first extensive use of submarine warfare, and a number of submarine actions occurred in the North Sea.

The Second World War also saw action in the North Sea, though it was restricted more to aircraft reconnaissance, and action by fighter/bomber aircraft, submarines, and smaller vessels such as minesweepers and torpedo boats.

In the aftermath of the war, hundreds of thousands of tons of chemical weapons were disposed of by being dumped in the North Sea.

After the war, the North Sea lost much of its military significance because it is bordered only by NATO member-states. However, it gained significant economic importance in the 1960s as the states around the North Sea began full-scale exploitation of its oil and gas resources. The North Sea continues to be an active trade route.

Countries that border the North Sea all claim the of territorial waters, within which they have exclusive fishing rights. The Common Fisheries Policy of the European Union (EU) exists to coordinate fishing rights and assist with disputes between EU states and the EU border state of Norway.

After the discovery of mineral resources in the North Sea, the Convention on the Continental Shelf established country rights largely divided along the median line. The median line is defined as the line "every point of which is equidistant from the nearest points of the baselines from which the breadth of the territorial sea of each State is measured."
The ocean floor border between Germany, the Netherlands, and Denmark was only reapportioned after protracted negotiations and a judgement of the International Court of Justice.

As early as 1859, oil was discovered in onshore areas around the North Sea and natural gas as early as 1910. Onshore resources, for example the K12-B field in the Netherlands continue to be exploited today.

Offshore test drilling began in 1966 and then, in 1969, Phillips Petroleum Company discovered the Ekofisk oil field distinguished by valuable, low-sulphur oil. Commercial exploitation began in 1971 with tankers and, after 1975, by a pipeline, first to Teesside, England and then, after 1977, also to Emden, Germany.

The exploitation of the North Sea oil reserves began just before the 1973 oil crisis, and the climb of international oil prices made the large investments needed for extraction much more attractive.
The start in 1973 of the oil reserves by UK allowed them to stop the declining position in the international trade in 1974, and a huge increase after the discovery and exploitation of the huge oil field by Phillips group in 1977 as the Brae field.

Although the production costs are relatively high, the quality of the oil, the political stability of the region, and the proximity of important markets in western Europe has made the North Sea an important oil-producing region. The largest single humanitarian catastrophe in the North Sea oil industry was the destruction of the offshore oil platform Piper Alpha in 1988 in which 167 people lost their lives.

Besides the Ekofisk oil field, the Statfjord oil field is also notable as it was the cause of the first pipeline to span the Norwegian trench. The largest natural gas field in the North Sea, Troll gas field, lies in the Norwegian trench, dropping over , requiring the construction of the enormous Troll A platform to access it.

The price of Brent Crude, one of the first types of oil extracted from the North Sea, is used today as a standard price for comparison for crude oil from the rest of the world. The North Sea contains western Europe's largest oil and natural gas reserves and is one of the world's key non-OPEC producing regions.

In the UK sector of the North Sea, the oil industry invested £14.4 billion in 2013, and was on track to spend £13 billion in 2014. Industry body Oil & Gas UK put the decline down to rising costs, lower production, high tax rates, and less exploration.

As of January 2018 The North Sea region contains 184 offshore rigs, which makes it the region with the highest number of offshore rigs in the world.

The North Sea is Europe's main fishery accounting for over 5% of international commercial fish caught. Fishing in the North Sea is concentrated in the southern part of the coastal waters. The main method of fishing is trawling.
In 1995, the total volume of fish and shellfish caught in the North Sea was approximately 3.5 million tonnes. Besides fish, it is estimated that one million tonnes of unmarketable by-catch is caught and discarded each year.

In recent decades, overfishing has left many fisheries unproductive, disturbing marine food chain dynamics and costing jobs in the fishing industry. Herring, cod and plaice fisheries may soon face the same plight as mackerel fishing, which ceased in the 1970s due to overfishing.
The objective of the European Union Common Fisheries Policy is to minimize the environmental impact associated with resource use by reducing fish discards, increasing productivity of fisheries, stabilising markets of fisheries and fish processing, and supplying fish at reasonable prices for the consumer.

Whaling was an important economic activity from the 9th until the 13th century for Flemish whalers. The medieval Flemish, Basque and Norwegian whalers who were replaced in the 16th century by Dutch, English, Danes and Germans, took massive numbers of whales and dolphins and nearly depleted the right whales. This activity likely led to the extinction of the Atlantic population of the once common gray whale. By 1902 the whaling had ended. After being absent for 300 years a single gray whale returned, it probably was the first of many more to find its way through the now ice-free Northwest Passage.

In addition to oil, gas, and fish, the states along the North Sea also take millions of cubic metres per year of sand and gravel from the ocean floor. These are used for beach nourishment, land reclamation
and construction.
Rolled pieces of amber may be picked up on the east coast of England.

Due to the strong prevailing winds, and shallow water, countries on the North Sea, particularly Germany and Denmark, have used the shore for wind power since the 1990s. The North Sea is the home of one of the first large-scale offshore wind farms in the world, Horns Rev 1, completed in 2002. Since then many other wind farms have been commissioned in the North Sea (and elsewhere). As of 2013 the 630 megawatt (MW) London Array is the largest offshore wind farm in the world, with the 504 (MW) Greater Gabbard wind farm the second largest, followed by the 367 MW Walney Wind Farm. All are off the coast of the UK. These projects will be dwarfed by subsequent wind farms that are in the pipeline, including Dogger Bank at 4,800 MW, Norfolk Bank (7,200 MW), and Irish Sea (4,200 MW). At the end of June 2013 total European combined offshore wind energy capacity was 6,040 MW. UK installed 513.5 MW offshore windpower in the first half-year of 2013.

The expansion of offshore wind farms has met with some resistance. Concerns have included shipping collisions and environmental effects on ocean ecology and wildlife such as fish and migratory birds, however, these concerns were found to be negligible in a long-term study in Denmark released in 2006 and again in a UK government study in 2009.
There are also concerns about reliability, and the rising costs of constructing and maintaining offshore wind farms. Despite these, development of North Sea wind power is continuing, with plans for additional wind farms off the coasts of Germany, the Netherlands, and the UK. There have also been proposals for a transnational power grid in the North Sea to connect new offshore wind farms.

Energy production from tidal power is still in a pre-commercial stage. The European Marine Energy Centre has installed a wave testing system at Billia Croo on the Orkney mainland and a tidal power testing station on the nearby island of Eday. Since 2003, a prototype Wave Dragon energy converter has been in operation at Nissum Bredning fjord of northern Denmark.

The beaches and coastal waters of the North Sea are destinations for tourists. The Belgian, Dutch, German and Danish coasts are developed for tourism. The North Sea coast of the United Kingdom has tourist destinations with beach resorts and golf courses. Fife in Scotland is famous for its links golf courses; the coastal city of St. Andrews is renowned as the "Home of Golf". The coast of North East England has several tourist towns such as Scarborough, Bridlington, Seahouses, Whitby, Robin Hood's Bay and Seaton Carew, and has some long sandy beaches and links golfing locations such as Seaton Carew Golf Club and Goswick Golf Club.

The North Sea Trail is a long-distance trail linking seven countries around the North Sea. Windsurfing and sailing are popular sports because of the strong winds. Mudflat hiking, recreational fishing and birdwatching are among other activities.

The climatic conditions on the North Sea coast have been claimed to be healthy. As early as the 19th century, travellers visited the North Sea coast for curative and restorative vacations. The sea air, temperature, wind, water, and sunshine are counted among the beneficial conditions that are said to activate the body's defences, improve circulation, strengthen the immune system, and have healing effects on the skin and the respiratory system.

The Wadden Sea in Denmark, Germany and the Netherlands is an UNESCO World Heritage Site.

The North Sea is important for marine transport and its shipping lanes are among the busiest in the world. Major ports are located along its coasts: Rotterdam, the busiest port in Europe and the fourth busiest port in the world by tonnage , Antwerp (was 16th) and Hamburg (was 27th), Bremen/Bremerhaven and Felixstowe, both in the top 30 busiest container seaports, as well as the Port of Bruges-Zeebrugge, Europe's leading ro-ro port.

Fishing boats, service boats for offshore industries, sport and pleasure craft, and merchant ships to and from North Sea ports and Baltic ports must share routes on the North Sea. The Dover Strait alone sees more than 400 commercial vessels a day. Because of this volume, navigation in the North Sea can be difficult in high traffic zones, so ports have established elaborate vessel traffic services to monitor and direct ships into and out of port.

The North Sea coasts are home to numerous canals and canal systems to facilitate traffic between and among rivers, artificial harbours, and the sea. The Kiel Canal, connecting the North Sea with the Baltic Sea, is the most heavily used artificial seaway in the world reporting an average of 89 ships per day not including sporting boats and other small watercraft in 2009. It saves an average of , instead of the voyage around the Jutland peninsula. The North Sea Canal connects Amsterdam with the North Sea.





</doc>
<doc id="21180" url="https://en.wikipedia.org/wiki?curid=21180" title="Natural Born Killers">
Natural Born Killers

Natural Born Killers is a 1994 American crime film directed by Oliver Stone and starring Woody Harrelson, Juliette Lewis, Robert Downey Jr., Tom Sizemore, and Tommy Lee Jones. The film tells the story of two victims of traumatic childhoods who became lovers and mass murderers, and are irresponsibly glorified by the mass media.

The film is based on an original screenplay by Quentin Tarantino that was heavily revised by Stone, writer David Veloz, and associate producer Richard Rutowski. Tarantino received a story credit though he subsequently disowned the film. Jane Hamsher, Don Murphy, and Clayton Townsend produced the film, with Arnon Milchan, Thom Mount, and Stone as executive producers.

The film was released on August 26, 1994 in the United States, and also screened at the Venice Film Festival on August 29, 1994. It was a box office success, grossing over $50 million against a production budget of $34 million, but received a mixed critical reception. Some critics praised the plot, acting, humor, and combination of action and romance, while others found the film overly violent and graphic. Notorious for its violent content and inspiring "copycat" crimes, the film was named the eighth most controversial film in history by "Entertainment Weekly" in 2006.

Mickey Knox and his wife Mallory stop at a diner in the New Mexico desert. A duo of rednecks arrive and begin sexually harassing Mallory as she dances by a jukebox. She initially encourages it before beating one of the men viciously. Mickey joins her, and the couple murder everyone in the diner, save one staff member, to whom they proudly declare their names before leaving. The couple camp in the desert, and Mallory reminisces about how she met Mickey, a meat deliveryman who serviced her family's household. After a whirlwind romance, the couple murdered Mallory's sexually abusive father and neglectful mother before having an unofficial marriage ceremony on a bridge.

Later, Mickey and Mallory hold a woman hostage in their hotel room. Angered by Mickey's desire for a threesome, Mallory leaves, and Mickey rapes the hostage. Mallory drives to a nearby gas station, where she flirts with a mechanic. They begin to have sex on the hood of a car, but Mallory kills him after suffering a flashback of being raped by her father. The pair continue their killing spree, ultimately claiming fifty-two victims in New Mexico, Arizona, and Nevada. Pursuing them is Detective Jack Scagnetti, who became obsessed with mass murderers after witnessing his mother being shot and killed by Charles Whitman when he was eight. Beneath his heroic facade, he is also a violent psychopath and has murdered prostitutes in his past. Following the Knoxes murder spree is a self-serving tabloid journalist, Wayne Gale, who profiles them on his show, "American Maniacs", soon elevating them to cult hero status.

Mickey and Mallory get lost in the desert after taking psychedelic mushrooms, and stumble upon a ranch owned by Warren Red Cloud, a Navajo man who provides them food and shelter. As Mickey and Mallory sleep, Warren senses evil in the couple and attempts to exorcise the demon he perceives in Mickey, chanting over him as he sleeps. Mickey, who has nightmares recounting his abusive parents, awakens during the exorcism, and shoots Warren to death. As the couple flee, they feel inexplicably guilty, and come across a giant field of rattlesnakes, where they are badly bitten. They reach a drugstore to purchase snakebite antidote, but the store is sold out. A pharmacist recognizes the couple and sets off an alarm before Mickey kills him. Police arrive shortly after and accost the couple, which ends in a shootout followed by police beating the couple while a news crew films the action.

One year later, an imprisoned Mickey and Mallory are scheduled to be transferred to psychiatric hospitals. Scagnetti orders Warden Dwight McClusky to kill the Knoxes during their transfer, and claim they tried to escape. Meanwhile, Gale has persuaded Mickey to give a live interview that will air after the Super Bowl. During the interview, Mickey declares himself a "natural born killer," a phrase that inspires other inmates to start a prison riot. After the interview is terminated by McClusky, Mickey is left alone with Gale, the film crew, and several guards; he manages to overpower a guard and kill most of the people in the room, taking Gale and several others hostage. Gale and his crew give a live television report that profiles the riot. Meanwhile, Scagnetti attempts to seduce Mallory in her cell. She responds by beating him viciously, and another guard subdues her with tear gas. Mickey and Gale reach Mallory's cell, where Mickey kills the guards and engages in a standoff with Scagnetti before Mallory murders him.

Gale's entire television crew is killed trying to escape the riot, while Gale himself begins indulging in violence, shooting at prison guards. Mickey and Mallory steal a van and escape into the woods with Gale, to whom they give a final interview before declaring that he must die. He attempts various arguments to change their minds, appealing to their trademark practice of leaving one survivor; Mickey informs him they are leaving a witness to tell the tale, his camera. Gale accepts his fate and is shot to death. Unbeknownst to the three, the entire exchange is transmitted to a horrified news anchor through Gale's in-ear microphone.

Several years later, Mickey and Mallory, still on the run, travel in an RV, as a pregnant Mallory watches their two children play.

One of the central themes of "Natural Born Killers" is the relationship between real-life violence and the mass media's coverage of it. This thematic preoccupation was declared in the film's promotional materials, with its theatrical poster advertising it as a "bold new film that takes a look at a country seduced by fame, obsessed by crime, and consumed by the media."

The character of Wayne Gale, the television host of "American Maniacs", functions in the film as a figurehead of lurid true crime television documentaries, which recycle real-life incidents of violence and criminal activity into entertainment for the general public. On several occasions, expressionistic jump cuts featuring Gale as a blood-soaked Satan are interspersed into the film, which Muir suggests emphasizes the film's assertion that mass media and crime mutually reinforce one another.

Media representation of the nuclear family has been identified as another theme in the film, particularly with the depiction of Mallory's dysfunctional family life, which includes a neglectful mother and a father who sexually abuses her. Muir notes that the sequence depicting Mallory's home life—presented as a television sitcom with the title "I Love Mallory" (a parody of "I Love Lucy")—charts "the colossal gulf between the imagery sold to America regarding family life and the truth, for many Americans, of such family life in the 1990s." The "sitcom" representation of Mallory's household results in a visual dichotomy between her "life as she imagined it should be (replete with an oppressive laugh track eradicating any scary sense of ambiguity)" and the "grim truth of it."

"Natural Born Killers" was based on a screenplay written by Quentin Tarantino, in which a married couple suddenly decide to go on a killing spree. Tarantino had sold an option for his script to producers Jane Hamsher and Don Murphy for $10,000 after he had tried, and failed, to direct it himself for $500,000. Hamsher and Murphy subsequently sold the screenplay to Warner Bros. Around the same time, Oliver Stone was made aware of the script. He was keen to find something more straightforward than his previous production, "Heaven & Earth" (1993), a difficult shoot which had left him exhausted.

David Veloz, associate producer Richard Rutowski, and Stone rewrote Tarantino's script, keeping much of the dialogue but changing the focus of the film from journalist Wayne Gale to Mickey and Mallory. The script was revised so drastically that Tarantino was credited for the story only. In a 1993 interview, Tarantino stated that he did not hold any animosity towards Stone, and that he wished the film well.

Initially, when producers Hamsher and Murphy had first brought the script to Stone's attention, he had envisioned it as an action film; "something Arnold Schwarzenegger would be proud of." As the project developed however, incidents such as the O. J. Simpson case, the Menendez brothers case, the Tonya Harding/Nancy Kerrigan incident, the Rodney King incident, and the Federal assault of the Branch Davidian sect all took place. Stone came to feel that the media was heavily involved in the outcome of all of these cases, and that the media had become an all-pervasive entity which marketed violence and suffering for the good of ratings. As such, he changed the tone of the film from one of purely action to a "vicious, coldhearted farce" on the media. Coloring Stone's approach to the material, and contributing to the violent nature of the film, were the anger and sadness he felt at the breakdown of his second marriage. He also said in an interview that the film was influenced by the "vitality" of Indian cinema.

Stone cast Woody Harrelson partly because, "frankly, he had that American, trashy look. There's something about Woody that evokes Kentucky or white trash." At the time, Harrelson was primarily known for his comedic performances, namely his role on the sitcom "Cheers", and Stone was compelled to cast him against type. Stone cast Lewis for similar reason, noting that, despite her success as portraying a defiled teenage daughter in "Cape Fear" (1991), he felt she could "pull off white trash, too. Juliette has malice in her eyes. She's got adorable eyes, but they jump and they gleam. I just felt they [were both] right. They didn't feel like they were upper-class people." Stone tried to convince Lewis to gain muscle mass for her role as Mallory so that she looked tougher, but she refused, saying she wanted the character to look like a pushover, not a bodybuilder.

Robert Downey Jr. was cast as Wayne Gale, the reporter chronicling the Knoxes; Downey prepared for his role as reporter Wayne Gale by spending time with Australian TV shock-king Steve Dunleavy, and later convinced Stone to allow him to portray Gale with an Australian accent. Tom Sizemore was cast as Detective Jack Scagnetti, the psychotic police officer with murderous impulses himself, while Tommy Lee Jones was cast as Dwight McClusky, a prison warden who appears in the last act of the film. Rodney Dangerfield, primarily known as a stand-up comedian, portrayed Mallory's rapist father, and was allowed by Stone to rewrite all of his own character's lines.

Principal photography took 56 days to shoot. Filming locations included the Rio Grande Gorge Bridge just west of Taos, New Mexico, where the wedding scene was filmed, and Stateville Correctional Center in Joliet, Illinois, where the prison riot was filmed. In Stateville, 80% of the prisoners are incarcerated for violent crimes. For the first two weeks on location at the prison, the extras were actual inmates with rubber weapons. For the subsequent two weeks, 200 extras were needed because the Stateville inmates were on lockdown. According to Tom Sizemore, during filming on the prison set, Stone would play African tribal music at full blast between takes to keep the frantic energy up. While shooting the POV scene wherein Mallory runs into the wire mesh, director of photography Robert Richardson broke his finger and the replacement cameraman cut his eye. According to Oliver Stone, he was not popular with the camera department on set that day. For the scenes involving rear projection, the projected footage was shot prior to principal photography, then edited together, and projected onto the stage, behind the live actors. For example, when Mallory drives past a building and flames are projected onto the wall, this was shot live using footage projected onto the facade of a real building.

The famous Coca-Cola polar bear ad is seen twice during the film. According to Stone, Coca-Cola approved the use of the ad without having a full idea of what the film was about. When they saw the completed film, they were furious.

"Natural Born Killers" was filmed and edited in a frenzied and psychedelic style, and features both color and black and white cinematography, as well as animation, and other unusual color schemes and visual compositions. Editing of the film lasted approximately 11 months, with the final film containing almost 3,000 cuts (most films have 600–700). The film also employs a wide range of camera angles, featuring Dutch tilts prominently throughout, with the camera rarely angling along a horizontal field of vision. Film scholar Robert Kolker notes that the Dutch angle's employment in the film is "the visual equivalent of a profound dislocation, a loss of object constancy, the slipperiness of subjectivity itself." Kolker comments that, unlike such films as "Bonnie and Clyde" from which "Natural Born Killers" draws influence, "from the very beginning...  the viewer is forced into a dual situation, neither one of which allows easy access to the main characters. One situation, continued throughout the film, is a kind of rhythmic attention created by a startling flow of images. Stone builds his visuals on unexpected linkages and disorienting juxtapositions within the shots and edits."

Because the film is thematically preoccupied with media, Stone sought to implement visual elements of popular television into the film's visual tableau: "It had never quite been done before — a mixture of stocks and styles. I was influenced, I have to say, by MTV and some of the styles I saw in the early '80s and '90s on television. But no one had tried that style over the course of 90, 100 minutes." Commercials which were commonly on the air at the time of the film's release make brief, intermittent appearances as well.

Concurrent with Stone's preoccupation with television as both a visual and thematic reference point, portions of the film are narrated through parodies of popular television series, including a sequence presented in the style of a sitcom about Mallory's dysfunctional family (titled "I Love Mallory"), a parody of "I Love Lucy". In the film's final montage, splices of real-life television news coverage of various criminal cases of the time are included, such as the O. J. Simpson case, the Menendez brothers, and the Tonya Harding/Nancy Kerrigan incident. Film scholar John Kenneth Muir notes this inclusion as an "exclamation point" concluding the film's thesis: "It seems to say, "Welcome to the tabloid-TV culture of America in the 1990s, where crime pays and pays well.""

The film's soundtrack was produced by Stone and Trent Reznor of Nine Inch Nails, who reportedly watched the film over 50 times to "get in the mood". Reznor reportedly produced the soundtrack while on tour. On his approach to compiling the soundtrack, Reznor told MTV:

I suggested to Oliver [Stone] to try to turn the soundtrack into a collage-of-sound, kind of the way the movie used music: make edits, add dialog, and make it something interesting, rather than a bunch of previously released music.

Some songs were written especially for the film or soundtrack, such as "Burn" by Nine Inch Nails.

In its opening weekend, "Natural Born Killers" grossed a total of $11.2 million in 1,510 theaters, finishing 1st. It finished its theatrical run with a total gross of $50.3 million, against its $34 million budget.

On review aggregator Rotten Tomatoes, the film has an approval rating of 46% based on 39 reviews, with an average rating of 5.63/10. The website's critical consensus reads, ""Natural Born Killers" explodes off the screen with style, but its satire is too blunt to offer any fresh insight into celebrity or crime -- pummeling the audience with depravity until the effect becomes deadening." On Metacritic, the film has an average weighted score of 74 out of 100, based on 20 critics, indicating "generally favorable reviews". Audiences polled by CinemaScore gave the film an average grade of "B–" on an A+ to F scale.

Roger Ebert of the "Chicago Sun-Times" gave the film four stars out of four and wrote, "Seeing this movie once is not enough. The first time is for the visceral experience, the second time is for the meaning." On his television show, his partner Gene Siskel agreed with him, adding extra praise to the scene featuring Rodney Dangerfield.

Other critics found the film unsuccessful in its aims. Hal Hinson of "The Washington Post" claimed that "Stone's sensibility is white-hot and personal. As much as he'd like us to believe that his camera is turned outward on the culture, it's vividly clear that he can't resist turning it inward on himself. This wouldn't be so troublesome if Stone didn't confuse the public and the private." Janet Maslin of "The New York Times" wrote, "for all its surface passions, "Natural Born Killers" never digs deep enough to touch the madness of such events, or even to send them up in any surprising way. Mr. Stone's vision is impassioned, alarming, visually inventive, characteristically overpowering. But it's no match for the awful truth."

James Berardinelli gave the film a negative review but his criticism was different from many other such pans, which generally said that Oliver Stone was a hypocrite for making an ultra-violent film in the guise of a critique of American attitudes. Berardinelli noted that the movie "hits the bullseye" as a satire of America's lust for bloodshed, but repeated Stone's main point so often and so loudly that it became unbearable.

At the 1994 Stinkers Bad Movie Awards, Harrelson was nominated for Worst Actor but lost to Bruce Willis for "Color of Night" and "North". The film was nominated for Worst Picture but lost to "North".

"Natural Born Killers" was released on VHS in 1995 by Warner Home Video. A director's cut version of the film was released the following year on VHS by Vidmark/Lionsgate, who also released a non-anamorphic DVD of the director's cut in 2000. Distribution rights to Stone's director's cut reverted from Lionsgate to Warner Bros. in 2009, after which Warner issued an anamorphic DVD edition as well as a Blu-ray.

After Quentin Tarantino attempted to publish his original screenplay to "Natural Born Killers" as a paperback book, as he had done with his scripts for "True Romance" and his own directorial efforts, "Reservoir Dogs" and "Pulp Fiction", the producers of "Natural Born Killers" filed a lawsuit against Tarantino, claiming that when he sold the script to them, he had forfeited the publishing rights; eventually, Tarantino was allowed to publish his original script.

Tarantino disowned the film, saying, "I hated that fucking movie. If you like my stuff, don't watch that movie."

When the film was first handed in to the MPAA, they told Stone they would give it an NC-17 unless he cut it. As such, Stone toned down the violence by cutting approximately four minutes of footage, and the MPAA re-rated the film as an R. In 1996, a Director's Cut was released on home video by Vidmark Entertainment and Pioneer Entertainment, as Warner Bros. wanted nothing to do with that particular version. Warner Home Video later released this cut on Blu-ray.

The film was banned completely upon release in Ireland, including – controversially – from cinema clubs. The ban was later quietly lifted.

In the UK, though the cinema release was delayed while the BBFC investigated reports that the film caused copycat murders in the USA and France, it was finally shown in cinemas in February 1995.

The original intended UK home video release in March 1996 was cancelled due to the Dunblane massacre in Scotland. In the meantime, Channel Five showed the film in November 1997. It was finally released on video in July 2001.

"Entertainment Weekly" ranked the film as the eighth most controversial film ever.

From almost the moment of its release, the film has been accused of encouraging and inspiring numerous murderers in North America, including the Heath High School shooting and the Columbine High School massacre. The Columbine killers even code-named their attack: "NBK", an acronym for "Natural Born Killers".





</doc>
<doc id="21181" url="https://en.wikipedia.org/wiki?curid=21181" title="Nancy Reagan">
Nancy Reagan

Nancy Davis Reagan (born Anne Frances Robbins; July 6, 1921 – March 6, 2016) was an American film actress and the second wife of Ronald Reagan, the 40th president of the United States. She was the first lady of the United States from 1981 to 1989.

She was born in New York City. After her parents separated, she lived in Maryland with an aunt and uncle for six years. When her mother remarried in 1929, she moved to Chicago and later was adopted by her mother's second husband. As Nancy Davis, she was a Hollywood actress in the 1940s and 1950s, starring in films such as "The Next Voice You Hear...", "Night into Morning", and "Donovan's Brain". In 1952, she married Ronald Reagan, who was then president of the Screen Actors Guild. They had two children together. Reagan was the first lady of California when her husband was governor from 1967 to 1975, and she began to work with the Foster Grandparents Program.

Reagan became First Lady of the United States in January 1981, following her husband's victory in the 1980 presidential election. Early in his first term, she was criticized largely due to her decision to replace the White House china, which had been paid for by private donations. She championed recreational drug prevention causes when she founded the "Just Say No" drug awareness campaign, which was considered her major initiative as First Lady. More discussion of her role ensued following a 1988 revelation that she had consulted an astrologer to assist in planning the president's schedule after the attempted assassination of her husband in 1981. She generally had a strong influence on her husband and played a role in a few of his personnel and diplomatic decisions.

After Ronald Reagan's term as president ended, the couple returned to their home in Bel Air, Los Angeles, California. Nancy devoted most of her time to caring for her husband, who was diagnosed with Alzheimer's disease in 1994, until his death at the age of 93 on June 5, 2004. Reagan remained active within the Reagan Library and in politics, particularly in support of embryonic stem cell research, until her death from congestive heart failure at age 94 on March 6, 2016.
She was born as Anne Frances Robbins on July 6, 1921, at Sloane Hospital for Women in Midtown Manhattan. She was of fully English descent. She was the only child of Kenneth Seymour Robbins (1892–1972), a farmer turned car salesman who had been born into a once-prosperous family, and his actress wife, Edith Prescott Luckett (1888–1987). Her godmother was silent-film-star Alla Nazimova. From birth, she was commonly called Nancy.

She lived her first two years in Flushing, Queens, a borough of New York City, in a two-story house on Roosevelt Avenue between 149th and 150th Streets. Her parents separated soon after her birth and were divorced in 1928. After their separation, her mother traveled the country to pursue acting jobs and Robbins was raised in Bethesda, Maryland, for six years by her aunt, Virginia Luckett, and uncle, Audley Gailbraith, where she attended Sidwell Friends School for kindergarten through second grade. Nancy later described longing for her mother during those years: "My favorite times were when Mother had a job in New York, and Aunt Virgie would take me by train to stay with her."

In 1929, her mother married Loyal Edward Davis (1896–1982), a prominent conservative neurosurgeon who moved the family to Chicago. Nancy and her stepfather got along very well; she later wrote that he was "a man of great integrity who exemplified old-fashioned values". He formally adopted her in 1938, and she would always refer to him as her father. At the time of the adoption, her name was legally changed to Nancy Davis. She attended the Girls' Latin School of Chicago (describing herself as an average student), from 1929, until she graduated in 1939, and later attended Smith College in Massachusetts, where she majored in English and drama, graduating in 1943.

In 1940, a young Davis had appeared as a National Foundation for Infantile Paralysis volunteer in a memorable short subject film shown in movie theaters to raise donations for the crusade against polio. "The Crippler" featured a sinister figure spreading over playgrounds and farms, laughing over its victims, until finally dispelled by the volunteer. It was very effective in raising contributions.

Following her graduation from college, Davis held jobs in Chicago as a sales clerk in Marshall Field's department store and as a nurse's aide. With the help of her mother's colleagues in theatre, including ZaSu Pitts, Walter Huston, and Spencer Tracy, she pursued a professional career as an actress. She first gained a part in Pitts' 1945 road tour of "Ramshackle Inn", moving to New York City. She landed the role of Si-Tchun, a lady-in-waiting, in the 1946 Broadway musical about the Orient, "Lute Song", starring Mary Martin and a pre-fame Yul Brynner. The show's producer told her, "You look like you could be Chinese."

After passing a screen test, she moved to California and signed a seven-year contract with Metro-Goldwyn-Mayer Studios Inc. (MGM) in 1949; she later remarked, "Joining Metro was like walking into a dream world." Her combination of attractive appearance—centered on her large eyes—and somewhat distant and understated manner made her hard at first for MGM to cast and publicize. Davis appeared in eleven feature films, usually typecast as a "loyal housewife", "responsible young mother", or "the steady woman". Jane Powell, Debbie Reynolds, Leslie Caron, and Janet Leigh were among the actresses with whom she competed for roles at MGM.

Davis' film career began with small supporting roles in two films that were released in 1949, "The Doctor and the Girl" with Glenn Ford and "East Side, West Side" starring Barbara Stanwyck. She played a child psychiatrist in the film noir "Shadow on the Wall" (1950) with Ann Sothern and Zachary Scott; her performance was called "beautiful and convincing" by "New York Times" critic A. H. Weiler. She co-starred in 1950's "The Next Voice You Hear...", playing a pregnant housewife who hears the voice of God from her radio. Influential reviewer Bosley Crowther of "The New York Times" wrote that "Nancy Davis [is] delightful as [a] gentle, plain, and understanding wife." In 1951, Davis appeared in "Night into Morning", her favorite screen role, a study of bereavement starring Ray Milland. Crowther said that Davis "does nicely as the fiancée who is widowed herself and knows the loneliness of grief", while another noted critic, "The Washington Post"<nowiki>'</nowiki>s Richard L. Coe, said Davis "is splendid as the understanding widow". MGM released Davis from her contract in 1952; she sought a broader range of parts, but also married Reagan, keeping her professional name as Davis, and had her first child that year. She soon starred in the science fiction film "Donovan's Brain" (1953); Crowther said that Davis, playing the role of a possessed scientist's "sadly baffled wife", "walked through it all in stark confusion" in an "utterly silly" film. In her next-to-last movie, "Hellcats of the Navy" (1957), she played nurse Lieutenant Helen Blair, and appeared in a film for the only time with her husband, playing what one critic called "a housewife who came along for the ride". Another reviewer, however, stated that Davis plays her part satisfactorily, and "does well with what she has to work with".

Author Garry Wills has said that Davis was generally underrated as an actress because her constrained part in "Hellcats" was her most widely seen performance. In addition, Davis downplayed her Hollywood goals: promotional material from MGM in 1949 said that her "greatest ambition" was to have a "successful happy marriage"; decades later, in 1975, she would say, "I was never really a career woman but [became one] only because I hadn't found the man I wanted to marry. I couldn't sit around and do nothing, so I became an actress." Ronald Reagan biographer Lou Cannon nevertheless characterized her as a "reliable" and "solid" performer who held her own in performances with better-known actors. After her final film, "Crash Landing" (1958), Davis appeared for a brief time as a guest star in television dramas, such as the "Zane Grey Theatre" episode "The Long Shadow" (1961), where she played opposite Ronald Reagan, as well as "Wagon Train" and "The Tall Man", until she retired as an actress in 1962.

During her career, Davis served for nearly ten years on the board of directors of the Screen Actors Guild. Decades later, Albert Brooks attempted to coax her out of acting retirement by offering her the title role opposite himself in his 1996 film "Mother". She declined in order to care for her husband, and Debbie Reynolds played the part.

During her Hollywood career, Davis dated many actors, including Clark Gable, Robert Stack, and Peter Lawford; she later called Gable the nicest of the stars she had met. On November 15, 1949, she met Ronald Reagan, who was then president of the Screen Actors Guild. She had noticed that her name had appeared on the Hollywood blacklist. Davis sought Reagan's help to maintain her employment as a guild actress in Hollywood and for assistance in having her name removed from the list. Ronald Reagan informed her that she had been confused with another actress of the same name. The two began dating and their relationship was the subject of many gossip columns; one Hollywood press account described their nightclub-free times together as "the romance of a couple who have no vices". Ronald Reagan was skeptical about marriage, however, following his painful 1949 divorce from Jane Wyman, and he still saw other women.

After three years of dating, they eventually decided to marry while discussing the issue in the couple's favorite booth at Chasen's, a restaurant in Beverly Hills. The couple wed on March 4, 1952, at the Little Brown Church in the San Fernando Valley of Los Angeles, in a simple and hastily arranged ceremony designed to avoid the press; the marriage was her first and his second. The only people in attendance were fellow actor William Holden (the best man) and his wife, actress Brenda Marshall (the matron of honor). Nancy was likely already pregnant during the ceremony; the couple's first child, Patricia Ann Reagan (later better known by her professional name, Patti Davis), was born less than eight months later on October 21, 1952. Their son, Ronald Prescott Reagan (later better known as Ron Reagan) was born six years later on May 20, 1958. Reagan also became stepmother to Maureen Reagan (1941–2001) and Michael Reagan (b. 1945), her husband's children from his first marriage to Jane Wyman.
Observers described Nancy and Ronald's relationship as intimate. As president and first lady, the Reagans were reported to display their affection frequently, with one press secretary noting, "They never took each other for granted. They never stopped courting." Ronald often called Nancy "Mommy"; she called him "Ronnie". While the president was recuperating in the hospital after the 1981 assassination attempt, Nancy wrote in her diary, "Nothing can happen to my Ronnie. My life would be over." In a letter to Nancy, Ronald wrote, "whatever I treasure and enjoy ... all would be without meaning if I didn't have you." In 1998, a few years after her husband had been given a diagnosis of Alzheimer's disease, Nancy told "Vanity Fair", "Our relationship is very special. We were very much in love and still are. When I say my life began with Ronnie, well, it's true. It did. I can't imagine life without him." Nancy was known for the focused and attentive look, termed "the Gaze", that she fastened upon her husband during his speeches and appearances.

President Reagan's death in June 2004 ended what Charlton Heston called "the greatest love affair in the history of the American Presidency".

Nancy's relationship with her children was not always as close as the bond with her husband. She frequently quarreled with her children and her stepchildren. Her relationship with Patti was the most contentious; Patti flouted American conservatism, rebelled against her parents by joining the nuclear freeze movement, and authored many anti-Reagan books. The nearly 20 years of family feuding left Patti very much estranged from both her mother and father. Soon after her father's Alzheimer's disease was diagnosed, Patti and her mother reconciled and began to speak on a daily basis. Nancy's disagreements with Michael were also public matters; in 1984, she was quoted as saying that the two were in an "estrangement right now". Michael responded that Nancy was trying to cover up for the fact she had not met his daughter, Ashley, who had been born nearly a year earlier. They too eventually made peace. Nancy was thought to be closest to her stepdaughter Maureen during the White House years, but each of the Reagan children experienced periods of estrangement from their parents.

Nancy Reagan was First Lady of California during her husband's two terms as governor. She disliked living in the state capital of Sacramento, which lacked the excitement, social life, and mild climate to which she was accustomed in Los Angeles. She first attracted controversy early in 1967; after four months' residence in the California Governor's Mansion in Sacramento, she moved her family into a wealthy suburb because fire officials had labelled the mansion as a "firetrap". Though the Reagans had leased the new house at their expense, the move was viewed as snobbish when the matter was brought to the attention of the general public. Reagan defended her actions as being for the good of her family, a judgment with which her husband readily agreed. Friends of the family later helped support the cost of the leased house, while Reagan supervised construction of a new ranch-style governor's residence in nearby Carmichael. The new residence was finished just as Ronald Reagan left office in 1975, but his successor, Jerry Brown, refused to live there. It was sold in 1982, and California governors lived in improvised arrangements until Brown moved into the Governor's Mansion in 2015.

In 1967, Governor Reagan appointed his wife to the California Arts Commission, and a year later she was named "Los Angeles Times"' Woman of the Year; in its profile, the "Times" labeled her "A Model First Lady". Her glamour, style, and youthfulness,
made her a frequent subject for press photographers. As first lady, Reagan visited veterans, the elderly, and the handicapped, and worked with a number of charities. She became involved with the Foster Grandparents Program, helping to popularize it in the United States and Australia. She later expanded her work with the organization after arriving in Washington, and wrote about her experiences in her 1982 book "To Love a Child". The Reagans held dinners for former POWs and Vietnam War veterans while governor and first lady.

Governor Reagan's gubernatorial time in office ended in 1975, and he did not run for a third term; instead, he met with advisors to discuss a possible bid for the 1976 presidency, challenging incumbent President Gerald Ford. Ronald still needed to convince a reluctant Nancy before running, however. She feared for her husband's health and his career as a whole, though she felt that he was the right man for the job and eventually approved. Nancy took on a traditional role in the campaign, holding coffees, luncheons, and talks. She also oversaw personnel, monitored her husband's schedule, and occasionally provided press conferences. The 1976 campaign included the so-called "battle of the queens", contrasting Nancy with First Lady Betty Ford. They both spoke out over the course of the campaign on similar issues, but with different approaches. Nancy was upset by the warmonger image that the Ford campaign had drawn of her husband.

Though he lost the 1976 Republican nomination, Ronald Reagan ran for the presidency a second time in 1980. He succeeded in winning the nomination and defeated incumbent rival Jimmy Carter in a landslide. During this second campaign, Nancy played a prominent role, and her management of staff became more apparent. She organized a meeting among feuding campaign managers John Sears and Michael Deaver and her husband, which resulted in Deaver leaving the campaign and Sears being given full control. After the Reagan camp lost the Iowa Caucus and fell behind in New Hampshire polls, Nancy organized a second meeting and decided it was time to fire Sears and his associates; she gave Sears a copy of the press release announcing his dismissal. Her influence on her husband became particularly notable; her presence at rallies, luncheons, and receptions increased his confidence.

Reagan became the first lady of the United States when Ronald Reagan was inaugurated as president in January 1981. Early in her husband's presidency, Reagan stated her desire to create a more suitable "first home" in the White House, as the building had fallen into a state of disrepair following years of neglect. White House aide Michael Deaver described the second and third floor family residence as having "cracked plaster walls, chipped paint [and] beaten up floors"; rather than use government funds to renovate and redecorate, she sought private donations. In 1981, Reagan directed a major renovation of several White House rooms, including all of the second and third floors and rooms adjacent to the Oval Office, including the press briefing room. The renovation included repainting walls, refinishing floors, repairing fireplaces, and replacing antique pipes, windows, and wires. The closet in the master bedroom was converted into a beauty parlor and dressing room, and the West bedroom was made into a small gymnasium.

The first lady secured the assistance of renowned interior designer Ted Graber, popular with affluent West Coast social figures, to redecorate the family living quarters. A Chinese-pattern, handpainted wallpaper was added to the master bedroom. Family furniture was placed in the president's private study. The first lady and her designer retrieved a number of White House antiques, which had been in storage, and placed them throughout the mansion. In addition, many of Reagan's own collectibles were put out for display, including around twenty-five Limoges Boxes, as well as some porcelain eggs and a collection of plates.

The extensive redecoration was paid for by private donations. Many significant and long-lasting changes occurred as a result of the renovation and refurbishment, of which Reagan said, "This house belongs to all Americans, and I want it to be something of which they can be proud." The renovations received some criticisms for being funded by tax-deductible donations, meaning some of it eventually did indirectly come from the tax-paying public.

Reagan's interest in fashion was another one of her trademarks. While her husband was still president-elect, press reports speculated about Reagan's social life and interest in fashion. In many press accounts, Reagan's sense of style was favorably compared to that of a previous first lady, Jacqueline Kennedy. Friends and those close to her remarked that, while fashionable like Kennedy, she would be different from other first ladies; close friend Harriet Deutsch was quoted as saying, "Nancy has her own imprint."

White House photographer Mary Anne Fackelman-Miner, who was assigned to Reagan, said of her, "She always photographed so easily and was at ease in front of the cameras."

Reagan's wardrobe consisted of dresses, gowns, and suits made by luxury designers, including James Galanos, Bill Blass, and Oscar de la Renta. Her white, hand-beaded, one shoulder Galanos 1981 inaugural gown was estimated to cost $10,000, while the overall price of her inaugural wardrobe was said to cost $25,000. She favored the color red, calling it "a picker-upper", and wore it accordingly. Her wardrobe included red so often that the fire-engine shade became known as "Reagan red". She employed two private hairdressers, who would style her hair on a regular basis in the White House.

Fashion designers were pleased with the emphasis Reagan placed on clothing. Adolfo said the first lady embodied an "elegant, affluent, well-bred, chic American look", while Bill Blass commented, "I don't think there's been anyone in the White House since Jacqueline Kennedy Onassis who has her flair." William Fine, president of cosmetic company Frances Denney, noted that she "stays in style, but she doesn't become trendy."

Though her elegant fashions and wardrobe were hailed as a "glamorous paragon of chic", they were also controversial subjects. In 1982, she revealed that she had accepted thousands of dollars in clothing, jewelry, and other gifts, but defended her actions by stating that she had borrowed the clothes, and that they would either be returned or donated to museums, and that she was promoting the American fashion industry. Facing criticism, she soon said she would no longer accept such loans. While often buying her clothes, she continued to borrow and sometimes keep designer clothes throughout her time as first lady, which came to light in 1988. None of this had been included on financial disclosure forms; the non-reporting of loans under $10,000 in liability was in violation of a voluntary agreement the White House had made in 1982, while not reporting more valuable loans or clothes not returned was a possible violation of the Ethics in Government Act. Reagan expressed through her press secretary "regrets that she failed to heed counsel's advice" on disclosing them.

Despite the controversy, many designers who allowed her to borrow clothing, noted that the arrangement was good for their businesses, as well as for the American fashion industry overall. In 1989, Reagan was honored at the annual gala awards dinner of the Council of Fashion Designers of America, during which she received the council's lifetime achievement award. Barbara Walters said of her, "She has served every day for eight long years the word 'style.'"

Approximately a year into her husband's first term, Nancy explored the idea of ordering new state china service for the White House. A full china service had not been purchased since the Truman administration in the 1940s, as only a partial service was ordered in the Johnson administration. She was quoted as saying, "The White House really badly, badly needs china." Working with Lenox, the primary porcelain manufacturer in America, the first lady chose a design scheme of a red with etched gold band, bordering the scarlet and cream colored ivory plates with a raised presidential seal etched in gold in the center. The full service comprised 4,370 pieces, with 19 pieces per individual set. The service totaled $209,508. Although it was paid for by private donations, some from the private J. P. Knapp Foundation, the purchase generated quite a controversy, for it was ordered at a time when the nation was undergoing an economic recession. Furthermore, news of the china purchase emerged at the same time that her husband's administration had proposed school lunch regulations that would allow ketchup to be counted as a vegetable.
The new china, White House renovations, expensive clothing, and her attendance at the wedding of Charles and Diana, Prince and Princess of Wales, gave her an aura of being "out of touch" with the American people during the recession. This built upon the reputation she had coming to Washington, wherein many people concluded that Reagan was a vain and shallow woman, and her taste for splendor inspired the derogatory nickname "Queen Nancy". While Jacqueline Kennedy had also faced some press criticism for her spending habits, Reagan's treatment was much more consistent and negative. In an attempt to deflect the criticism, she self-deprecatingly donned a baglady costume at the 1982 Gridiron Dinner and sang "Second-Hand Clothes", mimicking the song "Second-Hand Rose". The skit helped to restore her reputation.

Reagan reflected on the criticisms in her 1989 autobiography, "My Turn". She described lunching with former Democratic National Committee chairman Robert S. Strauss, wherein Strauss said to her, "When you first came to town, Nancy, I didn't like you at all. But after I got to know you, I changed my mind and said, 'She's some broad!'" Reagan responded, "Bob, based on the press reports I read then, I wouldn't have liked me either!"
After the presidency of Jimmy Carter (who dramatically reduced the formality of presidential functions), Reagan brought a Kennedy-esque glamour back into the White House. She hosted 56 state dinners over eight years. She remarked that hosting the dinners is "the easiest thing in the world. You don't have to do anything. Just have a good time and do a little business. And that's the way Washington works." The White House residence staff found Reagan demanding to work for during the preparation for the state dinners, with the first lady overseeing every aspect of meal presentations, and sometimes requesting one dessert after another be prepared, before finally settling on one she approved of.

In general, the first lady's desire for everything to appear just right in the White House led the residence staff to consider her not easy to work for, with tirades following what she perceived as mistakes. One staffer later recalled, "I remember hearing her call for her personal maid one day and it scared the dickens out of me—just her tone. I never wanted to be on the wrong side of her." She did show loyalty and respect to a number of the staff. In particular, she came to the public defense of a maid who was indicted on charges of helping to smuggle ammunition to Paraguay, providing an affidavit to the maid's good character (even though it was politically inopportune to do so at the time of the Iran–Contra affair); charges were subsequently dropped, and the maid returned to work at the White House.

In 1987, Mikhail Gorbachev became the first Soviet leader to visit Washington, D.C. since Nikita Khrushchev made the trip in 1959 at the height of the Cold War. Nancy was in charge of planning and hosting the important and highly anticipated state dinner, with the goal to impress both the Soviet leader and especially his wife Raisa Gorbacheva. After the meal, she recruited pianist Van Cliburn to play a rendition of "Moscow Nights" for the Soviet delegation, to which Mikhail and Raisa broke out into song. Secretary of State George P. Shultz later commented on the evening, saying "We felt the ice of the Cold War crumbling." Reagan concluded, "It was a perfect ending for one of the great evenings of my husband's presidency."

The first lady launched the "Just Say No" drug awareness campaign in 1982, which was her primary project and major initiative as first lady. Reagan first became aware of the need to educate young people about drugs during a 1980 campaign stop in Daytop village, New York. She remarked in 1981 that "Understanding what drugs can do to your children, understanding peer pressure and understanding why they turn to drugs is ... the first step in solving the problem." Her campaign focused on drug education and informing the youth of the danger of drug abuse.

In 1982, Reagan was asked by a schoolgirl what to do when offered drugs; Reagan responded: "Just say no." The phrase proliferated in the popular culture of the 1980s, and was eventually adopted as the name of club organizations and school anti-drug programs. Reagan became actively involved by traveling more than throughout the United States and several nations, visiting drug abuse prevention programs and drug rehabilitation centers. She also appeared on television talk shows, recorded public service announcements, and wrote guest articles. She appeared in single episodes of the television drama "Dynasty" and the sitcom "Diff'rent Strokes", to underscore support for the "Just Say No" campaign, and in a rock music video, "Stop the Madness" (1985).

In 1985, Reagan expanded the campaign to an international level by inviting the First Ladies of various nations to the White House for a conference on drug abuse. On October 27, 1986, President Reagan signed a drug enforcement bill into law, which granted $1.7 billion in funding to fight the perceived crisis and ensured a mandatory minimum penalty for drug offenses. Although the bill was criticized, Reagan considered it a personal victory. In 1988, she became the first first lady invited to address the United Nations General Assembly, where she spoke on international drug interdiction and trafficking laws.

Critics of Reagan's efforts questioned their purpose, labelled Reagan's approach to promoting drug awareness as simplistic, and argued that the program did not address many social issues associated with drug use, including unemployment, poverty, and family dissolution.

Reagan assumed the role of unofficial "protector" for her husband after the attempted assassination of him in 1981. On March 30 of that year, President Reagan and three others were shot by troubled 25-year old John Hinckley, Jr as they left the Washington Hilton hotel. Nancy was alerted and arrived at George Washington University Hospital, where the President was hospitalized. She recalled having seen "emergency rooms before, but I had never seen one like this – with my husband in it." She was escorted into a waiting room, and when granted access to see her husband, he quipped to her, "Honey, I forgot to duck", borrowing the defeated boxer Jack Dempsey's jest to his wife.

An early example of the first lady's protective nature occurred when Senator Strom Thurmond entered the President's hospital room that day in March, passing the Secret Service detail by claiming he was the President's "close friend", presumably to acquire media attention. Nancy was outraged and demanded he leave. While the President recuperated in the hospital, the first lady slept with one of his shirts to be comforted by the scent. When Ronald Reagan was released from the hospital on April 12, she escorted him back to the White House.

Press accounts framed Reagan as her husband's "chief protector", an extension of their general initial framing of her as a helpmate and a Cold War domestic ideal. As it happened, the day after her husband was shot, Reagan fell off a chair while trying to take down a picture to bring to him in the hospital; she suffered several broken ribs, but was determined to not reveal it publicly.

During the Reagan administration, Nancy Reagan consulted a San Francisco astrologer, Joan Quigley, who provided advice on which days and times would be optimal for the president's safety and success. White House Chief of Staff Donald Regan grew frustrated with this regimen, which created friction between him and the first lady. This friction escalated with the revelation of the Iran–Contra affair, an administration scandal, in which the first lady felt Regan was damaging the president. She thought he should resign, and expressed this to her husband, although he did not share her view. Regan wanted President Reagan to address the Iran-Contra matter in early 1987 by means of a press conference, though the first lady refused to allow her husband to overexert himself due to a recent prostate surgery and astrological warnings. She became so angry with Regan that he hung up on her during a 1987 telephone conversation. According to the recollections of ABC News correspondent Sam Donaldson, when the President heard of this treatment, he demanded—and eventually received—Regan's resignation. Vice President George H. W. Bush is also reported to have suggested to her to have Regan fired.

In his 1988 memoir, "For the Record: From Wall Street to Washington", Regan wrote the following about Nancy Reagan's consultations with an astrologer:

Regan further claimed that Quigley selected the date of the 1985 Geneva Summit. For her part, Quigley stated in 1998 that she had "'absolutely nothing'" to do with arranging the summit and added that others were "'overemphasizing'" her role; however, in 1990, she released a book in which she asserted that she was "in charge" of the President's scheduling during the Reagan administration.

Reagan acknowledged in her memoirs that she altered the President's schedule without his knowledge based on astrological advice, but argues that "no political decision was ever based [on astrology]". She added, "Astrology was simply one of the ways I coped with the fear I felt after my husband almost died ... Was astrology one of the reasons [further attempts did not occur]? I don't "really" believe it was, but I don't "really" believe it wasn't."

Nancy Reagan wielded a powerful influence over President Reagan. 
In her memoirs, Reagan stated, "I felt panicky every time [Ronald Reagan] left the White House". Following the assassination attempt, she strictly controlled access to the president; occasionally, she even attempted to influence her husband's decision making.

Beginning in 1985, she strongly encouraged her husband to hold "summit" conferences with Soviet general secretary Mikhail Gorbachev, and suggested they form a personal relationship beforehand. Both Ronald Reagan and Gorbachev had developed a productive relationship through their summit negotiations. The relationship between Nancy Reagan and Raisa Gorbacheva was anything but the friendly, diplomatic one between their husbands; Reagan found Gorbacheva hard to converse with and their relationship was described as "frosty". The two women usually had tea and discussed differences between the USSR and the United States. Visiting the United States for the first time in 1987, Gorbacheva irked Reagan with lectures on subjects ranging from architecture to socialism, reportedly prompting the American president's wife to quip, "Who does that dame think she is?"

Press framing of Reagan changed from that of just helpmate and protector to someone with hidden power. As the image of her as a political interloper grew, she sought to explicitly deny that she was the power behind the throne. At the end of her time as First Lady, however, she said that her husband had not been well-served by his staff. She acknowledged her role in reaction in influencing him on personnel decisions, saying "In no way do I apologize for it." She wrote in her memoirs, "I don't think I was as bad, or as extreme in my power or my weakness, as I was depicted," but went on, "However the first lady fits in, she has a unique and important role to play in looking after her husband. And it's only natural that she'll let him know what she thinks. I always did that for Ronnie, and I always will."

In October 1987, a mammogram detected a lesion in Reagan's left breast and she was subsequently diagnosed with breast cancer. She chose to undergo a mastectomy rather than a lumpectomy, and the breast was removed on October 17, 1987. Ten days after the operation, her 99-year-old mother, Edith Luckett Davis, died in Phoenix, Arizona, leading Reagan to dub the period "a terrible month".

After the surgery, more women across the country had mammograms, which exemplified the influence that the first lady possessed.

Though Reagan was a controversial first lady, 56 percent of Americans had a favorable opinion of her when her husband left office on January 20, 1989, with 18 percent having an unfavorable opinion, and the balance not giving an opinion. Compared to fellow First Ladies when their husbands left office, Reagan's approval was higher than those of Rosalynn Carter and Hillary Clinton. However, she was less popular than Barbara Bush, and her disapproval rating was double that of Carter's.

Upon leaving the White House, the couple returned to California, where they purchased a home in the wealthy East Gate Old Bel Air neighborhood of Bel Air, Los Angeles, dividing their time between Bel Air and the Reagan Ranch in Santa Barbara, California. Ronald and Nancy regularly attended the Bel Air Church as well. After leaving Washington, Reagan made numerous public appearances, many on behalf of her husband. She continued to reside at the Bel Air home, where she lived with her husband until he died on June 5, 2004.

In late 1989, the former first lady established the Nancy Reagan Foundation, which aimed to continue to educate people about the dangers of substance abuse. The Foundation teamed with the BEST Foundation For A Drug-Free Tomorrow in 1994, and developed the Nancy Reagan Afterschool Program. She continued to travel around the United States, speaking out against drug and alcohol abuse.

Her memoirs, "My Turn: The Memoirs of Nancy Reagan" (1989), are an account of her life in the White House, commenting openly about her influence within the Reagan administration, and discussing the myths and controversies that surrounded the couple. In 1991, the author Kitty Kelley wrote an unauthorized and largely uncited biography about Reagan, repeating accounts of a poor relationship with her children, and introducing rumors of alleged sexual relations with singer Frank Sinatra. A wide range of sources commented that Kelley's largely unsupported claims are most likely false.

In 1989, the IRS (Internal Revenue Service) began investigating the Reagans over allegations they owed additional tax on the gifts and loans of high-fashion clothes and jewellery to the first lady during their time in the White House (recipients benefiting from the display of such items recognize taxable income even if they are returned). In 1992, the IRS determined the Reagans had failed to include some $3 million worth of fashion items between 1983 and 1988 on their tax returns; they were billed for a large amount of back taxes and interest, which was subsequently paid.

After President Reagan revealed that he had been diagnosed with Alzheimer's disease in 1994, she made herself his primary caregiver, and became actively involved with the National Alzheimer's Association and its affiliate, the Ronald and Nancy Reagan Research Institute in Chicago, Illinois.

In April 1997, Nancy Reagan joined President Bill Clinton and former Presidents Ford and Bush in signing the Summit Declaration of Commitment in advocating for participation by private citizens in solving domestic issues within the United States.

Nancy Reagan was awarded the Presidential Medal of Freedom, the nation's highest civilian honor, by President George W. Bush on July 9, 2002. President Reagan received his own Presidential Medal of Freedom in January 1993. Reagan and her husband were jointly awarded the Congressional Gold Medal on May 16, 2002, at the United States Capitol building, and were only the third president and first lady to receive it; she accepted the medal on behalf of both of them.

Ronald Reagan died in their Bel Air home on June 5, 2004. During the seven-day state funeral, Nancy, accompanied by her children and military escort, led the nation in mourning. She kept a strong composure, traveling from her home to the Reagan Library for a memorial service, then to Washington, D.C., where her husband's body lay in state for 34 hours prior to a national funeral service in the Washington National Cathedral. She returned to the library in Simi Valley for a sunset memorial service and interment, where, overcome with emotion, she lost her composure and cried in public for the first time during the week. After receiving the folded flag, she kissed the casket and mouthed "I love you" before leaving. During the week, CNN journalist Wolf Blitzer said, "She's a very, very strong woman, even though she looks frail."

She had directed the detailed planning of the funeral, which included scheduling all the major events and asking former President George H. W. Bush, as well as former British Prime Minister Margaret Thatcher, former Soviet Union Leader Mikhail Gorbachev, and former Canadian Prime Minister Brian Mulroney to speak during the National Cathedral Service. She paid very close attention to the details, something she had always done in her husband's life. Betsy Bloomingdale, one of Reagan's closest friends, stated, "She looks a little frail. But she is very strong inside. She is. She has the strength. She is doing her last thing for Ronnie. And she is going to get it right." The funeral marked her first major public appearance since she delivered a speech to the 1996 Republican National Convention on her husband's behalf.

The funeral had a great impact on her public image. Following substantial criticism during her tenure as first lady, she was seen somewhat as a national heroine, praised by many for supporting and caring for her husband while he suffered from Alzheimer's disease. "U.S. News & World Report" opined, "after a decade in the shadows, a different, softer Nancy Reagan emerged."

Following her husband's death, Reagan remained active in politics, particularly relating to stem cell research. Beginning in 2004, she favored what many consider to be the Democratic Party's position, and urged President George W. Bush to support federally funded embryonic stem cell research, in the hope that this science could lead to a cure for Alzheimer's disease. Although she failed to change the president's position, she did support his campaign for a second term.
In 2005, Reagan was honored at a gala dinner at the Ronald Reagan Building in Washington, D.C., where guests included Dick Cheney, Harry Reid, and Condoleezza Rice.

In 2007, she attended the national funeral service for Gerald Ford in the Washington National Cathedral. Reagan hosted two 2008 Republican presidential debates at the Reagan Presidential Library, the first in May 2007 and the second in January 2008. On March 25, she formally endorsed Senator John McCain, then the presumptive Republican party nominee for president, but McCain would go on to lose the election to Barack Obama.

Reagan attended the funeral of Lady Bird Johnson in Austin, Texas, on July 14, 2007, and three days later accepted the highest Polish distinction, the Order of the White Eagle, on behalf of Ronald Reagan at the Reagan Library. The Reagan Library opened the temporary exhibit "Nancy Reagan: A First Lady's Style", which displayed over eighty designer dresses belonging to her.

Reagan's health and well-being became a prominent concern in 2008. In February, she suffered a fall at her Bel Air home and was taken to Saint John's Health Center in Santa Monica, California. Doctors reported that she did not break her hip as feared, and she was released from the hospital two days later. News commentators noted that Reagan's step had slowed significantly, as the following month she walked in very slow strides with John McCain.

In October 2008, Reagan was admitted to Ronald Reagan UCLA Medical Center after falling at home. Doctors determined that the 87-year-old had fractured her pelvis and sacrum, and could recuperate at home with a regimen of physical therapy. As a result of her mishap, medical articles were published containing information on how to prevent falls. In January 2009, Reagan was said to be "improving every day and starting to get out more and more".
In March 2009, she praised President Barack Obama for reversing the ban on federally funded embryonic stem cell research. She traveled to Washington, D.C. in June 2009 to unveil a statue of her late husband in the Capitol rotunda. She was also on hand as President Obama signed the Ronald Reagan Centennial Commission Act, and lunched privately with Michelle Obama. Reagan revealed in an interview with "Vanity Fair" that Michelle Obama had telephoned her for advice on living and entertaining in the White House. Following the death of Senator Ted Kennedy in August 2009, she said she was "terribly saddened ... Given our political differences, people are sometimes surprised how close Ronnie and I have been to the Kennedy family ... I will miss him." She attended the funeral of Betty Ford in Rancho Mirage, California, on July 12, 2011.

Reagan hosted a 2012 Republican presidential debate at the Reagan Presidential Library on September 7, 2011. She suffered a fall in March 2012. Two months later, she endured several broken ribs, which prevented her from attending a speech given by Paul Ryan in the Reagan Presidential Library in May 2012. She endorsed Republican presidential candidate Mitt Romney on May 31, 2012, explaining that her husband would have liked Romney's business background and what she called "strong principles". Following the death of former British Prime Minister Margaret Thatcher in April 2013, she stated, "The world has lost a true champion of freedom and democracy ... Ronnie and I knew her as a dear and trusted friend, and I will miss her."

On March 6, 2016, Nancy Reagan died of congestive heart failure at the age of 94. On March 7, President Barack Obama issued a presidential proclamation ordering the flag of the United States to be flown at half-staff until sunset on the day of Reagan's interment.

Her funeral was held on March 11 at the Ronald Reagan Presidential Library in Simi Valley, California. Representatives from ten first families were in attendance, including former president George W. Bush and first ladies Michelle Obama, Laura Bush, Hillary Clinton, and Rosalynn Carter. The other representatives were presidential children Steven Ford, Tricia Nixon Cox, Luci Baines Johnson, and Caroline Kennedy, and presidential grandchild Anne Eisenhower Flottl.

Other prominent individuals in attendance included California governor Jerry Brown and former governors Arnold Schwarzenegger and Pete Wilson, then-former House speaker Nancy Pelosi and former House speaker Newt Gingrich, and former members of the Reagan administration, including George P. Shultz and Edwin Meese. A sizable contingent from the Hollywood entertainment industry attended as well, including Mr. T, Maria Shriver (Schwarzenegger's then-wife), Wayne Newton, Johnny Mathis, Anjelica Huston, John Stamos, Tom Selleck, Bo Derek, and Melissa Rivers. In all there were some 1,000 guests.

Eulogies were given by former prime minister of Canada Brian Mulroney, former secretary of state James Baker, Diane Sawyer, Tom Brokaw, and her children Patti Davis and Ron Reagan. After the funeral, Nancy Reagan was interred next to her husband.

As noted earlier, Nancy Reagan was awarded the Presidential Medal of Freedom in 2002 and the Congressional Gold Medal, in the same year. 
In 1989, she received the Council of Fashion Designers of America's lifetime achievement award.

As First Lady, Nancy Reagan received an Honorary Doctorate of Laws degree from Pepperdine University in 1983.
Later, she received an Honorary Doctor of Humane Letters degree from Eureka College in Illinois, her husband's alma mater, in 2009.


As Nancy Davis, she also made a number of television appearances from 1953 to 1962, as a guest star in dramatic shows or installments of anthology series. These included "Ford Television Theatre" (her first appearance with Ronald Reagan came during a 1953 episode titled "First Born"), "Schlitz Playhouse of Stars", "Dick Powell's Zane Grey Theatre" (appearing with Ronald Reagan in the 1961 episode "The Long Shadow"), "Wagon Train", "The Tall Man", and "General Electric Theater" (hosted by Ronald Reagan).




</doc>
<doc id="21182" url="https://en.wikipedia.org/wiki?curid=21182" title="New Brunswick">
New Brunswick

New Brunswick () is one of four Atlantic provinces on the east coast of Canada. According to the Constitution of Canada, New Brunswick is the only bilingual province. About two-thirds of the population declare themselves anglophones and one-third francophones. One-third of the population describes themselves as bilingual. Atypically for Canada, only about half of the population lives in urban areas, mostly in Greater Moncton, Greater Saint John and the capital Fredericton.

Unlike the other Maritime provinces, New Brunswick's terrain is mostly forested uplands and much of the land is further from the coast, giving it a harsher climate. New Brunswick is 83% forested and less densely populated than the rest of the Maritimes.

New Brunswick was among the first places in North America to be explored and settled by Europeans. In 1784, after an influx of refugees from the American Revolutionary War settled in the area, the province was founded on territory from the partition of Nova Scotia. In 1785 Saint John became the first incorporated city in what is now Canada. The province prospered in the early 1800s and the population grew rapidly, reaching about a quarter of a million by mid-century. In 1867, New Brunswick was one of four founding provinces of Canada, along with Nova Scotia and the Province of Canada (now Ontario, and Quebec).

After Confederation, wooden shipbuilding and lumbering declined, while protectionism disrupted trade ties with New England. The mid-1900s found New Brunswick to be one of the poorest regions of Canada, now mitigated by Canadian transfer payments and improved support for rural areas. As of 2002, provincial gross domestic product was derived as follows: services (about half being government services and public administration) 43%; construction, manufacturing, and utilities 24%; real estate rental 12%; wholesale and retail 11%; agriculture, forestry, fishing, hunting, mining, oil and gas extraction 5%; transportation and warehousing 5%.

Tourism accounts for about 9% of the labour force directly or indirectly. Popular destinations include Fundy National Park and the Hopewell Rocks, Kouchibouguac National Park, and Roosevelt Campobello International Park. In 2013, 64 cruise ships called at Port of Saint John, carrying, on average, 2,600 passengers each.

Indigenous peoples have been in the area since about 7000 BC. At the time of European contact, inhabitants were the Mi'kmaq, the Maliseet, and the Passamaquoddy. Although these tribes did not leave a written record, their language is present in many placenames, such as Aroostook, Bouctouche, Memramcook, Petitcodiac, Quispamsis, Richibucto and Shediac.

New Brunswick may have been part of Vinland during the Norse exploration of North America, and Basque, Breton, and Norman fishermen may have visited the Bay of Fundy in the early 1500s.

The first documented European visits were by Jacques Cartier in 1534. In 1604, a party including Samuel de Champlain visited the mouth of the Saint John River on the eponymous Saint-Jean-Baptiste Day. Now Saint John, this was later the site of the first permanent European settlement in New Brunswick. French settlement eventually extended up the river to the site of present-day Fredericton. Other settlements in the southeast extended from Beaubassin, near the present-day border with Nova Scotia, to Baie Verte, and up the Petitcodiac, Memramcook, and Shepody Rivers.

By the early 1700s the French settlements formed a part of Acadia, a colonial division of New France. Acadia covered what is now the Maritimes, as well as bits of Quebec and Maine. The British conquest of most of the Acadian peninsula occurred during the Queen Anne's War, and was formalized in the Treaty of Utrecht of 1713. After the war, French Acadia was reduced to Île Saint-Jean (Prince Edward Island) and Île-Royale (Cape Breton Island). The ownership of continental Acadia (New Brunswick) remained disputed, with an informal border on the Isthmus of Chignecto. In an effort to limit British expansion into continental Acadia, the French built Fort Beauséjour at the isthmus in 1751.

From 1749 to 1755, the British engaged in a campaign to consolidate its control over Nova Scotia. The resulting conflict led to an Acadian Exodus to French controlled territories in North America, including portions of continental Acadia. In 1755, the British captured Fort Beauséjour, severing the Acadian supply lines to Nova Scotia, and Île-Royale. Unable to make most of the Acadians sign an unconditional oath of allegiance, British authorities undertook a campaign to expel the Acadians in the initial periods of the Seven Years' War.

Continental Acadia was eventually incorporated into the British colony of Nova Scotia, with nearly all of New France being surrendered to the British with the Treaty of Paris in 1763. Acadians that returned from exile discovered several thousand immigrants, mostly from New England, on their former lands. Some settled around Memramcook and along the Saint John River. In 1766, settlers from Pennsylvania founded Moncton, and English settlers from Yorkshire arrived in the Sackville area. However, settlement of the area remained slow in the mid 18th century.

After the American Revolution, about 10,000 loyalist refugees settled along the north shore of the Bay of Fundy, commemorated in the province's motto, ("hope restored"). The number reached almost 14,000 by 1784, with about one in ten eventually returning to America. New Brunswick was founded in 1784 upon the partition of Nova Scotia into two areas which became the Provinces of Nova Scotia and New Brunswick. In the same year New Brunswick formed its first elected assembly. The colony was named New Brunswick in honour of George III, King of Great Britain, King of Ireland, and Prince-elector of Brunswick-Lüneburg in what is now Germany. In 1785, Saint John became Canada's first incorporated city. The population of the colony reached 26,000 in 1806 and 35,000 in 1812.

The 1800s saw an age of prosperity based on wood export and shipbuilding, bolstered by The Canadian–American Reciprocity Treaty of 1854 and demand from the American Civil War. St. Martins became the third most productive shipbuilding town in the Maritimes, producing over 500 vessels. In 1848, responsible home government was granted and the 1850s saw the emergence of political parties largely organised along religious and ethnic lines. The first half of the 1800s saw large-scale immigration from Ireland and Scotland, with the population reaching 252,047 by 1861.

The notion of unifying the separate colonies of British North America was discussed increasingly in the 1860s. Many felt that the American Civil war was the result of weak central government and wished to avoid such violence and chaos. The 1864 Charlottetown Conference was intended to discuss a Maritime Union, but concerns over possible conquest by the Americans, coupled with a belief that Britain was unwilling to defend its colonies against American attack, led to a request from the Province of Canada (now Ontario and Quebec) to expand the meeting's scope. In 1866 the US cancelled the Canadian–American Reciprocity Treaty, leading to loss of trade with New England and prompting a desire to build trade within British North America, while Fenian raids increased support for union. On 1 July 1867, New Brunswick entered the Canadian Confederation along with Nova Scotia and the Province of Canada.

Confederation brought into existence the Intercolonial Railway in 1872, a consolidation of the existing Nova Scotia Railway, European and North American Railway, and Grand Trunk Railway. In 1879 John A. Macdonald's Conservatives enacted the National Policy which called for high tariffs and opposed free trade, disrupting the trading relationship between the Maritimes and New England. The economic situation was worsened by the decline of the wooden ship building industry. The railways and tariffs did foster the growth of new industries in the province such as textile manufacturing, iron mills, and sugar refineries, many of which eventually failed to compete with better capitalized industry in central Canada.

In 1937 New Brunswick had the highest infant mortality and illiteracy rates in Canada. At the end of the Great Depression the New Brunswick standard of living was much below the Canadian average. In 1940 the Rowell–Sirois Commission reported that the federal government attempts to manage the depression illustrated grave flaws in the Canadian constitution. While the federal government had most of the revenue gathering powers, the provinces had many expenditure responsibilities such as healthcare, education, and welfare, which were becoming increasingly expensive. The Commission recommended the creation of equalization payments, implemented in 1957.

After Canada joined World War II, 14 NB army units were organized, in addition to The Royal New Brunswick Regiment, and first deployed in the Italian campaign in 1943. After the Normandy landings they redeployed to northwestern Europe, along with The North Shore Regiment. The British Commonwealth Air Training Plan, a training program for ally pilots, established bases in Moncton, Chatham, and Pennfield Ridge, as well as a military typing school in Saint John. While relatively unindustrialized before the war, New Brunswick became home to 34 plants on military contracts from which the province received over $78 million. Prime Minister William Lyon Mackenzie King, who had promised no conscription, asked the provinces if they would release the government of said promise. New Brunswick voted 69.1% yes. The policy was not implemented until 1944, too late for many of the conscripts to be deployed. There were 1808 NB fatalities among the armed forces.

The Acadians in northern New Brunswick had long been geographically and linguistically isolated from the more numerous English speakers to the south. The population of French origin grew dramatically after Confederation, from about 16 per cent in 1871 to 34 per cent in 1931. Government services were often not available in French, and the infrastructure in Francophone areas was less developed than elsewhere. In 1960 Premier Louis Robichaud embarked on the New Brunswick Equal Opportunity program, in which education, rural road maintenance, and healthcare fell under the sole jurisdiction of a provincial government that insisted on equal coverage throughout the province, rather than the former county-based system.

The flag of New Brunswick, based on the coat of arms, was adopted in 1965. The conventional heraldic representations of a lion and a ship represent colonial ties with Europe, and the importance of shipping at the time the coat of arms was assigned.

Roughly square, New Brunswick is bordered on the north by Quebec, on the east by the Atlantic Ocean, on the south by the Bay of Fundy, and on the west by the US state of Maine. The southeast corner of the province is connected to Nova Scotia at the isthmus of Chignecto.

Glaciation has left much of New Brunswick's uplands with only shallow, acidic soils which have discouraged settlement but which are home to enormous forests.

New Brunswick's climate is more severe than that of the other Maritime provinces, which are lower and have more shoreline along the moderating sea. New Brunswick has a humid continental climate, with slightly milder winters on the Gulf of St. Lawrence coastline. Elevated parts of the far north of the province have a subarctic climate.

Evidence of climate change in New Brunswick can be seen in its more intense precipitation events, more frequent winter thaws, and one quarter to half the amount of snowpack. Today the sea level is about 30 cm higher than it was 100 years ago, and it is expected to rise twice that much again by the year 2100.

Most of New Brunswick is forested with secondary forest or tertiary forest. At the start of European settlement, the Maritimes were covered from coast to coast by a forest of mature trees, giants by today's standards. Today less than one per cent of old-growth Acadian forest remains, and the World Wide Fund for Nature lists the Acadian Forest as endangered. Following the frequent large scale disturbances caused by settlement and timber harvesting, the Acadian forest is not growing back as it was, but is subject to borealization. This means that exposure-resistant species that are well adapted to the frequent large scale disturbances common in the boreal forest are increasingly abundant. These include jack pine, balsam fir, black spruce, white birch, and poplar. Forest ecosystems support large carnivores such as the bobcat, Canada lynx, and black bear, and the large herbivores moose and white-tailed deer.

Fiddlehead greens are harvested from the Ostrich fern which grows on riverbanks. Furbish's lousewort, a perennial herb endemic to the shores of the upper Saint John River, is an endangered species threatened by habitat destruction, riverside development, forestry, littering and recreational use of the riverbank. Many wetlands are being disrupted by the highly invasive Introduced species purple loosestrife.

Bedrock types range from 1 billion to 200 million years old.
Much of the bedrock in the west and north derives from ocean deposits in the Ordovician that were subject to folding and igneous intrusion and that were eventually covered with lava during the Paleozoic, peaking during the Acadian orogeny.

During the Carboniferous era, about 340 million years ago, New Brunswick was in the Maritimes Basin, a sedimentary basin near the equator. Sediments, brought by rivers from surrounding highlands, accumulated there; after being compressed, they produced the Albert oil shales of southern New Brunswick. Eventually, sea water from the Panthalassic Ocean invaded the basin, forming the Windsor Sea. Once this receded, conglomerates, sandstones, and shales accumulated. The rust colour of these was caused by the oxidation of iron in the beds between wet and dry periods. Such late carboniferous rock formed the Hopewell Rocks, which have been shaped by the extreme tidal range of the Bay of Fundy.

In the early Triassic, as Pangea drifted north it was rent apart, forming the rift valley that is the Bay of Fundy. Magma pushed up through the cracks, forming basalt columns on Grand Manan.

New Brunswick lies entirely within the Appalachian Mountain range. All of the rivers of New Brunswick drain into the Gulf of Saint Lawrence to the east or the Bay of Fundy to the south. These watersheds include lands in Quebec and Maine.

New Brunswick and the rest of the Maritime Peninsula was covered by thick layers of ice during the last glacial period (the Wisconsinian glaciation). It cut U-shaped valleys in the Saint John and Nepisiguit River valleys and pushed granite boulders from the Miramichi highlands south and east, leaving them as erratics when the ice receded at the end of the Wisconsin glaciation, along with deposits such as the eskers between Woodstock and St George, which are today sources of sand and gravel.

The four Atlantic Provinces are Canada's least populated, with New Brunswick the third-least populous at 747,101 in 2016. The Atlantic provinces also have higher rural populations. New Brunswick was largely rural until 1951; since then, the rural-urban split has been roughly even. Population density in the Maritimes is above average among Canadian provinces, which reflects their small size and the fact that they do not possess large, unpopulated hinterlands, as do the other seven provinces and three territories.

New Brunswick's 107 municipalities cover of the province's land mass but are home to of its population. The three major urban areas are in the south of the province and are Greater Moncton, population 126,424, Greater Saint John, population 122,389, and Greater Fredericton, population 85,688.

In the 2001 census, the most commonly reported ethnicities were British 40%, French Canadian and Acadian 31%, Irish 18%, other European 7%, First Nations 3%, Asian Canadian 2%. Each person could choose more than one ethnicity.

According to the Canadian Constitution, both English and French are the official languages of New Brunswick, making it the only officially bilingual province. Government and public services are available in both English and French.
For education, English-language and French-language systems serve the two linguistic communities at all levels.

Anglophone New Brunswickers make up roughly two-thirds of the population, while about one-third are Francophone. Recently there has been growth in the numbers of people reporting themselves as bilingual, with 34% reporting that they speak both English and French. This reflects a trend across Canada.

In the 2011 census, 84% of provincial residents reported themselves as Christian: 52% were Roman Catholic, 8% Baptist, 8% United Church of Canada, 7% Anglican and 9% other Christian. Fifteen percent of residents reported no religion.

As of October 2017, seasonally-adjusted employment is 73,400 for the goods-producing sector and 280,900 for the services-producing sector. Those in the goods-producing industries are mostly employed in manufacturing or construction, while those in services work in social assistance, trades, and health care. A large portion of the economy is controlled by the Irving Group of Companies, which consists of the holdings of the family of K. C. Irving. The companies have significant holdings in agriculture, forestry, food processing, freight transport (including railways and trucking), media, oil, and shipbuilding.

The United States is the province's largest export market, accounting for 92% of a foreign trade valued in 2014 at almost $13 billion, with refined petroleum making up 63% of that, followed by seafood products, pulp, paper and sawmill products and non-metallic minerals (chiefly potash). The value of exports, mostly to the United States, was $1.6 billion in 2016. About half of that came from lobster. Other products include salmon, crab, and herring. In 2015, spending on non-resident tourism in New Brunswick was $441 million, which provided $87 million in tax revenue.

A large number of residents from New Brunswick are employed in the primary sector of industry. More than 13,000 New Brunswickers work in agriculture, shipping products worth over $1 billion, half of which is from crops, and half of that from potatoes, mostly in the Saint John River valley. McCain Foods is one of the world's largest manufacturers of frozen potato products. Other products include apples, cranberries, and maple syrup. New Brunswick was in 2015 the biggest producer of wild blueberries in Canada. The value of the livestock sector is about a quarter of a billion dollars, nearly half of which is dairy. Other sectors include poultry, fur, and goats, sheep, and pigs.

About 83% of New Brunswick is forested. Historically important, it accounted for more than 80% of exports in the mid-1800s. By the end of the 1800s the industry, and shipbuilding, were declining due to external economic factors. The 1920s saw the development of a pulp and paper industry. In the mid-1960s, forestry practices changed from the controlled harvests of a commodity to the cultivation of the forests. The industry employs nearly 12,000, generating revenues around $437 million.

Mining was historically unimportant in the province, but has grown since the 1950s. The province's GDP from the Mining and Quarrying industry in 2015 was $299.5 million. Mines in New Brunswick produce lead, zinc, copper, and potash.

Public education elementary and secondary education in the province is administered by the provincial Department of Education and Early Childhood Development. New Brunswick has a parallel system of Anglophone and Francophone public schools.

The province also operates five public post-secondary institutions, including a college and four universities. Four public universities operate campuses in New Brunswick, including the oldest English-language university in the country, the University of New Brunswick. The other universities in the province include Mount Allison University, St. Thomas University, and the Université de Moncton. All four universities offer undergraduate, and postgraduate education. Additionally, the Université de Moncton, and the University of New Brunswick also offer professional education. Medical education programs have also been established at both the Université de Moncton and at UNBSJ in Saint John (affiliated with Université de Sherbrooke and Dalhousie University respectively).

Public colleges in the province are managed as a part of the New Brunswick Community College (NBCC) system. In addition to public institutions, the province is also home to several private vocational schools, such as the Moncton Flight College; and universities, the largest being Crandall University.

Under Canadian federalism, power is divided between federal and provincial governments. Among areas under federal jurisdiction are citizenship, foreign affairs, national defence, fisheries, criminal law, indigenous policies, and many others. Provincial jurisdiction covers public lands, health, education, and local government, among other things. Jurisdiction is shared for immigration, pensions, agriculture, and welfare.

The parliamentary system of government is modelled on the British Westminster system. Forty-nine representatives, nearly always members of political parties, are elected to the Legislative Assembly of New Brunswick. The head of government is the Premier of New Brunswick, normally the leader of the party or coalition with the most seats in the legislative assembly. Governance is handled by the executive council (cabinet), with about 32 ministries. Ceremonial duties of the Monarchy in New Brunswick are mostly carried out by the Lieutenant Governor of New Brunswick.

Under amendments to the province's Legislative Assembly Act in 2007, a provincial election is held every four years. The two largest political parties are the New Brunswick Liberal Association and the Progressive Conservative Party of New Brunswick. Since the 2018 election, minor parties are the Green Party of New Brunswick and the People's Alliance of New Brunswick.

The Court of Appeal of New Brunswick is the highest provincial court. It hears appeals from:

The system consists of eight Judicial Districts, loosely based on the counties. The Chief Justice of New Brunswick serves at the apex of this court structure.

Ninety-two per cent of the land in the province, inhabited by about 35% of the population, is under provincial administration and has no local, elected representation. The 51% of the province that is Crown land is administered by the Department of Natural Resources and Energy Development.

Most of the province is administrated as a local service district (LSD), an unincorporated unit of local governance. As of 2017, there are 237 LSDs. Services, paid for by property taxes, include a variety of services such as fire protection, solid waste management, street lighting, and dog regulation. LSDs may elect advisory committees and work with the Department of Local Government to recommend how to spend locally collected taxes.

In 2006 there were three rural communities. This is a relatively new entity; to be created, it requires a population of 3,000 and a tax base of $200 million. In 2006 there were 101 municipalities.

Regional Service Commissions, which number 12, were introduced in 2013 to regulate regional planning and solid waste disposal, and provide a forum for discussion on a regional level of police and emergency services, climate change adaptation planning, and regional sport, recreational and cultural facilities. The commissions' administrative councils are populated by the mayors of each municipality or rural community within a region.

Historically the province was divided into counties with elected governance, but this was abolished in 1966. These were further subdivided into 152 parishes, which also lost their political significance in 1966 but are still used as census subdivisions by Statistics Canada.

New Brunswick has the most poorly-performing economy of any Canadian province, with a per capita income of $28,000. The government has historically run at a large deficit. With about half of the population being rural, it is expensive for the government to provide education and health services, which account for 60 per cent of government expenditure. Thirty-six per cent of the provincial budget is covered by federal cash transfers.

The government has frequently attempted to create employment through subsidies, which has often failed to generate long-term economic prosperity and has resulted in bad debt, examples of which include Bricklin, Atcon, and the Marriott call centre in Fredericton.

According to a 2014 study by the Atlantic Institute for Market Studies, the large public debt is a very serious problem. Government revenues are shrinking because of a decline in federal transfer payments. Though expenditures are down (through government pension reform and a reduction in the number of public employees), they have increased relative to GDP, necessitating further measures to reduce debt in the future.

In the 2014–15 fiscal year, provincial debt reached $12.2 billion or 37.7 per cent of nominal GDP, an increase over the $10.1 billion recorded in 2011–12. The debt-to-GDP ratio is projected to fall to 36.7% in 2019–20.

Publicly owned NB Power operates 13 of New Brunswick's generating stations, deriving power from fuel oil and diesel (1497 MW), hydro (889 MW), nuclear (660 MW), and coal (467 MW). There were 30 active natural gas production sites in 2012.

The Department of Transportation and Infrastructure maintains government facilities and the province's highway network and ferries. The Trans-Canada Highway is not under federal jurisdiction, and traverses the province from Edmundston following the Saint John River Valley, through Fredericton, Moncton, and on to Nova Scotia and Prince Edward Island.

Via Rail's Ocean service, which connects Montreal to Halifax, is currently the oldest continuously operated passenger route in North America, with stops from west to east at Campbellton, Charlo, Jacquet River, Petit Rocher, Bathurst, Miramichi, Rogersville, Moncton, and Sackville.

Canadian National Railway operates freight services along the same route, as well as a subdivision from Moncton to Saint John. The New Brunswick Southern Railway, a division of J. D. Irving Limited, together with its sister company Eastern Maine Railway form a continuous main line connecting Saint John and Brownville Junction, Maine.

There are about 61 historic places in New Brunswick, including Fort Beauséjour, Kings Landing Historical Settlement and the Village Historique Acadien. Established in 1842, the New Brunswick Museum in Saint John was designated as the provincial museum of New Brunswick. The province is also home to a number of other museums in addition to the provincial museum.

New Brunswick is home to a number of individuals that worked as musicians, in the performing arts, and/or the visual arts. Music of New Brunswick includes artists such as Henry Burr, Roch Voisine, Lenny Breau, and Édith Butler. Symphony New Brunswick, based in Saint John, tours extensively in the province. Symphony New Brunswick based in Saint John and the Atlantic Ballet Theatre of Canada (based in Moncton), tours nationally and internationally.

Theatre New Brunswick (based in Fredericton), tours plays around the province. Canadian playwright Norm Foster saw his early works premiere with Theatre New Brunswick. Other live theatre troops include the Théatre populaire d'Acadie in Caraquet, and Live Bait Theatre in Sackville. The refurbished Imperial and Capitol Theatres are found in Saint John and Moncton, respectively; the more modern Playhouse is in Fredericton.

Mount Allison University in Sackville began offering classes in 1854. The program came into its own under John A. Hammond, from 1893 to 1916. Alex Colville and Lawren Harris later studied and taught art there, and both Christopher Pratt and Mary Pratt were trained at Mount Allison. The university also opened an art gallery in 1895 and is named for its patron, John Owens of Saint John. The art gallery at Mount Allison University is presently the oldest university-operated art gallery in Canada. Modern New Brunswick artists include landscape painter Jack Humphrey, sculptor Claude Roussel, and Miller Brittain. The province is also home to the Beaverbrook Art Gallery, which was designated as the provincial art gallery in 1994.

Julia Catherine Beckwith, born in Fredericton, was Canada's first published novelist. Poet Bliss Carman and his cousin Charles G. D. Roberts were some of the first Canadians to achieve international fame for letters. Antonine Maillet was the first non-European winner of France's Prix Goncourt. Other modern writers include Alfred Bailey, Alden Nowlan, John Thompson, Douglas Lochhead, K. V. Johansen, David Adams Richards, and France Daigle. A recent New Brunswick Lieutenant-Governor, Herménégilde Chiasson, is a poet and playwright. "The Fiddlehead", established in 1945 at University of New Brunswick, is Canada's oldest literary magazine.

New Brunswick has four daily newspapers: the "Times & Transcript", serving eastern New Brunswick; the "Telegraph-Journal", based in Saint John and distributed province-wide; "The Daily Gleaner", based in Fredericton; and "L'Acadie Nouvelle", based in Caraquet. The three English-language dailies and the majority of the weeklies are owned and operated by Brunswick News—which is privately owned by James K. Irving. Due to its dominant position, critics have accused Brunswick News of being biased towards the Irving Group of Companies, including its reluctance to publish stories that are critical of the group.

The Canadian Broadcasting Corporation has anglophone television and radio operations in Fredericton. Télévision de Radio-Canada is based in Moncton. CTV and Global also operate stations in New Brunswick, which operate largely as sub-feeds of their stations in Halifax as part of regional networks.




</doc>
<doc id="21184" url="https://en.wikipedia.org/wiki?curid=21184" title="Nova Scotia">
Nova Scotia

Nova Scotia () is a province in eastern Canada. With a population of 923,598 as of 2016, it is the most populous of Canada's three Maritime provinces and four Atlantic provinces. It is the country's second-most densely populated province and second-smallest province by area, both after neighbouring Prince Edward Island. Its area of includes Cape Breton Island and 3,800 other coastal islands. The peninsula that makes up Nova Scotia's mainland is connected to the rest of North America by the Isthmus of Chignecto, on which the province's land border with New Brunswick is located. The province borders the Bay of Fundy to the west and the Atlantic Ocean to the south and east, and is separated from Prince Edward Island and the island of Newfoundland by the Northumberland and Cabot straits, respectively.

The land that comprises what is now Nova Scotia has been inhabited by the indigenous Miꞌkmaq people for thousands of years. France's first settlement in North America, , was established in 1605 and intermittently served in various locations as the capital of the French colony of Acadia for over a hundred years. The Fortress of Louisbourg was a key focus point in the struggle between the British and French for control of the area, changing hands numerous times until France relinquished its claims with the Treaty of Paris in 1763. During the American Revolutionary War, thousands of Loyalists settled in Nova Scotia. In 1848, Nova Scotia became the first British colony to achieve responsible government, and it federated in July 1867 with New Brunswick and the Province of Canada (now Ontario and Quebec) to form what is now the country of Canada.

Nova Scotia's capital and largest city is Halifax, which today is home to about 45 percent of the province's population. Halifax is the thirteenth-largest census metropolitan area in Canada, the largest city in Atlantic Canada, and Canada's second-largest coastal city after Vancouver.

"Nova Scotia" means "New Scotland" in Latin and is the recognized English-language name for the province. In both French and Scottish Gaelic, the province is directly translated as "New Scotland" (French: '. Gaelic: '). In general, Romance and Slavic languages use a direct translation of "New Scotland", while most other languages use direct transliterations of the Latin / English name.

The province was first named in the 1621 Royal Charter granting to Sir William Alexander in 1632 the right to settle lands including modern Nova Scotia, Cape Breton Island, Prince Edward Island, New Brunswick and the Gaspé Peninsula.

Nova Scotia is Canada's second-smallest province in area, after Prince Edward Island. The province's mainland is the Nova Scotia peninsula, surrounded by the Atlantic Ocean and including numerous bays and estuaries. Nowhere in Nova Scotia is more than from the ocean. Cape Breton Island, a large island to the northeast of the Nova Scotia mainland, is also part of the province, as is Sable Island, a small island notorious for being the site of offshore shipwrecks, approximately from the province's southern coast.

Nova Scotia has many ancient fossil-bearing rock formations. These formations are particularly rich on the Bay of Fundy's shores. Blue Beach near Hantsport, Joggins Fossil Cliffs, on the Bay of Fundy's shores, has yielded an abundance of Carboniferous-age fossils. Wasson's Bluff, near the town of Parrsboro, has yielded both Triassic- and Jurassic-age fossils.

The province contains 5,400 lakes.

Nova Scotia lies in the mid-temperate zone and, although the province is almost surrounded by water, the climate is closer to continental climate rather than maritime. The winter and summer temperature extremes of the continental climate are moderated by the ocean. However, winters are cold enough to be classified as continental—still being nearer the freezing point than inland areas to the west. The Nova Scotian climate is in many ways similar to the central Baltic Sea coast in Northern Europe, only wetter and snowier. This is true although Nova Scotia is some fifteen parallels further south. Areas not on the Atlantic coast experience warmer summers more typical of inland areas, and winter lows are a little colder.

Described on the provincial vehicle licence plate as Canada's Ocean Playground, Nova Scotia is surrounded by four major bodies of water: the Gulf of Saint Lawrence to the north, the Bay of Fundy to the west, the Gulf of Maine to the southwest, and the Atlantic Ocean to the east.

The province includes regions of the Mi'kmaq nation of Mi'kma'ki (""). (The territory of the Nation of Mi'kma'ki also includes the Maritimes, parts of Maine, Newfoundland and the Gaspé Peninsula.) The Mi'kmaq people are among the large Algonquian-language family and inhabited Nova Scotia at the time the first European colonists arrived.

Warfare was a notable feature in Nova Scotia during the 17th and 18th centuries. The French arrived in 1604, and Catholic Mi'kmaq and Acadians formed the majority of the population of the colony for the next 150 years. In 1605, French colonists established the first permanent European settlement in the future Canada (and the first north of Florida) at Port Royal, founding what would become known as Acadia.

During the first 80 years the French and Acadians lived in Nova Scotia, nine significant military clashes took place as the English and Scottish (later British), Dutch and French fought for possession of the area. These encounters happened at Port Royal, Saint John, Cap de Sable (present-day Port La Tour, Nova Scotia), Jemseg (1674 and 1758) and Baleine (1629). The Acadian Civil War took place from 1640 to 1645.

Beginning with King William's War in 1688, a series of six wars took place between the English/British and the French, with Nova Scotia being a consistent theatre of conflict between the two powers.

Hostilities between the British and French resumed from 1702 to 1713, known as Queen Anne's War. The British siege of Port Royal took place in 1710, ending French-rule in peninsular Acadia. The subsequent signing of the Treaty of Utrecht in 1713 formally recognized this, while returning Cape Breton Island (') and Prince Edward Island (') to the French. Despite the British conquest of Acadia in 1710, Nova Scotia remained primarily occupied by Catholic Acadians and Mi'kmaq, who confined British forces to Annapolis and to Canso. Present-day New Brunswick then still formed a part of the French colony of Acadia. Immediately after the capture of Port Royal in 1710, Francis Nicholson announced it would be renamed Annapolis Royal in honor of Queen Anne.

As a result of Father Rale's War (1722–1725), the Mi'kmaq signed a series of treaties with Great Britain in 1725. The British signed a treaty (or "agreement") with the Mi'kmaq, but the authorities have often disputed its definition of the rights of the Mi'kmaq to hunt and fish on their lands. However, conflict between the Acadians, Mi'kmaq, French, and the British persisted in the following decades with King George's War (1744–1748). 

Father Le Loutre's War (1749–1755) began when Edward Cornwallis arrived to establish Halifax with 13 transports on 21 June 1749. A General Court, made up of the governor and the Council, was the highest court in the colony at the time. Jonathan Belcher was sworn in as chief justice of the Nova Scotia Supreme Court on 21 October 1754. The first legislative assembly in Halifax, under the Governorship of Charles Lawrence, met on 2 October 1758. During the French and Indian War of 1754–63 (the North American theatre of the Seven Years' War of 1756–1763), the British deported the Acadians and recruited New England Planters to resettle the colony. The 75-year period of war ended with the Halifax Treaties between the British and the Mi'kmaq (1761). After the war, some Acadians were allowed to return and the British made treaties with the Mi’kmaq.

In 1763, most of Acadia (Cape Breton Island, St. John's Island (now Prince Edward Island), and New Brunswick) became part of Nova Scotia. In 1765, the county of Sunbury was created. This included the territory of present-day New Brunswick and eastern Maine as far as the Penobscot River. In 1769, St. John's Island became a separate colony.

The American Revolution (1775–1783) had a significant impact on shaping Nova Scotia. Initially, Nova Scotia—"the 14th American Colony" as some called it—displayed ambivalence over whether the colony should join the more southern colonies in their defiance of Britain, and rebellion flared at the Battle of Fort Cumberland (1776) and at the Siege of Saint John (1777). Throughout the war, American privateers devastated the maritime economy by capturing ships and looting almost every community outside of Halifax. These American raids alienated many sympathetic or neutral Nova Scotians into supporting the British. By the end of the war Nova Scotia had outfitted a number of privateers to attack American shipping. British military forces based at Halifax succeeded in preventing American support for rebels in Nova Scotia and deterred any invasion of Nova Scotia. However the British navy failed to establish naval supremacy. While the British captured many American privateers in battles such as the Naval battle off Halifax (1782), many more continued attacks on shipping and settlements until the final months of the war. The Royal Navy struggled to maintain British supply lines, defending convoys from American and French attacks as in the fiercely fought convoy battle, the Naval battle off Cape Breton (1781).

After the Thirteen Colonies and their French allies forced the British forces to surrender (1781), approximately 33,000 Loyalists (the King's Loyal Americans, allowed to place "United Empire Loyalist" after their names) settled in Nova Scotia (14,000 of them in what became New Brunswick) on lands granted by the Crown as some compensation for their losses. (The British administration divided Nova Scotia and hived off Cape Breton and New Brunswick in 1784). The Loyalist exodus created new communities across Nova Scotia, including Shelburne, which briefly became one of the larger British settlements in North America, and infused Nova Scotia with additional capital and skills. There are also a number of Black loyalists buried in unmarked graves in the Old Burying Ground (Halifax, Nova Scotia).

However the migration also caused political tensions between Loyalist leaders and the leaders of the existing New England Planters settlement. The Loyalist influx also pushed Nova Scotia's 2000 Mi'kmaq People to the margins as Loyalist land grants encroached on ill-defined native lands. As part of the Loyalist migration, about 3,000 Black Loyalists arrived; they founded the largest free Black settlement in North America at Birchtown, near Shelburne. Many Nova Scotian communities were settled by British regiments that fought in the war.

During the War of 1812, Nova Scotia's contribution to the British war effort involved communities either purchasing or building various privateer ships to attack U.S. vessels. Perhaps the most dramatic moment in the war for Nova Scotia occurred when HMS "Shannon" escorted the captured American frigate USS "Chesapeake" into Halifax Harbour (1813). Many of the U.S. prisoners were kept at Deadman's Island, Halifax.

During this century, Nova Scotia became the first colony in British North America and in the British Empire to achieve responsible government in January–February 1848 and become self-governing through the efforts of Joseph Howe. Nova Scotia had established representative government in 1758, an achievement later commemorated by the erection of the Dingle Tower in 1908.

Nova Scotians fought in the Crimean War of 1853–1856. The Welsford-Parker Monument in Halifax is the second-oldest war monument in Canada (1860) and the only Crimean War monument in North America. It commemorates the 1854–55 Siege of Sevastopol. 

Thousands of Nova Scotians fought in the American Civil War (1861–1865), primarily on behalf of the North. The British Empire (including Nova Scotia) in the conflict. As a result, Britain (and Nova Scotia) continued to trade with both the South and the North. Nova Scotia's economy boomed during the Civil War.

Soon after the American Civil War, Pro-Canadian Confederation premier Charles Tupper led Nova Scotia into Canadian Confederation on 1 July 1867, along with New Brunswick and the Province of Canada. The Anti-Confederation Party was led by Joseph Howe. Almost three months later, in the election of 18 September 1867, the Anti-Confederation Party won 18 out of 19 federal seats, and 36 out of 38 seats in the provincial legislature.

Throughout the 19th century, numerous businesses developed in Nova Scotia became of pan-Canadian and international importance: the Starr Manufacturing Company (first skate-manufacturer in Canada), the Bank of Nova Scotia, Cunard Line, Alexander Keith's Brewery, Morse's Tea Company (first tea company in Canada), among others. 

Nova Scotia became a world leader in both building and owning wooden sailing ships in the second half of the 19th century. Nova Scotia produced internationally recognized shipbuilders Donald McKay and William Dawson Lawrence. The fame Nova Scotia achieved from sailors was assured when Joshua Slocum became the first man to sail single-handedly around the world (1895). International attention continued into the following century with the many racing victories of the "Bluenose" schooner. Nova Scotia was also the birthplace and home of Samuel Cunard, a British shipping magnate (born at Halifax, Nova Scotia) who founded the Cunard Line.

In December 1917, about 2,000 people were killed in the Halifax Explosion.

In April 2020, a killing spree occurred across the province and became the deadliest rampage in Canada's history.

According to the 2006 Canadian census the largest ethnic group in Nova Scotia is Scottish (31.9%), followed by English (31.8%), Irish (21.6%), French (17.9%), German (11.3%), Aboriginal origin (5.3%), Dutch (4.1%), Black Canadians (2.8%), Welsh (1.9%) Italian (1.5%), and Scandinavian (1.4%). 40.9% of respondents identified their ethnicity as "Canadian".

The 2011 Canadian census showed a population of 921,727. Of the 904,285 singular responses to the census question concerning mother tongue, the most commonly reported languages were:
Figures shown are for the number of single-language responses and the percentage of total single-language responses.

Nova Scotia is home to the largest Scottish Gaelic-speaking community outside of Scotland, with a small number of native speakers in Pictou County, Antigonish County, and Cape Breton Island, and the language is taught in a number of secondary schools throughout the province. In 2018 the government launched a new Gaelic vehicle licence plate to raise awareness of the language and help fund Gaelic language and culture initiatives. They estimated that there were 2,000 Gaelic speakers in the province.

In 1871, the largest religious denominations were Protestant with 103,500 (27%); Roman Catholic with 102,000 (26%); Baptist with 73,295 (19%); Anglican with 55,124 (14%); Methodist with 40,748 (10%), Lutheran with 4,958 (1.3%); and Congregationalist with 2,538 (0.65%).

According to the 2011 census, the largest denominations by number of adherents were the Christians with 78.2%.About 21.18 % were Non-religious and 1 % were Muslims. Jews, Hindus and Sikhs constitute around 0.20%.

Nova Scotia's per capita GDP in 2016 was $44,924, significantly lower than the national average per capita GDP of $57,574. GDP growth has lagged behind the rest of the country for at least the past decade. As of 2017, the median family income in Nova Scotia was $85,970, below the national average of $92,990; in Halifax the figure rises to $98,870.

The province is the world's largest exporter of Christmas trees, lobster, gypsum, and wild berries. Its export value of fish exceeds $1 billion, and fish products are received by 90 countries around the world. Nevertheless, the province's imports far exceed its exports. While these numbers were roughly equal from 1992 until 2004, since that time the trade deficit has ballooned. In 2012, exports from Nova Scotia were 12.1% of provincial GDP, while imports were 22.6%.

Nova Scotia's traditionally resource-based economy has diversified in recent decades. The rise of Nova Scotia as a viable jurisdiction in North America, historically, was driven by the ready availability of natural resources, especially the fish stocks off the Scotian Shelf. The fishery was a pillar of the economy since its development as part of New France in the 17th century; however, the fishery suffered a sharp decline due to overfishing in the late 20th century. The collapse of the cod stocks and the closure of this sector resulted in a loss of approximately 20,000 jobs in 1992.

Other sectors in the province were also hit hard, particularly during the last two decades: coal mining in Cape Breton and northern mainland Nova Scotia has virtually ceased, and a large steel mill in Sydney closed during the 1990s. More recently, the high value of the Canadian dollar relative to the US dollar has hurt the forestry industry, leading to the shutdown of a long-running pulp and paper mill near Liverpool. Mining, especially of gypsum and salt and to a lesser extent silica, peat and barite, is also a significant sector. Since 1991, offshore oil and gas has become an important part of the economy, although production and revenue are now declining. However, agriculture remains an important sector in the province, particularly in the Annapolis Valley.

Nova Scotia's defence and aerospace sector generates approximately $500 million in revenues and contributes about $1.5 billion to the provincial economy each year. To date, 40% of Canada's military assets reside in Nova Scotia. Nova Scotia has the fourth-largest film industry in Canada hosting over 100 productions yearly, more than half of which are the products of international film and television producers. In 2015, the government of Nova Scotia eliminated tax credits to film production in the province, jeopardizing the industry given most other jurisdictions continue to offer such credits. The province also boasts a rapidly developing Information & Communication Technology (ICT) sector which consists of over 500 companies, and employs roughly 15,000 people.

In 2006, the manufacturing sector brought in over $2.6 billion in chained GDP, the largest output of any industrial sector in Nova Scotia. Michelin remains by far the largest single employer in this sector, operating three production plants in the province. Michelin is also the province's largest private-sector employer.

The Nova Scotia tourism industry includes more than 6,500 direct businesses, supporting nearly 40,000 jobs. Cruise ships pay regular visits to the province. In 2010, the Port of Halifax received 261,000 passengers and Sydney 69,000. This industry contributes approximately $1.3 billion annually to the economy. A 2008 Nova Scotia tourism campaign included advertising a fictional mobile phone called Pomegranate and establishing website, which after reading about "new phone" redirected to tourism info about region.

Nova Scotia's tourism industry showcases Nova Scotia's culture, scenery and coastline. Nova Scotia has many museums reflecting its ethnic heritage, including the Glooscap Heritage Centre, Grand-Pré National Historic Site, Hector Heritage Quay and the Black Cultural Centre for Nova Scotia. Other museums tell the story of its working history, such as the Cape Breton Miners' Museum, and the Maritime Museum of the Atlantic.

Nova Scotia is home to several internationally renowned musicians and there are visitor centres in the home towns of Hank Snow, Rita MacNeil, and Anne Murray Centre. There are also numerous music and cultural festivals such as the Stan Rogers Folk Festival, Celtic Colours, the Nova Scotia Gaelic Mod, Royal Nova Scotia International Tattoo, the Atlantic Film Festival and the Atlantic Fringe Festival.

The province has 87 National Historic Sites of Canada, including the Habitation at Port-Royal, the Fortress of Louisbourg and Citadel Hill (Fort George) in Halifax. Nova Scotia has two national parks, Kejimkujik and Cape Breton Highlands, and many other protected areas. The Bay of Fundy has the highest tidal range in the world, and the iconic Peggys Cove is internationally recognized and receives 600,000-plus visitors a year. Old Town Lunenburg is a port town on the South Shore that was declared a UNESCO World Heritage Site.

Acadian Skies and Mi'kmaq Lands is a starlight reserve in southwestern Nova Scotia. It is the first certified UNESCO-Starlight Tourist Destination. Starlight tourist destinations are locations that offer conditions for observations of stars which are protected from light pollution.

Nova Scotia is ordered by a parliamentary government within the construct of constitutional monarchy; the monarchy in Nova Scotia is the foundation of the executive, legislative, and judicial branches. The sovereign is Queen Elizabeth II, who also serves as head of state of 15 other Commonwealth countries, each of Canada's nine other provinces, and the Canadian federal realm, and resides predominantly in the United Kingdom. As such, the Queen's representative, the Lieutenant Governor of Nova Scotia (at present Arthur Joseph LeBlanc), carries out most of the royal duties in Nova Scotia.

The direct participation of the royal and viceroyal figures in any of these areas of governance is limited, though; in practice, their use of the executive powers is directed by the Executive Council, a committee of ministers of the Crown responsible to the unicameral, elected House of Assembly and chosen and headed by the Premier of Nova Scotia (presently Stephen McNeil), the head of government. To ensure the stability of government, the lieutenant governor will usually appoint as premier the person who is the current leader of the political party that can obtain the confidence of a plurality in the House of Assembly. The leader of the party with the second-most seats usually becomes the Leader of Her Majesty's Loyal Opposition (presently Tim Houston) and is part of an adversarial parliamentary system intended to keep the government in check.

Each of the 51 Members of the Legislative Assembly in the House of Assembly is elected by single member plurality in an electoral district or riding. General elections must be called by the lieutenant governor on the advice of the premier, or may be triggered by the government losing a confidence vote in the House. There are three dominant political parties in Nova Scotia: the Liberal Party, the New Democratic Party, and the Progressive Conservative Party. The other two registered parties are the Green Party of Nova Scotia and the Atlantica Party, neither of which has a seat in the House of Assembly.

The province's revenue comes mainly from the taxation of personal and corporate income, although taxes on tobacco and alcohol, its stake in the Atlantic Lottery Corporation, and oil and gas royalties are also significant. In 2006–07, the province passed a budget of $6.9 billion, with a projected $72 million surplus. Federal equalization payments account for $1.385 billion, or 20.07% of the provincial revenue. The province participates in the HST, a blended sales tax collected by the federal government using the GST tax system.

Nova Scotia no longer has any incorporated cities; they were amalgamated into Regional Municipalities in 1996.

The cuisine of Nova Scotia is typically Canadian with an emphasis on local seafood. One endemic dish (in the sense of "peculiar to" and "originating from") is the Halifax donair, a distant variant of the doner kebab prepared using thinly sliced beef meatloaf and a sweet condensed milk sauce. As well, hodge podge, a creamy soup of fresh baby vegetables, is native to Nova Scotia.

The province is also known for a dessert called blueberry fungy or blueberry grunt.

There are a number of festivals and cultural events that are recurring in Nova Scotia, or notable in its history. The following is an incomplete list of festivals and other cultural gatherings in the province:
Nova Scotia has produced numerous film actors. Academy Award nominee Ellen Page ("Juno", "Inception") was born in Halifax, Nova Scotia; five-time Academy Award nominee Arthur Kennedy ("Lawrence of Arabia", "High Sierra") called Nova Scotia his home; and two time Golden Globe winner Donald Sutherland ("MASH", "Ordinary People") spent most of his youth in the province. Other actors include John Paul Tremblay, Robb Wells, Mike Smith and John Dunsworth of "Trailer Park Boys" and actress Joanne Kelly of "Warehouse 13".

Nova Scotia has also produced numerous film directors such as Thom Fitzgerald ("The Hanging Garden"), Daniel Petrie ("Resurrection"—Academy Award nominee) and Acadian film director Phil Comeau's multiple award-winning local story ("Le secret de Jérôme").

Nova Scotian stories are the subject of numerous feature films: "Margaret's Museum" (starring Helena Bonham Carter); "The Bay Boy" (directed by Daniel Petrie and starring Kiefer Sutherland); "New Waterford Girl"; "The Story of Adele H." (the story of unrequited love of Adèle Hugo); and two films of "Evangeline" (one starring Miriam Cooper and another starring Dolores del Río).

There is a significant film industry in Nova Scotia. Feature filmmaking began in Canada with "Evangeline" (1913), made by Canadian Bioscope Company in Halifax, which released six films before it closed. The film has since been lost. Some of the award-winning feature films made in the province are "Titanic" (starring Leonardo DiCaprio and Kate Winslet); "The Shipping News" (starring Kevin Spacey and Julianne Moore); "" (starring Harrison Ford and Liam Neeson); "Amelia" (starring Hilary Swank, Richard Gere and Ewan McGregor) and "The Lighthouse" (starring Robert Pattinson and William Dafoe).

Nova Scotia has also produced numerous television series: "This Hour Has 22 Minutes", "Don Messer's Jubilee", "Black Harbour", "Haven", "Trailer Park Boys", "Mr. D", "Call Me Fitz", and "Theodore Tugboat". The "Jesse Stone" film series on CBS starring Tom Selleck is also routinely produced in the province.

Nova Scotia has long been a centre for artistic and cultural excellence. The capital, Halifax, hosts institutions such as Nova Scotia College of Art and Design University, Art Gallery of Nova Scotia, Neptune Theatre, Dalhousie Arts Centre, Two Planks and a Passion Theatre, and the Ship's Company Theatre. The province is home to avant-garde visual art and traditional crafting, writing and publishing and a film industry.

Much of the historic public art sculptures in the province were made by New York sculptor J. Massey Rhind as well as Canadian sculptors Hamilton MacCarthy, George Hill, Emanuel Hahn and Louis-Philippe Hébert. Some of this public art was also created by Nova Scotian John Wilson. Nova Scotian George Lang was a stone sculptor who also built many landmark buildings in the province, including the Welsford-Parker Monument. Two valuable sculptures/ monuments in the province are in St. Paul's Church (Halifax): one by John Gibson (for Richard John Uniacke, Jr.) and another monument by Sir Francis Leggatt Chantrey (for Amelia Ann Smyth). Both Gibson and Chantry were famous British sculptors during the Victorian era and have numerou sculptures in the Tate, Museum of Fine Arts, Boston and Westminster Abbey.

Some of the province's greatest painters were Maud Lewis, William Valentine, Maria Morris, Jack L. Gray, Mabel Killiam Day, Ernest Lawson, Frances Bannerman, Alex Colville, Tom Forrestall and ship portrait artist John O'Brien. Some of most notable artists whose works have been acquired by Nova Scotia are British artist Joshua Reynolds (collection of Art Gallery of Nova Scotia); William Gush and William J. Weaver (both have works in Province House); Robert Field (Government House), as well as leading American artists Benjamin West (self portrait in The Halifax Club, portrait of chief justice in Nova Scotia Supreme Court), John Singleton Copley, Robert Feke, and Robert Field (the latter three have works in the Uniacke Estate). Two famous Nova Scotian photographers are Wallace R. MacAskill and Sherman Hines. Three of the most accomplished illustrators were George Wylie Hutchinson, Bob Chambers (cartoonist) and Donald A. Mackay.

There are numerous Nova Scotian authors who have achieved international fame: Thomas Chandler Haliburton ("The Clockmaker"), Alistair MacLeod ("No Great Mischief"), Evelyn Richardson "(We Keep A Light)", Margaret Marshall Saunders "(Beautiful Joe)," Laurence B. Dakin "(Marco Polo)," and Joshua Slocum "(Sailing Alone Around the World)." Other authors include Johanna Skibsrud "(The Sentimentalists)," Alden Nowlan "(Bread, Wine and Salt)," George Elliott Clarke "(Execution Poems)," Lesley Choyce "(Nova Scotia: Shaped by the Sea)," Thomas Raddall "(Halifax: Warden of the North)," Donna Morrissey "(Kit's Law)," and Frank Parker Day "(Rockbound)."

Nova Scotia has also been the subject of numerous literary books. Some of the international best-sellers are: "Last Man Out: The Story of the Springhill Mining Disaster" (by Melissa Fay Greene) ; "Curse of the Narrows: The Halifax Explosion 1917" (by Laura MacDonald); "In the Village" (short story by Pulitzer Prize–winning author Elizabeth Bishop); and National Book Critics Circle Award winner "Rough Crossings" (by Simon Schama). Other authors who have written novels about Nova Scotian stories include: Linden MacIntyre ("The Bishop's Man"); Hugh MacLennan ("Barometer Rising"); Rebecca McNutt ("Mandy and Alecto"); Ernest Buckler ("The Valley and the Mountain"); Archibald MacMechan ("Red Snow on Grand Pré"), Henry Wadsworth Longfellow (long poem "Evangeline"); Lawrence Hill ("The Book of Negroes") and John Mack Faragher ("Great and Nobel Scheme").

Nova Scotia is home to Symphony Nova Scotia, a symphony orchestra based in Halifax. The province has produced more than its fair share of famous musicians, including Grammy Award winners Denny Doherty (from The Mamas & the Papas), Anne Murray, and Sarah McLachlan, country singers Hank Snow, George Canyon, and Drake Jensen, jazz vocalist Holly Cole, classical performers Portia White and Barbara Hannigan, multi Juno Award nominated rapper Classified, and such diverse artists as Rita MacNeil, Matt Mays, Sloan, Feist, Todd Fancey, The Rankin Family, Natalie MacMaster, Susan Crowe, Buck 65, Joel Plaskett, and the bands April Wine and Grand Dérangement

There are numerous songs written about Nova Scotia: The Ballad of Springhill (written by Peggy Seeger and performed by Irish folk singer Luke Kelly, a member of The Dubliners); several songs by Stan Rogers including Bluenose, Watching The Apples Grow, The Jeannie C (mentions Little Dover, NS), Barrett's Privateers, Giant, and The Rawdon Hills; Farewell to Nova Scotia (traditional); Blue Nose (Stompin' Tom Connors); She's Called Nova Scotia (by Rita MacNeil); Cape Breton (by David Myles); Acadian Driftwood (by Robbie Robertson); Acadie (by Daniel Lanois); Song For The Mira (by Allister MacGillivray) and My Nova Scotia Home (by Hank Snow).

Nova Scotia has produced many significant songwriters, such as Grammy Award winning Gordie Sampson, who has written songs for Carrie Underwood ("Jesus, Take the Wheel", "Just a Dream", "Get Out of This Town"), Martina McBride ("If I Had Your Name", "You're Not Leavin Me"), LeAnn Rimes ("Long Night", "Save Myself"), and George Canyon ("My Name"). Many of Hank Snow's songs went on to be recorded by the likes of The Rolling Stones, Elvis Presley, and Johnny Cash. Cape Bretoners Allister MacGillivray and Leon Dubinsky have both written songs which, by being covered by so many popular artists, and by entering the repertoire of so many choirs around the world, have become iconic representations of Nova Scotian style, values and ethos. Dubinsky's pop ballad "We Rise Again" might be called the unofficial anthem of Cape Breton.

Music producer Brian Ahern is a Nova Scotian. He got his start by being music director for CBC television's Singalong Jubilee. He later produced 12 albums for Anne Murray ("Snowbird", "Danny's Song" and "You Won't See Me"); 11 albums for Emmylou Harris (whom he married at his home in Halifax on 9 January 1977). He also produced discs for Johnny Cash, George Jones, Roy Orbison, Glen Campbell, Don Williams, Jesse Winchester and Linda Ronstadt.

Sport is an important part of Nova Scotia culture. There are numerous semi pro, university and amateur sports teams, for example, The Halifax Mooseheads, 2013 Canadian Hockey League Memorial Cup Champions, and the Cape Breton Screaming Eagles, both of the Quebec Major Junior Hockey League. The Halifax Hurricanes of the National Basketball League of Canada is another team that calls Nova Scotia home, and were 2016 league champions. Professional soccer came to the province in 2019 in the form of Canadian Premier League club HFX Wanderers FC.

The Nova Scotia Open is a professional golf tournament on the Web.com Tour since 2014.

The province has also produced numerous athletes such as Sidney Crosby (ice hockey), Nathan Mackinnon (ice hockey), Brad Marchand (ice hockey), Colleen Jones (curling), Al MacInnis (ice hockey), TJ Grant (mixed martial arts), Rocky Johnson (wrestling, and father of Dwayne "The Rock" Johnson), George Dixon (boxing) and Kirk Johnson (boxing). The achievements of Nova Scotian athletes are presented at the Nova Scotia Sport Hall of Fame.

The Minister of Education is responsible for the administration and delivery of education, as defined by the Education Act and other acts relating to colleges, universities and private schools. The powers of the Minister and the Department of Education are defined by the Ministerial regulations and constrained by the Governor-In-Council regulations.

All children until the age of 16 are legally required to attend school or the parent needs to perform home schooling. Nova Scotia's education system is split up into eight different regions including; Tri-County (22 schools), Annapolis Valley (42 schools), South Shore (25 schools), Chignecto-Central (67 schools), Halifax (135 schools), Strait (20 schools) and Cape Breton-Victoria Regional Centre for Education (39 schools).

Nova Scotia has more than 450 public schools for children. The public system offers primary to Grade 12. There are also private schools in the province. Public education is administered by seven regional school boards, responsible primarily for English instruction and French immersion, and also province-wide by the Conseil Scolaire Acadien Provincial, which administers French instruction to students whose primary language is French.

The Nova Scotia Community College system has 13 campuses around the province. With a focus on training and education, the college was established in 1988 by amalgamating the province's former vocational schools. In addition to the provincial community college system, there are more than 90 registered private colleges in Nova Scotia.

Ten universities are also situated in Nova Scotia, including Dalhousie University, University of King's College, Saint Mary's University, Mount Saint Vincent University, NSCAD University, Acadia University, Université Sainte-Anne, Saint Francis Xavier University, Cape Breton University and the Atlantic School of Theology.




</doc>
<doc id="21186" url="https://en.wikipedia.org/wiki?curid=21186" title="Northwest Territories">
Northwest Territories

The Northwest Territories (abbr. NT or NWT) is a federal territory of Canada. At a land area of approximately and a 2016 census population of 41,786, it is the second-largest and the most populous of the three territories in Northern Canada. Its estimated population as of 2020 is 44,982. Yellowknife became the territorial capital in 1967, following recommendations by the Carrothers Commission.

The Northwest Territories, a portion of the old North-Western Territory, entered the Canadian Confederation on July 15, 1870. Since then, the territory has been divided four times to create new provinces and territories or enlarge existing ones. Its current borders date from April 1, 1999, when the already-smaller territory was decreased again by the creation of a new territory of Nunavut to the east, via the "Nunavut Act" and the Nunavut Land Claims Agreement. While Nunavut is mostly Arctic tundra, the Northwest Territories has a slightly warmer climate and is both boreal forest (taiga) and tundra, and its most northern regions form part of the Canadian Arctic Archipelago.

The Northwest Territories is bordered by Canada's two other territories, Nunavut to the east and Yukon to the west, and by the provinces of British Columbia, Alberta, and Saskatchewan to the south, and may touch Manitoba at a quadripoint to the southeast.

The name is descriptive, adopted by the British government during the colonial era to indicate where it lay in relation to the rest of Rupert's Land. It is shortened from North-Western Territory, which became the term North-West Territories ("see" History).

In Inuktitut, the Northwest Territories are referred to as (), "beautiful land." The northernmost region of the territory is home to Inuvialuit, part of Inuit Nunangat called Nunangit, while the southern portion is called (an Athabaskan language word meaning "our land"). is the vast Dene country, stretching from central Alaska to Hudson Bay, within which lie the homelands of the numerous Dene nations.

There was some discussion of changing the name of the Northwest Territories after the splitting off of Nunavut, possibly to a term from an Indigenous language. One proposal was "Denendeh," as advocated by the former premier Stephen Kakfwi, among others. One of the most popular proposals for a new name—to name the territory "Bob"—began as a prank, but for a while it was at or near the top in the public-opinion polls.

In the end, a poll conducted prior to division showed that strong support remained to keep the name "Northwest Territories." This name arguably became more appropriate following division than it had been when the territories extended far into Canada's north-central and northeastern areas.

Located in northern Canada, the territory borders Canada's two other territories, Yukon to the west and Nunavut to the east, as well as three provinces: British Columbia to the southwest, and Alberta and Saskatchewan to the south. It possibly meets Manitoba at a quadripoint to the extreme southeast, though surveys have not been completed. It has a land area of .

Geographical features include Great Bear Lake, the largest lake entirely within Canada, and Great Slave Lake, the deepest body of water in North America at , as well as the Mackenzie River and the canyons of the Nahanni National Park Reserve, a national park and UNESCO World Heritage Site. Territorial islands in the Canadian Arctic Archipelago include Banks Island, Borden Island, Prince Patrick Island, and parts of Victoria Island and Melville Island. Its highest point is Mount Nirvana near the border with Yukon at an elevation of .

The Northwest Territories extends for more than and has a large climate variant from south to north. The southern part of the territory (most of the mainland portion) has a subarctic climate, while the islands and northern coast have a polar climate.

Summers in the north are short and cool, daytime highs of 14–17 degrees Celsius (57–63 °F), and lows of 1–5 degrees Celsius (34–41 °F). Winters are long and harsh, with daytime highs , lows , and the coldest nights typically reaching each year.

Extremes are common with summer highs in the south reaching and lows reaching below . In winter in the south, it is not uncommon for the temperatures to reach , but they can also reach the low teens during the day. In the north, temperatures can reach highs of , and lows into the low negatives. In winter in the north it is not uncommon for the temperatures to reach but they can also reach single digits during the day.

Thunderstorms are not rare in the south. In the north they are very rare, but do occur. Tornadoes are extremely rare but have happened with the most notable one happening just outside Yellowknife that destroyed a communications tower. The Territory has a fairly dry climate due to the mountains in the west.

About half of the territory is above the tree line. There are not many trees in most of the eastern areas of the territory, or in the north islands.

Prior to the arrival of Europeans, a number of First Nations and Inuit nations occupied the area that became the Northwest Territories. Inuit nations include the Caribou, Central, and Copper nations. First Nations groups include the Beaver, Chipewyan, Dogrib, Nahanni, Sekani, Slavey, and Yellowknives.

In 1670, the Hudson's Bay Company (HBC) was formed from a royal charter, and was granted a commercial monopoly over Rupert's Land. Present day Northwest Territories laid northwest of Rupert's Land, known as the North-Western Territory. Although not formally part of Rupert's Land, the HBC made regular use of the region as a part of its trading area. The Treaty of Utrecht saw the British became the only European power with practical access to the North-Western Territory, with the French surrendering its claim to the Hudson Bay coast.

Europeans have visited the region for the purposes of fur trading, and exploration for new trade routes, including the Northwest Passage. Arctic expeditions launched in the 19th century include the Coppermine expedition.

In 1867, first Canadian residential school opened in the region in Fort Resolution. The opening of the school was followed by several others in regions across the territory, thus contributing to it reaching the highest percentage of students in residential schools compared to other area in Canada.

The present-day territory came under the authority of the Government of Canada in July 1870, after the Hudson's Bay Company transferred Rupert's Land and the North-Western Territory to the British Crown, which subsequently transferred them to Canada, giving it the name the North-west Territories. This immense region comprised all of today's Canada except British Columbia, early form of Manitoba (a small square area around Winnipeg), early forms of present-day Ontario and Quebec (the coast of the Great Lakes, the Saint Lawrence River valley and the southern third of modern Quebec), the Maritimes (Nova Scotia, Prince Edward Island and New Brunswick), Newfoundland, the Labrador coast, and the Arctic Islands (except the southern half of Baffin Island).

After the 1870 transfer, some of the North-West Territories was whittled away. The province of Manitoba was enlarged in 1881 to a rectangular region composing the modern province's south. By the time British Columbia joined Confederation on July 20, 1871, it had already (1866) been granted the portion of North-Western Territory south of 60 degrees north and west of 120 degrees west, an area that comprised most of the Stickeen Territories.

After the North-West Rebellion of 1885 a North-West Territories Council was created in 1887 for regional government of Canada west of the province of Manitoba; the council was reorganized in 1888 as the Legislative Assembly of the North-West Territories. Frederick Haultain, an Ontario lawyer who practised at Fort Macleod from 1884, became its chairman in 1891 and Premier when the Assembly was reorganized in 1897. The modern provinces of Saskatchewan and Alberta were created in 1905. Contemporary records show Haultain recommended that the NWT become a single province, named Buffalo, but the Canadian government of Sir Wilfid Laurier acted otherwise.

In the meantime, the Province of Ontario was enlarged northwestward in 1882. Quebec was also extended northwards in 1898. Yukon was made a separate territory that year, due to the Klondike Gold Rush, to free the North-west Territories government in Regina from the burden of addressing the problems caused by the sudden boom of population and economic activity, and the influx of non-Canadians. One year after the provinces of Alberta and Saskatchewan were created in 1905, the Parliament of Canada renamed the "North-West Territories" as the "Northwest Territories", dropping all hyphenated forms of it.

Manitoba, Ontario and Quebec acquired the last addition to their modern landmass from the Northwest Territories in 1912. This left only the districts of Mackenzie, Franklin (which absorbed the remnants of Ungava in 1920) and Keewatin within what was then given the name Northwest Territories. In 1925, the boundaries of the Northwest Territories were extended all the way to the North Pole on the sector principle, vastly expanding its territory onto the northern ice cap. Between 1925 and 1999, the Northwest Territories covered a land area of —larger than India.

On April 1, 1999, a separate Nunavut territory was formed from the eastern Northwest Territories to represent the Inuit people.

The NWT is one of two jurisdictions in Canada – Nunavut being the other – where Aboriginal peoples are in the majority, constituting 50.4% of the population.

According to the 2016 Canadian census, the 10 major ethnic groups were:


French was made an official language in 1877 by the territorial government. After a lengthy and bitter debate resulting from a speech from the throne in 1888 by Lieutenant Governor Joseph Royal, the members of the day voted on more than one occasion to nullify that and make English the only language used in the assembly. After some conflict with the Confederation Government in Ottawa, and a decisive vote on January 19, 1892, the assembly members voted for an English-only territory.

Currently, the Northwest Territories' "Official Languages Act" recognizes the following eleven official languages:
NWT residents have a right to use any of the above languages in a territorial court, and in the debates and proceedings of the legislature. However, the laws are legally binding only in their French and English versions, and the NWT government only publishes laws and other documents in the territory's other official languages when the legislature asks it to. Furthermore, access to services in any language is limited to institutions and circumstances where there is a significant demand for that language or where it is reasonable to expect it given the nature of the services requested. In practical terms, English language services are universally available, and there is no guarantee that other languages, including French, will be used by any particular government service, except for the courts.

The 2016 census returns showed a population of 41,786. Of the 40,565 singular responses to the census question regarding each inhabitant's "mother tongue", the most reported languages were the following (italics indicate an official language of the NWT):
There were also 630 responses of both English and a "non-official language"; 35 of both French and a "non-official language"; 145 of both English and French, and about 400 people who either did not respond to the question, or reported multiple non-official languages, or else gave some other unenumerable response. (Figures shown are for the number of single language responses and the percentage of total single-language responses.)

The largest denominations by number of adherents according to the 2001 census were Roman Catholic with 16,940 (46.7%); the Anglican Church of Canada with 5,510 (14.9%); and the United Church of Canada with 2,230 (6.0%), while a total of 6,465 (17.4%) people stated no religion.

As of 2014, there are 33 official communities in the NWT. These range in size from Yellowknife with a population of 19,569 to Kakisa with 36 people. Governance of each community differs, some are run under various types of First Nations control, while others are designated as a city, town, village or hamlet, but most communities are municipal corporations. Yellowknife is the largest community and has the largest number of Aboriginal peoples, 4,520 (23.4%) people. However, Behchokǫ̀, with a population of 1,874, is the largest First Nations community, 1,696 (90.9%), and Inuvik with 3,243 people is the largest Inuvialuit community, 1,315 (40.5%). There is one Indian reserve in the NWT, Hay River Reserve, located on the south shore of the Hay River.

The Gross Domestic Product of the Northwest Territories was C$4.856 billion in 2017. The Northwest Territories has the highest per capita GDP of all provinces or territories in Canada, C$76,000 in 2009.

The NWT's geological resources include gold, diamonds, natural gas and petroleum. British Petroleum (BP) is the only oil company currently producing oil in the Territory. NWT diamonds are promoted as an alternative to purchasing blood diamonds. Two of the biggest mineral resource companies in the world, BHP Billiton and Rio Tinto mine many of their diamonds from the NWT. In 2010, NWT accounted for 28.5% of Rio Tinto's total diamond production (3.9 million carats, 17% more than in 2009, from the Diavik Diamond Mine) and 100% of BHP's (3.05 million carats from the EKATI mine).

During the winter, many international visitors go to Yellowknife to watch the auroras. Five areas managed by Parks Canada are situated within the territory. Aulavik National Park and Tuktut Nogait National Park are in the northern part of Northwest Territories. Portions of Wood Buffalo National Park are located within the Northwest Territories, although most of it is located in neighbouring Alberta. Parks Canada also manages two park reserves, Nááts'ihch'oh National Park Reserve, and Nahanni National Park Reserve.

As a territory, the NWT has fewer rights than the provinces. During his term, Premier Kakfwi pushed to have the federal government accord more rights to the territory, including having a greater share of the returns from the territory's natural resources go to the territory. Devolution of powers to the territory was an issue in the 20th general election in 2003, and has been ever since the territory began electing members in 1881.

The Commissioner of the NWT is the chief executive and is appointed by the Governor-in-Council of Canada on the recommendation of the federal Minister of Aboriginal Affairs and Northern Development. The position used to be more administrative and governmental, but with the devolution of more powers to the elected assembly since 1967, the position has become symbolic. The Commissioner had full governmental powers until 1980 when the territories were given greater self-government. The Legislative Assembly then began electing a cabinet and "Government Leader", later known as the Premier. Since 1985 the Commissioner no longer chairs meetings of the Executive Council (or cabinet), and the federal government has instructed commissioners to behave like a provincial Lieutenant Governor. Unlike Lieutenant Governors, the Commissioner of the Northwest Territories is not a formal representative of the Queen of Canada.

Unlike provincial governments and the government of Yukon, the government of the Northwest Territories does not have political parties, except for the period between 1898 and 1905. It is a consensus government called the Legislative Assembly. This group is composed of one member elected from each of the nineteen constituencies. After each general election, the new Assembly elects the Premier and the Speaker by secret ballot. Seven MLAs are also chosen as cabinet ministers, with the remainder forming the opposition.

The membership of the current Legislative Assembly was set by the 2019 Northwest Territories general election on October 1, 2019. Caroline Cochrane was selected as the new Premier on October 24, 2019.

The member of Parliament for the Northwest Territories is Michael McLeod (Liberal Party). The Commissioner of the Northwest Territories is Margaret Thom.

In the Parliament of Canada, the NWT comprises a single Senate division and a single House of Commons electoral district, titled Northwest Territories ("Western Arctic" until 2014).

The Northwest Territories is divided into five administrative regions (with regional seat):

The Government of Northwest Territories comprises the following departments:

Aboriginal issues in the Northwest Territories include the fate of the Dene who, in the 1940s, were employed to carry radioactive uranium ore from the mines on Great Bear Lake. Of the thirty plus miners who worked at the Port Radium site, at least fourteen have died due to various forms of cancer. A study was done in the community of Deline, called "A Village of Widows" by Cindy Kenny-Gilday, which indicated that the number of people involved were too small to be able to confirm or deny a link.

There has been racial tension based on a history of violent conflict between the Dene and the Inuit, who have now taken recent steps towards reconciliation.

Land claims in the NWT began with the Inuvialuit Final Agreement, signed on June 5, 1984. It was the first Land Claim signed in the Territory, and the second in Canada. It culminated with the creation of the Inuit homeland of Nunavut, the result of the Nunavut Land Claims Agreement, the largest land claim in Canadian history.

Another land claims agreement with the Tłı̨chǫ people created a region within the NWT called Tli Cho, between Great Bear and Great Slave Lakes, which gives the Tłı̨chǫ their own legislative bodies, taxes, resource royalties, and other affairs, though the NWT still maintains control over such areas as health and education. This area includes two of Canada's three diamond mines, at Ekati and Diavik.

Among the festivals in the region are the Great Northern Arts Festival, the Snowking Winter Festival, Folk on the Rocks music festival in Yellowknife, and Rockin the Rocks.

Northwest Territories has nine numbered highways. The longest is the Mackenzie Highway, which stretches from the Alberta Highway 35's northern terminus in the south at the Alberta – Northwest Territories border at the 60th parallel to Wrigley, Northwest Territories in the north. Ice roads and winter roads are also prominent and provide road access in winter to towns and mines which would otherwise be fly-in locations. Yellowknife Highway branches out from Mackenzie Highway and connects it to Yellowknife. Dempster Highway is the continuation of Klondike Highway. It starts just west of Dawson City, Yukon, and continues east for over to Inuvik. As of 2017, the all-season Inuvik-Tuktoyaktuk Highway connects Inuvik to communities along the Arctic Ocean as an extension of the Dempster Highway.

Yellowknife did not have an all-season road access to the rest of Canada's highway network until the completion of Deh Cho Bridge in 2012. Prior to that, traffic relied on ferry service in summer and ice road in winter to cross the Mackenzie River. This became a problem during spring and fall time when the ice was not thick enough to handle vehicle load but the ferry could not pass through the ice, which would require all goods from fuel to groceries to be airlifted during the transition period.

Yellowknife Transit is the public transportation agency in the city, and is the only transit system within the Northwest Territories.

Yellowknife Airport is the largest airport in the territory in terms of aircraft movements and passengers. It is the gateway airport to other destinations within the Northwest Territories. As the airport of the territory capital, it is part of the National Airports System. It is the hub of multiple regional airlines. Major airlines serving destinations within Northwest Territories include Buffalo Airways, Canadian North, First Air, North-Wright Airways.




</doc>
<doc id="21187" url="https://en.wikipedia.org/wiki?curid=21187" title="Nez Perce people">
Nez Perce people

The Nez Perce (; autonym: , meaning "the walking people" or "we, the people") are an Indigenous people of the Plateau who have lived on the Columbia River Plateau in the Pacific Northwest region for at least 11,500 years.

Members of the Sahaptin language group, the Niimíipuu were the dominant people of the Columbia Plateau for much of that time, especially after acquiring the horses that led them to breed the appaloosa horse in the 18th century.

Prior to "first contact" with Western civilization the Nimiipuu were economically and culturally influential in trade and war, interacting with other indigenous nations in a vast network from the western shores of Oregon and Washington, the high plains of Montana, and the northern Great Basin in southern Idaho and northern Nevada.

French explorers and trappers indiscriminately used and popularized the name "Nez Percé" for the Niimíipuu and nearby Chinook. The name translates as "pierced nose", but only the Chinook used that form of body modification.

Today they are a federally recognized tribe, the Nez Perce Tribe of Idaho, and govern their Indian reservation in Idaho through a central government headquartered in Lapwai, Idaho known as the Nez Perce Tribal Executive Committee (NPTEC). They are one of five federally recognized tribes in the state of Idaho. Some still speak their traditional language, and the Tribe owns and operates two casinos along the Clearwater River in Idaho in Kamiah, Idaho and outside of Lewiston, Idaho, health clinics, a police force and court, community centers, salmon fisheries, radio station, and other things that promote economic and cultural self-determination.

Cut off from most of their horticultural sites throughout the Camas Prairie by an 1863 treaty, confinement to reservations in Idaho, Washington and Oklahoma Indian Territory after the Nez Perce War of 1877, and Dawes Act of 1887 land allotments (today some Nez Perce lease land to farmers or loggers, but the Nez Perce only own 12% of their own reservation), the Nez Perce remain as a distinct culture and political economic influence within and outside their reservation. Today, hatching, harvesting and eating salmon is an important cultural and economic strength of the Nez Perce through full ownership or co-management of various salmon fish hatcheries, such as the Kooskia National Fish Hatchery in Kooskia, Idaho or the Dworshak National Fish Hatchery in Orofino, Idaho.

Their name for themselves is "Nimíipuu" (pronounced ), meaning, "The People", in their language, part of the Sahaptin family.

"Nez Percé" is an exonym given by French Canadian fur traders who visited the area regularly in the late 18th century, meaning literally "pierced nose". English-speaking traders and settlers adopted the name in turn. Since the late 20th century, the Nez Perce identify most often as Niimíipuu in Sahaptin. The Lakota/ Dakota named them the "Watopala", or "Canoe" people, from "Watopa". However, after Nez Perce became a more common name, they changed it to "Watopahlute". This comes from "pahlute", nasal passage and is simply a play on words. If translated literally, it would come out as either ""Nasal Passage of the Canoe"" (Watopa-pahlute) or ""Nasal Passage of the Grass"" (Wato-pahlute). The Assiniboine called them "Pasú oȟnógA wįcaštA", the Arikara "sinitčiškataríwiš". The tribe also uses the term "Nez Perce", as does the United States Government in its official dealings with them, and contemporary historians. Older historical ethnological works and documents use the French spelling of "Nez Percé", with the diacritic. The original French pronunciation is , with three syllables.

The interpreter of the Lewis and Clark Expedition mistakenly identified this people as the Nez Perce when the team encountered the tribe in 1805. Writing in 1889, anthropologist Alice Fletcher, who the U.S. government had sent to Idaho to allot the Nez Perce Reservation, explained the mistaken naming. She wrote,

In his journals, William Clark referred to the people as the Chopunnish , a transliteration of a Sahaptin term. According to D.E. Walker in 1998, writing for the Smithsonian, this term is an adaptation of the term "cú·pŉitpeľu" (the Nez Perce people). The term is formed from "cú·pŉit" (piercing with a pointed object) and "peľu" (people). By contrast, the "Nez Perce Language Dictionary" has a different analysis than did Walker for the term "cúpnitpelu". The prefix "cú"- means "in single file". This prefix, combined with the verb "-piní", "to come out (e.g. of forest, bushes, ice)". Finally, with the suffix of "-pelú", meaning "people or inhabitants of". Together, these three elements: "cú"- + -"piní" + "pelú" = "cúpnitpelu", or "the People Walking Single File Out of the Forest". Nez Perce oral tradition indicates the name "Cuupn'itpel'uu" meant "we walked out of the woods or walked out of the mountains" and referred to the time before the Nez Perce had horses.

The Nez Perce language, or Niimiipuutímt, is a Sahaptian language related to the several dialects of Sahaptin. The Sahaptian sub-family is one of the branches of the Plateau Penutian family, which in turn may be related to a larger Penutian grouping.

The Nez Perce territory at the time of Lewis and Clark (1804–1806) was approximately and covered parts of present-day Washington, Oregon, Montana, and Idaho, in an area surrounding the Snake (Weyikespe), Grande Ronde River, Salmon (Naco’x kuus) ("Chinook salmon Water") and the Clearwater (Koos-Kai-Kai) ("Clear Water") rivers. The tribal area extended from the Bitterroots in the east (the door to the Northwestern Plains of Montana) to the Blue Mountains in the west between latitudes 45°N and 47°N.

In 1800, the Nez Perce had more than 100 permanent villages, ranging from 50 to 600 individuals, depending on the season and social grouping. Archeologists have identified a total of about 300 related sites including camps and villages, mostly in the Salmon River Canyon. In 1805, the Nez Perce were the largest tribe on the Columbia River Plateau, with a population of about 6,000. By the beginning of the 20th century, the Nez Perce had declined to about 1,800 due to epidemics, conflicts with non-Indians, and other factors. A total of 3,499 Nez Perce were counted in the 2010 Census.

Like other Plateau tribes, the Nez Perce had seasonal villages and camps to take advantage of natural resources throughout the year. Their migration followed a recurring pattern from permanent winter villages through several temporary camps, nearly always returning to the same locations each year. The Nez Perce traveled via the Lolo Trail (Salish: Naptnišaqs – "Nez Perce Trail") (Khoo-say-ne-ise-kit) far east as the Plains (Khoo-sayn / Kuseyn) ("Buffalo country") of Montana to hunt buffalo (Qoq'a lx) and as far west as the Pacific Coast (’Eteyekuus) ("Big Water"). Before 1957 construction of The Dalles Dam, which flooded this area, Celilo Falls (Silayloo) was a favored location on the Columbia River (Xuyelp) ("The Great River") for salmon (lé'wliks)-fishing.

The Nez Perce had many allies and trading partners among neighboring peoples, but also enemies and ongoing antagonist tribes. To the north of them lived the Coeur d’Alene (Schitsu'umsh) (’Iskíicu’mix), Spokane (Sqeliz) (Heyéeynimuu), and further north the Kalispel (Ql̓ispé) (Qem’éespel’uu, both meaning "Camas People"), Colville (Páapspaloo) and Kootenay / Kootenai (Ktunaxa) (Kuuspel’úu), to the northwest lived the Palus (Pelúucpuu) and to the west the Cayuse (Lik-si-yu) (Weyíiletpuu – "Ryegrass People"), west bound there were found the Umatilla (Imatalamłáma) (Hiyówatalampoo), Walla Walla, Wasco (Wecq’úupuu) and Sk'in (Tike’éspel’uu) and northwest of the latter various Yakama bands (Lexéyuu), to the south lived the Snake Indians (various Northern Paiute (Numu) bands (Hey’ǘuxcpel’uu) in the southwest and Bannock (Nimi Pan a'kwati)-Northern Shoshone (Newe) bands (Tiwélqe) in the southeast), to the east lived the Lemhi Shoshone (Lémhaay), north of them the Bitterroot Salish / Flathead (Seliš) (Séelix), further east and northeast on the Northern Plains were the Crow (Apsáalooke) (’Isúuxe) and two powerful alliances – the Iron Confedery (Nehiyaw-Pwat) (named after the dominating Plains and Woods Cree (Paskwāwiyiniwak and Sakāwithiniwak) and Assiniboine (Nakoda) (Wihnen’íipel’uu), an alliance of northern plains Native American nations based around the fur trade, and later included the Stoney (Nakoda), Western Saulteaux / Plains Ojibwe (Bungi or Nakawē), and Métis) and the Blackfoot Confederacy (Niitsitapi or Siksikaitsitapi) (’Isq’óyxnix) (composed of three Blackfoot speaking peoples – the Piegan or Peigan (Piikáni), the Kainai or Bloods (Káínaa), and the Siksika or Blackfoot (Siksikáwa), later joined by the unrelated Sarcee (Tsuu T'ina) and (for a time) by Gros Ventre or Atsina (A'aninin)).


Because of large amount of inter-marriage between Nez Perce bands and neighboring tribes or bands to forge alliances and peace (often living in mixed bilingual villages together), the following bands were also counted to the Nez Perce (which today are viewed as being linguistically and culturally closely related, but separate ethnic groups):

The semi-sedentary Nez Percés were Hunter-gatherer without agriculture living in a society in which most or all food is obtained by foraging (collecting wild plants and roots and pursuing wild animals). They depended on hunting, fishing, and the gathering of wild roots and berries.

Nez Perce people historically depended on various Pacific salmon and Pacific trout for their food: Chinook salmon or ""nacoox"" (Oncorhynchus tschawytscha) were eaten the most, but other species such as Pacific lamprey (Entosphenus tridentatus or Lampetra tridentata), and chiselmouth. Other important fishes included the Sockeye salmon (Oncorhynchus nerka), Silver salmon or "ka'llay" (Oncorhynchus kisutch), Chum salmon or dog salmon or "ka'llay" (Oncorhynchus keta), Mountain whitefish or ""ci'mey"" (Prosopium williamsoni), White sturgeon (Acipenser transmontanus), White sucker or ""mu'quc"" (Catostomus commersonii), and varieties of trout – West Coast steelhead or ""heyey"" (Oncorhynchus mykiss), brook trout or ""pi'ckatyo"" (Salvelinus fontinalis), bull trout or ""i'slam"" (Salvelinus confluentus), and Cutthroat trout or ""wawa'lam"" (Oncorhynchus clarkii).

Prior to contact with Europeans, the Nez Perce's traditional hunting and fishing areas spanned from the Cascade Range in the west to the Bitterroot Mountains in the east.

Historically, in late May and early June, Nez Perce villagers crowded to communal fishing sites to trap eels, steelhead, and chinook salmon, or haul in fish with large dip nets. Fishing took place throughout the summer and fall, first on the lower streams and then on the higher tributaries, and catches also included salmon, sturgeon, whitefish, suckers, and varieties of trout. Most of the supplies for winter use came from a second run in the fall, when large numbers of Sockeye salmon, silver, and dog salmon appeared in the rivers.

Fishing is traditionally an important ceremonial and commercial activity for the Nez Perce tribe. Today Nez Perce fishers participate in tribal fisheries in the mainstream Columbia River between Bonneville and McNary dams. The Nez Perce also fish for spring and summer Chinook salmon and Rainbow trout/steelhead in the Snake River and its tributaries. The Nez Perce tribe runs the Nez Perce Tribal Hatchery on the Clearwater River, as well as several satellite hatchery programs. 
The first fishing of the season was accompanied by prescribed rituals and a ceremonial feast known as ""kooyit"". Thanksgiving was offered to the Creator and to the fish for having returned and given themselves to the people as food. In this way, it was hoped that the fish would return the next year.

Like salmon, plants contributed to traditional Nez Perce culture in both material and spiritual dimensions.

Aside from fish and game, Plant foods provided over half of the dietary calories, with winter survival depending largely on dried roots, especially Kouse, or ""qáamsit"" (when fresh) and ""qáaws"" (when peeled and dried) (Lomatium especially Lomatium cous), and Camas, or ""qém'es"" (Nez Perce: "sweet") (Camassia quamash), the first being roasted in pits, while the other was ground in mortars and molded into cakes for future use, both plants had been traditionally an important food and trade item. Women were primarily responsible for the gathering and preparing of these root crops. Camas bulbs were gathered in the region between the Salmon and Clearwater river drainages. Techniques for preparing and storing winter foods enabled people to survive times of colder winters with little or no fresh foods.

Favorite fruits dried for winter were serviceberries or ""kel"" (Amelanchier alnifolia or Saskatoon berry), black huckleberries or ""cemi'tk"" (Vaccinium membranaceum), red elderberries or ""mi'ttip"" (Sambucus racemosa var. melanocarpa), and chokecherries or ""ti'ms"" (Prunus virginiana var. melanocarpa). Nez Perce textiles were made primarily from dogbane or ""qeemu"" (Apocynum cannabinum or Indian hemp), tules or ""to'ko"" (Schoenoplectus acutus var. acutus), and western redcedar or ""tala'tat"" (Thuja plicata). The most important industrial woods were redcedar, ponderosa pine or ""la'qa"" (Pinus ponderosa), Douglas fir or ""pa'ps"" (Pseudotsuga menziesii), sandbar willow or ""tax's"" (Salix exigua), and hard woods such as Pacific yew or ""ta'mqay"" (Taxus brevifolia) and syringa or ""sise'qiy"" (Philadelphus lewisii or Indian arrowwood).

Many fishes and plants important to Nez Perce culture are today state symbols: the black huckleberry or ""cemi'tk"" is the official state fruit and the Indian arrowwood or ""sise'qiy"", the Douglas fir or ""pa'ps"" is the state tree of Oregon and the ponderosa pine or ""la'qa"" of Montana, the Chinook salmon is the state fish of Oregon, the cutthroat trout or ""wawa'lam"" of Idaho, Montana and Wyoming, and the West Coast steelhead or "heyey" of Washington.
The Nez Perce believed in spirits called "weyekins" (Wie-a-kins) which would, they thought, offer a link to the invisible world of spiritual power". The weyekin would protect one from harm and become a personal guardian spirit. To receive a weyekin, a seeker would go to the mountains alone on a vision quest. This included fasting and meditation over several days. While on the quest, the individual may receive a vision of a spirit, which would take the form of a mammal or bird. This vision could appear physically or in a dream or trance. The weyekin was to bestow the animal's powers on its bearer—for example; a deer might give its bearer swiftness. A person's weyekin was very personal. It was rarely shared with anyone and was contemplated in private. The weyekin stayed with the person until death.

Helen Hunt Jackson, author of "A Century of Dishonor", written in 1889 refers to the Nez Perce as "the richest, noblest, and most gentle" of Indian peoples as well as the most industrious.

The museum at the Nez Perce National Historical Park, headquartered in Spalding, Idaho, and managed by the National Park Service includes a research center, archives, and library. Historical records are available for on-site study and interpretation of Nez Perce history and culture. The park includes 38 sites associated with the Nez Perce in the states of Idaho, Montana, Oregon, and Washington, many of which are managed by local and state agencies.

In 1805 William Clark was the first known Euro-American to meet any of the tribe, excluding the aforementioned French Canadian traders. While he, Meriwether Lewis and their men were crossing the Bitterroot Mountains, they ran low of food, and Clark took six hunters and hurried ahead to hunt. On September 20, 1805, near the western end of the Lolo Trail, he found a small camp at the edge of the camas-digging ground, which is now called Weippe Prairie. The explorers were favorably impressed by the Nez Perce whom they met. Preparing to make the remainder of their journey to the Pacific by boats on rivers, they entrusted the keeping of their horses until they returned to "2 brothers and one son of one of the Chiefs." One of these Indians was "Walammottinin" (meaning "Hair Bunched and tied," but more commonly known as Twisted Hair). He was the father of Chief Lawyer, who by 1877 was a prominent member of the "Treaty" faction of the tribe. The Nez Perce were generally faithful to the trust; the party recovered their horses without serious difficulty when they returned.

Recollecting the Nez Perce encounter with the Lewis and Clark party, in 1889 anthropologist Alice Fletcher wrote that "the Lewis and Clark explorers were the first white men that many of the people had ever seen and the women thought them beautiful." She wrote that the Nez Perce "were kind to the tired and hungry party. They furnished fresh horses and dried meat and fish with wild potatoes and other roots which were good to eat, and the refreshed white men went further on, westward, leaving their bony, wornout horses for the Indians to take care of and have fat and strong when Lewis and Clark should come back on their way home." On their return trip they arrived at the Nez Perce encampment the following spring, again hungry and exhausted. The tribe constructed a large tent for them and again fed them. Desiring fresh red meat, the party offered an exchange for a Nez Perce horse. Quoting from the Lewis and Clark diary, Fletcher writes, "The hospitality of the Chiefs was offended at the idea of an exchange. He observed that his people had an abundance of young horses and that if we were disposed to use that food, we might have as many as we wanted." The party stayed with the Nez Perce for a month before moving on.

The Nez Perce were one of the tribal nations at the Walla Walla Council (1855) (along with the Cayuse, Umatilla, Walla Walla, and Yakama), which signed the Treaty of Walla Walla.

Under pressure from the European Americans, in the late 19th century the Nez Perce split into two groups: one side accepted the coerced relocation to a reservation and the other refused to give up their fertile land in Idaho and Oregon. Those willing to go to a reservation made a treaty in 1877. The flight of the non-treaty Nez Perce began on June 15, 1877, with Chief Joseph, Looking Glass, White Bird, Ollokot, Lean Elk (Poker Joe) and Toohoolhoolzote leading 2,900 men, women and children in an attempt to reach a peaceful sanctuary. They intended to seek shelter with their allies the Crow but, upon the Crow's refusal to offer help, the Nez Perce tried to reach the camp in Canada of Lakota Chief Sitting Bull. He had migrated there instead of surrendering after the Indian victory at the Battle of the Little Bighorn.
The Nez Perce were pursued by over 2,000 soldiers of the U.S. Army on an epic flight to freedom of more than across four states and multiple mountain ranges. The 800 Nez Perce warriors defeated or held off the pursuing troops in 18 battles, skirmishes, and engagements. More than 300 US soldiers and 1,000 Nez Perce (including women and children) were killed in these conflicts.

A majority of the surviving Nez Perce were finally forced to surrender on October 5, 1877, after the Battle of the Bear Paw Mountains in Montana, from the Canada–US border. Chief Joseph surrendered to General Oliver O. Howard of the U.S. Cavalry. During the surrender negotiations, Chief Joseph sent a message, usually described as a speech, to the US soldiers. It has become renowned as one of the greatest American speeches: "...Hear me, my chiefs, I am tired. My heart is sick and sad. From where the sun now stands, I will fight no more forever."

Chief Joseph went to Washington, D.C. in January 1879 to meet with the President and Congress, after which his account was published in the North American Review.

The route of the Nez Perce flight is preserved by the Nez Perce National Historic Trail. The annual Cypress Hills ride in June commemorates the Nez Perce people's attempt to escape to Canada.

In 1994 the Nez Perce tribe began a breeding program, based on crossbreeding the Appaloosa and a Central Asian breed called Akhal-Teke, to produce what they called the Nez Perce Horse. They wanted to restore part of their traditional horse culture, where they had conducted selective breeding of their horses, long considered a marker of wealth and status, and trained their members in a high quality of horsemanship. Social disruption due to reservation life and assimilationist pressures by Americans and the government resulted in the destruction of their horse culture in the 19th century. The 20th-century breeding program was financed by the United States Department of Health and Human Services, the Nez Perce tribe, and the nonprofit called the First Nations Development Institute. It has promoted businesses in Native American country that reflect values and traditions of the peoples. The Nez Perce Horse breed is noted for its speed.

The current tribal lands consist of a reservation in North Central Idaho at , primarily in the Camas Prairie region south of the Clearwater River, in parts of four counties. In descending order of surface area, the counties are Nez Perce, Lewis, Idaho, and Clearwater. The total land area is about , and the reservation's population at the 2000 census was 17,959.

Due to tribal loss of lands, the population on the reservation is predominantly white, nearly 90% in 1988. The largest community is the city of Orofino, near its northeast corner. Lapwai is the seat of tribal government, and it has the highest percentage of Nez Percé people as residents, at about 81.4 percent.

Similar to the opening of Native American lands in Oklahoma by allowing acquisition of surplus by non-natives after households received plots, the U.S. government opened the Nez Percé reservation for general settlement on November 18, 1895. The proclamation had been signed less than two weeks earlier by President Grover Cleveland. Thousands rushed to grab land on the reservation, staking out their claims even on land owned by Nez Percé families.




In addition, the Colville Indian Reservation in eastern Washington contains the Joseph band of Nez Percé.





</doc>
<doc id="21189" url="https://en.wikipedia.org/wiki?curid=21189" title="Neolithic">
Neolithic

The Neolithic (, also known as the "New Stone Age"), the final division of the Stone Age, began about 12,000 years ago when the first developments of farming appeared in the Epipalaeolithic Near East, and later in other parts of the world. 
The Neolithic division lasted (in that part of the world) until the transitional period of the Chalcolithic from about 6,500 years ago (4500 BC), marked by the development of metallurgy, leading up to the Bronze Age and Iron Age. In other places the Neolithic lasted longer. In Northern Europe, the Neolithic lasted until about 1700 BC, while in China it extended until 1200 BC. Other parts of the world (including the Americas and Oceania) remained broadly in the Neolithic stage of development until European contact.

The Neolithic comprises a progression of behavioral and cultural characteristics and changes, including the use of wild and domestic crops and of domesticated animals.

The term "Neolithic" derives from the Greek , "new", and , "stone", literally meaning "New Stone Age". The term was coined by Sir John Lubbock in 1865 as a refinement of the three-age system.

Following the ASPRO chronology, the Neolithic started in around 10,200 BC in the Levant, arising from the Natufian culture, when pioneering use of wild cereals evolved into early farming. The Natufian period or "proto-Neolithic" lasted from 12,500 to 9,500 BC, and is taken to overlap with the Pre-Pottery Neolithic (PPNA) of 10,200–8800 BC. As the Natufians had become dependent on wild cereals in their diet, and a sedentary way of life had begun among them, the climatic changes associated with the Younger Dryas (about 10,000 BC) are thought to have forced people to develop farming.

By 10,200–8,800 BC farming communities had arisen in the Levant and spread to Asia Minor, North Africa and North Mesopotamia. Mesopotamia is the site of the earliest developments of the Neolithic Revolution from around 10,000 BC.

Early Neolithic farming was limited to a narrow range of plants, both wild and domesticated, which included einkorn wheat, millet and spelt, and the keeping of dogs, sheep and goats. By about 6900–6400 BC, it included domesticated cattle and pigs, the establishment of permanently or seasonally inhabited settlements, and the use of pottery.

Not all of these cultural elements characteristic of the Neolithic appeared everywhere in the same order: the earliest farming societies in the Near East did not use pottery. In other parts of the world, such as Africa, South Asia and Southeast Asia, independent domestication events led to their own regionally distinctive Neolithic cultures, which arose completely independently of those in Europe and Southwest Asia. Early Japanese societies and other East Asian cultures used pottery "before" developing agriculture.

In the Middle East, cultures identified as Neolithic began appearing in the 10th millennium BC. Early development occurred in the Levant (e.g. Pre-Pottery Neolithic A and Pre-Pottery Neolithic B) and from there spread eastwards and westwards. Neolithic cultures are also attested in southeastern Anatolia and northern Mesopotamia by around 8000 BC.

The prehistoric Beifudi site near Yixian in Hebei Province, China, contains relics of a culture contemporaneous with the Cishan and Xinglongwa cultures of about 6000–5000 BC, Neolithic cultures east of the Taihang Mountains, filling in an archaeological gap between the two Northern Chinese cultures. The total excavated area is more than , and the collection of Neolithic findings at the site encompasses two phases.

The Neolithic 1 (PPNA) period began roughly around 10,000 BC in the Levant. A temple area in southeastern Turkey at Göbekli Tepe, dated to around 9500 BC, may be regarded as the beginning of the period. This site was developed by nomadic hunter-gatherer tribes, as evidenced by the lack of permanent housing in the vicinity, and may be the oldest known human-made place of worship. At least seven stone circles, covering , contain limestone pillars carved with animals, insects, and birds. Stone tools were used by perhaps as many as hundreds of people to create the pillars, which might have supported roofs. Other early PPNA sites dating to around 9500–9000 BC have been found in Tell es-Sultan (ancient Jericho), Israel (notably Ain Mallaha, Nahal Oren, and Kfar HaHoresh), Gilgal in the Jordan Valley, and Byblos, Lebanon. The start of Neolithic 1 overlaps the Tahunian and Heavy Neolithic periods to some degree.

The major advance of Neolithic 1 was true farming. In the proto-Neolithic Natufian cultures, wild cereals were harvested, and perhaps early seed selection and re-seeding occurred. The grain was ground into flour. Emmer wheat was domesticated, and animals were herded and domesticated (animal husbandry and selective breeding).

In 2006, remains of figs were discovered in a house in Jericho dated to 9400 BC. The figs are of a mutant variety that cannot be pollinated by insects, and therefore the trees can only reproduce from cuttings. This evidence suggests that figs were the first cultivated crop and mark the invention of the technology of farming. This occurred centuries before the first cultivation of grains.

Settlements became more permanent, with circular houses, much like those of the Natufians, with single rooms. However, these houses were for the first time made of mudbrick. The settlement had a surrounding stone wall and perhaps a stone tower (as in Jericho). The wall served as protection from nearby groups, as protection from floods, or to keep animals penned. Some of the enclosures also suggest grain and meat storage.

The Neolithic 2 (PPNB) began around 8800 BC according to the ASPRO chronology in the Levant (Jericho, West Bank). As with the PPNA dates, there are two versions from the same laboratories noted above. This system of terminology, however, is not convenient for southeast Anatolia and settlements of the middle Anatolia basin. A settlement of 3,000 inhabitants was found in the outskirts of Amman, Jordan. Considered to be one of the largest prehistoric settlements in the Near East, called 'Ain Ghazal, it was continuously inhabited from approximately 7250 BC to approximately 5000 BC.

Settlements have rectangular mud-brick houses where the family lived together in single or multiple rooms. Burial findings suggest an ancestor cult where people preserved skulls of the dead, which were plastered with mud to make facial features. The rest of the corpse could have been left outside the settlement to decay until only the bones were left, then the bones were buried inside the settlement underneath the floor or between houses.

Work at the site of 'Ain Ghazal in Jordan has indicated a later Pre-Pottery Neolithic C period. Juris Zarins has proposed that a Circum Arabian Nomadic Pastoral Complex developed in the period from the climatic crisis of 6200 BCE, partly as a result of an increasing emphasis in PPNB cultures upon domesticated animals, and a fusion with Harifian hunter gatherers in the Southern Levant, with affiliate connections with the cultures of Fayyum and the Eastern Desert of Egypt. Cultures practicing this lifestyle spread down the Red Sea shoreline and moved east from Syria into southern Iraq.

The Neolithic 3 (PN) began around 6,400 BC in the Fertile Crescent. By then distinctive cultures emerged, with pottery like the Halafian (Turkey, Syria, Northern Mesopotamia) and Ubaid (Southern Mesopotamia). This period has been further divided into PNA (Pottery Neolithic A) and PNB (Pottery Neolithic B) at some sites.

The Chalcolithic (Stone-Bronze) period began about 4500 BC, then the Bronze Age began about 3500 BC, replacing the Neolithic cultures.

Around 10,000 BC the first fully developed Neolithic cultures belonging to the phase Pre-Pottery Neolithic A (PPNA) appeared in the Fertile Crescent. Around 10,700–9400 BC a settlement was established in Tell Qaramel, north of Aleppo. The settlement included two temples dating to 9650 BC. Around 9000 BC during the PPNA, one of the world's first towns, Jericho, appeared in the Levant. It was surrounded by a stone wall and contained a population of 2,000–3,000 people and a massive stone tower. Around 6400 BC the Halaf culture appeared in Syria and Northern Mesopotamia.

In 1981 a team of researchers from the Maison de l'Orient et de la Méditerranée, including Jacques Cauvin and Oliver Aurenche divided Near East Neolithic chronology into ten periods (0 to 9) based on social, economic and cultural characteristics. In 2002 Danielle Stordeur and Frédéric Abbès advanced this system with a division into five periods. 
They also advanced the idea of a transitional stage between the PPNA and PPNB between 8800 and 8600 BC at sites like Jerf el Ahmar and Tell Aswad.

Alluvial plains (Sumer/Elam). Low rainfall makes irrigation systems necessary. Ubaid culture from 6,900 BC.

Domestication of sheep and goats reached Egypt from the Near East possibly as early as 6000 BC. Graeme Barker states "The first indisputable evidence for domestic plants and animals in the Nile valley is not until the early fifth millennium BC in northern Egypt and a thousand years later further south, in both cases as part of strategies that still relied heavily on fishing, hunting, and the gathering of wild plants" and suggests that these subsistence changes were not due to farmers migrating from the Near East but was an indigenous development, with cereals either indigenous or obtained through exchange. Other scholars argue that the primary stimulus for agriculture and domesticated animals (as well as mud-brick architecture and other Neolithic cultural features) in Egypt was from the Middle East.

In southeast Europe agrarian societies first appeared in the 7th millennium BC, attested by one of the earliest farming sites of Europe, discovered in Vashtëmi, southeastern Albania and dating back to 6500 BC. In Northwest Europe it is much later, typically lasting just under 3,000 years from c. 4500 BC–1700 BC.

Anthropomorphic figurines have been found in the Balkans from 6000 BC, and in Central Europe by around 5800 BC (La Hoguette). Among the earliest cultural complexes of this area are the Sesklo culture in Thessaly, which later expanded in the Balkans giving rise to Starčevo-Körös (Cris), Linearbandkeramik, and Vinča. Through a combination of cultural diffusion and migration of peoples, the Neolithic traditions spread west and northwards to reach northwestern Europe by around 4500 BC. The Vinča culture may have created the earliest system of writing, the Vinča signs, though archaeologist Shan Winn believes they most likely represented pictograms and ideograms rather than a truly developed form of writing.

The Cucuteni-Trypillian culture built enormous settlements in Romania, Moldova and Ukraine from 5300 to 2300 BC. The megalithic temple complexes of Ġgantija on the Mediterranean island of Gozo (in the Maltese archipelago) and of Mnajdra (Malta) are notable for their gigantic Neolithic structures, the oldest of which date back to around 3600 BC. The Hypogeum of Ħal-Saflieni, Paola, Malta, is a subterranean structure excavated around 2500 BC; originally a sanctuary, it became a necropolis, the only prehistoric underground temple in the world, and shows a degree of artistry in stone sculpture unique in prehistory to the Maltese islands. After 2500 BC, these islands were depopulated for several decades until the arrival of a new influx of Bronze Age immigrants, a culture that cremated its dead and introduced smaller megalithic structures called dolmens to Malta. In most cases there are small chambers here, with the cover made of a large slab placed on upright stones. They are claimed to belong to a population different from that which built the previous megalithic temples. It is presumed the population arrived from Sicily because of the similarity of Maltese dolmens to some small constructions found there.

Settled life, encompassing the transition from foraging to farming and pastoralism, began in South Asia in the region of Balochistan, Pakistan, around 7,000 BCE. At the site of Mehrgarh, Balochistan, presence can be documented of the domestication of wheat and barley, rapidly followed by that of goats, sheep, and cattle. In April 2006, it was announced in the scientific journal "Nature" that the oldest (and first "early Neolithic") evidence for the drilling of teeth "in vivo" (using bow drills and flint tips) was found in Mehrgarh.

In South India, the Neolithic began by 6500 BC and lasted until around 1400 BC when the Megalithic transition period began. South Indian Neolithic is characterized by Ash mounds from 2500 BC in Karnataka region, expanded later to Tamil Nadu.

In East Asia, the earliest sites include the Nanzhuangtou culture around 9500–9000 BC, Pengtoushan culture around 7500–6100 BC, and Peiligang culture around 7000–5000 BC.

The 'Neolithic' (defined in this paragraph as using polished stone implements) remains a living tradition in small and extremely remote and inaccessible pockets of West Papua (Indonesian New Guinea). Polished stone adze and axes are used in the present day () in areas where the availability of metal implements is limited. This is likely to cease altogether in the next few years as the older generation die off and steel blades and chainsaws prevail.

In 2012, news was released about a new farming site discovered in Munam-ri, Goseong, Gangwon Province, South Korea, which may be the earliest farmland known to date in east Asia. "No remains of an agricultural field from the Neolithic period have been found in any East Asian country before, the institute said, adding that the discovery reveals that the history of agricultural cultivation at least began during the period on the Korean Peninsula". The farm was dated between 3600 and 3000 BC. Pottery, stone projectile points, and possible houses were also found. "In 2002, researchers discovered prehistoric earthenware, jade earrings, among other items in the area". The research team will perform accelerator mass spectrometry (AMS) dating to retrieve a more precise date for the site.

In Mesoamerica, a similar set of events (i.e., crop domestication and sedentary lifestyles) occurred by around 4500 BC, but possibly as early as 11,000–10,000 BC. These cultures are usually not referred to as belonging to the Neolithic; in America different terms are used such as Formative stage instead of mid-late Neolithic, Archaic Era instead of Early Neolithic, and Paleo-Indian for the preceding period. 

The Formative stage is equivalent to the Neolithic Revolution period in Europe, Asia, and Africa. In the southwestern United States it occurred from 500 to 1200 AD when there was a dramatic increase in population and development of large villages supported by agriculture based on dryland farming of maize, and later, beans, squash, and domesticated turkeys. During this period the bow and arrow and ceramic pottery were also introduced. In later periods cities of considerable size developed, and some metallurgy by 700 BCE.

Australia, in contrast to New Guinea, has generally been held not to have had a Neolithic period, with a hunter-gatherer lifestyle continuing until the arrival of Europeans. This view can be challenged in terms of the definition of agriculture, but "Neolithic" remains a rarely used and not very useful concept in discussing Australian prehistory.

During most of the Neolithic age of Eurasia, people lived in small tribes composed of multiple bands or lineages. There is little scientific evidence of developed social stratification in most Neolithic societies; social stratification is more associated with the later Bronze Age. Although some late Eurasian Neolithic societies formed complex stratified chiefdoms or even states, generally states evolved in Eurasia only with the rise of metallurgy, and most Neolithic societies on the whole were relatively simple and egalitarian. Beyond Eurasia, however, states were formed during the local Neolithic in three areas, namely in the Preceramic Andes with the Norte Chico Civilization, Formative Mesoamerica and Ancient Hawaiʻi. However, most Neolithic societies were noticeably more hierarchical than the Upper Paleolithic cultures that preceded them and hunter-gatherer cultures in general.

The domestication of large animals (c. 8000 BC) resulted in a dramatic increase in social inequality in most of the areas where it occurred; New Guinea being a notable exception. Possession of livestock allowed competition between households and resulted in inherited inequalities of wealth. Neolithic pastoralists who controlled large herds gradually acquired more livestock, and this made economic inequalities more pronounced. However, evidence of social inequality is still disputed, as settlements such as Catal Huyuk reveal a striking lack of difference in the size of homes and burial sites, suggesting a more egalitarian society with no evidence of the concept of capital, although some homes do appear slightly larger or more elaborately decorated than others.

Families and households were still largely independent economically, and the household was probably the center of life. However, excavations in Central Europe have revealed that early Neolithic Linear Ceramic cultures (""Linearbandkeramik"") were building large arrangements of circular ditches between 4800 and 4600 BC. These structures (and their later counterparts such as causewayed enclosures, burial mounds, and henge) required considerable time and labour to construct, which suggests that some influential individuals were able to organise and direct human labour — though non-hierarchical and voluntary work remain possibilities.

There is a large body of evidence for fortified settlements at "Linearbandkeramik" sites along the Rhine, as at least some villages were fortified for some time with a palisade and an outer ditch. Settlements with palisades and weapon-traumatized bones, such as those found at the Talheim Death Pit, have been discovered and demonstrate that "...systematic violence between groups" and warfare was probably much more common during the Neolithic than in the preceding Paleolithic period. This supplanted an earlier view of the Linear Pottery Culture as living a "peaceful, unfortified lifestyle".

Control of labour and inter-group conflict is characteristic of tribal groups with social rank that are headed by a charismatic individual — either a 'big man' or a proto-chief — functioning as a lineage-group head. Whether a non-hierarchical system of organization existed is debatable, and there is no evidence that explicitly suggests that Neolithic societies functioned under any dominating class or individual, as was the case in the chiefdoms of the European Early Bronze Age. Theories to explain the apparent implied egalitarianism of Neolithic (and Paleolithic) societies have arisen, notably the Marxist concept of primitive communism.

The shelter of the early people changed dramatically from the Upper Paleolithic to the Neolithic era. In the Paleolithic, people did not normally live in permanent constructions. In the Neolithic, mud brick houses started appearing that were coated with plaster. The growth of agriculture made permanent houses possible. Doorways were made on the roof, with ladders positioned both on the inside and outside of the houses. The roof was supported by beams from the inside. The rough ground was covered by platforms, mats, and skins on which residents slept. Stilt-houses settlements were common in the Alpine and Pianura Padana (Terramare) region. Remains have been found at the Ljubljana Marshes in Slovenia and at the Mondsee and Attersee lakes in Upper Austria, for example.

A significant and far-reaching shift in human subsistence and lifestyle was to be brought about in areas where crop farming and cultivation were first developed: the previous reliance on an essentially nomadic hunter-gatherer subsistence technique or pastoral transhumance was at first supplemented, and then increasingly replaced by, a reliance upon the foods produced from cultivated lands. These developments are also believed to have greatly encouraged the growth of settlements, since it may be supposed that the increased need to spend more time and labor in tending crop fields required more localized dwellings. This trend would continue into the Bronze Age, eventually giving rise to permanently settled farming towns, and later cities and states whose larger populations could be sustained by the increased productivity from cultivated lands.

The profound differences in human interactions and subsistence methods associated with the onset of early agricultural practices in the Neolithic have been called the "Neolithic Revolution", a term coined in the 1920s by the Australian archaeologist Vere Gordon Childe.

One potential benefit of the development and increasing sophistication of farming technology was the possibility of producing surplus crop yields, in other words, food supplies in excess of the immediate needs of the community. Surpluses could be stored for later use, or possibly traded for other necessities or luxuries. Agricultural life afforded securities that nomadic life could not, and sedentary farming populations grew faster than nomadic.

However, early farmers were also adversely affected in times of famine, such as may be caused by drought or pests. In instances where agriculture had become the predominant way of life, the sensitivity to these shortages could be particularly acute, affecting agrarian populations to an extent that otherwise may not have been routinely experienced by prior hunter-gatherer communities. Nevertheless, agrarian communities generally proved successful, and their growth and the expansion of territory under cultivation continued.

Another significant change undergone by many of these newly agrarian communities was one of diet. Pre-agrarian diets varied by region, season, available local plant and animal resources and degree of pastoralism and hunting. Post-agrarian diet was restricted to a limited package of successfully cultivated cereal grains, plants and to a variable extent domesticated animals and animal products. Supplementation of diet by hunting and gathering was to variable degrees precluded by the increase in population above the carrying capacity of the land and a high sedentary local population concentration. In some cultures, there would have been a significant shift toward increased starch and plant protein. The relative nutritional benefits and drawbacks of these dietary changes and their overall impact on early societal development are still debated.

In addition, increased population density, decreased population mobility, increased continuous proximity to domesticated animals, and continuous occupation of comparatively population-dense sites would have altered sanitation needs and patterns of disease.

The identifying characteristic of Neolithic technology is the use of polished or ground stone tools, in contrast to the flaked stone tools used during the Paleolithic era.

Neolithic people were skilled farmers, manufacturing a range of tools necessary for the tending, harvesting and processing of crops (such as sickle blades and grinding stones) and food production (e.g. pottery, bone implements). They were also skilled manufacturers of a range of other types of stone tools and ornaments, including projectile points, beads, and statuettes. But what allowed forest clearance on a large scale was the polished stone axe above all other tools. Together with the adze, fashioning wood for shelter, structures and canoes for example, this enabled them to exploit their newly won farmland.

Neolithic peoples in the Levant, Anatolia, Syria, northern Mesopotamia and Central Asia were also accomplished builders, utilizing mud-brick to construct houses and villages. At Çatalhöyük, houses were plastered and painted with elaborate scenes of humans and animals. In Europe, long houses built from wattle and daub were constructed. Elaborate tombs were built for the dead. These tombs are particularly numerous in Ireland, where there are many thousand still in existence. Neolithic people in the British Isles built long barrows and chamber tombs for their dead and causewayed camps, henges, flint mines and cursus monuments. It was also important to figure out ways of preserving food for future months, such as fashioning relatively airtight containers, and using substances like salt as preservatives.

The peoples of the Americas and the Pacific mostly retained the Neolithic level of tool technology until the time of European contact. Exceptions include copper hatchets and spearheads in the Great Lakes region.

Most clothing appears to have been made of animal skins, as indicated by finds of large numbers of bone and antler pins that are ideal for fastening leather. Wool cloth and linen might have become available during the later Neolithic, as suggested by finds of perforated stones that (depending on size) may have served as spindle whorls or loom weights. The clothing worn in the Neolithic Age might be similar to that worn by Ötzi the Iceman, although he was not Neolithic (since he belonged to the later Copper Age).

Neolithic human settlements include:

The world's oldest known engineered roadway, the Sweet Track in England, dates from 3800 BC and the world's oldest freestanding structure is the Neolithic temple of Ġgantija in Gozo, Malta.

"Note: Dates are very approximate, and are only given for a rough estimate; consult each culture for specific time periods."

Early Neolithic 
"Periodization: The Levant: 9500–8000 BC; Europe: 5000–4000 BC; Elsewhere: varies greatly, depending on region."

Middle Neolithic
"Periodization: The Levant: 8000–6000 BC; Europe: 4000–3500 BC; Elsewhere: varies greatly, depending on region."

Later Neolithic 
"Periodization: 6500–4500 BC; Europe: 3500–3000 BC; Elsewhere: varies greatly, depending on region."


"Periodization: Near East: 4500–3300 BC; Europe: 3000–1700 BC; Elsewhere: varies greatly, depending on region. In the Americas, the Eneolithic ended as late as the 19th century AD for some peoples."



</doc>
<doc id="21190" url="https://en.wikipedia.org/wiki?curid=21190" title="Nomic">
Nomic

Nomic is a game created in 1982 by philosopher Peter Suber in which the of the game include mechanisms for the players to change those rules, usually beginning through a system of democratic voting. 

The initial was designed by Peter Suber, and first published in Douglas Hofstadter's column "Metamagical Themas" in "Scientific American" in June 1982. The column discussed Suber's then-upcoming book, "The Paradox of Self-Amendment", which was published some years later. Nomic now refers to many games, all based on the initial ruleset.

The game is in some ways modeled on modern government systems. It demonstrates that in any system where rule changes are possible, a situation may arise in which the resulting laws are contradictory or insufficient to determine what is in fact legal. Because the game models (and exposes conceptual questions about) a legal system and the problems of legal interpretation, it is named after (""), Greek for "law".

While the victory condition in Suber's initial ruleset is the accumulation of 100 points by the roll of dice, he once said that "this rule is deliberately boring so that players will quickly amend it to please themselves". Players can change the rules to such a degree that points can become irrelevant in favor of a true currency, or make victory an unimportant concern. Any rule in the game, including the rules specifying the criteria for winning and even the rule that rules must be obeyed, can be changed. Any loophole in the ruleset, however, may allow the first player to discover it the chance to pull a "scam" and modify the rules to win the game. Complicating this process is the fact that Suber's initial ruleset allows for the appointment of judges to preside over issues of rule interpretation.

The game can be played face-to-face with as many written notes as are required, or through any of a number of Internet media (usually an archived mailing list or Internet forum).

Initially, gameplay occurs in clockwise order, with each player taking a turn. In that turn, they propose a change in rules that all the other players vote on, and then roll a die to determine the number of points they add to their score. If this rule change is passed, it comes into effect at the end of their round. Any rule can be changed with varying degrees of difficulty, including the core rules of the game itself. As such, the gameplay may quickly change.

Under Suber's initial ruleset, rules are divided into two types: mutable and immutable. The main difference between these is that immutable rules must be changed into mutable rules (called "transmuting") before they can be modified or removed. Immutable rules also take precedence over mutable ones. A rule change may be:


Alternative starting rulesets exist for Internet and mail games, wherein gameplay occurs in alphabetical order by surname, and points added to the score are based on the success of a proposed rule change rather than random dice rolls.

Not only can every aspect of the rules be altered in some way over the course of a game of Nomic, but myriad variants also exist: some that have themes, begin with a single rule, or begin with a dictator instead of a democratic process to validate rules. Others combine Nomic with an existing game (such as Monopoly, chess, or in one humorously paradoxical attempt, Mornington Crescent). There is even a version in which the players are games of Nomic themselves. Even more unusual variants include a ruleset in which the rules are hidden from players' view, and a game which, instead of allowing voting on rules, splits into two sub-games, one with the rule, and one without it.

Online versions often have initial rulesets where play is not turn-based; typically, players in such games may propose rule changes at any time, rather than having to wait for their turn.

One spin-off of a now-defunct Nomic (Nomic World) is called the Fantasy Rules Committee; it adds every legal rule submitted by a player to the ruleset until the players run out of ideas, after which all the "fantasy rules" are repealed and the game begins again.

The game of Nomic is particularly suited to being played online, where all proposals and rules can be shared in web pages or email archives for ease of reference. Such games of Nomic sometimes last for a very long time – Agora has been running since 1993. The longevity of nomic games can pose a serious problem, in that the rulesets can grow so complex that current players do not fully understand them and prospective players are deterred from joining. One currently active game, BlogNomic, gets around this problem by dividing the game into "dynasties"; every time someone wins, a new dynasty begins, and all the rules except a privileged few are repealed. This keeps the game relatively simple and accessible. Nomicron (now defunct) was similar in that it had rounds – when a player won a round, a convention was started to plan for the next round. A game of Nomic on reddit, (now defunct), used a similar mechanism modeled on Nomicron's system.

Another facet of Nomic is the way in which the implementation of the rules affects the way the game of Nomic itself works. ThermodyNomic, for example, had a ruleset in which rule changes were carefully considered before implementation, and rules were rarely introduced which provide loopholes for the players to exploit. B Nomic, by contrast, was once described by one of its players as "the equivalent of throwing logical hand grenades".

This is essentially part of the differentiation between "procedural" games, where the aim (acknowledged or otherwise) is to tie the entire ruleset into a paradoxical condition during each turn (a player who has no legal move available wins), and "substantive" games, which try to avoid paradox and reward winning by achieving certain goals, such as attaining a given number of points.

While "Nomic" is traditionally capitalized as the proper name of the game it describes, it has also sometimes been used in a more informal way as a lowercased generic term, "nomic", referring to anything with Nomic-like characteristics, including games where the rules may be changed during play as well as non-gaming situations where it can be alleged that "rules lawyers" are tinkering with the process used to amend rules and policies (in an organization or community) in a manner akin to a game of Nomic.

In a computerized Nomic, the rules are interpreted by a computer, rather than by humans. This implies that the rules should be written in a language that a computer can understand, typically some sort of programming language or Game Description Language. Nomyx is such an implementation. The Nomyx website is defunct, but the code is still on GitHub.




</doc>
<doc id="21197" url="https://en.wikipedia.org/wiki?curid=21197" title="Nintendo">
Nintendo

Since then, Nintendo has produced some of the most successful consoles in the video game industry, such as the Game Boy, the Super Nintendo Entertainment System, the Wii, and the Nintendo Switch. Nintendo has also released numerous influential franchises, including "Donkey Kong", "Mario", "The Legend of Zelda", "Kirby", "Metroid", "Fire Emblem", "Splatoon", "Super Smash Bros.", and "Pokémon".

Nintendo has multiple subsidiaries in Japan and abroad, in addition to business partners such as The Pokémon Company and HAL Laboratory. Both the company and its staff have received numerous awards for their achievements, including Emmy Awards for Technology & Engineering, Game Developers Choice Awards and British Academy Games Awards among others. Nintendo is one of the wealthiest and most valuable companies in the Japanese market.

Nintendo was founded as on 23 September 1889 by craftsman Fusajiro Yamauchi in Shimogyō-ku, Kyoto, Japan, to produce and distribute . The word "Nintendo" is commonly assumed to mean 'leave luck to heaven', but there are no historical records to validate this assumption. It can alternatively be translated as 'the temple of free "hanafuda"<nowiki>'</nowiki>. Yamauchi manufactured these playing cards using white mulberry bark, which he painted by hand. He was able to market them despite the fact that gambling had been prohibited by Japanese authorities since 1633, because the cards incorporated illustrations rather than numbers.

With the increase of the cards' popularity, Yamauchi hired assistants to mass-produce in order to satisfy demand. Despite a favorable start, however, the company faced financial difficulties due to the slow and expensive manufacturing process, high product price, coupled with operating in a niche market, as well as the long durability of the cards, which impacted sales due to the low replacement rate. As a solution, Nintendo produced a cheaper and lower-quality line of playing cards, "Tengu", while also seeking to offer his products in other cities such as Osaka, where considerable profits were found in card games. In addition, local merchants were interested in the prospect of a continuous renewal of decks, thus avoiding the suspicions that reusing cards would generate.

According to data from the company itself, Nintendo's first western-style deck was put on the market in 1902, although other documents postpone the date to 1907, shortly after the Russo-Japanese War. The war created considerable difficulties for companies in the leisure sector, which were subject to new levies such as the "Karuta Zei" ('playing cards tax'). Despite this, Nintendo subsisted and, in 1907, entered into an agreement with Nihon Senbai—later known as the Japan Tobacco—to market its cards to various cigarette stores throughout the country. A promotional calendar distributed by Nintendo from the Taishō era dated to 1915 was found, indicating that the company was named Yamauchi Nintendo and used the Marufuku Nintendo Co. brand for its playing cards.

Japanese culture stipulated that for Nintendo Koppai to continue as a family business after Yamauchi's retirement, Yamauchi had to adopt his son-in-law so that he may take over the business. As result, Sekiryo Kaneda adopted the Yamauchi surname in 1907 and became the second president of Nintendo Koppai in 1929. By that time, Nintendo Koppai was the largest card game company in Japan.

In 1933, Sekiryo Kaneda established the company as a general partnership titled Yamauchi Nintendo & Co. Ltd., investing in the construction of a new corporate headquarters located next to the original building, near the Toba-kaidō train station. Because Sekiryo's marriage to Yamauchi's daughter produced no male heirs, he planned to adopt his son-in-law Shikanojo Inaba, an artist in the company's employ and the father of his grandson Hiroshi, born in 1927. However, Inaba abandoned his family and the company, so Hiroshi was made Sekiryo's eventual successor.

World War II negatively impacted the company as Japanese authorities prohibited the diffusion of foreign card games, and as the priorities of Japanese society shifted, its interest in recreational activities waned. During this time, Nintendo was partly supported by a financial injection from Hiroshi's wife Michiko Inaba, who came from a wealthy family. In 1947, Sekiryo founded the distribution company Marufuku Co. Ltd.

In 1950, due to Sekiryo's deteriorating health, Hiroshi assumed the presidency of Nintendo. His first actions involved several important changes in the operation of the company: in 1951, he changed the company name to Nintendo Playing Card Co. Ltd., while the Marufuku Company adopted the name Nintendo Karuta Co. Ltd. In 1952, he centralized the production of cards in the Kyoto factories, which led to the expansion of the offices. The company's new line of plastic cards enjoyed considerable success in Japan. Some of the company's employees, accustomed to a more cautious and conservative leadership, viewed the new measures with concern, and the rising tension led to a call for a strike. However, the measure had no major impact, as Hiroshi resorted to the dismissal of several dissatisfied workers.

In 1959, Nintendo entered into an agreement with Walt Disney to incorporate his company's animated characters into the cards. Nintendo also developed a distribution system that allowed it to offer its products in toy stores. By 1961, the company had sold more than 1.5 million card packs and held a high market share, for which it relied on televised advertising campaigns. The need for diversification led the company to list stock on the second section of the Osaka and Kyoto stock exchanges, in addition to becoming a public company and changing its name to Nintendo Co., Ltd. in 1963. In 1964, Nintendo earned an income of ¥150 million.

Although the company was experiencing a period of economic prosperity, the Disney cards and derived products made it dependent on the children's market. The situation was exacerbated by the falling sales of its adult-oriented "hanafuda" cards caused by Japanese society gravitating toward other hobbies such as pachinko, bowling and nightly outings. When Disney card sales began to show signs of exhaustion, Nintendo realized that it had no real alternative with which to alleviate the situation. After the 1964 Tokyo Olympics, Nintendo's stock price plummeted to its lowest recorded level of ¥60.

Between 1963 and 1968, Yamauchi invested in several business lines for Nintendo that were far from its traditional market and, for the most part, were unsuccessful. Among these ventures were packages of instant rice, a chain of love hotels, and a taxi service named "Daiya". Although the taxi service was better received than the previous efforts, Yamauchi rejected this initiative after a series of disagreements with local unions.

Yamauchi's experience with the previous initiatives led him to increase Nintendo's investment in a research and development department directed by Hiroshi Imanishi, an employee with a long history in other areas of the company. In 1969, Gunpei Yokoi joined the department and was responsible for coordinating various projects. Yokoi's experience in manufacturing electronic devices led Yamauchi to put him in charge of the company's games department, and his products would be mass-produced. During this period, Nintendo built a new production plant in Uji City, just outside of Kyoto, and distributed classic tabletop games such as chess, shogi, go, and mahjong, as well as other foreign games under the Nippon Game brand. The company's restructuring preserved a couple of areas dedicated to "hanafuda" card manufacturing.

The early 1970s represented a watershed moment in Nintendo's history as it released Japan's first electronic toy—the Nintendo Beam Gun, an optoelectronic pistol designed by Masayuki Uemura. In total, more than a million units were sold. During that period, Nintendo began trading on the main section of the Osaka stock exchange and opened a new headquarters. Other popular toys released at the time include the Ultra Hand, the Ultra Machine, the Ultra Scope, and the Love Tester, all designed by Yokoi. The Ultra Hand sold more than 1.2 million units in Japan.

The growing demand for Nintendo's products led Yamauchi to further expand the offices, for which he acquired the surrounding land and assigned the production of cards to the original Nintendo building. Meanwhile, Yokoi, Uemura, and new employees such as Genyo Takeda, continued to develop innovative products for the company. The Laser Clay Shooting System was released in 1973 and managed to surpass bowling in popularity. In 1974, Nintendo released "Wild Gunman", a skeet shooting simulator consisting of a 16 mm image projector with a sensor that detects a beam from the player's light gun. Both the Laser Clay Shooting System and "Wild Gunman" were successfully exported to Europe and North America. Despite this, Nintendo's production speeds were still slow compared to rival companies such as Bandai and Tomy, and their prices were high, which led to the discontinuation of some of their light gun products. The subsidiary Nintendo Leisure System Co., Ltd., which developed these products, was closed as a result of the economic impact dealt by the 1973 oil crisis.
Yamauchi, motivated by the successes of Atari and Magnavox with their video game consoles, acquired the Japanese distribution rights for the Magnavox Odyssey in 1974, and reached an agreement with Mitsubishi Electric to develop similar products between 1975 and 1978, including the first microprocessor for video games systems, the Color TV-Game series, and an arcade game inspired by Othello. During this period, Takeda developed the video game "EVR Race", and Shigeru Miyamoto joined Yokoi's team with the responsibility of designing the casing for the Color TV-Game consoles. In 1978, Nintendo's research and development department was split into two facilities, Nintendo Research & Development 1 and Nintendo Research & Development 2, respectively managed by Yokoi and Uemura.

Two key events in Nintendo's history occurred in 1979: its American subsidiary was opened in New York City, and a new department focused on arcade game development was created. In 1980, the first handheld video game system, the Game & Watch, was created by Yokoi from the technology used in portable calculators. It became one of Nintendo's most successful products, with over 43.4 million units sold worldwide during its production period, and for which 59 games were made in total.

Nintendo's success in arcade games grew in 1981 with the release of "Donkey Kong", which was developed by Miyamoto and one of the first video games that allowed the player character to jump. The character, Jumpman, would later become Mario and Nintendo's official mascot. Mario was named after Mario Segale, the landlord of Nintendo's offices in Tukwila, Washington.

In 1983, Nintendo opened a new production facility in Uji and was listed on the first section of the Tokyo Stock Exchange. Uemura, taking inspiration from the ColecoVision, began creating a new video game console that would incorporate a ROM cartridge format for video games as well as both a central processing unit and a physics processing unit. The Family Computer, or Famicom, was released in Japan in July 1983 along with three games adapted from their original arcade versions: "Donkey Kong", "Donkey Kong Jr." and "Popeye". Its success was such that in 1984, it surpassed the market share held by Sega's SG-1000. At this time, Nintendo adopted a series of guidelines that involved the validation of each game produced for the Famicom before its distribution on the market, agreements with developers to ensure that no Famicom game would be adapted to other consoles within two years of its release, and restricting developers from producing more than five games a year for the Famicom.
In the early 1980s, several video game consoles proliferated in the United States, as well as low-quality games produced by third-party developers, which oversaturated the market and led to the video game crash of 1983. Consequently, a recession hit the American video game industry, whose revenues went from over $3 billion to $100 million between 1983 and 1985. Nintendo's initiative to launch the Famicom in America was also impacted. To differentiate the Famicom from its competitors in America, Nintendo opted to redesign the Famicom as an "entertainment system" compatible with "Game Paks", a euphemism for cartridges, and with a design reminiscent of a VCR. Nintendo implemented a lockout chip in the Game Paks that gave it control on what games were published for the console to avoid the market saturation that occurred in the United States' market. The resulting product was the Nintendo Entertainment System, or NES, which was released in North America in 1985. The landmark titles "Super Mario Bros." and "The Legend of Zelda" were produced for the console by Miyamoto and Takashi Tezuka. The work of composer Koji Kondo for both games reinforced the idea that musical themes could act as a compliment to game mechanics rather than simply a miscellaneous element. Production of the NES lasted until 1995, and production of the Famicom lasted until 2003. In total, around 62 million Famicom and NES consoles were sold worldwide. During this period, Nintendo created a measure against piracy of its video games in the form of the Official Nintendo Seal of Quality, a seal that was added to their products so that customers may recognize their authenticity in the market. By this time, Nintendo's network of electronic suppliers had extended to around thirty companies, among which were Ricoh—Nintendo's main source for semiconductors—and the Sharp Corporation.

In 1988, Gunpei Yokoi and his team at Nintendo R&D1 conceived the Game Boy, the first handheld video game console to be compatible with interchangeable game cartridges. Nintendo released the Game Boy in 1989. In North America, the Game Boy was bundled with the popular third-party game "Tetris" after a difficult negotiation process with Elektronorgtechnica. The Game Boy was a significant success: in its first two weeks of sale in Japan, it sold out its initial inventory of 300,000 units, while in the United States, an additional 40,000 units were sold on its first day of distribution. Around this time, Nintendo entered into an agreement with Sony to develop the Super Famicom CD-ROM Adapter, a peripheral for the upcoming Super Famicom capable of playing CD-ROMs. However, the collaboration did not last as Yamauchi preferred to continue developing the technology with Philips, which would result in the CD-i, and Sony's independent efforts resulted in the creation of the PlayStation console.

The first issue of the magazine "Nintendo Power", which had an annual circulation of 1.5 million copies in the United States, was published in 1988. In July 1989, Nintendo held the first Nintendo Space World trade show under the name "Shoshinkai" for the purpose of announcing and demonstrating upcoming Nintendo products. The same year, the first World of Nintendo stores-within-a-store, which carried official Nintendo merchandise, were opened in the United States. According to company information, more than 25% of homes in the United States had an NES in 1989.

The late 1980s marked the slip of Nintendo's dominance in the video game market with the appearance of NEC's PC Engine and Sega's Mega Drive, game systems designed with a 16-bit architecture that allowed for improved graphics and audio compared to the NES. In response to the competition, Uemura designed the Super Famicom, which launched in 1990. The first batch of 300,000 consoles sold out in a matter of hours. The following year, as with the NES, Nintendo distributed a modified version of the Super Famicom to the United States market, titled the Super Nintendo Entertainment System (SNES). Launch games for the Super Famicom and SNES include "Super Mario World", "F-Zero", "Pilotwings", "SimCity", and "Gradius III". By mid-1992, over 46 million Super Famicom and SNES consoles were sold. The console's life cycle lasted until 1999 in the United States, and until 2003 in Japan.

In March 1990, the first Nintendo World Championship was held, with participants from 29 American cities competing for the title of "best Nintendo player in the world". In June 1990, the subsidiary Nintendo of Europe was opened in Großostheim, Germany; in 1993, subsequent subsidiaries were established in the Netherlands (where Bandai had previously distributed Nintendo's products), France, the United Kingdom, Spain, Belgium and Australia. In 1992, Nintendo acquired a majority stake in the Seattle Mariners baseball team, and sold its shares in 2016. Nintendo ceased manufacturing arcade games and systems in September 1992. In 1993, "Star Fox" was released, which marked an industry milestone by being the first video game to make use of the Super FX chip.

The proliferation of graphically violent video games, such as "Mortal Kombat", caused controversy and led to the creation of the Interactive Digital Software Association and the Entertainment Software Rating Board, in whose development Nintendo collaborated during 1994. These measures also encouraged Nintendo to abandon the content guildelines it had enforced since the release of the NES. Commercial strategies implemented by Nintendo during this time include the Nintendo Gateway System, an in-flight entertainment service available for airlines, cruise ships and hotels, and the "Play It Loud!" advertising campaign for Game Boys with different-colored casings. The Advanced Computer Modelling graphics used in "Donkey Kong Country" for the SNES and "Donkey Kong Land" for the Game Boy were a technological innovation, as was the Satellaview satellite modem peripheral for the Super Famicom, which allowed the digital transmission of data by means of a communications satellite.

In mid-1993, Nintendo and Silicon Graphics announced a strategic alliance to develop the Nintendo 64. NEC, Toshiba and Sharp also contributed technology to the console. The Nintendo 64 was marketed as one of the first consoles to be designed with 64-bit architecture. As part of an agreement with Midway Games, the arcade games "Killer Instinct" and "Cruis'n USA" were ported to the console. Although the Nintendo 64 was planned for release in 1995, the production schedules of third-party developers influenced a delay, and the console was released in June and September 1996 in Japan and the United States respectively, and in March 1997 in Europe. By the end of its production in 2002, around 33 million Nintendo 64 consoles were sold worldwide, and it is considered one of the most recognized video game systems in history. 388 games were produced for the Nintendo 64 in total, some of which – particularly "Super Mario 64", "" and "GoldenEye 007" – have been distinguished as some of the greatest of all time.

In 1995, Nintendo released the Virtual Boy, a console designed by Gunpei Yokoi with virtual reality technology and stereoscopic graphics. Critics were generally disappointed with the quality of the games and red-colored graphics, and complained of gameplay-induced headaches. The system sold poorly and was quietly discontinued. Amid the system's failure, Yokoi formally retired from Nintendo. In February 1996, "Pocket Monsters Red" and "Green", known internationally as "Pokémon Red" and "Blue", developed by Game Freak was released in Japan for the Game Boy, and established the popular "Pokémon" franchise. The game went on to sell 31.37 million units, with the video game series exceeding a total of 300 million units in sales as of 2017. In 1997, Nintendo released the Rumble Pak, a plug-in device that connects to the Nintendo 64 controller and produces a vibration during certain moments of a game.

In 1998, the Game Boy Color was released. In addition to allowing backward compatibility with Game Boy games, the console's similar capacity to the NES resulted in select adaptations of games from that library, such as "Super Mario Bros. Deluxe". Since then, over 118.6 million Game Boy and Game Boy Color consoles have been sold worldwide.

In May 1999, with the advent of the PlayStation 2, Nintendo entered an agreement with IBM and Panasonic to develop the 128-bit Gekko processor and the DVD drive to be used in Nintendo's next home console. Meanwhile, a series of administrative changes occurred in 2000, when Nintendo's corporate offices were moved to the Minami-ku neighborhood in Kyoto, and Nintendo Benelux was established to manage the Dutch and Belgian territories.

The year 2001 marked the introduction of two new Nintendo consoles: the Game Boy Advance, which was designed by Gwénaël Nicolas and stylistically departed from its predecessors, and the GameCube. During the first week of the Game Boy Advance's North American release in June 2001, over 500,000 units were sold, making it the fastest-selling video game console in the United States at the time. By the end of its production cycle in 2010, more than 81.5 million units had been sold worldwide. As for the GameCube, despite such distinguishing features as the miniDVD format of its games and internet connectivity for a limited number of games, its sales were lower than those of its predecessors, and during the six years of its production, 21.7 million units were sold worldwide. An innovative product developed by Nintendo during this time was the Nintendo e-Reader, a Game Boy Advance peripheral that allows the transfer of data stored on a series of cards to the console.

In 2002, the Pokémon Mini was released. Its dimensions were smaller than that of the Game Boy Advance and it weighed 70 grams, making it the smallest video game console in history. Nintendo collaborated with Sega and Namco to develop Triforce, an arcade board to facilitate the conversion of arcade titles to the GameCube. Following the European release of the GameCube in May 2002, Hiroshi Yamauchi announced his resignation as the president of Nintendo, and Satoru Iwata was selected by the company as his successor. Yamauchi would remain as advisor and director of the company until 2005, and he died in 2013. Iwata's appointment as president ended the Yamauchi succession at the helm of the company, a practice that had been in place since its foundation.

In 2003, Nintendo released the Game Boy Advance SP, an improved version of the Game Boy Advance that incorporated a folding design, an illuminated display and a rechargeable battery. By the end of its production cycle in 2010, over 43.5 million units had been sold worldwide. Nintendo also released the Game Boy Player, a peripheral that allows Game Boy and Game Boy Advance games to be played on the GameCube.

In 2004, the last remnants of Nintendo's original headquarters was reportedly demolished. Later that year, Nintendo released the Nintendo DS, which featured such innovations as dual screens – one of which being a touchscreen – and wireless connectivity for multiplayer play. Throughout its lifetime, more than 154 million units were sold, making it the most successful handheld console and the second best-selling console in history. In 2005, Nintendo released the Game Boy Micro, the last system in the Game Boy line. Sales did not meet Nintendo's expectations, with 2.5 million units being sold by 2007. In mid-2005, the Nintendo World Store was inaugurated in New York City.

Nintendo's next home console was conceived in 2001, although the designing commenced in 2003, taking inspiration from the Nintendo DS. The Wii was released in November 2006, with a total of 33 launch titles. With the Wii, Nintendo sought to reach a broader demographic than its seventh generation competitors, with the intention of also encompassing the "non-consumer" sector. To this end, Nintendo invested in a $200 million advertising campaign. The Wii's innovations include the Wii Remote controller, equipped with an accelerometer system and infrared sensors that allow it to detect its position in a three-dimensional environment with the aid of a sensor bar; the Nunchuk peripheral that includes an analog controller as well as an accelerometer; and the Wii MotionPlus expansion that increases the sensitivity of the main controller with the aid of gyroscopes. By 2016, more than 101 million Wii consoles had been sold worldwide, making it the most successful console of its generation, a distinction that Nintendo had not achieved since the 1990s with the SNES.

A number of accessories were released for the Wii from 2007 to 2010, such as the Wii Balance Board, the Wii Wheel and the WiiWare download service. In 2009, Nintendo Iberica S.A. expanded its commercial operations to Portugal through a new office in Lisbon. By that year, Nintendo held a 68.3% share of the worldwide handheld gaming market. In 2010, Nintendo celebrated the 25th anniversary of Mario's debut appearance, for which certain allusive products were put on sale. The event included the release of "Super Mario All-Stars 25th Anniversary Edition" and special editions of the Nintendo DSi XL and Wii.

 
Following an announcement in March 2010, Nintendo released the Nintendo 3DS in 2011. The console is capable of producing stereoscopic effects without the need for 3D glasses. By 2018, more than 69 million units had been sold worldwide; the figure increased to 75 million by the start of 2019. In 2011, Nintendo celebrated the 25th anniversary of "The Legend of Zelda" with the orchestra concert tour and the video game "".

The years 2012 and 2013 marked the introduction of two new Nintendo game consoles: the Wii U, which incorporated high-definition graphics and a GamePad controller with near-field communication technology, and the Nintendo 2DS, a version of the 3DS that lacks the clamshell-like design of Nintendo's previous handheld consoles and the stereoscopic effects of the 3DS. With 13.5 million units sold worldwide, the Wii U is the least successful video game console in Nintendo's history. In 2014, a new line of products was released consisting of figures of Nintendo characters called amiibos.

On 25 September 2013, Nintendo announced its acquisition of a 28% stake in PUX Corporation, a subsidiary of Panasonic, for the purpose of developing facial, voice and text recognition for its video games. Due to a 30% decrease in company income between April and December 2013, Iwata announced a temporary 50% cut to his salary, with other executives seeing reductions by 20%–30%. In January 2015, Nintendo ceased operations in the Brazilian market due in part to high import duties. Although this did not affect the rest of Nintendo's Latin American market due to an alliance with Juegos de Video Latinoamérica, in 2017, Nintendo reached an agreement with NC Games for Nintendo's products to resume distribution in Brazil.

On 11 July 2015, Iwata died of bile duct cancer, and after a couple of months in which Miyamoto and Takeda jointly operated the company, Tatsumi Kimishima was named as Iwata's successor on 16 September 2015. As part of the management's restructuring, Miyamoto and Takeda were respectively named creative and technological advisors.
The financial losses caused by the Wii U, along with Sony's intention to release its video games to other platforms such as smart TVs, motivated Nintendo to rethink its strategy concerning the production and distribution of its properties. In 2015, Nintendo formalized agreements with DeNA and Universal Parks & Resorts to extend its presence to smart devices and amusement parks respectively. In March 2016, Nintendo's first mobile app for the iOS and Android systems, "Miitomo", was released. Since then, Nintendo has produced other similar apps, such as "Super Mario Run", "Fire Emblem Heroes", "", "Mario Kart Tour" and "Pokémon Go", the last being developed by Niantic and having generated $115 million in revenue for Nintendo. The theme park area Super Nintendo World is set to open at Universal Studios Japan in 2020. In March 2016, the loyalty program My Nintendo replaced Club Nintendo.

The NES Classic Edition was released in November 2016. The console is a redesigned version of the NES that includes support for the HDMI interface and Wiimote compatibility. Its successor, the Super NES Classic Edition, was released in September 2017. By October 2018, around ten million units of both consoles combined had been sold worldwide.

The Wii U's successor in the eighth generation of video game consoles, the Switch, was released in March 2017. The Switch features a hybrid design as a home and handheld console, independently functioning Joy-Con controllers that each contain an accelerometer and gyroscope, and the simultaneous wireless connection of up to eight consoles. To expand its library, Nintendo entered alliances with several third-party and independent developers; by February 2019, more than 1,800 games had been released for the Switch. Worldwide sales of the Switch exceeded 55 million units by March 2020. In April 2018, the Nintendo Labo line was released, consisting of cardboard accessories that interact with the Switch and the Joy-Con controllers. More than one million units of the Nintendo Labo Variety Ki were sold in its first year on the market.

In 2018, Shuntaro Furukawa replaced Kimishima as company president, and in 2019, Doug Bowser succeeded Nintendo of America president Reggie Fils-Aimé. In April 2019, Nintendo formed an alliance with Tencent to distribute the Nintendo Switch in China starting in December. In April 2020, ValueAct Capital Partners announced an acquisition of $1.1 billion in Nintendo stock purchases, giving them an overall stake of 2% in Nintendo. On 6 January 2020, hotel and restaurant development company Plan See Do announced that the it would refurbish the former headquarters of Marufuku Nintendo Card Co. as an hotel, with plans to add 20 guest rooms, a restaurant, bar, and gym, with a planned opening date of mid 2021. The building belongs to Yamauchi Co., Ltd., an asset management company of Nintendo's founding family. It was further reported that the original 19th-century headquarters was apparently demolished and turned into a parking lot. Although the COVID-19 pandemic caused delays in the production and distribution of some of Nintendo's products, the situation "had limited impact on business results". in May 2020, Nintendo reported a 75% increase in income compared to the previous fiscal year, mainly contributed by the Nintendo Switch Online service.

Nintendo's central focus is the research, development, production and distribution of entertainment productsprimarily video game software and hardware and card games. Its main markets are Japan, America, and Europe, and more than 70% of its total sales come from the latter two territories.

Since the launch of the Color TV-Game in 1977, Nintendo has produced and distributed home, handheld, dedicated and hybrid consoles. Each has a variety of accessories and controllers, such as the NES Zapper, the Game Boy Camera, the Super NES Mouse, the Rumble Pak, the Wii MotionPlus, the Wii U Pro Controller, and the Switch Pro Controller.

Nintendo's first electronic games are arcade games. "EVR Race" (1975) is the company's first electromechanical game, and "Donkey Kong" (1981) is the first platform game in history. Since then, both Nintendo and other development companies have produced and distributed an extensive catalogue of video games for Nintendo's consoles. Nintendo's games are sold in both physical and digital formats; the latter are distributed via services such as the Nintendo eShop and the Nintendo Network.

Nintendo of America has engaged in several high-profile marketing campaigns to define and position its brand. One of its earliest and most enduring slogans was "Now you're playing with power!", used first to promote its Nintendo Entertainment System. It modified the slogan to include "SUPER power" for the Super Nintendo Entertainment System, and "PORTABLE power" for the Game Boy.

Its 1994 "Play It Loud!" campaign played upon teenage rebellion and fostered an edgy reputation. During the Nintendo 64 era, the slogan was "Get N or get out." During the GameCube era, the "Who Are You?" suggested a link between the games and the players' identities. The company promoted its Nintendo DS handheld with the tagline "Touching is Good." For the Wii, they used the "Wii would like to play" slogan to promote the console with the people who tried the games including "Super Mario Galaxy" and "Super Paper Mario". The Nintendo 3DS used the slogan "Take a look inside." The Wii U used the slogan "How U will play next." The Nintendo Switch uses the slogan "Switch and Play" in North America, and "Play anywhere, anytime, with anyone" in Europe.

During the peak of Nintendo's success in the video game industry in the 1990s, their name was ubiquitously used to refer to any video game console, regardless of the manufacturer. To prevent their trademark from becoming generic, Nintendo pushed usage of the term "game console", and succeeded in preserving their trademark.

Used since the 1960s, Nintendo's most recognizable logo is the racetrack shape, especially the red-colored wordmark typically (though not always) displayed on a white background, primarily used in the Western markets from 1985 to 2006. In Japan, a monochromatic version that lacks a colored background is on Nintendo's own Famicom, Super Famicom, Nintendo 64, GameCube, and handheld console packaging and marketing. Since 2006, in conjunction with the launch of the Wii, Nintendo changed its logo to a gray variant that lacks a colored background inside the wordmark, making it transparent. Nintendo's official, corporate logo remains this variation. For consumer products and marketing, a white variant on a red background has been used since 2015, and has been in full effect since the launch of the Nintendo Switch in 2017.




Nintendo's internal research and development operations are divided into three main divisions:


The Nintendo Entertainment Planning & Development division is the primary software development division at Nintendo, formed as a merger between their former Entertainment Analysis & Development and Software Planning & Development divisions in 2015. Led by Shinya Takahashi, the division holds the largest concentration of staff at the company, housing more than 800 engineers, producers, directors, planners and designers.

The Nintendo Platform Technology Development division is a combination of Nintendo's former Integrated Research & Development (or IRD) and System Development (or SDD) divisions. Led by Ko Shiota, the division is responsible for designing hardware and developing Nintendo's operating systems, developer environment and internal network as well as maintenance of the Nintendo Network.

The Nintendo Business Development division was formed following Nintendo's foray into software development for smart devices such as mobile phones and tablets. They are responsible for refining Nintendo's business model for the dedicated video game system business, and for furthering Nintendo's venture into development for smart devices.

Although most of the research and development is being done in Japan, there are some R&D facilities in the United States, Europe and China that are focused on developing software and hardware technologies used in Nintendo products. Although they all are subsidiaries of Nintendo (and therefore first-party), they are often referred to as external resources when being involved in joint development processes with Nintendo's internal developers by the Japanese personal involved. This can be seen in the "Iwata asks..." interview series. Nintendo Software Technology (NST) and Nintendo Technology Development (NTD) are located in Redmond, Washington, United States, while Nintendo European Research & Development ("NERD") is located in Paris, France, and Nintendo Network Service Database (NSD) is located in Kyoto, Japan.

Most external first-party software development is done in Japan, since the only overseas subsidiary is Retro Studios in the United States. Although these studios are all subsidiaries of Nintendo, they are often referred to as external resources when being involved in joint development processes with Nintendo's internal developers by the Nintendo Entertainment Planning & Development (EPD) division. 1-Up Studio and Nd Cube are located in Tokyo, Japan, while Monolith Soft has one studio located in Tokyo and another in Kyoto. Retro Studios is located in Austin, Texas.

Nintendo also established The Pokémon Company alongside Creatures and Game Freak in order to effectively manage the Pokémon brand. Similarly, Warpstar Inc. was formed through a joint investment with HAL Laboratory, which was in charge of the "" animated series. Both companies are investments from Nintendo, with Nintendo holding 32% of the shares of The Pokémon Company and 50% of the shares of Warpstar Inc.

In total there's 27 subsidiaries reported by the company with 20 being known as of Nintendo's Annual Report in 2020:


Bergsala, a third-party company based in Sweden, exclusively handles Nintendo operations in the Scandinavian region. Bergsala's relationship with Nintendo was established in 1981 when the company sought to distribute "Game & Watch" units to Sweden, which later expanded to the NES console by 1986. Bergsala were the only non-Nintendo owned distributor of Nintendo's products, up until 2019 when Tor Gaming gained distribution rights in Israel.

Nintendo has partnered with Tencent to release Nintendo products in China, following the lifting of the country's console ban in 2015. In addition to distributing hardware, Tencent will help bring Nintendo's games through the governmental approval process for video game software.

In January 2019, it was reported by ynet and IGN Israel that negotiations about official distribution of Nintendo products in the country were ongoing. After two months, IGN Israel announced that Tor Gaming Ltd., a company that established in earlier 2019, gained a distribution agreement with Nintendo of Europe, handling official retailing beginning at the start of March, followed by opening an official online store the next month. In June 2019, Tor Gaming launched an official Nintendo Store at Dizengoff Center in Tel Aviv, making it the second official Nintendo Store worldwide, 13 years after NYC.

Headquartered in Kyoto, Japan since the beginning, Nintendo Co., Ltd. oversees the organization's global operations and manages Japanese operations specifically. The company's two major subsidiaries, Nintendo of America and Nintendo of Europe, manage operations in North America and Europe respectively. Nintendo Co., Ltd. moved from its original Kyoto location to a new office in Higashiyama-ku, Kyoto, in 2000, this became the research and development building when the head office relocated to its location in Minami-ku, Kyoto.

Nintendo founded its North American subsidiary in 1980 as Nintendo of America (NoA). Hiroshi Yamauchi appointed his son-in-law Minoru Arakawa as president, who in turn hired his own wife and Yamauchi's daughter Yoko Yamauchi as the first employee. The Arakawa family moved from Vancouver to select an office in Manhattan, New York, due to its central status in American commerce. Both from extremely affluent families, their goals were set more by achievement than moneyand all their seed capital and products would now also be automatically inherited from Nintendo in Japan, and their inaugural target is the existing $8 billion-per-year coin-op arcade video game market and largest entertainment industry in the US, which already outclassed movies and television combined. During the couple's arcade research excursions, NoA hired gamer youths to work in the filthy, hot, ratty warehouse in New Jersey for the receiving and service of game hardware from Japan.

In late 1980 NoA contracted the Seattle-based arcade sales and distribution company Far East Video, consisting solely of experienced arcade salespeople Ron Judy and Al Stone. The two had already built a decent reputation and a distribution network, founded specifically for the independent import and sales of games from Nintendo because the Japanese company had for years been the under-represented maverick in America. Now as direct associates to the new NoA, they told Arakawa they could always clear all Nintendo inventory if Nintendo produced better games. Far East Video took NoA's contract for a fixed per-unit commission on the exclusive American distributorship of Nintendo games, to be settled by their Seattle-based lawyer, Howard Lincoln.

Based on favorable test arcade sites in Seattle, Arakawa wagered most of NoA's modest finances on a huge order of 3,000 "Radar Scope" cabinets. He panicked when the game failed in the fickle market upon its arrival from its four-month boat ride from Japan. Far East Video was already in financial trouble due to declining sales and Ron Judy borrowed his aunt's life savings of $50,000, while still hoping Nintendo would develop its first "Pac-Man"-sized hit. Arakawa regretted founding the Nintendo subsidiary, with the distressed Yoko trapped between her arguing husband and father.

Amid financial threat, Nintendo of America relocated from Manhattan to the Seattle metro to remove major stressors: the frenetic New York and New Jersey lifestyle and commute, and the extra weeks or months on the shipping route from Japan as was suffered by the "Radar Scope" disaster. With the Seattle harbor being the US's closest to Japan at only nine days by boat, and having a lumber production market for arcade cabinets, Arakawa's real estate scouts found a warehouse for rent containing three officesone for Arakawa and one for Judy and Stone. This warehouse in the Tukwila suburb was owned by Mario Segale after whom the Mario character would be named, and was initially managed by former Far East Video employee Don James. After one month, James recruited his college friend Howard Phillips as assistant, who soon took over as warehouse manager. The company remained at fewer than 10 employees for some time, handling sales, marketing, advertising, distribution, and limited manufacturing of arcade cabinets and "Game & Watch" handheld units, all sourced and shipped from Nintendo.

Arakawa was still panicked over NoA's ongoing financial crisis. With the parent company having no new game ideas, he had been repeatedly pleading for Yamauchi to reassign some top talent away from existing Japanese products to develop something for Americaespecially to redeem the massive dead stock of "Radar Scope" cabinets. Since all of Nintendo's key engineers and programmers were busy, and with NoA representing only a tiny fraction of the parent's overall business, Yamauchi allowed only the assignment of Gunpei Yokoi's young assistant who had no background in engineering, Shigeru Miyamoto.

NoA's staffexcept the sole young gamer Howard Phillipswere uniformly revolted at the sight of the freshman developer Miyamoto's debut game, which they had imported in the form of emergency conversion kits for the overstock of "Radar Scope" cabinets. The kits transformed the cabinets into NoA's massive windfall gain of from Miyamoto's smash hit "Donkey Kong" in 1981–1983 alone. They sold 4,000 new arcade units each month in America, making the 24-year-old Phillips "the largest volume shipping manager for the entire Port of Seattle". Arakawa used these profits to buy of land in Redmond in July 1982 and to perform the $50 million launch of the Nintendo Entertainment System in 1985 which revitalized the entire video game industry from its devastating 1983 crash. A second warehouse in Redmond was soon secured, and managed by Don James. The company stayed at around 20 employees for some years.

The organization was reshaped nationwide in the following decades, and those core sales and marketing business functions are now directed by the office in Redwood City, California. The company's distribution centers are Nintendo Atlanta in Atlanta, Georgia, and Nintendo North Bend in North Bend, Washington. , the Nintendo North Bend facility processes more than 20,000 orders a day to Nintendo customers, which include retail stores that sell Nintendo products in addition to consumers who shop Nintendo's website. Nintendo of America operates two retail stores in the United States: Nintendo New York on Rockefeller Plaza in New York City, which is open to the public; and Nintendo Redmond, co-located at NoA headquarters in Redmond, Washington, which is open only to Nintendo employees and invited guests. Nintendo of America's Canadian branch, Nintendo of Canada, is based in Vancouver, British Columbia with a distribution center in Toronto, Ontario. Nintendo Treehouse is NoA's localization team, composed of around 80 staff who are responsible for translating text from Japanese to English, creating videos and marketing plans, and quality assurance.

Nintendo's European subsidiary was established in June 1990, based in Großostheim, Germany. The company handles operations across Europe excluding Scandinavia, as well as South Africa. Nintendo of Europe's United Kingdom branch (Nintendo UK) handles operations in that country and in Ireland from its headquarters in Windsor, Berkshire. In June 2014, NOE initiated a reduction and consolidation process, yielding a combined 130 layoffs: the closing of its office and warehouse, and termination of all employment, in Großostheim; and the consolidation of all of those operations into, and terminating some employment at, its Frankfurt location. As of July 2018, the company employs 850 people. In 2019, NoE signed with Tor Gaming Ltd. for official distribution in Israel.

Nintendo's Australian subsidiary is based in Melbourne. It handles the publishing, distribution, sales, and marketing of Nintendo products in Australia, New Zealand, and Oceania (Cook Islands, Fiji, New Caledonia, Papua New Guinea, Samoa, and Vanuatu). It also manufactures some Wii games locally. Nintendo Australia is also a third-party distributor of some games from Rising Star Games, Bandai Namco Entertainment, Atlus, The Tetris Company, Sega, Koei Tecmo, and Capcom.

Nintendo's South Korean subsidiary was established on 7 July 2006, and is based in Seoul. In March 2016, the subsidiary was heavily downsized due to a corporate restructuring after analyzing shifts in the current market, laying off 80% of its employees, leaving only ten people, including CEO Hiroyuki Fukuda. This did not affect any games scheduled for release in South Korea, and Nintendo continued operations there as usual.

For many years, Nintendo had a policy of strict content guidelines for video games published on its consoles. Although Nintendo allowed graphic violence in its video games released in Japan, nudity and sexuality were strictly prohibited. Former Nintendo president Hiroshi Yamauchi believed that if the company allowed the licensing of pornographic games, the company's image would be forever tarnished. Nintendo of America went further in that games released for Nintendo consoles could not feature nudity, sexuality, profanity (including racism, sexism or slurs), blood, graphic or domestic violence, drugs, political messages or religious symbols (with the exception of widely unpracticed religions, such as the Greek Pantheon). The Japanese parent company was concerned that it may be viewed as a "Japanese Invasion" by forcing Japanese community standards on North American and European children. Past the strict guidelines, some exceptions have occurred: "Bionic Commando" (though swastikas were eliminated in the US version), "Smash TV" and "" contain human violence, the latter also containing implied sexuality and tobacco use; "River City Ransom" and "" contain nudity, and the latter also contains religious images, as do "" and "".

A known side effect of this policy is the Genesis version of "Mortal Kombat" having more than double the unit sales of the Super NES version, mainly because Nintendo had forced publisher Acclaim to recolor the red blood to look like white sweat and replace some of the more gory graphics in its release of the game, making it less violent. By contrast, Sega allowed blood and gore to remain in the Genesis version (though a code is required to unlock the gore). Nintendo allowed the Super NES version of "Mortal Kombat II" to ship uncensored the following year with a content warning on the packaging.

Video game ratings systems were introduced with the Entertainment Software Rating Board of 1994 and the Pan European Game Information of 2003, and Nintendo discontinued most of its censorship policies in favor of consumers making their own choices. Today, changes to the content of games are done primarily by the game's developer or, occasionally, at the request of Nintendo. The only clear-set rule is that ESRB AO-rated games will not be licensed on Nintendo consoles in North America, a practice which is also enforced by Sony and Microsoft, its two greatest competitors in the present market. Nintendo has since allowed several mature-content games to be published on its consoles, including these: "Perfect Dark", "Conker's Bad Fur Day", "Doom", "Doom 64", "BMX XXX", the "Resident Evil" series, "Killer7", the "Mortal Kombat" series, "", "BloodRayne", "Geist", "", "Bayonetta 2", "Devil's Third", and "". Certain games have continued to be modified, however. For example, Konami was forced to remove all references to cigarettes in the 2000 Game Boy Color game "Metal Gear Solid" (although the previous NES version of "Metal Gear" and the subsequent GameCube game "" both included such references, as did Wii game "MadWorld"), and maiming and blood were removed from the Nintendo 64 port of "Cruis'n USA". 

Another example is in the Game Boy Advance game "Mega Man Zero 3", in which one of the bosses, called Hellbat Schilt in the Japanese and European releases, was renamed Devilbat Schilt in the North American localization. In North America releases of the "Mega Man Zero" games, enemies and bosses killed with a saber attack do not gush blood as they do in the Japanese versions. However, the release of the Wii was accompanied by a number of even more controversial games, such as "Manhunt 2", "No More Heroes", "", and "MadWorld", the latter three of which were published exclusively for the console.

Nintendo of America also had guidelines before 1993 that had to be followed by its licensees to make games for the Nintendo Entertainment System, in addition to the above content guidelines. Guidelines were enforced through the 10NES lockout chip.

The last rule was circumvented in a number of ways; for example, Konami, wanting to produce more games for Nintendo's consoles, formed Ultra Games and later Palcom to produce more games as a technically different publisher. This disadvantaged smaller or emerging companies, as they could not afford to start additional companies. In another side effect, Square Co (now Square Enix) executives have suggested that the price of publishing games on the Nintendo 64 along with the degree of censorship and control that Nintendo enforced over its games, most notably "Final Fantasy VI", were factors in switching its focus towards Sony's PlayStation console.

In 1993, a class action suit was taken against Nintendo under allegations that their lockout chip enabled unfair business practices. The case was settled, with the condition that California consumers were entitled to a $3 discount coupon for a game of Nintendo's choice.

Nintendo has generally been proactive to assure its intellectual property in both hardware and software is protected. With the NES system, Nintendo employed a lock-out system that only allowed authorized game cartridges they manufactured to be playable on the system.

Nintendo has used emulation by itself or licensed from third parties to provide means to re-release games from their older platforms on newer systems, with Virtual Console, which re-released classic games as downloadable titles, the NES and SNES library for Nintendo Switch Online subscribers, and with dedicated consoles like the NES Mini and SNES Mini. However, Nintendo has taken a hard stance against unlicensed emulation of its video games and consoles, stating that it is the single largest threat to the intellectual property rights of video game developers. Further, Nintendo has taken action against fan-made games which have used significant facets of their IP, issuing cease & desist letters to these projects or Digital Millennium Copyright Act-related complaints to services that host these projects.

In recent years, Nintendo has taken legal action against sites that knowingly distribute ROM images of its games. On 19 July 2018, Nintendo sued Jacob Mathias, the owner of distribution websites LoveROMs and LoveRetro, for "brazen and mass-scale infringement of Nintendo's intellectual property rights". Nintendo settled with Mathias in November 2018 for more than along with relinquishing all ROM images in their ownership. While Nintendo is likely to have agreed to a smaller fine in private, the large amount was seen as a deterrent to prevent similar sites from sharing ROM images. Nintendo filed a separate suit against RomUniverse in September 2019 which also offered infringing copies of Nintendo DS and Switch games in addition to ROM images. Nintendo also successfully won a suit in the United Kingdom that same month to force the major Internet service providers in the country to block access to sites that offered copyright-infringing copies of Switch software or hacks for the Nintendo Switch to run unauthorized software. Ironically, individuals who hacked the Wii Virtual Console version of "Super Mario Bros." discovered that the ROM image Nintendo used had likely been downloaded from a ROM distribution site.

Nintendo sought enforcement action against a hacker that for several years had infiltrated Nintendo's internal database by various means including phishing to obtain plans for games and hardware for upcoming shows like E3. This was leaked to the Internet, impacting how Nintendo's own announcements were received. Though the person was a minor when Nintendo brought the United States Federal Bureau of Investigation (FBI) to investigate, and had been warned by the FBI to desist, the person continued over 2018 and 2019 as an adult, posting taunts on social media. The perpetrator was arrested in July 2019, and the FBI found documents confirming the hacks, many unauthorized game files, and child pornography, leading to the perpetrator's admission of guilt for all crimes in January 2020. Similarly, Nintendo alongside The Pokémon Company spent significant time to identify who had leaked information about "Pokémon Sword" and "Shield" several weeks before its planned Nintendo Directs, ultimately tracing the leaks back to a Portugal game journalist who leaked the information from official review copies of the game and subsequently severed ties with the publication.
In May 2020, a major leak of documents, including source code, designs, hardware drawings and documentation and other internal information primarily related to the Nintendo 64, GameCube, and Wii. The leak may have been related to BroadOn, a company that Nintendo had contracted to help with the Wii's design, but also may have been through Zammis Clark, a Malwarebytes employee and hacker who pleaded guilty to infiltrating Microsoft and Nintendo's servers between March and May 2018.

A second and larger leak occurred in July 2020, which has been called the "Gigaleak" as it contains gigabytes of data, and is believed related to the May 2020 leak. The leak includes the source code and prototypes for several early 1990s SNES games including "Super Mario Kart", "Yoshi's Island", "Star Fox", and "Star Fox 2", as well as internal development tools and system software components. The veracity of the material was confirmed by Dylan Cuthbert, a programmer for Nintendo during that period. The leak has the source to several Nintendo 64 games including "Super Mario 64" and "", and the console's operating system. The leak contains personal files from Nintendo employees, raising concerns about its origins and spread.

The gold sunburst seal was first used by Nintendo of America, and later Nintendo of Europe. It is displayed on any game, system, or accessory licensed for use on one of its video game consoles, denoting the game has been properly approved by Nintendo. The seal is also displayed on any Nintendo-licensed merchandise, such as trading cards, game guides, or apparel, albeit with the words "Official Nintendo Licensed Product."

In 2008, game designer Sid Meier cited the Seal of Quality as one of the three most important innovations in video game history, as it helped set a standard for game quality that protected consumers from shovelware.

In NTSC regions, this seal is an elliptical starburst named the "Official Nintendo Seal". Originally, for NTSC countries, the seal was a large, black and gold circular starburst. The seal read as follows: "This seal is your assurance that NINTENDO has approved and guaranteed the quality of this product." This seal was later altered in 1988: "approved and guaranteed" was changed to "evaluated and approved." In 1989, the seal became gold and white, as it currently appears, with a shortened phrase, "Official Nintendo Seal of Quality." It was changed in 2003 to read "Official Nintendo Seal."

The seal currently reads:
In PAL regions, the seal is a circular starburst named the "Original Nintendo Seal of Quality." Text near the seal in the Australian Wii manual states:
In 1992, Nintendo teamed with the Starlight Children's Foundation to build Starlight Fun Center mobile entertainment units and install them in hospitals. 1,000 Starlight Nintendo Fun Center units were installed by the end of 1995. These units combine several forms of multimedia entertainment, including gaming, and serve as a distraction to brighten moods and boost kids' morale during hospital stays.

Nintendo has consistently been ranked last in Greenpeace's "Guide to Greener Electronics" due to Nintendo's failure to publish information. Similarly, they are ranked last in the Enough Project's "Conflict Minerals Company Rankings" due to Nintendo's refusal to respond to multiple requests for information.

Like many other electronics companies, Nintendo offers a take-back recycling program which allows customers to mail in old products they no longer use. Nintendo of America claimed that it took in 548 tons of returned products in 2011, 98% of which was either reused or recycled.





</doc>
<doc id="21201" url="https://en.wikipedia.org/wiki?curid=21201" title="Nobel Prize">
Nobel Prize

The Nobel Prize (, ; ; ) is a set of annual international awards bestowed in several categories by Swedish and Norwegian institutions in recognition of academic, cultural, or scientific advances. The will of the Swedish chemist, engineer and industrialist Alfred Nobel established the five Nobel prizes in 1895. The prizes in Chemistry, Literature, Peace, Physics, and Physiology or Medicine were first awarded in 1901. The prizes are widely regarded as the most prestigious awards available in their respective fields.

In 1968, Sveriges Riksbank, Sweden's central bank, established the Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel. The award is based on a donation received by the Nobel Foundation in 1968 from Sveriges Riksbank on the occasion of the bank's 300th anniversary. The first Prize in Economic Sciences was awarded to Ragnar Frisch and Jan Tinbergen in 1969. The Prize in Economic Sciences is awarded by the Royal Swedish Academy of Sciences, Stockholm, Sweden, according to the same principles as for the Nobel Prizes that have been awarded since 1901. However, as it is not one of the prizes that Alfred Nobel established in his will in 1895, it is not a Nobel Prize.

The Royal Swedish Academy of Sciences awards the Nobel Prize in Chemistry, the Nobel Prize in Physics, and the Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel; the Nobel Assembly at the Karolinska Institute awards the Nobel Prize in Physiology or Medicine; the Swedish Academy grants the Nobel Prize in Literature; and the Norwegian Nobel Committee awards the Nobel Peace Prize.

Between 1901 and 2019, the Nobel Prizes (and the Prizes in Economic Sciences, from 1969 on) were awarded 597 times to 950 people and organizations. With some receiving the Nobel Prize more than once, this makes a total of 27 organizations and 908 individuals. The prize ceremonies take place annually in Stockholm, Sweden (with the exception of the Peace Prize ceremony, which is held in Oslo, Norway). Each recipient (known as a "laureate") receives a gold medal, a diploma, and a sum of money that has been decided by the Nobel Foundation. (, each prize is worth 9,000,000 SEK, or about , €848,678, or £716,224.) Medals made before 1980 were struck in 23-carat gold, and later in 18-carat green gold plated with a 24-carat gold coating.

The prize is not awarded posthumously; however, if a person is awarded a prize and dies before receiving it, the prize may still be presented. A prize may not be shared among more than three individuals, although the Nobel Peace Prize can be awarded to organizations of more than three people.

Alfred Nobel () was born on 21 October 1833 in Stockholm, Sweden, into a family of engineers. He was a chemist, engineer, and inventor. In 1894, Nobel purchased the Bofors iron and steel mill, which he made into a major armaments manufacturer. Nobel also invented ballistite. This invention was a precursor to many smokeless military explosives, especially the British smokeless powder cordite. As a consequence of his patent claims, Nobel was eventually involved in a patent infringement lawsuit over cordite. Nobel amassed a fortune during his lifetime, with most of his wealth coming from his 355 inventions, of which dynamite is the most famous.

In 1888, Nobel was astonished to read his own obituary, titled "The merchant of death is dead", in a French newspaper. It was Alfred's brother Ludvig who had died; the obituary was eight years premature. The article disconcerted Nobel and made him apprehensive about how he would be remembered. This inspired him to change his will. On 10 December 1896, Alfred Nobel died in his villa in San Remo, Italy, from a cerebral haemorrhage. He was 63 years old.

Nobel wrote several wills during his lifetime. He composed the last over a year before he died, signing it at the Swedish–Norwegian Club in Paris on 27 November 1895. To widespread astonishment, Nobel's last will specified that his fortune be used to create a series of prizes for those who confer the "greatest benefit on mankind" in physics, chemistry, physiology or medicine, literature, and peace. Nobel bequeathed 94% of his total assets, 31 million SEK (c. US$186 million, €150 million in 2008), to establish the five Nobel Prizes. Owing to skepticism surrounding the will, it was not approved by the Storting in Norway until 26 April 1897. The executors of the will, Ragnar Sohlman and Rudolf Lilljequist, formed the Nobel Foundation to take care of the fortune and to organise the awarding of prizes.

Nobel's instructions named a Norwegian Nobel Committee to award the Peace Prize, the members of whom were appointed shortly after the will was approved in April 1897. Soon thereafter, the other prize-awarding organizations were designated. These were Karolinska Institute on 7 June, the Swedish Academy on 9 June, and the Royal Swedish Academy of Sciences on 11 June. The Nobel Foundation reached an agreement on guidelines for how the prizes should be awarded; and, in 1900, the Nobel Foundation's newly created statutes were promulgated by King Oscar II. In 1905, the personal union between Sweden and Norway was dissolved.

According to his will and testament read in Stockholm on 30 December 1896, a foundation established by Alfred Nobel would reward those who serve humanity. The Nobel Prize was funded by Alfred Nobel's personal fortune. According to the official sources, Alfred Nobel bequeathed from the shares 94% of his fortune to the Nobel Foundation that now forms the economic base of the Nobel Prize.

The Nobel Foundation was founded as a private organization on 29 June 1900. Its function is to manage the finances and administration of the Nobel Prizes. In accordance with Nobel's will, the primary task of the Foundation is to manage the fortune Nobel left. Robert and Ludvig Nobel were involved in the oil business in Azerbaijan, and according to Swedish historian E. Bargengren, who accessed the Nobel family archive, it was this "decision to allow withdrawal of Alfred's money from Baku that became the decisive factor that enabled the Nobel Prizes to be established". Another important task of the Nobel Foundation is to market the prizes internationally and to oversee informal administration related to the prizes. The Foundation is not involved in the process of selecting the Nobel laureates. In many ways, the Nobel Foundation is similar to an investment company, in that it invests Nobel's money to create a solid funding base for the prizes and the administrative activities. The Nobel Foundation is exempt from all taxes in Sweden (since 1946) and from investment taxes in the United States (since 1953). Since the 1980s, the Foundation's investments have become more profitable and as of 31 December 2007, the assets controlled by the Nobel Foundation amounted to 3.628 billion Swedish "kronor" (c. US$560 million).

According to the statutes, the Foundation consists of a board of five Swedish or Norwegian citizens, with its seat in Stockholm. The Chairman of the Board is appointed by the Swedish King in Council, with the other four members appointed by the trustees of the prize-awarding institutions. An Executive Director is chosen from among the board members, a Deputy Director is appointed by the King in Council, and two deputies are appointed by the trustees. However, since 1995, all the members of the board have been chosen by the trustees, and the Executive Director and the Deputy Director appointed by the board itself. As well as the board, the Nobel Foundation is made up of the prize-awarding institutions (the Royal Swedish Academy of Sciences, the Nobel Assembly at Karolinska Institute, the Swedish Academy, and the Norwegian Nobel Committee), the trustees of these institutions, and auditors.

The capital of the Nobel Foundation today is invested 50% in shares, 20% bonds and 30% other investments (e.g. hedge funds or real estate). The distribution can vary by 10 percent. At the beginning of 2008, 64% of the funds were invested mainly in American and European stocks, 20% in bonds, plus 12% in real estate and hedge funds.

In 2011, the total annual cost was approximately 120 million krona, with 50 million krona as the prize money. Further costs to pay institutions and persons engaged in giving the prizes were 27.4 million krona. The events during the Nobel week in Stockholm and Oslo cost 20.2 million krona. The administration, Nobel symposium, and similar items had costs of 22.4 million krona. The cost of the Economic Sciences prize of 16.5 Million krona is paid by the Sveriges Riksbank.

Once the Nobel Foundation and its guidelines were in place, the Nobel Committees began collecting nominations for the inaugural prizes. Subsequently, they sent a list of preliminary candidates to the prize-awarding institutions.

The Nobel Committee's Physics Prize shortlist cited Wilhelm Röntgen's discovery of X-rays and Philipp Lenard's work on cathode rays. The Academy of Sciences selected Röntgen for the prize. In the last decades of the 19th century, many chemists had made significant contributions. Thus, with the Chemistry Prize, the Academy "was chiefly faced with merely deciding the order in which these scientists should be awarded the prize". The Academy received 20 nominations, eleven of them for Jacobus van 't Hoff. Van 't Hoff was awarded the prize for his contributions in chemical thermodynamics.

The Swedish Academy chose the poet Sully Prudhomme for the first Nobel Prize in Literature. A group including 42 Swedish writers, artists, and literary critics protested against this decision, having expected Leo Tolstoy to be awarded. Some, including Burton Feldman, have criticised this prize because they consider Prudhomme a mediocre poet. Feldman's explanation is that most of the Academy members preferred Victorian literature and thus selected a Victorian poet. The first Physiology or Medicine Prize went to the German physiologist and microbiologist Emil von Behring. During the 1890s, von Behring developed an antitoxin to treat diphtheria, which until then was causing thousands of deaths each year.

The first Nobel Peace Prize went to the Swiss Jean Henri Dunant for his role in founding the International Red Cross Movement and initiating the Geneva Convention, and jointly given to French pacifist Frédéric Passy, founder of the Peace League and active with Dunant in the Alliance for Order and Civilization.

In 1938 and 1939, Adolf Hitler's Third Reich forbade three laureates from Germany (Richard Kuhn, Adolf Friedrich Johann Butenandt, and Gerhard Domagk) from accepting their prizes. Each man was later able to receive the diploma and medal. Even though Sweden was officially neutral during the Second World War, the prizes were awarded irregularly. In 1939, the Peace Prize was not awarded. No prize was awarded in any category from 1940 to 1942, due to the occupation of Norway by Germany. In the subsequent year, all prizes were awarded except those for literature and peace.

During the occupation of Norway, three members of the Norwegian Nobel Committee fled into exile. The remaining members escaped persecution from the Germans when the Nobel Foundation stated that the Committee building in Oslo was Swedish property. Thus it was a safe haven from the German military, which was not at war with Sweden. These members kept the work of the Committee going, but did not award any prizes. In 1944, the Nobel Foundation, together with the three members in exile, made sure that nominations were submitted for the Peace Prize and that the prize could be awarded once again.

In 1968, Sweden's central bank Sveriges Riksbank celebrated its 300th anniversary by donating a large sum of money to the Nobel Foundation to be used to set up a prize in honour of Alfred Nobel. The following year, the Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel was awarded for the first time. The Royal Swedish Academy of Sciences became responsible for selecting laureates. The first laureates for the Economics Prize were Jan Tinbergen and Ragnar Frisch "for having developed and applied dynamic models for the analysis of economic processes". The Board of the Nobel Foundation decided that after this addition, it would allow no further new prizes.

The award process is similar for all of the Nobel Prizes, the main difference being who can make nominations for each of them.
Nomination forms are sent by the Nobel Committee to about 3,000 individuals, usually in September the year before the prizes are awarded. These individuals are generally prominent academics working in a relevant area. Regarding the Peace Prize, inquiries are also sent to governments, former Peace Prize laureates, and current or former members of the Norwegian Nobel Committee. The deadline for the return of the nomination forms is 31 January of the year of the award. The Nobel Committee nominates about 300 potential laureates from these forms and additional names. The nominees are not publicly named, nor are they told that they are being considered for the prize. All nomination records for a prize are sealed for 50 years from the awarding of the prize.

The Nobel Committee then prepares a report reflecting the advice of experts in the relevant fields. This, along with the list of preliminary candidates, is submitted to the prize-awarding institutions. The institutions meet to choose the laureate or laureates in each field by a majority vote. Their decision, which cannot be appealed, is announced immediately after the vote. A maximum of three laureates and two different works may be selected per award. Except for the Peace Prize, which can be awarded to institutions, the awards can only be given to individuals.

Although posthumous nominations are not presently permitted, individuals who died in the months between their nomination and the decision of the prize committee were originally eligible to receive the prize. This has occurred twice: the 1931 Literature Prize awarded to Erik Axel Karlfeldt, and the 1961 Peace Prize awarded to UN Secretary General Dag Hammarskjöld. Since 1974, laureates must be thought alive at the time of the October announcement. There has been one laureate, William Vickrey, who in 1996 died after the prize (in Economics) was announced but before it could be presented. On 3 October 2011, the laureates for the Nobel Prize in Physiology or Medicine were announced; however, the committee was not aware that one of the laureates, Ralph M. Steinman, had died three days earlier. The committee was debating about Steinman's prize, since the rule is that the prize is not awarded posthumously. The committee later decided that as the decision to award Steinman the prize "was made in good faith", it would remain unchanged.

Nobel's will provided for prizes to be awarded in recognition of discoveries made "during the preceding year". Early on, the awards usually recognised recent discoveries. However, some of those early discoveries were later discredited. For example, Johannes Fibiger was awarded the 1926 Prize in Physiology or Medicine for his purported discovery of a parasite that caused cancer. To avoid repeating this embarrassment, the awards increasingly recognised scientific discoveries that had withstood the test of time. According to Ralf Pettersson, former chairman of the Nobel Prize Committee for Physiology or Medicine, "the criterion 'the previous year' is interpreted by the Nobel Assembly as the year when the full impact of the discovery has become evident."
The interval between the award and the accomplishment it recognises varies from discipline to discipline. The Literature Prize is typically awarded to recognise a cumulative lifetime body of work rather than a single achievement. The Peace Prize can also be awarded for a lifetime body of work. For example, 2008 laureate Martti Ahtisaari was awarded for his work to resolve international conflicts. However, they can also be awarded for specific recent events. For instance, Kofi Annan was awarded the 2001 Peace Prize just four years after becoming the Secretary-General of the United Nations. Similarly Yasser Arafat, Yitzhak Rabin, and Shimon Peres received the 1994 award, about a year after they successfully concluded the Oslo Accords.

Awards for physics, chemistry, and medicine are typically awarded once the achievement has been widely accepted. Sometimes, this takes decades – for example, Subrahmanyan Chandrasekhar shared the 1983 Physics Prize for his 1930s work on stellar structure and evolution. Not all scientists live long enough for their work to be recognised. Some discoveries can never be considered for a prize if their impact is realised after the discoverers have died.

Except for the Peace Prize, the Nobel Prizes are presented in Stockholm, Sweden, at the annual Prize Award Ceremony on 10 December, the anniversary of Nobel's death. The recipients' lectures are normally held in the days prior to the award ceremony. The Peace Prize and its recipients' lectures are presented at the annual Prize Award Ceremony in Oslo, Norway, usually on 10 December. The award ceremonies and the associated banquets are typically major international events. The Prizes awarded in Sweden's ceremonies' are held at the Stockholm Concert Hall, with the Nobel banquet following immediately at Stockholm City Hall. The Nobel Peace Prize ceremony has been held at the Norwegian Nobel Institute (1905–1946), at the auditorium of the University of Oslo (1947–1989), and at Oslo City Hall (1990–present).

The highlight of the Nobel Prize Award Ceremony in Stockholm occurs when each Nobel laureate steps forward to receive the prize from the hands of the King of Sweden. In Oslo, the Chairman of the Norwegian Nobel Committee presents the Nobel Peace Prize in the presence of the King of Norway. At first, King Oscar II did not approve of awarding grand prizes to foreigners. It is said that he changed his mind once his attention had been drawn to the publicity value of the prizes for Sweden.

After the award ceremony in Sweden, a banquet is held in the Blue Hall at the Stockholm City Hall, which is attended by the Swedish Royal Family and around 1,300 guests. The Nobel Peace Prize banquet is held in Norway at the Oslo Grand Hotel after the award ceremony. Apart from the laureate, guests include the President of the Storting, on occasion the Swedish prime minister, and, since 2006, the King and Queen of Norway. In total, about 250 guests attend.

According to the statutes of the Nobel Foundation, each laureate is required to give a public lecture on a subject related to the topic of their prize. The Nobel lecture as a rhetorical genre took decades to reach its current format. These lectures normally occur during Nobel Week (the week leading up to the award ceremony and banquet, which begins with the laureates arriving in Stockholm and normally ends with the Nobel banquet), but this is not mandatory. The laureate is only obliged to give the lecture within six months of receiving the prize, but some have happened even later. For example, US President Theodore Roosevelt received the Peace Prize in 1906 but gave his lecture in 1910, after his term in office. The lectures are organized by the same association which selected the laureates.

The Nobel Foundation announced on 30 May 2012 that it had awarded the contract for the production of the five (Swedish) Nobel Prize medals to Svenska Medalj AB. Between 1902 and 2010, the Nobel Prize medals were minted by Myntverket (the Swedish Mint), Sweden's oldest company, which ceased operations in 2011 after 107 years. In 2011, the Mint of Norway, located in Kongsberg, made the medals. The Nobel Prize medals are registered trademarks of the Nobel Foundation.

Each medal features an image of Alfred Nobel in left profile on the obverse. The medals for physics, chemistry, physiology or medicine, and literature have identical obverses, showing the image of Alfred Nobel and the years of his birth and death. Nobel's portrait also appears on the obverse of the Peace Prize medal and the medal for the Economics Prize, but with a slightly different design. For instance, the laureate's name is engraved on the rim of the Economics medal. The image on the reverse of a medal varies according to the institution awarding the prize. The reverse sides of the medals for chemistry and physics share the same design.

All medals made before 1980 were struck in 23 carat gold. Since then, they have been struck in 18 carat green gold plated with 24 carat gold. The weight of each medal varies with the value of gold, but averages about for each medal. The diameter is and the thickness varies between and . Because of the high value of their gold content and tendency to be on public display, Nobel medals are subject to medal theft. During World War II, the medals of German scientists Max von Laue and James Franck were sent to Copenhagen for safekeeping. When Germany invaded Denmark, Hungarian chemist (and Nobel laureate himself) George de Hevesy dissolved them in aqua regia (nitro-hydrochloric acid), to prevent confiscation by Nazi Germany and to prevent legal problems for the holders. After the war, the gold was recovered from solution, and the medals re-cast.

Nobel laureates receive a diploma directly from the hands of the King of Sweden, or in the case of the peace prize, the Chairman of the Norwegian Nobel Committee. Each diploma is uniquely designed by the prize-awarding institutions for the laureates that receive them. The diploma contains a picture and text in Swedish which states the name of the laureate and normally a citation of why they received the prize. None of the Nobel Peace Prize laureates has ever had a citation on their diplomas.

The laureates are given a sum of money when they receive their prizes, in the form of a document confirming the amount awarded. The amount of prize money depends upon how much money the Nobel Foundation can award each year. The purse has increased since the 1980s, when the prize money was 880,000 SEK per prize (c. 2.6 million SEK altogether, US$350,000 today). In 2009, the monetary award was 10 million SEK (US$1.4 million). In June 2012, it was lowered to 8 million SEK. If two laureates share the prize in a category, the award grant is divided equally between the recipients. If there are three, the awarding committee has the option of dividing the grant equally, or awarding one-half to one recipient and one-quarter to each of the others. It is common for recipients to donate prize money to benefit scientific, cultural, or humanitarian causes.

Among other criticisms, the Nobel Committees have been accused of having a political agenda, and of omitting more deserving candidates. They have also been accused of Eurocentrism, especially for the Literature Prize.


Among the most criticised Nobel Peace Prizes was the one awarded to Henry Kissinger and Lê Đức Thọ. This led to the resignation of two Norwegian Nobel Committee members. Kissinger and Thọ were awarded the prize for negotiating a ceasefire between North Vietnam and the United States in January 1973. However, when the award was announced, both sides were still engaging in hostilities. Critics sympathetic to the North announced that Kissinger was not a peace-maker but the opposite, responsible for widening the war. Those hostile to the North and what they considered its deceptive practices during negotiations were deprived of a chance to criticise Lê Đức Thọ, as he declined the award.

Yasser Arafat, Shimon Peres, and Yitzhak Rabin received the Peace Prize in 1994 for their efforts in making peace between Israel and Palestine. Immediately after the award was announced, one of the five Norwegian Nobel Committee members denounced Arafat as a terrorist and resigned. Additional misgivings about Arafat were widely expressed in various newspapers.

Another controversial Peace Prize was that awarded to Barack Obama in 2009. Nominations had closed only eleven days after Obama took office as President of the United States, but the actual evaluation occurred over the next eight months. Obama himself stated that he did not feel deserving of the award, or worthy of the company in which it would place him. Past Peace Prize laureates were divided, some saying that Obama deserved the award, and others saying he had not secured the achievements to yet merit such an accolade. Obama's award, along with the previous Peace Prizes for Jimmy Carter and Al Gore, also prompted accusations of a left-wing bias.


The award of the 2004 Literature Prize to Elfriede Jelinek drew a protest from a member of the Swedish Academy, Knut Ahnlund. Ahnlund resigned, alleging that the selection of Jelinek had caused "irreparable damage to all progressive forces, it has also confused the general view of literature as an art". He alleged that Jelinek's works were "a mass of text shovelled together without artistic structure". The 2009 Literature Prize to Herta Müller also generated criticism. According to "The Washington Post", many US literary critics and professors were ignorant of her work. This made those critics feel the prizes were too Eurocentric.


In 1949, the neurologist António Egas Moniz received the Physiology or Medicine Prize for his development of the prefrontal leucotomy. The previous year, Dr. Walter Freeman had developed a version of the procedure which was faster and easier to carry out. Due in part to the publicity surrounding the original procedure, Freeman's procedure was prescribed without due consideration or regard for modern medical ethics. Endorsed by such influential publications as "The New England Journal of Medicine", leucotomy or "lobotomy" became so popular that about 5,000 lobotomies were performed in the United States in the three years immediately following Moniz's receipt of the Prize.

The Norwegian Nobel Committee confirmed that Mahatma Gandhi was nominated for the Peace Prize in 1937–39, 1947 and, a few days before he was assassinated in January 1948. Later, members of the Norwegian Nobel Committee expressed regret that he was not given the prize. Geir Lundestad, Secretary of Norwegian Nobel Committee in 2006, said, "The greatest omission in our 106 year history is undoubtedly that Mahatma Gandhi never received the Nobel Peace prize. Gandhi could do without the Nobel Peace prize. Whether Nobel committee can do without Gandhi is the question". In 1948, the year of Gandhi's death, the Nobel Committee declined to award a prize on the grounds that "there was no suitable living candidate" that year. Later, when the 14th Dalai Lama was awarded the Peace Prize in 1989, the chairman of the committee said that this was "in part a tribute to the memory of Mahatma Gandhi". Other high-profile individuals with widely recognised contributions to peace have been missed out. "Foreign Policy" lists Eleanor Roosevelt, Václav Havel, Ken Saro-Wiwa, Sari Nusseibeh, and Corazon Aquino as people who "never won the prize, but should have".

In 1965, UN Secretary General U Thant was informed by the Norwegian Permanent Representative to the UN that he would be awarded that year's prize and asked whether or not he would accept. He consulted staff and later replied that he would. At the same time, Chairman Gunnar Jahn of the Nobel Peace prize committee, lobbied heavily against giving U Thant the prize and the prize was at the last minute awarded to UNICEF. The rest of the committee all wanted the prize to go to U Thant, for his work in defusing the Cuban Missile Crisis, ending the war in the Congo, and his ongoing work to mediate an end to the Vietnam War. The disagreement lasted three years and in 1966 and 1967 no prize was given, with Gunnar Jahn effectively vetoing an award to U Thant.
The Literature Prize also has controversial omissions. Adam Kirsch has suggested that many notable writers have missed out on the award for political or extra-literary reasons. The heavy focus on European and Swedish authors has been a subject of criticism. The Eurocentric nature of the award was acknowledged by Peter Englund, the 2009 Permanent Secretary of the Swedish Academy, as a problem with the award and was attributed to the tendency for the academy to relate more to European authors. This tendency towards European authors still leaves some European writers on a list of notable writers that have been overlooked for the Literature Prize, including Europe's Leo Tolstoy, Anton Chekhov, J. R. R. Tolkien, Émile Zola, Marcel Proust, Vladimir Nabokov, James Joyce, August Strindberg, Simon Vestdijk, Karel Čapek, the New World's Jorge Luis Borges, Ezra Pound, John Updike, Arthur Miller, Mark Twain, and Africa's Chinua Achebe.

Candidates can receive multiple nominations the same year. Gaston Ramon received a total of 155 nominations in physiology or medicine from 1930 to 1953, the last year with public nomination data for that award . He died in 1963 without being awarded. Pierre Paul Émile Roux received 115 nominations in physiology or medicine, and Arnold Sommerfeld received 84 in physics. These are the three most nominated scientists without awards in the data published . Otto Stern received 79 nominations in physics 1925–43 before being awarded in 1943.

The strict rule against awarding a prize to more than three people is also controversial. When a prize is awarded to recognise an achievement by a team of more than three collaborators, one or more will miss out. For example, in 2002, the prize was awarded to Koichi Tanaka and John Fenn for the development of mass spectrometry in protein chemistry, an award that did not recognise the achievements of Franz Hillenkamp and Michael Karas of the Institute for Physical and Theoretical Chemistry at the University of Frankfurt. According to one of the nominees for the prize in physics, the three person limit deprived him and two other members of his team of the honor in 2013: the team of Carl Hagen, Gerald Guralnik, and Tom Kibble published a paper in 1964 that gave answers to how the cosmos began, but did not share the 2013 Physics Prize awarded to Peter Higgs and François Englert, who had also published papers in 1964 concerning the subject. All five physicists arrived at the same conclusion, albeit from different angles. Hagen contends that an equitable solution is to either abandon the three limit restriction, or expand the time period of recognition for a given achievement to two years.

Similarly, the prohibition of posthumous awards fails to recognise achievements by an individual or collaborator who dies before the prize is awarded. The Economics Prize was not awarded to Fischer Black, who died in 1995, when his co-author Myron Scholes received the honor in 1997 for their landmark work on option pricing along with Robert C. Merton, another pioneer in the development of valuation of stock options. In the announcement of the award that year, the Nobel committee prominently mentioned Black's key role.

Political subterfuge may also deny proper recognition. Lise Meitner and Fritz Strassmann, who co-discovered nuclear fission along with Otto Hahn, may have been denied a share of Hahn's 1944 Nobel Chemistry Award due to having fled Germany when the Nazis came to power. The Meitner and Strassmann roles in the research was not fully recognised until years later, when they joined Hahn in receiving the 1966 Enrico Fermi Award.

Alfred Nobel left his fortune to finance annual prizes to be awarded "to those who, during the preceding year, shall have conferred the greatest benefit on mankind". He stated that the Nobel Prizes in Physics should be given "to the person who shall have made the most important 'discovery' or 'invention' within the field of physics". Nobel did not emphasise discoveries, but they have historically been held in higher respect by the Nobel Prize Committee than inventions: 77% of the Physics Prizes have been given to discoveries, compared with only 23% to inventions. Christoph Bartneck and Matthias Rauterberg, in papers published in "Nature" and "Technoetic Arts", have argued this emphasis on discoveries has moved the Nobel Prize away from its original intention of rewarding the greatest contribution to society.

In terms of the most prestigious awards in STEM fields, only a small proportion have been awarded to women. Out of 210 laureates in Physics, 181 in Chemistry and 216 in Medicine between 1901 and 2018, there were only three female laureates in physics, five in chemistry and 12 in medicine. Factors proposed to contribute to the discrepancy between this and the roughly equal human sex ratio include biased nominations, fewer women than men being active in the relevant fields, Nobel Prizes typically being awarded decades after the research was done (reflecting a time when gender bias in the relevant fields was greater), a greater delay in awarding Nobel Prizes for women's achievements making longevity a more important factor for women (Nobel Prizes are not awarded posthumously), and a tendency to omit women from jointly awarded Nobel Prizes. Despite these factors, Marie Curie is to date the only person awarded Nobel Prizes in two different sciences (Physics in 1903, Chemistry in 1911); she is one of only three people who have received two Nobel Prizes in sciences (see Multiple laureates below).


Malala Yousafzai; at the age of 17, received Nobel Peace Prize (2014).


John B. Goodenough; at the age of 97, received Nobel Prize in Chemistry (2019).


Linus Pauling; received the prize twice. Nobel Prize in Chemistry (1954) and Nobel Peace Prize (1962)







Four people have received two Nobel Prizes. Marie Curie received the Physics Prize in 1903 for her work on radioactivity and the Chemistry Prize in 1911 for the isolation of pure radium, making her the only person to be awarded a Nobel Prize in two different sciences. Linus Pauling was awarded the 1954 Chemistry Prize for his research into the chemical bond and its application to the structure of complex substances. Pauling was also awarded the Peace Prize in 1962 for his activism against nuclear weapons, making him the only laureate of two unshared prizes. John Bardeen received the Physics Prize twice: in 1956 for the invention of the transistor and in 1972 for the theory of superconductivity. Frederick Sanger received the prize twice in Chemistry: in 1958 for determining the structure of the insulin molecule and in 1980 for inventing a method of determining base sequences in DNA.

Two organizations have received the Peace Prize multiple times. The International Committee of the Red Cross received it three times: in 1917 and 1944 for its work during the world wars; and in 1963 during the year of its centenary. The United Nations High Commissioner for Refugees has been awarded the Peace Prize twice for assisting refugees: in 1954 and 1981.

The Curie family has received the most prizes, with four prizes awarded to five individual laureates. Marie Curie received the prizes in Physics (in 1903) and Chemistry (in 1911). Her husband, Pierre Curie, shared the 1903 Physics prize with her. Their daughter, Irène Joliot-Curie, received the Chemistry Prize in 1935 together with her husband Frédéric Joliot-Curie. In addition, the husband of Marie Curie's second daughter, Henry Labouisse, was the director of UNICEF when he accepted the Nobel Peace Prize in 1965 on that organisation's behalf.

Although no family matches the Curie family's record, there have been several with two laureates. The husband-and-wife team of Gerty Cori and Carl Ferdinand Cori shared the 1947 Prize in Physiology or Medicine as did the husband-and-wife team of May-Britt Moser and Edvard Moser in 2014 (along with John O'Keefe). J. J. Thomson was awarded the Physics Prize in 1906 for showing that electrons are particles. His son, George Paget Thomson, received the same prize in 1937 for showing that they also have the properties of waves. William Henry Bragg and his son, William Lawrence Bragg, shared the Physics Prize in 1915 for inventing the X-ray crystallography. Niels Bohr was awarded the Physics prize in 1922, as was his son, Aage Bohr, in 1975. Manne Siegbahn, who received the Physics Prize in 1924, was the father of Kai Siegbahn, who received the Physics Prize in 1981. Hans von Euler-Chelpin, who received the Chemistry Prize in 1929, was the father of Ulf von Euler, who was awarded the Physiology or Medicine Prize in 1970. C. V. Raman was awarded the Physics Prize in 1930 and was the uncle of Subrahmanyan Chandrasekhar, who was awarded the same prize in 1983. Arthur Kornberg received the Physiology or Medicine Prize in 1959; Kornberg's son, Roger later received the Chemistry Prize in 2006. Jan Tinbergen, who was awarded the first Economics Prize in 1969, was the brother of Nikolaas Tinbergen, who received the 1973 Physiology or Medicine Prize. Alva Myrdal, Peace Prize laureate in 1982, was the wife of Gunnar Myrdal who was awarded the Economics Prize in 1974. Economics laureates Paul Samuelson and Kenneth Arrow were brothers-in-law. Frits Zernike, who was awarded the 1953 Physics Prize, is the great-uncle of 1999 Physics laureate Gerard 't Hooft. In 2019, married couple Abhijit Banerjee and Esther Duflo were awarded the Economics Prize.

Two laureates have voluntarily declined the Nobel Prize. In 1964, Jean-Paul Sartre was awarded the Literature Prize but refused, stating, "A writer must refuse to allow himself to be transformed into an institution, even if it takes place in the most honourable form." Lê Đức Thọ, chosen for the 1973 Peace Prize for his role in the Paris Peace Accords, declined, stating that there was no actual peace in Vietnam. George Bernard Shaw attempted to decline the prize money while accepting the 1925 Literature Prize; eventually it was agreed to use it to found the Anglo-Swedish Literary Foundation.

During the Third Reich, Adolf Hitler hindered Richard Kuhn, Adolf Butenandt, and Gerhard Domagk from accepting their prizes. All of them were awarded their diplomas and gold medals after World War II. In 1958, Boris Pasternak declined his prize for literature due to fear of what the Soviet Union government might do if he travelled to Stockholm to accept his prize. In return, the Swedish Academy refused his refusal, saying "this refusal, of course, in no way alters the validity of the award." The Academy announced with regret that the presentation of the Literature Prize could not take place that year, holding it back until 1989 when Pasternak's son accepted the prize on his behalf.

Aung San Suu Kyi was awarded the Nobel Peace Prize in 1991, but her children accepted the prize because she had been placed under house arrest in Burma; Suu Kyi delivered her speech two decades later, in 2012. Liu Xiaobo was awarded the Nobel Peace Prize in 2010 while he and his wife were under house arrest in China as political prisoners, and he was unable to accept the prize in his lifetime.

Being a symbol of scientific or literary achievement that is recognisable worldwide, the Nobel Prize is often depicted in fiction. This includes films like "The Prize" and "Nobel Son" about fictional Nobel laureates as well as fictionalised accounts of stories surrounding real prizes such as "Nobel Chor", a film based on the theft of Rabindranath Tagore's prize.

The memorial symbol "Planet of Alfred Nobel" was opened in Dnipropetrovsk University of Economics and Law in 2008. On the globe, there are 802 Nobel laureates' reliefs made of a composite alloy obtained when disposing of military strategic missiles.




</doc>
<doc id="21210" url="https://en.wikipedia.org/wiki?curid=21210" title="Niels Bohr">
Niels Bohr

Niels Henrik David Bohr (; 7 October 1885 – 18 November 1962) was a Danish physicist who made foundational contributions to understanding atomic structure and quantum theory, for which he received the Nobel Prize in Physics in 1922. Bohr was also a philosopher and a promoter of scientific research.

Bohr developed the Bohr model of the atom, in which he proposed that energy levels of electrons are discrete and that the electrons revolve in stable orbits around the atomic nucleus but can jump from one energy level (or orbit) to another. Although the Bohr model has been supplanted by other models, its underlying principles remain valid. He conceived the principle of complementarity: that items could be separately analysed in terms of contradictory properties, like behaving as a wave or a stream of particles. The notion of complementarity dominated Bohr's thinking in both science and philosophy.

Bohr founded the Institute of Theoretical Physics at the University of Copenhagen, now known as the Niels Bohr Institute, which opened in 1920. Bohr mentored and collaborated with physicists including Hans Kramers, Oskar Klein, George de Hevesy, and Werner Heisenberg. He predicted the existence of a new zirconium-like element, which was named hafnium, after the Latin name for Copenhagen, where it was discovered. Later, the element bohrium was named after him.

During the 1930s Bohr helped refugees from Nazism. After Denmark was occupied by the Germans, he had a famous meeting with Heisenberg, who had become the head of the German nuclear weapon project. In September 1943 word reached Bohr that he was about to be arrested by the Germans, and he fled to Sweden. From there, he was flown to Britain, where he joined the British Tube Alloys nuclear weapons project, and was part of the British mission to the Manhattan Project. After the war, Bohr called for international cooperation on nuclear energy. He was involved with the establishment of CERN and the Research Establishment Risø of the Danish Atomic Energy Commission and became the first chairman of the Nordic Institute for Theoretical Physics in 1957.

Bohr was born in Copenhagen, Denmark, on 7 October 1885, the second of three children of Christian Bohr, a professor of physiology at the University of Copenhagen, and Ellen Adler Bohr, who came from a wealthy Danish Jewish family prominent in banking and parliamentary circles. He had an elder sister, Jenny, and a younger brother Harald. Jenny became a teacher, while Harald became a mathematician and footballer who played for the Danish national team at the 1908 Summer Olympics in London. Niels was a passionate footballer as well, and the two brothers played several matches for the Copenhagen-based Akademisk Boldklub (Academic Football Club), with Niels as goalkeeper.
Bohr was educated at Gammelholm Latin School, starting when he was seven. In 1903, Bohr enrolled as an undergraduate at Copenhagen University. His major was physics, which he studied under Professor Christian Christiansen, the university's only professor of physics at that time. He also studied astronomy and mathematics under Professor Thorvald Thiele, and philosophy under Professor Harald Høffding, a friend of his father.

In 1905 a gold medal competition was sponsored by the Royal Danish Academy of Sciences and Letters to investigate a method for measuring the surface tension of liquids that had been proposed by Lord Rayleigh in 1879. This involved measuring the frequency of oscillation of the radius of a water jet. Bohr conducted a series of experiments using his father's laboratory in the university; the university itself had no physics laboratory. To complete his experiments, he had to make his own glassware, creating test tubes with the required elliptical cross-sections. He went beyond the original task, incorporating improvements into both Rayleigh's theory and his method, by taking into account the viscosity of the water, and by working with finite amplitudes instead of just infinitesimal ones. His essay, which he submitted at the last minute, won the prize. He later submitted an improved version of the paper to the Royal Society in London for publication in the "Philosophical Transactions of the Royal Society".

Harald became the first of the two Bohr brothers to earn a master's degree, which he earned for mathematics in April 1909. Niels took another nine months to earn his on the electron theory of metals, a topic assigned by his supervisor, Christiansen. Bohr subsequently elaborated his master's thesis into his much-larger Doctor of Philosophy (dr. phil.) thesis. He surveyed the literature on the subject, settling on a model postulated by Paul Drude and elaborated by Hendrik Lorentz, in which the electrons in a metal are considered to behave like a gas. Bohr extended Lorentz's model, but was still unable to account for phenomena like the Hall effect, and concluded that electron theory could not fully explain the magnetic properties of metals. The thesis was accepted in April 1911, and Bohr conducted his formal defence on 13 May. Harald had received his doctorate the previous year. Bohr's thesis was groundbreaking, but attracted little interest outside Scandinavia because it was written in Danish, a Copenhagen University requirement at the time. In 1921, the Dutch physicist Hendrika Johanna van Leeuwen would independently derive a theorem from Bohr's thesis that is today known as the Bohr–van Leeuwen theorem.
In 1910, Bohr met Margrethe Nørlund, the sister of the mathematician Niels Erik Nørlund. Bohr resigned his membership in the Church of Denmark on 16 April 1912, and he and Margrethe were married in a civil ceremony at the town hall in Slagelse on 1 August. Years later, his brother Harald similarly left the church before getting married. Bohr and Margrethe had six sons. The oldest, Christian, died in a boating accident in 1934, and another, Harald, died from childhood meningitis. Aage Bohr became a successful physicist, and in 1975 was awarded the Nobel Prize in physics, like his father. became a physician; , a chemical engineer; and Ernest, a lawyer. Like his uncle Harald, Ernest Bohr became an Olympic athlete, playing field hockey for Denmark at the 1948 Summer Olympics in London.

In September 1911, Bohr, supported by a fellowship from the Carlsberg Foundation, travelled to England. At the time, it was where most of the theoretical work on the structure of atoms and molecules was being done. He met J. J. Thomson of the Cavendish Laboratory and Trinity College, Cambridge. He attended lectures on electromagnetism given by James Jeans and Joseph Larmor, and did some research on cathode rays, but failed to impress Thomson. He had more success with younger physicists like the Australian William Lawrence Bragg, and New Zealand's Ernest Rutherford, whose 1911 small central nucleus Rutherford model of the atom had challenged Thomson's 1904 plum pudding model. Bohr received an invitation from Rutherford to conduct post-doctoral work at Victoria University of Manchester, where Bohr met George de Hevesy and Charles Galton Darwin (whom Bohr referred to as "the grandson of the real Darwin").

Bohr returned to Denmark in July 1912 for his wedding, and travelled around England and Scotland on his honeymoon. On his return, he became a "privatdocent" at the University of Copenhagen, giving lectures on thermodynamics. Martin Knudsen put Bohr's name forward for a "docent", which was approved in July 1913, and Bohr then began teaching medical students. His three papers, which later became famous as "the trilogy", were published in "Philosophical Magazine" in July, September and November of that year. He adapted Rutherford's nuclear structure to Max Planck's quantum theory and so created his Bohr model of the atom.

Planetary models of atoms were not new, but Bohr's treatment was. Taking the 1912 paper by Darwin on the role of electrons in the interaction of alpha particles with a nucleus as his starting point, he advanced the theory of electrons travelling in orbits around the atom's nucleus, with the chemical properties of each element being largely determined by the number of electrons in the outer orbits of its atoms. He introduced the idea that an electron could drop from a higher-energy orbit to a lower one, in the process emitting a quantum of discrete energy. This became a basis for what is now known as the old quantum theory.

In 1885, Johann Balmer had come up with his Balmer series to describe the visible spectral lines of a hydrogen atom:
where λ is the wavelength of the absorbed or emitted light and "R" is the Rydberg constant. Balmer's formula was corroborated by the discovery of additional spectral lines, but for thirty years, no one could explain why it worked. In the first paper of his trilogy, Bohr was able to derive it from his model:
where "m" is the electron's mass, "e" is its charge, "h" is Planck's constant and "Z" is the atom's atomic number (1 for hydrogen).

The model's first hurdle was the Pickering series, lines which did not fit Balmer's formula. When challenged on this by Alfred Fowler, Bohr replied that they were caused by ionised helium, helium atoms with only one electron. The Bohr model was found to work for such ions. Many older physicists, like Thomson, Rayleigh and Hendrik Lorentz, did not like the trilogy, but the younger generation, including Rutherford, David Hilbert, Albert Einstein, Enrico Fermi, Max Born and Arnold Sommerfeld saw it as a breakthrough. The trilogy's acceptance was entirely due to its ability to explain phenomena which stymied other models, and to predict results that were subsequently verified by experiments. Today, the Bohr model of the atom has been superseded, but is still the best known model of the atom, as it often appears in high school physics and chemistry texts.

Bohr did not enjoy teaching medical students. He decided to return to Manchester, where Rutherford had offered him a job as a reader in place of Darwin, whose tenure had expired. Bohr accepted. He took a leave of absence from the University of Copenhagen, which he started by taking a holiday in Tyrol with his brother Harald and aunt Hanna Adler. There, he visited the University of Göttingen and the Ludwig Maximilian University of Munich, where he met Sommerfeld and conducted seminars on the trilogy. The First World War broke out while they were in Tyrol, greatly complicating the trip back to Denmark and Bohr's subsequent voyage with Margrethe to England, where he arrived in October 1914. They stayed until July 1916, by which time he had been appointed to the Chair of Theoretical Physics at the University of Copenhagen, a position created especially for him. His docentship was abolished at the same time, so he still had to teach physics to medical students. New professors were formally introduced to King Christian X, who expressed his delight at meeting such a famous football player.

In April 1917 Bohr began a campaign to establish an Institute of Theoretical Physics. He gained the support of the Danish government and the Carlsberg Foundation, and sizeable contributions were also made by industry and private donors, many of them Jewish. Legislation establishing the Institute was passed in November 1918. Now known as the Niels Bohr Institute, it opened on 3 March 1921, with Bohr as its director. His family moved into an apartment on the first floor. Bohr's institute served as a focal point for researchers into quantum mechanics and related subjects in the 1920s and 1930s, when most of the world's best known theoretical physicists spent some time in his company. Early arrivals included Hans Kramers from the Netherlands, Oskar Klein from Sweden, George de Hevesy from Hungary, Wojciech Rubinowicz from Poland and Svein Rosseland from Norway. Bohr became widely appreciated as their congenial host and eminent colleague. Klein and Rosseland produced the Institute's first publication even before it opened.
The Bohr model worked well for hydrogen, but could not explain more complex elements. By 1919, Bohr was moving away from the idea that electrons orbited the nucleus and developed heuristics to describe them. The rare-earth elements posed a particular classification problem for chemists, because they were so chemically similar. An important development came in 1924 with Wolfgang Pauli's discovery of the Pauli exclusion principle, which put Bohr's models on a firm theoretical footing. Bohr was then able to declare that the as-yet-undiscovered element 72 was not a rare-earth element, but an element with chemical properties similar to those of zirconium. He was immediately challenged by the French chemist Georges Urbain, who claimed to have discovered a rare-earth element 72, which he called "celtium". At the Institute in Copenhagen, Dirk Coster and George de Hevesy took up the challenge of proving Bohr right and Urbain wrong. Starting with a clear idea of the chemical properties of the unknown element greatly simplified the search process. They went through samples from Copenhagen's Museum of Mineralogy looking for a zirconium-like element and soon found it. The element, which they named hafnium ("Hafnia" being the Latin name for Copenhagen) turned out to be more common than gold.

In 1922 Bohr was awarded the Nobel Prize in Physics "for his services in the investigation of the structure of atoms and of the radiation emanating from them". The award thus recognised both the Trilogy and his early leading work in the emerging field of quantum mechanics. For his Nobel lecture, Bohr gave his audience a comprehensive survey of what was then known about the structure of the atom, including the correspondence principle, which he had formulated. This states that the behaviour of systems described by quantum theory reproduces classical physics in the limit of large quantum numbers.

The discovery of Compton scattering by Arthur Holly Compton in 1923 convinced most physicists that light was composed of photons, and that energy and momentum were conserved in collisions between electrons and photons. In 1924, Bohr, Kramers and John C. Slater, an American physicist working at the Institute in Copenhagen, proposed the Bohr–Kramers–Slater theory (BKS). It was more a programme than a full physical theory, as the ideas it developed were not worked out quantitatively. BKS theory became the final attempt at understanding the interaction of matter and electromagnetic radiation on the basis of the old quantum theory, in which quantum phenomena were treated by imposing quantum restrictions on a classical wave description of the electromagnetic field.

Modelling atomic behaviour under incident electromagnetic radiation using "virtual oscillators" at the absorption and emission frequencies, rather than the (different) apparent frequencies of the Bohr orbits, led Max Born, Werner Heisenberg and Kramers to explore different mathematical models. They led to the development of matrix mechanics, the first form of modern quantum mechanics. The BKS theory also generated discussion of, and renewed attention to, difficulties in the foundations of the old quantum theory. The most provocative element of BKS – that momentum and energy would not necessarily be conserved in each interaction, but only statistically – was soon shown to be in conflict with experiments conducted by Walther Bothe and Hans Geiger. In light of these results, Bohr informed Darwin that "there is nothing else to do than to give our revolutionary efforts as honourable a funeral as possible".

The introduction of spin by George Uhlenbeck and Samuel Goudsmit in November 1925 was a milestone. The next month, Bohr travelled to Leiden to attend celebrations of the 50th anniversary of Hendrick Lorentz receiving his doctorate. When his train stopped in Hamburg, he was met by Wolfgang Pauli and Otto Stern, who asked for his opinion of the spin theory. Bohr pointed out that he had concerns about the interaction between electrons and magnetic fields. When he arrived in Leiden, Paul Ehrenfest and Albert Einstein informed Bohr that Einstein had resolved this problem using relativity. Bohr then had Uhlenbeck and Goudsmit incorporate this into their paper. Thus, when he met Werner Heisenberg and Pascual Jordan in Göttingen on the way back, he had become, in his own words, "a prophet of the electron magnet gospel".

Heisenberg first came to Copenhagen in 1924, then returned to Göttingen in June 1925, shortly thereafter developing the mathematical foundations of quantum mechanics. When he showed his results to Max Born in Göttingen, Born realised that they could best be expressed using matrices. This work attracted the attention of the British physicist Paul Dirac, who came to Copenhagen for six months in September 1926. Austrian physicist Erwin Schrödinger also visited in 1926. His attempt at explaining quantum physics in classical terms using wave mechanics impressed Bohr, who believed it contributed "so much to mathematical clarity and simplicity that it represents a gigantic advance over all previous forms of quantum mechanics".

When Kramers left the Institute in 1926 to take up a chair as professor of theoretical physics at the Utrecht University, Bohr arranged for Heisenberg to return and take Kramers's place as a "lektor" at the University of Copenhagen. Heisenberg worked in Copenhagen as a university lecturer and assistant to Bohr from 1926 to 1927.

Bohr became convinced that light behaved like both waves and particles and, in 1927, experiments confirmed the de Broglie hypothesis that matter (like electrons) also behaved like waves. He conceived the philosophical principle of complementarity: that items could have apparently mutually exclusive properties, such as being a wave or a stream of particles, depending on the experimental framework. He felt that it was not fully understood by professional philosophers.

In Copenhagen in 1927 Heisenberg developed his uncertainty principle, which Bohr embraced. In a paper he presented at the Volta Conference at Como in September 1927, he demonstrated that the uncertainty principle could be derived from classical arguments, without quantum terminology or matrices. Einstein preferred the determinism of classical physics over the probabilistic new quantum physics to which he himself had contributed. Philosophical issues that arose from the novel aspects of quantum mechanics became widely celebrated subjects of discussion. Einstein and Bohr had good-natured arguments over such issues throughout their lives.

In 1914 Carl Jacobsen, the heir to Carlsberg breweries, bequeathed his mansion to be used for life by the Dane who had made the most prominent contribution to science, literature or the arts, as an honorary residence (). Harald Høffding had been the first occupant, and upon his death in July 1931, the Royal Danish Academy of Sciences and Letters gave Bohr occupancy. He and his family moved there in 1932. He was elected president of the Academy on 17 March 1939.

By 1929 the phenomenon of beta decay prompted Bohr to again suggest that the law of conservation of energy be abandoned, but Enrico Fermi's hypothetical neutrino and the subsequent 1932 discovery of the neutron provided another explanation. This prompted Bohr to create a new theory of the compound nucleus in 1936, which explained how neutrons could be captured by the nucleus. In this model, the nucleus could be deformed like a drop of liquid. He worked on this with a new collaborator, the Danish physicist Fritz Kalckar, who died suddenly in 1938.

The discovery of nuclear fission by Otto Hahn in December 1938 (and its theoretical explanation by Lise Meitner) generated intense interest among physicists. Bohr brought the news to the United States where he opened the Fifth Washington Conference on Theoretical Physics with Fermi on 26 January 1939. When Bohr told George Placzek that this resolved all the mysteries of transuranic elements, Placzek told him that one remained: the neutron capture energies of uranium did not match those of its decay. Bohr thought about it for a few minutes and then announced to Placzek, Léon Rosenfeld and John Wheeler that "I have understood everything." Based on his liquid drop model of the nucleus, Bohr concluded that it was the uranium-235 isotope and not the more abundant uranium-238 that was primarily responsible for fission with thermal neutrons. In April 1940, John R. Dunning demonstrated that Bohr was correct. In the meantime, Bohr and Wheeler developed a theoretical treatment which they published in a September 1939 paper on "The Mechanism of Nuclear Fission".

Heisenberg said of Bohr that he was "primarily a philosopher, not a physicist". Bohr read the 19th-century Danish Christian existentialist philosopher, Søren Kierkegaard. Richard Rhodes argued in "The Making of the Atomic Bomb" that Bohr was influenced by Kierkegaard through Høffding. In 1909, Bohr sent his brother Kierkegaard's "Stages on Life's Way" as a birthday gift. In the enclosed letter, Bohr wrote, "It is the only thing I have to send home; but I do not believe that it would be very easy to find anything better ... I even think it is one of the most delightful things I have ever read." Bohr enjoyed Kierkegaard's language and literary style, but mentioned that he had some disagreement with Kierkegaard's philosophy. Some of Bohr's biographers suggested that this disagreement stemmed from Kierkegaard's advocacy of Christianity, while Bohr was an atheist.

There has been some dispute over the extent to which Kierkegaard influenced Bohr's philosophy and science. David Favrholdt argued that Kierkegaard had minimal influence over Bohr's work, taking Bohr's statement about disagreeing with Kierkegaard at face value, while Jan Faye argued that one can disagree with the content of a theory while accepting its general premises and structure. Regarding the nature of physics and quantum mechanics Bohr opined that "There is no quantum world. This is only an abstract physical description. It is wrong to think that the task of physics is to find out how nature is. Physics concerns what we can say about nature".

The rise of Nazism in Germany prompted many scholars to flee their countries, either because they were Jewish or because they were political opponents of the Nazi regime. In 1933, the Rockefeller Foundation created a fund to help support refugee academics, and Bohr discussed this programme with the President of the Rockefeller Foundation, Max Mason, in May 1933 during a visit to the United States. Bohr offered the refugees temporary jobs at the Institute, provided them with financial support, arranged for them to be awarded fellowships from the Rockefeller Foundation, and ultimately found them places at institutions around the world. Those that he helped included Guido Beck, Felix Bloch, James Franck, George de Hevesy, Otto Frisch, Hilde Levi, Lise Meitner, George Placzek, Eugene Rabinowitch, Stefan Rozental, Erich Ernst Schneider, Edward Teller, Arthur von Hippel and Victor Weisskopf.

In April 1940, early in the Second World War, Nazi Germany invaded and occupied Denmark. To prevent the Germans from discovering Max von Laue's and James Franck's gold Nobel medals, Bohr had de Hevesy dissolve them in aqua regia. In this form, they were stored on a shelf at the Institute until after the war, when the gold was precipitated and the medals re-struck by the Nobel Foundation. Bohr's own medal had been donated to an auction to the Fund for Finnish Relief, and was auctioned off in March 1940, along with the medal of August Krogh. The buyer later donated the two medals to the Danish Historical Museum in Frederiksborg Castle, where they are still kept.

Bohr kept the Institute running, but all the foreign scholars departed.

Bohr was aware of the possibility of using uranium-235 to construct an atomic bomb, referring to it in lectures in Britain and Denmark shortly before and after the war started, but he did not believe that it was technically feasible to extract a sufficient quantity of uranium-235. In September 1941, Heisenberg, who had become head of the German nuclear energy project, visited Bohr in Copenhagen. During this meeting the two men took a private moment outside, the content of which has caused much speculation, as both gave differing accounts.
According to Heisenberg, he began to address nuclear energy, morality and the war, to which Bohr seems to have reacted by terminating the conversation abruptly while not giving Heisenberg hints about his own opinions. Ivan Supek, one of Heisenberg's students and friends, claimed that the main subject of the meeting was Carl Friedrich von Weizsäcker, who had proposed trying to persuade Bohr to mediate peace between Britain and Germany.

In 1957, Heisenberg wrote to Robert Jungk, who was then working on the book "". Heisenberg explained that he had visited Copenhagen to communicate to Bohr the views of several German scientists, that production of a nuclear weapon was possible with great efforts, and this raised enormous responsibilities on the world's scientists on both sides. When Bohr saw Jungk's depiction in the Danish translation of the book, he drafted (but never sent) a letter to Heisenberg, stating that he never understood the purpose of Heisenberg's visit, was shocked by Heisenberg's opinion that Germany would win the war, and that atomic weapons could be decisive.

Michael Frayn's 1998 play "Copenhagen" explores what might have happened at the 1941 meeting between Heisenberg and Bohr. A BBC television film version of the play was first screened on 26 September 2002, with Stephen Rea as Bohr, and Daniel Craig as Heisenberg. The same meeting had previously been dramatised by the BBC's "Horizon" science documentary series in 1992, with Anthony Bate as Bohr, and Philip Anthony as Heisenberg. The meeting is also dramatized in the Norwegian/Danish/British miniseries "The Heavy Water War".

In September 1943, word reached Bohr and his brother Harald that the Nazis considered their family to be Jewish, since their mother was Jewish, and that they were therefore in danger of being arrested. The Danish resistance helped Bohr and his wife escape by sea to Sweden on 29 September. The next day, Bohr persuaded King Gustaf V of Sweden to make public Sweden's willingness to provide asylum to Jewish refugees. On 2 October 1943, Swedish radio broadcast that Sweden was ready to offer asylum, and the mass rescue of the Danish Jews by their countrymen followed swiftly thereafter. Some historians claim that Bohr's actions led directly to the mass rescue, while others say that, though Bohr did all that he could for his countrymen, his actions were not a decisive influence on the wider events. Eventually, over 7,000 Danish Jews escaped to Sweden.
When the news of Bohr's escape reached Britain, Lord Cherwell sent a telegram to Bohr asking him to come to Britain. Bohr arrived in Scotland on 6 October in a de Havilland Mosquito operated by the British Overseas Airways Corporation (BOAC). The Mosquitos were unarmed high-speed bomber aircraft that had been converted to carry small, valuable cargoes or important passengers. By flying at high speed and high altitude, they could cross German-occupied Norway, and yet avoid German fighters. Bohr, equipped with parachute, flying suit and oxygen mask, spent the three-hour flight lying on a mattress in the aircraft's bomb bay. During the flight, Bohr did not wear his flying helmet as it was too small, and consequently did not hear the pilot's intercom instruction to turn on his oxygen supply when the aircraft climbed to high altitude to overfly Norway. He passed out from oxygen starvation and only revived when the aircraft descended to lower altitude over the North Sea. Bohr's son Aage followed his father to Britain on another flight a week later, and became his personal assistant.

Bohr was warmly received by James Chadwick and Sir John Anderson, but for security reasons Bohr was kept out of sight. He was given an apartment at St James's Palace and an office with the British Tube Alloys nuclear weapons development team. Bohr was astonished at the amount of progress that had been made. Chadwick arranged for Bohr to visit the United States as a Tube Alloys consultant, with Aage as his assistant. On 8 December 1943, Bohr arrived in Washington, D.C., where he met with the director of the Manhattan Project, Brigadier General Leslie R. Groves, Jr. He visited Einstein and Pauli at the Institute for Advanced Study in Princeton, New Jersey, and went to Los Alamos in New Mexico, where the nuclear weapons were being designed. For security reasons, he went under the name of "Nicholas Baker" in the United States, while Aage became "James Baker". In May 1944 the Danish resistance newspaper De frie Danske reported that they had learned that 'the famous son of Denmark Professor Niels Bohr' in October the previous year had fled his country via Sweden to London and from there travelled to Moscow from where he could be assumed to support the war effort.

Bohr did not remain at Los Alamos, but paid a series of extended visits over the course of the next two years. Robert Oppenheimer credited Bohr with acting "as a scientific father figure to the younger men", most notably Richard Feynman. Bohr is quoted as saying, "They didn't need my help in making the atom bomb." Oppenheimer gave Bohr credit for an important contribution to the work on modulated neutron initiators. "This device remained a stubborn puzzle," Oppenheimer noted, "but in early February 1945 Niels Bohr clarified what had to be done."

Bohr recognised early that nuclear weapons would change international relations. In April 1944, he received a letter from Peter Kapitza, written some months before when Bohr was in Sweden, inviting him to come to the Soviet Union. The letter convinced Bohr that the Soviets were aware of the Anglo-American project, and would strive to catch up. He sent Kapitza a non-committal response, which he showed to the authorities in Britain before posting. Bohr met Churchill on 16 May 1944, but found that "we did not speak the same language". Churchill disagreed with the idea of openness towards the Russians to the point that he wrote in a letter: "It seems to me Bohr ought to be confined or at any rate made to see that he is very near the edge of mortal crimes."

Oppenheimer suggested that Bohr visit President Franklin D. Roosevelt to convince him that the Manhattan Project should be shared with the Soviets in the hope of speeding up its results. Bohr's friend, Supreme Court Justice Felix Frankfurter, informed President Roosevelt about Bohr's opinions, and a meeting between them took place on 26 August 1944. Roosevelt suggested that Bohr return to the United Kingdom to try to win British approval. When Churchill and Roosevelt met at Hyde Park on 19 September 1944, they rejected the idea of informing the world about the project, and the aide-mémoire of their conversation contained a rider that "enquiries should be made regarding the activities of Professor Bohr and steps taken to ensure that he is responsible for no leakage of information, particularly to the Russians".

In June 1950, Bohr addressed an "Open Letter" to the United Nations calling for international cooperation on nuclear energy. In the 1950s, after the Soviet Union's first nuclear weapon test, the International Atomic Energy Agency was created along the lines of Bohr's suggestion. In 1957 he received the first ever Atoms for Peace Award.

With the war now ended, Bohr returned to Copenhagen on 25 August 1945, and was re-elected President of the Royal Danish Academy of Arts and Sciences on 21 September. At a memorial meeting of the Academy on 17 October 1947 for King Christian X, who had died in April, the new king, Frederick IX, announced that he was conferring the Order of the Elephant on Bohr. This award was normally awarded only to royalty and heads of state, but the king said that it honoured not just Bohr personally, but Danish science. Bohr designed his own coat of arms which featured a taijitu (symbol of yin and yang) and a motto in , "opposites are complementary".

The Second World War demonstrated that science, and physics in particular, now required considerable financial and material resources. To avoid a brain drain to the United States, twelve European countries banded together to create CERN, a research organisation along the lines of the national laboratories in the United States, designed to undertake Big Science projects beyond the resources of any one of them alone. Questions soon arose regarding the best location for the facilities. Bohr and Kramers felt that the Institute in Copenhagen would be the ideal site. Pierre Auger, who organised the preliminary discussions, disagreed; he felt that both Bohr and his Institute were past their prime, and that Bohr's presence would overshadow others. After a long debate, Bohr pledged his support to CERN in February 1952, and Geneva was chosen as the site in October. The CERN Theory Group was based in Copenhagen until their new accommodation in Geneva was ready in 1957. Victor Weisskopf, who later became the Director General of CERN, summed up Bohr's role, saying that "there were other personalities who started and conceived the idea of CERN. The enthusiasm and ideas of the other people would not have been enough, however, if a man of his stature had not supported it."

Meanwhile, Scandinavian countries formed the Nordic Institute for Theoretical Physics in 1957, with Bohr as its chairman. He was also involved with the founding of the Research Establishment Risø of the Danish Atomic Energy Commission, and served as its first chairman from February 1956.

Bohr died of heart failure at his home in Carlsberg on 18 November 1962. He was cremated, and his ashes were buried in the family plot in the Assistens Cemetery in the Nørrebro section of Copenhagen, along with those of his parents, his brother Harald, and his son Christian. Years later, his wife's ashes were also interred there. On 7 October 1965, on what would have been his 80th birthday, the Institute for Theoretical Physics at the University of Copenhagen was officially renamed to what it had been called unofficially for many years: the Niels Bohr Institute.

Bohr received numerous honours and accolades. In addition to the Nobel Prize, he received the Hughes Medal in 1921, the Matteucci Medal in 1923, the Franklin Medal in 1926, the Copley Medal in 1938, the Order of the Elephant in 1947, the Atoms for Peace Award in 1957 and the Sonning Prize in 1961. He became foreign member of the Royal Netherlands Academy of Arts and Sciences in 1923, and of the Royal Society in 1926. The Bohr model's semicentennial was commemorated in Denmark on 21 November 1963 with a postage stamp depicting Bohr, the hydrogen atom and the formula for the difference of any two hydrogen energy levels: formula_3. Several other countries have also issued postage stamps depicting Bohr. In 1997, the Danish National Bank began circulating the 500-krone banknote with the portrait of Bohr smoking a pipe. An asteroid, 3948 Bohr, was named after him, as was the Bohr lunar crater and bohrium, the chemical element with atomic number 107.






</doc>
<doc id="21211" url="https://en.wikipedia.org/wiki?curid=21211" title="National Football League">
National Football League

The National Football League (NFL) is a professional American football league consisting of 32 teams, divided equally between the National Football Conference (NFC) and the American Football Conference (AFC). The NFL is one of the four major North American professional sports leagues, the highest professional level of American football in the world, the wealthiest professional sport league by revenue, and the sport league with the most valuable teams. The NFL's 17-week regular season runs from early September to late December, with each team playing 16 games and having one bye week. Following the conclusion of the regular season, seven teams from each conference (four division winners and three wild card teams) advance to the playoffs, a single-elimination tournament culminating in the Super Bowl, which is usually held on the first Sunday in February and is played between the champions of the NFC and AFC.

The NFL was formed in 1920 as the American Professional Football Association (APFA) before renaming itself the National Football League for the 1922 season. After initially determining champions through end-of-season standings, a playoff system was implemented in 1933 that culminated with the until 1966. Following an agreement to merge the NFL with the American Football League (AFL), the Super Bowl was first held in 1967 to determine a champion between the best teams from the two leagues and has remained as the final game of each NFL season since the merger was completed in 1970. Today, the NFL has the highest average attendance (67,591) of any professional sports league in the world and is the most popular sports league in the United States. The Super Bowl is also among the biggest club sporting events in the world, with the individual games accounting for many of the most watched television programs in American history and all occupying the Nielsen's Top 5 tally of the all-time most watched U.S. television broadcasts by 2015.

The Green Bay Packers hold the most combined NFL championships with 13, winning nine titles before the Super Bowl era and four Super Bowls afterwards. Since the creation of the Super Bowl, the Pittsburgh Steelers and New England Patriots both have the most championship titles at six.

On August 20, 1920, a meeting was held by representatives of the Akron Pros, Canton Bulldogs, Cleveland Indians, and Dayton Triangles at the Jordan and Hupmobile auto showroom in Canton, Ohio. This meeting resulted in the formation of the American Professional Football Conference (APFC), a group who, according to the "Canton Evening Repository", intended to "raise the standard of professional football in every way possible, to eliminate bidding for players between rival clubs and to secure cooperation in the formation of schedules".

Another meeting was held on September 17, 1920 with representatives from teams from four states: Akron, Canton, Cleveland, and Dayton from Ohio; the Hammond Pros and Muncie Flyers from Indiana; the Rochester Jeffersons from New York; and the Rock Island Independents, Decatur Staleys, and Racine (Chicago) Cardinals from Illinois. The league was renamed to the American Professional Football Association (APFA). The league elected Jim Thorpe as its first president, and consisted of 14 teams (the Buffalo All-Americans, Chicago Tigers, Columbus Panhandles, and Detroit Heralds joined the league during the year). The Massillon Tigers from Massillon, Ohio was also at the September 17 meeting, but did not field a team in 1920. Only two of these teams, the Decatur Staleys (now the Chicago Bears) and the Chicago Cardinals (now the Arizona Cardinals), remain.
Although the league did not maintain official standings for its 1920 inaugural season and teams played schedules that included non-league opponents, the APFA awarded the Akron Pros the championship by virtue of their (8 wins, 0 losses, and 3 ties) record. The first event occurred on September 26, 1920 when the Rock Island Independents defeated the non-league St. Paul Ideals 48–0 at Douglas Park. On October 3, 1920, the first full week of league play occurred.
The following season resulted in the Chicago Staleys controversially winning the title over the Buffalo All-Americans. On June 24, 1922, the APFA changed its name to the National Football League (NFL).

In 1932, the season ended with the Chicago Bears () and the Portsmouth Spartans () tied for first in the league standings. At the time, teams were ranked on a single table and the team with the highest winning percentage (not including ties, which were not counted towards the standings) at the end of the season was declared the champion; the only tiebreaker was that in the event of a tie, if two teams played twice in a season, the result of the second game determined the title (the source of the 1921 controversy). This method had been used since the league's creation in 1920, but no situation had been encountered where two teams were tied for first. The league quickly determined that a playoff game between Chicago and Portsmouth was needed to decide the league's champion. The teams were originally scheduled to play the playoff game, officially a regular season game that would count towards the regular season standings, at Wrigley Field in Chicago, but a combination of heavy snow and extreme cold forced the game to be moved indoors to Chicago Stadium, which did not have a regulation-size football field. Playing with altered rules to accommodate the smaller playing field, the Bears won the game 9–0 and thus won the championship. Fan interest in the "de facto" championship game led the NFL, beginning in 1933, to split into two divisions with a championship game to be played between the division champions. The 1934 season also marked the first of 12 seasons in which African Americans were absent from the league. The "de facto" ban was rescinded in 1946, following public pressure and coinciding with the removal of a similar ban in Major League Baseball.

The NFL was always the foremost professional football league in the United States; it nevertheless faced a large number of rival professional leagues through the 1930s and 1940s. Rival leagues included at least three separate American Football Leagues and the All-America Football Conference (AAFC), on top of various regional leagues of varying caliber. Three NFL teams trace their histories to these rival leagues, including the Los Angeles Rams (who came from a 1936 iteration of the American Football League), the Cleveland Browns and San Francisco 49ers (the last two of which came from the AAFC). By the 1950s, the NFL had an effective monopoly on professional football in the United States; its only competition in North America was the professional Canadian football circuit, which formally became the Canadian Football League (CFL) in 1958. With Canadian football being a different football code than the American game, the CFL established a niche market in Canada and still survives as an independent league.

A new professional league, the fourth American Football League (AFL), began play in 1960. The upstart AFL began to challenge the established NFL in popularity, gaining lucrative television contracts and engaging in a bidding war with the NFL for free agents and draft picks. The two leagues announced a merger on June 8, 1966, to take full effect in 1970. In the meantime, the leagues would hold a common draft and championship game. The game, the Super Bowl, was held four times before the merger, with the NFL winning Super Bowl I and Super Bowl II, and the AFL winning Super Bowl III and Super Bowl IV. After the league merged, it was reorganized into two conferences: the National Football Conference (NFC), consisting of most of the pre-merger NFL teams, and the American Football Conference (AFC), consisting of all of the AFL teams as well as three pre-merger NFL teams.

Today, the NFL is the most popular sports league in North America; much of its growth is attributed to former Commissioner Pete Rozelle, who led the league from 1960 to 1989. Overall annual attendance increased from 3 million at the beginning of his tenure to 17 million by the end of his tenure, and 400 million global viewers watched 1989's Super Bowl XXIII. The NFL established NFL Properties in 1963. The league's licensing wing, NFL Properties earns the league billions of dollars annually; Rozelle's tenure also marked the creation of NFL Charities and a national partnership with United Way. Paul Tagliabue was elected as commissioner to succeed Rozelle; his seventeen-year tenure, which ended in 2006, was marked by large increases in television contracts and the addition of four expansion teams, as well as the introduction of league initiatives to increase the number of minorities in league and team management roles. The league's current Commissioner, Roger Goodell, has focused on reducing the number of illegal hits and making the sport safer, mainly through fining or suspending players who break rules. These actions are among many the NFL is taking to reduce concussions and improve player safety.

From 1920 to 1934, the NFL did not have a set number of games for teams to play, instead setting a minimum. The league mandated a 12-game regular season for each team beginning in 1935, later shortening this to 11 games in 1937 and 10 games in 1943, mainly due to World War II. After the war ended, the number of games returned to 11 games in 1946 and to 12 in 1947. The NFL went to a 14-game schedule in 1961, which it retained until switching to the current 16-game schedule in 1978. Proposals to increase the regular season to 18 games have been made, but have been rejected in labor negotiations with the National Football League Players Association (NFLPA).

The NFL operated in a two-conference system from 1933 to 1966, where the champions of each conference would meet in the . If two teams tied for the conference lead, they would meet in a one-game playoff to determine the conference champion. In 1967, the NFL expanded from 15 teams to 16 teams. Instead of just evening out the conferences by adding the expansion New Orleans Saints to the seven-member Western Conference, the NFL realigned the conferences and split each into two four-team divisions. The four division champions would meet in the NFL playoffs, a two-round playoff. The NFL also operated the Playoff Bowl (officially the Bert Bell Benefit Bowl) from 1960 to 1969. Effectively a third-place game, pitting the two conference runners-up against each other, the league considers Playoff Bowls to have been exhibitions rather than playoff games. The league discontinued the Playoff Bowl in 1970 due to its perception as a game for losers.

Following the addition of the former AFL teams into the NFL in 1970, the NFL split into two conferences with three divisions each. The expanded league, now with twenty-six teams, would also feature an expanded eight-team playoff, the participants being the three division champions from each conference as well as one 'wild card' team (the team with the best win percentage) from each conference. In 1978, the league added a second wild card team from each conference, bringing the total number of playoff teams to ten, and a further two wild card teams were added in 1990 to bring the total to twelve. When the NFL expanded to 32 teams in 2002, the league realigned, changing the division structure from three divisions in each conference to four divisions in each conference. As each division champion gets a playoff bid, the number of wild card teams from each conference dropped from three to two. The playoffs expanded again in 2020, adding two more wild card teams to bring the total to fourteen playoff teams.

At the corporate level, the National Football League considers itself a trade association made up of and financed by its 32 member teams.<ref name="https://www.nytimes.com/2008/08/12/sports/football/12nfltax.html"></ref> Up until 2015, the league was an unincorporated nonprofit 501(c)(6) association. Section 501(c)(6) of the Internal Revenue Code provides an exemption from federal income taxation for "Business leagues, chambers of commerce, real-estate boards, boards of trade, or professional football leagues (whether or not administering a pension fund for football players), not organized for profit and no part of the net earnings of which inures to the benefit of any private shareholder or individual.". In contrast, each individual team (except the non-profit Green Bay Packers) is subject to tax because they make a profit.

The NFL gave up the tax exempt status in 2015 following public criticism; in a letter to the club owners, Commissioner Roger Goodell labeled it a "distraction", saying "the effects of the tax exempt status of the league office have been mischaracterized repeatedly in recent years... Every dollar of income generated through television rights fees, licensing agreements, sponsorships, ticket sales, and other means is earned by the 32 clubs and is taxable there. This will remain the case even when the league office and Management Council file returns as taxable entities, and the change in filing status will make no material difference to our business." As a result, the league office might owe around US $10 million in income taxes, but it is no longer required to disclose the salaries of its executive officers.

The league has three defined officers: the commissioner, secretary, and treasurer. Each conference has one defined officer, the president, which is essentially an honorary position with few powers and mostly ceremonial duties (such as awarding the conference championship trophy).

The commissioner is elected by affirmative vote of two-thirds or 18 (whichever is greater) of the members of the league, while the president of each conference is elected by an affirmative vote of three-fourths or ten of the conference members. The commissioner appoints the secretary and treasurer and has broad authority in disputes between clubs, players, coaches, and employees. He is the "principal executive officer" of the NFL and also has authority in hiring league employees, negotiating television contracts, disciplining individuals that own part or all of an NFL team, clubs, or employed individuals of an NFL club if they have violated league bylaws or committed "conduct detrimental to the welfare of the League or professional football". The commissioner can, in the event of misconduct by a party associated with the league, suspend individuals, hand down a fine of up to US$500,000, cancel contracts with the league, and award or strip teams of draft picks.

In extreme cases, the commissioner can offer recommendations to the NFL's Executive Committee up to and including the "cancellation or forfeiture" of a club's franchise or any other action he deems necessary. The commissioner can also issue sanctions up to and including a lifetime ban from the league if an individual connected to the NFL has bet on games or failed to notify the league of conspiracies or plans to bet on or fix games. The current Commissioner of the National Football League is Roger Goodell, who was elected in 2006 after Paul Tagliabue, the previous commissioner, retired.

NFL revenue is from three primary sources: NFL Ventures (merchandising), NFL Enterprises (NFL Network and NFL Sunday Ticket, which the league controls), and the television contract. The league distributes such revenue equally among teams, regardless of performance. each team receives $255 million annually from the league's television contract, up 150% from $99.9 million in 2010.

Most NFL teams' financial statements are secret. "The Kansas City Star" obtained the Kansas City Chiefs' tax returns for 2008–2010. According to the "Star", the team's revenue rose from $231 million in 2008 to $302 million in 2010. In 2010, two thirds of revenue came from the league: $99.8 million from NFL Ventures ($55.3 million) and NFL Enterprises ($44.6 million), and the $99.9 million share of the television contract. The remaining one third was from tickets ($42.4 million), corporate sponsorships ($6.6 million), food sales ($5 million), parking passes ($4.7 million), in-stadium advertising ($3.7 million), radio contract ($2.7 million), and miscellaneous sources.

The largest Chiefs expense in 2010 was $148 million for players, coaches, and other employees. Of the $38 million in operating income, Clark, Lamar Jr., two other children, and widow of former team owner Lamar Hunt divided $17.6 million, and reinvested the remaining $20 million into the team.

According to economist Richard D. Wolff, the NFL's revenue model is in contravention of the typical corporate structure. By redistributing profits to all teams the NFL is ensuring that one team will not dominate the league through excessive earnings. Roger Noll described the revenue sharing as the league's "most important structural weakness", however, as there is no disincentive against a team playing badly and the largest cost item, player salaries, is capped.

The NFL consists of 32 clubs divided into two conferences of 16 teams in each. Each conference is divided into four divisions of four clubs in each. During the regular season, each team is allowed a maximum of 55 players on its roster; only 48 of these may be active (eligible to play) on game days. Each team can also have a 12-player practice squad separate from its main roster.

Each NFL club is granted a franchise, the league's authorization for the team to operate in its home city. This franchise covers 'Home Territory' (the 75 miles surrounding the city limits, or, if the team is within 100 miles of another league city, half the distance between the two cities) and 'Home Marketing Area' (Home Territory plus the rest of the state the club operates in, as well as the area the team operates its training camp in for the duration of the camp). Each NFL member has the exclusive right to host professional football games inside its Home Territory and the exclusive right to advertise, promote, and host events in its Home Marketing Area. There are a couple of exceptions to this rule, mostly relating to teams with close proximity to each other: teams that operate in the same city (e.g. New York City and Los Angeles) or the same state (e.g. California, Florida, and Texas) share the rights to the city's Home Territory and the state's Home Marketing Area, respectively.

Every NFL team is based in the contiguous United States. Although no team is based in a foreign country, the Jacksonville Jaguars began playing one home game a year at Wembley Stadium in London, England, in 2013 as part of the NFL International Series. The Jaguars' agreement with Wembley was originally set to expire in 2016, but has since been extended through 2020. The Los Angeles Rams (2016–2019), Los Angeles Chargers (2018–2019) and the Las Vegas Raiders (2016–2019) also played one home game a year abroad as part of their agreement to relocate. The Buffalo Bills played one home game every season at Rogers Centre in Toronto, Ontario, Canada, as part of the Bills Toronto Series from to . Mexico also hosted an NFL regular-season game, a 2005 game between the San Francisco 49ers and Arizona Cardinals known as "Fútbol Americano", and 39 international preseason games were played from 1986 to 2005 as part of the American Bowl series. The Raiders and Houston Texans played a game in Mexico City at Estadio Azteca on November 21, 2016.

According to "Forbes", the Dallas Cowboys, at approximately US$5 billion, are the most valuable NFL franchise and the most valuable sports team in the world. Also, 26 of the 32 NFL teams rank among the Top 50 most valuable sports teams in the world; and 16 of the NFL's owners are listed on the Forbes 400, the most of any sports league or organization.

The 32 teams are organized into eight geographic divisions of four teams each. These divisions are further organized into two conferences, the National Football Conference and the American Football Conference. The two-conference structure has its origins in a time when major American professional football was organized into two independent leagues, the National Football League and its younger rival, the American Football League. The leagues merged in the late 1960s, adopting the older league's name and reorganizing slightly to ensure the same number of teams in both conferences.

The NFL season format consists of a four-week preseason, a seventeen-week regular season (each team plays 16 games), and a fourteen-team single-elimination playoff culminating in the Super Bowl, the league's championship game.

The NFL preseason begins with the Pro Football Hall of Fame Game, played at Fawcett Stadium in Canton.<ref name="NFL/Hall of Fame Game"></ref> Each NFL team is required to schedule four preseason games, two of which must be at its designated home stadium, but the teams involved in the Hall of Fame game, as well as any teams that played in an American Bowl game, play five preseason games. Preseason games are exhibition matches and do not count towards regular-season totals. Because the preseason does not count towards standings, teams generally do not focus on winning games; instead, they are used by coaches to evaluate their teams and by players to show their performance, both to their current team and to other teams if they get cut. The quality of preseason games has been criticized by some fans, who dislike having to pay full price for exhibition games, as well as by some players and coaches, who dislike the risk of injury the games have, while others have felt the preseason is a necessary part of the NFL season.

Currently, the thirteen opponents each team faces over the 16-game regular season schedule are set using a pre-determined formula: The league runs a seventeen-week, 256-game regular season. Since 2001, the season has begun the week after Labor Day (first Monday in September) and concluded the week after Christmas. The opening game of the season is normally a home game on a Thursday for the league's defending champion.

Most NFL games are played on Sundays, with a Monday night game typically held at least once a week and Thursday night games occurring on most weeks as well. NFL games are not normally played on Fridays or Saturdays until late in the regular season, as federal law prohibits professional football leagues from competing with college or high school football. Because high school and college teams typically play games on Friday and Saturday, respectively, the NFL cannot hold games on those days until the Friday before the third Saturday in December. While Saturday games late in the season are common, the league rarely holds Friday games, the most recent one being on Christmas Day in 2009. NFL games are rarely scheduled for Tuesday or Wednesday, and those days have only been used twice since 1948: in 2010, when a Sunday game was rescheduled to Tuesday due to a blizzard, and in 2012, when the Kickoff game was moved from Thursday to Wednesday to avoid conflict with the Democratic National Convention.

NFL regular season matchups are determined according to a scheduling formula. Within a division, all four teams play fourteen out of their sixteen games against common opponents – two games (home and away) are played against the other three teams in the division, while one game is held against all the members of a division from the NFC and a division from the AFC as determined by a rotating cycle (three years for the conference the team is in, and four years in the conference they are not in). The other two games are intraconference games, determined by the standings of the previous year – for example, if a team finishes first in its division, it will play two other first-place teams in its conference, while a team that finishes last would play two other last-place teams in the conference. In total, each team plays sixteen games and has one bye week, where they do not play any games.

Although a team's home and away opponents are known by the end of the previous year's regular season, the exact dates and times for NFL games are not determined until much later because the league has to account for, among other things, the Major League Baseball postseason and local events that could pose a scheduling conflict with NFL games. During the 2010 season, over 500,000 potential schedules were created by computers, 5,000 of which were considered "playable schedules" and were reviewed by the NFL's scheduling team. After arriving at what they felt was the best schedule out of the group, nearly 50 more potential schedules were developed to try to ensure that the chosen schedule would be the best possible one.

Following the conclusion of the regular season, the NFL Playoffs, a fourteen-team single elimination tournament, is then held. Seven teams are selected from each conference: the winners of each of the four divisions as well as three wild card teams (the three remaining teams with the best overall record). These teams are seeded according to overall record, with the division champions always ranking higher than any of the wild card teams. The top team (seeded one) from each conference are awarded a bye week, while the remaining six teams (seeded 2–7) from each conference compete in the first round of the playoffs, the Wild Card round, with the second seed competing against the seventh, the third seed competing against the sixth seed and the fourth seed competing against the fifth seed. The winners of the Wild Card round advance to the Divisional Round, which matches the lower seeded team against the first seed and the higher seeded team against the second seed. The winners of those games then compete in the Conference Championships, with the higher remaining seed hosting the lower remaining seed. The AFC and NFC champions then compete in the Super Bowl to determine the league champion.

The only other postseason event hosted by the NFL is the Pro Bowl, the league's all-star game. Since 2009, the Pro Bowl has been held the week before the Super Bowl; in previous years, the game was held the week following the Super Bowl, but in an effort to boost ratings, the game was moved to the week before. Because of this, players from the teams participating in the Super Bowl are exempt from participating in the game. The Pro Bowl is not considered as competitive as a regular-season game because the biggest concern of teams is to avoid injuries to the players.

The National Football League has used three different trophies to honor its champion over its existence. The first trophy, the Brunswick-Balke Collender Cup, was donated to the NFL (then APFA) in 1920 by the Brunswick-Balke Collender Corporation. The trophy, the appearance of which is only known by its description as a "silver loving cup", was intended to be a traveling trophy and not to become permanent until a team had won at least three titles. The league awarded it to the Akron Pros, champions of the inaugural 1920 season; however, the trophy was discontinued and its current whereabouts are unknown.

A second trophy, the Ed Thorp Memorial Trophy, was issued by the NFL from 1934 to 1967. The trophy's namesake, Ed Thorp, was a referee in the league and a friend to many early league owners; upon his death in 1934, the league created the trophy to honor him. In addition to the main trophy, which would be in the possession of the current league champion, the league issued a smaller replica trophy to each champion, who would maintain permanent control over it. The current location of the Ed Thorp Memorial Trophy, long thought to be lost, is believed to be possessed by the Green Bay Packers Hall of Fame.

The current trophy of the NFL is the Vince Lombardi Trophy. The Super Bowl trophy was officially renamed in 1970 after Vince Lombardi, who as head coach led the Green Bay Packers to victories in the first two Super Bowls. Unlike the previous trophies, a new Vince Lombardi Trophy is issued to each year's champion, who maintains permanent control of it. Lombardi Trophies are made by Tiffany & Co. out of sterling silver and are worth anywhere from US$25,000 to US$300,000. Additionally, each player on the winning team as well as coaches and personnel are awarded Super Bowl rings to commemorate their victory. The winning team chooses the company that makes the rings; each ring design varies, with the NFL mandating certain ring specifications (which have a degree of room for deviation), in addition to requiring the Super Bowl logo be on at least one side of the ring. The losing team are also awarded rings, which must be no more than half as valuable as the winners' rings, but those are almost never worn.

The conference champions receive trophies for their achievement. The champions of the NFC receive the George Halas Trophy, named after Chicago Bears founder George Halas, who is also considered as one of the co-founders of the NFL. The AFC champions receive the Lamar Hunt Trophy, named after Lamar Hunt, the founder of the Kansas City Chiefs and the principal founder of the American Football League. Players on the winning team also receive a conference championship ring.

The NFL recognizes a number of awards for its players and coaches at its annual NFL Honors presentation. The most prestigious award is the AP Most Valuable Player (MVP) award. Other major awards include the AP Offensive Player of the Year, AP Defensive Player of the Year, AP Comeback Player of the Year, and the AP Offensive and Defensive Rookie of the Year awards. Another prestigious award is the Walter Payton Man of the Year Award, which recognizes a player's off-field work in addition to his on-field performance. The NFL Coach of the Year award is the highest coaching award. The NFL also gives out weekly awards such as the FedEx Air & Ground NFL Players of the Week and the Pepsi MAX NFL Rookie of the Week awards.

In the United States, the National Football League has television contracts with four networks: CBS, ESPN, Fox, and NBC. Collectively, these contracts cover every regular season and postseason game. In general, CBS televises afternoon games in which the away team is an AFC team, and Fox carries afternoon games in which the away team belongs to the NFC. These afternoon games are not carried on all affiliates, as multiple games are being played at once; each network affiliate is assigned one game per time slot, according to a complicated set of rules. Since 2011, the league has reserved the right to give Sunday games that, under the contract, would normally air on one network to the other network (known as "flexible scheduling"). The only way to legally watch a regionally televised game not being carried on the local network affiliates is to purchase NFL Sunday Ticket, the league's out-of-market sports package, which is only available to subscribers to the DirecTV satellite service. The league also provides RedZone, an omnibus telecast that cuts to the most relevant plays in each game, live as they happen.

In addition to the regional games, the league also has packages of telecasts, mostly in prime time, that are carried nationwide. NBC broadcasts the primetime "Sunday Night Football" package', which includes the Thursday NFL Kickoff game that starts the regular season and a primetime Thanksgiving Day game. ESPN carries all Monday Night Football games. The NFL's own network, NFL Network, broadcasts a series titled "Thursday Night Football", which was originally exclusive to the network, but which in recent years has had several games simulcast on CBS (since 2014) and NBC (since 2016) (except the Thanksgiving and kickoff games, which remain exclusive to NBC). For the 2017 season, the NFL Network will broadcast 18 regular season games under its "Thursday Night Football" brand, 16 Thursday-evening contests (10 of which are simulcast on either NBC or CBS) as well as one of the NFL International Series games on a Sunday morning and one of the 2017 Christmas afternoon games. In addition, 10 of the Thursday night games will be streamed live on Amazon Prime. In 2017, the NFL games occupied the top three rates for a 30-second advertisement: $699,602 for Sunday Night Football, $550,709 for Thursday Night Football (NBC), and $549,791 for Thursday Night Football (CBS).

The Super Bowl television rights are rotated on a three-year basis between CBS, Fox, and NBC. In 2011, all four stations signed new nine-year contracts with the NFL, each running until 2022; CBS, Fox, and NBC are estimated by "Forbes" to pay a combined total of US$3 billion a year, while ESPN will pay US$1.9 billion a year. The league also has deals with Spanish-language broadcasters NBC Universo, Fox Deportes, and ESPN Deportes, which air Spanish language dubs of their respective English-language sister networks' games. The league's contracts do not cover preseason games, which individual teams are free to sell to local stations directly; a minority of preseason games are distributed among the league's national television partners.

Through the 2014 season, the NFL had a blackout policy in which games were 'blacked out' on local television in the home team's area if the home stadium was not sold out. Clubs could elect to set this requirement at only 85%, but they would have to give more ticket revenue to the visiting team; teams could also request a specific exemption from the NFL for the game. The vast majority of NFL games were not blacked out; only 6% of games were blacked out during the 2011 season, and only two games were blacked out in and none in . The NFL announced in March 2015 that it would suspend its blackout policy for at least the 2015 season. According to Nielsen, the NFL regular season since 2012 was watched by at least 200 million individuals, accounting for 80% of all television households in the United States and 69% of all potential viewers in the United States. NFL regular season games accounted for 31 out of the top 32 most-watched programs in the fall season and an NFL game ranked as the most-watched television show in all 17 weeks of the regular season. At the local level, NFL games were the highest-ranked shows in NFL markets 92% of the time. Super Bowls account for the 22 most-watched programs (based on total audience) in US history, including a record 167 million people that watched Super Bowl XLVIII, the conclusion to the 2013 season.

In addition to radio networks run by each NFL team, select NFL games are broadcast nationally by Westwood One (known as Dial Global for the 2012 season). These games are broadcast on over 500 networks, giving all NFL markets access to each primetime game. The NFL's deal with Westwood One was extended in 2012 and will run through 2017.

Some broadcasting innovations have either been introduced or popularized during NFL telecasts. Among them, the Skycam camera system was used for the first time in a live telecast, at a 1984 preseason NFL game in San Diego between the Chargers and 49ers, and televised by CBS. Commentator John Madden famously used a telestrator during games between the early 1980s to the mid-2000s, boosting the device's popularity.

The NFL, as a one-time experiment, distributed the October 25, 2015 International Series game from Wembley Stadium in London between the Buffalo Bills and Jacksonville Jaguars. The game was live streamed on the Internet exclusively via Yahoo!, except for over-the-air broadcasts on the local CBS-TV affiliates in the Buffalo and Jacksonville markets.

In 2015, the NFL began sponsoring a series of public service announcements to bring attention to domestic abuse and sexual assault in response to what was seen as poor handling of incidents of violence by players.

Each April (excluding 2014 when it took place in May), the NFL holds a draft of college players. The draft consists of seven rounds, with each of the 32 clubs getting one pick in each round. The draft order for non-playoff teams is determined by regular-season record; among playoff teams, teams are first ranked by the furthest round of the playoffs they reached, and then are ranked by regular-season record. For example, any team that reached the divisional round will be given a higher pick than any team that reached the conference championships, but will be given a lower pick than any team that did not make the divisional round. The Super Bowl champion always drafts last, and the losing team from the Super Bowl always drafts next-to-last. All potential draftees must be at least three years removed from high school in order to be eligible for the draft. Underclassmen that have met that criterion to be eligible for the draft must write an application to the NFL by January 15 renouncing their remaining college eligibility. Clubs can trade away picks for future draft picks, but cannot trade the rights to players they have selected in previous drafts.
Aside from the 7 picks each club gets, compensatory draft picks are given to teams that have lost more compensatory free agents than they have gained. These are spread out from rounds 3 to 7, and a total of 32 are given. Clubs are required to make their selection within a certain period of time, the exact time depending on which round the pick is made in. If they fail to do so on time, the clubs behind them can begin to select their players in order, but they do not lose the pick outright. This happened in the 2003 draft, when the Minnesota Vikings failed to make their selection on time. The Jacksonville Jaguars and Carolina Panthers were able to make their picks before the Vikings were able to use theirs. Selected players are only allowed to negotiate contracts with the team that picked them, but if they choose not to sign they become eligible for the next year's draft. Under the current collective bargaining contract, all contracts to drafted players must be four-year deals with a club option for a fifth. Contracts themselves are limited to a certain amount of money, depending on the exact draft pick the player was selected with. Players who were draft eligible but not picked in the draft are free to sign with any club.

The NFL operates several other drafts in addition to the NFL draft. The league holds a supplemental draft annually. Clubs submit emails to the league stating the player they wish to select and the round they will do so, and the team with the highest bid wins the rights to that player. The exact order is determined by a lottery held before the draft, and a successful bid for a player will result in the team forfeiting the rights to its pick in the equivalent round of the next NFL draft. Players are only eligible for the supplemental draft after being granted a petition for special eligibility. The league holds expansion drafts, the most recent happening in 2002 when the Houston Texans began play as an expansion team. Other drafts held by the league include an allocation draft in 1950 to allocate players from several teams that played in the dissolved All-America Football Conference and a supplemental draft in 1984 to give NFL teams the rights to players who had been eligible for the main draft but had not been drafted because they had signed contracts with the United States Football League or Canadian Football League.

Like the other major sports leagues in the United States, the NFL maintains protocol for a disaster draft. In the event of a 'near disaster' (less than 15 players killed or disabled) that caused the club to lose a quarterback, they could draft one from a team with at least three quarterbacks. In the event of a 'disaster' (15 or more players killed or disabled) that results in a club's season being canceled, a restocking draft would be held. Neither of these protocols has ever had to be implemented.

Free agents in the National Football League are divided into restricted free agents, who have three accrued seasons and whose current contract has expired, and unrestricted free agents, who have four or more accrued seasons and whose contract has expired. An accrued season is defined as "six or more regular-season games on a club's active/inactive, reserved/injured or reserve/physically unable to perform lists". Restricted free agents are allowed to negotiate with other clubs besides their former club, but the former club has the right to match any offer. If they choose not to, they are compensated with draft picks. Unrestricted free agents are free to sign with any club, and no compensation is owed if they sign with a different club.

Clubs are given one franchise tag to offer to any unrestricted free agent. The franchise tag is a one-year deal that pays the player 120% of his previous contract or no less than the average of the five highest-paid players at his position, whichever is greater. There are two types of franchise tags: exclusive tags, which do not allow the player to negotiate with other clubs, and non-exclusive tags, which allow the player to negotiate with other clubs but gives his former club the right to match any offer and two first-round draft picks if they decline to match it.

Clubs also have the option to use a transition tag, which is similar to the non-exclusive franchise tag but offers no compensation if the former club refuses to match the offer. Due to that stipulation, the transition tag is rarely used, even with the removal of the "poison pill" strategy (offering a contract with stipulations that the former club would be unable to match) that essentially ended the usage of the tag league-wide. Each club is subject to a salary cap, which is set at US$188.2 million for the 2019 season, US$11 million more than that of 2018.

Members of clubs' practice squads, despite being paid by and working for their respective clubs, are also simultaneously a kind of free agent and are able to sign to any other club's active roster (provided their new club is not their previous club's next opponent within a set number of days) without compensation to their previous club; practice squad players cannot be signed to other clubs' practice squads, however, unless released by their original club first.




</doc>
<doc id="21212" url="https://en.wikipedia.org/wiki?curid=21212" title="Nazi Germany">
Nazi Germany

Nazi Germany, officially known as the German Reich until 1943 and Greater German Reich in 1943–45, was the German state between 1933 and 1945, when Adolf Hitler and the Nazi Party (NSDAP) controlled the country which they transformed into a dictatorship. Under Hitler's rule, Germany quickly became a totalitarian state where nearly all aspects of life were controlled by the government. The Third Reich – meaning "Third Realm" or "Third Empire" – alluded to the Nazis' conceit that Nazi Germany was the successor to the earlier Holy Roman Empire (800–1806) and German Empire (1871–1918). The Third Reich, which Hitler and the Nazis referred to as the Thousand Year Reich, ended in May 1945 after just 12 years, when the Allies defeated Germany, ending World War II in Europe.

On 30 January 1933, Hitler was appointed Chancellor of Germany, the head of government, by the President of the Weimar Republic, Paul von Hindenburg, the head of State. The Nazi Party then began to eliminate all political opposition and consolidate its power. Hindenburg died on 2 August 1934 and Hitler became dictator of Germany by merging the offices and powers of the Chancellery and Presidency. A national referendum held 19 August 1934 confirmed Hitler as sole "Führer" (Leader) of Germany. All power was centralised in Hitler's person and his word became the highest law. The government was not a coordinated, co-operating body, but a collection of factions struggling for power and Hitler's favour. In the midst of the Great Depression, the Nazis restored economic stability and ended mass unemployment using heavy military spending and a mixed economy. Using deficit spending, the regime undertook a massive secret rearmament program and the construction of extensive public works projects, including the construction of "Autobahnen" (motorways). The return to economic stability boosted the regime's popularity.

Racism, Nazi eugenics, and especially antisemitism, were central ideological features of the regime. The Germanic peoples were considered by the Nazis to be the master race, the purest branch of the Aryan race. Discrimination and the persecution of Jews and Romani people began in earnest after the seizure of power. The first concentration camps were established in March 1933. Jews and others deemed undesirable were imprisoned, and liberals, socialists, and communists were killed, imprisoned, or exiled. Christian churches and citizens that opposed Hitler's rule were oppressed and many leaders imprisoned. Education focused on racial biology, population policy, and fitness for military service. Career and educational opportunities for women were curtailed. Recreation and tourism were organised via the Strength Through Joy program, and the 1936 Summer Olympics showcased Germany on the international stage. Propaganda Minister Joseph Goebbels made effective use of film, mass rallies, and Hitler's hypnotic oratory to influence public opinion. The government-controlled artistic expression, promoting specific art forms and banning or discouraging others.

The Nazi regime dominated neighbours through military threats in the years leading up to war. Nazi Germany made increasingly aggressive territorial demands, threatening war if these were not met. It seized Austria and almost all of Czechoslovakia in 1938 and 1939. Germany signed a non-aggression pact with the Soviet Union and invaded Poland on 1 September 1939, launching World War II in Europe. By early 1941, Germany controlled much of Europe. "Reichskommissariats" took control of conquered areas and a German administration was established in the remainder of Poland. Germany exploited the raw materials and labour of both its occupied territories and its allies.

Genocide and mass murder became hallmarks of the regime. Starting in 1939, hundreds of thousands of German citizens with mental or physical disabilities were murdered in hospitals and asylums. "Einsatzgruppen" paramilitary death squads accompanied the German armed forces inside the occupied territories and conducted the mass killings of millions of Jews and other Holocaust victims. After 1941, millions of others were imprisoned, worked to death, or murdered in Nazi concentration camps and extermination camps. This genocide is known as the Holocaust.

While the German invasion of the Soviet Union in 1941 was initially successful, the Soviet resurgence and entry of the United States into the war meant that the "Wehrmacht" (German armed forces) lost the initiative on the Eastern Front in 1943 and by late 1944 had been pushed back to the pre-1939 border. Large-scale aerial bombing of Germany escalated in 1944 and the Axis powers were driven back in Eastern and Southern Europe. After the Allied invasion of France, Germany was conquered by the Soviet Union from the east and the other Allies from the west, and capitulated in May 1945. Hitler's refusal to admit defeat led to massive destruction of German infrastructure and additional war-related deaths in the closing months of the war. The victorious Allies initiated a policy of denazification and put many of the surviving Nazi leadership on trial for war crimes at the Nuremberg trials.

Common English terms for the German state in the Nazi era are "Nazi Germany" and "Third Reich". The latter, a translation of the Nazi propaganda term "Drittes Reich", was first used in "Das Dritte Reich", a 1923 book by Arthur Moeller van den Bruck. The book counted the Holy Roman Empire (962–1806) as the first Reich and the German Empire (1871–1918) as the second.

Germany was known as the Weimar Republic during the years 1919 to 1933. It was a republic with a semi-presidential system. The Weimar Republic faced numerous problems, including hyperinflation, political extremism (including violence from left- and right-wing paramilitaries), contentious relationships with the Allied victors of World War I, and a series of failed attempts at coalition government by divided political parties. Severe setbacks to the German economy began after World War I ended, partly because of reparations payments required under the 1919 Treaty of Versailles. The government printed money to make the payments and to repay the country's war debt, but the resulting hyperinflation led to inflated prices for consumer goods, economic chaos, and food riots. When the government defaulted on their reparations payments in January 1923, French troops occupied German industrial areas along the Ruhr and widespread civil unrest followed.

The National Socialist German Workers' Party ("Nationalsozialistische Deutsche Arbeiterpartei", NSDAP), commonly known as the Nazi Party, was founded in 1920. It was the renamed successor of the German Workers' Party (DAP) formed one year earlier, and one of several far-right political parties then active in Germany. The Nazi Party platform included destruction of the Weimar Republic, rejection of the terms of the Treaty of Versailles, radical antisemitism, and anti-Bolshevism. They promised a strong central government, increased "Lebensraum" ("living space") for Germanic peoples, formation of a national community based on race, and racial cleansing via the active suppression of Jews, who would be stripped of their citizenship and civil rights. The Nazis proposed national and cultural renewal based upon the "Völkisch" movement. The party, especially its paramilitary organisation "Sturmabteilung" (SA; Storm Detachment), or Brownshirts, used physical violence to advance their political position, disrupting the meetings of rival organisations and attacking their members as well as Jewish people on the streets. Such far-right armed groups were common in Bavaria, and were tolerated by the sympathetic far-right state government of Gustav Ritter von Kahr.

When the stock market in the United States crashed on 24 October 1929, the effect in Germany was dire. Millions were thrown out of work and several major banks collapsed. Hitler and the Nazis prepared to take advantage of the emergency to gain support for their party. They promised to strengthen the economy and provide jobs. Many voters decided the Nazi Party was capable of restoring order, quelling civil unrest, and improving Germany's international reputation. After the federal election of 1932, the party was the largest in the Reichstag, holding 230 seats with 37.4 percent of the popular vote.

Although the Nazis won the greatest share of the popular vote in the two Reichstag general elections of 1932, they did not have a majority. Hitler therefore led a short-lived coalition government formed with the German National People's Party. Under pressure from politicians, industrialists, and the business community, President Paul von Hindenburg appointed Hitler as Chancellor of Germany on 30 January 1933. This event is known as the "Machtergreifung" ("seizure of power").

On the night of 27 February 1933, the Reichstag building was set afire. Marinus van der Lubbe, a Dutch communist, was found guilty of starting the blaze. Hitler proclaimed that the arson marked the start of a communist uprising. The Reichstag Fire Decree, imposed on 28 February 1933, rescinded most civil liberties, including rights of assembly and freedom of the press. The decree also allowed the police to detain people indefinitely without charges. The legislation was accompanied by a propaganda campaign that led to public support for the measure. Violent suppression of communists by the SA was undertaken nationwide and 4,000 members of the Communist Party of Germany were arrested.

In March 1933, the Enabling Act, an amendment to the Weimar Constitution, passed in the Reichstag by a vote of 444 to 94. This amendment allowed Hitler and his cabinet to pass laws—even laws that violated the constitution—without the consent of the president or the Reichstag. As the bill required a two-thirds majority to pass, the Nazis used intimidation tactics as well as the provisions of the Reichstag Fire Decree to keep several Social Democratic deputies from attending, and the Communists had already been banned. On 10 May, the government seized the assets of the Social Democrats, and they were banned on 22 June. On 21 June, the SA raided the offices of the German National People's Party – their former coalition partners – which then disbanded on 29 June. The remaining major political parties followed suit. On 14 July 1933 Germany became a one-party state with the passage of a law decreeing the Nazi Party to be the sole legal party in Germany. The founding of new parties was also made illegal, and all remaining political parties which had not already been dissolved were banned. The Enabling Act would subsequently serve as the legal foundation for the dictatorship the Nazis established. Further elections in November 1933, 1936, and 1938 were Nazi-controlled, with only members of the Party and a small number of independents elected.

The Hitler cabinet used the terms of the Reichstag Fire Decree and later the Enabling Act to initiate the process of "Gleichschaltung" ("co-ordination"), which brought all aspects of life under party control. Individual states not controlled by elected Nazi governments or Nazi-led coalitions were forced to agree to the appointment of Reich Commissars to bring the states in line with the policies of the central government. These Commissars had the power to appoint and remove local governments, state parliaments, officials, and judges. In this way Germany became a "de facto" unitary state, with all state governments controlled by the central government under the Nazis. The state parliaments and the "Reichsrat" (federal upper house) were abolished in January 1934, with all state powers being transferred to the central government.

All civilian organisations, including agricultural groups, volunteer organisations, and sports clubs, had their leadership replaced with Nazi sympathisers or party members; these civic organisations either merged with the Nazi Party or faced dissolution. The Nazi government declared a "Day of National Labor" for May Day 1933, and invited many trade union delegates to Berlin for celebrations. The day after, SA stormtroopers demolished union offices around the country; all trade unions were forced to dissolve and their leaders were arrested. The Law for the Restoration of the Professional Civil Service, passed in April, removed from their jobs all teachers, professors, judges, magistrates, and government officials who were Jewish or whose commitment to the party was suspect. This meant the only non-political institutions not under control of the Nazis were the churches.

The Nazi regime abolished the symbols of the Weimar Republic—including the black, red, and gold tricolour flag—and adopted reworked symbolism. The previous imperial black, white, and red tricolour was restored as one of Germany's two official flags; the second was the swastika flag of the Nazi Party, which became the sole national flag in 1935. The Party anthem "Horst-Wessel-Lied" ("Horst Wessel Song") became a second national anthem.

Germany was still in a dire economic situation, as six million people were unemployed and the balance of trade deficit was daunting. Using deficit spending, public works projects were undertaken beginning in 1934, creating 1.7 million new jobs by the end of that year alone. Average wages began to rise.

The SA leadership continued to apply pressure for greater political and military power. In response, Hitler used the "Schutzstaffel" (SS) and Gestapo to purge the entire SA leadership. Hitler targeted SA "Stabschef" (Chief of Staff) Ernst Röhm and other SA leaders who—along with a number of Hitler's political adversaries (such as Gregor Strasser and former chancellor Kurt von Schleicher)—were arrested and shot. Up to 200 people were killed from 30 June to 2 July 1934 in an event that became known as the Night of the Long Knives.

On 2 August 1934, Hindenburg died. The previous day, the cabinet had enacted the "Law Concerning the Highest State Office of the Reich", which stated that upon Hindenburg's death the office of president would be abolished and its powers merged with those of the chancellor. Hitler thus became head of state as well as head of government and was formally named as "Führer und Reichskanzler" ("Leader and Chancellor"), although eventually "Reichskanzler" was dropped. Germany was now a totalitarian state with Hitler at its head. As head of state, Hitler became Supreme Commander of the armed forces. The new law provided an altered loyalty oath for servicemen so that they affirmed loyalty to Hitler personally rather than the office of supreme commander or the state. On 19 August, the merger of the presidency with the chancellorship was approved by 90 percent of the electorate in a plebiscite.
Most Germans were relieved that the conflicts and street fighting of the Weimar era had ended. They were deluged with propaganda orchestrated by Minister of Public Enlightenment and Propaganda Joseph Goebbels, who promised peace and plenty for all in a united, Marxist-free country without the constraints of the Versailles Treaty. The Nazi Party obtained and legitimised power through its initial revolutionary activities, then through manipulation of legal mechanisms, the use of police powers, and by taking control of the state and federal institutions. The first major Nazi concentration camp, initially for political prisoners, was opened at Dachau in 1933. Hundreds of camps of varying size and function were created by the end of the war.

Beginning in April 1933, scores of measures defining the status of Jews and their rights were instituted. These measures culminated in the establishment of the Nuremberg Laws of 1935, which stripped them of their basic rights. The Nazis would take from the Jews their wealth, their right to intermarry with non-Jews, and their right to occupy many fields of labour (such as law, medicine, or education). Eventually the Nazis declared the Jews as undesirable to remain among German citizens and society.

In the early years of the regime, Germany was without allies, and its military was drastically weakened by the Versailles Treaty. France, Poland, Italy, and the Soviet Union each had reasons to object to Hitler's rise to power. Poland suggested to France that the two nations engage in a preventive war against Germany in March 1933. Fascist Italy objected to German claims in the Balkans and on Austria, which Benito Mussolini considered to be in Italy's sphere of influence.

As early as February 1933, Hitler announced that rearmament must begin, albeit clandestinely at first, as to do so was in violation of the Versailles Treaty. On 17 May 1933, Hitler gave a speech before the Reichstag outlining his desire for world peace and accepted an offer from American President Franklin D. Roosevelt for military disarmament, provided the other nations of Europe did the same. When the other European powers failed to accept this offer, Hitler pulled Germany out of the World Disarmament Conference and the League of Nations in October, claiming its disarmament clauses were unfair if they applied only to Germany. In a referendum held in November, 95 percent of voters supported Germany's withdrawal.

In 1934, Hitler told his military leaders that a war in the east should begin in 1942. The Saarland, which had been placed under League of Nations supervision for 15 years at the end of World War I, voted in January 1935 to become part of Germany. In March 1935, Hitler announced the creation of an air force, and that the "Reichswehr" would be increased to 550,000 men. Britain agreed to Germany building a naval fleet with the signing of the Anglo-German Naval Agreement on 18 June 1935.

When the Italian invasion of Ethiopia led to only mild protests by the British and French governments, on 7 March 1936 Hitler used the Franco-Soviet Treaty of Mutual Assistance as a pretext to order the army to march 3,000 troops into the demilitarised zone in the Rhineland in violation of the Versailles Treaty. As the territory was part of Germany, the British and French governments did not feel that attempting to enforce the treaty was worth the risk of war. In the one-party election held on 29 March, the Nazis received 98.9 percent support. In 1936, Hitler signed an Anti-Comintern Pact with Japan and a non-aggression agreement with Mussolini, who was soon referring to a "Rome-Berlin Axis".

Hitler sent military supplies and assistance to the Nationalist forces of General Francisco Franco in the Spanish Civil War, which began in July 1936. The German Condor Legion included a range of aircraft and their crews, as well as a tank contingent. The aircraft of the Legion destroyed the city of Guernica in 1937. The Nationalists were victorious in 1939 and became an informal ally of Nazi Germany.

In February 1938, Hitler emphasised to Austrian Chancellor Kurt Schuschnigg the need for Germany to secure its frontiers. Schuschnigg scheduled a plebiscite regarding Austrian independence for 13 March, but Hitler sent an ultimatum to Schuschnigg on 11 March demanding that he hand over all power to the Austrian Nazi Party or face an invasion. German troops entered Austria the next day, to be greeted with enthusiasm by the populace.

The Republic of Czechoslovakia was home to a substantial minority of Germans, who lived mostly in the Sudetenland. Under pressure from separatist groups within the Sudeten German Party, the Czechoslovak government offered economic concessions to the region. Hitler decided not just to incorporate the Sudetenland into the Reich, but to destroy the country of Czechoslovakia entirely. The Nazis undertook a propaganda campaign to try to generate support for an invasion. Top German military leaders opposed the plan, as Germany was not yet ready for war.

The crisis led to war preparations by Britain, Czechoslovakia, and France (Czechoslovakia's ally). Attempting to avoid war, British Prime Minister Neville Chamberlain arranged a series of meetings, the result of which was the Munich Agreement, signed on 29 September 1938. The Czechoslovak government was forced to accept the Sudetenland's annexation into Germany. Chamberlain was greeted with cheers when he landed in London, saying the agreement brought "peace for our time". In addition to the German annexation, Poland seized a narrow strip of land near Cieszyn on 2 October, while as a consequence of the Munich Agreement, Hungary demanded and received along their northern border in the First Vienna Award on 2 November. Following negotiations with President Emil Hácha, Hitler seized the rest of the Czech half of the country on 15 March 1939 and created the Protectorate of Bohemia and Moravia, one day after the proclamation of the Slovak Republic in the Slovak half. Also on 15 March, Hungary occupied and annexed the recently proclaimed and unrecognized Carpatho-Ukraine and an additional sliver of land disputed with Slovakia.

Austrian and Czech foreign exchange reserves were seized by the Nazis, as were stockpiles of raw materials such as metals and completed goods such as weaponry and aircraft, which were shipped to Germany. The "Reichswerke Hermann Göring" industrial conglomerate took control of steel and coal production facilities in both countries.

In January 1934, Germany signed a non-aggression pact with Poland. In March 1939, Hitler demanded the return of the Free City of Danzig and the Polish Corridor, a strip of land that separated East Prussia from the rest of Germany. The British announced they would come to the aid of Poland if it was attacked. Hitler, believing the British would not actually take action, ordered an invasion plan should be readied for September 1939. On 23 May, Hitler described to his generals his overall plan of not only seizing the Polish Corridor but greatly expanding German territory eastward at the expense of Poland. He expected this time they would be met by force.

The Germans reaffirmed their alliance with Italy and signed non-aggression pacts with Denmark, Estonia, and Latvia whilst trade links were formalised with Romania, Norway, and Sweden. Foreign Minister Joachim von Ribbentrop arranged in negotiations with the Soviet Union a non-aggression pact, the Molotov–Ribbentrop Pact, signed in August 1939. The treaty also contained secret protocols dividing Poland and the Baltic states into German and Soviet spheres of influence.

Germany's wartime foreign policy involved the creation of allied governments controlled directly or indirectly from Berlin. They intended to obtain soldiers from allies such as Italy and Hungary and workers and food supplies from allies such as Vichy France. Hungary was the fourth nation to join the Axis, signing the Tripartite Pact on 27 September 1940. Bulgaria signed the pact on 17 November. German efforts to secure oil included negotiating a supply from their new ally, Romania, who signed the Pact on 23 November, alongside the Slovak Republic. By late 1942, there were 24 divisions from Romania on the Eastern Front, 10 from Italy, and 10 from Hungary. Germany assumed full control in France in 1942, Italy in 1943, and Hungary in 1944. Although Japan was a powerful ally, the relationship was distant, with little co-ordination or co-operation. For example, Germany refused to share their formula for synthetic oil from coal until late in the war.

Germany invaded Poland and captured the Free City of Danzig on 1 September 1939, beginning World War II in Europe. Honouring their treaty obligations, Britain and France declared war on Germany two days later. Poland fell quickly, as the Soviet Union attacked from the east on 17 September. Reinhard Heydrich, chief of the "Sicherheitspolizei" (SiPo; Security Police) and "Sicherheitsdienst" (SD; Security Service), ordered on 21 September that Polish Jews should be rounded up and concentrated into cities with good rail links. Initially the intention was to deport them further east, or possibly to Madagascar. Using lists prepared in advance, some 65,000 Polish intelligentsia, noblemen, clergy, and teachers were killed by the end of 1939 in an attempt to destroy Poland's identity as a nation. Soviet forces advanced into Finland in the Winter War, and German forces saw action at sea. But little other activity occurred until May, so the period became known as the "Phoney War".

From the start of the war, a British blockade on shipments to Germany affected its economy. Germany was particularly dependent on foreign supplies of oil, coal, and grain. Thanks to trade embargoes and the blockade, imports into Germany declined by 80 per cent. To safeguard Swedish iron ore shipments to Germany, Hitler ordered the invasion of Denmark and Norway, which began on 9 April. Denmark fell after less than a day, while most of Norway followed by the end of the month. By early June, Germany occupied all of Norway.

Against the advice of many of his senior military officers, Hitler ordered an attack on France and the Low Countries, which began in May 1940. They quickly conquered Luxembourg and the Netherlands. After outmanoeuvring the Allies in Belgium and forcing the evacuation of many British and French troops at Dunkirk, France fell as well, surrendering to Germany on 22 June. The victory in France resulted in an upswing in Hitler's popularity and an upsurge in war fever in Germany.

In violation of the provisions of the Hague Convention, industrial firms in the Netherlands, France, and Belgium were put to work producing war materiel for Germany.
The Nazis seized from the French thousands of locomotives and rolling stock, stockpiles of weapons, and raw materials such as copper, tin, oil, and nickel. Payments for occupation costs were levied upon France, Belgium, and Norway. Barriers to trade led to hoarding, black markets, and uncertainty about the future. Food supplies were precarious; production dropped in most of Europe. Famine was experienced in many occupied countries.

Hitler's peace overtures to the new British Prime Minister Winston Churchill were rejected in July 1940. Grand Admiral Erich Raeder had advised Hitler in June that air superiority was a pre-condition for a successful invasion of Britain, so Hitler ordered a series of aerial attacks on Royal Air Force (RAF) airbases and radar stations, as well as nightly air raids on British cities, including London, Plymouth, and Coventry. The German Luftwaffe failed to defeat the RAF in what became known as the Battle of Britain, and by the end of October, Hitler realised that air superiority would not be achieved. He permanently postponed the invasion, a plan which the commanders of the German army had never taken entirely seriously. Several historians, including Andrew Gordon, believe the primary reason for the failure of the invasion plan was the superiority of the Royal Navy, not the actions of the RAF.

In February 1941, the German "Afrika Korps" arrived in Libya to aid the Italians in the North African Campaign. On 6 April, Germany launched an invasion of Yugoslavia and Greece. All of Yugoslavia and parts of Greece were subsequently divided between Germany, Hungary, Italy, and Bulgaria.

On 22 June 1941, contravening the Molotov–Ribbentrop Pact, about 3.8 million Axis troops attacked the Soviet Union. In addition to Hitler's stated purpose of acquiring "Lebensraum", this large-scale offensive—codenamed Operation Barbarossa—was intended to destroy the Soviet Union and seize its natural resources for subsequent aggression against the Western powers. The reaction among Germans was one of surprise and trepidation as many were concerned about how much longer the war would continue or suspected that Germany could not win a war fought on two fronts.
The invasion conquered a huge area, including the Baltic states, Belarus, and west Ukraine. After the successful Battle of Smolensk in September 1941, Hitler ordered Army Group Centre to halt its advance to Moscow and temporarily divert its Panzer groups to aid in the encirclement of Leningrad and Kiev. This pause provided the Red Army with an opportunity to mobilise fresh reserves. The Moscow offensive, which resumed in October 1941, ended disastrously in December. On 7 December 1941, Japan attacked Pearl Harbor, Hawaii. Four days later, Germany declared war on the United States.

Food was in short supply in the conquered areas of the Soviet Union and Poland, as the retreating armies had burned the crops in some areas, and much of the remainder was sent back to the Reich. In Germany, rations were cut in 1942. In his role as Plenipotentiary of the Four Year Plan, Hermann Göring demanded increased shipments of grain from France and fish from Norway. The 1942 harvest was good, and food supplies remained adequate in Western Europe.

Germany and Europe as a whole was almost totally dependent on foreign oil imports. In an attempt to resolve the shortage, in June 1942 Germany launched "Fall Blau" ("Case Blue"), an offensive against the Caucasian oilfields. The Red Army launched a counter-offensive on 19 November and encircled the Axis forces, who were trapped in Stalingrad on 23 November. Göring assured Hitler that the 6th Army could be supplied by air, but this turned out to be infeasible. Hitler's refusal to allow a retreat led to the deaths of 200,000 German and Romanian soldiers; of the 91,000 men who surrendered in the city on 31 January 1943, only 6,000 survivors returned to Germany after the war.

Losses continued to mount after Stalingrad, leading to a sharp reduction in the popularity of the Nazi Party and deteriorating morale. Soviet forces continued to push westward after the failed German offensive at the Battle of Kursk in the summer of 1943. By the end of 1943, the Germans had lost most of their eastern territorial gains. In Egypt, Field Marshal Erwin Rommel's "Afrika Korps" were defeated by British forces under Field Marshal Bernard Montgomery in October 1942. The Allies landed in Sicily in July 1943 and in Italy in September. Meanwhile, American and British bomber fleets based in Britain began operations against Germany. Many sorties were intentionally given civilian targets in an effort to destroy German morale. German aircraft production could not keep pace with losses, and without air cover the Allied bombing campaign became even more devastating. By targeting oil refineries and factories, they crippled the German war effort by late 1944.

On 6 June 1944, American, British, and Canadian forces established a front in France with the D-Day landings in Normandy. On 20 July 1944, Hitler survived an assassination attempt. He ordered brutal reprisals, resulting in 7,000 arrests and the execution of more than 4,900 people. The failed Ardennes Offensive (16 December 1944 – 25 January 1945) was the last major German offensive on the western front, and Soviet forces entered Germany on 27 January. Hitler's refusal to admit defeat and his insistence that the war be fought to the last man led to unnecessary death and destruction in the war's closing months. Through his Justice Minister Otto Georg Thierack, Hitler ordered that anyone who was not prepared to fight should be court-martialed, and thousands of people were put to death. In many areas, people surrendered to the approaching Allies in spite of exhortations of local leaders to continue to fight. Hitler ordered the destruction of transport, bridges, industries, and other infrastructure—a scorched earth decree—but Armaments Minister Albert Speer prevented this order from being fully carried out.

During the Battle of Berlin (16 April 1945 – 2 May 1945), Hitler and his staff lived in the underground "Führerbunker" while the Red Army approached. On 30 April, when Soviet troops were within two blocks of the Reich Chancellery, Hitler, along with his girlfriend and by then wife Eva Braun committed suicide. On 2 May, General Helmuth Weidling unconditionally surrendered Berlin to Soviet General Vasily Chuikov. Hitler was succeeded by Grand Admiral Karl Dönitz as Reich President and Goebbels as Reich Chancellor. Goebbels and his wife Magda committed suicide the next day after murdering their six children. Between 4 and 8 May 1945, most of the remaining German armed forces unconditionally surrendered. The German Instrument of Surrender was signed 8 May, marking the end of the Nazi regime and the end of World War II in Europe.

Popular support for Hitler almost completely disappeared as the war drew to a close. Suicide rates in Germany increased, particularly in areas where the Red Army was advancing. Among soldiers and party personnel, suicide was often deemed an honourable and heroic alternative to surrender. First-hand accounts and propaganda about the uncivilised behaviour of the advancing Soviet troops caused panic among civilians on the Eastern Front, especially women, who feared being raped. More than a thousand people (out of a population of around 16,000) committed suicide in Demmin on and around 1 May 1945 as the 65th Army of 2nd Belorussian Front first broke into a distillery and then rampaged through the town, committing mass rapes, arbitrarily executing civilians, and setting fire to buildings. High numbers of suicides took place in many other locations, including Neubrandenburg (600 dead), Stolp in Pommern (1,000 dead), and Berlin, where at least 7,057 people committed suicide in 1945.

Estimates of the total German war dead range from 5.5 to 6.9 million persons. A study by German historian Rüdiger Overmans puts the number of German military dead and missing at 5.3 million, including 900,000 men conscripted from outside of Germany's 1937 borders. Richard Overy estimated in 2014 that about 353,000 civilians were killed in Allied air raids. Other civilian deaths include 300,000 Germans (including Jews) who were victims of Nazi political, racial, and religious persecution and 200,000 who were murdered in the Nazi euthanasia program. Political courts called "Sondergerichte" sentenced some 12,000 members of the German resistance to death, and civil courts sentenced an additional 40,000 Germans. Mass rapes of German women also took place.

As a result of their defeat in World War I and the resulting Treaty of Versailles, Germany lost Alsace-Lorraine, Northern Schleswig, and Memel. The Saarland became a protectorate of France under the condition that its residents would later decide by referendum which country to join, and Poland became a separate nation and was given access to the sea by the creation of the Polish Corridor, which separated Prussia from the rest of Germany, while Danzig was made a free city.

Germany regained control of the Saarland through a referendum held in 1935 and annexed Austria in the "Anschluss" of 1938. The Munich Agreement of 1938 gave Germany control of the Sudetenland, and they seized the remainder of Czechoslovakia six months later. Under threat of invasion by sea, Lithuania surrendered the Memel district in March 1939.

Between 1939 and 1941, German forces invaded Poland, Denmark, Norway, France, Luxembourg, the Netherlands, Belgium, Yugoslavia, Greece, and the Soviet Union. Germany annexed parts of northern Yugoslavia in April 1941, while Mussolini ceded Trieste, South Tyrol, and Istria to Germany in 1943.

Some of the conquered territories were incorporated into Germany as part of Hitler's long-term goal of creating a Greater Germanic Reich. Several areas, such as Alsace-Lorraine, were placed under the authority of an adjacent "Gau" (regional district). The "Reichskommissariate" (Reich Commissariats), quasi-colonial regimes, were established in some occupied countries. Areas placed under German administration included the Protectorate of Bohemia and Moravia, "Reichskommissariat Ostland" (encompassing the Baltic states and Belarus), and "Reichskommissariat Ukraine". Conquered areas of Belgium and France were placed under control of the Military Administration in Belgium and Northern France. Belgian Eupen-Malmedy, which had been part of Germany until 1919, was annexed. Part of Poland was incorporated into the Reich, and the General Government was established in occupied central Poland. The governments of Denmark, Norway ("Reichskommissariat Norwegen"), and the Netherlands ("Reichskommissariat Niederlande") were placed under civilian administrations staffed largely by natives. Hitler intended to eventually incorporate many of these areas into the Reich. Germany occupied the Italian protectorate of Albania and the Italian governorate of Montenegro in 1943 and installed a puppet government in occupied Serbia in 1941.

The Nazis were a far-right fascist political party which arose during the social and financial upheavals that occurred following the end of World War I. The Party remained small and marginalised, receiving 2.6% of the federal vote in 1928, prior to the onset of the Great Depression in 1929. By 1930 the Party won 18.3% of the federal vote, making it the Reichstag's second largest political party. While in prison after the failed Beer Hall Putsch of 1923, Hitler wrote "Mein Kampf", which laid out his plan for transforming German society into one based on race. Nazi ideology brought together elements of antisemitism, racial hygiene, and eugenics, and combined them with pan-Germanism and territorial expansionism with the goal of obtaining more "Lebensraum" for the Germanic people. The regime attempted to obtain this new territory by attacking Poland and the Soviet Union, intending to deport or kill the Jews and Slavs living there, who were viewed as being inferior to the Aryan master race and part of a Jewish-Bolshevik conspiracy. The Nazi regime believed that only Germany could defeat the forces of Bolshevism and save humanity from world domination by International Jewry. Other people deemed life unworthy of life by the Nazis included the mentally and physically disabled, Romani people, homosexuals, Jehovah's Witnesses, and social misfits.

Influenced by the "Völkisch" movement, the regime was against cultural modernism and supported the development of an extensive military at the expense of intellectualism. Creativity and art were stifled, except where they could serve as propaganda media. The party used symbols such as the Blood Flag and rituals such as the Nazi Party rallies to foster unity and bolster the regime's popularity.

Hitler ruled Germany autocratically by asserting the "Führerprinzip" ("leader principle"), which called for absolute obedience of all subordinates. He viewed the government structure as a pyramid, with himself—the infallible leader—at the apex. Party rank was not determined by elections, and positions were filled through appointment by those of higher rank. The party used propaganda to develop a cult of personality around Hitler. Historians such as Kershaw emphasise the psychological impact of Hitler's skill as an orator. Roger Gill states: "His moving speeches captured the minds and hearts of a vast number of the German people: he virtually hypnotized his audiences".

While top officials reported to Hitler and followed his policies, they had considerable autonomy. He expected officials to "work towards the Führer" – to take the initiative in promoting policies and actions in line with party goals and Hitler's wishes, without his involvement in day-to-day decision-making. The government was a disorganised collection of factions led by the party elite, who struggled to amass power and gain the Führer's favour. Hitler's leadership style was to give contradictory orders to his subordinates and to place them in positions where their duties and responsibilities overlapped. In this way he fostered distrust, competition, and infighting among his subordinates to consolidate and maximise his own power.

Successive "Reichsstatthalter" decrees between 1933 and 1935 abolished the existing "Länder" (constituent states) of Germany and replaced them with new administrative divisions, the "Gaue", governed by Nazi leaders ("Gauleiters"). The change was never fully implemented, as the Länder were still used as administrative divisions for some government departments such as education. This led to a bureaucratic tangle of overlapping jurisdictions and responsibilities typical of the administrative style of the Nazi regime.

Jewish civil servants lost their jobs in 1933, except for those who had seen military service in World War I. Members of the Party or party supporters were appointed in their place. As part of the process of "Gleichschaltung", the Reich Local Government Law of 1935 abolished local elections, and mayors were appointed by the Ministry of the Interior.

In August 1934, civil servants and members of the military were required to swear an oath of unconditional obedience to Hitler. These laws became the basis of the "Führerprinzip", the concept that Hitler's word overrode all existing laws. Any acts that were sanctioned by Hitler—even murder—thus became legal. All legislation proposed by cabinet ministers had to be approved by the office of Deputy Führer Rudolf Hess, who could also veto top civil service appointments.

Most of the judicial system and legal codes of the Weimar Republic remained in place to deal with non-political crimes. The courts issued and carried out far more death sentences than before the Nazis took power. People who were convicted of three or more offences—even petty ones—could be deemed habitual offenders and jailed indefinitely. People such as prostitutes and pickpockets were judged to be inherently criminal and a threat to the community. Thousands were arrested and confined indefinitely without trial.
A new type of court, the "Volksgerichtshof" ("People's Court"), was established in 1934 to deal with political cases. This court handed out over 5,000 death sentences until its dissolution in 1945. The death penalty could be issued for offences such as being a communist, printing seditious leaflets, or even making jokes about Hitler or other officials. The Gestapo was in charge of investigative policing to enforce National Socialist ideology as they located and confined political offenders, Jews, and others deemed undesirable. Political offenders who were released from prison were often immediately re-arrested by the Gestapo and confined in a concentration camp.

The Nazis used propaganda to promulgate the concept of "Rassenschande" ("race defilement") to justify the need for racial laws. In September 1935, the Nuremberg Laws were enacted. These laws initially prohibited sexual relations and marriages between Aryans and Jews and were later extended to include "Gypsies, Negroes or their bastard offspring". The law also forbade the employment of German women under the age of 45 as domestic servants in Jewish households. The Reich Citizenship Law stated that only those of "German or related blood" could be citizens. Thus Jews and other non-Aryans were stripped of their German citizenship. The law also permitted the Nazis to deny citizenship to anyone who was not supportive enough of the regime. A supplementary decree issued in November defined as Jewish anyone with three Jewish grandparents, or two grandparents if the Jewish faith was followed.

The unified armed forces of Germany from 1935 to 1945 were called the "Wehrmacht" (defence force). This included the "Heer" (army), "Kriegsmarine" (navy), and the "Luftwaffe" (air force). From 2 August 1934, members of the armed forces were required to pledge an oath of unconditional obedience to Hitler personally. In contrast to the previous oath, which required allegiance to the constitution of the country and its lawful establishments, this new oath required members of the military to obey Hitler even if they were being ordered to do something illegal. Hitler decreed that the army would have to tolerate and even offer logistical support to the "Einsatzgruppen"—the mobile death squads responsible for millions of deaths in Eastern Europe—when it was tactically possible to do so. "Wehrmacht" troops also participated directly in the Holocaust by shooting civilians or committing genocide under the guise of anti-partisan operations. The party line was that the Jews were the instigators of the partisan struggle and therefore needed to be eliminated. On 8 July 1941, Heydrich announced that all Jews in the eastern conquered territories were to be regarded as partisans and gave the order for all male Jews between the ages of 15 and 45 to be shot. By August, this was extended to include the entire Jewish population.

In spite of efforts to prepare the country militarily, the economy could not sustain a lengthy war of attrition. A strategy was developed based on the tactic of "Blitzkrieg" ("lightning war"), which involved using quick coordinated assaults that avoided enemy strong points. Attacks began with artillery bombardment, followed by bombing and strafing runs. Next the tanks would attack and finally the infantry would move in to secure the captured area. Victories continued through mid-1940, but the failure to defeat Britain was the first major turning point in the war. The decision to attack the Soviet Union and the decisive defeat at Stalingrad led to the retreat of the German armies and the eventual loss of the war. The total number of soldiers who served in the "Wehrmacht" from 1935 to 1945 was around 18.2 million, of whom 5.3 million died.

The "Sturmabteilung" (SA; Storm Detachment), or Brownshirts, founded in 1921, was the first paramilitary wing of the Nazi Party; their initial assignment was to protect Nazi leaders at rallies and assemblies. They also took part in street battles against the forces of rival political parties and violent actions against Jews and others. Under Ernst Röhm's leadership the SA grew by 1934 to over half a million members—4.5 million including reserves—at a time when the regular army was still limited to 100,000 men by the Versailles Treaty.

Röhm hoped to assume command of the army and absorb it into the ranks of the SA. Hindenburg and Defence Minister Werner von Blomberg threatened to impose martial law if the activities of the SA were not curtailed. Therefore, less than a year and a half after seizing power, Hitler ordered the deaths of the SA leadership, including Rohm. After the purge of 1934, the SA was no longer a major force.

Initially a small bodyguard unit under the auspices of the SA, the "Schutzstaffel" (SS; Protection Squadron) grew to become one of the largest and most powerful groups in Nazi Germany. Led by "Reichsführer-SS" Heinrich Himmler from 1929, the SS had over a quarter million members by 1938. Himmler initially envisioned the SS as being an elite group of guards, Hitler's last line of defence. The Waffen-SS, the military branch of the SS, evolved into a second army. It was dependent on the regular army for heavy weaponry and equipment, and most units were under tactical control of the High Command of the Armed Forces (OKW). By the end of 1942, the stringent selection and racial requirements that had initially been in place were no longer followed. With recruitment and conscription based only on expansion, by 1943 the Waffen-SS could not longer claim to be an elite fighting force.

SS formations committed many war crimes against civilians and allied servicemen. From 1935 onward, the SS spearheaded the persecution of Jews, who were rounded up into ghettos and concentration camps. With the outbreak of World War II, the SS "Einsatzgruppen" units followed the army into Poland and the Soviet Union, where from 1941 to 1945 they killed more than two million people, including 1.3 million Jews. A third of the "Einsatzgruppen" members were recruited from Waffen-SS personnel. The "SS-Totenkopfverbände" (death's head units) ran the concentration camps and extermination camps, where millions more were killed. Up to 60,000 Waffen-SS men served in the camps.

In 1931, Himmler organised an SS intelligence service which became known as the "Sicherheitsdienst" (SD; Security Service) under his deputy, Heydrich. This organisation was tasked with locating and arresting communists and other political opponents. Himmler established the beginnings of a parallel economy under the auspices of the SS Economy and Administration Head Office. This holding company owned housing corporations, factories, and publishing houses.

The most pressing economic matter the Nazis initially faced was the 30 percent national unemployment rate. Economist Dr. Hjalmar Schacht, President of the Reichsbank and Minister of Economics, created a scheme for deficit financing in May 1933. Capital projects were paid for with the issuance of promissory notes called Mefo bills. When the notes were presented for payment, the Reichsbank printed money. Hitler and his economic team expected that the upcoming territorial expansion would provide the means of repaying the soaring national debt. Schacht's administration achieved a rapid decline in the unemployment rate, the largest of any country during the Great Depression. Economic recovery was uneven, with reduced hours of work and erratic availability of necessities, leading to disenchantment with the regime as early as 1934.

In October 1933, the Junkers Aircraft Works was expropriated. In concert with other aircraft manufacturers and under the direction of Aviation Minister Göring, production was ramped up. From a workforce of 3,200 people producing 100 units per year in 1932, the industry grew to employ a quarter of a million workers manufacturing over 10,000 technically advanced aircraft annually less than ten years later.

An elaborate bureaucracy was created to regulate imports of raw materials and finished goods with the intention of eliminating foreign competition in the German marketplace and improving the nation's balance of payments. The Nazis encouraged the development of synthetic replacements for materials such as oil and textiles. As the market was experiencing a glut and prices for petroleum were low, in 1933 the Nazi government made a profit-sharing agreement with IG Farben, guaranteeing them a 5 percent return on capital invested in their synthetic oil plant at Leuna. Any profits in excess of that amount would be turned over to the Reich. By 1936, Farben regretted making the deal, as excess profits were by then being generated. In another attempt to secure an adequate wartime supply of petroleum, Germany intimidated Romania into signing a trade agreement in March 1939.

Major public works projects financed with deficit spending included the construction of a network of "Autobahnen" and providing funding for programmes initiated by the previous government for housing and agricultural improvements. To stimulate the construction industry, credit was offered to private businesses and subsidies were made available for home purchases and repairs. On the condition that the wife would leave the workforce, a loan of up to 1,000 Reichsmarks could be accessed by young couples of Aryan descent who intended to marry, and the amount that had to be repaid was reduced by 25 percent for each child born. The caveat that the woman had to remain unemployed outside the home was dropped by 1937 due to a shortage of skilled labourers.

Envisioning widespread car ownership as part of the new Germany, Hitler arranged for designer Ferdinand Porsche to draw up plans for the "KdF-wagen" (Strength Through Joy car), intended to be an automobile that everyone could afford. A prototype was displayed at the International Motor Show in Berlin on 17 February 1939. With the outbreak of World War II, the factory was converted to produce military vehicles. None were sold until after the war, when the vehicle was renamed the Volkswagen (people's car).

Six million people were unemployed when the Nazis took power in 1933 and by 1937 there were fewer than a million. This was in part due to the removal of women from the workforce. Real wages dropped by 25 percent between 1933 and 1938. After the dissolution of the trade unions in May 1933, their funds were seized and their leadership arrested, including those who attempted to co-operate with the Nazis. A new organisation, the German Labour Front, was created and placed under Nazi Party functionary Robert Ley. The average work week was 43 hours in 1933; by 1939 this increased to 47 hours.

By early 1934, the focus shifted towards rearmament. By 1935, military expenditures accounted for 73 percent of the government's purchases of goods and services. On 18 October 1936, Hitler named Göring as Plenipotentiary of the Four Year Plan, intended to speed up rearmament. In addition to calling for the rapid construction of steel mills, synthetic rubber plants, and other factories, Göring instituted wage and price controls and restricted the issuance of stock dividends. Large expenditures were made on rearmament in spite of growing deficits. Plans unveiled in late 1938 for massive increases to the navy and air force were impossible to fulfil, as Germany lacked the finances and material resources to build the planned units, as well as the necessary fuel required to keep them running. With the introduction of compulsory military service in 1935, the "Reichswehr", which had been limited to 100,000 by the terms of the Versailles Treaty, expanded to 750,000 on active service at the start of World War II, with a million more in the reserve. By January 1939, unemployment was down to 301,800 and it dropped to only 77,500 by September.

The Nazi war economy was a mixed economy that combined a free market with central planning. Historian Richard Overy describes it as being somewhere in between the command economy of the Soviet Union and the capitalist system of the United States.

In 1942, after the death of Armaments Minister Fritz Todt, Hitler appointed Albert Speer as his replacement. Wartime rationing of consumer goods led to an increase in personal savings, funds which were in turn lent to the government to support the war effort. By 1944, the war was consuming 75 percent of Germany's gross domestic product, compared to 60 percent in the Soviet Union and 55 percent in Britain. Speer improved production by centralising planning and control, reducing production of consumer goods, and using forced labour and slavery. The wartime economy eventually relied heavily upon the large-scale employment of slave labour. Germany imported and enslaved some 12 million people from 20 European countries to work in factories and on farms. Approximately 75 percent were Eastern European. Many were casualties of Allied bombing, as they received poor air raid protection. Poor living conditions led to high rates of sickness, injury, and death, as well as sabotage and criminal activity. The wartime economy also relied upon large-scale robbery, initially through the state seizing the property of Jewish citizens and later by plundering the resources of occupied territories.

Foreign workers brought into Germany were put into four classifications: guest workers, military internees, civilian workers, and Eastern workers. Each group was subject to different regulations. The Nazis issued a ban on sexual relations between Germans and foreign workers.

By 1944, over a half million women served as auxiliaries in the German armed forces. The number of women in paid employment only increased by 271,000 (1.8 percent) from 1939 to 1944. As the production of consumer goods had been cut back, women left those industries for employment in the war economy. They also took jobs formerly held by men, especially on farms and in family-owned shops.

Very heavy strategic bombing by the Allies targeted refineries producing synthetic oil and gasoline, as well as the German transportation system, especially rail yards and canals. The armaments industry began to break down by September 1944. By November, fuel coal was no longer reaching its destinations and the production of new armaments was no longer possible. Overy argues that the bombing strained the German war economy and forced it to divert up to one-fourth of its manpower and industry into anti-aircraft resources, which very likely shortened the war.

During the course of the war, the Nazis extracted considerable plunder from occupied Europe. Historian and war correspondent William L. Shirer writes: "The total amount of [Nazi] loot will never be known; it has proved beyond man's capacity to accurately compute." Gold reserves and other foreign holdings were seized from the national banks of occupied nations, while large "occupation costs" were usually imposed. By the end of the war, occupation costs were calculated by the Nazis at 60 billion Reichsmarks, with France alone paying 31.5 billion. The Bank of France was forced to provide 4.5 billion Reichsmarks in "credits" to Germany, while a further 500,000 Reichsmarks were assessed against Vichy France by the Nazis in the form of "fees" and other miscellaneous charges. The Nazis exploited other conquered nations in a similar way. After the war, the United States Strategic Bombing Survey concluded Germany had obtained 104 billion Reichsmarks in the form of occupation costs and other wealth transfers from occupied Europe, including two-thirds of the gross domestic product of Belgium and the Netherlands.

Nazi plunder included private and public art collections, artefacts, precious metals, books, and personal possessions. Hitler and Göring in particular were interested in acquiring looted art treasures from occupied Europe, the former planning to use the stolen art to fill the galleries of the planned "Führermuseum" (Leader's Museum), and the latter for his personal collection. Göring, having stripped almost all of occupied Poland of its artworks within six months of Germany's invasion, ultimately grew a collection valued at over 50 million Reichsmarks. In 1940, the Reichsleiter Rosenberg Taskforce was established to loot artwork and cultural material from public and private collections, libraries, and museums throughout Europe. France saw the greatest extent of Nazi plunder. Some 26,000 railroad cars of art treasures, furniture, and other looted items were sent to Germany from France. By January 1941, Rosenberg estimated the looted treasures from France to be valued at over one billion Reichsmarks. In addition, soldiers looted or purchased goods such as produce and clothing—items, which were becoming harder to obtain in Germany—for shipment home.

Goods and raw materials were also taken. In France, an estimated of cereals were seized during the course of the war, including 75 percent of its oats. In addition, 80 percent of the country's oil and 74 percent of its steel production were taken. The valuation of this loot is estimated to be 184.5 billion francs. In Poland, Nazi plunder of raw materials began even before the German invasion had concluded.

Following Operation Barbarossa, the Soviet Union was also plundered. In 1943 alone, 9,000,000 tons of cereals, of fodder, of potatoes, and of meats were sent back to Germany. During the course of the German occupation, some 12 million pigs and 13 million sheep were taken. The value of this plunder is estimated at 4 billion Reichsmarks. This relatively low number in comparison to the occupied nations of Western Europe can be attributed to the devastating fighting on the Eastern Front.

Racism and antisemitism were basic tenets of the Nazi Party and the Nazi regime. Nazi Germany's racial policy was based on their belief in the existence of a superior master race. The Nazis postulated the existence of a racial conflict between the Aryan master race and inferior races, particularly Jews, who were viewed as a mixed race that had infiltrated society and were responsible for the exploitation and repression of the Aryan race.

Discrimination against Jews began immediately after the seizure of power. Following a month-long series of attacks by members of the SA on Jewish businesses and synagogues, on 1 April 1933 Hitler declared a national boycott of Jewish businesses. The Law for the Restoration of the Professional Civil Service passed on 7 April forced all non-Aryan civil servants to retire from the legal profession and civil service. Similar legislation soon deprived other Jewish professionals of their right to practise, and on 11 April a decree was promulgated that stated anyone who had even one Jewish parent or grandparent was considered non-Aryan. As part of the drive to remove Jewish influence from cultural life, members of the National Socialist Student League removed from libraries any books considered un-German, and a nationwide book burning was held on 10 May.

The regime used violence and economic pressure to encourage Jews to voluntarily leave the country. Jewish businesses were denied access to markets, forbidden to advertise, and deprived of access to government contracts. Citizens were harassed and subjected to violent attacks. Many towns posted signs forbidding entry to Jews.

In November 1938 a young Jewish man requested an interview with the German ambassador in Paris and met with a legation secretary, whom he shot and killed to protest his family's treatment in Germany. This incident provided the pretext for a pogrom the Nazis incited against the Jews on 9 November 1938. Members of the SA damaged or destroyed synagogues and Jewish property throughout Germany. At least 91 German Jews were killed during this pogrom, later called "Kristallnacht", the Night of Broken Glass. Further restrictions were imposed on Jews in the coming months – they were forbidden to own businesses or work in retail shops, drive cars, go to the cinema, visit the library, or own weapons, and Jewish pupils were removed from schools. The Jewish community was fined one billion marks to pay for the damage caused by "Kristallnacht" and told that any insurance settlements would be confiscated. By 1939, around 250,000 of Germany's 437,000 Jews had emigrated to the United States, Argentina, Great Britain, Palestine, and other countries. Many chose to stay in continental Europe. Emigrants to Palestine were allowed to transfer property there under the terms of the Haavara Agreement, but those moving to other countries had to leave virtually all their property behind, and it was seized by the government.

Like the Jews, the Romani people were subjected to persecution from the early days of the regime. The Romani were forbidden to marry people of German extraction. They were shipped to concentration camps starting in 1935 and many were killed. Following the invasion of Poland, 2,500 Roma and Sinti people were deported from Germany to the General Government, where they were imprisoned in labour camps. The survivors were likely exterminated at Bełżec, Sobibor, or Treblinka. A further 5,000 Sinti and Austrian Lalleri people were deported to the Łódź Ghetto in late 1941, where half were estimated to have died. The Romani survivors of the ghetto were subsequently moved to the Chełmno extermination camp in early 1942.

The Nazis intended on deporting all Romani people from Germany, and confined them to "Zigeunerlager" (Gypsy camps) for this purpose. Himmler ordered their deportation from Germany in December 1942, with few exceptions. A total of 23,000 Romani were deported to Auschwitz concentration camp, of whom 19,000 died. Outside of Germany, the Romani people were regularly used for forced labour, though many were killed. In the Baltic states and the Soviet Union, 30,000 Romani were killed by the SS, the German Army, and "Einsatzgruppen". In occupied Serbia, 1,000 to 12,000 Romani were killed, while nearly all 25,000 Romani living in the Independent State of Croatia were killed. The estimates at end of the war put the total death toll at around 220,000, which equalled approximately 25 percent of the Romani population in Europe.

Action T4 was a programme of systematic murder of the physically and mentally handicapped and patients in psychiatric hospitals that took place mainly from 1939 to 1941, and continued until the end of the war. Initially the victims were shot by the "Einsatzgruppen" and others; gas chambers and gas vans using carbon monoxide were used by early 1940. Under the Law for the Prevention of Hereditarily Diseased Offspring, enacted on 14 July 1933, over 400,000 individuals underwent compulsory sterilisation. Over half were those considered mentally deficient, which included not only people who scored poorly on intelligence tests, but those who deviated from expected standards of behaviour regarding thrift, sexual behaviour, and cleanliness. Most of the victims came from disadvantaged groups such as prostitutes, the poor, the homeless, and criminals. Other groups persecuted and killed included Jehovah's Witnesses, homosexuals, social misfits, and members of the political and religious opposition.

Germany's war in the East was based on Hitler's long-standing view that Jews were the great enemy of the German people and that "Lebensraum" was needed for Germany's expansion. Hitler focused his attention on Eastern Europe, aiming to conquer Poland and the Soviet Union. After the occupation of Poland in 1939, all Jews living in the General Government were confined to ghettos, and those who were physically fit were required to perform compulsory labour. In 1941 Hitler decided to destroy the Polish nation completely; within 15 to 20 years the General Government was to be cleared of ethnic Poles and resettled by German colonists. About 3.8 to 4 million Poles would remain as slaves, part of a slave labour force of 14 million the Nazis intended to create using citizens of conquered nations.

The "Generalplan Ost" ("General Plan for the East") called for deporting the population of occupied Eastern Europe and the Soviet Union to Siberia, for use as slave labour or to be murdered. To determine who should be killed, Himmler created the "Volksliste", a system of classification of people deemed to be of German blood. He ordered that those of Germanic descent who refused to be classified as ethnic Germans should be deported to concentration camps, have their children taken away, or be assigned to forced labour. The plan also included the kidnapping of children deemed to have Aryan-Nordic traits, who were presumed to be of German descent. The goal was to implement "Generalplan Ost" after the conquest of the Soviet Union, but when the invasion failed Hitler had to consider other options. One suggestion was a mass forced deportation of Jews to Poland, Palestine, or Madagascar.

In addition to eliminating Jews, the Nazis planned to reduce the population of the conquered territories by 30 million people through starvation in an action called the Hunger Plan. Food supplies would be diverted to the German army and German civilians. Cities would be razed and the land allowed to return to forest or resettled by German colonists. Together, the Hunger Plan and "Generalplan Ost" would have led to the starvation of 80 million people in the Soviet Union. These partially fulfilled plans resulted in the democidal deaths of an estimated 19.3 million civilians and prisoners of war (POWs) throughout the USSR and elsewhere in Europe. During the course of the war, the Soviet Union lost a total of 27 million people; less than nine million of these were combat deaths. One in four of the Soviet population were killed or wounded.

Around the time of the failed offensive against Moscow in December 1941, Hitler resolved that the Jews of Europe were to be exterminated immediately. While the murder of Jewish civilians had been ongoing in the occupied territories of Poland and the Soviet Union, plans for the total eradication of the Jewish population of Europe—eleven million people—were formalised at the Wannsee Conference on 20 January 1942. Some would be worked to death and the rest would be killed in the implementation of the Final Solution to the Jewish Question. Initially the victims were killed by "Einsatzgruppen" firing squads, then by stationary gas chambers or by gas vans, but these methods proved impractical for an operation of this scale. By 1942 extermination camps equipped with gas chambers were established at Auschwitz, Chełmno, Sobibor, Treblinka, and elsewhere. The total number of Jews murdered is estimated at 5.5 to six million, including over a million children.

The Allies received information about the murders from the Polish government-in-exile and Polish leadership in Warsaw, based mostly on intelligence from the Polish underground. German citizens had access to information about what was happening, as soldiers returning from the occupied territories reported on what they had seen and done. Historian Richard J. Evans states that most German citizens disapproved of the genocide.

Poles were viewed by Nazis as subhuman non-Aryans, and during the German occupation of Poland 2.7 million ethnic Poles were killed. Polish civilians were subject to forced labour in German industry, internment, wholesale expulsions to make way for German colonists, and mass executions. The German authorities engaged in a systematic effort to destroy Polish culture and national identity. During operation AB-Aktion, many university professors and members of the Polish intelligentsia were arrested, transported to concentration camps, or executed. During the war, Poland lost an estimated 39 to 45 percent of its physicians and dentists, 26 to 57 percent of its lawyers, 15 to 30 percent of its teachers, 30 to 40 percent of its scientists and university professors, and 18 to 28 percent of its clergy.

The Nazis captured 5.75 million Soviet prisoners of war, more than they took from all the other Allied powers combined. Of these, they killed an estimated 3.3 million, with 2.8 million of them being killed between June 1941 and January 1942. Many POWs starved to death or resorted to cannibalism while being held in open-air pens at Auschwitz and elsewhere.

From 1942 onward, Soviet POWs were viewed as a source of forced labour, and received better treatment so they could work. By December 1944, 750,000 Soviet POWs were working, including in German armaments factories (in violation of the Hague and Geneva conventions), mines, and farms.

Antisemitic legislation passed in 1933 led to the removal of all Jewish teachers, professors, and officials from the education system. Most teachers were required to belong to the "Nationalsozialistischer Lehrerbund" (NSLB; National Socialist Teachers League) and university professors were required to join the National Socialist German Lecturers. Teachers had to take an oath of loyalty and obedience to Hitler, and those who failed to show sufficient conformity to party ideals were often reported by students or fellow teachers and dismissed. Lack of funding for salaries led to many teachers leaving the profession. The average class size increased from 37 in 1927 to 43 in 1938 due to the resulting teacher shortage.

Frequent and often contradictory directives were issued by Interior Minister Wilhelm Frick, Bernhard Rust of the Reich Ministry of Science, Education and Culture, and other agencies regarding content of lessons and acceptable textbooks for use in primary and secondary schools. Books deemed unacceptable to the regime were removed from school libraries. Indoctrination in National Socialist thought was made compulsory in January 1934. Students selected as future members of the party elite were indoctrinated from the age of 12 at Adolf Hitler Schools for primary education and National Political Institutes of Education for secondary education. Detailed National Socialist indoctrination of future holders of elite military rank was undertaken at Order Castles.

Primary and secondary education focused on racial biology, population policy, culture, geography, and physical fitness. The curriculum in most subjects, including biology, geography, and even arithmetic, was altered to change the focus to race. Military education became the central component of physical education, and education in physics was oriented toward subjects with military applications, such as ballistics and aerodynamics. Students were required to watch all films prepared by the school division of the Reich Ministry of Public Enlightenment and Propaganda.

At universities, appointments to top posts were the subject of power struggles between the education ministry, the university boards, and the National Socialist German Students' League. In spite of pressure from the League and various government ministries, most university professors did not make changes to their lectures or syllabus during the Nazi period. This was especially true of universities located in predominantly Catholic regions. Enrolment at German universities declined from 104,000 students in 1931 to 41,000 in 1939, but enrolment in medical schools rose sharply as Jewish doctors had been forced to leave the profession, so medical graduates had good job prospects. From 1934, university students were required to attend frequent and time-consuming military training sessions run by the SA. First-year students also had to serve six months in a labour camp for the Reich Labour Service; an additional ten weeks service were required of second-year students.

Women were a cornerstone of Nazi social policy. The Nazis opposed the feminist movement, claiming that it was the creation of Jewish intellectuals, instead advocating a patriarchal society in which the German woman would recognise that her "world is her husband, her family, her children, and her home". Feminist groups were shut down or incorporated into the National Socialist Women's League, which coordinated groups throughout the country to promote motherhood and household activities. Courses were offered on childrearing, sewing, and cooking. Prominent feminists, including Anita Augspurg, Lida Gustava Heymann, and Helene Stöcker, felt forced to live in exile. The League published the "NS-Frauen-Warte", the only Nazi-approved women's magazine in Nazi Germany; despite some propaganda aspects, it was predominantly an ordinary woman's magazine.

Women were encouraged to leave the workforce, and the creation of large families by racially suitable women was promoted through a propaganda campaign. Women received a bronze award—known as the "Ehrenkreuz der Deutschen Mutter" (Cross of Honour of the German Mother)—for giving birth to four children, silver for six, and gold for eight or more. Large families received subsidies to help with expenses. Though the measures led to increases in the birth rate, the number of families having four or more children declined by five percent between 1935 and 1940. Removing women from the workforce did not have the intended effect of freeing up jobs for men, as women were for the most part employed as domestic servants, weavers, or in the food and drink industries—jobs that were not of interest to men. Nazi philosophy prevented large numbers of women from being hired to work in munitions factories in the build-up to the war, so foreign labourers were brought in. After the war started, slave labourers were extensively used. In January 1943, Hitler signed a decree requiring all women under the age of fifty to report for work assignments to help the war effort. Thereafter women were funnelled into agricultural and industrial jobs, and by September 1944 14.9 million women were working in munitions production.

Nazi leaders endorsed the idea that rational and theoretical work was alien to a woman's nature, and as such discouraged women from seeking higher education. A law passed in April 1933 limited the number of females admitted to university to ten percent of the number of male attendees. This resulted in female enrolment in secondary schools dropping from 437,000 in 1926 to 205,000 in 1937. The number of women enrolled in post-secondary schools dropped from 128,000 in 1933 to 51,000 in 1938. However, with the requirement that men be enlisted into the armed forces during the war, women comprised half of the enrolment in the post-secondary system by 1944.
Women were expected to be strong, healthy, and vital. The sturdy peasant woman who worked the land and bore strong children was considered ideal, and women were praised for being athletic and tanned from working outdoors. Organisations were created for the indoctrination of Nazi values. From 25 March 1939 membership in the Hitler Youth was made compulsory for all children over the age of ten. The "Jungmädelbund" (Young Girls League) section of the Hitler Youth was for girls age 10 to 14 and the "Bund Deutscher Mädel" (BDM; League of German Girls) was for young women age 14 to 18. The BDM's activities focused on physical education, with activities such as running, long jumping, somersaulting, tightrope walking, marching, and swimming.

The Nazi regime promoted a liberal code of conduct regarding sexual matters and was sympathetic to women who bore children out of wedlock. Promiscuity increased as the war progressed, with unmarried soldiers often intimately involved with several women simultaneously. Soldier's wives were frequently involved in extramarital relationships. Sex was sometimes used as a commodity to obtain better work from a foreign labourer. Pamphlets enjoined German women to avoid sexual relations with foreign workers as a danger to their blood.

With Hitler's approval, Himmler intended that the new society of the Nazi regime should destigmatise illegitimate births, particularly of children fathered by members of the SS, who were vetted for racial purity. His hope was that each SS family would have between four and six children. The "Lebensborn" (Fountain of Life) association, founded by Himmler in 1935, created a series of maternity homes to accommodate single mothers during their pregnancies. Both parents were examined for racial suitability before acceptance. The resulting children were often adopted into SS families. The homes were also made available to the wives of SS and Nazi Party members, who quickly filled over half the available spots.

Existing laws banning abortion except for medical reasons were strictly enforced by the Nazi regime. The number of abortions declined from 35,000 per year at the start of the 1930s to fewer than 2,000 per year at the end of the decade, though in 1935 a law was passed allowing abortions for eugenics reasons.

Nazi Germany had a strong anti-tobacco movement, as pioneering research by Franz H. Müller in 1939 demonstrated a causal link between smoking and lung cancer. The Reich Health Office took measures to try to limit smoking, including producing lectures and pamphlets. Smoking was banned in many workplaces, on trains, and among on-duty members of the military. Government agencies also worked to control other carcinogenic substances such as asbestos and pesticides. As part of a general public health campaign, water supplies were cleaned up, lead and mercury were removed from consumer products, and women were urged to undergo regular screenings for breast cancer.

Government-run health care insurance plans were available, but Jews were denied coverage starting in 1933. That same year, Jewish doctors were forbidden to treat government-insured patients. In 1937, Jewish doctors were forbidden to treat non-Jewish patients and in 1938 their right to practice medicine was removed entirely.

Medical experiments, many of them pseudoscientific, were performed on concentration camp inmates beginning in 1941. The most notorious doctor to perform medical experiments was SS-"Hauptsturmführer" Dr. Josef Mengele, camp doctor at Auschwitz. Many of his victims died or were intentionally killed. Concentration camp inmates were made available for purchase by pharmaceutical companies for drug testing and other experiments.

Nazi society had elements supportive of animal rights and many people were fond of zoos and wildlife. The government took several measures to ensure the protection of animals and the environment. In 1933, the Nazis enacted a stringent animal-protection law that affected what was allowed for medical research. The law was only loosely enforced, and in spite of a ban on vivisection, the Ministry of the Interior readily handed out permits for experiments on animals.

The Reich Forestry Office under Göring enforced regulations that required foresters to plant a variety of trees to ensure suitable habitat for wildlife, and a new Reich Animal Protection Act became law in 1933. The regime enacted the Reich Nature Protection Act in 1935 to protect the natural landscape from excessive economic development. It allowed for the expropriation of privately owned land to create nature preserves and aided in long-range planning. Perfunctory efforts were made to curb air pollution, but little enforcement of existing legislation was undertaken once the war began.

When the Nazis seized power in 1933, roughly 67 percent of the population of Germany was Protestant, 33 percent was Roman Catholic, while Jews made up less than 1 percent. According to 1939 census, 54 percent considered themselves Protestant, 40 percent Roman Catholic, 3.5 percent "Gottgläubig" (God-believing; a Nazi religious movement) and 1.5 percent nonreligious.

Under the "Gleichschaltung" process, Hitler attempted to create a unified Protestant Reich Church from Germany's 28 existing Protestant state churches, with the ultimate goal of eradication of the churches in Germany. Pro-Nazi Ludwig Müller was installed as Reich Bishop and the pro-Nazi pressure group German Christians gained control of the new church. They objected to the Old Testament because of its Jewish origins and demanded that converted Jews be barred from their church. Pastor Martin Niemöller responded with the formation of the Confessing Church, from which some clergymen opposed the Nazi regime. When in 1935 the Confessing Church synod protested the Nazi policy on religion, 700 of their pastors were arrested. Müller resigned and Hitler appointed Hanns Kerrl as Minister for Church Affairs to continue efforts to control Protestantism. In 1936, a Confessing Church envoy protested to Hitler against the religious persecutions and human rights abuses. Hundreds more pastors were arrested. The church continued to resist and by early 1937 Hitler abandoned his hope of uniting the Protestant churches. Niemöller was arrested on 1 July 1937 and spent most of the next seven years in Sachsenhausen concentration camp and Dachau. Theological universities were closed and pastors and theologians of other Protestant denominations were also arrested.
Persecution of the Catholic Church in Germany followed the Nazi takeover. Hitler moved quickly to eliminate political Catholicism, rounding up functionaries of the Catholic-aligned Bavarian People's Party and Catholic Centre Party, which along with all other non-Nazi political parties ceased to exist by July. The "Reichskonkordat" (Reich Concordat) treaty with the Vatican was signed in 1933, amid continuing harassment of the church in Germany. The treaty required the regime to honour the independence of Catholic institutions and prohibited clergy from involvement in politics. Hitler routinely disregarded the Concordat, closing all Catholic institutions whose functions were not strictly religious. Clergy, nuns and lay leaders were targeted, with thousands of arrests over the ensuing years, often on trumped-up charges of currency smuggling or immorality. Several Catholic leaders were targeted in the 1934 Night of the Long Knives assassinations. Most Catholic youth groups refused to dissolve themselves and Hitler Youth leader Baldur von Schirach encouraged members to attack Catholic boys in the streets. Propaganda campaigns claimed the church was corrupt, restrictions were placed on public meetings and Catholic publications faced censorship. Catholic schools were required to reduce religious instruction and crucifixes were removed from state buildings.

Pope Pius XI had the ""Mit brennender Sorge"" ("With Burning Concern") encyclical smuggled into Germany for Passion Sunday 1937 and read from every pulpit as it denounced the systematic hostility of the regime toward the church. In response, Goebbels renewed the regime's crackdown and propaganda against Catholics. Enrolment in denominational schools dropped sharply and by 1939 all such schools were disbanded or converted to public facilities. Later Catholic protests included the 22 March 1942 pastoral letter by the German bishops on "The Struggle against Christianity and the Church". About 30 percent of Catholic priests were disciplined by police during the Nazi era. A vast security network spied on the activities of clergy and priests were frequently denounced, arrested or sent to concentration camps – many to the dedicated clergy barracks at Dachau. In the areas of Poland annexed in 1939, the Nazis instigated a brutal suppression and systematic dismantling of the Catholic Church.

Alfred Rosenberg, head of the Nazi Party Office of Foreign Affairs and Hitler's appointed cultural and educational leader for Nazi Germany, considered Catholicism to be among the Nazis' chief enemies. He planned the "extermination of the foreign Christian faiths imported into Germany", and for the Bible and Christian cross to be replaced in all churches, cathedrals, and chapels with copies of "Mein Kampf" and the swastika. Other sects of Christianity were also targeted, with Chief of the Nazi Party Chancellery Martin Bormann publicly proclaiming in 1941, "National Socialism and Christianity are irreconcilable." Shirer writes that opposition to Christianity within Party leadership was so pronounced that, "the Nazi regime intended to eventually destroy Christianity in Germany, if it could, and substitute the old paganism of the early tribal Germanic gods and the new paganism of the Nazi extremists."

While no unified resistance movement opposing the Nazi regime existed, acts of defiance such as sabotage and labour slowdowns took place, as well as attempts to overthrow the regime or assassinate Hitler. The banned Communist and Social Democratic parties set up resistance networks in the mid-1930s. These networks achieved little beyond fomenting unrest and initiating short-lived strikes. Carl Friedrich Goerdeler, who initially supported Hitler, changed his mind in 1936 and was later a participant in the July 20 plot. The Red Orchestra spy ring provided information to the Allies about Nazi war crimes, helped orchestrate escapes from Germany, and distributed leaflets. The group was detected by the Gestapo and more than 50 members were tried and executed in 1942. Communist and Social Democratic resistance groups resumed activity in late 1942, but were unable to achieve much beyond distributing leaflets. The two groups saw themselves as potential rival parties in post-war Germany, and for the most part did not co-ordinate their activities. The White Rose resistance group was primarily active in 1942–43, and many of its members were arrested or executed, with the final arrests taking place in 1944. Another civilian resistance group, the Kreisau Circle, had some connections with the military conspirators, and many of its members were arrested after the failed 20 July plot.

While civilian efforts had an impact on public opinion, the army was the only organisation with the capacity to overthrow the government. A major plot by men in the upper echelons of the military originated in 1938. They believed Britain would go to war over Hitler's planned invasion of Czechoslovakia, and Germany would lose. The plan was to overthrow Hitler or possibly assassinate him. Participants included Generaloberst Ludwig Beck, Generaloberst Walther von Brauchitsch, Generaloberst Franz Halder, Admiral Wilhelm Canaris, and Generalleutnant Erwin von Witzleben, who joined a conspiracy headed by Oberstleutnant Hans Oster and Major Helmuth Groscurth of the Abwehr. The planned coup was cancelled after the signing of the Munich Agreement in September 1938. Many of the same people were involved in a coup planned for 1940, but again the participants changed their minds and backed down, partly because of the popularity of the regime after the early victories in the war. Attempts to assassinate Hitler resumed in earnest in 1943, with Henning von Tresckow joining Oster's group and attempting to blow up Hitler's plane in 1943. Several more attempts followed before the failed 20 July 1944 plot, which was at least partly motivated by the increasing prospect of a German defeat in the war. The plot, part of Operation Valkyrie, involved Claus von Stauffenberg planting a bomb in the conference room at Wolf's Lair at Rastenburg. Hitler, who narrowly survived, later ordered savage reprisals resulting in the execution of more than 4,900 people.

The regime promoted the concept of "Volksgemeinschaft", a national German ethnic community. The goal was to build a classless society based on racial purity and the perceived need to prepare for warfare, conquest and a struggle against Marxism. The German Labour Front founded the "Kraft durch Freude" (KdF; Strength Through Joy) organisation in 1933. As well as taking control of tens of thousands of privately run recreational clubs, it offered highly regimented holidays and entertainment such as cruises, vacation destinations and concerts.

The "Reichskulturkammer" (Reich Chamber of Culture) was organised under the control of the Propaganda Ministry in September 1933. Sub-chambers were set up to control aspects of cultural life such as film, radio, newspapers, fine arts, music, theatre and literature. Members of these professions were required to join their respective organisation. Jews and people considered politically unreliable were prevented from working in the arts, and many emigrated. Books and scripts had to be approved by the Propaganda Ministry prior to publication. Standards deteriorated as the regime sought to use cultural outlets exclusively as propaganda media.

Radio became popular in Germany during the 1930s; over 70 percent of households owned a receiver by 1939, more than any other country. By July 1933, radio station staffs were purged of leftists and others deemed undesirable. Propaganda and speeches were typical radio fare immediately after the seizure of power, but as time went on Goebbels insisted that more music be played so that listeners would not turn to foreign broadcasters for entertainment.

Newspapers, like other media, were controlled by the state; the Reich Press Chamber shut down or bought newspapers and publishing houses. By 1939, over two-thirds of the newspapers and magazines were directly owned by the Propaganda Ministry. The Nazi Party daily newspaper, the "Völkischer Beobachter" ("Ethnic Observer"), was edited by Rosenberg, who also wrote "The Myth of the Twentieth Century", a book of racial theories espousing Nordic superiority. Goebbels controlled the wire services and insisted that all newspapers in Germany only publish content favourable to the regime. Under Goebbels, the Propaganda Ministry issued two dozen directives every week on exactly what news should be published and what angles to use; the typical newspaper followed the directives closely, especially regarding what to omit. Newspaper readership plummeted, partly because of the decreased quality of the content and partly because of the surge in popularity of radio. Propaganda became less effective towards the end of the war, as people were able to obtain information outside of official channels.

Authors of books left the country in droves and some wrote material critical of the regime while in exile. Goebbels recommended that the remaining authors concentrate on books themed on Germanic myths and the concept of blood and soil. By the end of 1933, over a thousand books—most of them by Jewish authors or featuring Jewish characters—had been banned by the Nazi regime. Nazi book burnings took place; nineteen such events were held on the night of 10 May 1933. Tens of thousands of books from dozens of figures, including Albert Einstein, Sigmund Freud, Helen Keller, Alfred Kerr, Marcel Proust, Erich Maria Remarque, Upton Sinclair, Jakob Wassermann, H. G. Wells, and Émile Zola were publicly burned. Pacifist works, and literature espousing liberal, democratic values were targeted for destruction, as well as any writings supporting the Weimar Republic or those written by Jewish authors.

Hitler took a personal interest in architecture and worked closely with state architects Paul Troost and Albert Speer to create public buildings in a neoclassical style based on Roman architecture. Speer constructed imposing structures such as the Nazi party rally grounds in Nuremberg and a new Reich Chancellery building in Berlin. Hitler's plans for rebuilding Berlin included a gigantic dome based on the Pantheon in Rome and a triumphal arch more than double the height of the Arc de Triomphe in Paris. Neither structure was built.

Hitler's belief that abstract, Dadaist, expressionist and modern art were decadent became the basis for policy. Many art museum directors lost their posts in 1933 and were replaced by party members. Some 6,500 modern works of art were removed from museums and replaced with works chosen by a Nazi jury. Exhibitions of the rejected pieces, under titles such as "Decadence in Art", were launched in sixteen different cities by 1935. The Degenerate Art Exhibition, organised by Goebbels, ran in Munich from July to November 1937. The exhibition proved wildly popular, attracting over two million visitors.

Composer Richard Strauss was appointed president of the "Reichsmusikkammer" (Reich Music Chamber) on its founding in November 1933. As was the case with other art forms, the Nazis ostracised musicians who were deemed racially unacceptable and for the most part disapproved of music that was too modern or atonal. Jazz was considered especially inappropriate and foreign jazz musicians left the country or were expelled. Hitler favoured the music of Richard Wagner, especially pieces based on Germanic myths and heroic stories, and attended the Bayreuth Festival each year from 1933 to 1942.

Movies were popular in Germany in the 1930s and 1940s, with admissions of over a billion people in 1942, 1943 and 1944. By 1934, German regulations restricting currency exports made it impossible for US film makers to take their profits back to America, so the major film studios closed their German branches. Exports of German films plummeted, as their antisemitic content made them impossible to show in other countries. The two largest film companies, Universum Film AG and Tobis, were purchased by the Propaganda Ministry, which by 1939 was producing most German films. The productions were not always overtly propagandistic, but generally had a political subtext and followed party lines regarding themes and content. Scripts were pre-censored.

Leni Riefenstahl's "Triumph of the Will" (1935)—documenting the 1934 Nuremberg Rally—and "Olympia" (1938)—covering the 1936 Summer Olympics—pioneered techniques of camera movement and editing that influenced later films. New techniques such as telephoto lenses and cameras mounted on tracks were employed. Both films remain controversial, as their aesthetic merit is inseparable from their propagandising of National Socialist ideals.

The Allied powers organised war crimes trials, beginning with the Nuremberg trials, held from November 1945 to October 1946, of 23 top Nazi officials. They were charged with four counts—conspiracy to commit crimes, crimes against peace, war crimes and crimes against humanity—in violation of international laws governing warfare. All but three of the defendants were found guilty and twelve were sentenced to death. Twelve Subsequent Nuremberg trials of 184 defendants were held between 1946 and 1949. Between 1946 and 1949, the Allies investigated 3,887 cases, of which 489 were brought to trial. The result was convictions of 1,426 people; 297 of these were sentenced to death and 279 to life in prison, with the remainder receiving lesser sentences. About 65 percent of the death sentences were carried out. Poland was more active than other nations in investigating war crimes, for example prosecuting 673 of the total 789 Auschwitz staff brought to trial.

The political programme espoused by Hitler and the Nazis brought about a world war, leaving behind a devastated and impoverished Europe. Germany itself suffered wholesale destruction, characterised as "Stunde Null" (Zero Hour). The number of civilians killed during the Second World War was unprecedented in the history of warfare. As a result, Nazi ideology and the actions taken by the regime are almost universally regarded as gravely immoral. Historians, philosophers, and politicians often use the word "evil" to describe Hitler and the Nazi regime. Interest in Nazi Germany continues in the media and the academic world. While Evans remarks that the era "exerts an almost universal appeal because its murderous racism stands as a warning to the whole of humanity", young neo-Nazis enjoy the shock value that Nazi symbols or slogans provide. The display or use of Nazi symbolism such as flags, swastikas, or greetings is illegal in Germany and Austria.

The process of denazification, which was initiated by the Allies as a way to remove Nazi Party members was only partially successful, as the need for experts in such fields as medicine and engineering was too great. However, expression of Nazi views was frowned upon, and those who expressed such views were frequently dismissed from their jobs. From the immediate post-war period through the 1950s, people avoided talking about the Nazi regime or their own wartime experiences. While virtually every family suffered losses during the war has a story to tell, Germans kept quiet about their experiences and felt a sense of communal guilt, even if they were not directly involved in war crimes.

The trial of Adolf Eichmann in 1961 and the broadcast of the television miniseries "Holocaust" in 1979 brought the process of "Vergangenheitsbewältigung" (coping with the past) to the forefront for many Germans. Once study of Nazi Germany was introduced into the school curriculum starting in the 1970s, people began researching the experiences of their family members. Study of the era and a willingness to critically examine its mistakes has led to the development of a strong democracy in Germany, but with lingering undercurrents of antisemitism and neo-Nazi thought.

In 2017 a Körber Foundation survey found that 40 percent of 14-year-olds in Germany did not know what Auschwitz was. The journalist Alan Posener attributed the country's "growing historical amnesia" in part to a failure by the German film and television industry to reflect the country's history accurately.



</doc>
<doc id="21214" url="https://en.wikipedia.org/wiki?curid=21214" title="Naraoiidae">
Naraoiidae

Naraoiidae is a family, of extinct, soft-shelled trilobite-like arthropods, that belongs to the order Nectaspida. Species included in the Naraoiidae are known from the second half of the Lower Cambrian to the end of the Upper Silurian. The total number of collection sites is limited and distributed over a vast period of time: Maotianshan Shale and Balang Formation (China), Burgess Shale and Bertie Formation (Canada), the Šárka Formation (Czech Republic), Emu Bay Shale (Australia), Idaho and Utah (USA). This is probably due to the rare occurrence of the right circumstances for soft tissue preservation, needed for these non-calcified exoskeletons.

Naraoiids probably were deposit feeders ("Naraoia" and "Pseudonaraoia"), predators or scavengers ("Misszhouia"), living on the sea floor.

The species of the family "Naraoiidae" are almost flat (dorso-ventrally). The upper (or dorsal) side of the body consists of a non-calcified transversely oval or semi-circular headshield (cephalon), and a circular to long oval tailshield (pygidium) equal to or longer than the cephalon, without any body segments in between. The body is narrowed at the articulation between cephalon and pygidium. The antennas are long and many-segmented. There are no eyes. The 17 to 25 pairs of legs have two branches on a common basis, like trilobites. The outer (dorsal) branches of the limbs (exopods) have flattened side branches (setae) on the shaft (probably acting as gills). The inner branches (or endopods are composed of 6 or 7 segments (or podomeres).

Naraoiidae lack thoracic segments (or tergites), while the species of the sister family Liwiidae have between 3 and 6 tergites.

The taxonomic placement of the Naraoiidae has long been debated until detailed appendages were uncovered, that showed that "N. compacta" shares biramous legs of very comparable anatomy with trilobites. Some debate is still going on if the parent taxon "Nektaspida" should be included in the Trilobita, or is better placed as a sister group.



</doc>
<doc id="21215" url="https://en.wikipedia.org/wiki?curid=21215" title="Northwest Passage">
Northwest Passage

The Northwest Passage (NWP) is the sea route between the Atlantic and Pacific oceans through the Arctic Ocean, along the northern coast of North America via waterways through the Canadian Arctic Archipelago. The eastern route along the Arctic coasts of Norway and Siberia is accordingly called the Northeast Passage (NEP).

The various islands of the archipelago are separated from one another and from the Canadian mainland by a series of Arctic waterways collectively known as the Northwest Passages or Northwestern Passages.

For centuries, European explorers sought a navigable passage as a possible trade route to Asia. An ice-bound northern route was discovered in 1850 by the Irish explorer Robert McClure; it was through a more southerly opening in an area explored by the Scotsman John Rae in 1854 that Norwegian Roald Amundsen made the first complete passage in 1903–1906. Until 2009, the Arctic pack ice prevented regular marine shipping throughout most of the year. Arctic sea ice decline has rendered the waterways more navigable for ice navigation.

The contested sovereignty claims over the waters may complicate future shipping through the region: the Canadian government maintains that the Northwestern Passages are part of Canadian Internal Waters, but the United States and various European countries claim that they are an international strait and transit passage, allowing free and unencumbered passage. If, as has been claimed, parts of the eastern end of the Passage are barely deep, the route's viability as a Euro-Asian shipping route is reduced. In 2016 a Chinese shipping line expressed a desire to make regular voyages of cargo ships using the passage to the eastern United States and Europe, after a successful passage by "Nordic Orion" of 73,500 tonnes deadweight tonnage in September 2013. Fully loaded, "Nordic Orion" sat too deep in the water to sail through the Panama Canal.

Before the Little Ice Age (late Middle Ages to the 19th century), Norwegian Vikings sailed as far north and west as Ellesmere Island, Skraeling Island and Ruin Island for hunting expeditions and trading with the Inuit and people of the Dorset culture who already inhabited the region. Between the end of the 15th century and the 20th century, colonial powers from Europe dispatched explorers in an attempt to discover a commercial sea route north and west around North America. The Northwest Passage represented a new route to the established trading nations of Asia.

England called the hypothetical northern route the "Northwest Passage." The desire to establish such a route motivated much of the European exploration of both coasts of North America, also known as the New World. When it became apparent that there was no route through the heart of the continent, attention turned to the possibility of a passage through northern waters. There was a lack of scientific knowledge about conditions; for instance, some people believed that seawater was incapable of freezing. (As late as the mid-18th century, Captain James Cook had reported that Antarctic icebergs had yielded fresh water, seemingly confirming the hypothesis). Explorers thought that an open water route close to the North Pole must exist. The belief that a route lay to the far north persisted for several centuries and led to numerous expeditions into the Arctic. Many ended in disaster, including that by Sir John Franklin in 1845. While searching for him the McClure Arctic Expedition discovered the Northwest Passage in 1850.

In 1906, the Norwegian explorer Roald Amundsen first successfully completed a passage from Greenland to Alaska in the sloop . Since that date, several fortified ships have made the journey.

From east to west, the direction of most early exploration attempts, expeditions entered the passage from the Atlantic Ocean via the Davis Strait and through Baffin Bay, both of which are in Canada. Five to seven routes have been taken through the Canadian Arctic Archipelago, via the McClure Strait, Dease Strait, and the Prince of Wales Strait, but not all of them are suitable for larger ships. From there ships passed through waterways through the Beaufort Sea, Chukchi Sea, and Bering Strait (separating Russia and Alaska), into the Pacific Ocean.

In the 21st century, major changes to the ice pack due to climate change have stirred speculation that the passage may become clear enough of ice to permit safe commercial shipping for at least part of the year. On August 21, 2007, the Northwest Passage became open to ships without the need of an icebreaker. According to Nalan Koc of the Norwegian Polar Institute, this was the first time the Passage has been clear since they began keeping records in 1972. The Northwest Passage opened again on August 25, 2008. It is usually reported in mainstream media that ocean thawing will open up the Northwest Passage (and the Northern Sea Route) for various kind of ships, making it possible to sail around the Arctic ice cap and possibly cutting thousands of miles off shipping routes. Warning that the NASA satellite images indicated the Arctic may have entered a "death spiral" caused by climate change, Professor Mark Serreze, a sea ice specialist at the U.S. National Snow and Ice Data Center (NSIDC) said: "The passages are open. It's a historic event. We are going to see this more and more as the years go by."

On the other hand, some thick sections of ice will remain hard to melt in the shorter term. Such drifting and large chunks of ice, especially in springtime, can be problematic as they can clog entire straits or severely damage a ship's hull. Cargo routes may therefore be slower and uncertain, depending on prevailing conditions and the ability to predict them. Because a plurality of containerized traffic operates in a just-in-time mode (which does not tolerate delays well) and the relative isolation of the passage (which impedes shipping companies from optimizing their operations by grouping multiple stopovers on the same itinerary), the Northwest Passage and other Arctic routes are not always seen as promising shipping lanes by industry insiders, at least for the time being. The uncertainty related to physical damage to ships is also thought to translate into higher insurance premiums, especially because of the technical challenges posed by Arctic navigation (as of 2014, only 12 percent of Canada's Arctic waters have been charted to modern standards).

The Beluga group of Bremen, Germany, sent the first Western commercial vessels through the Northern Sea Route (Northeast Passage) in 2009. Canada's Prime Minister Stephen Harper announced that "ships entering the North-West passage should first report to his government."

The first commercial cargo ship to have sailed through the Northwest Passage was in August 1969. SS "Manhattan", of 115,000 deadweight tonnage, was the largest commercial vessel ever to navigate the Northwest Passage.

The largest passenger ship to navigate the Northwest Passage was the cruise liner of gross tonnage 69,000. Starting on August 10, 2016, the ship sailed from Vancouver to New York City with 1,500 passengers and crew, taking 28 days.

In 2018, two of the freighters leaving Baffinland's port in the Milne Inlet, on Baffin Island's north shore, were bound for ports in Asia. Those freighters did not sail west through the remainder of the Northwest Passage, they sailed east, rounded the tip of Greenland, and transitted Russia's Northern Sea Route.

The Northwest Passage includes three sections:

Many attempts were made to find a salt water exit west from Hudson Bay, but the Fury and Hecla Strait in the far north is blocked by ice. The eastern entrance and main axis of the northwest passage, the Parry Channel, was found in 1819. The approach from the west through Bering Strait is impractical because of the need to sail around ice near Point Barrow. East of Point Barrow the coast is fairly clear in summer. This area was mapped in pieces from overland in 1821–1839. This leaves the large rectangle north of the coast, south of Parry Channel and east of Baffin Island. This area was mostly mapped in 1848–1854 by ships looking for Franklin's lost expedition. The first crossing was made by Amundsen in 1903–1905. He used a small ship and hugged the coast.

The International Hydrographic Organization defines the limits of the Northwestern Passages as follows:

As a result of their westward explorations and their settlement of Greenland, the Vikings sailed as far north and west as Ellesmere Island, Skraeling Island for hunting expeditions and trading with Inuit groups. The subsequent arrival of the Little Ice Age is thought to have been one of the reasons that European seafaring into the Northwest Passage ceased until the late 15th century.

In 1539, Hernán Cortés commissioned Francisco de Ulloa to sail along the Baja California Peninsula on the western coast of North America. Ulloa concluded that the Gulf of California was the southernmost section of a strait supposedly linking the Pacific with the Gulf of Saint Lawrence. His voyage perpetuated the notion of the Island of California and saw the beginning of a search for the Strait of Anián.

The strait probably took its name from Ania, a Chinese province mentioned in a 1559 edition of Marco Polo's book; it first appears on a map issued by Italian cartographer Giacomo Gastaldi about 1562. Five years later Bolognino Zaltieri issued a map showing a narrow and crooked Strait of Anian separating Asia from the Americas. The strait grew in European imagination as an easy sea lane linking Europe with the residence of Khagan (the Great Khan) in Cathay (northern China).

Cartographers and seamen tried to demonstrate its reality. Sir Francis Drake sought the western entrance in 1579. The Greek pilot Juan de Fuca, sailing from Acapulco (in Mexico) under the flag of the Spanish crown, claimed he had sailed the strait from the Pacific to the North Sea and back in 1592. The Spaniard Bartholomew de Fonte claimed to have sailed from Hudson Bay to the Pacific via the strait in 1640.

The first recorded attempt to discover the Northwest Passage was the east–west voyage of John Cabot in 1497, sent by Henry VII in search of a direct route to the Orient. In 1524, Charles V sent Estêvão Gomes to find a northern Atlantic passage to the Spice Islands. An English expedition was launched in 1576 by Martin Frobisher, who took three trips west to what is now the Canadian Arctic in order to find the passage. Frobisher Bay, which he first charted, is named after him.

As part of another expedition, in July 1583 Sir Humphrey Gilbert, who had written a treatise on the discovery of the passage and was a backer of Frobisher, claimed the territory of Newfoundland for the English crown. On August 8, 1585, the English explorer John Davis entered Cumberland Sound, Baffin Island.

The major rivers on the east coast were also explored in case they could lead to a transcontinental passage. Jacques Cartier's explorations of the Saint Lawrence River in 1535 were initiated in hope of finding a way through the continent. Cartier became persuaded that the St. Lawrence was the Passage; when he found the way blocked by rapids at what is now Montreal, he was so certain that these rapids were all that was keeping him from China (in French, "la Chine"), that he named the rapids for China. Samuel de Champlain renamed them Sault Saint-Louis in 1611, but the name was changed to Lachine Rapids in the mid-19th century.

In 1602, George Weymouth became the first European to explore what would later be called Hudson Strait when he sailed into the Strait. Weymouth's expedition to find the Northwest Passage was funded jointly by the British East India Company and the Muscovy Company. "Discovery" was the same ship used by Henry Hudson on his final voyage.

John Knight, employed by the British East India Company and the Muscovy Company, set out in 1606 to follow up on Weymouth's discoveries and find the Northwest Passage. After his ship ran aground and was nearly crushed by ice, Knight disappeared while searching for a better anchorage.

In 1609, Henry Hudson sailed up what is now called the Hudson River in search of the Passage; encouraged by the saltiness of the water in the estuary, he reached present-day Albany, New York, before giving up. On September 14, 1609, the explorer Henry Hudson entered the Tappan Zee while sailing upstream from New York Harbor. At first, Hudson believed the widening of the river indicated that he had found the Northwest Passage. He proceeded upstream as far as present-day Troy before concluding that no such strait existed there. He later explored the Arctic and Hudson Bay.

In 1611, while in James Bay, Hudson's crew mutinied. They set Hudson and his teenage son John, along with seven sick, infirm, or loyal crewmen, adrift in a small open boat. He was never seen again. Cree oral legend reports that the survivors lived and traveled with the Cree for more than a year.

A mission was sent out in 1612, again in "Discovery", commanded by Sir Thomas Button to find Henry Hudson and continue through the Northwest Passage. After failing to find Hudson, and exploring the west coast of Hudson Bay, Button returned home due to illness in the crew. In 1614, William Gibbons attempted to find the Passage, but was turned back by ice. The next year, 1615, Robert Bylot, a survivor of Hudson's crew, returned to Hudson Strait in "Discovery", but was turned back by ice. Bylot tried again in 1616 with William Baffin. They sailed as far as Lancaster Sound and reached 77°45′ North latitude, a record which stood for 236 years, before being blocked by ice.

On May 9, 1619, under the auspices of King Christian IV of Denmark–Norway, Jens Munk set out with 65 men and the king's two ships, "Einhörningen" (Unicorn), a small frigate, and "Lamprenen" (Lamprey), a sloop, which were outfitted under his own supervision. His mission was to discover the Northwest Passage to the Indies and China. Munk penetrated Davis Strait as far north as 69°, found Frobisher Bay, and then spent almost a month fighting his way through Hudson Strait. In September 1619, he found the entrance to Hudson Bay and spent the winter near the mouth of the Churchill River. Cold, famine, and scurvy destroyed so many of his men that only he and two other men survived. With these men, he sailed for home with "Lamprey" on July 16, 1620, reaching Bergen, Norway, on September 20, 1620.

René-Robert Cavelier, Sieur de La Salle built the sailing ship, , in his quest to find the Northwest Passage via the upper Great Lakes. "Le Griffon" disappeared in 1679 on the return trip of her maiden voyage. In the spring of 1682, La Salle made his famous voyage down the Mississippi River to the Gulf of Mexico. La Salle led an expedition from France in 1684 to establish a French colony on the Gulf of Mexico. He was murdered by his followers in 1687.
Henry Ellis, born in Ireland, was part of a company aiming to discover the Northwest Passage in May 1746. After the difficult extinction of a fire on board the ship, he sailed to Greenland, where he traded goods with the Inuit peoples on July 8, 1746. He crossed to the town of Fort Nelson and spent the summer on the Hayes River. He renewed his efforts in June 1747, without success, before returning to England.

In 1772, Samuel Hearne travelled overland northwest from Hudson Bay to the Arctic Ocean, thereby proving that there was no strait connecting Hudson Bay to the Pacific Ocean.

 Most Northwest Passage expeditions originated in Europe or on the east coast of North America, seeking to traverse the Passage in the westbound direction. Some progress was made in exploring the western reaches of the imagined passage.

In 1728 Vitus Bering, a Danish Navy officer in Russian service, used the strait first discovered by Semyon Dezhnyov in 1648 but later accredited to and named after Bering (the Bering Strait). He concluded that North America and Russia were separate land masses by sailing between them. In 1741 with Lieutenant Aleksei Chirikov, he explored seeking further lands beyond Siberia. While they were separated, Chirikov discovered several of the Aleutian Islands while Bering charted the Alaskan region. His ship was wrecked off the Kamchatka Peninsula, as many of his crew were disabled by scurvy.

The Spanish made several voyages to the northwest coast of North America during the late 18th century. Determining whether a Northwest Passage existed was one of the motives for their efforts. Among the voyages that involved careful searches for a Passage included the 1775 and 1779 voyages of Juan Francisco de la Bodega y Quadra. The journal of Francisco Antonio Mourelle, who served as Quadra's second in command in 1775, fell into English hands. It was translated and published in London, stimulating exploration.

Captain James Cook made use of the journal during his explorations of the region. In 1791 Alessandro Malaspina sailed to Yakutat Bay, Alaska, which was rumoured to be a Passage. In 1790 and 1791 Francisco de Eliza led several exploring voyages into the Strait of Juan de Fuca, searching for a possible Northwest Passage and finding the Strait of Georgia. To fully explore this new inland sea, an expedition under Dionisio Alcalá Galiano was sent in 1792. He was explicitly ordered to explore all channels that might turn out to be a Northwest Passage.

In 1776, Captain James Cook was dispatched by the Admiralty in Great Britain on an expedition to explore the Passage. A 1745 act, when extended in 1775, promised a £20,000 prize for whoever discovered the passage. Initially the Admiralty had wanted Charles Clerke to lead the expedition, with Cook (in retirement following his exploits in the Pacific) acting as a consultant. However, Cook had researched Bering's expeditions, and the Admiralty ultimately placed their faith in the veteran explorer to lead, with Clerke accompanying him.

After journeying through the Pacific, to make an attempt from the west, Cook began at Nootka Sound in April 1778. He headed north along the coastline, charting the lands and searching for the regions sailed by the Russians 40 years previously. The Admiralty's orders had commanded the expedition to ignore all inlets and rivers until they reached a latitude of 65°N. Cook, however, failed to make any progress in sighting a Northwestern Passage.

Various officers on the expedition, including William Bligh, George Vancouver, and John Gore, thought the existence of a route was 'improbable'. Before reaching 65°N they found the coastline pushing them further south, but Gore convinced Cook to sail on into the Cook Inlet in the hope of finding the route. They continued to the limits of the Alaskan peninsula and the start of the chain of Aleutian Islands. Despite reaching 70°N, they encountered nothing but icebergs.

From 1792 to 1794, the Vancouver Expedition (led by George Vancouver who had previously accompanied Cook) surveyed in detail all the passages from the Northwest Coast. He confirmed that there was no such passage south of the Bering Strait. This conclusion was supported by the evidence of Alexander MacKenzie, who explored the Arctic and Pacific Oceans in 1793.

In the first half of the 19th century, some parts of the Northwest Passage (north of the Bering Strait) were explored separately by many expeditions, including those by John Ross, Elisha Kent Kane, William Edward Parry, and James Clark Ross; overland expeditions were also led by John Franklin, George Back, Peter Warren Dease, Thomas Simpson, and John Rae. In 1826 Frederick William Beechey explored the north coast of Alaska, discovering Point Barrow.

Sir Robert McClure was credited with the discovery of the Northwest Passage in 1851 when he looked across McClure Strait from Banks Island and viewed Melville Island. However, this strait was not navigable to ships at that time. The only usable route linking the entrances of Lancaster Sound and Dolphin and Union Strait was discovered by John Rae in 1854.

In 1845, a lavishly equipped two-ship expedition led by Sir John Franklin sailed to the Canadian Arctic to chart the last unknown swaths of the Northwest Passage. Confidence was high, as they estimated there was less than remaining of unexplored Arctic mainland coast. When the ships failed to return, relief expeditions and search parties explored the Canadian Arctic, which resulted in a thorough charting of the region, along with a possible passage. Many artifacts from the expedition were found over the next century and a half, including notes that the ships were ice-locked in 1846 near King William Island, about halfway through the passage, and unable to break free. Records showed Franklin died in 1847 and Captain Francis Rawdon Moira Crozier took over command. In 1848 the expedition abandoned the two ships and its members tried to escape south across the tundra by sledge. Although some of the crew may have survived into the early 1850s, no evidence has ever been found of any survivors. In 1853 explorer John Rae was told by local Inuit about the disastrous fate of Franklin's expedition, but his reports were not welcomed in Britain.

Starvation, exposure and scurvy all contributed to the men's deaths. In 1981 Owen Beattie, an anthropologist from the University of Alberta, examined remains from sites associated with the expedition. This led to further investigations and the examination of tissue and bone from the frozen bodies of three seamen, John Torrington, William Braine and John Hartnell, exhumed from the permafrost of Beechey Island. Laboratory tests revealed high concentrations of lead in all three (the expedition carried 8,000 tins of food sealed with a lead-based solder). Another researcher has suggested botulism caused deaths among crew members. New evidence, confirming reports first made by John Rae in 1854 based on Inuit accounts, has shown that the last of the crew resorted to cannibalism of deceased members in an effort to survive.

During the search for Franklin, Commander Robert McClure and his crew in traversed the Northwest Passage from west to east in the years 1850 to 1854, partly by ship and partly by sledge. McClure started out from England in December 1849, sailed the Atlantic Ocean south to Cape Horn and entered the Pacific Ocean. He sailed the Pacific north and passed through the Bering Strait, turning east at that point and reaching Banks Island.

McClure's ship was trapped in the ice for three winters near Banks Island, at the western end of Viscount Melville Sound. Finally McClure and his crew—who were by that time dying of starvation—were found by searchers who had travelled by sledge over the ice from a ship of Sir Edward Belcher's expedition. They rescued McClure and his crew, returning with them to Belcher's ships, which had entered the Sound from the east. McClure and his crew returned to England in 1854 on one of Belcher's ships. They were the first people known to circumnavigate the Americas and to discover and transit the Northwest Passage, albeit by ship and by sledge over the ice. (Both McClure and his ship were found by a party from HMS "Resolute", one of Belcher's ships, so his sledge journey was relatively short.)

This was an astonishing feat for that day and age, and McClure was knighted and promoted in rank. (He was made rear-admiral in 1867.) Both he and his crew also shared £10,000 awarded them by the British Parliament. In July 2010 Canadian archaeologists found his ship, HMS "Investigator," fairly intact but sunk about below the surface.

The expeditions by Franklin and McClure were in the tradition of British exploration: well-funded ship expeditions using modern technology, and usually including British Naval personnel. By contrast, John Rae was an employee of the Hudson's Bay Company, which operated a far-flung trade network and drove exploration of the Canadian North. They adopted a pragmatic approach and tended to be land-based. While Franklin and McClure tried to explore the passage by sea, Rae explored by land. He used dog sleds and techniques of surviving in the environment which he had learned from the native Inuit. The Franklin and McClure expeditions each employed hundreds of personnel and multiple ships. John Rae's expeditions included fewer than ten people and succeeded. Rae was also the explorer with the best safety record, having lost only one man in years of traversing Arctic lands. In 1854, Rae returned to the cities with information from the Inuit about the disastrous fate of the Franklin expedition.

The first explorer to conquer the Northwest Passage solely by ship was the Norwegian explorer Roald Amundsen. In a three-year journey between 1903 and 1906, Amundsen explored the passage with a crew of six. Amundsen, who had sailed to escape creditors seeking to stop the expedition, completed the voyage in the converted 45 net register tonnage () herring boat "Gjøa". "Gjøa" was much smaller than vessels used by other Arctic expeditions and had a shallow draft. Amundsen intended to hug the shore, live off the limited resources of the land and sea through which he was to travel, and had determined that he needed to have a tiny crew to make this work. (Trying to support much larger crews had contributed to the catastrophic failure of John Franklin's expedition fifty years previously). The ship's shallow draft was intended to help her traverse the shoals of the Arctic straits.

Amundsen set out from Kristiania (Oslo) in June 1903 and was west of the Boothia Peninsula by late September. "Gjøa" was put into a natural harbour on the south shore of King William Island; by October 3 she was iced in. There the expedition remained for nearly two years, with the expedition members learning from the local Inuit people and undertaking measurements to determine the location of the North Magnetic Pole. The harbour, now known as Gjoa Haven, later developed as the only permanent settlement on the island.

After completing the Northwest Passage portion of this trip and having anchored near Herschel Island, Amundsen skied to the city of Eagle, Alaska. He sent a telegram announcing his success and skied the return to rejoin his companions. Although his chosen east–west route, via the Rae Strait, contained young ice and thus was navigable, some of the waterways were extremely shallow ( deep), making the route commercially impractical.

The first traversal of the Northwest Passage via dog sled was accomplished by Greenlander Knud Rasmussen while on the Fifth Thule Expedition (1921–1924). Rasmussen and two Greenland Inuit travelled from the Atlantic to the Pacific over the course of 16 months via dog sled.

Canadian Royal Canadian Mounted Police officer Henry Larsen was the second to sail the passage, crossing west to east, leaving Vancouver on June 23, 1940 and arriving at Halifax on October 11, 1942. More than once on this trip, he was uncertain whether , a Royal Canadian Mounted Police "ice-fortified" schooner, would survive the pressures of the sea ice. At one point, Larsen wondered "if we had come this far only to be crushed like a nut on a shoal and then buried by the ice." The ship and all but one of her crew survived the winter on Boothia Peninsula. Each of the men on the trip was awarded a medal by Canada's sovereign, King George VI, in recognition of this feat of Arctic navigation.

Later in 1944, Larsen's return trip was far more swift than his first. He made the trip in 86 days to sail back from Halifax, Nova Scotia, to Vancouver, British Columbia. He set a record for traversing the route in a single season. The ship, after extensive upgrades, followed a more northerly, partially uncharted route.

In 1954, completed the east-to-west transit, under the command of Captain O.C.S. Robertson, conducting hydrographic soundings along the route. She was the first warship (and the first deep draft ship) to transit the Northwest Passage and the first warship to circumnavigate North America. In 1956, HMCS "Labrador" again completed the east-to-west transit, this time under the command of Captain T.C. Pullen.

On July 1, 1957, the United States Coast Guard Cutter departed in company with and to search for a deep-draft channel through the Arctic Ocean and to collect hydrographic information. The US Coast Guard Squadron was escorted through Bellot Strait and the Eastern Arctic by HMCS "Labrador". Upon her return to Greenland waters, "Storis" became the first U.S.-registered vessel to circumnavigate North America. Shortly after her return in late 1957, she was reassigned to her new home port of Kodiak, Alaska.

In 1960, completed the first submarine transit of the Northwest Passage, heading east-to-west.

In 1969, SS "Manhattan" made the passage, accompanied by the Canadian icebreakers and . The U.S. Coast Guard icebreakers and also sailed in support of the expedition.

"Manhattan" was a specially reinforced supertanker sent to test the viability of the passage for the transport of oil. While "Manhattan" succeeded, the route was deemed not to be cost-effective. The United States built the Alaska Pipeline instead.

In June 1977, sailor Willy de Roos left Belgium to attempt the Northwest Passage in his steel yacht "Williwaw". He reached the Bering Strait in September and after a stopover in Victoria, British Columbia, went on to round Cape Horn and sail back to Belgium, thus being the first sailor to circumnavigate the Americas entirely by ship.

In 1981 as part of the Transglobe Expedition, Ranulph Fiennes and Charles R. Burton completed the Northwest Passage. They left Tuktoyaktuk on July 26, 1981, in the open Boston Whaler and reached Tanquary Fiord on August 31, 1981. Their journey was the first open-boat transit from west to east and covered around , taking a route through Dolphin and Union Strait following the south coast of Victoria and King William islands, north to Resolute Bay via Franklin Strait and Peel Sound, around the south and east coasts of Devon Island, through Hell Gate and across Norwegian Bay to Eureka, Greely Bay and the head of Tanquary Fiord. Once they reached Tanquary Fiord, they had to trek via Lake Hazen to Alert before setting up their winter base camp.

In 1984, the commercial passenger vessel (which sank in the Antarctic Ocean in 2007) became the first cruise ship to navigate the Northwest Passage.

In July 1986, Jeff MacInnis and Mike Beedell set out on an catamaran called "Perception" on a 100-day sail, west to east, through the Northwest Passage. This pair was the first to sail the passage, although they had the benefit of doing so over a couple of summers.

In July 1986, David Scott Cowper set out from England in a lifeboat named "Mabel El Holland", and survived three Arctic winters in the Northwest Passage before reaching the Bering Strait in August 1989. He continued around the world via the Cape of Good Hope to return to England on September 24, 1990. His was the first vessel to circumnavigate the world via the Northwest Passage.

On July 1, 2000, the Royal Canadian Mounted Police patrol vessel , having assumed the name "St Roch II", departed Vancouver on a "Voyage of Rediscovery." "Nadon"s mission was to circumnavigate North America via the Northwest Passage and the Panama Canal, recreating the epic voyage of her predecessor, "St. Roch." The Voyage of Rediscovery was intended to raise awareness concerning "St. Roch" and kick off the fund-raising efforts necessary to ensure the continued preservation of "St. Roch". The voyage was organized by the Vancouver Maritime Museum and supported by a variety of corporate sponsors and agencies of the Canadian government.
"Nadon" is an aluminum, catamaran-hulled, high-speed patrol vessel. To make the voyage possible, she was escorted and supported by the Canadian Coast Guard icebreaker . The Coast Guard vessel was chartered by the Voyage of Rediscovery and crewed by volunteers. Throughout the voyage, she provided a variety of necessary services, including provisions and spares, fuel and water, helicopter facilities, and ice escort; she also conducted oceanographic research during the voyage. The Voyage of Rediscovery was completed in five and a half months, with "Nadon" reaching Vancouver on December 16, 2000.

On September 1, 2001, "Northabout", an aluminium sailboat with diesel engine, built and captained by Jarlath Cunnane, completed the Northwest Passage east-to-west from Ireland to the Bering Strait. The voyage from the Atlantic to the Pacific was completed in 24 days. Cunnane cruised in "Northabout" in Canada for two years before returning to Ireland in 2005 via the Northeast Passage; he completed the first east-to-west circumnavigation of the pole by a single sailboat. The Northeast Passage return along the coast of Russia was slower, starting in 2004, requiring an ice stop and winter over in Khatanga, Siberia. He returned to Ireland via the Norwegian coast in October 2005. On January 18, 2006, the Cruising Club of America awarded Jarlath Cunnane their Blue Water Medal, an award for "meritorious seamanship and adventure upon the sea displayed by amateur sailors of all nationalities."

On July 18, 2003, a father-and-son team, Richard and Andrew Wood, with Zoe Birchenough, sailed the yacht "Norwegian Blue" into the Bering Strait. Two months later she sailed into the Davis Strait to become the first British yacht to transit the Northwest Passage from west to east. She also became the only British vessel to complete the Northwest Passage in one season, as well as the only British sailing yacht to return from there to British waters.

In 2006, a scheduled cruise liner () successfully ran the Northwest Passage, helped by satellite images telling the location of sea ice.

On May 19, 2007, a French sailor, Sébastien Roubinet, and one other crew member left Anchorage, Alaska, in "Babouche", a ice catamaran designed to sail on water and slide over ice. The goal was to navigate west to east through the Northwest Passage by sail only. Following a journey of more than , Roubinet reached Greenland on September 9, 2007, thereby completing the first Northwest Passage voyage made in one season without engine.
In April 2009, planetary scientist Pascal Lee and a team of four on the Northwest Passage Drive Expedition drove the HMP "Okarian" Humvee rover a record-setting on sea-ice from Kugluktuk to Cambridge Bay, Nunavut, the longest distance driven on sea-ice in a road vehicle. The HMP "Okarian" was being ferried from the North American mainland to the Haughton–Mars Project (HMP) Research Station on Devon Island, where it would be used as a simulator of future pressurized rovers for astronauts on the Moon and Mars. The HMP "Okarian" was eventually flown from Cambridge Bay to Resolute Bay in May 2009, and then driven again on sea-ice by Lee and a team of five from Resolute to the West coast of Devon Island in May 2010. The HMP "Okarian" reached the HMP Research Station in July 2011. The Northwest Passage Drive Expedition is captured in the motion picture documentary film "Passage To Mars" (2016).

In 2009, sea ice conditions were such that at least nine small vessels and two cruise ships completed the transit of the Northwest Passage. These trips included one by Eric Forsyth on board the Westsail sailboat "Fiona", a boat he built in the 1980s. Self-financed, Forsyth, a retired engineer from the Brookhaven National Laboratory, and winner of the Cruising Club of America's Blue Water Medal, sailed the Canadian Archipelago with sailor Joey Waits, airline captain Russ Roberts and carpenter David Wilson. After successfully sailing the Passage, the 77-year-old Forsyth completed the circumnavigation of North America, returning to his home port on Long Island, New York.

Cameron Dueck and his crew aboard the 40-foot sailing yacht Silent Sound also transited in the summer of 2009. Their voyage began in Victoria, BC on June 6 and they arrived in Halifax on October 10. Dueck wrote a book about the voyage called The New Northwest Passage.

On September 9, 2010, Bear Grylls and a team of five completed a point-to-point navigation between Pond Inlet and Tuktoyaktuk in the Northwest Territories on a rigid inflatable boat (RIB). The expedition drew attention to how the effects of global warming made this journey possible and raised funds for the Global Angels charity.

On August 30, 2012 Sailing yacht , , an English SY, successfully completed the Northwest Passage in Nome, Alaska, while sailing a northern route never sailed by a sailing pleasure vessel before. After six cruising seasons in the Arctic (Greenland, Baffin Bay, Devon Island, Kane Basin, Lancaster Sound, Peel Sound, Regent Sound) and four seasons in the South (Antarctic Peninsula, Patagonia, Falkland Islands, South Georgia), SY "Billy Budd", owned by and under the command of an Italian sporting enthusiast, Mariacristina Rapisardi. Crewed by Marco Bonzanigo, five Italian friends, one Australian, one Dutch, one South African, and one New Zealander, it sailed through the Northwest Passage. The northernmost route was chosen. "Billy Budd" sailed through the Parry Channel, Viscount Melville Sound and Prince of Wales Strait, a channel long and wide which flows south into the Amundsen Gulf. During the passage "Billy Budd" – likely a first for a pleasure vessel – anchored in Winter Harbour in Melville Island, the very same site where almost 200 years ago Sir William Parry was blocked by ice and forced to winter.

On August 29, 2012, the Swedish yacht "Belzebub II," a fibreglass cutter captained by Canadian Nicolas Peissel, Swede Edvin Buregren and Morgan Peissel, became the first sailboat in history to sail through McClure Strait, part of a journey of achieving the most northerly Northwest Passage. "Belzebub II" departed Newfoundland following the coast of Greenland to Qaanaaq before tracking the sea ice to Grise Fiord, Canada's most northern community. From there the team continued through Parry Channel into McClure Strait and the Beaufort Sea, tracking the highest latitudes of 2012's record sea ice depletion before completing their Northwest Passage September 14, 2012. The expedition received extensive media coverage, including recognition by former U.S. Vice President Al Gore. The accomplishment is recorded in the Polar Scott Institute's record of Northwest Passage Transits and recognized by the Explorers Club and the Royal Canadian Geographic Society.

At 18:45 GMT on September 18, 2012, "Best Explorer", a steel cutter , skipper Nanni Acquarone, passing between the two Diomedes, was the first Italian sailboat to complete the Northwest Passage along the classical Amundsen route. Twenty-two Italian amateur sailors took part of the trip, in eight legs from Tromsø, Norway, to King Cove, Alaska, totalling . Later in 2019 "Best Explorer" skppered again by Nanni Acquarone became the first Italian sailboat to circumnavigate the Arctic sailing north of Siberia from Petropavlovsk-Kamchatsky to Tromsø and the second ever to do it clockwise.

Setting sail from Nome, Alaska, on August 18, 2012, and reaching Nuuk, Greenland, on September 12, 2012, became the largest passenger vessel to transit the Northwest Passage. The ship, carrying 481 passengers, for 26 days and at sea, followed in the path of Captain Roald Amundsen. "The World" transit of the Northwest Passage was documented by "National Geographic" photographer Raul Touzon.

In September 2013, became the first commercial bulk carrier to transit the Northwest Passage. She was carrying a cargo of of coking coal from Port Metro Vancouver, Canada, to the Finnish Port of Pori, more than would have been possible via the traditional Panama Canal route. The Northwest Passage shortened the distance by compared to traditional route via the Panama Canal.

In August and September 2016 a cruise ship was sailed through the Northwest Passage. The ship "Crystal Serenity", (with 1,000 passengers, and 600 crew) left Seward, Alaska, used Amundsen's route and reached New York on September 17. Tickets for the 32-day trip started at $22,000 and were quickly sold out. The trip was repeated in 2017. In 2017 33 vessels made a complete transit, breaking the prior record of 20 in 2012.

In September 2018, sailing yacht "Infinity" (a 36·6 m ketch) and her 22-person crew successfully sailed through the Northwest Passage. This was part of their mission to plant the Flag of Planet Earth on the remaining Arctic ice. Supported by the initiative, EarthToday, this voyage was a symbol for future global collaboration against climate change. The Flag of Planet Earth was planted on September 21, 2018, the International Day of Peace.

The Canadian government classifies the waters of the Northwest Passage in the Canadian Arctic Archipelago, as internal waters of Canada as per the United Nations Convention on the Law of the Sea and by the precedent in the drawing of baselines for other archipelagos, giving Canada the right to bar transit through these waters. Some maritime nations, including the United States and some of the European Union, claim these waters to be an international strait, where foreign vessels have the right of "transit passage." In such a regime, Canada would have the right to enact fishing and environmental regulation, and fiscal and smuggling laws, as well as laws intended for the safety of shipping, but not the right to close the passage. If the passage's deep waters become completely ice-free in summer months, they will be particularly enticing for supertankers that are too big to pass through the Panama Canal and must otherwise navigate around the tip of South America.

The dispute between Canada and the United States arose in 1969 with the trip of the U.S. oil tanker through the Arctic Archipelago. The prospect of more American traffic headed to the Prudhoe Bay Oil Field made the Canadian government realize that political action was required.

In 1985, the U.S. Coast Guard icebreaker passed through from Greenland to Alaska; the ship submitted to inspection by the Canadian Coast Guard before passing through, but the event infuriated the Canadian public and resulted in a diplomatic incident. The United States government, when asked by a Canadian reporter, indicated that they did not ask for permission as they insist that the waters were an international strait. The Canadian government issued a declaration in 1986 reaffirming Canadian rights to the waters. The United States refused to recognize the Canadian claim. In 1988 the governments of Canada and the United States signed an agreement, "Arctic Cooperation," that resolved the practical issue without solving the sovereignty questions. Under the law of the sea, ships engaged in transit passage are not permitted to engage in research. The agreement states that all U.S. Coast Guard and Navy vessels are engaged in research, and so would require permission from the Government of Canada to pass through.

However, in late 2005, it was reported that U.S. nuclear submarines had travelled unannounced through Canadian Arctic waters, breaking the "Arctic Cooperation" agreement and sparking outrage in Canada. In his first news conference after the 2006 federal election, Prime Minister-designate Stephen Harper contested an earlier statement made by the U.S. ambassador that Arctic waters were international, stating the Canadian government's intention to enforce its sovereignty there. The allegations arose after the U.S. Navy released photographs of surfaced at the North Pole.

On April 9, 2006, Canada's Joint Task Force (North) declared that the Canadian Forces will no longer refer to the region as the Northwest Passage, but as the Canadian Internal Waters. The declaration came after the successful completion of Operation Nunalivut (Inuktitut for "the land is ours"), which was an expedition into the region by five military patrols.

In 2006 a report prepared by the staff of the Parliamentary Information and Research Service of Canada suggested that because of the September 11 attacks, the United States might be less interested in pursuing the international waterways claim in the interests of having a more secure North American perimeter. This report was based on an earlier paper, "The Northwest Passage Shipping Channel: Is Canada's Sovereignty Really Floating Away?" by Andrea Charron, given to the 2004 Canadian Defence and Foreign Affairs Institute Symposium. Later in 2006 former United States Ambassador to Canada, Paul Cellucci agreed with this position; however, the succeeding ambassador, David Wilkins, stated that the Northwest Passage was in international waters.

On July 9, 2007, Prime Minister Harper announced the establishment of a deep-water port in the far North. In the press release Harper said, "Canada has a choice when it comes to defending our sovereignty over the Arctic. We either use it or lose it. And make no mistake, this Government intends to use it. Because Canada's Arctic is central to our national identity as a northern nation. It is part of our history. And it represents the tremendous potential of our future."

On July 10, 2007, Rear Admiral Timothy McGee of the U.S. Navy and Rear Admiral Brian Salerno of the U.S. Coast Guard announced that the United States would be increasing its ability to patrol the Arctic.

In June 2019, the U.S. State Department spokesperson Morgan Ortagus said the United States "believes that Canada's claim of the Northwest Passage are internal waters of Canada as inconsistent with international law" despite historical precedent regarding archipelago baselines.

In the summer of 2000, two Canadian ships took advantage of thinning summer ice cover on the Arctic Ocean to make the crossing. It is thought that climate change is likely to open the passage for increasing periods, making it potentially attractive as a major shipping route. However, the passage through the Arctic Ocean would require significant investment in escort vessels and staging ports, and it would remain seasonal. Therefore, the Canadian commercial marine transport industry does not anticipate the route as a viable alternative to the Panama Canal within the next 10 to 20 years (as of 2004).

On September 14, 2007, the European Space Agency stated that ice loss that year had opened up the historically impassable passage, setting a new low of ice cover as seen in satellite measurements which went back to 1978. According to the Arctic Climate Impact Assessment, the latter part of the 20th century and the start of the 21st had seen marked shrinkage of ice cover. The extreme loss in 2007 rendered the passage "fully navigable." However, the ESA study was based only on analysis of satellite images and could in practice not confirm anything about the actual navigation of the waters of the passage. ESA suggested the passage would be navigable "during reduced ice cover by multi-year ice pack" (namely sea ice surviving one or more summers) where previously any traverse of the route had to be undertaken during favourable seasonable climatic conditions or by specialist vessels or expeditions. The agency's report speculated that the conditions prevalent in 2007 had shown the passage may "open" sooner than expected. An expedition in May 2008 reported that the passage was not yet continuously navigable even by an icebreaker and not yet ice-free.

Scientists at a meeting of the American Geophysical Union on December 13, 2007, revealed that NASA satellites observing the western Arctic showed a 16% decrease in cloud coverage during the summer of 2007 compared to 2006. This would have the effect of allowing more sunlight to penetrate Earth's atmosphere and warm the Arctic Ocean waters, thus melting sea ice and contributing to the opening the Northwest Passage.

In 2006 the cruise liner MS "Bremen" successfully ran the Northwest Passage, helped by satellite images telling where sea ice was.

On November 28, 2008, the Canadian Broadcasting Corporation reported that the Canadian Coast Guard confirmed the first commercial ship sailed through the Northwest Passage. In September 2008, , owned by Desgagnés Transarctik Inc. and, along with the Arctic Cooperative, is part of Nunavut Sealift and Supply Incorporated (NSSI), transported cargo from Montreal to the hamlets of Cambridge Bay, Kugluktuk, Gjoa Haven, and Taloyoak. A member of the crew is reported to have claimed that "there was no ice whatsoever." Shipping from the east was to resume in the fall of 2009. Although sealift is an annual feature of the Canadian Arctic this is the first time that the western communities have been serviced from the east. The western portion of the Canadian Arctic is normally supplied by Northern Transportation Company Limited (NTCL) from Hay River, and the eastern portion by NNSI and NTCL from Churchill and Montreal.

In January 2010, the ongoing reduction in the Arctic sea ice led telecoms cable specialist Kodiak-Kenai Cable to propose the laying of a fiberoptic cable connecting London and Tokyo, by way of the Northwest Passage, saying the proposed system would nearly cut in half the time it takes to send messages from the United Kingdom to Japan.

In September 2013, the first large ice strengthened sea freighter, "Nordic Orion", used the passage.

In 2016 a new record was set when the cruise ship "Crystal Serenity" transited with 1,700 passengers and crew. "Crystal Serenity" is the largest cruise ship to navigate the Northwest Passage. Starting on August 10, 2016, the ship sailed from Vancouver to New York City, taking 28 days for the journey.

Scientists believe that reduced sea ice in the Northwest Passage has permitted some new species to migrate across the Arctic Ocean. The gray whale "Eschrichtius robustus" has not been seen in the Atlantic since it was hunted to extinction there in the 18th century, but in May 2010, one such whale turned up in the Mediterranean. Scientists speculated the whale had followed its food sources through the Northwest Passage and simply kept on going.

The plankton species "Neodenticula seminae" had not been recorded in the Atlantic for 800,000 years. Over the past few years, however, it has become increasingly prevalent there. Again, scientists believe that it got there through the reopened Northwest Passage.

In August 2010, two bowhead whales from West Greenland and Alaska respectively, entered the Northwest Passage from opposite directions and spent approximately 10 days in the same area.





</doc>
<doc id="21216" url="https://en.wikipedia.org/wiki?curid=21216" title="Nevada">
Nevada

Nevada ( or , ) is a state in the Western United States. It is bordered by Oregon to the northwest, Idaho to the northeast, California to the west, Arizona to the southeast, and Utah to the east. Nevada is the 7th most extensive, the 32nd most populous, but the 9th least densely populated of the U.S. states. Nearly three-quarters of Nevada's people live in Clark County, which contains the Las Vegas–Paradise metropolitan area, including three of the state's four largest incorporated cities. Nevada's capital is Carson City.

Nevada is officially known as the "Silver State" because of the importance of silver to its history and economy. It is also known as the "Battle Born State" because it achieved statehood during the Civil War (the words "Battle Born" also appear on the state flag); as the "Sagebrush State", for the native plant of the same name; and as the "Sage-hen State".

Nevada is largely desert and semi-arid, much of it within the Great Basin. Areas south of the Great Basin are within the Mojave Desert, while Lake Tahoe and the Sierra Nevada lie on the western edge. About 86% of the state's land is managed by various jurisdictions of the U.S. federal government, both civilian and military.

Before European contact — and still today, American Indians of the Paiute, Shoshone, and Washoe tribes inhabited the land that is now Nevada. The first Europeans to explore the region were Spanish. They called the region "Nevada" (snowy) because of the snow which covered the mountains in winter. The area formed part of the Viceroyalty of New Spain, and became part of Mexico when it gained independence in 1821. The United States annexed the area in 1848 after its victory in the Mexican–American War, and it was incorporated as part of Utah Territory in 1850. The discovery of silver at the Comstock Lode in 1859 led to a population boom that became an impetus to the creation of Nevada Territory out of western Utah Territory in 1861. Nevada became the 36th state on October 31, 1864, as the second of two states added to the Union during the Civil War (the first being West Virginia).

Nevada has a reputation for its libertarian laws. In 1940, with a population of just over 110,000 people, Nevada was by far the least-populated state, with less than half the population of the next least-populated state, Wyoming. However, legalized gambling and lenient marriage and divorce laws transformed Nevada into a major tourist destination in the 20th century. Nevada is the only U.S. state where prostitution is legal, though it is illegal in its most populated regions — Clark County (Las Vegas), Washoe County (Reno) and Carson City (which, as an independent city, is not within the boundaries of any county). The tourism industry remains Nevada's largest employer, with mining continuing as a substantial sector of the economy: Nevada is the fourth-largest producer of gold in the world. 

The name "Nevada" comes from the Spanish "nevada" , meaning "snow-covered".

Nevadans pronounce the second syllable with the "a" as in "trap" () while some people from outside of the state can pronounce it with the "a" as in "palm" (). Although the latter pronunciation is closer to the Spanish pronunciation, it is not the pronunciation used by Nevadans. State Assemblyman Harry Mortenson proposed a bill to recognize the alternate (quasi-Spanish) pronunciation of Nevada, though the bill was not supported by most legislators and never received a vote. The Nevadan pronunciation is the one used by the state legislature. At one time, the state's official tourism organization, TravelNevada, stylized the name of the state as "Nevăda", with a breve over the "a" indicating the locally preferred pronunciation, which was also available as a license plate design until 2007.

Nevada is almost entirely within the Basin and Range Province and is broken up by many north-south mountain ranges. Most of these ranges have endorheic valleys between them, which belies the image portrayed by the term Great Basin.

Much of the northern part of the state is within the Great Basin, a mild desert that experiences hot temperatures in the summer and cold temperatures in the winter. Occasionally, moisture from the Arizona Monsoon will cause summer thunderstorms; Pacific storms may blanket the area with snow. The state's highest recorded temperature was in Laughlin (elevation of ) on June 29, 1994. The coldest recorded temperature was set in San Jacinto in 1972, in the northeastern portion of the state.

The Humboldt River crosses the state from east to west across the northern part of the state, draining into the Humboldt Sink near Lovelock. Several rivers drain from the Sierra Nevada eastward, including the Walker, Truckee, and Carson rivers. All of these rivers are endorheic basins, ending in Walker Lake, Pyramid Lake, and the Carson Sink, respectively. However, not all of Nevada is within the Great Basin. Tributaries of the Snake River drain the far north, while the Colorado River, which also forms much of the boundary with Arizona, drains much of southern Nevada.

The mountain ranges, some of which have peaks above , harbor lush forests high above desert plains, creating sky islands for endemic species. The valleys are often no lower in elevation than , while some in central Nevada are above .

The southern third of the state, where the Las Vegas area is situated, is within the Mojave Desert. The area receives less rain in the winter but is closer to the Arizona Monsoon in the summer. The terrain is also lower, mostly below , creating conditions for hot summer days and cool to chilly winter nights.

Nevada and California have by far the longest diagonal line (in respect to the cardinal directions) as a state boundary at just over . This line begins in Lake Tahoe nearly offshore (in the direction of the boundary), and continues to the Colorado River where the Nevada, California, and Arizona boundaries merge southwest of the Laughlin Bridge.

The largest mountain range in the southern portion of the state is the Spring Mountain Range, just west of Las Vegas. The state's lowest point is along the Colorado River, south of Laughlin.

Nevada has 172 mountain summits with of prominence. Nevada ranks second in the United States by the number of mountains, behind Alaska, and ahead of California, Montana, and Washington. Nevada is the most mountainous state in the contiguous United States.

Nevada is the driest state in the United States. It is made up of mostly desert and semi-arid climate regions, and, with the exception of the Las Vegas Valley, the average summer diurnal temperature range approaches in much of the state. While winters in northern Nevada are long and fairly cold, the winter season in the southern part of the state tends to be of short duration and mild. Most parts of Nevada receive scarce precipitation during the year. The most rain that falls in the state falls on the lee side (east and northeast slopes) of the Sierra Nevada.

The average annual rainfall per year is about ; the wettest parts get around . Nevada's highest recorded temperature is at Laughlin on June 29, 1994 and the lowest recorded temperature is at San Jacinto on January 8, 1937. Nevada's reading is the third highest statewide record high temperature of a U.S. state, just behind Arizona's reading and California's reading.

The vegetation of Nevada is diverse and differs by state area. Nevada contains six biotic zones: alpine, sub-alpine, ponderosa pine, pinion-juniper, sagebrush and creosotebush.

Nevada is divided into political jurisdictions designated as "counties". Carson City is officially a consolidated municipality; however, for many purposes under state law, it is considered to be a county. As of 1919, there were 17 counties in the state, ranging from .

Lake County, one of the original nine counties formed in 1861, was renamed Roop County in 1862. Part of the county became Lassen County, California in 1864. In 1883, Washoe County annexed the portion that remained in Nevada.

In 1969, Ormsby County was dissolved and the Consolidated Municipality of Carson City was created by the Legislature in its place coterminous with the old boundaries of Ormsby County.

Bullfrog County was formed in 1987 from part of Nye County. After the creation was declared unconstitutional, the county was abolished in 1989.

Humboldt county was designated as a county in 1856 by Utah Territorial Legislature and again in 1861 by the new Nevada Legislature.

Clark County is the most populous county in Nevada, accounting for nearly three-quarters of its residents. Las Vegas, Nevada's most populous city, has been the county seat since the county was created in 1909 from a portion of Lincoln County, Nevada. Before that, it was a part of Arizona Territory. Clark County attracts numerous tourists: An estimated 44 million people visited Clark County in 2014.

Washoe County is the second-most populous county of Nevada. Its county seat is Reno. Washoe County includes the Reno–Sparks metropolitan area.

Lyon County is the third most populous county. It was one of the nine original counties created in 1861. It was named after Nathaniel Lyon, the first Union General to be killed in the Civil War. Its current county seat is Yerington. Its first county seat was established at Dayton on November 29, 1861.

Francisco Garcés was the first European in the area. Nevada was annexed as a part of the Spanish Empire in the northwestern territory of New Spain. Administratively, the area of Nevada was part of the Commandancy General of the Provincias Internas in the Viceroyalty of New Spain. Nevada became a part of Alta California ("Upper California") province in 1804 when the Californias were split. With the Mexican War of Independence won in 1821, the province of Alta California became a territory (state) of Mexico, with a small population. Jedediah Smith entered the Las Vegas Valley in 1827, and Peter Skene Ogden traveled the Humboldt River in 1828. When the Mormons created the State of Deseret in 1847, they laid claim to all of Nevada within the Great Basin and the Colorado watershed. They also founded the first white settlement in what is now Nevada, Mormon Station (modern-day Genoa), in 1851. In June 1855, William Bringhurst and 29 fellow Mormon missionaries from Utah arrived at a site just northeast of downtown Las Vegas and built a 150-foot square adobe fort, the first permanent structure erected in the valley, which remained under the control of Salt Lake City until the winter of 1858–1859.

As a result of the Mexican–American War and the Treaty of Guadalupe Hidalgo, Mexico permanently lost Alta California in 1848. The new areas acquired by the United States continued to be administered as territories. As part of the Mexican Cession (1848) and the subsequent California Gold Rush that used Emigrant Trails through the area, the state's area evolved first as part of the Utah Territory, then the Nevada Territory (March 2, 1861; named for the Sierra Nevada).
See History of Utah, History of Las Vegas, and the discovery of the first major U.S. deposit of silver ore in Comstock Lode under Virginia City, Nevada, in 1859.

On March 2, 1861, the Nevada Territory separated from the Utah Territory and adopted its current name, shortened from "The Sierra Nevada" (Spanish for "snow-covered mountain range"). The 1861 southern boundary is commemorated by Nevada Historical Markers 57 and 58 in Lincoln and Nye counties.

Eight days before the presidential election of 1864, Nevada became the 36th state in the union, despite lacking the minimum requisite 60,000 residents in order to become a state. (At the time Nevada's population was little more than 10,000.) Rather than sending the Nevada constitution to Washington by Pony Express, the full text was sent by telegraph at a cost of $4,303.27 — the most costly telegraph on file at the time for a single dispatch. Finally, the response from Washington came on October 31, 1864: "the pain is over, the child is born, Nevada this day was admitted into the Union". Statehood was rushed to the date of October 31 to help ensure Abraham Lincoln's reelection on November8 and post-Civil War Republican dominance in Congress, as Nevada's mining-based economy tied it to the more industrialized Union. As it turned out, however, Lincoln and the Republicans won the election handily and did not need Nevada's help.

Nevada is one of only two states to significantly expand its borders after admission to the Union. (The other is Missouri, which acquired additional territory in 1837 due to the Platte Purchase.)

In 1866 another part of the western Utah Territory was added to Nevada in the eastern part of the state, setting the current eastern boundary.

Nevada achieved its current southern boundaries on January 18, 1867, when it absorbed the portion of Pah-Ute County in the Arizona Territory west of the Colorado River, essentially all of present-day Nevada south of the 37th parallel. The transfer was prompted by the discovery of gold in the area, and officials thought Nevada would be better able to oversee the expected population boom. This area includes most of what is now Clark County and the Las Vegas metropolitan area.

Mining shaped Nevada's economy for many years (see "Silver mining in Nevada"). When Mark Twain lived in Nevada during the period described in "Roughing It", mining had led to an industry of speculation and immense wealth. However, both mining and population declined in the late 19th century. However, the rich silver strike at Tonopah in 1900, followed by strikes in Goldfield and Rhyolite, again put Nevada's population on an upward trend.

Unregulated gambling was commonplace in the early Nevada mining towns but was outlawed in 1909 as part of a nationwide anti-gambling crusade. Because of subsequent declines in mining output and the decline of the agricultural sector during the Great Depression, Nevada again legalized gambling on March 19, 1931, with approval from the legislature. Governor Fred B. Balzar's signature enacted the most liberal divorce laws in the country and open gambling. The reforms came just eight days after the federal government presented the $49 million construction contract for Boulder Dam (now Hoover Dam).

The Nevada Test Site, northwest of the city of Las Vegas, was founded on January 11, 1951, for the testing of nuclear weapons. The site consists of about of the desert and mountainous terrain. Nuclear testing at the Nevada Test Site began with a bomb dropped on Frenchman Flat on January 27, 1951. The last atmospheric test was conducted on July 17, 1962, and the underground testing of weapons continued until September 23, 1992. The location is known for having the highest concentration of nuclear-detonated weapons in the U.S.

Over 80% of the state's area is owned by the federal government. The primary reason for this is homesteads were not permitted in large enough sizes to be viable in the arid conditions that prevail throughout desert Nevada. Instead, early settlers would homestead land surrounding a water source, and then graze livestock on the adjacent public land, which is useless for agriculture without access to water (this pattern of ranching still prevails).

The United States Census Bureau estimates the population of Nevada on July 1, 2019, was 3,080,156, an increase of 45,764 residents (1.51%) since the 2018 US Census estimate and an increase of 379,605 residents (14.06%) since the 2010 United States Census. Nevada had the highest percentage growth in population from 2017 to 2018. At the 2010 Census, 6.9% of the state's population were reported as under 5, 24.6% were under 18, and 12.0% were 65 or older. Females made up about 49.5% of the population.

Since the 2010 census, the population of Nevada had a natural increase of 87,581 (the net difference between 222,508 births and 134,927 deaths); and an increase due to net migration of 146,626 (of which 104,032 was due to domestic and 42,594 was due to international migration).

The center of population of Nevada is in southern Nye County. In this county, the unincorporated town of Pahrump, west of Las Vegas on the California state line, has grown very rapidly from 1980 to 2010. At the 2010 census, the town had 36,441 residents. Las Vegas grew from a gulch of 100 people in 1900 to 10,000 by 1950 to 100,000 by 1970, and was America's fastest-growing city and metropolitan area from 1960 to 2000.

From about the 1940s until 2003, Nevada was the fastest-growing state in the U.S. percentage-wise. Between 1990 and 2000, Nevada's population increased by 66%, while the nation's population increased by 13%. More than two-thirds of the population live in Clark County, which is coextensive with the Las Vegas metropolitan area. Thus, in terms of population, Nevada is one of the most centralized states in the nation.

Henderson and North Las Vegas are among the top 20 fastest-growing U.S. cities with populations over 100,000. The rural community of Mesquite northeast of Las Vegas was an example of micropolitan growth in the 1990s and 2000s. Other desert towns like Indian Springs and Searchlight on the outskirts of Las Vegas have seen some growth as well.

Since 1950, the rate of population born in Nevada has never peaked 27 percent, the lowest rate of all states. In 2012, only 25% of Nevadans were born in-state.

Large numbers of new residents in the state originate from California, which led some locals to feel their state is being "Californicated".

According to the 2017 American Community Survey, 28.2% of Nevada's population were of Hispanic or Latino origin (of any race): Mexican (21.4%), Puerto Rican (0.9%), Cuban (1.0%), and other Hispanic or Latino origin (4.8%). The five largest non-Hispanic White ancestry groups were: German (11.3%), Irish (9.0%), English (6.9%), Italian (5.8%), and American (4.7%).

In 1980, non-Hispanic whites made up 83.3% of the state's population.

As of 2011, 63.6% of Nevada's population younger than age1 were minorities. Las Vegas is a minority majority city. According to the United States Census Bureau estimates, as of July 1, 2018, non-Hispanic Whites made up 48.7% of Nevada's population.

In Douglas, Mineral, and Pershing counties, a plurality of residents are of Mexican ancestry. In Nye County and Humboldt County, residents are mostly of German ancestry; Washoe County has many Irish Americans. Americans of English descent form pluralities in Lincoln County, Churchill County, Lyon County, White Pine County, and Eureka County.

Asian Americans lived in the state since the California Gold Rush of the 1850s brought thousands of Chinese miners to Washoe county. They were followed by a few hundred Japanese farmworkers in the late 19th century. By the late 20th century, many immigrants from China, Japan, Korea, the Philippines, Bangladesh, India, and Vietnam came to the Las Vegas metropolitan area. The city now has one of America's most prolific Asian American communities, with a mostly Chinese and Taiwanese area known as "Chinatown" west of I-15 on Spring Mountain Road. Filipino Americans form the largest Asian American group in the state, with a population of more than 113,000. They comprise 56.5% of the Asian American population in Nevada and constitute about 4.3% of the entire state's population.

Mining booms drew many Greek and Eastern European immigrants to Nevada.

Native American tribes in Nevada are the Koso, Paiute, Panamint, Shoshoni, Walapi, Washoe and Ute tribes.

The top countries of origin for immigrants in Nevada were Mexico (39.5 percent of immigrants), the Philippines (14.3 percent), El Salvador (5.2 percent), China (3.1 percent), and Cuba (3 percent).

"Note: Births within the table do not add up, due to Hispanics being counted both by their ethnicity and by their race, giving a higher overall number."


A small percentage of Nevada's population lives in rural areas. The culture of these places differs significantly from major metropolitan areas. People in these rural counties tend to be native Nevada residents, unlike in the Las Vegas and Reno areas, where the vast majority of the population was born in another state. The rural population is also less diverse in terms of race and ethnicity. Mining plays an important role in the economies of the rural counties, with tourism being less prominent. Ranching also has a long tradition in rural Nevada.

Church attendance in Nevada is among the lowest of all U.S. states. In a 2009 Gallup poll only 30% of Nevadans said they attended church weekly or almost weekly, compared to 42% of all Americans (only four states were found to have a lower attendance rate than Nevada's).

Major religious affiliations of the people of Nevada are: Protestant 35%, no religion 28%, Roman Catholic 25%, Latter-day Saint 4%, Jewish 2%, Hindu less than 1%, Buddhist 0.5% and Islam less than 0.1%. Parts of Nevada (in the eastern parts of the state) are situated in the Mormon Corridor.

The largest denominations by number of adherents in 2010 were the Roman Catholic Church with 451,070; The Church of Jesus Christ of Latter-day Saints with 175,149; and the Southern Baptist Convention with 45,535; Buddhist congregations 14,727; Bahá'í 1,723; and Muslim 1,700. The Jewish community is represented by The Rohr Jewish Learning Institute and Chabad.

The economy of Nevada is tied to tourism (especially entertainment and gambling related), mining, and cattle ranching. Nevada's industrial outputs are tourism, entertainment, mining, machinery, printing and publishing, food processing, and electric equipment. The Bureau of Economic Analysis estimates Nevada's total state product in 2018 was $170 billion. The state's per capita personal income in 2018 was $43,820, ranking 35th in the nation. Nevada's state debt in 2012 was calculated to be $7.5 billion, or $3,100 per taxpayer. As of December 2014, the state's unemployment rate was 6.8%.

The economy of Nevada has long been tied to vice industries. "[Nevada was] founded on mining and refounded on sin—beginning with prizefighting and easy divorce a century ago and later extending to gaming and prostitution", said the August 21, 2010 issue of "The Economist".

In portions of the state outside of the Las Vegas and Reno metropolitan areas mining plays a major economic role. By value, gold is by far the most important mineral mined. In 2004, of gold worth $2.84 billion were mined in Nevada, and the state accounted for 8.7% of world gold production (see "Gold mining in Nevada"). Silver is a distant second, with worth $69 million mined in 2004 (see "Silver mining in Nevada"). Other minerals mined in Nevada include construction aggregates, copper, gypsum, diatomite and lithium. Despite its rich deposits, the cost of mining in Nevada is generally high, and output is very sensitive to world commodity prices.

Cattle ranching is a major economic activity in rural Nevada. Nevada's agricultural outputs are cattle, hay, alfalfa, dairy products, onions, and potatoes. As of January 1, 2006, there were an estimated 500,000 head of cattle and 70,000 head of sheep in Nevada. Most of these animals forage on rangeland in the summer, with supplemental feed in the winter. Calves are generally shipped to out-of-state feedlots in the fall to be fattened for the market. Over 90% of Nevada's of cropland is used to grow hay, mostly alfalfa, for livestock feed.

The largest employers in the state, as of the first fiscal quarter of 2011, are the following, according to the Nevada Department of Employment, Training and Rehabilitation:

Amtrak's "California Zephyr" train uses the Union Pacific's original transcontinental railroad line in daily service from Chicago to Emeryville, California, serving Elko, Winnemucca, and Reno. Las Vegas has had no passenger train service since Amtrak's Desert Wind was discontinued in 1997. Amtrak Thruway Motorcoaches provide connecting service from Las Vegas to trains at Needles, California, Los Angeles, and Bakersfield, California; and from Stateline, Nevada, to Sacramento, California. There have been a number of proposals to re-introduce service to either Los Angeles or Southern California.

The Union Pacific Railroad has some railroads in the north and south of Nevada. Greyhound Lines provide some bus service to the state.

Interstate 15 passes through the southern tip of the state, serving Las Vegas and other communities. I-215 and spur route I-515 also serve the Las Vegas metropolitan area. Interstate 80 crosses through the northern part of Nevada, roughly following the path of the Humboldt River from Utah in the east and the Truckee River westward through Reno into California. It has a spur route, I-580. Nevada also is served by several U.S. highways: US6, US 50, US 93, US 95 and US 395. There are also 189 Nevada state routes. Many of Nevada's counties have a system of county routes as well, though many are not signed or paved in rural areas. Nevada is one of a few states in the U.S. that does not have a continuous interstate highway linking its two major population centers—the road connection between the Las Vegas and Reno areas is a combination of several different Interstate and U.S. highways. The Interstate 11 proposed routing may eventually remedy this.

The state is one of just a few in the country to allow semi-trailer trucks with three trailers—what might be called a "road train" in Australia. But American versions are usually smaller, in part because they must ascend and descend some fairly steep mountain passes.

RTC Transit is the public transit system in the Las Vegas metropolitan area. The agency is the largest transit agency in the state and operates a network of bus service across the Las Vegas Valley, including the use of The Deuce, double-decker buses, on the Las Vegas Strip and several outlying routes. RTC RIDE operates a system of local transit bus service throughout the Reno-Sparks metropolitan area. Other transit systems in the state include Carson City's JAC. Most other counties in the state do not have public transportation at all.

Additionally, a monorail system provides public transportation in the Las Vegas area. The Las Vegas Monorail line services several casino properties and the Las Vegas Convention Center on the east side of the Las Vegas Strip, running near Paradise Road, with a possible future extension to McCarran International Airport. Several hotels also run their own monorail lines between each other, which are typically several blocks in length.

McCarran International Airport in Las Vegas is the busiest airport serving Nevada. The Reno-Tahoe International Airport (formerly known as the Reno Cannon International Airport) is the other major airport in the state.

Nevada has had a thriving solar energy sector. An independent study in 2013 concluded that solar users created a $36m net benefit. However, in December 2015, the Public Utility Commission let the state's only power company, NV Energy, charge higher rates and fees to solar panel users, leading to an immediate collapse of rooftop solar panel use 

In December 1987, Congress amended the Nuclear Waste Policy Act to designate Yucca Mountain nuclear waste repository as the only site to be characterized as a permanent repository for all of the nation's highly radioactive waste.

Under the Constitution of the State of Nevada, the powers of the Nevada government are divided among three separate departments: the Executive consisting of the Governor of Nevada and their cabinet along with the other elected constitutional officers; the Legislative consisting of the Nevada Legislature, which includes the Assembly and the Senate; and the Judicial consisting of the Supreme Court of Nevada and lower courts.

The Governor of Nevada is the chief magistrate of Nevada, the head of the executive department of the state's government, and the commander-in-chief of the state's military forces. The current Governor of Nevada is Steve Sisolak, a Democrat.

The Nevada Legislature is a bicameral body divided into an Assembly and Senate. Members of the Assembly serve two years, and members of the Senate serve four years. Both houses of the Nevada Legislature will be impacted by term limits starting in 2010, as Senators and Assemblymen/women will be limited to a maximum of twelve years in each house (by appointment or election which is a lifetime limit)—a provision of the constitution which was recently upheld by the Supreme Court of Nevada in a unanimous decision. Each session of the Legislature meets for a constitutionally mandated 120 days in every odd-numbered year, or longer if the Governor calls a special session.

On December 18, 2018, Nevada became the first in the United States with a female majority in its legislature. Women hold nine of the 21 seats in the Nevada Senate, and 23 of the 42 seats in the Nevada Assembly.

The Supreme Court of Nevada is the state supreme court and the head of the Nevada Judiciary. Original jurisdiction is divided between the district courts (with general jurisdiction), and justice courts and municipal courts (both of limited jurisdiction). Appeals from District Courts are made directly to the Nevada Supreme Court, which under a deflective model of jurisdiction, has the discretion to send cases to the Court of Appeals for final resolution.

Incorporated towns in Nevada, known as cities, are given the authority to legislate anything not prohibited by law. A recent movement has begun to permit home rule to incorporate Nevada cities to give them more flexibility and fewer restrictions from the Legislature. Town Boards for unincorporated towns are limited local governments created by either the local county commission, or by referendum, and form a purely advisory role and in no way diminish the responsibilities of the county commission that creates them.

In 1900, Nevada's population was the smallest of all states and was shrinking, as the difficulties of living in a "barren desert" began to outweigh the lure of silver for many early settlers. Historian Lawrence Friedman has explained what happened next:
Nevada, in a burst of ingenuity, built an economy by exploiting its sovereignty. Its strategy was to legalize all sorts of things that were illegal in California... after the easy divorce came easy marriage and casino gaming. Even prostitution is legal in Nevada, in any county that decides to allow it. Quite a few of them do.

With the advent of air conditioning for summertime use and Southern Nevada's mild winters, the fortunes of the state began to turn around, as it did for Arizona, making these two states the fastest growing in the Union.

Nevada is the only state where prostitution is legal—in a licensed brothel in a county which has specifically voted to permit it. It is illegal in larger jurisdictions such as Clark County (which contains Las Vegas), Washoe County (which contains Reno), and the independent city of Carson City.

Nevada's early reputation as a "divorce haven" arose from the fact that before the no-fault divorce revolution in the 1970s, divorces were difficult to obtain in the United States. Already having legalized gambling and prostitution, Nevada continued the trend of boosting its profile by adopting one of the most liberal divorce statutes in the nation. This resulted in "Williams v. North Carolina (1942)", , in which the U.S. Supreme Court ruled North Carolina had to give "full faith and credit" to a Nevada divorce. The Court modified its decision in "Williams v. North Carolina" (1945), , by holding a state need not recognize a Nevada divorce unless one of the parties was domiciled there at the time the divorce was granted and the forum state was entitled to make its own determination.

As of 2009, Nevada's divorce rate was above the national average.

Nevada's tax laws are intended to draw new residents and businesses to the state. Nevada has no personal income tax or corporate income tax. Since Nevada does not collect income data it cannot share such information with the federal government, the IRS.

The state sales tax (similar to VAT or GST) in Nevada is variable depending upon the county. The statewide tax rate is 6.85%, with five counties (Elko, Esmeralda, Eureka, Humboldt, and Mineral) charging this amount. Counties may impose additional rates via voter approval or through approval of the state legislature; therefore, the applicable sales tax varies by county from 6.85% to 8.375% (Clark County). Clark County, which includes Las Vegas, imposes four separate county option taxes in addition to the statewide rate: 0.25% for flood control, 0.50% for mass transit, 0.25% for infrastructure, and 0.25% for more cops. In Washoe County, which includes Reno, the sales tax rate is 7.725%, due to county option rates for flood control, the ReTRAC train trench project, and mass transit, and an additional county rate approved under the Local Government Tax Act of 1991. The minimum Nevada sales tax rate changed on July 1, 2009.

The lodging tax rate in unincorporated Clark County, which includes the Las Vegas Strip, is 12%. Within the boundaries of the cities of Las Vegas and Henderson, the lodging tax rate is 13%.

Corporations such as Apple Inc. allegedly have set up investment companies and funds in Nevada to avoid paying taxes.

In 2009, the Nevada Legislature passed a bill creating a domestic partnership registry that enables gay couples to enjoy the same rights as married couples. In June 2015, gay marriage became legal in Nevada.

Nevada provides a friendly environment for the formation of corporations, and many (especially California) businesses have incorporated in Nevada to take advantage of the benefits of the Nevada statute. Nevada corporations offer great flexibility to the Board of Directors and simplify or avoid many of the rules that are cumbersome to business managers in some other states. In addition, Nevada has no franchise tax, although it does require businesses to have a license for which the business has to pay the state.

Similarly, many U.S. states have usury laws limiting the amount of interest a lender can charge, but federal law allows corporations to 'import' these laws from their home state.

Nevada has very liberal alcohol laws. Bars are permitted to remain open 24 hours, with no "last call". Liquor stores, convenience stores and supermarkets may also sell alcohol 24  hours per day and may sell beer, wine and spirits.

In 2016, Nevada voters approved Question2, which legalized the possession, transportation and cultivation of personal use amounts of marijuana for adults age 21 years and older, and authorized the creation of a regulated market for the sale of marijuana to adults age 21 years and older through state-licensed retail outlets. Nevada voters had previously approved medical marijuana in 2000, but rejected marijuana legalization in a similar referendum in 2006. Marijuana in all forms remains illegal under federal law.

Aside from cannabis legalization, non-alcohol drug laws are a notable exception to Nevada's otherwise libertarian principles. It is notable for having the harshest penalties for drug offenders in the country. Nevada remains the only state to still use mandatory minimum sentencing guidelines for possession of drugs.

Nevada voters enacted a smoking ban ("The Nevada Clean Indoor Air Act") in November 2006 that became effective on December 8, 2006. It outlaws smoking in most workplaces and public places. Smoking is permitted in bars, but only if the bar serves no food, or the bar is inside a larger casino. Smoking is also permitted in casinos, certain hotel rooms, tobacco shops, and brothels. However, some businesses do not obey this law and the government tends not to enforce it. In 2011, smoking restrictions in Nevada were loosened for certain places which allow only people 21 or older inside.

In 2006, the crime rate in Nevada was about 24% higher than the national average rate, though crime has since decreased. Property crimes accounted for about 85% of the total crime rate in Nevada, which was 21% higher than the national rate. The remaining 20.3% were violent crimes. A complete listing of crime data in the state for 2013 can be found here:

Due to heavy growth in the southern portion of the state, there is a noticeable divide between the politics of northern and southern Nevada. The north has long maintained control of key positions in state government, even while the population of southern Nevada is larger than the rest of the state combined. The north sees the high population south becoming more influential and perhaps commanding majority rule. The south sees the north as the "old guard" trying to rule as an oligarchy. This has fostered some resentment, however, due to a term limit amendment passed by Nevada voters in 1994, and again in 1996, some of the north's hold over key positions will soon be forfeited to the south, leaving northern Nevada with less power.

Historically, northern Nevada has been very Republican. The more rural counties of the north are among the most conservative regions of the country. Carson City, the state's capital, is a Republican-leaning swing city/county. Washoe County, home to Reno, has historically been strongly Republican, but now has become more of a Democratic-leaning swing county. Clark County, home to Las Vegas, has been a stronghold for the Democratic Party since it was founded in 1909, having voted Republican only six times and once for a third-party candidate. Clark and Washoe counties have long dominated the state's politics. Between them, they cast 87 percent of Nevada's vote, and elect a substantial majority of the state legislature. The last Republican to carry Clark County was George H.W. Bush in 1988, and the last Republican to carry Washoe County was George W. Bush in 2004. The great majority of the state's elected officials are from either Las Vegas or Reno.

Nevada voted for the winner in nearly every presidential election from 1912 to 2012, with the exception in 1976 when it voted for Gerald Ford over Jimmy Carter. This includes Nevada supporting Democrats John F. Kennedy and Lyndon B. Johnson in 1960 and 1964, respectively. Republican Richard Nixon in 1968 and in 1972, Republican Ronald Reagan in 1980 and in 1984, Republican George H.W. Bush in 1988, Democrat Bill Clinton in 1992 and 1996, Republican George W. Bush in 2000 and 2004, and Democrat Barack Obama winning the state in both 2008 and 2012. This gives the state status as a political bellwether. From 1912 to 2012, Nevada has been carried by the presidential victor the most out of any state (26 of 27 elections). In 2016, Nevada lost its bellwether status when it narrowly cast its votes for Hillary Clinton. Nevada was one of only three states won by John F. Kennedy in the American West in the election of 1960, albeit narrowly. 

Hillary Clinton narrowly defeated Trump in Nevada in 2016, winning 47.92% of votes to Trump's 45.5%. 

The state's U.S. Senators are Democrats Catherine Cortez Masto and Jacky Rosen. The Governorship is held by Steve Sisolak, a Democrat.

Nevada is the only U.S. state to have a none of the above option available on its ballots. Officially called None of These Candidates, the option was first added to the ballot in 1975 and is used in all statewide elections, including president, US Senate and all state constitutional positions. In the event "None of These Candidates" receives a plurality of votes in the election, the candidate with the next-highest total is elected.

Education in Nevada is achieved through public and private elementary, middle, and high schools, as well as colleges and universities.

A May 2015 educational reform law expanded school choice options to 450,000 Nevada students who are at up to 185% of the federal poverty level. Education savings accounts (ESAs) are enabled by the new law to help pay the tuition for private schools. Alternatively, families "can use funds in these accounts to also pay for textbooks and tutoring".

Approximately 86.9% of Nevada high school students graduate, putting it below the national average of 88.3%.

Public school districts in Nevada include:


The Nevada Aerospace Hall of Fame provides educational resources and promotes the aerospace and aviation history of the state.



There are 68 designated wilderness areas in Nevada, protecting some under the jurisdiction of the National Park Service, U.S. Forest Service, and Bureau of Land Management.

The Nevada state parks comprise protected areas managed by the state of Nevada, including state parks, state historic sites, and state recreation areas. There are 24 state park units, including Van Sickle Bi-State Park which opened in July 2011 and is operated in partnership with the state of California.

Resort areas like Las Vegas, Reno, Lake Tahoe, and Laughlin attract visitors from around the nation and world. In FY08 their 266 casinos (not counting ones with annual revenue under a million dollars) brought in $12 "billion" in gaming revenue and another $13 billion in non-gaming revenue. A review of gaming statistics can be found at Nevada gaming area.

Nevada has by far the most hotel rooms per capita in the United States. According to the American Hotel and Lodging Association, there were 187,301 rooms in 584 hotels (of 15 or more rooms). The state is ranked just below California, Texas, Florida, and New York in the total number of rooms, but those states have much larger populations. Nevada has one hotel room for every 14 residents, far above the national average of one hotel room per 67 residents.

Prostitution is legal in parts of Nevada in licensed brothels, but only counties with populations under 400,000 have the option to legalize it. Although prostitution is not a major part of the Nevada economy, employing roughly 300 women as independent contractors, it is a very visible endeavor. Of the 14 counties permitted to legalize prostitution under state law, eight have chosen to legalize brothels. State law prohibits prostitution in Clark County (which contains Las Vegas), and Washoe County (which contains Reno). However, prostitution is legal in Storey County, which is part of the Reno–Sparks metropolitan area.

The Las Vegas Valley is home to the Vegas Golden Knights of the National Hockey League who began to play in the 2017–18 NHL season at T-Mobile Arena on the Las Vegas Strip in Paradise, Nevada, the Las Vegas Raiders of the National Football League who began play at Allegiant Stadium in Las Vegas in 2020 after moving from Oakland, California, and the Las Vegas Aces of the WNBA who began playing in 2018 at Mandalay Bay Events Center. The team moved from San Antonio. 

Nevada takes pride in college sports, most notably its college football. College teams in the state include the Nevada Wolf Pack (representing the University of Nevada, Reno) and the UNLV Rebels (representing the University of Nevada, Las Vegas), both in the Mountain West Conference (MW).

UNLV is most remembered for its men's basketball program, which experienced its height of supremacy in the late 1980s and early 1990s. Coached by Jerry Tarkanian, the Runnin' Rebels became one of the most elite programs in the country. In 1990, UNLV won the Men's Division I Championship by defeating Duke 103–73, which set tournament records for most points scored by a team and largest margin of victory in the national title game.

In 1991, UNLV finished the regular season undefeated, a feat that would not be matched in Division I men's basketball for more than 20 years. Forward Larry Johnson won several awards, including the Naismith Award. UNLV reached the Final Four yet again, but lost their national semifinal against Duke 79–77. The Runnin' Rebels were the Associated Press pre-season No.1 back to back (1989–90, 1990–91). North Carolina is the only other team to accomplish that (2007–08, 2008–09).

The state's involvement in major-college sports is not limited to its local schools. In the 21st century, the Las Vegas area has become a significant regional center for college basketball conference tournaments. The MW, West Coast Conference, and Western Athletic Conference all hold their men's and women's tournaments in the area, and the Pac-12 holds its men's tournament there as well. The Big Sky Conference, after decades of holding its men's and women's conference tournaments at campus sites, began holding both tournaments in Reno in 2016.

Las Vegas has hosted several professional boxing matches, most recently at the MGM Grand Garden Arena with bouts such as Mike Tyson vs. Evander Holyfield, Evander Holyfield vs. Mike Tyson II, Oscar De La Hoya vs. Floyd Mayweather and Oscar De La Hoya vs. Manny Pacquiao and at the newer T-Mobile Arena with Canelo Álvarez vs. Amir Khan.

Along with significant rises in popularity in mixed martial arts (MMA), a number of fight leagues such as the UFC have taken interest in Las Vegas as a primary event location due to the number of suitable host venues. The Mandalay Bay Events Center and MGM Grand Garden Arena are among some of the more popular venues for fighting events such as MMA and have hosted several UFC and other MMA title fights. The city has held the most UFC events with 86 events.

The state is also home to the Las Vegas Motor Speedway, which hosts NASCAR's Pennzoil 400 and South Point 400. Two venues in the immediate Las Vegas area host major annual events in rodeo. The Thomas & Mack Center, built for UNLV men's basketball, hosts the National Finals Rodeo. The PBR World Finals, operated by the bull riding-only Professional Bull Riders, was also held at the Thomas & Mack Center before moving to T-Mobile Arena in 2016.

The state is also home to one of the most famous tennis players of all time, Andre Agassi, and current baseball superstar Bryce Harper.

Several United States Navy ships have been named USS "Nevada" in honor of the state. They include:

Area 51 is near Groom Lake, a dry salt lake bed. The much smaller Creech Air Force Base is in Indian Springs, Nevada; Hawthorne Army Depot in Hawthorne; the Tonopah Test Range near Tonopah; and Nellis AFB in the northeast part of the Las Vegas Valley. Naval Air Station Fallon in Fallon; NSAWC, (pronounced "EN-SOCK") in western Nevada. NSAWC consolidated three Command Centers into a single Command Structure under a flag officer on July 11, 1996. The Naval Strike Warfare Center (STRIKE "U") based at NAS Fallon since 1984, was joined with the Navy Fighter Weapons School (TOPGUN) and the Carrier Airborne Early Warning Weapons School (TOPDOME) which both moved from NAS Miramar as a result of a Base Realignment and Closure (BRAC) decision in 1993 which transferred that installation back to the Marine Corps as MCAS Miramar. The Seahawk Weapon School was added in 1998 to provide tactical training for Navy helicopters.

These bases host a number of activities including the Joint Unmanned Aerial Systems Center of Excellence, the Naval Strike and Air Warfare Center, Nevada Test and Training Range, Red Flag, the U.S. Air Force Thunderbirds, the United States Air Force Warfare Center, the United States Air Force Weapons School, and the United States Navy Fighter Weapons School.





</doc>
<doc id="21217" url="https://en.wikipedia.org/wiki?curid=21217" title="Native Americans in the United States">
Native Americans in the United States

Native Americans, also known as American Indians, Indigenous Americans and other terms, are the indigenous peoples of the United States, except Hawaii and territories of the United States. There are 574 federally recognized tribes living within the US, about half of which are associated with Indian reservations. The term "American Indian" excludes Native Hawaiians and some Alaskan Natives, while "Native Americans" (as defined by the United States Census) are American Indians, plus Alaska Natives of all ethnicities. The US Census does not include Native Hawaiians, Samoans, or Chamorros, instead being included in the Census grouping of "Native Hawaiian and other Pacific Islander".

The ancestors of living Native Americans arrived in what is now the United States at least 15,000 years ago, possibly much earlier, from Asia via Beringia. A vast variety of peoples, societies and cultures subsequently developed. European colonization of the Americas, which began in 1492, resulted in a precipitous decline in Native American population through introduced diseases, warfare, ethnic cleansing, and slavery. After its formation, the United States, as part of its policy of settler colonialism, continued to wage war and perpetrated massacres against many Native American peoples, removed them from their ancestral lands, and subjected them to one-sided treaties and to discriminatory government policies, later focused on forced assimilation, into the 20th century. Since the 1960s, Native American self-determination movements have resulted in changes to the lives of Native Americans, though there are still many contemporary issues faced by Native Americans. Today, there are over five million Native Americans in the United States, 78% of whom live outside reservations.

When the United States was created, established Native American tribes were generally considered semi-independent nations, as they generally lived in communities separate from white settlers. The federal government signed treaties at a government-to-government level until the Indian Appropriations Act of 1871 ended recognition of independent native nations, and started treating them as "domestic dependent nations" subject to federal law. This law did preserve the rights and privileges agreed to under the treaties, including a large degree of tribal sovereignty. For this reason, many (but not all) Native American reservations are still independent of state law and actions of tribal citizens on these reservations are subject only to tribal courts and federal law.

The Indian Citizenship Act of 1924 granted U.S. citizenship to all Native Americans born in the United States who had not yet obtained it. This emptied the "Indians not taxed" category established by the United States Constitution, allowed natives to vote in state and federal elections, and extended the Fourteenth Amendment protections granted to people "subject to the jurisdiction" of the United States. However, some states continued to deny Native Americans voting rights for several decades. Bill of Rights protections do not apply to tribal governments, except for those mandated by the Indian Civil Rights Act of 1968.

Since the end of the 15th century, the migration of Europeans to the Americas has led to centuries of population, cultural, and agricultural transfer and adjustment between Old and New World societies, a process known as the Columbian exchange. As most Native American groups had historically preserved their histories by oral traditions and artwork, the first written sources of the contact were written by Europeans.
Ethnographers commonly classify the indigenous peoples of North America into ten geographical regions with shared cultural traits, called cultural areas. Some scholars combine the Plateau and Great Basin regions into the Intermontane West, some separate Prairie peoples from Great Plains peoples, while some separate Great Lakes tribes from the Northeastern Woodlands. The ten cultural areas are as follows:


At the time of the first contact, the indigenous cultures were quite different from those of the proto-industrial and mostly Christian immigrants. Some Northeastern and Southwestern cultures, in particular, were matrilineal and operated on a more collective basis than that with which Europeans were familiar. The majority of indigenous American tribes maintained their hunting grounds and agricultural lands for use of the entire tribe. Europeans at that time had cultures that had developed concepts of individual property rights with respect to land that were extremely different. The differences in cultures between the established Native Americans and immigrant Europeans, as well as shifting alliances among different nations in times of war, caused extensive political tension, ethnic violence, and social disruption.

Even before the European settlement of what is now the United States, Native Americans suffered high fatalities from contact with new European diseases, to which they had not yet acquired immunity; the diseases were endemic to the Spanish and other Europeans, and spread by direct contact and likely through pigs that escaped from expeditions. Smallpox epidemics are thought to have caused the greatest loss of life for indigenous populations. William M. Denevan, noted author and Professor Emeritus of Geography at the University of Wisconsin-Madison, said on this subject in his essay "The Pristine Myth: The Landscape of the Americas in 1492"; "The decline of native American populations was rapid and severe, probably the greatest demographic disaster ever. Old World diseases were the primary killer. In many regions, particularly the tropical lowlands, populations fell by 90 percent or more in the first century after the contact. "

Estimates of the pre-Columbian population of what today constitutes the U.S. vary significantly, ranging from William M. Denevan's 3.8 million in his 1992 work "The Native Population of the Americas in 1492", to 18 million in Henry F. Dobyns' "Their Number Become Thinned" (1983). Henry F. Dobyns' work, being the highest single point estimate by far within the realm of professional academic research on the topic, has been criticized for being "politically motivated". Perhaps Dobyns' most vehement critic is David Henige, a bibliographer of Africana at the University of Wisconsin, whose "Numbers From Nowhere" (1998) is described as "a landmark in the literature of demographic fulmination". "Suspect in 1966, it is no less suspect nowadays," Henige wrote of Dobyns's work. "If anything, it is worse."

After the thirteen colonies revolted against Great Britain and established the United States, President George Washington and Secretary of War Henry Knox conceived of the idea of "civilizing" Native Americans in preparation for assimilation as U.S. citizens. Assimilation (whether voluntary, as with the Choctaw, or forced) became a consistent policy through American administrations. During the 19th century, the ideology of manifest destiny became integral to the American nationalist movement. Expansion of European-American populations to the west after the American Revolution resulted in increasing pressure on Native American lands, warfare between the groups, and rising tensions. In 1830, the U.S. Congress passed the Indian Removal Act, authorizing the government to relocate Native Americans from their homelands within established states to lands west of the Mississippi River, accommodating European-American expansion. This resulted in the ethnic cleansing of many tribes, with the brutal, forced marches coming to be known as The Trail of Tears.

Contemporary Native Americans have a unique relationship with the United States because they may be members of nations, tribes, or bands with sovereignty and treaty rights. Cultural activism since the late 1960s has increased political participation and led to an expansion of efforts to teach and preserve indigenous languages for younger generations and to establish a greater cultural infrastructure: Native Americans have founded independent newspapers and online media, recently including First Nations Experience, the first Native American television channel; established Native American studies programs, tribal schools, and universities, and museums and language programs; and have increasingly been published as authors in numerous genres.

The terms used to refer to Native Americans have at times been controversial. The ways Native Americans refer to themselves vary by region and generation, with many older Native Americans self-identifying as "Indians" or "American Indians", while younger Native Americans often identify as "Indigenous" or "Aboriginal". The term "Native American" has not traditionally included Native Hawaiians or certain Alaskan Natives, such as Aleut, Yup'ik, or Inuit peoples. By comparison, the indigenous peoples of Canada are generally known as First Nations.

It is not definitively known how or when the Native Americans first settled the Americas and the present-day United States. The prevailing theory proposes that people migrated from Eurasia across Beringia, a land bridge that connected Siberia to present-day Alaska during the Ice Age, and then spread southward throughout the Americas over the subsequent generations. Genetic evidence suggests at least three waves of migrants arrived from Asia, with the first occurring at least fifteen thousand years ago. These migrations may have begun as early as 30,000 years ago and continued through to about 10,000 years ago, when the land bridge became submerged by the rising sea level caused by the ending of the last glacial period.

The pre-Columbian era incorporates all period subdivisions in the history and prehistory of the Americas before the appearance of significant European influences on the American continents, spanning the time of the original settlement in the Upper Paleolithic period to European colonization during the Early Modern period. While technically referring to the era before Christopher Columbus' voyages of 1492 to 1504, in practice the term usually includes the history of American indigenous cultures until they were conquered or significantly influenced by Europeans, even if this happened decades or even centuries after Columbus' initial landing.

Native American cultures are not normally included in characterizations of advanced stone age cultures as "Neolithic," which is a category that more often includes only the cultures in Eurasia, Africa, and other regions. The archaeological periods used are the classifications of archaeological periods and cultures established in Gordon Willey and Philip Phillips' 1958 book "Method and Theory in American Archaeology". They divided the archaeological record in the Americas into five phases; see Archaeology of the Americas.

Numerous Paleoindian cultures occupied North America, with some arrayed around the Great Plains and Great Lakes of the modern United States and Canada, as well as adjacent areas to the West and Southwest. According to the oral histories of many of the indigenous peoples of the Americas, they have been living on this continent since their genesis, described by a wide range of traditional creation stories. Other tribes have stories that recount migrations across long tracts of land and a great river, believed to be the Mississippi River. Genetic and linguistic data connect the indigenous people of this continent with ancient northeast Asians. Archeological and linguistic data has enabled scholars to discover some of the migrations within the Americas.

Archeological evidence at the Gault site near Austin, Texas, demonstrates that pre-Clovis peoples settled in Texas some 16,000—20,000 years ago. Evidence of pre-Clovis cultures have also been found in the Paisley Caves in south-central Oregon and butchered mastodon bones in a sinkhole near Tallahassee, Florida. More convincingly but also controversially, another pre-Clovis has been discovered at Monte Verde, Chile.

The Clovis culture, a megafauna hunting culture, is primarily identified by the use of fluted spear points. Artifacts from this culture were first excavated in 1932 near Clovis, New Mexico. The Clovis culture ranged over much of North America and also appeared in South America. The culture is identified by the distinctive Clovis point, a flaked flint spear-point with a notched flute, by which it was inserted into a shaft. Dating of Clovis materials has been by association with animal bones and by the use of carbon dating methods. Recent reexaminations of Clovis materials using improved carbon-dating methods produced results of 11,050 and 10,800 radiocarbon years B.P. (roughly 9100 to 8850 BCE).
The Folsom Tradition was characterized by the use of Folsom points as projectile tips and activities known from kill sites, where slaughter and butchering of bison took place. Folsom tools were left behind between 9000 BCE and 8000 BCE.

Na-Dené-speaking peoples entered North America starting around 8000 BCE, reaching the Pacific Northwest by 5000 BCE, and from there migrating along the Pacific Coast and into the interior. Linguists, anthropologists, and archaeologists believe their ancestors comprised a separate migration into North America, later than the first Paleo-Indians. They migrated into Alaska and northern Canada, south along the Pacific Coast, into the interior of Canada, and south to the Great Plains and the American Southwest. Na-Dené-speaking peoples were the earliest ancestors of the Athabascan-speaking peoples, including the present-day and historical Navajo and Apache. They constructed large multi-family dwellings in their villages, which were used seasonally. People did not live there year-round, but for the summer to hunt and fish, and to gather food supplies for the winter.

Since the 1990s, archeologists have explored and dated eleven Middle Archaic sites in present-day Louisiana and Florida at which early cultures built complexes with multiple earthwork mounds; they were societies of hunter-gatherers rather than the settled agriculturalists believed necessary according to the theory of Neolithic Revolution to sustain such large villages over long periods. The prime example is Watson Brake in northern Louisiana, whose 11-mound complex is dated to 3500 BCE, making it the oldest, dated site in North America for such complex construction. It is nearly 2,000 years older than the Poverty Point site. Construction of the mounds went on for 500 years until the site was abandoned about 2800 BCE, probably due to changing environmental conditions.

The Oshara Tradition people lived from 700–1000 CE. They were part of the Southwestern Archaic Tradition centered in north-central New Mexico, the San Juan Basin, the Rio Grande Valley, southern Colorado, and southeastern Utah.

Poverty Point culture is a Late Archaic archaeological culture that inhabited the area of the lower Mississippi Valley and surrounding Gulf Coast. The culture thrived from 2200 BCE to 700 BCE, during the Late Archaic period. Evidence of this culture has been found at more than 100 sites, from the major complex at Poverty Point, Louisiana (a UNESCO World Heritage Site) across a range to the Jaketown Site near Belzoni, Mississippi.

The Formative, Classic and post-Classic stages are sometimes incorporated together as the Post-archaic period, which runs from 1000 BCE onward. Sites & cultures include: Adena, Old Copper, Oasisamerica, Woodland, Fort Ancient, Hopewell tradition and Mississippian cultures.

The Woodland period of North American pre-Columbian cultures refers to the time period from roughly 1000 BCE to 1000 CE in the eastern part of North America. The Eastern Woodlands cultural region covers what is now eastern Canada south of the Subarctic region, the Eastern United States, along to the Gulf of Mexico. The Hopewell tradition describes the common aspects of the culture that flourished along rivers in the northeastern and midwestern United States from 100 BCE to 500 CE, in the Middle Woodland period. The Hopewell tradition was not a single culture or society, but a widely dispersed set of related populations. They were connected by a common network of trade routes, This period is considered a developmental stage without any massive changes in a short period, but instead having a continuous development in stone and bone tools, leather working, textile manufacture, tool production, cultivation, and shelter construction.

The indigenous peoples of the Pacific Northwest Coast were of many nations and tribal affiliations, each with distinctive cultural and political identities, but they shared certain beliefs, traditions, and practices, such as the centrality of salmon as a resource and spiritual symbol. Their gift-giving feast, potlatch, is a highly complex event where people gather in order to commemorate special events. These events include the raising of a Totem pole or the appointment or election of a new chief. The most famous artistic feature of the culture is the Totem pole, with carvings of animals and other characters to commemorate cultural beliefs, legends, and notable events.

The Mississippian culture was a mound-building Native American civilization archaeologists date from approximately 800 CE to 1600 CE, varying regionally. It was composed of a series of urban settlements and satellite villages (suburbs) linked together by a loose trading network, the largest city being Cahokia, believed to be a major religious center. The civilization flourished in what is now the Midwestern, Eastern, and Southeastern United States.

Numerous pre-Columbian societies were sedentary, such as the Pueblo peoples, Mandan, Hidatsa and others, and some established large settlements, even cities, such as Cahokia, in what is now Illinois. The Iroquois League of Nations or "People of the Long House" was a politically advanced, democratic society, which is thought by some historians to have influenced the United States Constitution, with the Senate passing a resolution to this effect in 1988. Other historians have contested this interpretation and believe the impact was minimal, or did not exist, pointing to numerous differences between the two systems and the ample precedents for the constitution in European political thought.

After 1492, European exploration and colonization of the Americas revolutionized how the Old and New Worlds perceived themselves. Many of the first major contacts were in Florida and the Gulf coast by Spanish explorers.

From the 16th through the 19th centuries, the population of Native Americans sharply declined. Most mainstream scholars believe that, among the various contributing factors, epidemic disease was the overwhelming cause of the population decline of the Native Americans because of their lack of immunity to new diseases brought from Europe. It is difficult to estimate the number of pre-Columbian Native Americans who were living in what is today the United States of America. Estimates range from a low of 2.1 million to a high of 18 million (Dobyns 1983). By 1800, the Native population of the present-day United States had declined to approximately 600,000, and only 250,000 Native Americans remained in the 1890s. Chicken pox and measles, endemic but rarely fatal among Europeans (long after being introduced from Asia), often proved deadly to Native Americans. In the 100 years following the arrival of the Spanish to the Americas, large disease epidemics depopulated large parts of the eastern United States in the 16th century.

There are a number of documented cases where diseases were deliberately spread among Native Americans as a form of biological warfare. The most well-known example occurred in 1763, when Sir Jeffery Amherst, Commander-in-Chief of the Forces of the British Army, wrote praising the use of smallpox-infected blankets to "extirpate" the Indian race. Blankets infected with smallpox were given to Native Americans besieging Fort Pitt. The effectiveness of the attempt is unclear.

In 1634, Fr. Andrew White of the Society of Jesus established a mission in what is now the state of Maryland, and the purpose of the mission, stated through an interpreter to the chief of an Indian tribe there, was "to extend civilization and instruction to his ignorant race, and show them the way to heaven". Fr. Andrew's diaries report that by 1640, a community had been founded which they named St. Mary's, and the Indians were sending their children there "to be educated among the English". This included the daughter of the Piscataway Indian chief Tayac, which exemplifies not only a school for Indians, but either a school for girls, or an early co-ed school. The same records report that in 1677, "a school for humanities was opened by our Society in the centre of [Maryland], directed by two of the Fathers; and the native youth, applying themselves assiduously to study, made good progress. Maryland and the recently established school sent two boys to St. Omer who yielded in abilities to few Europeans, when competing for the honor of being first in their class. So that not gold, nor silver, nor the other products of the earth alone, but men also are gathered from thence to bring those regions, which foreigners have unjustly called ferocious, to a higher state of virtue and cultivation."

Through the mid-17th century the Beaver Wars were fought over the fur trade between the Iroquois and the Hurons, the northern Algonquians, and their French allies. During the war the Iroquois destroyed several large tribal confederacies, including the Huron, Neutral, Erie, Susquehannock, and Shawnee, and became dominant in the region and enlarged their territory.

In 1727, the Sisters of the Order of Saint Ursula founded Ursuline Academy in New Orleans, which is currently the oldest continuously operating school for girls and the oldest Catholic school in the United States. From the time of its foundation, it offered the first classes for Native American girls, and would later offer classes for female African-American slaves and free women of color.
Between 1754 and 1763, many Native American tribes were involved in the French and Indian War/Seven Years' War. Those involved in the fur trade tended to ally with French forces against British colonial militias. The British had made fewer allies, but it was joined by some tribes that wanted to prove assimilation and loyalty in support of treaties to preserve their territories. They were often disappointed when such treaties were later overturned. The tribes had their own purposes, using their alliances with the European powers to battle traditional Native enemies. Some Iroquois who were loyal to the British, and helped them fight in the American Revolution, fled north into Canada.

After European explorers reached the West Coast in the 1770s, smallpox rapidly killed at least 30% of Northwest Coast Native Americans. For the next eighty to one hundred years, smallpox and other diseases devastated native populations in the region. Puget Sound area populations, once estimated as high as 37,000 people, were reduced to only 9,000 survivors by the time settlers arrived en masse in the mid-19th century.

Smallpox epidemics in 1780–82 and 1837–38 brought devastation and drastic depopulation among the Plains Indians. By 1832, the federal government established a smallpox vaccination program for Native Americans ("The Indian Vaccination Act of 1832"). It was the first federal program created to address a health problem of Native Americans.

With the meeting of two worlds, animals, insects, and plants were carried from one to the other, both deliberately and by chance, in what is called the Columbian Exchange. In the 16th century, Spaniards and other Europeans brought horses to Mexico. Some of the horses escaped and began to breed and increase their numbers in the wild. As Native Americans adopted use of the animals, they began to change their cultures in substantial ways, especially by extending their nomadic ranges for hunting. The reintroduction of the horse to North America had a profound impact on Native American culture of the Great Plains.

King Philip's War, also called Metacom's War or Metacom's Rebellion, was the last major armed conflict between Native American inhabitants of present-day southern New England and English colonists and their Native American allies from 1675 to 1676. It continued in northern New England (primarily on the Maine frontier) even after King Philip was killed, until a treaty was signed at Casco Bay in April 1678.

Some European philosophers considered Native American societies to be truly "natural" and representative of a golden age known to them only in folk history.

During the American Revolution, the newly proclaimed United States competed with the British for the allegiance of Native American nations east of the Mississippi River. Most Native Americans who joined the struggle sided with the British, based both on their trading relationships and hopes that colonial defeat would result in a halt to further colonial expansion onto Native American land. The first native community to sign a treaty with the new United States Government was the Lenape.

In 1779 the Sullivan Expedition was carried out during the American Revolutionary War against the British and the four allied nations of the Iroquois. George Washington gave orders that made it clear he wanted the Iroquois threat completely eliminated:

The British made peace with the Americans in the Treaty of Paris (1783), through which they ceded vast Native American territories to the United States without informing or consulting with the Native Americans.

The United States was eager to expand, develop farming and settlements in new areas, and satisfy land hunger of settlers from New England and new immigrants. The national government initially sought to purchase Native American land by treaties. The states and settlers were frequently at odds with this policy.

United States policy toward Native Americans continued to evolve after the American Revolution. George Washington and Henry Knox believed that Native Americans were equals but that their society was inferior. Washington formulated a policy to encourage the "civilizing" process. Washington had a six-point plan for civilization which included:

In the late 18th century, reformers starting with Washington and Knox, supported educating native children and adults, in efforts to "civilize" or otherwise assimilate Native Americans to the larger society (as opposed to relegating them to reservations). The Civilization Fund Act of 1819 promoted this civilization policy by providing funding to societies (mostly religious) who worked on Native American improvement.

The population of California Indians was reduced by 90% during the 19th century—from more than 200,000 in the early 19th century to approximately 15,000 at the end of the century, mostly due to disease. Epidemics swept through California Indian Country, such as the 1833 malaria epidemic. The population went into decline as a result of the Spanish authorities forcing Native Californians to live in the missions where they contracted diseases from which they had little immunity. Dr. Cook estimates that 15,250 or 45% of the population decrease in the Missions was caused by disease. Two epidemics of measles, one in 1806 and the other in 1828, caused many deaths. The mortality rates were so high that the missions were constantly dependent upon new conversions. During the California Gold Rush, many natives were killed by incoming settlers as well as by militia units financed and organized by the California government. Some scholars contend that the state financing of these militias, as well as the US government's role in other massacres in California, such as the Bloody Island and Yontoket Massacres, in which up to 400 or more natives were killed in each massacre, constitutes a campaign of genocide against the native people of California.

As American expansion continued, Native Americans resisted settlers' encroachment in several regions of the new nation (and in unorganized territories), from the Northwest to the Southeast, and then in the West, as settlers encountered the Native American tribes of the Great Plains. East of the Mississippi River, an intertribal army led by Tecumseh, a Shawnee chief, fought a number of engagements in the Northwest during the period 1811–12, known as Tecumseh's War. During the War of 1812, Tecumseh's forces allied themselves with the British. After Tecumseh's death, the British ceased to aid the Native Americans south and west of Upper Canada and American expansion proceeded with little resistance. Conflicts in the Southeast include the Creek War and Seminole Wars, both before and after the Indian Removals of most members of the Five Civilized Tribes.

In the 1830s, President Andrew Jackson signed the Indian Removal Act of 1830, a policy of relocating Indians from their homelands to Indian Territory and reservations in surrounding areas to open their lands for non-native settlements. This resulted in the Trail of Tears.
In July 1845, the New York newspaper editor John L. O'Sullivan coined the phrase, "Manifest Destiny", as the "design of Providence" supporting the territorial expansion of the United States. Manifest Destiny had serious consequences for Native Americans, since continental expansion for the U.S. took place at the cost of their occupied land. A justification for the policy of conquest and subjugation of the indigenous people emanated from the stereotyped perceptions of all Native Americans as "merciless Indian savages" (as described in the United States Declaration of Independence). Sam Wolfson in "The Guardian" writes, "The declaration's passage has often been cited as an encapsulation of the dehumanizing attitude toward indigenous Americans that the US was founded on."

The Indian Appropriations Act of 1851 set the precedent for modern-day Native American reservations through allocating funds to move western tribes onto reservations since there were no more lands available for relocation.

Native American nations on the plains in the west continued armed conflicts with the U.S. throughout the 19th century, through what were called generally Indian Wars. Notable conflicts in this period include the Dakota War, Great Sioux War, Snake War, Colorado War, and Texas-Indian Wars. Expressing the frontier anti-Indian sentiment, Theodore Roosevelt believed the Indians were destined to vanish under the pressure of white civilization, stating in an 1886 lecture:

One of the last and most notable events during the Indian wars was the Wounded Knee Massacre in 1890. In the years leading up to it the U.S. government had continued to seize Lakota lands. A Ghost Dance ritual on the Northern Lakota reservation at Wounded Knee, South Dakota, led to the U.S. Army's attempt to subdue the Lakota. The dance was part of a religious movement founded by the Northern Paiute spiritual leader Wovoka that told of the return of the Messiah to relieve the suffering of Native Americans and promised that if they would live righteous lives and perform the Ghost Dance properly, the European American colonists would vanish, the bison would return, and the living and the dead would be reunited in an Edenic world. On December 29 at Wounded Knee, gunfire erupted, and U.S. soldiers killed up to 300 Indians, mostly old men, women, and children.

Native Americans served in both the Union and Confederate military during the American Civil War. At the outbreak of the war, for example, the minority party of the Cherokees gave its allegiance to the Confederacy, while originally the majority party went for the North. Native Americans fought knowing they might jeopardize their independence, unique cultures, and ancestral lands if they ended up on the losing side of the Civil War. 28,693 Native Americans served in the Union and Confederate armies during the Civil War, participating in battles such as Pea Ridge, Second Manassas, Antietam, Spotsylvania, Cold Harbor, and in Federal assaults on Petersburg. A few Native American tribes, such as the Creek and the Choctaw, were slaveholders and found a political and economic commonality with the Confederacy. The Choctaw owned over 2,000 slaves.

In the 19th century, the incessant westward expansion of the United States incrementally compelled large numbers of Native Americans to resettle further west, often by force, almost always reluctantly. Native Americans believed this forced relocation illegal, given the Treaty of Hopewell of 1785. Under President Andrew Jackson, United States Congress passed the Indian Removal Act of 1830, which authorized the President to conduct treaties to exchange Native American land east of the Mississippi River for lands west of the river.

As many as 100,000 Native Americans relocated to the West as a result of this Indian removal policy. In theory, relocation was supposed to be voluntary and many Native Americans did remain in the East. In practice, great pressure was put on Native American leaders to sign removal treaties. The most egregious violation, the Trail of Tears, was the removal of the Cherokee by President Jackson to Indian Territory. The 1864 deportation of the Navajos by the U.S. government occurred when 8,000 Navajos were forced to an internment camp in Bosque Redondo, where, under armed guards, more than 3,500 Navajo and Mescalero Apache men, women, and children died from starvation and disease.

In 1817, the Cherokee became the first Native Americans recognized as U.S. citizens. Under Article 8 of the 1817 Cherokee treaty, "Upwards of 300 Cherokees (Heads of Families) in the honest simplicity of their souls, made an election to become American citizens".

Factors establishing citizenship included:

After the American Civil War, the Civil Rights Act of 1866 states, "that all persons born in the United States, and not subject to any foreign power, excluding Indians not taxed, are hereby declared to be citizens of the United States".

In 1871, Congress added a rider to the Indian Appropriations Act, signed into law by President Ulysses S. Grant, ending United States recognition of additional Native American tribes or independent nations, and prohibiting additional treaties.

After the Indian wars in the late 19th century, the government established Native American boarding schools, initially run primarily by or affiliated with Christian missionaries. At this time, American society thought that Native American children needed to be acculturated to the general society. The boarding school experience was a total immersion in modern American society, but it could prove traumatic to children, who were forbidden to speak their native languages. They were taught Christianity and not allowed to practice their native religions, and in numerous other ways forced to abandon their Native American identities.

Before the 1930s, schools on the reservations provided no schooling beyond the sixth grade. To obtain more, boarding school was usually necessary. Small reservations with a few hundred people usually sent their children to nearby public schools. The "Indian New Deal" of the 1930s closed many of the boarding schools, and downplayed the assimilationist goals. The Indian Division of the Civilian Conservation Corps operated large-scale construction projects on the reservations, building thousands of new schools and community buildings. Under the leadership of John Collier the Bureau of Indian Affairs (BIA) brought in progressive educators to reshape Indian education. The BIA by 1938 taught 30,000 students in 377 boarding and day schools, or 40% of all Indian children in school. The Navajo largely opposed schooling of any sort, but the other tribes accepted the system. There were now high schools on larger reservations, educating not only teenagers but also an adult audience. There were no Indian facilities for higher education. They deemphasized textbooks, emphasized self-esteem, and started teaching Indian history. They promoted traditional arts and crafts of the sort that could be conducted on the reservations, such as making jewelry. The New Deal reformers met significant resistance from parents and teachers, and had mixed results. World War II brought younger Indians in contact with the broader society through military service and work in the munitions industries. The role of schooling was changed to focus on vocational education for jobs in urban America.

Since the rise of self-determination for Native Americans, they have generally emphasized education of their children at schools near where they live. In addition, many federally recognized tribes have taken over operations of such schools and added programs of language retention and revival to strengthen their cultures. Beginning in the 1970s, tribes have also founded colleges at their reservations, controlled, and operated by Native Americans, to educate their young for jobs as well as to pass on their cultures.

On August 29, 1911, Ishi, generally considered to have been the last Native American to live most of his life without contact with European-American culture, was discovered near Oroville, California.

In 1919, the United States under President Woodrow Wilson granted citizenship to all Native Americans who had served in World War I. Nearly 10,000 men had enlisted and served, a high number in relation to their population. Despite this, in many areas Native Americans faced local resistance when they tried to vote and were discriminated against with barriers to voter registration.

On June 2, 1924, U.S. President Calvin Coolidge signed the Indian Citizenship Act, which made all Native Americans born in the United States and its territories American citizens. Prior to passage of the act, nearly two-thirds of Native Americans were already U.S. citizens, through marriage, military service or accepting land allotments. The Act extended citizenship to "all non-citizen Indians born within the territorial limits of the United States".

Charles Curtis, a Congressman and longtime US Senator from Kansas, was of Kaw, Osage, Potawatomi, and European ancestry. After serving as a United States Representative and being repeatedly re-elected as United States Senator from Kansas, Curtis served as Senate Minority Whip for 10 years and as Senate Majority Leader for five years. He was very influential in the Senate. In 1928 he ran as the vice-presidential candidate with Herbert Hoover for president, and served from 1929 to 1933. He was the first person with significant Native American ancestry and the first person with acknowledged non-European ancestry to be elected to either of the highest offices in the land.

American Indians today in the United States have all the rights guaranteed in the U.S. Constitution, can vote in elections, and run for political office. Controversies remain over how much the federal government has jurisdiction over tribal affairs, sovereignty, and cultural practices.

Mid-century, the Indian termination policy and the Indian Relocation Act of 1956 marked a new direction for assimilating Native Americans into urban life.

The census counted 332,000 Indians in 1930 and 334,000 in 1940, including those on and off reservations in the 48 states. Total spending on Indians averaged $38 million a year in the late 1920s, dropping to a low of $23 million in 1933, and returning to $38 million in 1940.

Some 44,000 Native Americans served in the United States military during World War II: at the time, one-third of all able-bodied Indian men from eighteen to fifty years of age. Described as the first large-scale exodus of indigenous peoples from the reservations since the removals of the 19th century, the men's service with the U.S. military in the international conflict was a turning point in Native American history. The overwhelming majority of Native Americans welcomed the opportunity to serve; they had a voluntary enlistment rate that was 40% higher than those drafted.

Their fellow soldiers often held them in high esteem, in part since the legend of the tough Native American warrior had become a part of the fabric of American historical legend. White servicemen sometimes showed a lighthearted respect toward Native American comrades by calling them "chief". The resulting increase in contact with the world outside of the reservation system brought profound changes to Native American culture. "The war", said the U.S. Indian Commissioner in 1945, "caused the greatest disruption of Native life since the beginning of the reservation era", affecting the habits, views, and economic well-being of tribal members. The most significant of these changes was the opportunity—as a result of wartime labor shortages—to find well-paying work in cities, and many people relocated to urban areas, particularly on the West Coast with the buildup of the defense industry.

There were also losses as a result of the war. For instance, a total of 1,200 Pueblo men served in World War II; only about half came home alive. In addition, many more Navajo served as code talkers for the military in the Pacific. The code they made, although cryptologically very simple, was never cracked by the Japanese.

Military service and urban residency contributed to the rise of American Indian activism, particularly after the 1960s and the occupation of Alcatraz Island (1969–1971) by a student Indian group from San Francisco. In the same period, the American Indian Movement (AIM) was founded in Minneapolis, and chapters were established throughout the country, where American Indians combined spiritual and political activism. Political protests gained national media attention and the sympathy of the American public.

Through the mid-1970s, conflicts between governments and Native Americans occasionally erupted into violence. A notable late 20th-century event was the Wounded Knee incident on the Pine Ridge Indian Reservation. Upset with tribal government and the failures of the federal government to enforce treaty rights, about 300 Oglala Lakota and AIM activists took control of Wounded Knee on February 27, 1973.

Indian activists from around the country joined them at Pine Ridge, and the occupation became a symbol of rising American Indian identity and power. Federal law enforcement officials and the national guard cordoned off the town, and the two sides had a standoff for 71 days. During much gunfire, one United States Marshal was wounded and paralyzed. In late April, a Cherokee and local Lakota man were killed by gunfire; the Lakota elders ended the occupation to ensure no more lives were lost.

In June 1975, two FBI agents seeking to make an armed robbery arrest at Pine Ridge Reservation were wounded in a firefight, and killed at close range. The AIM activist Leonard Peltier was sentenced in 1976 to two consecutive terms of life in prison for the FBI deaths.

In 1968, the government enacted the Indian Civil Rights Act. This gave tribal members most of the protections against abuses by tribal governments that the Bill of Rights accords to all U.S. citizens with respect to the federal government. In 1975, the U.S. government passed the Indian Self-Determination and Education Assistance Act, marking the culmination of fifteen years of policy changes. It resulted from American Indian activism, the Civil Rights Movement, and community development aspects of President Lyndon Johnson's social programs of the 1960s. The Act recognized the right and need of Native Americans for self-determination. It marked the U.S. government's turn away from the 1950s policy of termination of the relationship between tribes and the government. The U.S. government encouraged Native Americans' efforts at self-government and determining their futures. Tribes have developed organizations to administer their own social, welfare and housing programs, for instance. Tribal self-determination has created tension with respect to the federal government's historic trust obligation to care for Indians; however, the Bureau of Indian Affairs has never lived up to that responsibility.

Navajo Community College, now called Diné College, the first tribal college, was founded in Tsaile, Arizona, in 1968 and accredited in 1979. Tensions immediately arose between two philosophies: one that the tribal colleges should have the same criteria, curriculum and procedures for educational quality as mainstream colleges, the other that the faculty and curriculum should be closely adapted to the particular historical culture of the tribe. There was a great deal of turnover, exacerbated by very tight budgets. In 1994, the U.S. Congress passed legislation recognizing the tribal colleges as land-grant colleges, which provided opportunities for large-scale funding. Thirty-two tribal colleges in the United States belong to the American Indian Higher Education Consortium. By the early 21st century, tribal nations had also established numerous language revival programs in their schools.

In addition, Native American activism has led major universities across the country to establish Native American studies programs and departments, increasing awareness of the strengths of Indian cultures, providing opportunities for academics, and deepening research on history and cultures in the United States. Native Americans have entered academia; journalism and media; politics at local, state and federal levels; and public service, for instance, influencing medical research and policy to identify issues related to American Indians.

In 2009, an "apology to Native Peoples of the United States" was included in the Defense Appropriations Act. It stated that the U.S. "apologizes on behalf of the people of the United States to all Native Peoples for the many instances of violence, maltreatment, and neglect inflicted on Native Peoples by citizens of the United States".

In 2013, jurisdiction over persons who were not tribal members under the Violence Against Women Act was extended to Indian Country. This closed a gap which prevented arrest or prosecution by tribal police or courts of abusive partners of tribal members who were not native or from another tribe.

Migration to urban areas continued to grow with 70% of Native Americans living in urban areas in 2012, up from 45% in 1970 and 8% in 1940. Urban areas with significant Native American populations include Minneapolis, Denver, Albuquerque, Phoenix, Tucson, Chicago, Oklahoma City, Houston, New York City, Los Angeles, and Rapid City. Many lived in poverty. Racism, unemployment, drugs and gangs were common problems which Indian social service organizations such as the Little Earth housing complex in Minneapolis attempted to address. Grassroots efforts to support urban Indigenous populations have also taken place, as in the case of Bringing the Circle Together in Los Angeles.

The 2010 Census showed that the U.S. population on April 1, 2010, was 308.7 million. Out of the total U.S. population, 2.9 million people, or 0.9 percent, reported American Indian or Alaska Native alone. In addition, 2.3 million people or another 0.7 percent, reported American Indian or Alaska Native in combination with one or more other races. Together, these two groups totaled 5.2 million people. Thus, 1.7 percent of all people in the United States identified as American Indian or Alaska Native, either alone or in combination with one or more other races.

The definition of American Indian or Alaska Native used in the 2010 census:

According to Office of Management and Budget, "American Indian or Alaska Native" refers to a person having origins in any of the original peoples of North and South America (including Central America) and who maintains tribal affiliation or community attachment.

The 2010 census permitted respondents to self-identify as being of one or more races. Self-identification dates from the census of 1960; prior to that the race of the respondent was determined by opinion of the census taker. The option to select more than one race was introduced in 2000. If American Indian or Alaska Native was selected, the form requested the individual provide the name of the "enrolled or principal tribe".

The census counted 248,000 Native Americans in 1890, 332,000 in 1930 and 334,000 in 1940, including those on and off reservations in the 48 states. Total spending on Native Americans averaged $38 million a year in the late 1920s, dropping to a low of $23 million in 1933, and returning to $38 million in 1940.

78% of Native Americans live outside a reservation. Full-blood individuals are more likely to live on a reservation than mixed-blood individuals. The Navajo, with 286,000 full-blood individuals, is the largest tribe if only full-blood individuals are counted; the Navajo are the tribe with the highest proportion of full-blood individuals, 86.3%. The Cherokee have a different history; it is the largest tribe with 819,000 individuals, and it has 284,000 full-blood individuals.

As of 2012, 70% of Native Americans live in urban areas, up from 45% in 1970 and 8% in 1940. Urban areas with significant Native American populations include Minneapolis, Denver, Phoenix, Tucson, Chicago, Oklahoma City, Houston, New York City, and Los Angeles. Many live in poverty. Racism, unemployment, drugs and gangs are common problems which Indian social service organizations such as the Little Earth housing complex in Minneapolis attempt to address.

According to 2003 United States Census Bureau estimates, a little over one-third of the 2,786,652 Native Americans in the United States live in three states: California (413,382), Arizona (294,137) and Oklahoma (279,559).

In 2010, the U.S. Census Bureau estimated that about 0.8% of the U.S. population was of American Indian or Alaska Native descent. This population is unevenly distributed across the country. Below, all fifty states, as well as the District of Columbia and Puerto Rico, are listed by the proportion of residents citing American Indian or Alaska Native ancestry, based on the 2010 U.S. Census.

Below are numbers for U.S. citizens self-identifying to selected tribal groupings, according to the 2000 U.S. census.

There are 573 federally recognized tribal governments in the United States. These tribes possess the right to form their own governments, to enforce laws (both civil and criminal) within their lands, to tax, to establish requirements for membership, to license and regulate activities, to zone, and to exclude persons from tribal territories. Limitations on tribal powers of self-government include the same limitations applicable to states; for example, neither tribes nor states have the power to make war, engage in foreign relations, or coin money (this includes paper currency).

Many Native Americans and advocates of Native American rights point out that the U.S. federal government's claim to recognize the "sovereignty" of Native American peoples falls short, given that the United States wishes to govern Native American peoples and treat them as subject to U.S. law. Such advocates contend that full respect for Native American sovereignty would require the U.S. government to deal with Native American peoples in the same manner as any other sovereign nation, handling matters related to relations with Native Americans through the Secretary of State, rather than the Bureau of Indian Affairs. The Bureau of Indian Affairs reports on its website that its "responsibility is the administration and management of of land held in trust by the United States for American Indians, Indian tribes, and Alaska Natives". Many Native Americans and advocates of Native American rights believe that it is condescending for such lands to be considered "held in trust" and regulated in any fashion by other than their own tribes, whether the U.S. or Canadian governments, or any other non-Native American authority.

, the largest groups in the United States by population were Navajo, Cherokee, Choctaw, Sioux, Chippewa, Apache, Blackfeet, Iroquois, and Pueblo. In 2000, eight of ten Americans with Native American ancestry were of mixed ancestry. It is estimated that by 2100 that figure will rise to nine out of ten.

In addition, there are a number of tribes that are recognized by individual states, but not by the federal government. The rights and benefits associated with state recognition vary from state to state.

Some tribal groups have been unable to document the cultural continuity required for federal recognition. The Muwekma Ohlone of the San Francisco bay area are pursuing litigation in the federal court system to establish recognition. Many of the smaller eastern tribes, long considered remnants of extinct peoples, have been trying to gain official recognition of their tribal status. Several tribes in Virginia and North Carolina have gained state recognition. Federal recognition confers some benefits, including the right to label arts and crafts as Native American and permission to apply for grants that are specifically reserved for Native Americans. But gaining federal recognition as a tribe is extremely difficult; to be established as a tribal group, members have to submit extensive genealogical proof of tribal descent and continuity of the tribe as a culture.
In July 2000, the Washington State Republican Party adopted a resolution recommending that the federal and legislative branches of the U.S. government terminate tribal governments. In 2007, a group of Democratic Party congressmen and congresswomen introduced a bill in the U.S. House of Representatives to "terminate" the Cherokee Nation. This was related to their voting to exclude Cherokee Freedmen as members of the tribe unless they had a Cherokee ancestor on the Dawes Rolls, although all Cherokee Freedmen and their descendants had been members since 1866.

As of 2004, various Native Americans are wary of attempts by others to gain control of their reservation lands for natural resources, such as coal and uranium in the West.

In the state of Virginia, Native Americans face a unique problem. Until 2017 Virginia previously had no federally recognized tribes but the state had recognized eight. This is related historically to the greater impact of disease and warfare on the Virginia Indian populations, as well as their intermarriage with Europeans and Africans. Some people confused the ancestry with culture, but groups of Virginia Indians maintained their cultural continuity. Most of their early reservations were ended under the pressure of early European settlement.

Some historians also note the problems of Virginia Indians in establishing documented continuity of identity, due to the work of Walter Ashby Plecker (1912–1946). As registrar of the state's Bureau of Vital Statistics, he applied his own interpretation of the one-drop rule, enacted in law in 1924 as the state's Racial Integrity Act. It recognized only two races: "white" and "colored".

Plecker, a segregationist, believed that the state's Native Americans had been "mongrelized" by intermarriage with African Americans; to him, ancestry determined identity, rather than culture. He thought that some people of partial black ancestry were trying to "pass" as Native Americans. Plecker thought that anyone with any African heritage had to be classified as colored, regardless of appearance, amount of European or Native American ancestry, and cultural/community identification. Plecker pressured local governments into reclassifying all Native Americans in the state as "colored", and gave them lists of family surnames to examine for reclassification based on his interpretation of data and the law. This led to the state's destruction of accurate records related to families and communities who identified as Native American (as in church records and daily life). By his actions, sometimes different members of the same family were split by being classified as "white" or "colored". He did not allow people to enter their primary identification as Native American in state records. In 2009, the Senate Indian Affairs Committee endorsed a bill that would grant federal recognition to tribes in Virginia.

To achieve federal recognition and its benefits, tribes must prove continuous existence since 1900. The federal government has maintained this requirement, in part because through participation on councils and committees, federally recognized tribes have been adamant about groups' satisfying the same requirements as they did.

The Civil Rights Movement was a very significant moment for the rights of Native Americans and other people of color. Native Americans faced racism and prejudice for hundreds of years, and this increased after the American Civil War. Native Americans, like African Americans, were subjected to the Jim Crow Laws and segregation in the Deep South especially after they were made citizens through the Indian Citizenship Act of 1924. As a body of law, Jim Crow institutionalized economic, educational, and social disadvantages for Native Americans, and other people of color living in the south. Native American identity was especially targeted by a system that only wanted to recognize white or colored, and the government began to question the legitimacy of some tribes because they had intermarried with African Americans. Native Americans were also discriminated and discouraged from voting in the southern and western states.

In the south segregation was a major problem for Native Americans seeking education, but the NAACP's legal strategy would later change this. Movements such as Brown v. Board of Education was a major victory for the Civil Rights Movement headed by the NAACP, and inspired Native Americans to start participating in the Civil Rights Movement. Dr. Martin Luther King Jr. began assisting Native Americans in the south in the late 1950s after they reached out to him. At that time the remaining Creek in Alabama were trying to completely desegregate schools in their area. In this case, light-complexioned Native children were allowed to ride school buses to previously all white schools, while dark-skinned Native children from the same band were barred from riding the same buses. Tribal leaders, upon hearing of King's desegregation campaign in Birmingham, Alabama, contacted him for assistance. He promptly responded and through his intervention the problem was quickly resolved. Dr. King would later make trips to Arizona visiting Native Americans on reservations, and in churches encouraging them to be involved in the Civil Rights Movement. In King's book "Why We Can't Wait" he writes:
Our nation was born in genocide when it embraced the doctrine that the original American, the Indian, was an inferior race. Even before there were large numbers of Negroes on our shores, the scar of racial hatred had already disfigured colonial society. From the sixteenth century forward, blood flowed in battles over racial supremacy. We are perhaps the only nation which tried as a matter of national policy to wipe out its indigenous population. Moreover, we elevated that tragic experience into a noble crusade. Indeed, even today we have not permitted ourselves to reject or to feel remorse for this shameful episode. Our literature, our films, our drama, our folklore all exalt it.

Native Americans would then actively participate and support the NAACP, and the Civil Rights Movement. The National Indian Youth Council (NIYC) would soon rise in 1961 to fight for Native American rights during the Civil Rights Movement, and were strong supporters of Dr. Martin Luther King Jr.. During the 1963 March on Washington there was a sizable Native American contingent, including many from South Dakota, and many from the Navajo nation. Native Americans also participated the Poor People's Campaign in 1968. The NIYC were very active supporters of the Poor People's Campaign unlike the National Congress of American Indians (NCAI); the NIYC and other Native organizations met with King in March 1968 but the NCAI disagreed on how to approach the anti-poverty campaign; the NCAI decided against participating in the march. The NCAI wished to pursue their battles in the courts and with Congress, unlike the NIYC. The NAACP also inspired the creation of the Native American Rights Fund (NARF) which was patterned after the NAACP's Legal Defense and Education Fund. Furthermore, the NAACP continued to organize to stop mass incarceration and end the criminalization of Native Americans and other communities of people of color. The following is an excerpt from a statement from Mel Thom on May 1, 1968, during a meeting with Secretary of State Dean Rusk: (It was written by members of the Workshop on American Indian Affairs and the NIYC)

Native American struggles amid poverty to maintain life on the reservation or in larger society have resulted in a variety of health issues, some related to nutrition and health practices. The community suffers a vulnerability to and disproportionately high rate of alcoholism.

Recent studies also point to rising rates of stroke, heart disease, and diabetes in the Native American population.

In a study conducted in 2006–2007, non-Native Americans admitted they rarely encountered Native Americans in their daily lives. While sympathetic toward Native Americans and expressing regret over the past, most people had only a vague understanding of the problems facing Native Americans today. For their part, Native Americans told researchers that they believed they continued to face prejudice, mistreatment, and inequality in the broader society.

Federal contractors and subcontractors, such as businesses and educational institutions, are legally required to adopt equal opportunity employment and affirmative action measures intended to prevent discrimination against employees or applicants for employment on the basis of "color, religion, sex, or national origin". For this purpose, a Native American is defined as "A person having origins in any of the original peoples of North and South America (including Central America), and who maintains a tribal affiliation or community attachment". The passing of the Indian Relocation Act saw a 56% increase in Native American city dwellers over 40 years. The Native American urban poverty rate exceeds that of reservation poverty rates due to discrimination in hiring processes. However, self-reporting is permitted: "Educational institutions and other recipients should allow students and staff to self-identify their race and ethnicity unless self-identification is not practicable or feasible."

Self-reporting opens the door to "box checking" by people who, despite not having a substantial relationship to Native American culture, innocently or fraudulently check the box for Native American.

The difficulties that Native Americans face in the workforce, for example, a lack of promotions and wrongful terminations are attributed to racial stereotypes and implicit biases. Native American business owners are seldom offered auxiliary resources that are crucial for entrepreneurial success.

American Indian activists in the United States and Canada have criticized the use of Native American mascots in sports, as perpetuating stereotypes. This is considered cultural appropriation.
There has been a steady decline in the number of secondary school and college teams using such names, images, and mascots. Some tribal team names have been approved by the tribe in question, such as the Seminole Tribe of Florida's approving use of their name for the teams of Florida State University.

Among professional teams, only the NBA's Golden State Warriors discontinued use of Native American-themed logos in 1971. Controversy has remained regarding teams such as the NFL's Washington Redskins, whose name is considered to be a racial slur, and MLB's Cleveland Indians, whose usage of a caricature called Chief Wahoo has also faced protest.

Native Americans have been depicted by American artists in various ways at different periods. A number of 19th- and 20th-century United States and Canadian painters, often motivated by a desire to document and preserve Native culture, specialized in Native American subjects. Among the most prominent of these were Elbridge Ayer Burbank, George Catlin, Seth Eastman, Paul Kane, W. Langdon Kihn, Charles Bird King, Joseph Henry Sharp, and John Mix Stanley.

In the 20th century, early portrayals of Native Americans in movies and television roles were first performed by European Americans dressed in mock traditional attire. Examples included "The Last of the Mohicans" (1920), "Hawkeye and the Last of the Mohicans" (1957), and "F Troop" (1965–67). In later decades, Native American actors such as Jay Silverheels in "The Lone Ranger" television series (1949–57) came to prominence. Roles of Native Americans were limited and not reflective of Native American culture. By the 1970s some Native American film roles began to show more complexity, such as those in "Little Big Man" (1970), "Billy Jack" (1971), and "The Outlaw Josey Wales" (1976), which depicted Native Americans in minor supporting roles.

For years, Native people on U.S. television were relegated to secondary, subordinate roles. During the years of the series "Bonanza" (1959–1973), no major or secondary Native characters appeared on a consistent basis. The series "The Lone Ranger" (1949–1957), "Cheyenne" (1955–1963), and "Law of the Plainsman" (1959–1963) had Native characters who were essentially aides to the central white characters. This continued in such series as "How the West Was Won". These programs resembled the "sympathetic" yet contradictory film "Dances With Wolves" of 1990, in which, according to Ella Shohat and Robert Stam, the narrative choice was to relate the Lakota story as told through a Euro-American voice, for wider impact among a general audience.
Like the 1992 remake of "The Last of the Mohicans" and "" (1993), "Dances with Wolves" employed a number of Native American actors, and made an effort to portray Indigenous languages.

In 2009 "We Shall Remain" (2009), a television documentary by Ric Burns and part of the "American Experience" series, presented a five-episode series "from a Native American perspective". It represented "an unprecedented collaboration between Native and non-Native filmmakers and involves Native advisors and scholars at all levels of the project". The five episodes explore the impact of King Philip's War on the northeastern tribes, the "Native American confederacy" of Tecumseh's War, the U.S.-forced relocation of Southeastern tribes known as the Trail of Tears, the pursuit and capture of Geronimo and the Apache Wars, and concludes with the Wounded Knee incident, participation by the American Indian Movement, and the increasing resurgence of modern Native cultures since.

Native Americans are often known as Indians or American Indians. The term "Native American" was introduced in the United States in preference to the older term "Indian" to distinguish the indigenous peoples of the Americas from the people of India and to avoid negative stereotypes associated with the term "Indian". In 1995, a plurality of indigenous Americans, however, preferred the term "American Indian" and many tribes include the word Indian in their formal title.

Criticism of the neologism "Native American" comes from diverse sources. Russell Means, an American Indian activist, opposed the term "Native American" because he believed it was imposed by the government without the consent of American Indians. He has also argued that the use of the word "Indian" derives not from a confusion with India but from a Spanish expression "en Dios" meaning "in God" (and a near-homophone of the Spanish word for "Indians", "indios").

A 1995 U.S. Census Bureau survey found that more Native Americans in the United States preferred "American Indian" to "Native American". Most American Indians are comfortable with "Indian", "American Indian", and "Native American", and the terms are often used interchangeably. The traditional term is reflected in the name chosen for the National Museum of the American Indian, which opened in 2004 on the Mall in Washington, D.C.

Gambling has become a leading industry. Casinos operated by many Native American governments in the United States are creating a stream of gambling revenue that some communities are beginning to leverage to build diversified economies. Although many Native American tribes have casinos, the impact of Native American gaming is widely debated. Some tribes, such as the Winnemem Wintu of Redding, California, feel that casinos and their proceeds destroy culture from the inside out. These tribes refuse to participate in the gambling industry.

Numerous tribes around the country have entered the financial services market including the Otoe-Missouria, Tunica-Biloxi, and the Rosebud Sioux. Because of the challenges involved in starting a financial services business from scratch, many tribes hire outside consultants and vendors to help them launch these businesses and manage the regulatory issues involved.
Similar to the tribal sovereignty debates that occurred when tribes first entered the gaming industry, the tribes, states, and federal government are currently in disagreement regarding who possesses the authority to regulate these e-commerce business entities.

Prosecution of serious crime, historically endemic on reservations, was required by the 1885 Major Crimes Act, 18 U.S.C. §§1153, 3242, and court decisions to be investigated by the federal government, usually the Federal Bureau of Investigation, and prosecuted by United States Attorneys of the United States federal judicial district in which the reservation lies.

A December 13, 2009 "New York Times" article about growing gang violence on the Pine Ridge Indian Reservation estimated that there were 39 gangs with 5,000 members on that reservation alone. Navajo country recently reported 225 gangs in its territory.

As of 2012, a high incidence of rape continued to impact Native American women and Alaskan native women. According to the Department of Justice, 1 in 3 Native women have suffered rape or attempted rape, more than twice the national rate. About 46 percent of Native American women have been raped, beaten, or stalked by an intimate partner, according to a 2010 study by the Centers for Disease Control. According to Professor N. Bruce Duthu, "More than 80 percent of Indian victims identify their attacker as non-Indian".

Today, other than tribes successfully running casinos, many tribes struggle, as they are often located on reservations isolated from the main economic centers of the country. The estimated 2.1 million Native Americans are the most impoverished of all ethnic groups. According to the 2000 Census, an estimated 400,000 Native Americans reside on reservation land. While some tribes have had success with gaming, only 40% of the 562 federally recognized tribes operate casinos. According to a 2007 survey by the U.S. Small Business Administration, only 1% of Native Americans own and operate a business.

The barriers to economic development on Native American reservations have been identified by Joseph Kalt and Stephen Cornell of the Harvard Project on American Indian Economic Development at Harvard University, in their report: "What Can Tribes Do? Strategies and Institutions in American Indian Economic Development" (2008), are summarized as follows:

A major barrier to development is the lack of entrepreneurial knowledge and experience within Indian reservations. "A general lack of education and experience about business is a significant challenge to prospective entrepreneurs", was the report on Native American entrepreneurship by the Northwest Area Foundation in 2004. "Native American communities that lack entrepreneurial traditions and recent experiences typically do not provide the support that entrepreneurs need to thrive. Consequently, experiential entrepreneurship education needs to be embedded into school curricula and after-school and other community activities. This would allow students to learn the essential elements of entrepreneurship from a young age and encourage them to apply these elements throughout life". "Rez Biz" magazine addresses these issues.

Some scholars argue that the existing theories and practices of economic development are not suitable for Native American communities—given the lifestyle, economic, and cultural differences, as well as the unique history of Native American-U.S. relations. Most economic development research were not conducted on Native American communities. The federal government fails to consider place-based issues of American Indian poverty by generalizing the demographic. In addition, the concepts of economic development threatens to upend the multidimensionality of Native American culture. The dominance of federal government involvement in indigenous developmental activities perpetuates and exacerbates the salvage paradigm.

Native land that is owned by individual Native Americans sometimes cannot be developed because of fractionalization. Fractionalization occurs when a landowner dies, and their land is inherited by their children, but not subdivided. This means that one parcel might be owned by 50 different individuals. A majority of those holding interest must agree to any proposal to develop the land, and establishing this consent is time-consuming, cumbersome, and sometimes impossible.
Another landownership issue on reservations is checkerboarding, where Tribal land is interspersed with land owned by the federal government on behalf of Natives, individually owned plots, and land owned by non-Native individuals. This prevents Tribal governments from securing plots of land large enough for economic development or agricultural uses.
Because reservation land is owned “in trust” by the federal government, individuals living on reservations cannot build equity in their homes. This bars Native Americans from getting loans, as there is nothing that a bank can collect if the loan is not paid. Past efforts to encourage landownership (such as the Dawes Act) resulted in a net loss of Tribal land. After they were familiarized with their smallholder status, Native American landowners were lifted of trust restrictions and their land would get transferred back to them, contingent on a transactional fee to the federal government. The transfer fee discouraged Native American land ownership, with 65% of tribal owned land being sold to non-Native Americans by the 1920s. Activists against property rights point to historical evidence of communal ownership of land and resources by tribes. They claim that because of this history, property rights are foreign to Natives and have no place in the modern reservation system. Those in favor of property rights cite examples of tribes negotiating with colonial communities or other tribes about fishing and hunting rights in an area. Land ownership was also a challenge because of the different definitions of land that the Natives and the Europeans had. Most Native American tribes thought of property rights more as "borrowing" the land, while those from Europe thought of land as individual property.

State-level efforts such as the Oklahoma Indian Welfare Act were attempts to contain tribal land in Native American hands. However, more bureaucratic decisions only expanded the size of the bureaucracy. The knowledge disconnect between the decision-making bureaucracy and Native American stakeholders resulted in ineffective development efforts.

Traditional Native American entrepreneurship does not prioritize profit maximization, rather, business transactions must have align with their social and cultural values. In response to indigenous business philosophy, the federal government created policies that aimed to formalize their business practices, which undermined the Native American status quo. Additionally, legal disputes interfered with tribal land leasing, which were settled with the verdict against tribal sovereignty.

Often, bureaucratic overseers of development are far removed from Native American communities, and lack the knowledge and understanding to develop plans or make resource allocation decisions. The top-down heavy involvement in developmental operations corrupts bureaucrats into further self-serving agenda. Such incidences include fabricated reports that exaggerate results.

While Native American urban poverty is attributed to hiring and workplace discrimination in a heterogeneous setting, reservation and trust land poverty rates are endogenous to deserted opportunities in isolated regions.

Historical trauma is described as collective emotional and psychological damage throughout a person's lifetime and across multiple generations. Examples of historical trauma can be seen through the Wounded Knee Massacre of 1890, where over 200 unarmed Lakota were killed, and the Dawes Allotment Act of 1887, when American Indians lost four-fifths of their land.

American Indian youth have higher rates of substance and alcohol abuse deaths than the general population. Many American Indians can trace the beginning of their substance and alcohol abuse to a traumatic event related to their offender's own substance abuse. A person's substance abuse can be described as a defense mechanism against the user's emotions and trauma. For American Indians alcoholism is a symptom of trauma passed from generation to generation and influenced by oppressive behaviors and policies by the dominant Euro-American society. Boarding schools were made to "Kill the Indian, Save the man". Shame among American Indians can be attributed to the hundreds of years of discrimination.

The culture of Pre-Columbian North America is usually defined by the concept of the culture area, namely a geographical region where shared cultural traits occur. The northwest culture area, for example shared common traits such as salmon fishing, woodworking, and large villages or towns and a hierarchical social structure.

Though cultural features, language, clothing, and customs vary enormously from one tribe to another, there are certain elements which are encountered frequently and shared by many tribes. Early European American scholars described the Native Americans as having a society dominated by clans.

European colonization of the Americas had a major impact on Native American cultures through what is known as the Columbian exchange. The Columbian exchange, also known as the Columbian interchange, was the widespread transfer of plants, animals, culture, human populations, technology, and ideas between the Americas and Eurasia (the Old World) in the 15th and 16th centuries, following Christopher Columbus's 1492 voyage. The Columbian exchange generally had a destructive impact on Native American cultures through disease, and a 'clash of cultures', whereby European values of private land ownership, the family, and division of labor, led to conflict, appropriation of traditional communal lands and changed how the indigenous tribes practiced slavery.

The impact of the Columbian exchange was not entirely negative however. For example, the re-introduction of the horse to North America allowed the Plains Indian to revolutionize their ways of life by making hunting, trading, and warfare far more effective, and to greatly improve their ability to transport possessions and move their settlements.

The Great Plains tribes were still hunting the bison when they first encountered the Europeans. The Spanish reintroduction of the horse to North America in the 17th century and Native Americans' learning to use them greatly altered the Native Americans' cultures, including changing the way in which they hunted large game. Horses became such a valuable, central element of Native lives that they were counted as a measure of wealth by many tribes.

In the early years, as Native peoples encountered European explorers and settlers and engaged in trade, they exchanged food, crafts, and furs for blankets, iron and steel implements, horses, trinkets, firearms, and alcoholic beverages.

The Na-Dené, Algic, and Uto-Aztecan families are the largest in terms of number of languages. Uto-Aztecan has the most speakers (1.95 million) if the languages in Mexico are considered (mostly due to 1.5 million speakers of Nahuatl); Na-Dené comes in second with approximately 200,000 speakers (nearly 180,000 of these are speakers of Navajo), and Algic in third with about 180,000 speakers (mainly Cree and Ojibwe). Na-Dené and Algic have the widest geographic distributions: Algic currently spans from northeastern Canada across much of the continent down to northeastern Mexico (due to later migrations of the Kickapoo) with two outliers in California (Yurok and Wiyot); Na-Dené spans from Alaska and western Canada through Washington, Oregon, and California to the U.S. Southwest and northern Mexico (with one outlier in the Plains). Several families consist of only 2 or 3 languages. Demonstrating genetic relationships has proved difficult due to the great linguistic diversity present in North America. Two large (super-) family proposals, Penutian and Hokan, look particularly promising. However, even after decades of research, a large number of families remain.

A number of words used in English have been derived from Native American languages.

To counteract a shift to English, some Native American tribes have initiated language immersion schools for children, where an Indigenous American language is the medium of instruction. For example, the Cherokee Nation initiated a 10-year language preservation plan that involved raising new fluent speakers of the Cherokee language from childhood on up through school immersion programs as well as a collaborative community effort to continue to use the language at home. This plan was part of an ambitious goal that, in 50 years, will result in 80% or more of the Cherokee people being fluent in the language. The Cherokee Preservation Foundation has invested $3 million in opening schools, training teachers, and developing curricula for language education, as well as initiating community gatherings where the language can be actively used. Formed in 2006, the Kituwah Preservation & Education Program (KPEP) on the Qualla Boundary focuses on language immersion programs for children from birth to fifth grade, developing cultural resources for the general public and community language programs to foster the Cherokee language among adults.

There is also a Cherokee language immersion school in Tahlequah, Oklahoma, that educates students from pre-school through eighth grade. Because Oklahoma's official language is English, Cherokee immersion students are hindered when taking state-mandated tests because they have little competence in English. The Department of Education of Oklahoma said that in 2012 state tests: 11% of the school's sixth-graders showed proficiency in math, and 25% showed proficiency in reading; 31% of the seventh-graders showed proficiency in math, and 87% showed proficiency in reading; 50% of the eighth-graders showed proficiency in math, and 78% showed proficiency in reading. The Oklahoma Department of Education listed the charter school as a Targeted Intervention school, meaning the school was identified as a low-performing school but has not so that it was a Priority School. Ultimately, the school made a C, or a 2.33 grade point average on the state's A-F report card system. The report card shows the school getting an F in mathematics achievement and mathematics growth, a C in social studies achievement, a D in reading achievement, and an A in reading growth and student attendance. "The C we made is tremendous," said school principal Holly Davis, "[t]here is no English instruction in our school's younger grades, and we gave them this test in English." She said she had anticipated the low grade because it was the school's first year as a state-funded charter school, and many students had difficulty with English. Eighth graders who graduate from the Tahlequah immersion school are fluent speakers of the language, and they usually go on to attend Sequoyah High School where classes are taught in both English and Cherokee.

Historical diets of Native Americans differed dramatically region to region. Different peoples might have relayed more heavily of agriculture, horticulture, hunting, fishing, or gathering of wild plants and fungi. Tribes developed diets best suited for their environments.

Iñupiat, Yupiit, Unangan, and fellow Alaska Natives fished, hunted, and harvested wild plants, but did not rely on agriculture. Coastal peoples relied more heavily on sea mammals, fish, and fish eggs, while inland peoples hunted caribou and moose. Alaskan Natives prepared and preserved dried and smoked meat and fish.

Pacific Northwest tribes crafted seafaring dugouts long for fishing.

In the Eastern Woodlands, early peoples independently invented agricultural and by 1800 BCE developed the crops of the Eastern Agricultural Complex, which include squash ("Cucurbita pepo ssp. ovifera"), sunflower ("Helianthus annuus var. macrocarpus"), goosefoot ("Chenopodium berlandieri"), and marsh elder ("Iva annua var. macrocarpa").

The Sonoran desert region including parts of Arizona and California, part of a region known as Aridoamerica, relied heavily on the tepary bean ("Phaseolus acutifolius") as a staple crop. This and other desert crops, mesquite bead pods, "tunas" (prickly pear fruit), cholla buds, saguaro cactus fruit, and acorns are being actively promoted today by Tohono O'odham Community Action. In the Southwest, some communities developed irrigation techniques while others, such as the Hopi dry-farmed. They filled storehouses with grain as protection against the area's frequent droughts.

Maize or corn, first cultivated in what is now Mexico was traded north into Aridoamerica and Oasisamerica, southwest. From there, maize cultivation spread throughout the Great Plains and Eastern Woodlands by 200 CE. Native farmers practiced polycropping maize, beans, and squash; these crops are known as the Three Sisters. The beans would replace the nitrogen, which the maize leached from the ground, as well as using corn stalks for support for climbing.

The agriculture gender roles of the Native Americans varied from region to region. In the Southwest area, men prepared the soil with hoes. The women were in charge of planting, weeding, and harvesting the crops. In most other regions, the women were in charge of most agriculture, including clearing the land. Clearing the land was an immense chore since the Native Americans rotated fields.

Europeans in the eastern part of the continent observed that Native Americans cleared large areas for cropland. Their fields in New England sometimes covered hundreds of acres. Colonists in Virginia noted thousands of acres under cultivation by Native Americans.
Early farmers commonly used tools such as the hoe, maul, and dibber. The hoe was the main tool used to till the land and prepare it for planting; then it was used for weeding. The first versions were made out of wood and stone. When the settlers brought iron, Native Americans switched to iron hoes and hatchets. The dibber was a digging stick, used to plant the seed. Once the plants were harvested, women prepared the produce for eating. They used the maul to grind the corn into mash. It was cooked and eaten that way or baked as corn bread.

Native American religious practices, beliefs, and philosophies differ widely across tribes. These spiritualities, practices, beliefs, and philosophies may accompany adherence to another faith, or can represent a person's primary religious, faith, spiritual or philosophical identity. Much Native American spirituality exists in a tribal-cultural continuum, and as such cannot be easily separated from tribal identity itself.

Cultural spiritual, philosophical, and faith ways differ from tribe to tribe and person to person. Some tribes include the use of sacred leaves and herbs such as tobacco, sweetgrass or sage. Many Plains tribes have sweatlodge ceremonies, though the specifics of the ceremony vary among tribes. Fasting, singing and prayer in the ancient languages of their people, and sometimes drumming are also common.

The Midewiwin Lodge is a medicine society inspired by the oral history and prophesies of the Ojibwa (Chippewa) and related tribes.

Another significant religious body among Native peoples is known as the Native American Church. It is a syncretistic church incorporating elements of Native spiritual practice from a number of different tribes as well as symbolic elements from Christianity. Its main rite is the peyote ceremony. Prior to 1890, traditional religious beliefs included Wakan Tanka. In the American Southwest, especially New Mexico, a syncretism between the Catholicism brought by Spanish missionaries and the native religion is common; the religious drums, chants, and dances of the Pueblo people are regularly part of Masses at Santa Fe's Saint Francis Cathedral. Native American-Catholic syncretism is also found elsewhere in the United States. (e.g., the National Kateri Tekakwitha Shrine in Fonda, New York, and the National Shrine of the North American Martyrs in Auriesville, New York).

The eagle feather law (Title 50 Part 22 of the Code of Federal Regulations) stipulates that only individuals of certifiable Native American ancestry enrolled in a federally recognized tribe are legally authorized to obtain eagle feathers for religious or spiritual use. The law does not allow Native Americans to give eagle feathers to non-Native Americans.

Gender roles are differentiated in many Native American tribes. Many Natives have retained traditional expectations of sexuality and gender, and continue to do so in contemporary life despite continued and on-going colonial pressures.

Whether a particular tribe is predominantly matrilineal or patrilineal, often both sexes have some degree of decision-making power within the tribe. Many Nations, such as the Haudenosaunee Five Nations and the Southeast Muskogean tribes, have matrilineal or Clan Mother systems, in which property and hereditary leadership are controlled by and passed through the maternal lines. In these Nations, the children are considered to belong to the mother's clan. In Cherokee culture, women own the family property. When traditional young women marry, their husbands may join them in their mother's household.

Matrilineal structures enable young women to have assistance in childbirth and rearing, and protect them in case of conflicts between the couple. If a couple separates or the man dies, the woman has her family to assist her. In matrilineal cultures the mother's brothers are usually the leading male figures in her children's lives; fathers have no standing in their wife and children's clan, as they still belong to their own mother's clan. Hereditary clan chief positions pass through the mother's line and chiefs have historically been selected on recommendation of women elders, who could also disapprove of a chief.

In the patrilineal tribes, such as the Omaha, Osage, Ponca, and Lakota, hereditary leadership passes through the male line, and children are considered to belong to the father and his clan. In patrilineal tribes, if a woman marries a non-Native, she is no longer considered part of the tribe, and her children are considered to share the ethnicity and culture of their father.

In patriarchal tribes, gender roles tend to be rigid. Men have historically hunted, traded and made war while, as life-givers, women have primary responsibility for the survival and welfare of the families (and future of the tribe). Women usually gather and cultivate plants, use plants and herbs to treat illnesses, care for the young and the elderly, make all the clothing and instruments, and process and cure meat and skins from the game. Some mothers use cradleboards to carry an infant while working or traveling. In matriarchal and egalitarian nations, the gender roles are usually not so clear-cut, and are even less so in the modern era.

At least several dozen tribes allowed polygyny to sisters, with procedural and economic limits.

Lakota, Dakota, and Nakota girls are encouraged to learn to ride, hunt and fight. Though fighting in war has mostly been left to the boys and men, occasionally women have fought as well – both in battles and in defense of the home – especially if the tribe was severely threatened.

Native American leisure time led to competitive individual and team sports. Jim Thorpe, Joe Hipp, Notah Begay III, Chris Wondolowski, Jacoby Ellsbury, Joba Chamberlain, Kyle Lohse, Sam Bradford, Jack Brisco, Tommy Morrison, Billy Mills, Angel Goodrich, Shoni Schimmel, and Kyrie Irving are well known professional athletes.
Native American ball sports, sometimes referred to as lacrosse, stickball, or baggataway, were often used to settle disputes, rather than going to war, as a civil way to settle potential conflict. The Choctaw called it "isitoboli" ("Little Brother of War"); the Onondaga name was "dehuntshigwa'es" ("men hit a rounded object"). There are three basic versions, classified as Great Lakes, Iroquoian, and Southern.

The game is played with one or two rackets or sticks and one ball. The object of the game is to land the ball in the opposing team's goal (either a single post or net) to score and to prevent the opposing team from scoring on your goal. The game involves as few as 20 or as many as 300 players with no height or weight restrictions and no protective gear. The goals could be from around apart to about ; in lacrosse the field is .

Chunkey was a game that consisted of a stone-shaped disk that was about 1–2 inches in diameter. The disk was thrown down a corridor so that it could roll past the players at great speed. The disk would roll down the corridor, and players would throw wooden shafts at the moving disk. The object of the game was to strike the disk or prevent your opponents from hitting it.
Jim Thorpe, a Sauk and Fox Native American, was an all-round athlete playing football and baseball in the early 20th century. Future President Dwight Eisenhower injured his knee while trying to tackle the young Thorpe. In a 1961 speech, Eisenhower recalled Thorpe: "Here and there, there are some people who are supremely endowed. My memory goes back to Jim Thorpe. He never practiced in his life, and he could do anything better than any other football player I ever saw."

In the 1912 Olympics, Thorpe could run the 100-yard dash in 10 seconds flat, the 220 in 21.8 seconds, the 440 in 51.8 seconds, the 880 in 1:57, the mile in 4:35, the 120-yard high hurdles in 15 seconds, and the 220-yard low hurdles in 24 seconds. He could long jump 23 ft 6 in and high-jump 6 ft 5 in. He could pole vault , put the shot , throw the javelin , and throw the discus . Thorpe entered the U.S. Olympic trials for the pentathlon and the decathlon.

Louis Tewanima, Hopi people, was an American two-time Olympic distance runner and silver medalist in the 10,000 meter run in 1912. He ran for the Carlisle Indian School where he was a teammate of Jim Thorpe. His silver medal in 1912 remained the best U.S. achievement in this event until another Indian, Billy Mills, won the gold medal in 1964. Tewanima also competed at the 1908 Olympics, where he finished in ninth place in the marathon.[1]

Ellison Brown, of the Narragansett people from Rhode Island, better known as "Tarzan" Brown, won two Boston Marathons (1936, 1939) and competed on the United States Olympic team in the 1936 Olympic Games in Berlin, Germany, but did not finish due to injury. He qualified for the 1940 Olympic Games in Helsinki, Finland, but the games were canceled due to the outbreak of World War II.

Billy Mills, a Lakota and USMC officer, won the gold medal in the 10,000 meter run at the 1964 Tokyo Olympics. He was the only American ever to win the Olympic gold in this event. An unknown before the Olympics, Mills finished second in the U.S. Olympic trials.

Billy Kidd, part Abenaki from Vermont, became the first American male to medal in alpine skiing in the Olympics, taking silver at age 20 in the slalom in the 1964 Winter Olympics at Innsbruck, Austria. Six years later at the 1970 World Championships, Kidd won the gold medal in the combined event and took the bronze medal in the slalom.

Ashton Locklear (Lumbee), an uneven bars specialist was an alternate for the 2016 Summer Olympics U.S. gymnastics team, the Final Five. In 2016, Kyrie Irving (Sioux) also helped Team USA win the gold medal at the 2016 Summer Olympics. With the win, he became just the fourth member of Team USA to capture the NBA championship and an Olympic gold medal in the same year, joining LeBron James, Michael Jordan, and Scottie Pippen.

Traditional Native American music is almost entirely monophonic, but there are notable exceptions. Native American music often includes drumming or the playing of rattles or other percussion instruments but little other instrumentation. Flutes and whistles made of wood, cane, or bone are also played, generally by individuals, but in former times also by large ensembles (as noted by Spanish conquistador de Soto). The tuning of modern flutes is typically pentatonic.

Performers with Native American parentage have occasionally appeared in American popular music such as Rita Coolidge, Wayne Newton, Gene Clark, Buffy Sainte-Marie, Blackfoot, Redbone (members are also of Mexican descent), and CocoRosie. Some, such as John Trudell, have used music to comment on life in Native America. Other musicians such as R. Carlos Nakai, Joanne Shenandoah and Robert "Tree" Cody integrate traditional sounds with modern sounds in instrumental recordings, whereas the music by artist Charles Littleleaf is derived from ancestral heritage as well as nature. A variety of small and medium-sized recording companies offer an abundance of recent music by Native American performers young and old, ranging from pow-wow drum music to hard-driving rock-and-roll and rap. In the International world of ballet dancing Maria Tallchief was considered America's first major prima ballerina, and was the first person of Native American descent to hold the rank. along with her sister Marjorie Tallchief both became star ballerinas.

The most widely practiced public musical form among Native Americans in the United States is that of the pow-wow. At pow-wows, such as the annual Gathering of Nations in Albuquerque, New Mexico, members of drum groups sit in a circle around a large drum. Drum groups play in unison while they sing in a native language and dancers in colorful regalia dance clockwise around the drum groups in the center. Familiar pow-wow songs include honor songs, intertribal songs, crow-hops, sneak-up songs, grass-dances, two-steps, welcome songs, going-home songs, and war songs. Most indigenous communities in the United States also maintain traditional songs and ceremonies, some of which are shared and practiced exclusively within the community.

The Iroquois, living around the Great Lakes and extending east and north, used strings or belts called "wampum" that served a dual function: the knots and beaded designs mnemonically chronicled tribal stories and legends, and further served as a medium of exchange and a unit of measure. The keepers of the articles were seen as tribal dignitaries.

Pueblo peoples crafted impressive items associated with their religious ceremonies. "Kachina" dancers wore elaborately painted and decorated masks as they ritually impersonated various ancestral spirits.
Pueblo people are particularly noted for their traditional high-quality pottery, often with geometric designs and floral, animal and bird motifs. Sculpture was not highly developed, but carved stone and wood fetishes were made for religious use. Superior weaving, embroidered decorations, and rich dyes characterized the textile arts. Both turquoise and shell jewelry were created, as were formalized pictorial arts.

Navajo spirituality focused on the maintenance of a harmonious relationship with the spirit world, often achieved by ceremonial acts, usually incorporating sandpainting. For the Navajo the sand painting is not merely a representational object, but a dynamic spiritual entity with a life of its own, which helped the patient at the centre of the ceremony re-establish a connection with the life force. These vivid, intricate, and colorful sand creations were erased at the end of the healing ceremony.

The Native American arts and crafts industry brings in more than a billion in gross sales annually.

Native American art comprises a major category in the world art collection. Native American contributions include pottery, paintings, jewellery, weavings, sculpture, basketry, and carvings. Franklin Gritts was a Cherokee artist who taught students from many tribes at Haskell Institute (now Haskell Indian Nations University) in the 1940s, the "Golden Age" of Native American painters. The integrity of certain Native American artworks is protected by the Indian Arts and Crafts Act of 1990, that prohibits representation of art as Native American when it is not the product of an enrolled Native American artist. Attorney Gail Sheffield and others claim that this law has had "the unintended consequence of sanctioning discrimination against Native Americans whose tribal affiliation was not officially recognized". Native artists such as Jeanne Rorex Bridges (Echota Cherokee) who was not enrolled ran the risk of fines or imprisonment if they continued to sell their art while affirming their Indian heritage.

Interracial relations between Native Americans, Europeans, and Africans is a complex issue that has been mostly neglected with "few in-depth studies on interracial relationships". Some of the first documented cases of European/Native American intermarriage and contact were recorded in Post-Columbian Mexico. One case is that of Gonzalo Guerrero, a European from Spain, who was shipwrecked along the Yucatan Peninsula, and fathered three Mestizo children with a Mayan noblewoman. Another is the case of Hernán Cortés and his mistress La Malinche, who gave birth to another of the first multi-racial people in the Americas.

European impact was immediate, widespread, and profound already during the early years of colonization and the creation of the countries which currently exist in the Americas. Europeans living among Native Americans were often called "white indians". They "lived in native communities for years, learned native languages fluently, attended native councils, and often fought alongside their native companions".

Early contact was often charged with tension and emotion, but also had moments of friendship, cooperation, and intimacy. Marriages took place in English, Spanish, French, and Russian colonies between Native Americans and Europeans though Native American women were also the victims of rape. 

There was fear on both sides, as the different peoples realized how different their societies were. Many whites regarded Native people as "savages" because the Native people were not Protestant or Roman Catholic and therefore the Native people were not considered to be human beings. Orthodox Christians never viewed Native people as savages or sub-human. The Native American author, Andrew J. Blackbird, wrote in his "History of the Ottawa and Chippewa Indians of Michigan" (1897), that white settlers introduced some immoralities into Native American tribes. Many Native Americans suffered because the Europeans introduced alcohol. Many Native people do not break down alcohol in the same way as people of Eurasian background. Many Native people were learning what their body could tolerate of this new substance and died as a result of imbibing too much. alcohol-intolerant.

Blackbird wrote:

The U.S. government had two purposes when making land agreements with Native Americans: to open it up more land for white settlement, and to "ease tensions" (in other words assimilate Native people to Eurasian social ways) between whites and Native Americans by forcing the Native Americans to use the land in the same way as did the whites—for subsistence farms. The government used a variety of strategies to achieve these goals; many treaties required Native Americans to become farmers in order to keep their land. Government officials often did not translate the documents which Native Americans were forced to sign, and native chiefs often had little or no idea what they were signing.
For a Native American man to marry a white woman, he had to get consent of her parents, as long as "he can prove to support her as a white woman in a good home". In the early 19th century, the Shawnee Tecumseh and blonde hair, blue-eyed Rebbecca Galloway had an interracial affair. In the late 19th century, three European-American middle-class women teachers at Hampton Institute married Native American men whom they had met as students.

As European-American women started working independently at missions and Indian schools in the western states, there were more opportunities for their meeting and developing relationships with Native American men. For instance, Charles Eastman, a man of European and Lakota origin whose father sent both his sons to Dartmouth College, got his medical degree at Boston University and returned to the West to practice. He married Elaine Goodale, whom he met in South Dakota. He was the grandson of Seth Eastman, a military officer from Maine, and a chief's daughter. Goodale was a young European-American teacher from Massachusetts and a reformer, who was appointed as the U.S. superintendent of Native American education for the reservations in the Dakota Territory. They had six children together.

The majority of Native American tribes did practice some form of slavery before the European introduction of African slavery into North America, but none exploited slave labor on a large scale. Most Native American tribes did not barter captives in the pre-colonial era, although they sometimes exchanged enslaved individuals with other tribes in peace gestures or in exchange for their own members. When Europeans arrived as colonists in North America, Native Americans changed their practice of slavery dramatically. Native Americans began selling war captives to Europeans rather than integrating them into their own societies as they had done before. As the demand for labor in the West Indies grew with the cultivation of sugar cane, Europeans enslaved Native Americans for the Thirteen Colonies, and some were exported to the "sugar islands". The British settlers, especially those in the southern colonies, purchased or captured Native Americans to use as forced labor in cultivating tobacco, rice, and indigo. Accurate records of the numbers enslaved do not exist because vital statistics and census reports were at best infrequent. Scholars estimate tens to hundreds of thousands of Native Americans may have been enslaved by the Europeans, being sold by Native Americans themselves or Europeans. 
Slaves became a caste of people who were foreign to the English (Native Americans, Africans and their descendants) and non-Christians. The Virginia General Assembly defined some terms of slavery in 1705:

The slave trade of Native Americans lasted only until around 1750. It gave rise to a series of devastating wars among the tribes, including the Yamasee War. The Indian Wars of the early 18th century, combined with the increasing importation of African slaves, effectively ended the Native American slave trade by 1750. Colonists found that Native American slaves could easily escape, as they knew the country. The wars cost the lives of numerous colonial slave traders and disrupted their early societies. The remaining Native American groups banded together to face the Europeans from a position of strength. Many surviving Native American peoples of the southeast strengthened their loose coalitions of language groups and joined confederacies such as the Choctaw, the Creek, and the Catawba for protection. Even after the Indian Slave Trade ended in 1750 the enslavement of Native Americans continued in the west, and also in the Southern states mostly through kidnappings.

Both Native American and African enslaved women suffered rape and sexual harassment by male slaveholders and other white men.

African and Native Americans have interacted for centuries. The earliest record of Native American and African contact occurred in April 1502, when Spanish colonists transported the first Africans to Hispaniola to serve as slaves.
Sometimes Native Americans resented the presence of African Americans. The "Catawaba tribe in 1752 showed great anger and bitter resentment when an African American came among them as a trader". To gain favor with Europeans, the Cherokee exhibited the strongest color prejudice of all Native Americans. Because of European fears of a unified revolt of Native Americans and African Americans, the colonists tried to encourage hostility between the ethnic groups: "Whites sought to convince Native Americans that African Americans worked against their best interests." In 1751, South Carolina law stated:

In addition, in 1758 the governor of South Carolina James Glen wrote:

Europeans considered both races inferior and made efforts to make both Native Americans and Africans enemies. Native Americans were rewarded if they returned escaped slaves, and African Americans were rewarded for fighting in the late 19th-century Indian Wars.

"Native Americans, during the transitional period of Africans becoming the primary race enslaved, were enslaved at the same time and shared a common experience of enslavement. They worked together, lived together in communal quarters, produced collective recipes for food, shared herbal remedies, myths and legends, and in the end they intermarried." Because of a shortage of men due to warfare, many tribes encouraged marriage between the two groups, to create stronger, healthier children from the unions.

In the 18th century, many Native American women married freed or runaway African men due to a decrease in the population of men in Native American villages. Records show that many Native American women bought African men but, unknown to the European sellers, the women freed and married the men into their tribe. When African men married or had children by a Native American woman, their children were born free, because the mother was free (according to the principle of "partus sequitur ventrem", which the colonists incorporated into law).

While numerous tribes used captive enemies as servants and slaves, they also often adopted younger captives into their tribes to replace members who had died. In the Southeast, a few Native American tribes began to adopt a slavery system similar to that of the American colonists, buying African American slaves, especially the Cherokee, Choctaw, and Creek. Though less than 3% of Native Americans owned slaves, divisions grew among the Native Americans over slavery. Among the Cherokee, records show that slave holders in the tribe were largely the children of European men who had shown their children the economics of slavery. As European colonists took slaves into frontier areas, there were more opportunities for relationships between African and Native American peoples.

In the 2010 Census, nearly 3 million people indicated that their race was Native American (including Alaska Native). Of these, more than 27% specifically indicated "Cherokee" as their ethnic origin. Many of the First Families of Virginia claim descent from Pocahontas or some other "Indian princess". This phenomenon has been dubbed the "Cherokee Syndrome". Across the US, numerous individuals cultivate an opportunistic ethnic identity as Native American, sometimes through Cherokee heritage groups or Indian Wedding Blessings.

Many tribes, especially those in the Eastern United States, are primarily made up of individuals with an unambiguous Native American identity, despite being predominantly of European ancestry. More than 75% of those enrolled in the Cherokee Nation have less than one-quarter Cherokee blood, and the current Principal Chief of the Cherokee Nation, Bill John Baker, is 1/32 Cherokee, amounting to about 3%.

Historically, numerous Native Americans assimilated into colonial and later American society, e.g. through adopting English and converting to Christianity. In many cases, this process occurred through forced assimilation of children sent off to special boarding schools far from their families. Those who could pass for white had the advantage of white privilege Today, after generations of racial whitening through hypergamy and interracial marriage, many Native Americans are visually indistinguishable from White Americans, unlike mestizos in the United States, who may in fact have little or no non-indigenous ancestry. Considered a property that would hold Indians back on the road to civilization, Indian blood could be diluted over generations through interbreeding with Euro-American populations. Native Americans were seen as capable of cultural evolution (unlike Africans) and therefore of cultural absorption into the white populace. “Kill the Indian, save the man” was a mantra of nineteenth-century U.S. assimilation policies.

Native Americans are more likely than any other racial group to practice interracial marriage, resulting in an ever-declining proportion of indigenous blood among those who claim a Native American identity. Some tribes will even resort to disenrollment of tribal members unable to provide scientific "proof" of Native ancestry, usually through a Certificate of Degree of Indian Blood. Disenrollment has become a contentious issue in Native American reservation politics.

Intertribal mixing was common among many Native American tribes prior to European contact, as they would adopt captives taken in warfare. Individuals often had ancestry from more than one tribe, particularly after tribes lost so many members from disease in the colonial era and after. Bands or entire tribes occasionally split or merged to form more viable groups in reaction to the pressures of climate, disease and warfare.

A number of tribes traditionally adopted captives into their group to replace members who had been captured or killed in battle. Such captives were from rival tribes and later were taken from raids on European settlements. Some tribes also sheltered or adopted white traders and runaway slaves, and others owned slaves of their own. Tribes with long trading histories with Europeans show a higher rate of European admixture, reflecting years of intermarriage between Native American women and European men, often seen as advantageous to both sides. A number of paths to genetic and ethnic diversity among Native Americans have occurred.

In recent years, genetic genealogists have been able to determine the proportion of Native American ancestry carried by the African-American population. The literary and history scholar Henry Louis Gates, Jr., had experts on his TV programs who discussed African-American ancestry. They stated that 5% of African Americans have at least 12.5% Native American ancestry, or the equivalent to one great-grandparent, which may represent more than one distant ancestor. A greater percentage could have a smaller proportion of Indian ancestry, but their conclusions show that popular estimates of Native American admixture may have been too high. More recent genetic testing research of 2015, have found varied ancestries which show different tendencies by region and sex of ancestors. Though DNA testing is limited these studies found that on average, African Americans have 73.2–82.1% West African, 16.7%–29% European, and 0.8–2% Native American genetic ancestry, with large variation between individuals.

DNA testing is not sufficient to qualify a person for specific tribal membership, as it cannot distinguish among Native American tribes; however some tribes such as the Meskwaki Nation require a DNA test in order to enroll in the tribe. 

Most DNA testing examines few lineages that comprise a minuscule percentage of one’s total ancestry, approximately less than 1 percent of total DNA. Every human being has about one thousand ancestors going back ten generations.

In "Native American DNA: Tribal Belonging and the False Promise of Genetic Science", Kim Tallbear states that a person, "… could have up to two Native American grandparents and show no sign of Native American ancestry. For example, a genetic male could have a maternal grandfather (from whom he did not inherit his Y chromosome) and a paternal grandmother (from whom he did not inherit his mtDNA) who were descended from Native American founders, but mtDNA and Y-chromosome analyses would not detect them."

Native American identity has historically been based on culture, not just biology, as many American Indian peoples adopted captives from their enemies and assimilated them into their tribes. The Indigenous Peoples Council on Biocolonialism (IPCB) notes that:

"Native American markers" are not found solely among Native Americans. While they occur more frequently among Native Americans, they are also found in people in other parts of the world.

Geneticists state:

Not all Native Americans have been tested; especially with the large number of deaths due to disease such as smallpox, it is unlikely that Native Americans only have the genetic markers they have identified [so far], even when their maternal or paternal bloodline does not include a [known] non-Native American.

To receive tribal services, a Native American must be a certified (or enrolled) member of a federally recognized tribal organization. Each tribal government makes its own rules for eligibility of citizens or tribal members. Among tribes, qualification for enrollment may be based upon a required percentage of Native American "blood" (or the "blood quantum") of an individual seeking recognition, or documented descent from an ancestor on the Dawes Rolls or other registers. But, the federal government has its own standards related to who qualifies for services available to certified Native Americans. For instance, federal scholarships for Native Americans require the student both to be enrolled in a federally recognized tribe "and" to be of at least one-quarter Native American descent (equivalent to one grandparent), attested to by a Certificate of Degree of Indian Blood (CDIB) card issued by the federal government.

Some tribes have begun requiring genealogical DNA testing of individuals' applying for membership, but this is usually related to an individual's proving parentage or direct descent from a certified member. Requirements for tribal membership vary widely by tribe. The Cherokee require documented direct genealogical descent from a Native American listed on the early 1906 Dawes Rolls. Tribal rules regarding recognition of members who have heritage from multiple tribes are equally diverse and complex. Federally recognized tribes do not accept genetic-ancestry results as appropriate documentation for enrollment and do not advise applicants to submit such documentation.

Tribal membership conflicts have led to a number of legal disputes, court cases, and the formation of activist groups. One example of this are the Cherokee Freedmen. Today, they include descendants of African Americans once enslaved by the Cherokees, who were granted, by federal treaty, citizenship in the historic Cherokee Nation as freedmen after the Civil War. The modern Cherokee Nation, in the early 1980s, passed a law to require that all members must prove descent from a Cherokee Native American (not Cherokee Freedmen) listed on the Dawes Rolls, resulting in the exclusion of some individuals and families who had been active in Cherokee culture for years.

Since the census of 2000, people may identify as being of more than one race. Since the 1960s, the number of people claiming Native American ancestry has grown significantly and by the 2000 census, the number had more than doubled. Sociologists attribute this dramatic change to "ethnic shifting" or "ethnic shopping"; they believe that it reflects a willingness of people to question their birth identities and adopt new ethnicities which they find more compatible.

The author Jack Hitt writes:

The journalist Mary Annette Pember notes that identifying with Native American culture may be a result of a person's increased interest in genealogy, the romanticization of the lifestyle, and a family tradition of Native American ancestors in the distant past. There are different issues if a person wants to pursue enrollment as a member of a tribe. Different tribes have different requirements for tribal membership; in some cases persons are reluctant to enroll, seeing it as a method of control initiated by the federal government; and there are individuals who are 100% Native American but, because of their mixed tribal heritage, do not qualify to belong to any individual tribe. Pember concludes:

The genetic history of indigenous peoples of the Americas primarily focuses on human Y-chromosome DNA haplogroups and human mitochondrial DNA haplogroups. "Y-DNA" is passed solely along the patrilineal line, from father to son, while "mtDNA" is passed down the matrilineal line, from mother to offspring of both sexes. Neither recombines, and thus Y-DNA and mtDNA change only by chance mutation at each generation with no intermixture between parents' genetic material. Autosomal "atDNA" markers are also used, but differ from mtDNA or Y-DNA in that they overlap significantly. Autosomal DNA is generally used to measure the average continent-of-ancestry genetic admixture in the entire human genome and related isolated populations. Within mtDNA, genetic scientists have found specific nucleotide sequences classified as “Native American markers” because the sequences are understood to have been inherited through the generations of genetic females within populations that first settled the “New World.” There are five primary Native American mtDNA haplogroups in which there are clusters of closely linked markers inherited together. All five haplogroups have been identified by researchers as “prehistoric Native North American samples,” and it is commonly asserted that the majority of living Native Americans possess one of the common five mtDNA haplogroup markers.

The genetic pattern indicates Indigenous Americans experienced two very distinctive genetic episodes; first with the initial-peopling of the Americas, and secondly with European colonization of the Americas. The former is the determinant factor for the number of gene lineages, zygosity mutations and founding haplotypes present in today's Indigenous Amerindian populations.

Human settlement of the New World occurred in stages from the Bering sea coast line, with an initial 15,000 to 20,000-year layover on Beringia for the small founding population. The micro-satellite diversity and distributions of the Y lineage specific to South America indicates that certain Amerindian populations have been isolated since the initial colonization of the region. The Na-Dené, Inuit and Indigenous Alaskan populations exhibit haplogroup Q-M242 (Y-DNA) mutations, however, that are distinct from other indigenous Amerindians, and that have various mtDNA and atDNA mutations. This suggests that the paleo-Indian migrants into the northern extremes of North America and Greenland were descended from a later, independent migrant population.

Genetic analyses of HLA I and HLA II genes as well as HLA-A, -B, and -DRB1 gene frequencies links the Ainu people of northern Japan and southeastern Russia to some Indigenous peoples of the Americas, especially to populations on the Pacific Northwest Coast such as Tlingit. Scientists suggest that the main ancestor of the Ainu and of some Native American groups can be traced back to Paleolithic groups in Southern Siberia.



</doc>
<doc id="21218" url="https://en.wikipedia.org/wiki?curid=21218" title="Nights into Dreams">
Nights into Dreams

Development began after the release of "Sonic & Knuckles" in 1994, although the concept originated during the development of "Sonic the Hedgehog 2" two years prior. Development was led by Sonic Team veterans Yuji Naka, Naoto Ohshima, and Takashi Iizuka. Naka began the project with the main idea revolving around flight, and Ohshima designed the character Nights to resemble an angel that could fly like a bird. Ohshima designed Nights as an androgynous character. The team conducted research on dreaming and REM sleep, and was influenced by the works and theories of psychoanalysts Carl Jung and Sigmund Freud. An analogue controller, the Saturn 3D controller, was designed alongside the game and included with some retail copies.

"Nights into Dreams" received acclaim for its graphics, gameplay, soundtrack, and atmosphere. It has appeared on several lists of the greatest games of all time. An abbreviated Christmas-themed version, "Christmas Nights", was released in December 1996. The game was ported to the PlayStation 2 in 2008 in Japan and a high-definition version was released worldwide for PlayStation 3, Xbox 360, and Windows in 2012. A sequel, "", was released for the Wii in 2007.

"Nights into Dreams" is split into seven levels, referred to as "Dreams". The levels are distributed between the two teenage characters: three are unique to Claris, three to Elliot, and each play through an identical final seventh level, "Twin Seeds". Initially, only Claris' "Spring Valley" and Elliot's "Splash Garden" levels are available, and successful completion of one of these unlocks the next level in that character's path. Previously completed stages may be revisited to improve the player's high scores; a grade between A and F is given to the player upon completion, but a "C" grade (or better) in all the selected character's levels must be achieved to unlock the relevant "Twin Seeds" stage for that character. Points are accumulated depending on how fast the player completes a level, and extra points are awarded when the player flies through rings.

Each level is split up into four "Mares" set in Nightopia and a boss fight which takes place in Nightmare. In each level, players initially control Claris or Elliot, who immediately have their Ideyas (spherical objects that contain emotions) of hope, wisdom, intelligence and purity stolen from them by Wizeman's minions, leaving behind only their Ideya of courage. The goal of each Mare is to recover one of the stolen Ideya by collecting 20 blue chips and delivering them to the cage holding the Ideyas, which overloads and releases the orb it holds. If the player walks around the landscape for too long, they are pursued by a sentient alarm clock which awakens the character and end the level if it comes into contact with the player. The majority of the gameplay centres on flying sequences, which are triggered by walking into the Ideya Palace near the start of each level so that the character merges with the imprisoned Nights. Once the flying sequence is initiated, the time limit begins.

In the flying sections, the player controls Nights' flight along a predetermined route through each Mare, resembling that of a 2D platformer. The player has only a limited period of time available before Nights falls to the ground and transforms back into Claris or Elliot, and each collision with an enemy subtracts five seconds from the time remaining. The player's time is replenished each time they return an Ideya to the Ideya Palace. While flying, Nights can use a "Drill Dash" to travel faster, as well as defeat certain reverie enemies scattered throughout the level. Grabbing onto certain enemies causes Nights to spin around, which launches both Nights and the enemy in the direction the boost was initiated. Various acrobatic manoeuvres can be performed, including the "Paraloop", whereby flying around in a complete circle and connecting the trail of stars left in Nights' wake causes any items within the loop to be attracted towards Nights. The game features a combo system known as "Linking", whereby actions such as collecting items and flying through rings are worth more points when performed in quick succession. Power-ups may be gained by flying through several predetermined rings, indicated by a bonus barrel. The power-ups include a speed boost, point multiplier and an air pocket.

The player receives a grade based on their score at the end of each Mare, and an overall grade for the level after clearing all four Mares. Nights is then transported to Nightmare for a boss fight against one of Wizeman's "Level Two" Nightmarens. Each boss fight has a time limit, and the game ends if the player runs out of time during the battle. Upon winning the boss fight, the player is awarded a score multiplier based on how quickly the boss was defeated, which is then applied to the score earned in the Nightopia section to produce the player's final score for that Dream. The game also features a multiplayer mode, which allows two players to battle each other by using a splitscreen. One player controls Nights, whereas the other controls Reala. The winner is determined by the first player to defeat the other, which is accomplished by hitting or paralooping the other player three times.

The game features an artificial life system known as "A-Life", which involves entities called Nightopians and keeps track of their moods. It is possible to have them mate with other Nightopians, which creates hybrids known as "Superpians". The more the game is played, the more inhabitants appear, and environmental features and aesthetics change. The A-Life system features an evolving music engine, allowing tempo, pitch, and melody to alter depending on the state of Nightopians within the level. The feature runs from the Sega Saturn's internal clock, which alters features in the A-Life system depending on the time.

Every night, all human dreams are played out in Nightopia and Nightmare, the two parts of the dream world. In Nightopia, distinct aspects of dreamers' personalities are represented by luminous coloured spheres known as "Ideya". The evil ruler of Nightmare, Wizeman the Wicked, is stealing this dream energy from sleeping visitors in order to gather power and take control of Nightopia and eventually the real world. To achieve this, he creates five beings called "Nightmaren": jester-like, flight-capable beings, which include Jackle, Clawz, Gulpo, Gillwing and Puffy as well as many minor maren. He also creates two "Level One" Nightmaren: Nights and Reala. However, Nights rebels against Wizeman's plans, and is punished by being imprisoned inside an Ideya palace, a container for dreamers' Ideya.

One day, Elliot Edwards and Claris Sinclair, two teenagers from the city of Twin Seeds, go through failures. Elliot is a basketball player who enjoys a game with his friends. He is challenged by a group of older school students and suffers a humiliating defeat on the court. Claris is a talented singer and her ambition is to perform on stage. She auditions for a part in the events commemorating the centenary of the city of Twin Seeds. Standing in front of the judges, she is overcome by stage fright and does not perform well, which causes her to lose all hopes of getting the role. When they go to sleep that night, both Elliot and Claris suffer nightmares that replay the events. They escape into Nightopia and find that they both possess the rare Red Ideya of Courage, the only type that Wizeman cannot steal.

Once in Nightopia, they discover and release Nights, who tells them about dreams and Wizeman and his plans; the three begin a journey to stop Wizeman and restore peace to Nightopia. When they defeat Wizeman and Reala, peace is returned to Nightopia and the world of Nightmare is suppressed. The next day, back in Twin Seeds, a centenary ceremony begins. Elliot is seen walking through the parade until he has a vision of Nights looking at him through a billboard. Realizing that Claris is performing in a hall, Elliot runs through the crowd and sees Claris on stage in front of a large audience, singing well. The two look at each other, and are transitioned to a spring valley in Nightopia, which leaves ambiguity as to whether what they achieved was real or just a dream.

"Nights" was developed by Sonic Team, the Sega development division that had created the "Sonic the Hedgehog" games for Genesis. The "Nights" concept originated during the development of "Sonic the Hedgehog 2" in 1992, but development did not begin until after the release of "Sonic & Knuckles" in late 1994. Programming began in April 1995 and total development spanned six months. Yuji Naka was lead programmer and producer, while Naoto Ohshima and Takashi Iizuka were director and lead designer, respectively. Naka and Ohshima felt they had spent enough time with the "Sonic" franchise and were eager to work on new concepts. According to Naka, the initial development team consisted of seven people, and grew to 20 as programmers arrived.

"Sonic" creator and project director Ohshima created the character of Nights based on his inspirations from travelling Europe and western Asia. He came to the conclusion that the character should resemble an angel and fly like a bird. Naka originally intended to make "Nights into Dreams" a slow-paced game, but as development progressed the gameplay pace gradually increased, in similar vein to "Sonic" games. The initial concept envisioned the flying character in a rendered 2D sprite art, with side-scrolling features similar to "Sonic the Hedgehog". The team were hesitant to switch the game from 2D to 3D, as Naka was sceptical that appealing characters could be created with polygons, in contrast to traditional pixel sprites, which Sonic Team's designers found "more expressive". According to Izuka, the game design and story took two years to finalise. The game's difficulty was designed with the intent that young and inexperienced players would be able to complete the game, while more experienced players would be compelled by the replay value.

The game was developed using Silicon Graphics workstations for graphical designs and Sega Saturn emulators running on Hewlett-Packard machines for programming. There were problems during early stages of development because of a lack of games to use as reference; the team had to redesign the Spring Valley level numerous times and build "everything from scratch". The team used the Sega Graphics Library operating system, said by many developers to make programming for the Saturn dramatically easier, only sparingly, instead creating the game almost entirely with custom libraries. Because the Sonic Team offices did not include soundproof studios, team members recorded sound effects at night. According to Naka, every phrase in the game has a meaning; for example, "abayo" is Japanese slang for "goodbye". The team felt that the global market would be less resistant to a game featuring full 3D CGI cut scenes than 2D anime. Norihiro Nishiyama, the designer of the in-game movies, felt that the 3D cutscenes were a good method to show the different concepts of dreaming and waking up. Naka said that the movies incorporate realism to make it more difficult for the player to disambiguate the boundary between dreams and reality.

The development took longer than expected because of the team's inexperience with Saturn hardware and uncertainty about using the full 560 megabyte space on the CD-ROM. The team initially thought that the game would consume around 100 megabytes of data, and at one point considered releasing it on two separate discs. Iizuka said that the most difficult part of development was finding a way of handling the "contradiction" of using 2D sidescroller controls in a fully 3D game. Naka limited the game's flying mechanic to "invisible 2D tracks" because early beta testing revealed that the game was too difficult to play in full 3D. The standard Saturn gamepad was found to be insufficient to control Nights in flight, so the team developed the Saturn analog controller to be used with the game. It took about six months to develop, and the team went through many ideas for alternate controllers, including one shaped like a Nights doll.

Iizuka said "Nights" was inspired by anime and Cirque du Soleil's "Mystère" theatrical performance. The team researched dream sequences and REM sleep, including the works of psychoanalysts Carl Jung, Sigmund Freud and Friedrich Holtz. Iizuka studied dreams and theories about them, such as Jung's theories of dream archetypes. Naka said that Nights reflected Jung's analytical "shadow" theory, whereas Claris and Elliot were inspired by Jung's animus and anima.

"Nights into Dreams" was introduced alongside an optional gamepad, the Saturn 3D controller, included with some copies of the game. It features an analogue stick and analogue triggers designed specifically for the game to make movement easier. Sonic Team noted the successful twinning of the Nintendo 64 controller with "Super Mario 64" (1996), and realised that the default Saturn controller was better suited to arcade games than "Nights into Dreams". During development, director Steven Spielberg visited the Sonic Team studio and became the first person outside the team to play the game. Naka asked him to use an experimental version of the Saturn 3D controller, and it was jokingly referred to as the "Spielberg controller" throughout development. 

Because the Nights character was testing very young in focus groups, Sega used a nighttime scene for the cover art to give the game a more mature look. "Nights" was marketed with a budget of $10 million, which included television and print advertisements in the United States. In the US, it was advertised with the slogan "Prepare to fly".

, or Christmas NiGHTS into Dreams..., is a Christmas-themed two-level game of "Nights into Dreams" released in December 1996. Iizuka stated that "Christmas Nights" was created to increase Saturn sales. Development began in July 1996 and took three to four months, according to Naka. Designer Takao Miyoshi recalled working "in the peak of summer ... holed up at the office listening to 'Jingle Bells'".

In Japan, it was included as part of a Christmas Sega Saturn bundle, and distributed free to Saturn owners who covered the shipping cost. Elsewhere it was given away with the purchase of Saturn games such as "Daytona USA Championship Circuit Edition" (1996) or issues of "Sega Saturn Magazine" and "Next Generation Magazine". In the United Kingdom, "Christmas Nights" was not included with the "Sega Saturn Magazine" until December 1997. 

"Christmas Nights" follows Elliot and Claris during the holiday season following their adventures with Nights. Realizing that the Christmas Star is missing from the Twin Seeds Christmas tree, the pair travel to Nightopia to find it, where they reunite with Nights and retrieve the Christmas Star from Gillwing's lair.

"Christmas Nights" contains the full version of Claris' Spring Valley dream level from "Nights into Dreams", playable as both Claris and Elliot. The Saturn's internal clock changes elements according to the date and time: December activates "Christmas Nights" mode, replacing item boxes with Christmas presents, greenery with snow and gumdrops, rings with wreaths, and Ideya captures with Christmas trees; Nightopians wear elf costumes, and the music is replaced with a rendition of "Jingle Bells" and an a cappella version of the "Nights" theme song. During the "Winter Nights" period, the Spring Valley weather changes according to the hour. Other changes apply on New Year's Day; on April Fool's Day, Reala replaces Nights as the playable character.

The game features several unlockable bonuses, such as being able to play the game's soundtrack, observe the status of the A-life system, experiment with the game's music mixer, time attack one Mare, or play as Sega's mascot Sonic the Hedgehog in the minigame "Sonic the Hedgehog: Into Dreams." Sonic may only play through Spring Valley on foot, and must defeat the boss: an inflatable Dr. Robotnik. The music is a remixed version of "Final Fever", the final boss battle music from the Japanese and European version of "Sonic CD" (1993). In the HD version of "Nights", the "Christmas Nights" content is playable after the game has been cleared once.

Sonic Team made a prototype Saturn sequel with the title "Air Nights" for the Saturn, and began development for the Dreamcast. In August 1999, Naka confirmed that a sequel was in development; by December 2000, however, it had been cancelled. Naka expressed reluctance to develop a sequel, but later said he was interested in using "Nights into Dreams" "to reinforce Sega's identity". Aside from a handheld electronic game released by Tiger Electronics and small minigames featured in several Sega games, no sequel was released for a Sega console.

On 1 April 2007, a sequel, "," was announced for the Wii. The game was first previewed on Portuguese publication "Maxi Consolas", after the release of short reveals from the "Official Nintendo Magazine" and "Game Reactor". The sequel is a Wii exclusive, making use of the Wii Remote. The gameplay involves the use of various masks, and features a multiplayer mode for two players in addition to Nintendo Wi-Fi Connection online functions. The game was developed by Sega Studio USA, with Iizuka, one of the designers of the original game, serving as producer. It was released in Japan and the United States in December 2007, and in Europe and Australia on 18 January 2008. In 2010, Iizuka said that he would be interested in making a third "Nights into Dreams" game.

"Nights into Dreams" has an average score of 89 percent at GameRankings, based on an aggregate of nine reviews. In Japan, "Nights into Dreams" was the best-selling Saturn game and the 21st best-selling game of 1996, with 392,383 copies sold. The PS2 version sold 6,828 units in Japan, bringing total Japanese sales to 399,211 units.

The graphics and flight mechanics were the most praised aspects. Tom Guise from "Computer and Video Games" heralded the game's flight system and freedom as captivating and stated that "Nights into Dreams" is the "perfect evolution" of a "Sonic" game. Scary Larry of "GamePro" said flying using the analogue joystick "is a breeze" and that the gameplay is fun, enjoyable, and impressive. He gave it a 4.5 out of 5 for graphics and a 5 out of 5 in every other category (sound, control, and FunFactor). "Entertainment Weekly" said its "graceful acrobatic stunts" offer "a more compelling sensation of soaring than most flight simulators". "Edge" praised the game's analogue controller and called the levels "well-designed and graphically unrivaled", but the reviewer expressed disappointment in the limited level count compared to "Super Mario 64", and suggested that "Nights" seemed to prioritise technical achievements and Saturn selling points over gameplay with as clear a focus as "Sonic". Martin Robinson from Eurogamer opined that the flight mechanics were a "giddy thrill". Colin Ferris from Game Revolution praised the graphics and speed of the game as breathtaking and awe-inspiring, concluding that it offered the best qualities of the fifth-generation machines. "GameFan" praised the combination of "lush graphics, amazing music, and totally unique gameplay". "Next Generation" criticised the speed, saying that the only disappointing aspect was the way "it all rushes by so fast". However, the magazine praised the two-player mode and the innovative method of grading the player once they completed a level. "Electronic Gaming Monthly"s four reviewers were impressed with both the technical aspects and style of the graphics, and said the levels are great fun to explore, though they expressed disappointment that the game was not genuinely 3D and said it did not manage to surpass "Super Mario 64".

Levi Buchanan from IGN believed that the console "was not built to handle "Nights"" due to the game occasionally clipping and warping, though he admitted that the graphics were "pretty darn good". A reviewer from "Mean Machines Sega" praised the game's vibrant colours and detailed textures, and described its animation as being "fluid as water". The reviewer also noted occasional pop-in and glitching during the game. Rad Automatic from the British "Sega Saturn Magazine" praised the visuals and colour scheme as rich in both texture and detail, while suggesting that "Nights into Dreams" is "one of the most captivating games the Saturn has witnessed yet." "Next Generation" similarly commended the game's visuals, stating that they were "beyond a doubt" the most fluid and satisfying for any game on any system. Upon release, the Japanese "Sega Saturn Magazine" opined that the game would have a significant impact on the video game industry, particularly that in the action game genre. The reviewer also stated that the game felt better through the use of the analogue pad, in contrast to the conventional controller, and also praised the light and smooth feeling the analogue pad portrayed during gameplay.

Reviewers also praised the game's soundtrack and audio effects. Paul Davies from "Computer and Video Games" cited the game as having "the best music ever"; in the same review, Tom Guise attributed the music to creating a hypnotically magical atmosphere. Ferris stated that the music and sound effects were that of a dream world, and asserted that they were fitting for a game like "Nights into Dreams". IGN's Buchanan praised the game's soundtrack, stating that each stage's soundtrack is "quite good" and that the sound effects "fit in perfectly with the dream universe".

In "Electronic Gaming Monthly"s "Best of '96" awards, "Nights Into Dreams" was a runner-up for Flying Game of the Year (behind "Pilotwings 64"), Nights was a runner-up for Coolest Mascot (behind Mario), and the Saturn analog controller, which the magazine called the "Nights Controller", won Best Peripheral. The following year "EGM" ranked it the 70th best console video game of all time, describing it as "Unlike anything you've seen before ... a 2.5-D platform game without the platforms."

"Nights into Dreams" has appeared on several best-game-of-all-time lists. In a January 2000 poll by "Computer and Video Games", readers placed the game 15th on their "100 Greatest Games" list, directly behind "Super Mario 64". IGN ranked the game as the 94th best game of all time in their "Top 100 Games" list in 2007, and in 2008, Levi Buchanan ranked it fourth in his list of the top 10 Sega Saturn games. "Next Generation" ranked the game 25th in its list of the "100 Greatest Games of All Time" in their September 1996 issue (i.e. one month before they actually reviewed the game, and roughly two months before it saw release outside Japan). 1UP ranked the game third in its "Top Ten Cult Classics" list. In 2014, GamesRadar listed "Nights into Dreams" as the best Sega Saturn game of all time, stating that the game "tapped into a new kind of platform gameplay for its era". Naka said that the release of "Nights" was when Sonic Team was truly formed as a brand.

Sega released a remake of "Nights into Dreams" for the PlayStation 2 exclusively in Japan on 21 February 2008. It includes 16:9 wide screen support, an illustration gallery and features the ability to play the game in classic Saturn graphics. The game was also featured in a bundle named the Nightopia Dream Pack, which includes a reprint of a picture book that was released in Japan alongside the original Saturn game. A "Nights into Dreams" handheld electronic game was released by Tiger Electronics in 1997, and a port of it was later released for Tiger's unsuccessful R-Zone console.

A high definition remaster of the PlayStation 2 version was released for PlayStation Network on 2 October 2012 and for Xbox Live Arcade on 5 October 2012. A Windows version was released via Steam on 17 December 2012, with online score leaderboards and the option to play with enhanced graphics or with the original Saturn graphics. The HD version also includes "Christmas Nights," but the two-player mode and "Sonic the Hedgehog" level were removed.

Claris and Eliot make a cameo appearance in Sonic Team's "Burning Rangers" (1998), with both Claris and Eliot sending the Rangers emails thanking them for their help. "Nights into Dreams"-themed pinball areas feature in "Sonic Adventure" (1998) and "Sonic Pinball Party" (2003), with soundtrack being featured in the latter game. The PlayStation 2 games "" (2003) and "Sega SuperStars" (2004) both feature minigames based on "Nights into Dreams", in which Nights is controlled using the player's body. Nights is also an unlockable character in "Sonic Riders" (2006) and "" (2008).

A minigame version of "Nights into Dreams" is playable through using the Nintendo GameCube – Game Boy Advance link cable connectivity with "Phantasy Star Online Episode I & II" (2000) and "Billy Hatcher and the Giant Egg" (2003). Following a successful fan campaign by a "Nights into Dreams" fansite, the character Nights was integrated into "Sonic & Sega All-Stars Racing" (2010) as a traffic guard. Nights and Reala also appear as playable characters in "Sega Superstars Tennis" (2008) and "Sonic & All-Stars Racing Transformed" (2012), the latter of which also features a "Nights into Dreams"-themed racetrack. The limited Deadly Six edition of "Sonic Lost World" (2013) features a "Nights into Dreams"-inspired stage, "Nightmare Zone", as downloadable content.

In February 1998, Archie Comics adapted "Nights into Dreams" into a three-issue comic book miniseries to test whether a Nights comic would sell well in North America. The first miniseries was loosely based on the game, with Nights identified as male despite the character's androgynous design. The company later released a second three-issue miniseries, continuing the story of the first, but the series did not gain enough sales to warrant an ongoing series. It was later added to a list of guest franchises featured in Archie Comics' "Worlds Unite" crossover between its "Sonic the Hedgehog" and "Mega Man" comics.

Citations
Bibliography


</doc>
<doc id="21220" url="https://en.wikipedia.org/wiki?curid=21220" title="Negligence per se">
Negligence per se

Negligence "per se" is a doctrine in US law whereby an act is considered negligent because it violates a statute (or regulation). The doctrine is effectively a form of strict liability.

In order to prove negligence "per se", the plaintiff usually must show that:

In some jurisdictions, negligence "per se" creates merely a rebuttable presumption of negligence.

A typical example is one in which a contractor violates a building code when constructing a house. The house then collapses, injuring somebody. The violation of the building code establishes negligence "per se" and the contractor will be found liable, so long as the contractor's breach of the code was the cause (proximate cause and actual cause) of the injury.

A famous early case in negligence "per se" is "Gorris v. Scott" (1874), a Court of Exchequer case that established that the harm in question must be of the kind that the statute was intended to prevent. "Gorris" involved a shipment of sheep that was washed overboard but would not have been washed overboard had the shipowner complied with the regulations established pursuant to the Contagious Diseases (Animals) Act 1869, which required that livestock be transported in pens to segregate potentially-infected animal populations from uninfected ones. Chief Baron Fitzroy Kelly held that as the statute was intended to prevent the spread of disease, rather than the loss of livestock in transit, the plaintiff could not claim negligence "per se".

A subsequent New York Court of Appeals case, "Martin v. Herzog" (1920), penned by Judge Benjamin N. Cardozo, first presented the notion that negligence "per se" could be absolute evidence of negligence in certain cases.

Negligence "per se" involves the concept of strict liability. Within the law of negligence there has been a move away from strict liability (as typified by "Re Polemis") to a standard of reasonable care (as seen in "Donoghue v Stevenson", "The Wagon Mound (No. 1)", and "Hughes v Lord Advocate"). This is true not just for breach of the common law, but also for breach of statutory duty. The criminal law case of "Sweet v Parsley" (which required "mens rea" to be read into a criminal statue) follows this trend. In this light, "negligence "per se"" may be criticised as running counter to the general tendency.




</doc>
<doc id="21221" url="https://en.wikipedia.org/wiki?curid=21221" title="Neuromyotonia">
Neuromyotonia

Neuromyotonia (NMT) is a form of peripheral nerve hyperexcitability that causes spontaneous muscular activity resulting from repetitive motor unit action potentials of peripheral origin. NMT along with Morvan's syndrome are the most severe types in the Peripheral Nerve Hyperexciteability spectrum. Example of two more common and less severe syndromes in the spectrum are Cramp Fasciculation Syndrome and Benign Fasciculation Syndrome. NMT can have both hereditary and acquired (non- inherited) forms. The prevalence of NMT is unknown.

NMT is a diverse disorder. As a result of muscular hyperactivity, patients may present with muscle cramps, stiffness, myotonia-like symptoms (slow relaxation), associated walking difficulties, hyperhidrosis (excessive sweating), myokymia (quivering of a muscle), fasciculations (muscle twitching), fatigue, exercise intolerance, myoclonic jerks and other related symptoms. The symptoms (especially the stiffness and fasciculations) are most prominent in the calves, legs, trunk, and sometimes the face and neck, but can also affect other body parts. NMT symptoms may fluctuate in severity and frequency. Symptoms range from mere inconvenience to debilitating. At least a third of people also experience sensory symptoms.

The three causes of NMT are:

The acquired form is the most common, accounting for up to 80 percent of all cases and is suspected to be autoimmune-mediated, which is usually caused by antibodies against the neuromuscular junction.

The exact cause is unknown. However, autoreactive antibodies can be detected in a variety of peripheral (e.g. myasthenia gravis, Lambert-Eaton myasthenic syndrome) and central nervous system (e.g. paraneoplastic cerebellar degeneration, paraneoplastic limbic encephalitis) disorders. Their causative role has been established in some of these diseases but not all. Neuromyotonia is considered to be one of these with accumulating evidence for autoimmune origin over the last few years. Autoimmune neuromyotonia is typically caused by antibodies that bind to potassium channels on the motor nerve resulting in continuous/hyper-excitability. Onset is typically seen between the ages of 15–60, with most experiencing symptoms before the age of 40. Some neuromyotonia cases do not only improve after plasma exchange but they may also have antibodies in their serum samples against voltage-gated potassium channels. Moreover, these antibodies have been demonstrated to reduce potassium channel function in neuronal cell lines.

Diagnosis is clinical and initially consists of ruling out more common conditions, disorders, and diseases, and usually begins at the general practitioner level. A doctor may conduct a basic neurological exam, including coordination, strength, reflexes, sensation, etc. A doctor may also run a series of tests that include blood work and MRIs.

From there, a patient is likely to be referred to a neurologist or a neuromuscular specialist. The neurologist or specialist may run a series of more specialized tests, including needle electromyography EMG/ and nerve conduction studies (NCS) (these are the most important tests), chest CT (to rule out paraneoplastic) and specific blood work looking for voltage-gated potassium channel antibodies, acetylcholine receptor antibody, and serum immunofixation, TSH, ANA ESR, EEG etc. Neuromyotonia is characterized electromyographically by doublet, triplet or multiplet single unit discharges that have a high, irregular intraburst frequency. Fibrillation potentials and fasciculations are often also present with electromyography.

Because the condition is so rare, it can often be years before a correct diagnosis is made.

NMT is not fatal and many of the symptoms can be controlled. However, because NMT mimics some symptoms of motor neuron disease (ALS) and other more severe diseases, which may be fatal, there can often be significant anxiety until a diagnosis is made. In some rare cases, acquired neuromyotonia has been misdiagnosed as amyotrophic lateral sclerosis (ALS) particularly if fasciculations may be evident in the absence of other clinical features of ALS. However, fasciculations are rarely the first sign of ALS as the hallmark sign is weakness. Similarly, multiple sclerosis has been the initial misdiagnosis in some NMT patients. In order to get an accurate diagnosis see a trained neuromuscular specialist.

People diagnosed with Benign Fasciculation Syndrome or Enhanced Physiological Tremor may experience similar symptoms as NMT, although it is unclear today whether BFS or EPT are weak forms of NMT.

There are three main types of NMT:

Neuromyotonia is a type of peripheral nerve hyperexcitability. Peripheral nerve hyperexcitability is an umbrella diagnosis that includes (in order of severity of symptoms from least severe to most severe) benign fasciculation syndrome, cramp fasciculation syndrome, neuromyotonia and morvan's syndrome. Some doctors will only give the diagnosis of peripheral nerve hyperexcitability as the differences between the three are largely a matter of the severity of the symptoms and can be subjective. However, some objective EMG criteria have been established to help distinguish between the three.

Moreover, the generic use of the term "peripheral nerve hyperexcitability syndromes" to describe the aforementioned conditions is recommended and endorsed by several prominent researchers and practitioners in the field.

There is no known cure for neuromyotonia, but the condition is treatable. Anticonvulsants, including phenytoin and carbamazepine, usually provide significant relief from the stiffness, muscle spasms, and pain associated with neuromyotonia. Plasma exchange and IVIg treatment may provide short-term relief for patients with some forms of the acquired disorder. It is speculated that the plasma exchange causes an interference with the function of the voltage-dependent potassium channels, one of the underlying issues of hyper-excitability in autoimmune neuromyotonia. Botox injections also provide short-term relief. Immunosuppressants such as Prednisone may provide long term relief for patients with some forms of the acquired disorder.

The long-term prognosis is uncertain, and has mostly to do with the underlying cause; i.e. autoimmune, paraneoplastic, etc. However, in recent years increased understanding of the basic mechanisms of NMT and autoimmunity has led to the development of novel treatment strategies. NMT disorders are now amenable to treatment and their prognoses are good. Many patients respond well to treatment, which usually provide significant relief of symptoms. Some cases of spontaneous remission have been noted, including Isaac's original two patients when followed up 14 years later.

While NMT symptoms may fluctuate, they generally don't deteriorate into anything more serious, and with the correct treatment the symptoms are manageable.

A very small proportion of cases with NMT may develop central nervous system findings in their clinical course, causing a disorder called Morvan's syndrome, and they may also have antibodies against potassium channels in their serum samples. Sleep disorder is only one of a variety of clinical conditions observed in Morvan's syndrome cases ranging from confusion and memory loss to hallucinations and delusions. However, this is a separate disorder.

Some studies have linked NMT with certain types of cancers, mostly lung and thymus, suggesting that NMT may be paraneoplastic in some cases. In these cases, the underlying cancer will determine prognosis. However, most examples of NMT are autoimmune and not associated with cancer.


</doc>
<doc id="21224" url="https://en.wikipedia.org/wiki?curid=21224" title="Napoleon (disambiguation)">
Napoleon (disambiguation)

Napoleon (1769–1821) also known as Napoleon Bonaparte or Napoleon I, was a French military leader and emperor.

Napoleon, Napoléon, or Napoleón, or Napoleone may also refer to:










</doc>
<doc id="21226" url="https://en.wikipedia.org/wiki?curid=21226" title="Neurology">
Neurology

Neurology (from , "string, nerve" and the suffix -logia, "study of") is a branch of medicine dealing with disorders of the nervous system. Neurology deals with the diagnosis and treatment of all categories of conditions and disease involving the central and peripheral nervous systems (and their subdivisions, the autonomic and somatic nervous systems), including their coverings, blood vessels, and all effector tissue, such as muscle. Neurological practice relies heavily on the field of neuroscience, the scientific study of the nervous system. 

A neurologist is a physician specializing in neurology and trained to investigate, or diagnose and treat neurological disorders. Neurologists may also be involved in clinical research, clinical trials, and basic or translational research. While neurology is a nonsurgical specialty, its corresponding surgical specialty is neurosurgery.

Significant overlap occurs between the fields of neurology and psychiatry, with the boundary between the two disciplines and the conditions they treat being somewhat nebulous.

Many neurological disorders have been described as listed. These can affect the central nervous system (brain and spinal cord), the peripheral nervous system, the autonomic nervous system, and the muscular system.

The academic discipline began between the 15th and 16th centuries with the work and research of many neurologists such as Thomas Willis, Robert Whytt, Matthew Baillie, Charles Bell, Moritz Heinrich Romberg, Duchenne de Boulogne, William A. Hammond, Jean-Martin Charcot, and John Hughlings Jackson.

 Many neurologists also have additional training or interest in one area of neurology, such as stroke, epilepsy, neuromuscular, sleep medicine, pain management, or movement disorders.

In the United States and Canada, neurologists are physicians having completed postgraduate training in neurology after graduation from medical school. Neurologists complete, on average, about 8 years of medical college education and clinical training, which includes obtaining a four-year undergraduate degree, a medical degree (DO or MD), which comprises an additional four years of study, then completing one year of basic clinical training and four years of residency. The four-year residency consists of one year of internal medicine internship training followed by three years of training in neurology.

Some neurologists receive additional subspecialty training focusing on a particular area of the fields. These training programs are called fellowships, and are one to two years in duration. Subspecialties include brain injury medicine, clinical neurophysiology, epilepsy, hospice and palliative medicine, neurodevelopmental disabilities, neuromuscular medicine, pain medicine, sleep medicine, neurocritical care, vascular neurology (stroke), behavioral neurology, child neurology, headache, multiple sclerosis, neuroimaging, neurorehabilitation.

In Germany, a compulsory year of psychiatry must be done to complete a residency of neurology.

In the United Kingdom and Ireland, neurology is a subspecialty of general (internal) medicine. After five years of medical school and two years as a Foundation Trainee, an aspiring neurologist must pass the examination for Membership of the Royal College of Physicians (or the Irish equivalent) and complete two years of core medical training before entering specialist training in neurology. Up to the 1960s, some intending to become neurologists would also spend two years working in psychiatric units before obtaining a diploma in psychological medicine. However, that was uncommon and, now that the MRCPsych takes three years to obtain, would no longer be practical. A period of research is essential, and obtaining a higher degree aids career progression. Many found it was eased after an attachment to the Institute of Neurology at Queen Square, London. Some neurologists enter the field of rehabilitation medicine (known as physiatry in the US) to specialise in neurological rehabilitation, which may include stroke medicine, as well as brain injuries.

During a neurological examination, the neurologist reviews the patient's health history with special attention to the current condition. The patient then takes a neurological exam. Typically, the exam tests mental status, function of the cranial nerves (including vision), strength, coordination, reflexes, and sensation. This information helps the neurologist determine whether the problem exists in the nervous system and the clinical localization. Localization of the pathology is the key process by which neurologists develop their differential diagnosis. Further tests may be needed to confirm a diagnosis and ultimately guide therapy and appropriate management.

Neurologists examine patients who are referred to them by other physicians in both the inpatient and outpatient settings. Neurologists begin their interactions with patients by taking a comprehensive medical history, and then performing a physical examination focusing on evaluating the nervous system. Components of the neurological examination include assessment of the patient's cognitive function, cranial nerves, motor strength, sensation, reflexes, coordination, and gait.

In some instances, neurologists may order additional diagnostic tests as part of the evaluation. Commonly employed tests in neurology include imaging studies such as computed axial tomography (CAT) scans, magnetic resonance imaging (MRI), and ultrasound of major blood vessels of the head and neck. Neurophysiologic studies, including electroencephalography (EEG), needle electromyography (EMG), nerve conduction studies (NCSs) and evoked potentials are also commonly ordered. Neurologists frequently perform lumbar punctures to assess characteristics of a patient's cerebrospinal fluid. Advances in genetic testing have made genetic testing an important tool in the classification of inherited neuromuscular disease and diagnosis of many other neurogenetic diseases. The role of genetic influences on the development of acquired neurologic diseases is an active area of research.

Some of the commonly encountered conditions treated by neurologists include headaches, radiculopathy, neuropathy, stroke, dementia, seizures and epilepsy, Alzheimer's disease, attention deficit/hyperactivity disorder, Parkinson's disease, Tourette's syndrome, multiple sclerosis, head trauma, sleep disorders, neuromuscular diseases, and various infections and tumors of the nervous system. Neurologists are also asked to evaluate unresponsive patients on life support to confirm brain death.

Treatment options vary depending on the neurological problem. They can include referring the patient to a physiotherapist, prescribing medications, or recommending a surgical procedure.

Some neurologists specialize in certain parts of the nervous system or in specific procedures. For example, clinical neurophysiologists specialize in the use of EEG and intraoperative monitoring to diagnose certain neurological disorders. Other neurologists specialize in the use of electrodiagnostic medicine studies – needle EMG and NCSs. In the US, physicians do not typically specialize in all the aspects of clinical neurophysiology – i.e. sleep, EEG, EMG, and NCSs. The American Board of Clinical Neurophysiology certifies US physicians in general clinical neurophysiology, epilepsy, and intraoperative monitoring. The American Board of Electrodiagnostic Medicine certifies US physicians in electrodiagnostic medicine and certifies technologists in nerve-conduction studies. Sleep medicine is a subspecialty field in the US under several medical specialties including anesthesiology, internal medicine, family medicine, and neurology. Neurosurgery is a distinct specialty that involves a different training path, and emphasizes the surgical treatment of neurological disorders.

Also, many nonmedical doctors, those with doctoral degrees(usually PhDs) in subjects such as biology and chemistry, study and research the nervous system. Working in laboratories in universities, hospitals, and private companies, these neuroscientists perform clinical and laboratory experiments and tests to learn more about the nervous system and find cures or new treatments for diseases and disorders.

A great deal of overlap occurs between neuroscience and neurology. Many neurologists work in academic training hospitals, where they conduct research as neuroscientists in addition to treating patients and teaching neurology to medical students.

Neurologists are responsible for the diagnosis, treatment, and management of all the conditions mentioned above. When surgical or endovascular intervention is required, the neurologist may refer the patient to a neurosurgeon or an interventional neuroradiologist. In some countries, additional legal responsibilities of a neurologist may include making a finding of brain death when it is suspected that a patient has died. Neurologists frequently care for people with hereditary (genetic) diseases when the major manifestations are neurological, as is frequently the case. Lumbar punctures are frequently performed by neurologists. Some neurologists may develop an interest in particular subfields, such as stroke, dementia, movement disorders, neurointensive care, headaches, epilepsy, sleep disorders, chronic pain management, multiple sclerosis, or neuromuscular diseases.

Some overlap also occurs with other specialties, varying from country to country and even within a local geographic area. Acute head trauma is most often treated by neurosurgeons, whereas sequelae of head trauma may be treated by neurologists or specialists in rehabilitation medicine. Although stroke cases have been traditionally managed by internal medicine or hospitalists, the emergence of vascular neurology and interventional neuroradiology has created a demand for stroke specialists. The establishment of Joint Commission-certified stroke centers has increased the role of neurologists in stroke care in many primary, as well as tertiary, hospitals. Some cases of nervous system infectious diseases are treated by infectious disease specialists. Most cases of headache are diagnosed and treated primarily by general practitioners, at least the less severe cases. Likewise, most cases of sciatica are treated by general practitioners, though they may be referred to neurologists or surgeons (neurosurgeons or orthopedic surgeons). Sleep disorders are also treated by pulmonologists and psychiatrists. Cerebral palsy is initially treated by pediatricians, but care may be transferred to an adult neurologist after the patient reaches a certain age. Physical medicine and rehabilitation physicians also in the US diagnosis and treat patients with neuromuscular diseases through the use of electrodiagnostic studies (needle EMG and nerve-conduction studies) and other diagnostic tools. In the United Kingdom and other countries, many of the conditions encountered by older patients such as movement disorders, including Parkinson's disease, stroke, dementia, or gait disorders, are managed predominantly by specialists in geriatric medicine.

Clinical neuropsychologists are often called upon to evaluate brain-behavior relationships for the purpose of assisting with differential diagnosis, planning rehabilitation strategies, documenting cognitive strengths and weaknesses, and measuring change over time (e.g., for identifying abnormal aging or tracking the progression of a dementia)

In some countries, e.g. US and Germany, neurologists may subspecialize in clinical neurophysiology, the field responsible for EEG and intraoperative monitoring, or in electrodiagnostic medicine nerve conduction studies, EMG, and evoked potentials. In other countries, this is an autonomous specialty (e.g., United Kingdom, Sweden, Spain).

Although mental illnesses are believed by many to be neurological disorders affecting the central nervous system, traditionally they are classified separately, and treated by psychiatrists. In a 2002 review article in the "American Journal of Psychiatry", Professor Joseph B. Martin, Dean of Harvard Medical School and a neurologist by training, wrote, "the separation of the two categories is arbitrary, often influenced by beliefs rather than proven scientific observations. And the fact that the brain and mind are one makes the separation artificial anyway".

Neurological disorders often have psychiatric manifestations, such as poststroke depression, depression and dementia associated with Parkinson's disease, mood and cognitive dysfunctions in Alzheimer's disease, and Huntington disease, to name a few. Hence, the sharp distinction between neurology and psychiatry is not always on a biological basis. The dominance of psychoanalytic theory in the first three-quarters of the 20th century has since then been largely replaced by a focus on pharmacology. Despite the shift to a medical model, brain science has not advanced to a point where scientists or clinicians can point to readily discernible pathologic lesions or genetic abnormalities that in and of themselves serve as reliable or predictive biomarkers of a given mental disorder.

The emerging field of neurological enhancement highlights the potential of therapies to improve such things as workplace efficacy, attention in school, and overall happiness in personal lives. However, this field has also given rise to questions about neuroethics and the psychopharmacology of lifestyle drugs can have negative and positive effects on neurology because different types 
of drugs can depend on people and their lives [Cheyanne l.dorsey]



</doc>
<doc id="21227" url="https://en.wikipedia.org/wiki?curid=21227" title="Nu">
Nu

Nu or NU may refer to:














</doc>
<doc id="21228" url="https://en.wikipedia.org/wiki?curid=21228" title="Niue">
Niue

Niue ( or ; ) is an island country in the South Pacific Ocean, northeast of New Zealand. Niue's land area is about and its population, predominantly Polynesian, was about 1,600 in 2016. Niue is located 2,400 kilometres northeast of New Zealand in a triangle between Tonga, Samoa, and the Cook Islands. It is 604 kilometers northeast of Tonga. The island is commonly referred to as "The Rock", which comes from the traditional name "Rock of Polynesia". Niue is one of the world's largest coral islands. The terrain of the island has two noticeable levels. The higher level is made up of a limestone cliff running along the coast, with a plateau in the centre of the island reaching approximately 60 metres (200 feet) high above sea level. The lower level is a coastal terrace approximately 0.5 km (0.3 miles) wide and about 25–27 metres (80–90 feet) high, which slopes down and meets the sea in small cliffs. A coral reef surrounds the island, with the only major break in the reef being in the central western coast, close to the capital, Alofi.

Niue is a self-governing state in free association with New Zealand, and New Zealand conducts most diplomatic relations on its behalf. Niueans are citizens of New Zealand, and Queen Elizabeth II is head of state in her capacity as Queen of New Zealand. Between 90% and 95% of Niuean people live in New Zealand, along with about 70% of the speakers of the Niuean language. Niue is a bilingual country, with 30% of the population speaking both Niuean and English. The percentage of monolingual English-speaking people is only 11%, while 46% are monolingual Niuean speakers.

Niue is not a member of the United Nations (UN), but UN organisations have accepted its status as a freely-associated state as equivalent to independence for the purposes of international law. As such, Niue is a member of some UN specialised agencies (such as UNESCO, and the WHO), and is invited, alongside the other non-UN member state, the Cook Islands, to attend United Nations conferences open to "all states". Niue has been a member of the Pacific Community since 1980.

Niue is subdivided into 14 "villages" (municipalities). Each village has a village council that elects its chairperson. The villages are at the same time electoral districts; each village sends an assemblyperson to the Parliament of Niue. A small and democratic nation, Niueans hold legislative elections every three years.

The "Niue Integrated Strategic Plan" (NISP), adopted in 2003, is the national development plan, setting national priorities for development in areas such as financial sustainability. Since the late 20th century Niue has become a leader in green growth; the European Union is helping the nation convert to renewable energy. In January 2004, Niue was hit by Cyclone Heta, which caused extensive damage to the island, including wiping out most of South Alofi. The disaster set the island back about two years from its planned timeline to implement the NISP since national efforts concentrated on recovery.

Polynesians from Samoa settled Niue around 900 AD. Further settlers arrived from Tonga in the 16th century.

Until the beginning of the 18th century, Niue appears to have had no national government or national leader; chiefs and heads of families exercised authority over segments of the population. A succession of "patu-iki" (kings) ruled, beginning with Puni-mata. Tui-toga, who reigned from 1875 to 1887, was the first Christian king.

The first Europeans to sight Niue sailed under Captain James Cook in 1774. Cook made three attempts to land, but the inhabitants refused to grant permission to do so. He named the island "Savage Island" because, as legend has it, the natives who "greeted" him were painted in what appeared to be blood. The substance on their teeth was hulahula, a native red fe'i banana. For the next couple of centuries, Niue was known as Savage Island until its original name, "Niue", which translates as "behold the coconut", regained use.

Whaling vessels were some of the most regular visitors to the island in the nineteenth century. The first on record was the "Fanny" in February 1824. The last known whaler to visit was the "Albatross" in November 1899.

The next notable European visitors represented the London Missionary Society; they arrived on the "Messenger of Peace". After many years of trying to land a European missionary, they abducted a Niuean named Nukai Peniamina and trained him as a pastor at the Malua Theological College in Samoa. Peniamina returned in 1846 on the "John Williams" as a missionary with the help of Toimata Fakafitifonua. He was finally allowed to land in Uluvehi Mutalau after a number of attempts in other villages had failed. The chiefs of Mutalau village allowed him to land and assigned over 60 warriors to protect him day and night at the fort in Fupiu.
In July 1849 Captain John Erskine visited the island in HMS "Havannah".

Christianity was first taught to the Mutalau people before it spread to all the villages. Originally other major villages opposed the introduction of Christianity and had sought to kill Peniamina. The people from the village of Hakupu, although the last village to receive Christianity, came and asked for a "word of God"; hence, their village was renamed "Ha Kupu Atua" meaning "any word of God", or "Hakupu" for short.

In 1889 the chiefs and rulers of Niue, in a letter to Queen Victoria, asked her "to stretch out towards us your mighty hand, that Niue may hide herself in it and be safe". After expressing anxiety lest some other nation should take possession of the island, the letter continued: "We leave it with you to do as seems best to you. If you send the flag of Britain that is well; or if you send a Commissioner to reside among us, that will be well". The British did not initially take up the offer. In 1900 a petition by the Cook Islanders asking for annexation included Niue "if possible". In a document dated 19 October 1900, the "King" and Chiefs of Niue consented to "Queen Victoria taking possession of this island". A despatch to the Secretary of State for the Colonies from the Governor of New Zealand referred to the views expressed by the Chiefs in favour of "annexation" and to this document as "the deed of cession". A British Protectorate was declared, but it remained short-lived. Niue was brought within the boundaries of New Zealand on 11 June 1901 by the same Order and Proclamation as the Cook Islands. The Order limited the islands to which it related by reference to an area in the Pacific described by co-ordinates, and Niue, at 19.02 S., 169.55 W, lies within that area.

The New Zealand Parliament restored self-government in Niue with the 1974 constitution, following a referendum in 1974 in which Niueans had three options: independence, self-government or continuation as a New Zealand territory. The majority selected self-government, and Niue's written constitution
was promulgated as supreme law. Robert Rex, ethnically part European, part native, was elected by the Niue Assembly as the first premier, a position he held until his death 18 years later. Rex became the first Niuean to receive a knighthood – in 1984.

In January 2004 Cyclone Heta hit Niue, killing two people and causing extensive damage to the entire island, including wiping out most of the south of the capital, Alofi.

On March 7, 2020, the International Dark Sky Association announced that Niue had become the first Dark Sky Preserve Nation.

The Niue Constitution Act of 1974 vests executive authority in Her Majesty the Queen in Right of New Zealand and in the Governor-General of New Zealand. The Constitution specifies that everyday practice involves the exercise of sovereignty by Cabinet, composed of the Premier (currently Dalton Tagelagi since 11 June 2020) and of three other ministers. The Premier and ministers are members of the Niue Legislative Assembly, the nation's parliament.

The Assembly consists of 20 members, 14 of them elected by the electors of each village constituency, and six by all registered voters in all constituencies. Electors must be New Zealand citizens, resident for at least three months, and candidates must be electors and resident for 12 months. Everyone born in Niue must register on the electoral roll.

Niue has no political parties; all Assembly members are independents. The only Niuean political party to have ever existed, the Niue People's Party (1987–2003), won once (in 2002) before disbanding the following year.

The Legislative Assembly elects a Speaker as its first official in the first sitting of the Assembly following an election. The speaker calls for nominations for premier; the candidate with the most votes from the 20 members is elected. The premier selects three other members to form a Cabinet, the executive arm of government. General elections take place every three years, most recently on 30 May 2020.

The judiciary, independent of the executive and the legislature, includes a High Court and a Court of Appeal, with appeals to the Judicial Committee of the Privy Council in London.

Niue has operated as a self-governing state in free association with New Zealand since 3 September 1974, when the people endorsed the Constitution in a plebiscite. Niue is fully responsible for its internal affairs. Niue's position concerning its external relations is less clear-cut. Section 6 of the Niue Constitution Act provides that: "Nothing in this Act or in the Constitution shall affect the responsibilities of Her Majesty the Queen in right of New Zealand for the external affairs and defence of Niue." Section 8 elaborates but still leaves the position unclear: Effect shall be given to the provisions of sections 6 and 7 [concerning external affairs and defence and economic and administrative assistance respectively] of this Act, and to any other aspect of the relationship between New Zealand and Niue which may from time to time call for positive co-operation between New Zealand and Niue after consultation between the Prime Minister of New Zealand and the Premier of Niue, and in accordance with the policies of their respective Governments; and, if it appears desirable that any provision be made in the law of Niue to carry out these policies, that provision may be made in the manner prescribed in the Constitution, but not otherwise."

Niue has a representative mission (High Commission) in Wellington, New Zealand.
It is a member of the Pacific Islands Forum and of a number of regional and international agencies. It is not a member of the United Nations, but is a state party to the United Nations Convention on the Law of the Sea, the United Nations Framework Convention on Climate Change, the Ottawa Treaty and the Treaty of Rarotonga. The country became a member state of UNESCO on 26 October 1993.

Traditionally, Niue's foreign relations and defence have been regarded as the responsibility of New Zealand. However, in recent years Niue has begun to follow its own foreign relations, independent of New Zealand, in some spheres. It established diplomatic relations with the People's Republic of China on 12 December 2007. The joint communique signed by Niue and China differs in its treatment of the Taiwan question from that agreed by New Zealand and China. New Zealand "acknowledged" China's position on Taiwan but has never expressly agreed with it, but Niue "recognises that there is only one China in the world, the Government of the People's Republic of China is the sole legal government representing the whole of China and Taiwan is an inalienable part of the territory of China." Niue established diplomatic relations with India on 30 August 2012. On 10 June 2014 the Government of Niue announced that Niue had established diplomatic relations with Turkey. The Honourable Minister of Infrastructure Dalton Tagelagi formalised the agreement at the Pacific Small Island States Foreign Ministers meeting in Istanbul, Turkey. The Memorandum of Understanding with Turkey is part of increasing Niue's foreign relationship with countries including the People's Republic of China, India, Australia, Thailand, Samoa, Cook Islands and Singapore.

People of Niue have fought as part of the New Zealand military. During World War I (1914-1918), Niue sent about 200 soldiers as part of the New Zealand (Māori) Pioneer Battalion in the New Zealand forces.

Niue is not a republic, but for a number of years the ISO list of country names (ISO-3166-1) listed its full name as "the Republic of Niue" . In its newsletter of 14 July 2011, the ISO acknowledged that this was a mistake and the words "the Republic of" were deleted from the ISO list of country names.

Niue is a raised coral atoll in the southern Pacific Ocean, east of Tonga. There are three outlying coral reefs within the Exclusive Economic Zone, with no land area:

Besides these, Albert Meyer Reef, (almost long and wide, least depth , southwest) is not officially claimed by Niue, and the existence of Haymet Rocks ( east-southeast) is in doubt.

Niue is one of the world's largest coral islands. The terrain consists of steep limestone cliffs along the coast with a central plateau rising to about above sea level. A coral reef surrounds the island, with the only major break in the reef being in the central western coast, close to the capital, Alofi. A number of limestone caves occur near the coast.

The island is roughly oval in shape (with a diameter of about ), with two large bays indenting the western coast, Alofi Bay in the centre and Avatele Bay in the south. Between these is the promontory of Halagigie Point. A small peninsula, TePā Point (Blowhole Point), is close to the settlement of Avatele in the southwest. Most of the population resides close to the west coast, around the capital, and in the northwest.

Some of the soils are geochemically very unusual. They are extremely weathered tropical soils, with high levels of iron and aluminium oxides (oxisol) and mercury, and they contain high levels of natural radioactivity. There is almost no uranium, but the radionucleides Th-230 and Pa-231 head the decay chains. This is the same distribution of elements as found naturally on very deep seabeds, but the geochemical evidence suggests that the origin of these elements is extreme weathering of coral and brief sea submergence 120,000 years ago. Endothermal upwelling, by which mild volcanic heat draws deep seawater up through the porous coral, almost certainly contributes.

No adverse health effects from the radioactivity or the other trace elements have been demonstrated, and calculations show that the level of radioactivity is probably much too low to be detected in the population. These unusual soils are very rich in phosphate, but it is not accessible to plants, being in the very insoluble form of iron phosphate, or crandallite. It is thought that similar radioactive soils may exist on Lifou and Mare near New Caledonia, and Rennell in the Solomon Islands, but no other locations are known.

According to the World Health Organization, residents are evidently very susceptible to skin cancer. In 2002 Niue reported skin cancer deaths at a rate of 2,482 per 100,000 people – far higher than any other country.

Niue is separated from New Zealand by the International Date Line. The time difference is 23 hours during the Southern Hemisphere winter and 24 hours when New Zealand uses Daylight Saving Time.

The island has a tropical climate, with most rainfall occurring between November and April.

A leader in green growth, Niue is also focusing on solar power provision, with help from the European Union. However, Niue currently deals with one of the highest rates of greenhouse gas production per capita in the world. Niue aims to become 80% renewable by 2025. The Niue Island Organic Farmers Association is currently paving way to a Multilateral Environmental Agreement (MEA) committed to making Niue the world's first fully organic nation by 2020.

In July 2009 a solar panel system was installed, injecting about 50 kW into the Niue national power grid. This is nominally 6% of the average 833 kW electricity production. The solar panels are at Niue High School (20 kW), Niue Power Corporation office (1.7 kW) and the Niue Foou Hospital (30 kW). The EU-funded grid-connected photovoltaic systems are supplied under the REP-5 programme and were installed recently by the Niue Power Corporation on the roofs of the high school and the power station office and on ground-mounted support structures in front of the hospital. They will be monitored and maintained by the NPC. In 2014 two additional solar power installations were added to the Niue national power grid, one funded under PALM5 of Japan is located outside of the Tuila power station – so far only this has battery storage, the other under European Union funding is located opposite the Niue International Airport Terminal.

Niue's economy is small. Its gross domestic product (GDP) was NZ$17 million in 2003, or US$10 million at purchasing power parity. Niue's GDP has increased to US$24.9 million in 2016. Niue uses the New Zealand dollar.

The Niue Integrated Strategic Plan (NISP) is the national development plan, setting national priorities for development. Cyclone Heta set the island back about two years from its planned timeline to implement the NISP, since national efforts concentrated on recovery efforts. In 2008, Niue had yet to fully recover. After Heta, the government made a major commitment to rehabilitate and develop the private sector. The government allocated $1 million for the private sector, and spent it on helping businesses devastated by the cyclone, and on construction of the Fonuakula Industrial Park. This industrial park is now completed and some businesses are already operating from there. The Fonuakula Industrial Park is managed by the Niue Chamber of Commerce, a not-for-profit organisation providing advisory services to businesses.

Joint ventures

The government and the Reef Group from New Zealand started two joint ventures in 2003 and 2004 to develop fisheries and a 120-hectare noni juice operation. Noni fruit comes from "Morinda citrifolia", a small tree with edible fruit. Niue Fish Processors Ltd (NFP) is a joint venture company processing fresh fish, mainly tuna (yellowfin, big eye and albacore), for export to overseas markets. NFP operates out of a state-of-the-art fish plant in Amanau Alofi South, completed and opened in October 2004.

Trade

Niue is negotiating free trade agreements with other Pacific countries, PICTA Trade in Services (PICTA TIS), Economic Partnership Agreements with the European Union, and PACERPlus with Australia and New Zealand. The Office of the Chief Trade Adviser (OCTA) has been set up to assist Niue and other Pacific countries in the negotiation of the PACERPlus.

Mining

In August 2005, an Australian mining company, Yamarna Goldfields, suggested that Niue might have the world's largest deposit of uranium. By early September these hopes were seen as overoptimistic, and in late October the company cancelled its plans, announcing that exploratory drilling had identified nothing of commercial value. The Australian Securities and Investments Commission filed charges in January 2007 against two directors of the company, now called Mining Projects Group Ltd, alleging that their conduct had been deceptive and that they engaged in insider trading. This case was settled out of court in July 2008, both sides withdrawing their claims.

Remittances from expatriates were a major source of foreign exchange in the 1970s and early 1980s. Continuous migration to New Zealand has shifted most members of nuclear and extended families there, removing the need to send remittances back home. In the late 1990s, PFTAC conducted studies on the balance of payments, which confirmed that Niueans are receiving few remittances but are sending more money overseas.

Foreign aid

Foreign aid has been Niue's principal source of income. Although most aid comes from New Zealand, this is currently being phased out with reductions of NZ$250,000 each year. The country will need to rely more upon its own economy. The government generates some revenue, mainly from income tax, import tax and the lease of phone lines. 

Offshore banking

The government briefly considered offshore banking. Under pressure from the US Treasury, Niue agreed to end its support for schemes designed to minimise tax in countries like New Zealand. Niue provides automated Companies Registration, administered by the New Zealand Ministry of Economic Development. The Niue Legislative Assembly passed the Niue Consumption Tax Act in the first week of February 2009, and the 12.5% tax on goods and services was expected to take effect on 1 April 2009. Income tax has been lowered, and import tax may be reset to zero except for "sin" items like tobacco, alcohol and soft drinks. Tax on secondary income has been lowered from 35% to 10%, with the stated goal of fostering increased labour productivity.

Internet

In 1997, the Internet Assigned Numbers Authority (IANA), under contract with the US Department of Commerce, assigned the Internet Users Society-Niue (IUS-N), a private nonprofit, as manager of the .nu top-level domain on the Internet. IUS-N's charitable purpose was – and continues to be – to use revenue from the registration of .nu domain names to fund low-cost or free Internet services for the people of Niue. In a letter to ICANN in 2007, IUS-N's independent auditors reported IUS-N had invested US$3 million for Internet services in Niue between 1999 and 2005 from .nu domain name registration revenue during that period. In 1999, IUS-N and the Government of Niue signed an agreement whereby the Government recognised that IUS-N managed the .nu ccTLD under IANA's authority and IUS-N committed to provide free Internet services to government departments as well as to Niue's private citizens. A newly elected government later disputed that agreement and attempted to assert a claim on the domain name, including a requirement for IUS-N to make direct payments of compensation to the Government. In 2005, a Government-appointed Commission of Inquiry into the dispute released its report, which found no merit in the government's claims; the government subsequently dismissed the claims in 2007. Starting in 2003, IUS-N began installing WiFi connections throughout the capital village of Alofi and in several nearby villages and schools, and has been expanding WiFi coverage into the outer villages since then, making Niue the first WiFi Nation. To assure security for Government departments, IUS-N provides the government with a secure DSL connection to IUS-N's satellite Internet link, at no cost.

Agriculture is very important to the lifestyle of Niueans and the economy, and around 204 square kilometres of the land area are available for agriculture. Subsistence agriculture is very much part of Niue's culture, where nearly all the households have plantations of taro. Taro is a staple food, and the pink taro now dominant in the taro markets in New Zealand and Australia is an intellectual property of Niue. This is one of the naturally occurring taro varieties on Niue, and has a strong resistance to pests. The Niue taro is known in Samoa as "talo Niue" and in international markets as pink taro. Niue exports taro to New Zealand.
Tapioca or cassava, yams and kumara also grow very well, as do different varieties of bananas. Coconut meat, passionfruit and limes dominated exports in the 1970s, but in 2008 vanilla, noni and taro were the main export crops.

Most families grow their own food crops for subsistence and sell their surplus at the Niue Makete in Alofi, or export to their families in New Zealand.
Coconut crab, or uga, is also part of the food chain; it lives in the forest and coastal areas.

In 2003, the government made a commitment to develop and expand vanilla production with the support of NZAID. Vanilla has grown wild on Niue for a long time. Despite the setback caused by the devastating Cyclone Heta in early 2004, work on vanilla production continues. The expansion plan started with the employment of the unemployed or underemployed labour force to help clear land, plant supporting trees and plant vanilla vines. The approach to accessing land includes planning to have each household plant a small plot of around half to to be cleared and planted with vanilla vines. There are a lot of planting materials for supporting trees to meet demand for the expansion of vanilla plantations, but a severe shortage of vanilla vines for planting stock. There are the existing vanilla vines, but cutting them for planting stock will reduce or stop the vanilla from producing beans. At the moment, the focus is in the areas of harvesting and marketing.

The last agricultural census was in 1989.

Tourism is one of the three priority economic sectors (the other two are fisheries and agriculture) for economic development. In 2006, estimated visitor expenditure reached (equivalent to about $M in ) making tourism a major industry for Niue. Niue will continue to receive direct support from the government and overseas donor agencies. The only airport is Niue International Airport. Air New Zealand is the sole airline, flying twice a week from Auckland. In the early 1990s Niue International Airport was served by a local airline, Niue Airlines, but it closed in 1992.

There is a tourism development strategy to increase the number of rooms available to tourists at a sustainable level. Niue is trying to attract foreign investors to invest in the tourism industry by offering import and company tax concessions as incentives. New Zealand businessman Earl Hagaman, founder of Scenic Hotel Group, was awarded a contract in 2014 to manage the Matavai Resort in Niue after he made a $101,000 political donation to the New Zealand National Party, which at that time led a minority government in New Zealand. The resort is subsidized by New Zealand, which wants to bolster tourism there. In 2015 New Zealand announced $7.5m in additional funding for expansion of the resort. The selection of the Matavai contractor was made by the Niue Tourism Property Trust, whose trustees are appointed by New Zealand Foreign Affairs minister Murray McCully. Prime Minister John Key said he did not handle campaign donations, and that Niue premier Toke Talagi has long pursued tourism as a growth strategy. McCully denied any link between the donation, the foreign aid and the contractor selection.

Niue became the world's first dark sky country in March 2020. The entire island maintains standards of light development and keeps light pollution limited. Visitors will be able to enjoy guided Astro-tours led by trained Niuean community members. Viewing sites which are used for whale-watching and accessing the sea, as well as the roads that cross the island, make ideal viewing locations.

The sailing season begins in May. Alofi Bay has many mooring buoys and yacht crews can lodge at Niue Backpackers. The anchorage in Niue is one of the least protected in the South Pacific, so much that cruise ship tenders are often unable to risk landing passengers due to weather or sea conditions, as well as the associated risk of having them stranded ashore. Other challenges of the anchorage are a primarily coral bottom and many deep spots. Mooring buoys are attached to seine floats that support the mooring lines away from seabed obstructions.

On 27 October 2016, Niue officially declared that all its national debt was paid off. The Government plans to spend money saved from servicing loans on increasing pensions and offering incentives to lure expatriates back home. However, Niue is not entirely independent. New Zealand pays $14 million in aid each year and Niue still depends on New Zealand. Premier Toke Talagi said Niue managed to pay off US$4 million of debt and had "no interest" in borrowing again, particularly from huge powers such as China.

The first computers were Apple machines brought in by the University of the South Pacific Extension Centre around the early 1980s. The Treasury Department computerised its general ledger in 1986 using NEC personal computers that were IBM PC XT compatible. The Census of Households and Population in 1986 was the first to be processed using a personal computer with the assistance of David Marshall, FAO Adviser on Agricultural Statistics, advising UNFPA Demographer Dr Lawrence Lewis and Niue Government Statistician Bill Vakaafi Motufoou to switch from using manual tabulation cards. In 1987 Statistics Niue got its new personal computer NEC PC AT use for processing the 1986 census data; personnel were sent on training in Japan and New Zealand to use the new computer. The first Computer Policy was developed and adopted in 1988. 

In 2003, Niue became the first country in the world to provide state-funded wireless internet to all inhabitants.

In August 2008 it has been reported that all school students have what is known as the OLPC XO-1, a specialised laptop by the One Laptop per Child project designed for children in the developing world. Niue was also a location of tests for the OpenBTS project, which aims to deliver low-cost GSM base stations built with open source software.
In July 2011, Telecom Niue launched pre-paid mobile services (Voice/EDGE – 2.5G) as Rokcell Mobile based on the commercial GSM product of vendor Lemko. Three BTS sites will cover the nation. International roaming is not currently available. The fibre optic cable ring is now completed around the island (FTTC), Internet/ADSL services were rolled out towards the end of 2011.

In January 2015 Telecom Niue completed the laying of the fibre optic cable around Niue connecting all the 14 villages, making land line phones and ADSL internet connection available to households.

Niue is part of a group of some Polynesian countries forming a consortium whom will very soon switch on the undersea Manatua Fibre Cable, connecting Tahiti, French Polynesia, Cook Islands, Niue and Samoa.

The following demographic statistics are from the CIA World Factbook.






Niue is the birthplace of New Zealand artist and writer John Pule. Author of "The Shark That Ate the Sun", he also paints tapa cloth inspired designs on canvas. In 2005, he co-wrote "Hiapo: Past and Present in Niuean Barkcloth", a study of a traditional Niuean artform, with Australian writer and anthropologist Nicholas Thomas.

Taoga Niue is a new Government Department responsible for the preservation of culture, tradition and heritage. Recognising its importance, the Government has added Taoga Niue as the sixth pillar of the Niue Integrated Strategic Plan (NISP).

Niue has two broadcast outlets, Television Niue and Radio Sunshine, managed and operated by the Broadcasting Corporation of Niue, and one newspaper, the "Niue Star".

Despite being a small country, a number of sports are popular. Rugby union is the most popular sport, played by both men and women; Niue was the 2008 FORU Oceania Cup champions. Netball is played only by women. There is a nine-hole golf course at Fonuakula. There is a lawn bowling green under construction. Association Football is a popular sport, as evidenced by the Niue Soccer Tournament, though the Niue national football team has played only two matches. Rugby league is also a popular sport. Niue Rugby League have only started making strides within the international arena since their first ever test match against Vanuatu, going down 22–20 in 2013. On 4 October 2014, the Niue rugby league team record their first ever international test match win defeating the Philippines 36–22. In May 2015, Niue Rugby League recorded their second international test match win against the South African Rugby League side, 48–4. Niue now sit 31st in the Rugby League World Rankings.




</doc>
<doc id="21230" url="https://en.wikipedia.org/wiki?curid=21230" title="New England (disambiguation)">
New England (disambiguation)

New England is a region of north-eastern United States, comprising Connecticut, Maine, Massachusetts, Rhode Island, New Hampshire, and Vermont.

New England may also refer to:










</doc>
<doc id="21231" url="https://en.wikipedia.org/wiki?curid=21231" title="Nirvana (band)">
Nirvana (band)

Nirvana was an American rock band formed in Aberdeen, Washington, in 1987. Founded by lead singer and guitarist Kurt Cobain and bassist Krist Novoselic, the band went through a succession of drummers before recruiting Dave Grohl in 1990. Characterized by their punk aesthetic, Nirvana's fusion of pop melodies with noise, combined with their themes of abjection and social alienation, made them hugely popular during their short tenure. Their music maintains a popular following and continues to influence modern rock and roll culture. 

In the late 1980s, Nirvana established itself as part of the Seattle grunge scene, releasing its first album, "Bleach", for the independent record label Sub Pop in 1989. They developed a sound that relied on dynamic contrasts, often between quiet verses and loud, heavy choruses. After signing to major label DGC Records in 1991, Nirvana found unexpected mainstream success with "Smells Like Teen Spirit", the first single from their landmark second album "Nevermind" (1991). A cultural phenomenon of the 1990s, the album went on to be certified Diamond by the RIAA. Nirvana's sudden success popularized alternative rock and was credited for ending the dominance of hair metal, while they were also often referenced as the figurehead band of Generation X.

Following extensive tours and the 1992 compilation album "Incesticide" and EP "Hormoaning", Nirvana released their abrasive and less mainstream sounding third studio album, "In Utero" (1993). The album topped both the American and British album charts, and was widely acclaimed. Nirvana disbanded following Cobain's death in April 1994. Various posthumous releases have been overseen by Novoselic, Grohl, and Cobain's widow Courtney Love. The posthumous live album "MTV Unplugged in New York" (1994) won Best Alternative Music Performance at the 1996 Grammy Awards.

During their three years as a mainstream act, Nirvana received an American Music Award, Brit Award and Grammy Award, as well as seven MTV Video Music Awards and two NME Awards. By 2009, they had sold over 75 million records worldwide, making them one of the best-selling bands of all time. They achieved five number-one hits on the "Billboard" Alternative Songs chart and four number-one albums on the "Billboard" 200. In 2004, "Rolling Stone" named Nirvana among the greatest artists of all time. They were inducted into the Rock and Roll Hall of Fame in their first year of eligibility in 2014.

Singer and guitarist Kurt Cobain and bassist Krist Novoselic met while attending Aberdeen High School in Washington state. The pair became friends while frequenting the practice space of the Melvins. Cobain wanted to form a band with Novoselic, but Novoselic did not respond for a long period. Cobain gave him a demo tape of his project Fecal Matter. Three years after the two first met, Novoselic notified Cobain that he had finally listened to the Fecal Matter demo and suggested they start a group. Their first band, the Sellouts, was a Creedence Clearwater Revival tribute band. They recruited Bob McFadden on drums, but after a month the project fell apart. In early 1987, Cobain and Novoselic recruited drummer Aaron Burckhard. They practiced material from Cobain's Fecal Matter tape but started writing new material soon after forming.
During its initial months, the band went through a series of names, including Fecal Matter, Skid Row and Ted Ed Fred. The group settled on Nirvana because, according to Cobain, "I wanted a name that was kind of beautiful or nice and pretty instead of a mean, raunchy punk name like the Angry Samoans". Novoselic and Cobain moved to Tacoma and Olympia, Washington respectively. They temporarily lost contact with Burckhard, and instead practiced with Dale Crover of the Melvins. Nirvana recorded its first demos in January 1988.

In early 1988, Crover moved to San Francisco but recommended Dave Foster as his replacement on drums. Foster's tenure with Nirvana lasted only a few months; during a stint in jail, he was replaced by Burckhard, who again departed after telling Cobain he was too hungover to practice one day. Cobain and Novoselic put an ad seeking a replacement drummer in "The Rocket", a Seattle music publication, but received no satisfactory responses. Meanwhile, a mutual friend introduced them to drummer Chad Channing, and the three musicians agreed to jam together. Channing continued to jam with Cobain and Novoselic; however, by Channing's own account, "They never actually said 'okay, you're in.'" Channing played his first show with Nirvana in May 1988.

Nirvana released its first single, a cover of Shocking Blue's "Love Buzz", in November 1988 on the Seattle independent record label Sub Pop. They did their first ever interview with John Robb in "Sounds", which made their release its single of the week. The following month, the band began recording its debut album, "Bleach", with local producer Jack Endino. "Bleach" was influenced by the heavy dirge-rock of the Melvins and the 1980s punk rock of Mudhoney, as well as the 1970s heavy metal of Black Sabbath. The money for the recording sessions for "Bleach", listed as $606.17 on the album sleeve, was supplied by Jason Everman, who was subsequently brought into the band as the second guitarist. Though Everman did not play on the album, he received a credit on "Bleach" because, according to Novoselic, they "wanted to make him feel more at home in the band". Just prior to the album's release, Nirvana became the first band to sign an extended contract with Sub Pop.

"Bleach" was released in June 1989, and became a favorite of college radio stations. After the album's release Nirvana embarked on its first national tour, but ended up canceling the last few dates and returning to Washington state due to increasing differences with Everman. No one told Everman he was fired; Everman later said he had actually quit. Although Sub Pop did not promote "Bleach" as much as other releases, it was a steady seller, and had initial sales of 40,000 copies. However, Cobain was upset by the label's lack of promotion and distribution for the album. In late 1989, the band recorded the "Blew" EP with producer Steve Fisk. In an interview around that time with John Robb in "Sounds", Cobain commented that the band's music was changing, saying "The early songs were really angry... But as time goes on the songs are getting poppier and poppier as I get happier and happier. The songs are now about conflicts in relationships, emotional things with other human beings".

In April 1990, Nirvana began working on their next album with producer Butch Vig at Smart Studios in Madison, Wisconsin. Cobain and Novoselic became disenchanted with Channing's drumming, and Channing expressed frustration at not being involved in songwriting. As bootlegs of Nirvana demos with Vig began to circulate in the music industry and draw attention from major labels, Channing left the band. That July, Nirvana recorded the single "Sliver" with Mudhoney drummer Dan Peters. Dale Crover filled in on drums on Nirvana's seven-date American West Coast tour with Sonic Youth that August.

In September 1990, Buzz Osborne of the Melvins introduced the band to drummer Dave Grohl, whose Washington, D.C. band Scream had broken up. Grohl auditioned for Novoselic and Cobain days after arriving in Seattle; Novoselic later said, "We knew in two minutes that he was the right drummer." Grohl told "Q" :"I remember being in the same room with them and thinking, 'What? "That"'s Nirvana? Are you kidding?' Because on their record cover they looked like psycho lumberjacks... I was like, 'What, that little dude and that big motherfucker? You're kidding me'."

Disenchanted with Sub Pop and with the Smart Studios sessions generating interest, Nirvana decided to look for a deal with a major record label since no indie label could buy the group out of its contract. Cobain and Novoselic consulted Soundgarden and Alice in Chains manager Susan Silver for advice. They met Silver in Los Angeles and she introduced them to agent Don Muller and music business attorney Alan Mintz, who was specialized in finding deals for new bands. Mintz started sending out Nirvana's demo tape to major labels looking for deals. Following repeated recommendations by Sonic Youth's Kim Gordon, Nirvana signed to DGC Records in 1990. When Nirvana was inducted into the Rock and Roll Hall of Fame in 2014, Novoselic thanked Silver during his speech for "introducing them to the music industry properly".

After signing, the band began recording its first major label album, "Nevermind". The group was offered a number of producers, but held out for Vig. Rather than record at Vig's Madison studio as they had in 1990, production shifted to Sound City Studios in Van Nuys, Los Angeles, California. For two months, the band worked through a variety of songs. Some, such as "In Bloom" and "Breed", had been in Nirvana's repertoire for years, while others, including "On a Plain" and "Stay Away", lacked finished lyrics until midway through the recording process. After the recording sessions were completed, Vig and the band set out to mix the album. However, the recording sessions had run behind schedule and the resulting mixes were deemed unsatisfactory. Slayer mixer Andy Wallace was brought in to create the final mix. After the album's release, members of Nirvana expressed dissatisfaction with the polished sound the mixer had given "Nevermind".
Initially, DGC Records was hoping to sell 250,000 copies of "Nevermind", the same they had achieved with Sonic Youth's "Goo". However, the first single "Smells Like Teen Spirit" quickly gained momentum, boosted by major airplay of the music video on MTV. As it toured Europe during late 1991, the band found that its shows were dangerously oversold, that television crews were becoming a constant presence onstage, and that "Smells Like Teen Spirit" was almost omnipresent on radio and music television. By Christmas 1991, "Nevermind" was selling 400,000 copies a week in the US. In January 1992, the album displaced Michael Jackson's "Dangerous" at number one on the "Billboard" album charts, and topped the charts in numerous other countries. The month "Nevermind" reached number one, "Billboard" proclaimed, "Nirvana is that rare band that has everything: critical acclaim, industry respect, pop radio appeal, and a rock-solid college/alternative base." The album eventually sold over seven million copies in the United States and over 30 million worldwide. Nirvana's sudden success was credited for popularizing alternative rock and ending the dominance of hair metal. 

Citing exhaustion, Nirvana decided not to undertake another American tour in support of "Nevermind", instead opting to make only a handful of performances later that year. In March 1992, Cobain sought to reorganize the group's songwriting royalties (which to this point had been split equally) to better represent that he wrote the majority of the music. Grohl and Novoselic did not object, but when Cobain wanted the agreement to be retroactive to the release of "Nevermind", the disagreements between the two sides came close to breaking up the band. After a week of tension, Cobain received a retroactive share of 75 percent of the royalties. Bad feelings about the situation remained within the group afterward.

Amid rumors that the band was disbanding due to Cobain's health, Nirvana headlined the closing night of England's 1992 Reading Festival. Cobain personally programmed the performance lineup. Nirvana's performance at Reading is often regarded by the press as one of the most memorable of the group's career. A few days later, Nirvana performed at the MTV Video Music Awards; despite the network's refusal to let the band play the new song "Rape Me", Cobain strummed and sang the first few bars of the song before breaking into "Lithium". The band received awards for the Best Alternative Video and Best New Artist categories.

DGC had hoped to have a new Nirvana album ready for a late 1992 holiday season; instead, it released the compilation album "Incesticide" in December 1992. A joint venture between DGC and Sub Pop, "Incesticide" collected various rare Nirvana recordings and was intended to provide the material for a better price and higher quality than bootlegs. As "Nevermind" had been out for 15 months and had yielded a fourth single in "In Bloom" by that point, Geffen/DGC opted not to heavily promote "Incesticide", which was certified gold by the Recording Industry Association of America the following February.

In February 1993, Nirvana released "Puss"/"Oh, the Guilt", a split single with The Jesus Lizard, on the independent label Touch & Go. Meanwhile, the group chose Steve Albini, who had a reputation as a principled and opinionated individual in the American indie music scene, to record its third album. While there was speculation that the band chose Albini to record the album due to his underground credentials, Cobain insisted that Albini's sound was simply the one he had always wanted Nirvana to have: a "natural" recording without layers of studio trickery. Nirvana traveled to Pachyderm Studio in Cannon Falls, Minnesota, in that February to record the album. The sessions with Albini were productive and quick, and the album was recorded and mixed in two weeks for $25,000.

Several weeks after the completion of the recording sessions, stories ran in the "Chicago Tribune" and "Newsweek" that quoted sources claiming DGC considered the album "unreleasable". As a result, fans began to believe that the band's creative vision might be compromised by their label. While the stories about DGC shelving the album were untrue, the band actually was unhappy with certain aspects of Albini's mixes; they thought the bass levels were too low, and Cobain felt that "Heart-Shaped Box" and "All Apologies" did not sound "perfect". Longtime R.E.M. producer Scott Litt was called in to remix these two songs, with Cobain adding additional instrumentation and backing vocals."In Utero" topped both the American and British album charts. "Time"s Christopher John Farley wrote in his review of the album, "Despite the fears of some alternative-music fans, Nirvana hasn't gone mainstream, though this potent new album may once again force the mainstream to go Nirvana". "In Utero" went on to sell over 5 million copies in the United States. That October, Nirvana embarked on its first tour of the United States in two years with support from Half Japanese and the Breeders. For the tour, the band added Pat Smear of the punk rock band Germs as a second guitarist.

In November, Nirvana recorded a performance for the television program "MTV Unplugged". Augmented by Smear and cellist Lori Goldston, the band broke convention for the show by choosing not to play their most recognizable songs. Instead, they performed several covers, and invited Cris and Curt Kirkwood of the Meat Puppets to join them for renditions of three Meat Puppets songs.

In early 1994, Nirvana embarked on a European tour. Nirvana's final concert took place in Munich, Germany, on March 1. In Rome, on the morning of March 4, Cobain's wife, Courtney Love, found Cobain unconscious in their hotel room and he was rushed to the hospital. Cobain had reacted to a combination of prescribed Rohypnol and alcohol. The rest of the tour was canceled. In the ensuing weeks, Cobain's heroin addiction resurfaced. Following an intervention, Cobain was convinced to admit himself into drug rehabilitation. After less than a week, he left the rehabilitation facility and returned to Seattle. One week later, on April 8, 1994, Cobain was found dead of a self-inflicted shotgun wound at his home in the Denny-Blaine neighborhood of the city.

In August 1994, DGC announced a double album, "Verse Chorus Verse," comprising live material from throughout Nirvana's career, including its "MTV Unplugged" performance. However, Novoselic and Grohl found assembling the material so soon after Cobain's death emotionally overwhelming, and the album was canceled. Instead, in November, DGC released the "MTV Unplugged" performance as "MTV Unplugged in New York;" it debuted at number one on the "Billboard" charts, and earned Nirvana a Grammy Award for Best Alternative Music Album. It was followed by Nirvana's first full-length live video, "Live! Tonight! Sold Out!!". In 1996, the live album "From the Muddy Banks of the Wishkah" became the third consecutive Nirvana release to debut at the top of the "Billboard" album chart.

Grohl founded a new band, Foo Fighters. He and Novoselic decided against Novoselic joining Foo Fighters; Grohl said it would have felt "really natural" for them to work together again, but would have been uncomfortable for the other band members and place more pressure on Grohl. Novoselic turned his attention to political activism. In 1997, Novoselic, Grohl, and Love formed the limited liability company Nirvana LLC to oversee all Nirvana projects. A 45-track box set of Nirvana rarities was scheduled for release in October 2001. However, shortly before the release date, Love filed a suit to dissolve Nirvana LLC, and an injunction was issued preventing the release of any new Nirvana material until the case was resolved. Love contended that Cobain was the band, that Grohl and Novoselic were sidemen, and that she had signed the partnership agreement originally under bad advice. Grohl and Novoselic countersued, asking the court to remove Love from the partnership and to replace her with another representative of Cobain's estate.

The day before the case was set to go to trial in October 2002, Love, Novoselic, and Grohl announced that they had reached a settlement. The next month, the best-of compilation "Nirvana" was released, featuring the previously unreleased track "You Know You're Right", the last song Nirvana recorded. It debuted at number three on the "Billboard" album chart. The box set, "With the Lights Out", was finally released in November 2004. The release contained early Cobain demos, rough rehearsal recordings, and live tracks recorded throughout the band's history. An album of selected tracks from the box set, "", was released in late 2005.
In April 2006, Love announced that she was selling 25 percent of her stake in the Nirvana song catalog in a deal estimated at $50 million. The share of Nirvana's publishing was purchased by Primary Wave Music, which was founded by Larry Mestel, a former CEO of Virgin Records. Love sought to assure Nirvana's fanbase that the music would not simply be licensed to the highest bidder: "We are going to remain very tasteful and true to the spirit of Nirvana while taking the music to places it has never been before". Further releases have included the DVD releases of "Live! Tonight! Sold Out!!" in 2006, and the full version of "MTV Unplugged in New York" in 2007. In November 2009, Nirvana's performance at the 1992 Reading Festival was released on CD and DVD as "Live at Reading," alongside a deluxe 20th-anniversary edition of "Bleach." DGC released a number of 20th anniversary deluxe-edition packages of "Nevermind" in September 2011 and "In Utero" in September 2013.

In 2012, Grohl, Novoselic, and Smear joined Paul McCartney at . The performance featured the premiere of a new song written by the four, "Cut Me Some Slack". A studio recording was released on the soundtrack to "Sound City", a film by Grohl. On July 19, 2013, they played with McCartney again during the encore of his Safeco Field "Out There" concert in Seattle, the first time Nirvana members had played together in their hometown in over 15 years.

In 2014, Cobain, Novoselic, and Grohl were inducted into the Rock and Roll Hall of Fame. At the induction ceremony, Novoselic, Grohl and Smear performed a four-song set with guest vocalists Joan Jett, Kim Gordon, St. Vincent, and Lorde. Novoselic, Grohl, and Smear then performed a full show at Brooklyn's St. Vitus Bar with Jett, Gordon, St. Vincent, J Mascis, and John McCauley as guest vocalists. At the ceremony, Grohl thanked Burckhard, Crover, Peters and Channing for their time in Nirvana. Everman also attended.

At Clive Davis' annual pre-Grammy party in 2016, the surviving members of Nirvana reunited to perform the David Bowie song "The Man Who Sold the World", which Nirvana covered in their "MTV Unplugged" performance. Beck accompanied them on acoustic guitar and lead vocals. In October 2018, Novoselic and Grohl reunited during the finale of the Cal Jam festival at Glen Helen Amphitheater in San Bernardino County, California, joined by guest vocalists John McCauley and Joan Jett. On June 25, 2019, "The New York Times Magazine" listed Nirvana among hundreds of artists whose material was destroyed in the 2008 Universal fire. In January 2020, Novoselic and Grohl reunited for a performance at a benefit for The Art of Elysium at the Hollywood Palladium, joined by Beck, St Vincent, and Grohl's daughter Violet Grohl.

Cobain described Nirvana's initial sound as "a Gang of Four and Scratch Acid ripoff". When Nirvana recorded "Bleach", Cobain felt he had to fit the expectations of the Sub Pop grunge sound to build a fanbase, and suppressed his arty and pop songwriting in favor of a more rock sound. Nirvana biographer Michael Azerrad argued, "Ironically, it was the restrictions of the Sub Pop sound that helped the band find its musical identity." Azerrad stated that by acknowledging that they had grown up listening to Black Sabbath and Aerosmith, they had been able to move on from their derivative early sound.

Nirvana used dynamic shifts that went from quiet to loud. Cobain sought to mix heavy and pop musical sounds, saying, "I wanted to be totally Led Zeppelin in a way and then be totally extreme punk rock and then do real wimpy pop songs." When Cobain heard the Pixies' 1988 album "Surfer Rosa" after recording "Bleach", he felt it had the sound he wanted to achieve but had been too intimidated to try. The Pixies' subsequent popularity encouraged Cobain to follow his instincts as a songwriter. Like the Pixies, Nirvana moved between "spare bass-and-drum grooves and shrill bursts of screaming guitar and vocals". Near the end of his life, Cobain said the band had become bored of the "limited" formula, but expressed doubt that they were skilled enough to try other dynamics.

Cobain's rhythm guitar style, which relied on power chords, low-note riffs, and a loose left-handed technique, featured the key components to the band's songs. Cobain would often initially play a song's verse riff in a clean tone, then double it with distorted guitars when he repeated the part. In some songs the guitar would be absent from the verses entirely to allow the drums and bass guitar to support the vocals, or it would only play sparse melodies like the two-note pattern used in "Smells Like Teen Spirit". Cobain rarely played standard guitar solos, opting to play variations of the song's melody as single note lines. Cobain's solos were mostly blues-based and discordant, which music writer Jon Chappell described as "almost an iconoclastic parody of the traditional instrumental break", a quality typified by the note-for-note replication of the lead melody in "Smells Like Teen Spirit" and the atonal solo for "Breed". The band had no formal musical training; Cobain said: "I have no concept of knowing how to be a musician at all what-so-ever... I couldn't even pass Guitar 101".

Grohl's drumming "took Nirvana's sound to a new level of intensity". Azerrad stated that Grohl's "powerful drumming propelled the band to a whole new plane, visually as well as musically", noting, "Although Dave is a merciless basher, his parts are also distinctly musical—it wouldn't be difficult to figure out what song he was playing even without the rest of the music".

From 1992, Cobain and Novoselic would tune their guitars to E flat for studio and live performances (until then, their live tunings were to concert pitch). Cobain noted, "We play so hard we can't tune our guitars fast enough". The band made a habit of destroying its equipment after shows. Novoselic said he and Cobain created the "shtick" in order to get off of the stage sooner. Cobain stated it began as an expression of his frustration with previous drummer Channing making mistakes and dropping out entirely during performances.

Everett True said in 1989, "Nirvana songs treat the banal and pedestrian with a unique slant". Cobain came up with the basic components of each song (usually writing them on an acoustic guitar), as well as the singing style and the lyrics. He emphasized that Novoselic and Grohl "have a big part in deciding on how long a song should be and how many parts it should have. So I don't like to be considered the sole songwriter". When asked which part of the songs he would write first, Cobain responded, "I don’t know. I really don't know. I guess I start with the verse and then go into the chorus".

Cobain usually wrote lyrics for songs minutes before recording them. Cobain said, "When I write a song the lyrics are the least important subject. I can go through two or three different subjects in a song and the title can mean absolutely nothing at all". Cobain told "Spin" in 1993 that he "didn't give a flying fuck" what the lyrics on "Bleach" were about, figuring "Let's just scream some negative lyrics and as long as they're not sexist and don't get too embarrassing it'll be okay", while the lyrics to "Nevermind" were taken from two years of poetry he had accumulated, which he cut up and chose lines he preferred from. In comparison, Cobain stated that the lyrics to "In Utero" were "more focused, they're almost built on themes". Cobain didn't write necessarily in a linear fashion, instead relying on juxtapositions of contradictory images to convey emotions and ideas. Often in his lyrics, Cobain would present an idea then reject it; the songwriter explained, "I'm such a nihilistic jerk half the time and other times I'm so vulnerable and sincere [.. The songs are] like a mixture of both of them. That's how most people my age are".

Characterized by their punk aesthetic, Nirvana often fused pop melodies with noise. Combined with their themes of abjection and alienation, the band became hugely popular during their short tenure. Stephen Thomas Erlewine wrote that prior to Nirvana, "alternative music was consigned to specialty sections of record stores, and major labels considered it to be, at the very most, a tax write-off". Following the release of "Nevermind", "nothing was ever quite the same, for better and for worse". The success of "Nevermind" not only popularized grunge, but also established "the cultural and commercial viability of alternative rock in general". While other alternative bands had hits before, Nirvana "broke down the doors forever", according to Erlewine. Erlewine further stated that Nirvana's breakthrough "didn't eliminate the underground", but rather "just gave it more exposure". Erlewine also stated that Nirvana's breakthrough "popularized so-called 'Generation X' and 'slacker' culture". Immediately following Cobain's death, numerous headlines referred to Nirvana's frontman as "the voice of a generation", although he had rejected such labeling during his lifetime. 

In 1992, Jon Pareles of "The New York Times" reported that Nirvana's breakthrough had made others in the alternative scene impatient for achieving similar success, noting, "Suddenly, all bets are off. No one has the inside track on which of dozens, perhaps hundreds, of ornery, obstreperous, unkempt bands might next appeal to the mall-walking millions". Record company executives offered large advances and record deals to bands, and previous strategies of building audiences for alternative rock groups had been replaced by the opportunity to achieve mainstream popularity quickly.Nirvana didn't invent alternative rock, but they were one of the bands foremost in bringing it to the masses. The Seattle trio had an unmistakeable sound - a genius blend of Kurt Cobain's raspy voice and gnashing guitars, Dave Grohl's relentless drumming and Krist Novoselic's uniting bass-work that connected with fans in a hail of alternately melodic and hard-charging songs that would become signature classics. – "Billboard" magazineMichael Azerrad argued in his Nirvana biography "" (1993) that "Nevermind" marked an epochal generational shift in music similar to the rock-and-roll explosion in the 1950s and the end of the baby boomer generation's dominance of the musical landscape. Azerrad wrote, ""Nevermind" came along at exactly the right time. This was music by, for, and about a whole new group of young people who had been overlooked, ignored, or condescended to." Regarding the success of "Nevermind", Fugazi frontman Guy Picciotto said: "It was like our record could have been a hobo pissing in the forest for the amount of impact it had ... It felt like we were playing ukuleles all of a sudden because of the disparity of the impact of what they did."

Nirvana is one of the best-selling bands of all time, having sold over 75 million records worldwide. With over 25 million RIAA-certified units, the band is also the 80th-best-selling music artist in the United States. The band have achieved ten top 40 hits on the "Billboard" Alternative Songs chart, including five No. 1's. Two of the band's studio albums and two of their live albums have reached the top spot on the "Billboard" 200. Nirvana has been awarded one Diamond, three Multi-Platinum, seven Platinum and two Gold certified albums in the United States by the RIAA, and four Multi-Platinum, four Platinum, two Gold and one Silver certified albums in the UK by the BPI. "Nevermind", the band's most successful album, has sold over 30 million copies worldwide, making it one of the best-selling albums ever. Their most successful song, "Smells Like Teen Spirit", is among the best-selling singles of all time, having sold 8 million copies.

Since its breakup, Nirvana has continued to receive acclaim. In 2003, they were selected as one of the inductees of the "Mojo" Hall of Fame 100. The band also received a nomination in 2004 from the UK Music Hall of Fame for the title of "Greatest Artist of the 1990s". "Rolling Stone" placed Nirvana at number 27 on their list of the "100 Greatest Artists of All Time" in 2004, and at number 30 on their updated list in 2011. In 2003, the magazine's senior editor David Fricke picked Kurt Cobain as the 12th best guitarist of all time. "Rolling Stone" later ranked Cobain as the 45th greatest singer in 2008 and 73rd greatest guitarist of all time in 2011. VH1 ranked Nirvana as the 42nd greatest artists of rock and roll in 1998, the 7th greatest hard rock artists in 2000, and the 14th greatest artists of all time in 2010.

Nirvana's contributions to music have also received recognition. The Rock and Roll Hall of Fame has inducted two of Nirvana's recordings, "Smells Like Teen Spirit" and "All Apologies", into its list of "The Songs That Shaped Rock and Roll". The museum also ranked "Nevermind" number 10 on its "The Definitive 200 Albums of All Time" list in 2007. In 2005, the Library of Congress added "Nevermind" to the National Recording Registry, which collects "culturally, historically or aesthetically important" sound recordings from the 20th century. In 2011, four of Nirvana's songs appeared on "Rolling Stone"s updated list of "The 500 Greatest Songs of All Time", with "Smells Like Teen Spirit" ranking the highest at number 9. Three of the band's albums were ranked on the magazine's 2012 list of "The 500 Greatest Albums of All Time", with "Nevermind" placing the highest at number 17. The same three Nirvana albums were also placed on "Rolling Stone"'s 2011 list of "The 100 Best Albums of the Nineties", with "Nevermind" ranking the highest at number 1, making it the greatest album of the decade. "Time" included "Nevermind" on its list of "The All-TIME 100 Albums" in 2006, labeling it "the finest album of the 1990s". In 2011, the magazine also added "Smells Like Teen Spirit" on its list of "The All-TIME 100 Songs", and "Heart-Shaped Box" on its list of "The 30 All-TIME Best Music Videos".

Nirvana was announced in their first year of eligibility as being part of the 2014 class of inductees into the Rock and Roll Hall of Fame on December 17, 2013. The induction ceremony was held April 10, 2014, in Brooklyn, New York, at the Barclays Center. However, Channing, who was informed of his omission by SMS, was not included in the induction, as the accolade was only applied to Cobain, Novoselic and Grohl.







</doc>
<doc id="21232" url="https://en.wikipedia.org/wiki?curid=21232" title="Nirvana (British band)">
Nirvana (British band)

Nirvana are an English pop rock band, formed in London, England in 1965. Though the band achieved only limited commercial success, they were acclaimed both by music industry professionals and by critics. In 1985, the band reformed. The members of the band took the popular American rock band with the same name to court over the usage of the name, eventually reaching a settlement.

Nirvana was created as the performing arm of the London-based songwriting partnership of Irish musician Patrick Campbell-Lyons and Greek composer Alex Spyropoulos. On their recordings Campbell-Lyons and Spyropoulos supplied all the vocals. The instrumental work was primarily undertaken by top session musicians and orchestral musicians - with Campbell-Lyons providing a little guitar and Spyropoulos contributing some keyboards.

Musically, Campbell-Lyons and Spyropoulos blended myriad musical styles including rock, pop, folk, jazz, Latin rhythms and classical music, primarily augmented by baroque chamber-style arrangements to create a unique entity.

In October 1967, they released their first album: a concept album produced by Chris Blackwell titled "The Story of Simon Simopath". The album was one of the first narrative concept albums ever released, predating story-driven concept albums such as The Pretty Things' "S.F. Sorrow" (December 1968), The Who's "Tommy" (April 1969) and The Kinks' "Arthur" (September 1969), and the Moody Blues album "Days of Future Passed" (November 1967) by a month.

Island Records launched Nirvana's first album "with a live show at the Saville Theatre, sharing a bill with fellow label acts Traffic, Spooky Tooth, and Jackie Edwards."

Unable to perform their songs live as a duo and with the impending release of their first album, Campbell-Lyons and Spyropoulos decided to create a live performing ensemble, The Nirvana Ensemble, and they recruited four musicians to enable them to undertake concerts and TV appearances. Though hired to be part of the live performance group rather than as band members, these four musicians were also included in the photograph alongside the core duo on the album cover of their first album to assist in projecting an image of a group rather than a duo. However they were not core founding members of the group and within a few months Nirvana had reverted to its original two-person lineup. The four musicians who augmented Campbell-Lyons and Spyropoulos on their live appearances and television shows for those few months were Ray Singer (guitar), Brian Henderson (bass), Sylvia A. Schuster (cello) and Michael Coe (French horn, viola). Sue & Sunny also participated in The Nirvana Ensemble, providing vocals.

The band appeared on French television with Salvador Dalí, who splashed black paint on them during a performance of their second single "Rainbow Chaser." Campbell-Lyons kept the jacket, but regrets that Dalí did not sign any of their paint-splashed clothes. Island Records allegedly sent the artist an invoice for the cleaning of Schuster's cello.

Following the minor chart success of "Rainbow Chaser", "live appearances became increasingly rare" and the songwriting duo at the core of Nirvana "decided to disband the sextet" and to rely on session musicians for future recordings. Spyropoulos cited Schuster's departure due to pregnancy as the instigator for the band returning to its core membership. Campbell-Lyons also cited the high cost of having the additional members as a reason for their departure. Schuster later became principal cellist of the BBC Symphony Orchestra.

In 1968, the duo recorded their second album, "All of Us", which featured a similar broad range of musical styles as their first album.

Their third album, "Black Flower", was rejected by Blackwell, comparing it disparagingly to Francis Lai's "A Man and a Woman". Under the title, "To Markos III" (supposedly named for a "rich uncle" of Spyropoulos who helped finance the album), it was released in the UK on the Pye label in May 1970, though reportedly only 250 copies were pressed it was deleted shortly after. One track, "Christopher Lucifer," was a jibe at Blackwell.

In 1971 the duo amicably separated, with Campbell-Lyons the primary contributor to the next two Nirvana albums, "Local Anaesthetic" 1971, and "Songs of Love And Praise" 1972, the latter featuring the return of Sylvia Schuster. Campbell-Lyons subsequently worked as a solo artist and issued further albums: "Me and My Friend", 1973, "The Electric Plough", 1981, and "The Hero I Might Have Been", 1983, though these did not enjoy commercial success.

The duo reunited in 1985, touring Europe and releasing a compilation album "Black Flower" (Bam-Caruso, 1987) which contained some new material. ("Black Flower" had been the original planned title of their third album). In the 1990s two further albums were released. "Secret Theatre" 1994 compiled rare tracks and demos, while "Orange and Blue" 1996 contained previously unreleased material including a flower-power cover of the song "Lithium" originally recorded by the Seattle grunge band of the same name, Nirvana. According to the band's official website, this was intended as part of a tongue-in-cheek album called "Nirvana Sings Nirvana" that was aborted when lead singer Kurt Cobain died. When the recording was presented on the "Orange and Blue" album, Campbell-Lyons's liner notes treated it seriously and with allusion to Heathcliff from "Wuthering Heights". Also according to the website, the band still wanted to open for Hole even after Cobain's death.

The original group filed a lawsuit in California against the Seattle grunge band in 1992. The matter was settled out of court on undisclosed terms that apparently allowed both bands to continue using the name and issuing new recordings without any packaging disclaimers or caveats to distinguish one Nirvana from the other. Music writer Everett True has claimed that Cobain's record label paid $100,000 to the original Nirvana to permit Cobain's band continued use of the name.

In 1999, they released a three-disc CD anthology titled "Chemistry", including twelve previously unreleased tracks and some new material. Their first three albums were reissued on CD by Universal Records in 2003 and received critical acclaim. In 2005, Universal (Japan) reissued "Local Anaesthetic" and "Songs of Love And Praise".

In 2018 a new album was released on the Island label "Rainbow Chaser: The 60s Recordings (The Island Years)" which featured the first 2 albums from this UK psychedelic group in a 2CD package, featuring 52 tracks with 27 previously unreleased outtakes, demos and alternative versions. Included in these tracks are some rare gems that clearly show how the group influenced pop at the time.

The group were in the school of baroque-flavoured, melodic pop-rock music typified by the Beach Boys of "Pet Sounds" and "God Only Knows", the Zombies of "Odessey and Oracle" and "Time of the Season", the Procol Harum of "A Whiter Shade of Pale", the Moody Blues of "Days of Future Passed" and "Nights in White Satin" and the Kinks of "Waterloo Sunset" and the Love of "Forever Changes". The majority of the tracks on Nirvana's albums fell into that broad genre of contemporary popular music, not easily categorized but perhaps best described as the baroque or chamber strand of progressive rock, soft rock or orchestral pop and chamber pop.

The Nirvana song "Rainbow Chaser" is thought to be the first-ever British recording to feature the audio effect known as phasing or flanging throughout an entire track, as distinct from occasionally within a song such as the Small Faces in "Itchycoo Park". Phasing was, by 1967, heavily identified with the musical style known as psychedelia, and as "Rainbow Chaser" was the only Nirvana single to achieve commercial success, peaking at number 34 in UK Singles Chart during May 1968, they were invariably tagged as a "psychedelic" band. "Rainbow Chaser" was one of the few Nirvana recordings that had any connection with "psychedelic" music. "Orange and Blue", though, was acknowledged to have been written under the influence of LSD according to the liner notes of the eponymous album.

A who's-who of behind-the-scenes craftsmen, who went on to become Britain's top producers, arrangers, engineers and mixers of the 1970s, chose to work with Nirvana in the late 1960s and in essence cut their studio teeth working with Nirvana. Two of these arranger/producers actually worked with Nirvana before working with The Beatles and The Rolling Stones.

Nirvana's producers, arrangers, engineers and mixers included:
Mike Weighell, Nova Studios, beginning of the 1970s.

Others who worked on production with Nirvana include Muff Winwood (formerly of the Spencer Davis Group); arranger/producer Mike Hurst who worked with Jimmy Page, Cat Stevens, Manfred Mann, Spencer Davis Group and Colin Blunstone; arranger Johnny Scott who arranged for the Hollies and subsequently scored films such as "The Shooting Party" and "".

Top musicians who played on Nirvana sessions include: Lesley Duncan, Big Jim Sullivan, Herbie Flowers, Billy Bremner (later of Rockpile/Dave Edmunds fame), Luther Grosvenor, Clem Cattini and the full lineup of rock band Spooky Tooth, Pete Kelly (also known as Patrick Joseph Kelly)(Keyboards) who also co-wrote the 'Modus Operandi'track on the 'Local Anaesthetic' album.





</doc>
<doc id="21233" url="https://en.wikipedia.org/wiki?curid=21233" title="Nirvana">
Nirvana

In Indian religions, nirvana is synonymous with "moksha" and "mukti". All Indian religions assert it to be a state of perfect quietude, freedom, highest happiness as well as the liberation from or ending of "samsara", the repeating cycle of birth, life and death.

However, Buddhist and non-Buddhist traditions describe these terms for liberation differently. In the Buddhist context, "nirvana" refers to realization of non-self and emptiness, marking the end of rebirth by stilling the "fires" that keep the process of rebirth going. In Hindu philosophy, it is the union of or the realization of the identity of Atman with Brahman, depending on the Hindu tradition. In Jainism, nirvana is also the soteriological goal, representing the release of a soul from karmic bondage and samsara.

The word "nirvāṇa", states Steven Collins, is from the verbal root "vā" "blow" in the form of past participle "vāna" "blown", prefixed with the preverb "nis" meaning "out". Hence the original meaning of the word is "blown out, extinguished". Sandhi changes the sounds: the "v" of "vāna" causes "nis" to become "nir", and then the "r" of "nir" causes retroflexion of the following "n": "nis+vāna" > "nirvāṇa".

The term "nirvana" in the soteriological sense of "blown out, extinguished" state of liberation does not appear in the Vedas nor in the Upanishads. According to Collins, "the Buddhists seem to have been the first to call it "nirvana"." However, the ideas of spiritual liberation using different terminology, with the concept of soul and Brahman, appears in Vedic texts and Upanishads, such as in verse 4.4.6 of the Brihadaranyaka Upanishad. This may have been deliberate use of words in early Buddhism, suggests Collins, since Atman and Brahman were described in Vedic texts and Upanishads with the imagery of fire, as something good, desirable and liberating.

"Nirvāṇa" is a term found in the texts of all major Indian religions – Buddhism, Hinduism, Jainism and Sikhism. It refers to the profound peace of mind that is acquired with "moksha", liberation from samsara, or release from a state of suffering, after respective spiritual practice or sādhanā.

The idea of "moksha" is connected to the Vedic culture, where it conveyed a notion of "amrtam", "immortality", and also a notion of a "timeless", "unborn", or "the still point of the turning world of time". It was also its timeless structure, the whole underlying "the spokes of the invariable but incessant wheel of time". The hope for life after death started with notions of going to the worlds of the Fathers or Ancestors and/or the world of the Gods or Heaven.

The earliest Vedic texts incorporate the concept of life, followed by an afterlife in heaven and hell based on cumulative virtues (merit) or vices (demerit). However, the ancient Vedic Rishis challenged this idea of afterlife as simplistic, because people do not live an equally moral or immoral life. Between generally virtuous lives, some are more virtuous; while evil too has degrees, and either permanent heaven or permanent hell is disproportionate. The Vedic thinkers introduced the idea of an afterlife in heaven or hell in proportion to one's merit, and when this runs out, one returns and is reborn. The idea of rebirth following "running out of merit" appears in Buddhist texts as well. This idea appears in many ancient and medieval texts, as "Saṃsāra", or the endless cycle of life, death, rebirth and redeath, such as section 6:31 of the "Mahabharata" and verse 9.21 of the "Bhagavad Gita". The Saṃsara, the life after death, and what impacts rebirth came to be seen as dependent on karma.

The liberation from Saṃsāra "d"eveloped as an ultimate goal and soteriological value in the Indian culture, and called by different terms such as nirvana, moksha, mukti and kaivalya. This basic scheme underlies Hinduism, Jainism and Buddhism, where "the ultimate aim is the timeless state of "moksa", or, as the Buddhists first seem to have called it, nirvana."

Although the term occurs in the literatures of a number of ancient Indian traditions, the concept is most commonly associated with Buddhism. It was later adopted by other Indian religions, but with different meanings and description ("Moksha"), such as in the Hindu text "Bhagavad Gita" of the "Mahabharata".

Nirvana ("nibbana") literally means "blowing out" or "quenching". It is the most used as well as the earliest term to describe the soteriological goal in Buddhism: release from the cycle of rebirth ("saṃsāra"). Nirvana is part of the Third Truth on "cessation of dukkha" in the Four Noble Truths doctrine of Buddhism. It is the goal of the Noble Eightfold Path.

The Buddha is believed in the Buddhist scholastic tradition to have realized two types of nirvana, one at enlightenment, and another at his death. The first is called "sopadhishesa-nirvana" (nirvana with a remainder), the second "parinirvana" or "anupadhishesa-nirvana" (nirvana without remainder, or final nirvana).

In the Buddhist tradition, nirvana is described as the extinguishing of the "fires" that cause rebirths and associated suffering. The Buddhist texts identify these three "three fires" or "three poisons" as "raga" (greed, sensuality), "dvesha" (aversion, hate) and "avidyā" or "moha" (ignorance, delusion).

The state of nirvana is also described in Buddhism as cessation of all afflictions, cessation of all actions, cessation of rebirths and suffering that are a consequence of afflictions and actions. Liberation is described as identical to "anatta" ("anatman", non-self, lack of any self). In Buddhism, liberation is achieved when all things and beings are understood to be with no Self. Nirvana is also described as identical to achieving "sunyata" (emptiness), where there is no essence or fundamental nature in anything, and everything is empty.

In time, with the development of Buddhist doctrine, other interpretations were given, such as being an unconditioned state, a fire going out for lack of fuel, abandoning weaving ("vana") together of life after life, and the elimination of desire. However, Buddhist texts have asserted since ancient times that nirvana is more than "destruction of desire", it is "the object of the knowledge" of the Buddhist path.

The most ancient texts of Hinduism such as the Vedas and early Upanishads don't mention the soteriological term "Nirvana". This term is found in texts such as the Bhagavad Gita and the Nirvana Upanishad, likely composed in the post-Buddha era. The concept of Nirvana is described differently in Buddhist and Hindu literature. Hinduism has the concept of Atman – the soul, self – asserted to exist in every living being, while Buddhism asserts through its "anatman" doctrine that there is no Atman in any being. Nirvana in Buddhism is "stilling mind, cessation of desires, and action" unto emptiness, states Jeaneane Fowler, while nirvana in post-Buddhist Hindu texts is also "stilling mind but not inaction" and "not emptiness", rather it is the knowledge of true Self (Atman) and the acceptance of its universality and unity with metaphysical Brahman.

The ancient soteriological concept in Hinduism is moksha, described as the liberation from the cycle of birth and death through self-knowledge and the eternal connection of Atman (soul, self) and metaphysical Brahman. Moksha is derived from the root "muc*" () which means free, let go, release, liberate; Moksha means "liberation, freedom, emancipation of the soul". In the Vedas and early Upanishads, the word mucyate () appears, which means to be set free or release - such as of a horse from its harness.

The traditions within Hinduism state that there are multiple paths ("marga") to moksha: "jnana-marga", the path of knowledge; "bhakti-marga", the path of devotion; and "karma-marga", the path of action.

The term Brahma-nirvana appears in verses 2.72 and 5.24-26 of the Bhagavad Gita. It is the state of release or liberation; the union with the Brahman. According to Easwaran, it is an experience of blissful egolessness.

According to Zaehner, Johnson and other scholars, "nirvana" in the Gita is a Buddhist term adopted by the Hindus. Zaehner states it was used in Hindu texts for the first time in the Bhagavad Gita, and that the idea therein in verse 2.71-72 to "suppress one's desires and ego" is also Buddhist. According to Johnson the term "nirvana" is borrowed from the Buddhists to confuse the Buddhists, by linking the Buddhist nirvana state to the pre-Buddhist Vedic tradition of metaphysical absolute called Brahman.

According to Mahatma Gandhi, the Hindu and Buddhist understanding of "nirvana" are different because the nirvana of the Buddhists is shunyata, emptiness, but the nirvana of the Gita means peace and that is why it is described as brahma-nirvana (oneness with Brahman).

The terms "moksa" and "nirvana" are often used interchangeably in the Jain texts.

Uttaradhyana Sutra provides an account of Sudharman – also called Gautama, and one of the disciples of Mahavira – explaining the meaning of nirvana to Kesi, a disciple of Parshva.

The term "Nirvana" (also mentioned is "parinirvana") in the thirteenth or fourtheenth century Manichaean work "The great song to Mani" and "The story of the Death of Mani", referring to the "realm of light".

The concept of liberation as "extinction of suffering", along with the idea of "sansara" as the "cycle of rebirth" is also part of Sikhism. Nirvana appears in Sikh texts as the term "Nirban". However, the more common term is "Mukti" or "Moksh", a salvation concept wherein loving devotion to God is emphasized for liberation from endless cycle of rebirths.





</doc>
<doc id="21238" url="https://en.wikipedia.org/wiki?curid=21238" title="Neva">
Neva

The Neva (, ; ) is a river in northwestern Russia flowing from Lake Ladoga through the western part of Leningrad Oblast (historical region of Ingria) to the Neva Bay of the Gulf of Finland. Despite its modest length of , it is the fourth largest river in Europe in terms of average discharge (after the Volga, the Danube and the Rhine).

The Neva is the only river flowing from Lake Ladoga. It flows through the city of Saint Petersburg, three smaller towns of Shlisselburg, Kirovsk and Otradnoye, and dozens of settlements. The river is navigable throughout and is part of the Volga–Baltic Waterway and White Sea–Baltic Canal. It is a site of numerous major historical events, including the Battle of the Neva in 1240 which gave Alexander Nevsky his name, the founding of Saint Petersburg in 1703, and the Siege of Leningrad by the German army during World War II. The Neva river played a vital role in trade between Byzantium and Scandinavia.

The area of the Neva river was originally inhabited by Finnic people. The word "neva" is widely spread in Finnic languages with similar meanings. In Finnish ("neva") it means poor fen, in Karelian ("neva") watercourse and in Estonian ("nõva") waterway. 

It has also been argued that the name derives from the Indo-European adjective "newā" which means new. The river began to flow around 1350 BC. However, the place names of the area don't support any Indo-European influence in the area before Scandinavian traders and Slavs started to enter the region in the 8th century CE.

In the Paleozoic, 300–400 million years ago, the entire territory of the modern delta of the Neva River was covered by a sea. Modern relief was formed as a result of glacier activity. Its retreat formed the Littorina Sea, the water level of which was some higher than the present level of the Baltic Sea. Then, the Tosna was flowing in the modern bed of the Neva, from east to west into the Litorinal Sea. In the north of the Karelian Isthmus, the Littorina Sea united by a wide strait with Lake Ladoga. The Mga then flowed to the east, into Lake Ladoga, near the modern source of the Neva; the Mga then was separated from the basin of the Tosna.

Near the modern Lake Ladoga, land rose faster, and a closed reservoir was formed. Its water level began to rise, eventually flooded the valley of Mga and broke into the valley of the river Tosna. The Ivanovo rapids of the modern Neva were created in the breakthrough area. So about 2000 BC the Neva was created with its tributaries Tosna and Mga. According to some newer data, it happened at 1410–1250 BC making the Neva a rather young river. The valley of Neva is formed by glacial and post-glacial sediments and it did not change much over the past 2500 years. The delta of Neva was formed at that time, which is actually pseudodelta, as it was formed not by accumulation of river material but by plunging into the past sediments.

The Neva flows out of Lake Ladoga near Shlisselburg, flows through the Neva Lowland and discharges into the Baltic Sea in the Gulf of Finland. It has a length of , and the shortest distance from the source to the mouth is . The river banks are low and steep, on average about and at the mouth. There are three sharp turns: the Ivanovskye rapids, at Nevsky Forest Park of the Ust-Slavyanka region (the so-called crooked knee) and near the Smolny Institute, below the mouth of the river Okhta. The river declines in elevation between source and mouth. At one point the river crosses a moraine ridge and forms the Ivanovskye rapids. There, at the beginning of the rapids, is the narrowest part of the river: . The average flow rate in the rapids is about . The average width along the river is . The widest places, at , are in the delta, near the gates of the marine trading port, at the end of the Ivanovskye rapids near the confluence of the river Tosna, and near the island Fabrinchny near the source. The average depth is ; the maximum of is reached above the Liteyny Bridge, and the minimum of is in Ivanovskye rapids.

In the area of Neva basin, rainfall greatly exceeds evaporation; the latter accounts for only 37.7 percent of the water consumption from Neva and the remaining 62.3 percent is water runoff. Since 1859, the largest volume of was observed in 1924 and the lowest in 1900 at . The average annual discharge is or on average. Because of the uniform water-flow from Lake Ladoga to the Neva over the whole year, there are almost no floods and corresponding water rise in the spring. The Neva freezes throughout from early December to early April. The ice thickness is within Saint Petersburg and in other areas. Ice congestion may form in winter in the upper reaches of the river, this sometimes causes upstream floods. Of the total ice volume of Lake Ladoga, , less than 5 percent enters the Neva. The average summer water temperature is , and the swimming season lasts only about 1.5 months. The water is fresh, with medium turbidity; the average salinity is 61.3 mg/L and the calcium bicarbonate content is 7 mg/L.

The basin area of Neva is 5,000 km², including the pools of Lake Ladoga and Onega (281,000 km²). The basin contains 26,300 lakes and has a complex hydrological network of more than 48,300 rivers, however only 26 flow directly into Neva. The main tributaries are Mga, Tosna, Izhora, Slavyanka and Murzinka on the left, and Okhta and Chyornaya Rechka on the right side of Neva.

The hydrological network had been altered by the development of St. Petersburg through its entire history. When it was founded in 1703, the area was low and swampy and required construction of canals and ponds for drainage. The earth excavated during their construction was used to raise the city. At the end of the 19th century, the delta of Neva consisted of 48 rivers and canals and 101 islands. The most significant distributaries of the delta are listed in the table. Before construction of the Obvodny Canal, the left tributary of that area was the Volkovka; its part at the confluence is now called Monastyrka. The Ladoga Canal starts at the root of Neva and connects it along the southern coast of Lake Ladoga with the Volkhov.

Some canals of the delta were filled over time, so that only 42 islands remained by 1972, all within the city limits of St. Petersburg. The largest islands are Vasilyevsky at , Petrogradsky at , Krestovsky at , and Dekabristov at ; others include Zayachy, Yelagin and Kamenny Islands. At the source of the Neva, near Shlisselburg, there are the two small islands of Orekhovy and Fabrichny. Island Glavryba lies up the river, above the town of Otradnoye.

There is almost no aquatic vegetation in Neva. The river banks mostly consist of sand, podsol, gleysols, peat and boggy peat soils. Several centuries ago, the whole territory of the Neva lowland was covered by pine and spruce mossy forests. They were gradually reduced by the fires and cutting for technical needs. Extensive damage was caused during World War II: in St. Petersburg, the forests were reduced completely, and in the upper reaches down to 40 to 50 percent. Forest were replanted after the war with spruce, pine, cedar, Siberian larch, oak, Norway maple, elm, America, ash, apple tree, mountain ash and other species. The shrubs include barberry, lilac, jasmine, hazel, honeysuckle, hawthorn, rose hip, viburnum, juniper, elder, shadbush and many others. 

Nowadays, the upper regions of the river are dominated by birch and pine-birch grass-shrub forests and in the middle regions there are swampy pine forests. In St. Petersburg, along the Neva, there are many gardens and parks, including the Summer Garden, Field of Mars, Rumyantsev, Smolny, Alexander Gardens, Garden of the Alexander Nevsky Lavra and many others.

Because of the rapid flow, cold water and lack of quiet pools and aquatic vegetation the diversity of fish species in Neva is small. Permanent residents include such undemanding to environment species as perch, ruffe and roaches. Many fish species are transitory, of which commercial value have smelt, vendace and partly salmon.

Floods in St. Petersburg are usually caused by the overflow of the delta of Neva and by surging water in the eastern part of Neva Bay. They are registered when the water rises above with respect to a gauge at the Mining Institute. More than 300 floods occurred after the city was founded in 1703. Three of them were catastrophic: on 7 November 1824, when water rose to ; on 23 September 1924 when it reached , and 10 September 1777 when it rose to . However, a much larger flood of was described in 1691.

Besides flooding as a result of tidal waves, in 1903, 1921 and 1956 floods were caused by the melting of snow.

The Federal Service for Hydrometeorology and Environmental Monitoring of Russia classifies the Neva as a "heavily polluted" river. The main pollutants include copper, zinc, manganese, nitrites and nitrogen. The dirtiest tributaries of the Neva are the Mga, Slavyanka, Ohta and Chernaya. Hundreds of factories pour wastewater into the Neva within St. Petersburg, and petroleum is regularly transported along the river. The annual influx of pollutants is 80,000 tonnes, and the heaviest polluters are Power-and-heating Plant 2 (), "Plastpolymer" and "Obukhov State Plant". The biggest polluters in the Leningrad Oblast are the cities of Shlisselburg, Kirovsk and Otradnoye, as well as the Kirov thermal power station. More than 40 oil spills are registered on the river every year. In 2008 the Federal Service of St. Petersburg announced that no beach of the Neva was fit for swimming.

Cleaning of wastewater in St. Petersburg started in 1979; by 1997 about 74% of wastewater was purified. This proportion rose to 85% in 2005, to 91.7% by 2008, and Feliks Karamzinov expected it to reach almost 100% by 2011 with the completion of the expansion of the main sewerage plant.

Many sites of ancient people, up to nine thousand years old, were found within the territory of the Neva basin. It is believed that around twelve thousand years BC, Finno-Ugric peoples (Votes and Izhorians) moved to this area from the Ural Mountains.

In the 8th and 9th centuries AD, the area was inhabited by the East Slavs who were mainly engaged in slash and burn agriculture, hunting and fishing. From the 8th to 13th centuries, Neva provided a waterway from Scandinavia to the Byzantine Empire. In the 9th century, the area belonged to Veliky Novgorod. Neva is already mentioned in the Life of Alexander Nevsky (13th century). At that time, Veliky Novgorod was engaged in nearly constant wars with Sweden. A major battle occurred on 15 July 1240 at the confluence of the Izhora and Neva Rivers. The Russian army, led by the 20-year-old Prince Alexander Yaroslavich, aimed to stop the planned Swedish invasion. The Swedish army was defeated; the prince showed personal courage in combat and received the honorary name of "Nevsky".

As a result of the Russian defeat in the Ingrian War of 1610–17 and the concomitant Treaty of Stolbovo, the area of the Neva River became part of Swedish Ingria. Beginning in 1642, the capital of Ingria was Nyen, a city near the Nyenschantz fortress. Because of financial and religious oppression, much of the Orthodox population left the Neva region, emptying 60 percent of the villages by 1620. The abandoned areas became populated by people from the Karelian Isthmus and Savonia.

As a result of the Great Northern War of 1700–21, the valley of Neva River became part of Russian Empire. On 16 May 1703, the city of St. Petersburg was founded in the mouth of Neva and became capital of Russia in 1712. Neva became the central part of the city. It was cleaned, intersected with canals and enclosed with embankments. In 1715, construction began of the first wooden embankment between the Admiralty building and the Summer Garden. In the early 1760s works started to cover it in granite and to build bridges across Neva and its canals and tributaries, such as the Hermitage Bridge.

From 1727 to 1916, the temporary Isaakievsky pontoon bridge was early constructed between the modern Saint Isaac's Square and Vasilievsky Island. A similar, but much longer Trinity pontoon bridge, which spanned , was brought from the Summer Garden to Petrogradsky Island. The first permanent bridge across Neva, Blagoveshchensky Bridge, was opened in 1850, and the second, Liteyny Bridge, came into operation in 1879.

In 1858, a "Joint-stock company St. Petersburg water supply" was established, which built the first water supply network in the city. A two-stage water purification station was constructed in 1911. The development of the sewerage system began only in 1920, after the October Revolution, and by 1941, the sewerage network was long.

Every winter from 1895 to 1910, electric tramways were laid on the ice of the river, connecting the Senate Square, Vasilievsky island, Palace Embankment and other parts of the city. The power was supplied through the rails and a top cable supported by wooden piles frozen into the ice. The service was highly successful and ran without major accidents except for a few failures in the top electrical wires. The trams ran at the speed of and could carry 20 passengers per carriage. The carriages were converted from the used horsecars. About 900,000 passengers were transported over a regular season between 20 January and 21 March. The sparking of contacts at the top wires amused spectators in the night.
The first concrete bridge across Neva, the Volodarsky Bridge, was built in 1936. During World War II, from 8 September 1941 to 27 January 1944 Leningrad was in the devastating German Siege. On 30 August 1941, the German army captured Mga and came to Neva. On 8 September Germans captured Shlisselburg and cut all land communications and waterways to St. Petersburg (then Leningrad). The siege was partly relieved in January 1943, and ended on 27 January 1944.

A river station was built above the Volodarsky Bridge in 1970 which could accept 10 large ships at a time. Wastewater treatment plants were built in Krasnoselsk in 1978, on the Belyi Island in 1979–83, and in Olgino in 1987–94. The South-West Wastewater Treatment Plant was constructed in 2003–05.

Neva has very few shoals and its banks are steep, making the river suited for navigation. Utkino Backwaters were constructed in the late 19th century to park unused ships. Neva is part of the major Volga–Baltic Waterway and White Sea – Baltic Canal, however it has relatively low transport capacity because of its width, depth and bridges. Neva is available for vessels with capacity below 5,000 tonnes. Major transported goods include timber from Arkhangelsk and Vologda; apatite, granite and diabase from Kola Peninsula; cast iron and steel from Cherepovets; coal from Donetsk and Kuznetsk; pyrite from Ural; potassium chloride from Solikamsk; oil from Volga region. There are also many passenger routes to Moscow, Astrakhan, Rostov, Perm, Nizhny Novgorod, Valaam and other destinations. Navigation season on the Neva River runs from late April to November.

To the west of Shlisselburg, an oil pipeline runs under the river. The pipeline is part of the Baltic Pipeline System, which provides oil from Timan-Pechora plate, West Siberia, Ural, Kazakhstan and Primorsk to the Gulf of Finland. The long pipeline lies below the river bottom and delivers about 42 million tonnes of oil a year.

Near the Ladozhsky Bridge there is an underwater tunnel to host a gas pipeline Nord Stream. The tunnel has a diameter of and a length of and is laid at a maximum depth of .

Neva is the main source of water (96 percent) of St. Petersburg and its suburbs. From 26 June 2009, St. Petersburg started processing the drinking water by ultraviolet light, abandoning the use of chlorine for disinfection. Neva also has developed fishery, both commercial and recreational.

Leningrad Oblast:
St. Petersburg:
St. Petersburg, Neva delta

Construction of the Novo-Admiralteisky Bridge, a movable drawbridge across the river, has been approved, but will not commence before 2011.

Whereas most tourist attractions of Neva are located within St. Petersburg, there are several historical places upstream, in the Leningrad Oblast. They include the fortress Oreshek, which was built in 1323 on the Orekhovy Island at the source of Neva River, south-west of the Petrokrepost Bay, near the city of Shlisselburg. The waterfront of Schlisselburg has a monument of Peter I. In the city, there are Blagoveshchensky Cathedral (1764–95) and a still functioning Orthodox church of St. Nicholas, built in 1739. On the river bank stands the Church of the Intercession. Raised in 2007, it is a wooden replica of a historical church which stood on the southern shore of Lake Onega. That church was constructed in 1708 and it burned down in 1963. It is believed to be the forerunner of the famous Kizhi Pogost.

Old Ladoga Canal, built in the first half of the 18th century, is a water transport route along the shore of Lake Ladoga which is connecting the River Volkhov and Neva. Some of its historical structures are preserved, such as a four-chamber granite sluice (1836) and a bridge (1832).

On 21 August 1963 a Soviet twinjet Tu-124 airliner performed an emergency water landing on Neva near the Finland Railway Bridge. The plane took off from Tallinn-Ülemiste Airport (TLL) at 08:55 on 21 August 1963 with 45 passengers and seven crew on board and was scheduled to land at Moscow-Vnukovo (VKO). After liftoff, the crew noticed that the nose gear undercarriage did not retract, and the ground control diverted the flight to Leningrad (LED) because of fog at Tallinn. While circling above St. Petersburg at the altitude of at , under unclear circumstances (lack of fuel was one of the factors), both engines stalled. The crew performed emergency landing on the Neva River, barely missing some of its bridges and an 1898-built steam tugboat. The tugboat rushed to the plane and towed it to the shore. No casualties were sustained at any stage. The plane captain was first fired from job but then restored and awarded with the Order of the Red Star.



</doc>
