<doc id="21240" url="https://en.wikipedia.org/wiki?curid=21240" title="Nissan">
Nissan

, is a Japanese multinational automobile manufacturer headquartered in Nishi-ku, Yokohama. The company sells its cars under the Nissan, Infiniti, and Datsun brands with in-house performance tuning products labelled Nismo. The company traces its name to the Nissan "zaibatsu", now called Nissan Group.

Since 1999, Nissan has been part of the Renault–Nissan–Mitsubishi Alliance (Mitsubishi joining in 2016), a partnership between Nissan and Mitsubishi Motors of Japan, with Renault of France. As of 2013, Renault holds a 43.4% voting stake in Nissan, while Nissan holds a 15% non-voting stake in Renault. From October 2016 onwards, Nissan holds a 34% controlling stake in Mitsubishi Motors.

In 2013, Nissan was the sixth largest automaker in the world, after Toyota, General Motors, Volkswagen Group, Hyundai Motor Group, and Ford. Taken together, the Renault–Nissan Alliance would be the world's fourth largest automaker. Nissan is the leading Japanese brand in China, Russia and Mexico.

In 2014, Nissan was the largest car manufacturer in North America.

Nissan is the world's largest electric vehicle (EV) manufacturer, with global sales of more than 320,000 all-electric vehicles as of April 2018. The top-selling vehicle of the car-maker's fully electric lineup is the Nissan LEAF, an all-electric car and the world's top-selling highway-capable plug-in electric car in history.

In January 2018, Nissan CEO Hiroto Saikawa announced that all Infiniti vehicles launched from 2021 will be hybrid vehicles or all-electric vehicles.

Masujiro Hashimoto founded the on 1 July 1911 in Azabu-Hiroo district of Tokyo. In 1914, the company produced its first car, called the DAT.

The new car's model name was an acronym of the company's investors' surnames:

It was renamed to Kaishinsha Motorcar Co., Ltd. in 1918, and again to DAT Jidosha & Co., Ltd. (DAT Motorcar Co.) in 1925. DAT Motors built trucks in addition to the DAT and Datsun passenger cars. The vast majority of its output were trucks, due to an almost non- existent consumer market for passenger cars at the time, and disaster recovery efforts as a result of the 1923 Great Kantō earthquake. Beginning in 1918, the first DAT trucks were produced for the military market. At the same time, Jitsuyo Jidosha Co., Ltd. (jitsuyo means practical use or utility) produced small trucks using parts, and materials imported from the United States.

Commercial operations were placed on hold during Japan's participation in World War I, and the company contributed to the war effort.

In 1926 the Tokyo-based DAT Motors merged with the Osaka-based a.k.a. Jitsuyo Jidosha Seizo (established 1919 as a Kubota subsidiary) to become in Osaka until 1932. From 1923 to 1925, the company produced light cars and trucks under the name of Lila.

In 1931, DAT came out with a new smaller car, called the Datsun Type 11, the first "Datson", meaning "Son of DAT". Later in 1933 after Nissan Group "zaibatsu" took control of DAT Motors, the last syllable of Datson was changed to "sun", because "son" also means "loss" in Japanese, hence the name .

In 1933, the company name was Nipponized to and was moved to Yokohama.

In 1928, Yoshisuke Aikawa (nickname: Gisuke/Guisuke Ayukawa) founded the holding company Nihon Sangyo (日本産業 Japan Industries or Nihon Industries). The name 'Nissan' originated during the 1930s as an abbreviation used on the Tokyo Stock Exchange for Nihon Sangyo. This company was Nissan "Zaibatsu" which included Tobata Casting and Hitachi. At this time Nissan controlled foundries and auto parts businesses, but Aikawa did not enter automobile manufacturing until 1933.

The zaibatsu eventually grew to include 74 firms and became the fourth-largest in Japan during World War II.

In 1931, DAT Jidosha Seizo became affiliated with Tobata Casting and was merged into Tobata Casting in 1933. As Tobata Casting was a Nissan company, this was the beginning of Nissan's automobile manufacturing.

In 1934, Aikawa separated the expanded automobile parts division of Tobata Casting and incorporated it as a new subsidiary, which he named . The shareholders of the new company however were not enthusiastic about the prospects of the automobile in Japan, so Aikawa bought out all the Tobata Casting shareholders (using capital from Nihon Industries) in June 1934. At this time, Nissan Motor effectively became owned by Nihon Sangyo and Hitachi.

In 1935, construction of its Yokohama plant was completed. 44 Datsuns were shipped to Asia, Central and South America. In 1935, the first car manufactured by an integrated assembly system rolled off the line at the Yokohama plant. Nissan built trucks, airplanes, and engines for the Imperial Japanese Army. In November 1937 Nissan's headquarter was moved to Hsinking, the capital of Manchukuo. In December the company changed name to Manchuria Heavy Industries Developing Co (MHID).

In 1940, first knockdown kits were shipped to Dowa Jidosha Kogyo (Dowa Automobile), one of MHID's companies, for assembly. In 1944, the head office was moved to Nihonbashi, Tokyo, and the company name was changed to Nissan Heavy Industries, Ltd., which the company kept through 1949.

DAT had inherited Kubota's chief designer, American engineer William R. Gorham. This, along with Aikawa's 1908 visit to Detroit, was to greatly affect Nissan's future. Although it had always been Aikawa's intention to use cutting-edge auto making technology from America, it was Gorham that carried out the plan. Most of the machinery and processes originally came from the United States. When Nissan started to assemble larger vehicles under the "Nissan" brand in 1937, much of the design plans and plant facilities were supplied by the Graham-Paige Company. Nissan also had a Graham license under which passenger cars, buses, and trucks were made.

In David Halberstam's 1986 book "The Reckoning," Halberstam states "In terms of technology, Gorham was the founder of the Nissan Motor Company" and that "young Nissan engineers who had never met him spoke of him as a god and could describe in detail his years at the company and his many inventions."

From 1934 Datsun began to build Austin 7s under license. This operation became the greatest success of 's overseas licensing of its Seven and marked the beginning of Datsun's international success.

In 1952, Nissan entered into a legal agreement with Austin, for Nissan to assemble 2,000 Austins from imported partially assembled sets and sell them in Japan under the Austin trademark. The agreement called for Nissan to make all Austin parts locally within three years, a goal Nissan met. Nissan produced and marketed Austins for seven years. The agreement also gave Nissan the rights to use Austin patents, which Nissan used in developing its own engines for its Datsun line of cars. In 1953, British-built Austins were assembled and sold, but by 1955, the Austin A50 – completely built by Nissan and featuring a new 1489 cc engine—was on the market in Japan. Nissan produced 20,855 Austins from 1953 to 1959.

Nissan leveraged the Austin patents to further develop their own modern engine designs past what the Austin's A- and B-family designs offered. The apex of the Austin-derived engines was the new design A series engine in 1966. In 1967, Nissan introduced its new highly advanced four-cylinder overhead cam (OHC) Nissan L engine, which while similar to Mercedes-Benz OHC designs was a totally new engine designed by Nissan. This engine powered the new Datsun 510, which gained Nissan respect in the worldwide sedan market. Then, in 1969 Nissan introduced the Datsun 240Z sports car which used a six-cylinder variation of the L series engine, developed under Nissan Machinery (Nissan Koki Co., Ltd. ) in 1964, a former remnant of another auto manufacturer Kurogane. The 240Z was an immediate sensation and lifted Nissan to world-class status in the automobile market.

During the Korean War, Nissan was a major vehicle producer for the U.S. Army. After the Korean War ended, significant levels of anti-communist sentiment existed in Japan. The union that organized Nissan's workforce was strong and militant. Nissan was in financial difficulties, and when wage negotiations came, the company took a hard line. Workers were locked out, and several hundred were fired. The Japanese government and the U.S. occupation forces arrested several union leaders. The union ran out of strike funds and was defeated. A new labor union was formed, with Shioji Ichiro one of its leaders. Ichiro had studied at Harvard University on a U.S. government scholarship. He advanced an idea to trade wage cuts against saving 2,000 jobs. Ichiro's idea was made part of a new union contract that prioritized productivity. Between 1955 and 1973, Nissan "expanded rapidly on the basis of technical advances supported – and often suggested – by the union." Ichiro became president of the Confederation of Japan Automobile Workers Unions and "the most influential figure in the right wing of the Japanese labor movement."

In 1966, Nissan merged with the Prince Motor Company, bringing more upmarket cars, including the Skyline and Gloria, into its selection. The Prince name was eventually abandoned, and successive Skylines and Glorias bore the Nissan name. "Prince," was used at the Japanese Nissan dealership "Nissan Prince Shop" until 1999, when "Nissan Red Stage" replaced it. Nissan Red Stage itself has been replaced as of 2007. The Skyline lives on as the G Series of Infiniti.

To capitalize on the renewed investment during 1964 Summer Olympics, Nissan established the gallery on the second and third floors of the San-ai building, located in Ginza, Tokyo. To attract visitors, Nissan started using beautiful female showroom attendants where Nissan held a competition to choose five candidates as the first class of Nissan Miss Fairladys, modeled after "Datsun Demonstrators" from the 1930s who introduced cars. The Fairlady name was used as a link to the popular Broadway play "My Fair Lady" of the era. Miss Fairladys became the marketers of the Datsun Fairlady 1500.

In April 2008, 14 more Miss Fairlady candidates were added, for a total of 45 Nissan Miss Fairlady pageants (22 in Ginza, 8 in Sapporo, 7 in Nagoya, 7 in Fukuoka).

In April 2012, 7 more Miss Fairlady candidates were added, for a total of 48 Nissan Miss Fairlady pageants (26 in Ginza, 8 in Sapporo, 7 in Nagoya, 7 in Fukuoka).

In April 2013, 6 more Miss Fairlady candidates were added to Ginza showroom, for a total of 27 48th Ginza Nissan Miss Fairlady pageants.

In the 1950s, Nissan decided to expand into worldwide markets. Nissan management realized their Datsun small car line would fill an unmet need in markets such as Australia and the world's largest car market, the United States. They first showed the Datsun Bluebird at the 1958 Los Angeles Auto Show. The company formed a U.S. subsidiary, Nissan Motor Corporation U.S.A., in Gardena, California in 1960, headed by Yutaka Katayama. Nissan continued to improve their sedans with the latest technological advancements and chic Italianate styling in sporty cars such as the Datsun Fairlady roadsters, the race-winning 411 series, the Datsun 510 and the Datsun 240Z. By 1970, Nissan had become one of the world's largest exporters of automobiles.

In the wake of the 1973 oil crisis, consumers worldwide (especially in the lucrative U.S. market) began turning to high-quality small economy cars. To meet the growing demand for its new Nissan Sunny, the company built new factories in Mexico (Nissan Mexicana was established in the early 1960s and commenced manufacturing since 1966 at their Cuernavaca assembly facility, making it their first North American assembly plant), Australia, New Zealand, Taiwan, United States (Nissan Motor Manufacturing Corporation USA was established in 1980) and South Africa. The "Chicken Tax" of 1964 placed a 25% tax on commercial vans imported to the United States. In response, Nissan, Toyota Motor Corp. and Honda Motor Co. began building plants in the U.S. in the early 1980s. Nissan's initial assembly plant Smyrna assembly plant (which broke ground in 1980) at first built only trucks such as the 720 and Hardbody, but has since expanded to produce several car and SUV lines, including the Altima, Maxima, Rogue, Pathfinder, Infiniti QX60 and LEAF all-electric car. The addition of mass-market automobiles was in response to the 1981 Voluntary Export Restraints imposed by the U.S. Government. An engine plant in Decherd, Tennessee followed, most recently a second assembly plant was established in Canton, Mississippi. In 1970, Teocar was created, which was a Greek assembly plant created in cooperation with Theoharakis. It was situated in Volos, Greece and its geographical location was perfect as the city had a major port. The plant started production in 1980, assembling Datsun pick-up trucks and continuing with the Nissan Cherry and Sunny automobiles. Until May 1995 170,000 vehicles were made, mainly for Greece.

By the early 1980s, Nissan (Datsun) had long been the best selling Japanese brand in Europe. In order to overcome export tariffs and delivery costs to its European customers, Nissan contemplated establishing a plant in Europe. Nissan tried to convert the Greek plant into one manufacturing cars for all European countries. However, due to issues with the Greek government not only did that not happen but the plant itself was closed. A joint venture with Italy's then state-owned Alfa Romeo was also entered in 1980, leading to Italian production of the Nissan Cherry and an Alfa-badged version, the Alfa Romeo Arna. After an extensive review, Nissan decided to go it alone instead. The City of Sunderland in the north east of England was chosen for its skilled workforce and its location near major ports. The plant was completed in 1986 as the subsidiary Nissan Motor Manufacturing (UK) Ltd. By 2007, it was producing 400,000 vehicles per year, landing it the title of the most productive plant in Europe.

In 2001, Nissan established a manufacturing plant in Brazil. In 2005, Nissan added operations in India, through its subsidiary Nissan Motor India Pvt. Ltd. With its global alliance partner, Renault, Nissan invested $990 million to set up a manufacturing facility in Chennai, catering to the Indian market as well as a base for exports of small cars to Europe.
Nissan entered the Middle East market in 1957 when it sold its first car in Saudi Arabia. Nissan sold nearly 520,000 new vehicles in China in 2009 in a joint venture with Dongfeng Motor. To meet increased production targets, Dongfeng-Nissan expanded its production base in Guangzhou, which would become Nissan's largest factory around the globe in terms of production capacity. Nissan also has moved and expanded its Nissan Americas Inc. headquarters, moving from Los Angeles to Franklin, Tennessee in the Nashville area.

In June 2001, Carlos Ghosn was named chief executive officer of Nissan. In May 2005, Ghosn was named president of Nissan's partner company Renault. He was appointed president and CEO of Renault on 6 May 2009. Nissan's management is a trans-cultural, diverse team. In February 2017, Ghosn announced he would step down as CEO of Nissan on 1 April 2017, while remaining chairman of the company. He was replaced as CEO by his then-deputy Hiroto Saikawa. On 19 November 2018, Ghosn was fired as chairman following his arrest for the alleged underreporting of his income to Japanese financial authorities. After 108 days in detention, Ghosn was released on bail, but after 29 days he was again detained on new charges (4 April 2019). He had been due to hold a news conference, but instead, his lawyers released a video of Ghosn alleging this 2018-2019 Nissan scandal is itself evidence of value destruction and Nissan corporate mismanagement. In September 2019, Saikawa resigned as CEO, following allegations of improper payments received by him. Yasuhiro Yamauchi was appointed as acting CEO. In October 2019, the company announced it had appointed Makoto Uchida as its next CEO. The appointment would be made "effective" by 1 January 2020 at the latest. On 1 December 2019, Uchida became CEO.

In the U.S., Nissan has been increasing its reliance on sales to daily-rental companies like Enterprise Rent-A-Car or Hertz. In 2016, Nissan's rental sales jumped 37% and in 2017 Nissan became the only major automaker to boost rental sales when the Detroit Three cut back less profitable deliveries to daily-rental companies, which traditionally are the biggest customers of domestic automakers.

In late July 2019, Nissan announced it would lay off 12,500 employees over the next 3 years, citing a 95% year on year net income fall. Hiroto Saikawa, CEO at the time, confirmed the majority of those cuts would be plant workers.

Nissan's first final assembly robots were installed in the Murayama plant (where the then-new March/Micra was assembled) in 1982. In 1984 the Zama plant began to be robotized; this automation process then continued throughout Nissan's factories.

Nissan electric vehicles have been produced intermittently since 1946. In 2010 the Nissan Leaf plug-in battery electric vehicle was introduced; it was the world's most sold plug-in electric car for nearly a decade. It was preceded by the Altra and the Hypermini. Until surpassed by Tesla, Nissan was the world's largest electric vehicle (EV) manufacturer, with global sales of more than 320,000 all-electric vehicles as of April 2018.

Luxgen and Nissan partner to assemble vehicles in the Philippines with its affiliate Nissan Motor Philippines Inc. (NMPI).

In Australia, between 1989 and 1992, Nissan Australia shared models with Ford Australia under a government-backed rationalisation scheme known as the Button Plan, with a version of the Nissan Pintara being sold as the Ford Corsair and a version of the Ford Falcon as the Nissan Ute. A variant of the Nissan Patrol was sold as the Ford Maverick during the 1988–94 model years.

In North America, Nissan partnered with Ford from 1993 to 2002 to market the Ohio built Mercury Villager and the Nissan Quest. The two minivans were virtually identical aside from cosmetic differences. In 2002, Nissan and Ford announced the discontinuation of the arrangement.

In Europe, Nissan and Ford Europe partnered to produce the Nissan Terrano II and the badge-engineered Ford Maverick, a mid-size SUV produced at the Nissan Motor Ibérica S.A (NMISA) plant in Barcelona, Spain. The Maverick/Terrano II was a popular vehicle sold throughout Europe and Australasia. It was also sold in Japan as a captive import, with the Nissan model marketed as the Nissan Mistral.

Nissan licensed the Volkswagen Santana. Production began in 1984, at Nissan's Zama, Kanagawa, and ended in May 1990.

From 1983 to 1987, Nissan cooperated with Alfa Romeo to build the Arna. The goal was for Alfa to compete in the family hatchback market segment, and for Nissan to establish a foothold in the European market. After Alfa Romeo's takeover by Fiat, both the car and cooperation were discontinued.

In Europe, GM and Nissan co-operated on the Light Commercial vehicle the Nissan Primastar. The high roof version is built in the NMISA plant in Barcelona, Spain; while the low roof version is built at Vauxhall Motors/Opel's Luton plant in Bedfordshire, UK

In 2013, GM announced its intentions to rebadge the Nissan NV200 commercial van as the 2015 model year Chevrolet City Express, to be introduced by end of 2014. Holden, GM's Australian subsidiary, sold versions of the Nissan Pulsar as the Holden Astra between 1984 and 1989.

LDV Group sold a badge engineered light commercial vehicle version of the Nissan Serena as the LDV Cub from 1996 to 2001. The Nissan equivalent was marketed as the Nissan Vannette Cargo.

In 1999, facing severe financial difficulties, Nissan entered an alliance with Renault S.A. of France.

Signed on 27 March 1999, the "Renault-Nissan Alliance" was the first of its kind involving a Japanese and French car manufacturer, each with its own distinct corporate culture and brand identity. In the spring of 2000, Yanase, Japan's premier seller of imported automobiles, cancelled its licensing contract with Renault, and Nissan took over as the sole licensee.

The Renault-Nissan Alliance has evolved over the years to Renault holding 43.4% of Nissan shares, while Nissan holds 15% of Renault shares. The alliance itself is incorporated as the Renault-Nissan B.V., founded on 28 March 2002 under Dutch law. Renault-Nissan B.V. is equally owned by Renault and Nissan.

Under CEO Ghosn's "Nissan Revival Plan" (NRP), the company has rebounded in what many leading economists consider to be one of the most spectacular corporate turnarounds in history, catapulting Nissan to record profits and a dramatic revitalization of both its Nissan and Infiniti model line-ups. Ghosn has been recognized in Japan for the company's turnaround in the midst of an ailing Japanese economy. Ghosn and the Nissan turnaround were featured in Japanese manga and popular culture. His achievements in revitalizing Nissan were noted by the Japanese Government, which awarded him the Japan Medal with Blue Ribbon in 2004.

On 7 April 2010, Daimler AG exchanged a 3.9% share of its holdings for 3.9% from both Nissan and Renault. This triple alliance allows for the increased sharing of technology and development costs, encouraging global cooperation and mutual development.

On 12 December 2012, the Renault–Nissan Alliance formed a joint venture with Russian Technologies (Alliance Rostec Auto BV) with the aim of becoming the long-term controlling shareholder of AvtoVAZ, Russia's largest car company and owner of the country's biggest selling brand, Lada. The takeover was completed in June 2014, and the two companies of the Renault-Nissan Alliance took a combined 67.1% stake of Alliance Rostec, which in turn acquired a 74.5% of AvtoVAZ, thereby giving Renault and Nissan indirect control over the Russian manufacturer. Ghosn was appointed chairman of the board of AvtoVAZ on 27 June 2013.

Taken together, the Renault–Nissan Alliance sells one in ten cars worldwide, and would be the world's fourth largest automaker with 2013 sales of 8,266,098 units.


Presidents and chief executive officers of Nissan:

"Nissan:" Nissan's volume models are sold worldwide under the Nissan brand.

"Datsun:" Until 1983, Nissan automobiles in most export markets were sold under the Datsun brand. In 1984 the Datsun brand was phased out and the Nissan brand was phased in. All cars in 1984 had both the Datsun and Nissan branding on them and in 1985 the Datsun name was completely dropped. In July 2013, Nissan announced the relaunch of Datsun as a brand targeted at emerging markets.

"Infiniti:" Since 1989, Nissan has sold its luxury models under the Infiniti brand. In 2012, Infiniti changed its headquarters to Hong Kong, where it is incorporated as Infiniti Global Limited. Its president is former BMW executive Roland Krueger. From 2014, Infiniti cars are sold in Japan.

"Nismo:" Nissan's in-house tuning shop is Nismo, short for "Nissan Motorsport International Limited." Nismo is being re-positioned as Nissan's performance brand.

For many years, Nissan used a red wordmark for the company, and car "badges" for the "Nissan" and "Infiniti" brands.

At Nissan's 2013 earnings press conference in Yokohama, Nissan unveiled "a new steel-blue logo that spells out—literally—the distinction between Nissan the company and Nissan the brand." Using a blue-gray color scheme, the new corporate logo did read NISSAN MOTOR COMPANY. Underneath were the "badge" logos for the Nissan, Infiniti and Datsun brands.

Later in 2013, the Nissan "Company" logo changed to the Nissan "Corporation" logo. The latter was the logo used by Nissan Motor Co., Ltd. up to early 2020.

In July 2020, Nissan introduced new corporate and brand logos, as part of an image revamp tied to the Ariya launch.

Nissan has produced an extensive range of mainstream cars and trucks, initially for domestic consumption but exported around the world since the 1950s.

It also produced several memorable sports cars, including the Datsun Fairlady 1500, 1600 and 2000 Roadsters, the Z-car, an affordable sports car originally introduced in 1969; and the GT-R, a powerful all-wheel-drive sports coupe.

In 1985, Nissan created a tuning division, "Nismo", for competition and performance development of such cars. One of Nismo's latest models is the 370Z Nismo.

Nissan also sells a range of kei cars, mainly as a joint venture with other Japanese manufacturers like Suzuki or Mitsubishi. Until 2013, Nissan rebadged kei cars built by other manufacturers. Beginning in 2013, Nissan and Mitsubishi shared the development of the Nissan DAYZ / Mitsubishi eK Wagon series. Nissan also has shared model development of Japanese domestic cars with other manufacturers, particularly Mazda, Subaru, Suzuki and Isuzu.

In China, Nissan produces cars in association with the Dongfeng Motor Group including the 2006 Nissan Livina Geniss, the first in a range of a new worldwide family of a medium-sized car.

In 2010, Nissan created another tuning division, "IPL", this time for their premium/luxury brand Infiniti.

In 2011, after Nissan released the Nissan NV-Series in the United States, Canada, and Mexico, Nissan created a commercial sub-brand called "Nissan Commercial Vehicles" which focuses on commercial vans, pickup trucks, and fleet vehicles for the US, Canadian, and Mexican Markets.

In 2013, Nissan launched the Qashqai SUV in South Africa, along with their new motorsport Qashqai Car Games. It is the same year when the Datsun brand was relaunched by Nissan after a 27-year hiatus.

Nissan launched their Nissan Intelligent Mobility vision in 2016 by revealing the IDS Concept at the 2016 Geneva Motor Show. Most Nissan vehicles like the Dayz, Rogue and Leaf are equipped with Nissan Intelligent Mobility technology.

In 2018, Nissan launched the sixth-generation Altima at the 2018 New York Auto Show.

In May 2020, Nissan announced that the company would cut production capacity by 20% due to the Covid-19 pandemic. The company announced it would shut down factories in Indonesia and Spain, and would exit the South Korean car market. Nissan announced that the Infiniti brand will be pulled out as well alongside the Nissan brand by December due to worsening business environment amidst the pandemic.

As of 2007 in Japan, Nissan sells its products with internationally recognized "Nissan" signage, using a chrome circle with "Nissan" across the front.

Previously, Nissan used two dealership names called , , and , established in 1999 after the merger with Renault.

Nissan Red Stage was the result of combining an older sales channel of dealerships under the names , established in 1966 after the merger of Prince Motors by Nissan, which sold the Nissan Skyline, and , which sold cars developed from the Nissan Sunny at its introduction in 1966. The word "satio" is Latin, which means "ample" or "sufficient". was briefly known previously as "Nissan Cony Store" when they assumed operations of a small "kei" manufacturer called Aichi Machine Industry Co., Ltd. (愛知機械工業) who manufactured the "", "Guppy" and brand of "kei" cars and trucks until 1970, when the network was renamed for the Nissan Cherry.

Nissan Blue Stage was the result of combining older sales channels, called in 1955, then renamed "Nissan Bluebird Store" in 1966, selling Nissan's original post-war products called the Datsun Bluebird, Datsun Sports, Datsun Truck, Datsun Cablight, Datsun Cabstar, Nissan Junior, and Nissan Cedric. was established in 1965 and offered luxury sedans like the Nissan Laurel and the Nissan President. In 1970, Nissan also set up a separate sales chain which sold used cars including auctions, called , which they still maintain.

In the early days of Nissan's dealership network, Japanese consumers were directed towards specific Nissan stores for cars that were of a specific size and pricepoint. Over time as sales progressed and the Japanese automotive industry became more prolific, vehicles that were dedicated to particular stores were badge engineered, given different names, and shared within the existing networks thereby selling the same platforms at different locations. The networks allowed Nissan to better compete with the network established earlier by Toyota at Japanese locations.
Starting in 1960, another sales distribution channel was established that sold diesel products for commercial use, called Nissan Diesel until the diesel division was sold in 2007 to Volvo AB. To encourage retail sales, Nissan passenger vehicles that were installed with diesel engines, like the Cedric, were available at Nissan Diesel locations.

All cars sold at Nissan Blue Stage (1999–2005):

All cars sold at Nissan Store (later Nissan Bluebird Store, Nissan Exhibition), Nissan Motor Store, (1955–1999):

All cars sold at Nissan Red Stage (1999–2005):

All cars sold at Nissan Prince Store, Nissan Satio Store, Nissan Cherry Store (1966–1999):

Nissan has classified several vehicles as "premium" and select dealerships offer the "Nissan Premium Factory" catalog. Vehicles in this category are:

Nissan Cabstar (日産・キャブスター "Nissan Kyabusutā") is the name used in Japan for two lines of pickup trucks and light commercial vehicles sold by Nissan and built by UD Nissan Diesel, a Volvo AB company and by Renault-Nissan Alliance for the European market. The name originated with the 1968 Datsun Cabstar, but this was gradually changed over to "Nissan" badging in the early 1980s. The lighter range (1-1.5 tons) replaced the earlier Cabstar and Homer, while the heavier Caball and Clipper were replaced by the 2–4 ton range Atlas (日産・アトラス "Nissan Atorasu"). The nameplate was first introduced in December 1981.The Cabstar is known also as the Nissan Cabstar, Renault Maxity and Samsung SV110 depending on the location. The range has been sold across the world. It shares its platform with the Nissan Caravan.

The Nissan Titan was introduced in 2004, as a full-size pickup truck produced for the North American market, the truck shares the stretched Nissan F-Alpha platform with the Nissan Armada and Infiniti QX56 SUVs. It was listed by Edmunds.com as the best full-size truck. The second generation Titan was revealed at the 2016 North American International Auto Show as a 2017 model year vehicle.

The first Cabstar (A320) appeared in March 1968, as a replacement for the earlier Datsun Cablight. It is a cab-over engine truck and was available either as a truck, light van (glazed van), or as a "route van" (bus). It uses the 1189 cc Nissan D12 engine with 56 PS (41 kW). After some modifications and the new 1.3 liter J13 engine, with 67 PS (49 kW), in August 1970 the code became A321. The Cabstar underwent another facelift with an entirely new front clip in May 1973. The 1483 cc J15 engine became standard fitment at this time (PA321), with 77 PS (57 kW) at 5200 rpm. The Cabstar was placed just beneath the slightly bigger Homer range in Nissan's commercial vehicle lineup. It received a full makeover in January 1976, although the van models were not replaced.

The F20 Nissan Homer, introduced in January 1976, was also sold as the Nissan Datsun Cabstar in Japan. Both ranges were sold with either a 1.5 (J15) or a 2.0 liter (H20) petrol inline-four or with the 2.2 liter SD22 diesel engine. The F20 received a desmogged engine range in September 1979 and with it a new chassis code, F21. Manufacturing of the heavier range (H40-series) Atlas began in December 1981, while the lighter series Atlas (F22) was introduced in February 1982 – this succeeded both the Homer and Cabstar ranges and the nameplate has not been used in the Japanese market since.

The Atlas F22 was sold in Europe as the Nissan Cabstar and proved a popular truck in the UK market due to its reliability and ability to carry weight. From 1990 the range widened and was sold as the Cabstar E. Actually (2015) the Cabstar is manufactured in the NSIO (Nissan Spanish Industrial Operations) Plant in Ávila, Spain under the brand name of NT400.

Nissan introduced its first battery electric vehicle, the Nissan Altra at the Los Angeles International Auto Show on 29 December 1997. Unveiled in 2009, the EV-11 prototype electric car was based on the Nissan Tiida (Versa in North America), with the conventional gasoline engine replaced with an all-electric drivetrain.

In 2010, Nissan introduced the Nissan LEAF as the first mass-market, all-electric vehicle launched globally. , the Nissan Leaf was the world's best selling highway-capable all-electric car ever. Global sales totaled 100,000 Leafs by mid January 2014, representing a 45% market share of worldwide pure electric vehicles sold since 2010. Global Leaf sales passed the 200,000 unit milestone in December 2015, and the Leaf continued ranking as the all-time best selling all-electric car.

Nissan's second all-electric vehicle, the Nissan e-NV200, was announced in November 2013. Series production at the Nissan Plan in Barcelona, Spain, began on 7 May 2014. The e-NV200 commercial van is based on the Nissan Leaf. Nissan plans to launch two additional battery electric vehicles by March 2017.

In June 2016, Nissan announced it will introduce its first range extender car in Japan before March 2017. The series plug-in hybrid will use a new hybrid system, dubbed e-Power, which debuted with the Nissan Gripz concept crossover showcased at the September 2015 Frankfurt Auto Show. , Nissan electric vehicles were sold in 48 world markets. Nissan global electric vehicle sales passed 275,000 units in December 2016.

The second-generation Leaf was launched by Nissan in Japan.
In August 2013 Nissan announced its plans to launch several driverless cars by 2020. The company is building a dedicated autonomous driving proving ground in Japan, to be completed in 2014. Nissan installed its autonomous car technology in a Nissan Leaf all-electric car for demonstration purposes. The car was demonstrated at Nissan 360 test drive event held in California in August 2013. In September 2013, the Leaf fitted with the prototype Advanced Driver Assistance System was granted a license plate that allows it to drive on Japanese public roads. The testing car will be used by Nissan engineers to evaluate how its in-house autonomous driving software performs in the real world. Time spent on public roads will help refine the car's software for fully automated driving. The autonomous Leaf was demonstrated on public roads for the first time at a media event held in Japan in November 2013. The Leaf drove on the Sagami Expressway in Kanagawa Prefecture, near Tokyo. Nissan vice chairman Toshiyuki Shiga and the prefecture's governor, Yuji Kuroiwa, rode in the car during the test.

Nissan has also had a number of ventures outside the automotive industry, most notably the Tu–Ka mobile phone service (est. 1994), which was sold to DDI and Japan Telecom (both now merged into KDDI) in 1999. Nissan offers a subscription-based telematics service in select vehicles to drivers in Japan, called CarWings. Nissan also owns Nissan Marine, a joint venture with Tohatsu Corp that produces motors for smaller boats and other maritime equipment.

Nissan also built solid rocket motors for orbital launch vehicles such as the Lambda 4S and M-V. The aerospace and defense division of Nissan was sold to IHI Corporation in 2000.

Nismo is the motorsports division of Nissan, founded in 1984. Nismo cars have participated in the All Japan Sports Prototype Championship, Super GT, IMSA GT Championship, World Sportscar Championship, FIA World Endurance Championship, British Touring Car Championship, Supercars Championship and Blancpain GT Series. Also, they were featured at the World Series by Nissan from 1998 to 2004.

Nissan sponsored the Los Angeles Open golf tournament from 1987 to 2007.

Beginning in 2015, Nissan became the naming rights sponsor for Nissan Stadium, the home of the Tennessee Titans and Tennessee State University football teams in Nashville. Nissan also became the official sponsor of the Heisman Trophy and UEFA Champions League.

Nissan's central research is inside the Oppama Plant site, Yokosuka, which began its operation in 1961, at the former site of Imperial Japanese Navy's Airborne Squadron base. In 1982, Nissan's technical centers in Suginami, Tokyo and Tsurumi, Yokohama were combined into one: Nissan Technical Center (NTC) in Atsugi, Kanagawa, at the foot of Mount Ōyama of the Tanzawa Mountains. At its 30th anniversary in 2012, NTC employed 9,500 employees in product development, design, production engineering, and purchasing.

Nissan Technical Center works closely with its overseas operations: Nissan Technical Center (NTC)/North America, NTC/Mexico, Nissan Design America, and Nissan Silicon Valley Office.

In 2007, the company opened Nissan Advanced Technology Center (NATC), near the NTC site. It works in close contact with the central research, the Silicon Valley office, the technical office near the Nissan headquarters in central Yokohama, and the overseas offices in Detroit, Silicon Valley, and Moscow.

Nissan's test courses are in Tochigi (two courses), Yokosuka and Hokkaido.

In mid- 2018, Nissan launched its first of many planned software and information technology development centers in Thiruvananthapuram, Kerala, India.

Data extracted from Nissan's international corporate website.



</doc>
<doc id="21241" url="https://en.wikipedia.org/wiki?curid=21241" title="Norway">
Norway

Norway (; ; ; ; ), officially the Kingdom of Norway, is a Nordic country in Northern Europe whose mainland territory comprises the western and northernmost portion of the Scandinavian Peninsula; Mainland Norway and the remote island of Jan Mayen as well as the archipelago of Svalbard form Metropolitan Norway. The subantarctic Bouvet Island is a dependent territory of the Kingdom of Norway. Norway also lays claim to the Antarctic territories of Queen Maud Land and Peter I Island.

Norway has a total area of and a population of 5,312,300 (as of August 2018). The country shares a long eastern border with Sweden (1,619 km or 1,006 mi long). Norway is bordered by Finland and Russia to the north-east, and the Skagerrak strait to the south, with Denmark on the other side. Norway has an extensive coastline, facing the North Atlantic Ocean and the Barents Sea. The maritime influence also dominates Norway's climate with mild lowland temperatures on the sea coasts, whereas the interior, while colder, is also a lot milder than areas elsewhere in the world on such northerly latitudes. Even during polar night in the north, temperatures above freezing are commonplace on the coastline. The maritime influence brings high rainfall and snowfall to some areas of the country.

Harald V of the House of Glücksburg is the current King of Norway. Erna Solberg has been prime minister since 2013 when she replaced Jens Stoltenberg. As a unitary sovereign state with a constitutional monarchy, Norway divides state power between the parliament, the cabinet and the supreme court, as determined by the 1814 constitution. The kingdom was established in 872 as a merger of many petty kingdoms and has existed continuously for 1, years. From 1537 to 1814, Norway was a part of the Kingdom of Denmark–Norway, and from 1814 to 1905, it was in a personal union with the Kingdom of Sweden. Norway was neutral during the First World War. Norway remained neutral until April 1940 when the country was invaded and occupied by Germany until the end of Second World War.

Norway has both administrative and political subdivisions on two levels: counties and municipalities. The Sámi people have a certain amount of self-determination and influence over traditional territories through the Sámi Parliament and the Finnmark Act. Norway maintains close ties with both the European Union and the United States. Norway is also a founding member of the United Nations, NATO, the European Free Trade Association, the Council of Europe, the Antarctic Treaty, and the Nordic Council; a member of the European Economic Area, the WTO, and the OECD; and a part of the Schengen Area. In addition, the Norwegian languages share mutual intelligibility with Danish and Swedish.

Norway maintains the Nordic welfare model with universal health care and a comprehensive social security system, and its values are rooted in egalitarian ideals. The Norwegian state has large ownership positions in key industrial sectors, having extensive reserves of petroleum, natural gas, minerals, lumber, seafood, and fresh water. The petroleum industry accounts for around a quarter of the country's gross domestic product (GDP). On a per-capita basis, Norway is the world's largest producer of oil and natural gas outside of the Middle East.

The country has the fourth-highest per-capita income in the world on the World Bank and IMF lists. On the CIA's GDP (PPP) per capita list (2015 estimate) which includes autonomous territories and regions, Norway ranks as number eleven. It has the world's largest sovereign wealth fund, with a value of US$1 trillion. Norway has had the highest Human Development Index ranking in the world since 2009, a position also held previously between 2001 and 2006; it also has the highest inequality-adjusted ranking per 2018. Norway ranked first on the World Happiness Report for 2017 and currently ranks first on the OECD Better Life Index, the Index of Public Integrity, and the Democracy Index. Norway also has one of the lowest crime rates in the world.

Norway has two official names: "Norge" in Bokmål and "Noreg" in Nynorsk. The English name Norway comes from the Old English word "Norþweg" mentioned in 880, meaning "northern way" or "way leading to the north", which is how the Anglo-Saxons referred to the coastline of Atlantic Norway similar to scientific consensus about the origin of the Norwegian language name. The Anglo-Saxons of Britain also referred to the kingdom of Norway in 880 as "Norðmanna land".

There is some disagreement about whether the native name of Norway originally had the same etymology as the English form. According to the traditional dominant view, the first component was originally "norðr", a cognate of English "north", so the full name was "Norðr vegr", "the way northwards", referring to the sailing route along the Norwegian coast, and contrasting with "suðrvegar" "southern way" (from Old Norse suðr) for (Germany), and "austrvegr" "eastern way" (from austr) for the Baltic. In the translation of Orosius for Alfred, the name is "Norðweg", while in younger Old English sources the ð is gone. In the 10th century many Norsemen settled in Northern France, according to the sagas, in the area that was later called Normandy from "norðmann" (Norseman or Scandinavian), although not a Norwegian possession. In France "normanni" or "northmanni" referred to people of Norway, Sweden or Denmark. Until around 1800 inhabitants of Western Norway were referred to as "nordmenn" (northmen) while inhabitants of Eastern Norway were referred to as "austmenn" (eastmen).

According to another theory, the first component was a word "nór", meaning "narrow" (Old English "nearu") or "northern", referring to the inner-archipelago sailing route through the land ("narrow way"). The interpretation as "northern", as reflected in the English and Latin forms of the name, would then have been due to later folk etymology. This latter view originated with philologist Niels Halvorsen Trønnes in 1847; since 2016 it as also advocated by language student and activist Klaus Johan Myrvoll and was adopted by philology professor Michael Schulte. The form "Nore" is still used in placenames such as the village of Nore and lake Norefjorden in Buskerud county, and still has the same meaning. Among other arguments in favour of the theory, it is pointed out that the word has a long vowel in Skaldic poetry and is not attested with <ð> in any native Norse texts or inscriptions (the earliest runic attestations have the spellings "nuruiak" and "nuriki"). This resurrected theory has received some pushback by other scholars on various grounds, e. g. the uncontroversial presence of the element "norðr" in the ethnonym "norðrmaðr" "Norseman, Norwegian person" (modern Norwegian "nordmann"), and the adjective "norrǿnn" "northern, Norse, Norwegian", as well as the very early attestations of the Latin and Anglo-Saxon forms with <nowiki>
! </nowiki>.

In a Latin manuscript of 849, the name "Northuagia" is mentioned, while a French chronicle of c. 900 uses the names "Northwegia" and "Norwegia". When Ohthere of Hålogaland visited King Alfred the Great in England in the end of the ninth century, the land was called "Norðwegr" (lit. "Northway") and "norðmanna land" (lit. "Northmen's land"). According to Ohthere, "Norðmanna" lived along the Atlantic coast, the Danes around Skagerrak og Kattegat, while the Sámi people (the "Fins") had a nomadic lifestyle in the wide interior. Ohthere told Alfred that he was "the most northern of all Norwegians", presumably at Senja island or closer to Tromsø. He also said that beyond the wide wilderness in Norway's southern part was the land of the Swedes, "Svealand".

The adjective "Norwegian", recorded from c. 1600, is derived from the latinisation of the name as "Norwegia"; in the adjective "Norwegian", the Old English spelling '-weg' has survived.

After Norway had become Christian, "Noregr" and "Noregi" had become the most common forms, but during the 15th century, the newer forms "Noreg(h)" and "Norg(h)e", found in medieval Icelandic manuscripts, took over and have survived until the modern day.

The first inhabitants were the Ahrensburg culture (11th to 10th millennia BC), which was a late Upper Paleolithic culture during the Younger Dryas, the last period of cold at the end of the Weichselian glaciation. The culture is named after the village of Ahrensburg, north-east of Hamburg in the German state of Schleswig-Holstein, where wooden arrow shafts and clubs have been excavated. The earliest traces of human occupation in Norway are found along the coast, where the huge ice shelf of the last ice age first melted between 11,000 and 8,000 BC. The oldest finds are stone tools dating from 9,500 to 6,000 BC, discovered in Finnmark (Komsa culture) in the north and Rogaland (Fosna culture) in the south-west. However, theories about two altogether different cultures (the Komsa culture north of the Arctic Circle being one and the Fosna culture from Trøndelag to Oslofjord being the other) were rendered obsolete in the 1970s.

More recent finds along the entire coast revealed to archaeologists that the difference between the two can simply be ascribed to different types of tools and not to different cultures. Coastal fauna provided a means of livelihood for fishermen and hunters, who may have made their way along the southern coast about 10,000 BC when the interior was still covered with ice. It is now thought that these so-called "Arctic" peoples came from the south and followed the coast northward considerably later.

In the southern part of the country are dwelling sites dating from about 5,000 BC. Finds from these sites give a clearer idea of the life of the hunting and fishing peoples. The implements vary in shape and mostly are made of different kinds of stone; those of later periods are more skilfully made. Rock carvings (i.e. petroglyphs) have been found, usually near hunting and fishing grounds. They represent game such as deer, reindeer, elk, bears, birds, seals, whales, and fish (especially salmon and halibut), all of which were vital to the way of life of the coastal peoples. The rock carvings at Alta in Finnmark, the largest in Scandinavia, were made at sea level from 4,200 to 500 BC and mark the progression of the land as the sea rose after the last ice age ended.

Between 3000 and 2500 BC, new settlers (Corded Ware culture) arrived in eastern Norway. They were Indo-European farmers who grew grain and kept cows and sheep. The hunting-fishing population of the west coast was also gradually replaced by farmers, though hunting and fishing remained useful secondary means of livelihood.

From about 1500 BC, bronze was gradually introduced, but the use of stone implements continued; Norway had few riches to barter for bronze goods, and the few finds consist mostly of elaborate weapons and brooches that only chieftains could afford. Huge burial cairns built close to the sea as far north as Harstad and also inland in the south are characteristic of this period. The motifs of the rock carvings differ slightly from those typical of the Stone Age. Representations of the Sun, animals, trees, weapons, ships, and people are all strongly stylised.

Thousands of rock carvings from this period depict ships, and the large stone burial monuments known as stone ships, suggest that ships and seafaring played an important role in the culture at large. The depicted ships most likely represent sewn plank built canoes used for warfare, fishing and trade. These ship types may have their origin as far back as the neolithic period and they continue into the Pre-Roman Iron Age, as exemplified by the Hjortspring boat.

Little has been found dating from the early Iron Age (the last 500 years BC). The dead were cremated, and their graves contain few burial goods. During the first four centuries AD, the people of Norway were in contact with Roman-occupied Gaul. About 70 Roman bronze cauldrons, often used as burial urns, have been found. Contact with the civilised countries farther south brought a knowledge of runes; the oldest known Norwegian runic inscription dates from the 3rd century. At this time, the amount of settled area in the country increased, a development that can be traced by coordinated studies of topography, archaeology, and place-names. The oldest root names, such as nes, vik, and bø ("cape," "bay," and "farm"), are of great antiquity, dating perhaps from the Bronze Age, whereas the earliest of the groups of compound names with the suffixes vin ("meadow") or heim ("settlement"), as in Bjǫrgvin (Bergen) or Sǿheim (Seim), usually date from the 1st century AD.

Archaeologists first made the decision to divide the Iron Age of Northern Europe into distinct pre-Roman and Roman Iron Ages after Emil Vedel unearthed a number of Iron Age artefacts in 1866 on the island of Bornholm. They did not exhibit the same permeating Roman influence seen in most other artefacts from the early centuries AD, indicating that parts of northern Europe had not yet come into contact with the Romans at the beginning of the Iron Age.

The destruction of the Western Roman Empire by the Germanic peoples in the 5th century is characterised by rich finds, including tribal chiefs' graves containing magnificent weapons and gold objects. Hill forts were built on precipitous rocks for defence. Excavation has revealed stone foundations of farmhouses long—one even long—the roofs of which were supported on wooden posts. These houses were family homesteads where several generations lived together, with people and cattle under one roof.

These states were based on either clans or tribes (e.g., the Horder of Hordaland in western Norway). By the 9th century, each of these small states had "things" (local or regional assemblies) for negotiating and settling disputes. The "thing" meeting places, each eventually with a hörgr (open-air sanctuary) or a heathen hof (temple; literally "hill"), were usually situated on the oldest and best farms, which belonged to the chieftains and wealthiest farmers. The regional "things" united to form even larger units: assemblies of deputy yeomen from several regions. In this way, the "lagting" (assemblies for negotiations and lawmaking) developed. The Gulating had its meeting place by Sognefjord and may have been the centre of an aristocratic confederation along the western fjords and islands called the Gulatingslag. The Frostating was the assembly for the leaders in the Trondheimsfjord area; the Earls of Lade, near Trondheim, seem to have enlarged the Frostatingslag by adding the coastland from Romsdalsfjord to Lofoten.

From the 8th to the 10th century, the wider Scandinavian region was the source of Vikings. The looting of the monastery at Lindisfarne in Northeast England in 793 by Norse people has long been regarded as the event which marked the beginning of the Viking Age. This age was characterised by expansion and emigration by Viking seafarers. They colonised, raided, and traded in all parts of Europe. Norwegian Viking explorers discovered Iceland by accident in the 9th century when heading for the Faroe Islands, and eventually came across Vinland, known today as Newfoundland, in Canada. The Vikings from Norway were most active in the northern and western British Isles and eastern North America isles.

According to tradition, Harald Fairhair unified them into one in 872 after the Battle of Hafrsfjord in Stavanger, thus becoming the first king of a united Norway. Harald's realm was mainly a South Norwegian coastal state. Fairhair ruled with a strong hand and according to the sagas, many Norwegians left the country to live in Iceland, the Faroe Islands, Greenland, and parts of Britain and Ireland. The modern-day Irish cities of Dublin, Limerick and Waterford were founded by Norwegian settlers.

Norse traditions were replaced slowly by Christian ones in the late 10th and early 11th centuries. One of the most important sources for the history of the 11th century Vikings is the treaty between the Icelanders and Olaf Haraldsson, king of Norway circa 1015 to 1028. This is largely attributed to the missionary kings Olav Tryggvasson and St. Olav. Haakon the Good was Norway's first Christian king, in the mid-10th century, though his attempt to introduce the religion was rejected. Born sometime in between 963–969, Olav Tryggvasson set off raiding in England with 390 ships. He attacked London during this raiding. Arriving back in Norway in 995, Olav landed in Moster. There he built a church which became the first Christian church ever built in Norway. From Moster, Olav sailed north to Trondheim where he was proclaimed King of Norway by the Eyrathing in 995.

Feudalism never really developed in Norway or Sweden, as it did in the rest of Europe. However, the administration of government took on a very conservative feudal character. The Hanseatic League forced the royalty to cede to them greater and greater concessions over foreign trade and the economy. The League had this hold over the royalty because of the loans the Hansa had made to the royalty and the large debt the kings were carrying. The League's monopolistic control over the economy of Norway put pressure on all classes, especially the peasantry, to the degree that no real burgher class existed in Norway.

From the 1040s to 1130, the country was at peace. In 1130, the civil war era broke out on the basis of unclear succession laws, which allowed all the king's sons to rule jointly. For periods there could be peace, before a lesser son allied himself with a chieftain and started a new conflict. The Archdiocese of Nidaros was created in 1152 and attempted to control the appointment of kings. The church inevitably had to take sides in the conflicts, with the civil wars also becoming an issue regarding the church's influence of the king. The wars ended in 1217 with the appointment of Håkon Håkonsson, who introduced clear law of succession.

From 1000 to 1300, the population increased from 150,000 to 400,000, resulting both in more land being cleared and the subdivision of farms. While in the Viking Age all farmers owned their own land, by 1300, seventy percent of the land was owned by the king, the church, or the aristocracy. This was a gradual process which took place because of farmers borrowing money in poor times and not being able to repay. However, tenants always remained free men and the large distances and often scattered ownership meant that they enjoyed much more freedom than continental serfs. In the 13th century, about twenty percent of a farmer's yield went to the king, church and landowners.

The 14th century is described as Norway's Golden Age, with peace and increase in trade, especially with the British Islands, although Germany became increasingly important towards the end of the century. Throughout the High Middle Ages, the king established Norway as a sovereign state with a central administration and local representatives.

In 1349, the Black Death spread to Norway and had within a year killed a third of the population. Later plagues reduced the population to half the starting point by 1400. Many communities were entirely wiped out, resulting in an abundance of land, allowing farmers to switch to more animal husbandry. The reduction in taxes weakened the king's position, and many aristocrats lost the basis for their surplus, reducing some to mere farmers. High tithes to church made it increasingly powerful and the archbishop became a member of the Council of State.

The Hanseatic League took control over Norwegian trade during the 14th century and established a trading center in Bergen. In 1380, Olaf Haakonsson inherited both the Norwegian and Danish thrones, creating a union between the two countries. In 1397, under Margaret I, the Kalmar Union was created between the three Scandinavian countries. She waged war against the Germans, resulting in a trade blockade and higher taxation on Norwegian goods, which resulted in a rebellion. However, the Norwegian Council of State was too weak to pull out of the union.

Margaret pursued a centralising policy which inevitably favoured Denmark, because it had a greater population than Norway and Sweden combined. Margaret also granted trade privileges to the Hanseatic merchants of Lübeck in Bergen in return for recognition of her right to rule, and these hurt the Norwegian economy. The Hanseatic merchants formed a state within a state in Bergen for generations. Even worse were the pirates, the "Victual Brothers", who launched three devastating raids on the port (the last in 1427).

Norway slipped ever more to the background under the Oldenburg dynasty (established 1448). There was one revolt under Knut Alvsson in 1502. Norwegians had some affection for King Christian II, who resided in the country for several years. Norway took no part in the events which led to Swedish independence from Denmark in the 1520s.

Upon the death of Haakon V (King of Norway) in 1319, Magnus Erikson, at just three years old, inherited the throne as King Magnus VII of Norway. At the same time, a movement to make Magnus King of Sweden proved successful, and both the kings of Sweden and of Denmark were elected to the throne by their respective nobles, Thus, with his election to the throne of Sweden, both Sweden and Norway were united under King Magnus VII.

In 1349, the Black Death radically altered Norway, killing between 50% and 60% of its population and leaving it in a period of social and economic decline. The plague left Norway very poor. Although the death rate was comparable with the rest of Europe, economic recovery took much longer because of the small, scattered population. Even before the plague, the population was only about 500,000. After the plague, many farms lay idle while the population slowly increased. However, the few surviving farms' tenants found their bargaining positions with their landlords greatly strengthened.
King Magnus VII ruled Norway until 1350, when his son, Haakon, was placed on the throne as Haakon VI. In 1363, Haakon VI married Margaret, the daughter of King Valdemar IV of Denmark. Upon the death of Haakon VI, in 1379, his son, Olaf IV, was only 10 years old. Olaf had already been elected to the throne of Denmark on 3 May 1376. Thus, upon Olaf's accession to the throne of Norway, Denmark and Norway entered personal union. Olaf's mother and Haakon's widow, Queen Margaret, managed the foreign affairs of Denmark and Norway during the minority of Olaf IV.

Margaret was working toward a union of Sweden with Denmark and Norway by having Olaf elected to the Swedish throne. She was on the verge of achieving this goal when Olaf IV suddenly died. However, Denmark made Margaret temporary ruler upon the death of Olaf. On 2 February 1388, Norway followed suit and crowned Margaret. Queen Margaret knew that her power would be more secure if she were able to find a king to rule in her place. She settled on Eric of Pomerania, grandson of her sister. Thus at an all-Scandinavian meeting held at Kalmar, Erik of Pomerania was crowned king of all three Scandinavian countries. Thus, royal politics resulted in personal unions between the Nordic countries, eventually bringing the thrones of Norway, Denmark, and Sweden under the control of Queen Margaret when the country entered into the Kalmar Union.

After Sweden broke out of the Kalmar Union in 1521, Norway tried to follow suit, but the subsequent rebellion was defeated, and Norway remained in a union with Denmark until 1814, a total of 434 years. During the national romanticism of the 19th century, this period was by some referred to as the "400-Year Night", since all of the kingdom's royal, intellectual, and administrative power was centred in Copenhagen in Denmark. In fact, it was a period of great prosperity and progress for Norway, especially in terms of shipping and foreign trade, and it also secured the country's revival from the demographic catastrophe it suffered in the Black Death. Based on the respective natural resources, Denmark–Norway was in fact a very good match since Denmark supported Norway's needs for grain and food supplies, and Norway supplied Denmark with timber, metal, and fish.

With the introduction of Protestantism in 1536, the archbishopric in Trondheim was dissolved, and Norway lost its independence, and effectually became a colony of Denmark. The Church's incomes and possessions were instead redirected to the court in Copenhagen. Norway lost the steady stream of pilgrims to the relics of St. Olav at the Nidaros shrine, and with them, much of the contact with cultural and economic life in the rest of Europe.

Eventually restored as a kingdom (albeit in legislative union with Denmark) in 1661, Norway saw its land area decrease in the 17th century with the loss of the provinces Båhuslen, Jemtland, and Herjedalen to Sweden, as the result of a number of disastrous wars with Sweden. In the north, however, its territory was increased by the acquisition of the northern provinces of Troms and Finnmark, at the expense of Sweden and Russia.

The famine of 1695–1696 killed roughly 10% of Norway's population. The harvest failed in Scandinavia at least nine times between 1740 and 1800, with great loss of life.

After Denmark–Norway was attacked by the United Kingdom at the 1807 Battle of Copenhagen, it entered into an alliance with Napoleon, with the war leading to dire conditions and mass starvation in 1812. As the Danish kingdom found itself on the losing side in 1814, it was forced, under terms of the Treaty of Kiel, to cede Norway to the king of Sweden, while the old Norwegian provinces of Iceland, Greenland, and the Faroe Islands remained with the Danish crown. Norway took this opportunity to declare independence, adopted a constitution based on American and French models, and elected the Crown Prince of Denmark and Norway, Christian Frederick, as king on 17 May 1814. This is the famous Syttende Mai (Seventeenth of May) holiday celebrated by Norwegians and Norwegian-Americans alike. "Syttende Mai" is also called "Norwegian Constitution Day".

Norwegian opposition to the great powers' decision to link Norway with Sweden caused the Norwegian–Swedish War to break out as Sweden tried to subdue Norway by military means. As Sweden's military was not strong enough to defeat the Norwegian forces outright, and Norway's treasury was not large enough to support a protracted war, and as British and Russian navies blockaded the Norwegian coast, the belligerents were forced to negotiate the Convention of Moss. According to the terms of the convention, Christian Frederik abdicated the Norwegian throne and authorised the Parliament of Norway to make the necessary constitutional amendments to allow for the personal union that Norway was forced to accept. On 4 November 1814, the Parliament (Storting) elected Charles XIII of Sweden as king of Norway, thereby establishing the union with Sweden. Under this arrangement, Norway kept its liberal constitution and its own independent institutions, though it shared a common monarch and common foreign policy with Sweden. Following the recession caused by the Napoleonic Wars, economic development of Norway remained slow until economic growth began around 1830.

This period also saw the rise of the Norwegian romantic nationalism, as Norwegians sought to define and express a distinct national character. The movement covered all branches of culture, including literature (Henrik Wergeland [1808–1845], Bjørnstjerne Bjørnson [1832–1910], Peter Christen Asbjørnsen [1812–1845], Jørgen Moe [1813–1882]), painting (Hans Gude [1825–1903], Adolph Tidemand [1814–1876]), music (Edvard Grieg [1843–1907]), and even language policy, where attempts to define a native written language for Norway led to today's two official written forms for Norwegian: Bokmål and Nynorsk.

King Charles III John, who came to the throne of Norway and Sweden in 1818, was the second king following Norway's break from Denmark and the union with Sweden. Charles John was a complex man whose long reign extended to 1844. He protected the constitution and liberties of Norway and Sweden during the age of Metternich. As such, he was regarded as a liberal monarch for that age. However, he was ruthless in his use of paid informers, the secret police and restrictions on the freedom of the press to put down public movements for reform—especially the Norwegian national independence movement.

The Romantic Era that followed the reign of King Charles III John brought some significant social and political reforms. In 1854, women won the right to inherit property in their own right, just like men. In 1863, the last trace of keeping unmarried women in the status of minors was removed. Furthermore, women were then eligible for different occupations, particularly the common school teacher. By mid-century, Norway's democracy was limited by modern standards: Voting was limited to officials, property owners, leaseholders and burghers of incorporated towns.

Still, Norway remained a conservative society. Life in Norway (especially economic life) was "dominated by the aristocracy of professional men who filled most of the important posts in the central government". There was no strong bourgeosie class in Norway to demand a breakdown of this aristocratic control of the economy. Thus, even while revolution swept over most of the countries of Europe in 1848, Norway was largely unaffected by revolts that year.

Marcus Thrane was a Utopian socialist. He made his appeal to the labouring classes urging a change of social structure "from below upwards." In 1848, he organised a labour society in Drammen. In just a few months, this society had a membership of 500 and was publishing its own newspaper. Within two years, 300 societies had been organised all over Norway, with a total membership of 20,000 persons. The membership was drawn from the lower classes of both urban and rural areas; for the first time these two groups felt they had a common cause. In the end, the revolt was easily crushed; Thrane was captured and in 1855, after four years in jail, was sentenced to three additional years for crimes against the safety of the state. Upon his release, Marcus Thrane attempted unsuccessfully to revitalise his movement, but after the death of his wife, he migrated to the United States.

In 1898, all men were granted universal suffrage, followed by all women in 1913.

Christian Michelsen, a shipping magnate and statesman, and Prime Minister of Norway from 1905 to 1907, played a central role in the peaceful separation of Norway from Sweden on 7 June 1905. A national referendum confirmed the people's preference for a monarchy over a republic. However, no Norwegian could legitimately claim the throne, since none of Norway's noble families could claim descent from medieval royalty. In European tradition, royal or "blue" blood is a precondition for laying claim to the throne.

The government then offered the throne of Norway to Prince Carl of Denmark, a prince of the Dano-German royal house of Schleswig-Holstein-Sonderburg-Glücksburg and a distant relative of several of Norway's medieval kings. After centuries of close ties between Norway and Denmark, a prince from the latter was the obvious choice for a European prince who could best relate to the Norwegian people. Following the plebiscite, he was unanimously elected king by the Norwegian Parliament, the first king of a fully independent Norway in 508 years (1397: Kalmar Union); he took the name Haakon VII. In 1905, the country welcomed the prince from neighbouring Denmark, his wife Maud of Wales and their young son to re-establish Norway's royal house.

Throughout the First World War, Norway was in principle a neutral country. In reality, however, Norway had been pressured by the British to hand over increasingly large parts of its large merchant fleet to the British at low rates, as well as to join the trade blockade against Germany. Norwegian merchant marine ships, often with Norwegian sailors still on board, were then sailing under the British flag and at risk of being sunk by German submarines. Thus, many Norwegian sailors and ships were lost. Thereafter, the world ranking of the Norwegian merchant navy fell from fourth place to sixth in the world.

Norway also proclaimed its neutrality during the Second World War, but despite this, it was invaded by German forces on 9 April 1940. Although Norway was unprepared for the German surprise attack (see: Battle of Drøbak Sound, Norwegian Campaign, and Invasion of Norway), military and naval resistance lasted for two months. Norwegian armed forces in the north launched an offensive against the German forces in the Battles of Narvik, until they were forced to surrender on 10 June after losing British support which had been diverted to France during the German invasion of France.

King Haakon and the Norwegian government escaped to Rotherhithe in London. Throughout the war they sent inspirational radio speeches and supported clandestine military actions in Norway against the Germans. On the day of the invasion, the leader of the small National-Socialist party Nasjonal Samling, Vidkun Quisling, tried to seize power, but was forced by the German occupiers to step aside. Real power was wielded by the leader of the German occupation authority, Reichskommissar Josef Terboven. Quisling, as "minister president", later formed a collaborationist government under German control. Up to 15,000 Norwegians volunteered to fight in German units, including the Waffen-SS.

The fraction of the Norwegian population that supported Germany was traditionally smaller than in Sweden, but greater than is generally appreciated today. It included a number of prominent personalities such as the Nobel-prize winning novelist Knut Hamsun. The concept of a "Germanic Union" of member states fit well into their thoroughly nationalist-patriotic ideology.

Many Norwegians and persons of Norwegian descent joined the Allied forces as well as the Free Norwegian Forces. In June 1940, a small group had left Norway following their king to Britain. This group included 13 ships, five aircraft, and 500 men from the Royal Norwegian Navy. By the end of the war, the force had grown to 58 ships and 7,500 men in service in the Royal Norwegian Navy, 5 squadrons of aircraft (including Spitfires, Sunderland flying boats and Mosquitos) in the newly formed Norwegian Air Force, and land forces including the Norwegian Independent Company 1 and 5 Troop as well as No. 10 Commandos.

During the five years of German occupation, Norwegians built a resistance movement which fought the German occupation forces with both civil disobedience and armed resistance including the destruction of Norsk Hydro's heavy water plant and stockpile of heavy water at Vemork, which crippled the German nuclear programme (see: "Norwegian heavy water sabotage"). More important to the Allied war effort, however, was the role of the Norwegian Merchant Marine. At the time of the invasion, Norway had the fourth-largest merchant marine fleet in the world. It was led by the Norwegian shipping company Nortraship under the Allies throughout the war and took part in every war operation from the evacuation of Dunkirk to the Normandy landings. Every December Norway gives a Christmas tree to the United Kingdom as thanks for the British assistance during the Second World War. A ceremony takes place to erect the tree in London's Trafalgar Square. Svalbard was not occupied by German troops. Germany secretly established a meteorological station in 1944. The crew was stuck after the general capitulation in May 1945 and were rescued by a Norwegian seal hunter on 4 September. They surrendered to the seal hunter as the last German soldiers to surrender in WW2.

From 1945 to 1962, the Labour Party held an absolute majority in the parliament. The government, led by prime minister Einar Gerhardsen, embarked on a program inspired by Keynesian economics, emphasising state financed industrialisation and co-operation between trade unions and employers' organisations. Many measures of state control of the economy imposed during the war were continued, although the rationing of dairy products was lifted in 1949, while price control and rationing of housing and cars continued until 1960.

The wartime alliance with the United Kingdom and the United States was continued in the post-war years. Although pursuing the goal of a socialist economy, the Labour Party distanced itself from the Communists (especially after the Communists' seizure of power in Czechoslovakia in 1948), and strengthened its foreign policy and defence policy ties with the US. Norway received Marshall Plan aid from the United States starting in 1947, joined the Organisation for Economic Co-operation and Development (OECD) one year later, and became a founding member of the North Atlantic Treaty Organization (NATO) in 1949.

The first oil was discovered at the small Balder field in 1967, production only began in 1999. In 1969, the Phillips Petroleum Company discovered petroleum resources at the Ekofisk field west of Norway. In 1973, the Norwegian government founded the State oil company, Statoil. Oil production did not provide net income until the early 1980s because of the large capital investment that was required to establish the country's petroleum industry. Around 1975, both the proportion and absolute number of workers in industry peaked. Since then labour-intensive industries and services like factory mass production and shipping have largely been outsourced.

Norway was a founding member of the European Free Trade Association (EFTA). Norway was twice invited to join the European Union, but ultimately declined to join after referendums that failed by narrow margins in 1972 and 1994.

In 1981, a Conservative government led by Kåre Willoch replaced the Labour Party with a policy of stimulating the stagflated economy with tax cuts, economic liberalisation, deregulation of markets, and measures to curb record-high inflation (13.6% in 1981).

Norway's first female prime minister, Gro Harlem Brundtland of the Labour party, continued many of the reforms of her conservative predecessor, while backing traditional Labour concerns such as social security, high taxes, the industrialisation of nature, and feminism. By the late 1990s, Norway had paid off its foreign debt and had started accumulating a sovereign wealth fund. Since the 1990s, a divisive question in politics has been how much of the income from petroleum production the government should spend, and how much it should save.

In 2011, Norway suffered two terrorist attacks on the same day conducted by Anders Behring Breivik which struck the government quarter in Oslo and a summer camp of the Labour party's youth movement at Utøya island, resulting in 77 deaths and 319 wounded.

The 2013 Norwegian parliamentary election brought a more conservative government to power, with the Conservative Party and the Progress Party winning 43% of the electorate's votes.

Norway's core territory comprises the western and northernmost portion of the Scandinavian Peninsula; the remote island of Jan Mayen and the archipelago of Svalbard are also part of the Kingdom of Norway. The Antarctic Peter I Island and the sub-Antarctic Bouvet Island are dependent territories and thus not considered part of the Kingdom. Norway also lays claim to a section of Antarctica known as Queen Maud Land. From the Middle Ages to 1814 Norway was part of the Danish kingdom. Norwegian possessions in the North Atlantic, Faroe Islands, Greenland, and Iceland, remained Danish when Norway was passed to Sweden at the Treaty of Kiel. Norway also comprised Bohuslän until 1658, Jämtland and Härjedalen until 1645, Shetland and Orkney until 1468, and the Hebrides and Isle of Man until the Treaty of Perth in 1266.

Norway comprises the western and northernmost part of Scandinavia in Northern Europe. Norway lies between latitudes 57° and 81° N, and longitudes 4° and 32° E. Norway is the northernmost of the Nordic countries and if Svalbard is included also the easternmost. Vardø at 31° 10' 07" east of Greenwich lies further east than St. Petersburg and Istanbul. Norway includes the northernmost point on the European mainland. The rugged coastline is broken by huge fjords and thousands of islands. The coastal baseline is . The coastline of the mainland including fjords stretches , when islands are included the coastline has been estimated to . Norway shares a land border with Sweden, with Finland, and with Russia to the east. To the north, west and south, Norway is bordered by the Barents Sea, the Norwegian Sea, the North Sea, and Skagerrak. The Scandinavian Mountains form much of the border with Sweden.

At (including Svalbard and Jan Mayen) (and without), much of the country is dominated by mountainous or high terrain, with a great variety of natural features caused by prehistoric glaciers and varied topography. The most noticeable of these are the fjords: deep grooves cut into the land flooded by the sea following the end of the Ice Age. Sognefjorden is the world's second deepest fjord, and the world's longest at . Hornindalsvatnet is the deepest lake in all Europe. Norway has about 400,000 lakes. There are 239,057 registered islands. Permafrost can be found all year in the higher mountain areas and in the interior of Finnmark county. Numerous glaciers are found in Norway.

The land is mostly made of hard granite and gneiss rock, but slate, sandstone, and limestone are also common, and the lowest elevations contain marine deposits. Because of the Gulf Stream and prevailing westerlies, Norway experiences higher temperatures and more precipitation than expected at such northern latitudes, especially along the coast. The mainland experiences four distinct seasons, with colder winters and less precipitation inland. The northernmost part has a mostly maritime Subarctic climate, while Svalbard has an Arctic tundra climate.

Because of the large latitudinal range of the country and the varied topography and climate, Norway has a larger number of different habitats than almost any other European country. There are approximately 60,000 species in Norway and adjacent waters (excluding bacteria and viruses). The Norwegian Shelf large marine ecosystem is considered highly productive.

The southern and western parts of Norway, fully exposed to Atlantic storm fronts, experience more precipitation and have milder winters than the eastern and far northern parts. Areas to the east of the coastal mountains are in a rain shadow, and have lower rain and snow totals than the west. The lowlands around Oslo have the warmest and sunniest summers, but also cold weather and snow in wintertime.Because of Norway's high latitude, there are large seasonal variations in daylight. From late May to late July, the sun never completely descends beneath the horizon in areas north of the Arctic Circle (hence Norway's description as the "Land of the Midnight sun"), and the rest of the country experiences up to 20 hours of daylight per day. Conversely, from late November to late January, the sun never rises above the horizon in the north, and daylight hours are very short in the rest of the country.

The coastal climate of Norway is exceptionally mild compared with areas on similar latitudes elsewhere in the world, with the Gulf Stream passing directly offshore the northern areas of the Atlantic coast, continuously warming the region in the winter. Temperature anomalies found in coastal locations are exceptional, with Røst and Værøy lacking a meteorological winter in spite of being north of the Arctic Circle. The Gulf Stream has this effect only on the northern parts of Norway, not in the south, despite what is commonly believed. The northern coast of Norway would thus be ice-covered if not for the Gulf Stream. As a side-effect, the Scandinavian Mountains prevent continental winds from reaching the coastline, causing very cool summers throughout Atlantic Norway. Oslo has more of a continental climate, similar to Sweden's. The mountain ranges have subarctic and tundra climates. There is also very high rainfall in areas exposed to the Atlantic, such as Bergen. Oslo, in comparison, is dry, being in a rain shadow. Skjåk in Oppland county is also in the rain shadow and is one of the driest places with precipitation annually. Finnmarksvidda and the interior valleys of Troms and Nordland also receive less than annually. Longyearbyen is the driest place in Norway with .

Parts of southeastern Norway including parts of Mjøsa have warm-summer humid continental climates (Köppen Dfb), while the more southern and western coasts are mostly of the oceanic climate (Cfb). Further inland in southeastern and northern Norway, the subarctic climate (Dfc) dominates; this is especially true for areas in the rain shadow of the Scandinavian Mountains. Some of the inner valleys of Oppland get so little precipitation annually, thanks to the rain shadow effect, that they meet the requirements for dry-summer subarctic climates (Dsc). In higher altitudes, close to the coasts of southern and western Norway, one can find the rare subpolar oceanic climate (Cfc). This climate is also common in Northern Norway, usually in lower altitudes, all the way down to sea level. A small part of the northernmost coast of Norway has the tundra/alpine/polar climate (ET). Large parts of Norway are covered by mountains and high altitude plateaus, many of which also exhibit the tundra/alpine/polar climate (ET).

The total number of species include 16,000 species of insects (probably 4,000 more species yet to be described), 20,000 species of algae, 1,800 species of lichen, 1,050 species of mosses, 2,800 species of vascular plants, up to 7,000 species of fungi, 450 species of birds (250 species nesting in Norway), 90 species of mammals, 45 fresh-water species of fish, 150 salt-water species of fish, 1,000 species of fresh-water invertebrates, and 3,500 species of salt-water invertebrates. About 40,000 of these species have been described by science. The red list of 2010 encompasses 4,599 species.

Seventeen species are listed mainly because they are endangered on a global scale, such as the European beaver, even if the population in Norway is not seen as endangered. The number of threatened and near-threatened species equals to 3,682; it includes 418 fungi species, many of which are closely associated with the small remaining areas of old-growth forests, 36 bird species, and 16 species of mammals. In 2010, 2,398 species were listed as endangered or vulnerable; of these were 1250 listed as vulnerable (VU), 871 as endangered (EN), and 276 species as critically endangered (CR), among which were the grey wolf, the Arctic fox (healthy population on Svalbard) and the pool frog.

The largest predator in Norwegian waters is the sperm whale, and the largest fish is the basking shark. The largest predator on land is the polar bear, while the brown bear is the largest predator on the Norwegian mainland. The largest land animal on the mainland is the elk (American English: moose). The elk in Norway is known for its size and strength and is often called "skogens konge", "king of the forest".

Attractive and dramatic scenery and landscape are found throughout Norway. The west coast of southern Norway and the coast of northern Norway present some of the most visually impressive coastal sceneries in the world. National Geographic has listed the Norwegian fjords as the world's top tourist attraction. The country is also home to the natural phenomena of the Midnight sun (during summer), as well as the Aurora borealis known also as the Northern lights.

The 2016 Environmental Performance Index from Yale University, Columbia University and the World Economic Forum put Norway in seventeenth place, immediately below Croatia and Switzerland. The index is based on environmental risks to human health, habitat loss, and changes in CO2 emissions. The index notes over-exploitation of fisheries, but not Norway's whaling or oil exports.

Norway is considered to be one of the most developed democracies and states of justice in the world. From 1814, c. 45% of men (25 years and older) had the right to vote, whereas the United Kingdom had c. 20% (1832), Sweden c. 5% (1866), and Belgium c. 1.15% (1840). Since 2010, Norway has been classified as the world's most democratic country by the Democracy Index.

According to the Constitution of Norway, which was adopted on 17 May 1814 and inspired by the United States Declaration of Independence and French Revolution of 1776 and 1789, respectively, Norway is a unitary constitutional monarchy with a parliamentary system of government, wherein the King of Norway is the head of state and the prime minister is the head of government. Power is separated among the legislative, executive and judicial branches of government, as defined by the Constitution, which serves as the country's supreme legal document.

The monarch officially retains executive power. But following the introduction of a parliamentary system of government, the duties of the monarch have since become strictly representative and ceremonial, such as the formal appointment and dismissal of the Prime Minister and other ministers in the executive government. Accordingly, the Monarch is commander-in-chief of the Norwegian Armed Forces, and serves as chief diplomatic official abroad and as a symbol of unity. Harald V of the House of Schleswig-Holstein-Sonderburg-Glücksburg was crowned King of Norway in 1991, the first since the 14th century who has been born in the country. Haakon, Crown Prince of Norway, is the legal and rightful heir to the throne and the Kingdom.

In practice, the Prime Minister exercises the executive powers. Constitutionally, legislative power is vested with both the government and the Parliament of Norway, but the latter is the supreme legislature and a unicameral body. Norway is fundamentally structured as a representative democracy. The Parliament can pass a law by simple majority of the 169 representatives, who are elected on the basis of proportional representation from 19 constituencies for four-year terms.

150 are elected directly from the 19 constituencies, and an additional 19 seats ("levelling seats") are allocated on a nationwide basis to make the representation in parliament correspond better with the popular vote for the political parties. A 4% election threshold is required for a party to gain levelling seats in Parliament. There are a total of 169 members of parliament.

The Parliament of Norway, called the "Stortinget" (meaning Grand Assembly), ratifies national treaties developed by the executive branch. It can impeach members of the government if their acts are declared unconstitutional. If an indicted suspect is impeached, Parliament has the power to remove the person from office.

The position of prime minister, Norway's head of government, is allocated to the member of Parliament who can obtain the confidence of a majority in Parliament, usually the current leader of the largest political party or, more effectively, through a coalition of parties. A single party generally does not have sufficient political power in terms of the number of seats to form a government on its own. Norway has often been ruled by minority governments.

The prime minister nominates the cabinet, traditionally drawn from members of the same political party or parties in the Storting, making up the government. The PM organises the executive government and exercises its power as vested by the Constitution. Norway has a state church, the Lutheran Church of Norway, which has in recent years gradually been granted more internal autonomy in day-to-day affairs, but which still has a special constitutional status. Formerly, the PM had to have more than half the members of cabinet be members of the Church of Norway, meaning at least ten out of the 19 ministries. This rule was however removed in 2012. The issue of separation of church and state in Norway has been increasingly controversial, as many people believe it is time to change this, to reflect the growing diversity in the population. A part of this is the evolution of the public school subject Christianity, a required subject since 1739. Even the state's loss in a battle at the European Court of Human Rights at Strasbourg in 2007 did not settle the matter. As of 1 January 2017, the Church of Norway is a separate legal entity, and no longer a branch of the civil service.

Through the Council of State, a privy council presided over by the monarch, the prime minister and the cabinet meet at the Royal Palace and formally consult the Monarch. All government bills need the formal approval by the monarch before and after introduction to Parliament. The Council reviews and approves all of the monarch's actions as head of state. Although all government and parliamentary acts are decided beforehand, the privy council is an example of symbolic gesture the king retains.

Members of the Storting are directly elected from party-lists proportional representation in nineteen plural-member constituencies in a national multi-party system. Historically, both the Norwegian Labour Party and Conservative Party have played leading political roles. In the early 21st century, the Labour Party has been in power since the 2005 election, in a Red–Green Coalition with the Socialist Left Party and the Centre Party.

Since 2005, both the Conservative Party and the Progress Party have won numerous seats in the Parliament, but not sufficient in the 2009 general election to overthrow the coalition. Commentators have pointed to the poor co-operation between the opposition parties, including the Liberals and the Christian Democrats. Jens Stoltenberg, the leader of the Labour Party, continued to have the necessary majority through his multi-party alliance to continue as PM until 2013.

In national elections in September 2013, voters ended eight years of Labor rule. Two political parties, Høyre and Fremskrittspartiet, elected on promises of tax cuts, more spending on infrastructure and education, better services and stricter rules on immigration, formed a government. Coming at a time when Norway's economy is in good condition with low unemployment, the rise of the right appeared to be based on other issues. Erna Solberg became prime minister, the second female prime minister after Brundtland and the first conservative prime minister since Syse. Solberg said her win was "a historic election victory for the right-wing parties".

Norway, a unitary state, is divided into eleven first-level administrative counties ("fylke"). The counties are administered through directly elected county assemblies who elect the County Governor. Additionally, the King and government are represented in every county by a fylkesmann, who effectively acts as a Governor. As such, the Government is directly represented at a local level through the County Governors' offices. The counties are then sub-divided into 356 second-level municipalities ("kommuner"), which in turn are administered by directly elected municipal council, headed by a mayor and a small executive cabinet. The capital of Oslo is considered both a county and a municipality.

Norway has two integral overseas territories: Jan Mayen and Svalbard, the only developed island in the archipelago of the same name, located miles away to the north. There are three Antarctic and Subantarctic dependencies: Bouvet Island, Peter I Island, and Queen Maud Land. On most maps, there had been an unclaimed area between Queen Maud Land and the South Pole until 12 June 2015 when Norway formally annexed that area.

96 settlements have city status in Norway. In most cases, the city borders are coterminous with the borders of their respective municipalities. Often, Norwegian city municipalities include large areas that are not developed; for example, Oslo municipality contains large forests, located north and south-east of the city, and over half of Bergen municipality consists of mountainous areas.
The counties of Norway are:

Norway uses a civil law system where laws are created and amended in Parliament and the system regulated through the Courts of justice of Norway. It consists of the Supreme Court of 20 permanent judges and a Chief Justice, appellate courts, city and district courts, and conciliation councils. The judiciary is independent of executive and legislative branches. While the Prime Minister nominates Supreme Court Justices for office, their nomination must be approved by Parliament and formally confirmed by the Monarch in the Council of State. Usually, judges attached to regular courts are formally appointed by the Monarch on the advice of the Prime Minister.

The Courts' strict and formal mission is to regulate the Norwegian judicial system, interpret the Constitution, and as such implement the legislation adopted by Parliament. In its judicial reviews, it monitors the legislative and executive branches to ensure that they comply with provisions of enacted legislation.

The law is enforced in Norway by the Norwegian Police Service. It is a Unified National Police Service made up of 27 Police Districts and several specialist agencies, such as Norwegian National Authority for the Investigation and Prosecution of Economic and Environmental Crime, known as "Økokrim"; and the National Criminal Investigation Service, known as "Kripos", each headed by a chief of police. The Police Service is headed by the National Police Directorate, which reports to the Ministry of Justice and the Police. The Police Directorate is headed by a National Police Commissioner. The only exception is the Norwegian Police Security Agency, whose head answers directly to the Ministry of Justice and the Police.

Norway abolished the death penalty for regular criminal acts in 1902. The legislature abolished the death penalty for high treason in war and war-crimes in 1979. Reporters Without Borders, in its 2007 Worldwide Press Freedom Index, ranked Norway at a shared first place (along with Iceland) out of 169 countries.

In general, the legal and institutional framework in Norway is characterised by a high degree of transparency, accountability and integrity, and the perception and the occurrence of corruption are very low. Norway has ratified all relevant international anti-corruption conventions, and its standards of implementation and enforcement of anti-corruption legislation are considered very high by many international anti-corruption working groups such as the OECD Anti-Bribery Working Group. However, there are some isolated cases showing that some municipalities have abused their position in public procurement processes.

Norwegian prisons are humane, rather than tough, with emphasis on rehabilitation. At 20%, Norway's re-conviction rate is among the lowest in the world.

Norway maintains embassies in 82 countries. 60 countries maintain an embassy in Norway, all of them in the capital, Oslo.

Norway is a founding member of the United Nations (UN), the North Atlantic Treaty Organization (NATO), the Council of Europe and the European Free Trade Association (EFTA). Norway issued applications for accession to the European Union (EU) and its predecessors in 1962, 1967 and 1992, respectively. While Denmark, Sweden and Finland obtained membership, the Norwegian electorate rejected the treaties of accession in referenda in 1972 and 1994.

After the 1994 referendum, Norway maintained its membership in the European Economic Area (EEA), an arrangement granting the country access to the internal market of the Union, on the condition that Norway implements the Union's pieces of legislation which are deemed relevant (of which there were approximately seven thousand by 2010) Successive Norwegian governments have, since 1994, requested participation in parts of the EU's co-operation that go beyond the provisions of the EEA agreement. Non-voting participation by Norway has been granted in, for instance, the Union's Common Security and Defence Policy, the Schengen Agreement, and the European Defence Agency, as well as 19 separate programmes.

Norway participated in the 1990s brokering of the Oslo Accords, an unsuccessful attempt to resolve the Israeli–Palestinian conflict.

The Norwegian Armed Forces numbers about 25,000 personnel, including civilian employees. According to 2009 mobilisation plans, full mobilisation produces approximately 83,000 combatant personnel. Norway has conscription (including 6–12 months of training); in 2013, the country became the first in Europe and NATO to draft women as well as men. However, due to less need for conscripts after the Cold War ended with the break-up of the Soviet Union, few people have to serve if they are not motivated. The Armed Forces are subordinate to the Norwegian Ministry of Defence. The Commander-in-Chief is King Harald V. The military of Norway is divided into the following branches: the Norwegian Army, the Royal Norwegian Navy, the Royal Norwegian Air Force, the Norwegian Cyber Defence Force and the Home Guard.

In response to its being overrun by Germany in 1940, the country was one of the founding nations of the North Atlantic Treaty Organization (NATO) on 4 April 1949. At present, Norway contributes in the International Security Assistance Force (ISAF) in Afghanistan. Additionally, Norway has contributed in several missions in contexts of the United Nations, NATO, and the Common Security and Defence Policy of the European Union.

Norwegians enjoy the second-highest GDP per-capita among European countries (after Luxembourg), and the sixth-highest GDP (PPP) per-capita in the world. Today, Norway ranks as the second-wealthiest country in the world in monetary value, with the largest capital reserve per capita of any nation. According to the CIA World Factbook, Norway is a net external creditor of debt. Norway maintained first place in the world in the UNDP Human Development Index (HDI) for six consecutive years (2001–2006), and then reclaimed this position in 2009. The standard of living in Norway is among the highest in the world. "Foreign Policy" magazine ranks Norway last in its Failed States Index for 2009, judging Norway to be the world's most well-functioning and stable country. The OECD ranks Norway fourth in the 2013 equalised Better Life Index and third in intergenerational earnings elasticity.

The Norwegian economy is an example of a mixed economy; a prosperous capitalist welfare state it features a combination of free market activity and large state ownership in certain key sectors, influenced by both liberal governments from the late 19th century and later by social democratic governments in the postwar era. Public health care in Norway is free (after an annual charge of around 2000 kroner for those over 16), and parents have 46 weeks paid parental leave. The state income derived from natural resources includes a significant contribution from petroleum production. Norway has an unemployment rate of 4.8%, with 68% of the population aged 15–74 employed. People in the labour force are either employed or looking for work. 9.5% of the population aged 18–66 receive a disability pension and 30% of the labour force are employed by the government, the highest in the OECD. The hourly productivity levels, as well as average hourly wages in Norway, are among the highest in the world.

The egalitarian values of Norwegian society have kept the wage difference between the lowest paid worker and the CEO of most companies as much less than in comparable western economies. This is also evident in Norway's low Gini coefficient.

The state has large ownership positions in key industrial sectors, such as the strategic petroleum sector (Statoil), hydroelectric energy production (Statkraft), aluminium production (Norsk Hydro), the largest Norwegian bank (DNB), and telecommunication provider (Telenor). Through these big companies, the government controls approximately 30% of the stock values at the Oslo Stock Exchange. When non-listed companies are included, the state has even higher share in ownership (mainly from direct oil licence ownership). Norway is a major shipping nation and has the world's 6th largest merchant fleet, with 1,412 Norwegian-owned merchant vessels.
By referendums in 1972 and 1994, Norwegians rejected proposals to join the European Union (EU). However, Norway, together with Iceland and Liechtenstein, participates in the European Union's single market through the European Economic Area (EEA) agreement. The EEA Treaty between the European Union countries and the EFTA countries—transposed into Norwegian law via "EØS-loven"—describes the procedures for implementing European Union rules in Norway and the other EFTA countries. Norway is a highly integrated member of most sectors of the EU internal market. Some sectors, such as agriculture, oil and fish, are not wholly covered by the EEA Treaty. Norway has also acceded to the Schengen Agreement and several other intergovernmental agreements among the EU member states.

The country is richly endowed with natural resources including petroleum, hydropower, fish, forests, and minerals. Large reserves of petroleum and natural gas were discovered in the 1960s, which led to a boom in the economy. Norway has obtained one of the highest standards of living in the world in part by having a large amount of natural resources compared to the size of the population. In 2011, 28% of state revenues were generated from the petroleum industry.

Norway is the first country which banned cutting of trees (deforestation), in order to prevent rain forests from vanishing. The country declared its intention at the UN Climate Summit in 2014, alongside Great Britain and Germany. Crops, that are typically linked to forests' destruction are timber, soy, palm oil and beef. Now Norway has to find a new way to provide these essential products without exerting negative influence on its environment.


Export revenues from oil and gas have risen to over 40% of total exports and constitute almost 20% of the GDP. Norway is the fifth-largest oil exporter and third-largest gas exporter in the world, but it is not a member of OPEC. In 1995, the Norwegian government established the sovereign wealth fund ("Government Pension Fund – Global"), which would be funded with oil revenues, including taxes, dividends, sales revenues and licensing fees. This was intended to reduce overheating in the economy from oil revenues, minimise uncertainty from volatility in oil price, and provide a cushion to compensate for expenses associated with the ageing of the population.

The government controls its petroleum resources through a combination of state ownership in major operators in the oil fields (with approximately 62% ownership in Statoil in 2007) and the fully state-owned Petoro, which has a market value of about twice Statoil, and SDFI. Finally, the government controls licensing of exploration and production of fields. The fund invests in developed financial markets outside Norway. Spending from the fund is constrained by the budgetary rule ("Handlingsregelen"), which limits spending over time to no more than the real value yield of the fund, originally assumed to be 4% a year, but lowered in 2017 to 3% of the fund's total value.

Between 1966 and 2013, Norwegian companies drilled 5085 oil wells, mostly in the North Sea. Of these 3672 are "utviklingsbrønner" (regular production); 1413 are "letebrønner" (exploration); and 1405 have been terminated ("avsluttet").

Oil fields not yet in production phase include: Wisting Central—calculated size in 2013, 65–156 million barrels of oil and , ("utvinnbar") of gas. and the Castberg Oil Field ("Castberg-feltet")—calculated size 540 million barrels of oil, and ("utvinnbar") of gas. Both oil fields are located in the Barents Sea.

Norway is also the world's second-largest exporter of fish (in value, after China). Fish from fish farms and catch constitutes the second largest (behind oil/natural gas) export product measured in value.

Hydroelectric plants generate roughly 98–99% of Norway's electric power, more than any other country in the world.

Norway contains significant mineral resources, and in 2013, its mineral production was valued at US$1.5 billion (Norwegian Geological Survey data). The most valuable minerals are calcium carbonate (limestone), building stone, nepheline syenite, olivine, iron, titanium, and nickel.


In 2017, the Government Pension Fund controlled assets surpassed a value of US$1 trillion (equal to US$190,000 per capita), about 250% of Norway's 2017 GDP. It is the largest sovereign wealth fund in the world. The fund controls about 1.3% of all listed shares in Europe, and more than 1% of all the publicly traded shares in the world. The Norwegian Central Bank operates investment offices in London, New York, and Shanghai. Guidelines implemented in 2007 allow the fund to invest up to 60% of the capital in shares (maximum of 40% prior), while the rest may be placed in bonds and real-estate. As the stock markets tumbled in September 2008, the fund was able to buy more shares at low prices. In this way, the losses incurred by the market turmoil was recuperated by November 2009.

Other nations with economies based on natural resources, such as Russia, are trying to learn from Norway by establishing similar funds. The investment choices of the Norwegian fund are directed by ethical guidelines; for example, the fund is not allowed to invest in companies that produce parts for nuclear weapons. Norway's highly transparent investment scheme is lauded by the international community. The future size of the fund is closely linked to the price of oil and to developments in international financial markets.

In 2000, the government sold one-third of the state-owned oil company Statoil in an IPO. The next year, the main telecom supplier, Telenor, was listed on Oslo Stock Exchange. The state also owns significant shares of Norway's largest bank, DnB NOR and the airline SAS. Since 2000, economic growth has been rapid, pushing unemployment down to levels not seen since the early 1980s (unemployment in 2007: 1.3%). The international financial crisis has primarily affected the industrial sector, but unemployment has remained low, and was at 3.3% (86,000 people) in August 2011. In contrast to Norway, Sweden had substantially higher actual and projected unemployment numbers as a result of the recession. Thousands of mainly young Swedes migrated to Norway for work during these years, which is easy, as the labour market and social security systems overlap in the Nordic Countries. In the first quarter of 2009, the GNP of Norway surpassed Sweden's for the first time in history, although its population is half the size.

Due to the low population density, narrow shape and long coastlines of Norway, its public transport is less developed than in many European countries, especially outside the major cities. The country has long-standing water transport traditions, but the Norwegian Ministry of Transport and Communications has in recent years implemented rail, road, and air transport through numerous subsidiaries to develop the country's infrastructure. Under discussion is development of a new high-speed rail system between the nation's largest cities.

Norway's main railway network consists of of standard gauge lines, of which is double track and high-speed rail (210 km/h) while 62% is electrified at . The railways transported 56,827,000 passengers 2,956 million passenger-kilometres and 24,783,000 tonnes of cargo 3,414 million tonne-kilometres. The entire network is owned by the Norwegian National Rail Administration. All domestic passenger trains except the Airport Express Train are operated by Norges Statsbaner (NSB). Several companies operate freight trains.
Investment in new infrastructure and maintenance is financed through the state budget, and subsidies are provided for passenger train operations. NSB operates long-haul trains, including night trains, regional services and four commuter train systems, around Oslo, Trondheim, Bergen and Stavanger.

Norway has approximately of road network, of which are paved and are motorway. The four tiers of road routes are national, county, municipal and private, with national and primary county roads numbered en route. The most important national routes are part of the European route scheme. The two most prominent are the European route E6 going north–south through the entire country, and the E39, which follows the West Coast. National and county roads are managed by the Norwegian Public Roads Administration.

Norway has the world's largest registered stock of plug-in electric vehicles per capita. In March 2014, Norway became the first country where over 1 in every 100 passenger cars on the roads is a plug-in electric. The plug-in electric segment market share of new car sales is also the highest in the world. According to a report by Dagens Næringsliv in June 2016, the country would like to ban sales of gasoline and diesel powered vehicles as early as 2025. In June 2017, 42% of new cars registered were electric.

Of the 98 airports in Norway, 52 are public, and 46 are operated by the state-owned Avinor. Seven airports have more than one million passengers annually. A total of 41,089,675 passengers passed through Norwegian airports in 2007, of whom 13,397,458 were international.

The central gateway to Norway by air is Oslo Airport, Gardermoen. Located about northeast of Oslo, it is hub for the two major Norwegian airlines: Scandinavian Airlines and Norwegian Air Shuttle, and for regional aircraft from Western Norway. There are departures to most European countries and some intercontinental destinations. A direct high-speed train connects to Oslo Central Station every 10 minutes for a 20 min ride.

Norway's population was 5,096,300 people in October 2013. Norwegians are an ethnic North Germanic people. Since the late 20th century, Norway has attracted immigrants from southern and central Europe, the Mideast, Africa, Asia and beyond.

The total fertility rate (TFR) in 2018 was estimated at 1.56 children born per woman, below the replacement rate of 2.1, it remains considerably below the high of 4.69 children born per woman in 1877. In 2018 the median age of the Norwegian population was 39.3 years.

In 2012, an official study showed that 86% of the total population have at least one parent who was born in Norway. More than 710,000 individuals (13%) are immigrants and their descendants; there are 117,000 children of immigrants, born in Norway.

Of these 710,000 immigrants and their descendants:

In 2013, the Norwegian government said that 14% of the Norwegian population were immigrants or children of two immigrant parents. About 6% of the immigrant population come from EU, North America and Australia, and about 8.1% come from Asia, Africa and Latin America.

In 2012, of the total 660,000 with immigrant background, 407,262 had Norwegian citizenship (62.2%).

Immigrants have settled in all Norwegian municipalities. The cities or municipalities with the highest share of immigrants in 2012 were Oslo (32%) and Drammen (27%). The share in Stavanger was 16%. According to Reuters, Oslo is the "fastest growing city in Europe because of increased immigration". In recent years, immigration has accounted for most of Norway's population growth. In 2011, 16% of newborn children were of immigrant background.

The Sámi people are indigenous to the Far North and have traditionally inhabited central and northern parts of Norway and Sweden, as well as areas in northern Finland and in Russia on the Kola Peninsula. Another national minority are the Kven people, descendants of Finnish-speaking people who migrated to northern Norway from the 18th up to the 20th century. From the 19th century up to the 1970s, the Norwegian government tried to assimilate both the Sámi and the Kven, encouraging them to adopt the majority language, culture and religion. Because of this "Norwegianization process", many families of Sámi or Kven ancestry now identify as ethnic Norwegian.
Particularly in the 19th century, when economic conditions were difficult in Norway, tens of thousands of people migrated to the United States and Canada, where they could work and buy land in frontier areas. Many went to the Midwest and Pacific Northwest. In 2006, according to the US Census Bureau, almost 4.7 million persons identified as Norwegian Americans, which was larger than the population of ethnic Norwegians in Norway itself. In the 2011 Canadian census, 452,705 Canadian citizens identified as having Norwegian ancestry.


, the number of immigrants or children of two immigrants residing in Norway was 710,465, or 14.1% of the total population, up from 183,000 in 1992. Yearly immigration has increased since 2005. While yearly net immigration in 2001–2005 was on average 13,613, it increased to 37,541 between 2006 and 2010, and in 2011 net immigration reached 47,032. This is mostly because of increased immigration by residents of the EU, in particular from Poland.

In 2012, the immigrant community (which includes immigrants and children born in Norway of immigrant parents) grew by 55,300, a record high. Net immigration from abroad reached 47,300 (300 higher than in 2011), while immigration accounted for 72% of Norway's population growth. 17% of newborn children were born to immigrant parents. Children of Pakistani, Somali and Vietnamese parents made up the largest groups of all Norwegians born to immigrant parents.

Pakistani Norwegians are the largest non-European minority group in Norway. Most of their 32,700 members live in and around Oslo. The Iraqi and Somali immigrant populations have increased significantly in recent years. After the enlargement of the EU in 2004, a wave of immigrants arrived from Central and Northern Europe, particularly Poland, Sweden and Lithuania. The fastest growing immigrant groups in 2011 in absolute numbers were from Poland, Lithuania and Sweden. The policies of immigration and integration have been the subject of much debate in Norway.

Separation of church and state happened significantly later in Norway than in most of Europe, and remains incomplete. In 2012, the Norwegian parliament voted to grant the Church of Norway greater autonomy, a decision which was confirmed in a constitutional amendment on 21 May 2012.

Until 2012 parliamentary officials were required to be members of the Evangelical-Lutheran Church of Norway, and at least half of all government ministers had to be a member of the state church. As state church, the Church of Norway's clergy were viewed as state employees, and the central and regional church administrations were part of the state administration. Members of the Royal family are required to be members of the Lutheran church. On 1 January 2017, Norway made the church independent of the state, but retained the Church's status as the "people's church".

Most Norwegians are registered at baptism as members of the Church of Norway, which has been Norway's state church since its establishment. In recent years the church has been granted increasing internal autonomy, but it retains its special constitutional status and other special ties to the state, and the constitution requires that the reigning monarch must be a member and states that the country's values are based on its Christian and humanist heritage. Many remain in the church to participate in the community and practices such as baptism, confirmation, marriage and burial rites. About 70.6% of Norwegians were members of the Church of Norway in 2017. In 2017, about 53.6% of all newborns were baptised and about 57.9% of all 15-year-old persons were confirmed in the church.

According to the 2010 Eurobarometer Poll, 22% of Norwegian citizens responded that "they believe there is a God", 44% responded that "they believe there is some sort of spirit or life force" and 29% responded that "they don't believe there is any sort of spirit, God or life force". Five percent gave no response. In the early 1990s, studies estimated that between 4.7% and 5.3% of Norwegians attended church on a weekly basis. This figure has dropped to about 2%. 

In 2010, 10% of the population was religiously unaffiliated, while another 9% were members of religious communities outside the Church of Norway. Other Christian denominations total about 4.9% of the population, the largest of which is the Roman Catholic Church, with 83,000 members, according to 2009 government statistics. The "Aftenposten" (Norwegian, The Evening Post) in October 2012 reported there were about 115,234 registered Roman Catholics in Norway; the reporter estimated that the total number of people with a Roman Catholic background may be 170,000–200,000 or higher.

Others include Pentecostals (39,600), the Evangelical Lutheran Free Church of Norway (19,600), Methodists (11,000), Baptists (9,900), Eastern Orthodox (9,900), Brunstad Christian Church (6,800), Seventh-day Adventists (5,100), Assyrians and Chaldeans, and others. The Swedish, Finnish and Icelandic Lutheran congregations in Norway have about 27,500 members in total. Other Christian denominations comprise less than 1% each, including 4,000 members in The Church of Jesus Christ of Latter-day Saints and 12,000 Jehovah's Witnesses.
Among non-Christian religions, Islam is the largest, with 166,861 registered members (2018), and probably fewer than 200,000 in total. It is practised mainly by Somali, Arab, Bosniak, Kurdish and Turkish immigrants, as well as Norwegians of Pakistani descent.

Other religions comprise less than 1% each, including 819 adherents of Judaism. Indian immigrants introduced Hinduism to Norway, which in 2011 has slightly more than 5,900 adherents, or 1% of non-Lutheran Norwegians. Sikhism has approximately 3,000 adherents, with most living in Oslo, which has two gurdwaras. Sikhs first came to Norway in the early 1970s. The troubles in Punjab after Operation Blue Star and riots committed against Sikhs in India after the assassination of Indira Gandhi led to an increase in Sikh refugees moving to Norway. Drammen also has a sizeable population of Sikhs; the largest gurdwara in north Europe was built in Lier. There are eleven Buddhist organisations, grouped under the Buddhistforbundet organisation, with slightly over 14,000 members, which make up 0.2% of the population. The Baha'i religion has slightly more than 1,000 adherents. Around 1.7% (84,500) of Norwegians belong to the secular Norwegian Humanist Association.

From 2006 to 2011, the fastest-growing religious communities in Norway were Eastern Orthodox Christianity and Oriental Orthodox Christianity, which grew in membership by 80%; however, their share of the total population remains small, at 0.2%. It is associated with the huge immigration from Eritrea and Ethiopia, and to a lesser extent from Central and Eastern European and Middle Eastern countries. Other fast-growing religions were Roman Catholicism (78.7%), Hinduism (59.6%), Islam (48.1%), and Buddhism (46.7%).

As in other Scandinavian countries, the ancient Norse followed a form of native Germanic paganism known as Norse paganism. By the end of the 11th century, when Norway had been Christianised, the indigenous Norse religion and practices were prohibited. Remnants of the native religion and beliefs of Norway survive today in the form of names, referential names of cities and locations, the days of the week, and other parts of everyday language. Modern interest in the old ways has led to a revival of pagan religious practices in the form of "Åsatru." The Norwegian "Åsatrufellesskapet Bifrost" formed in 1996; in 2011, the fellowship had about 300 members. "Foreningen Forn Sed" was formed in 1999 and has been recognised by the Norwegian government.

The Sámi minority retained their shamanistic religion well into the 18th century, when most converted to Christianity under the influence of Dano-Norwegian Lutheran missionaries. Although some insist that "indigenous Sámi religion had effectively been eradicated,' athropologist Gutorm Gjessing's "Changing Lapps" (1954) argues that the Sámi's "were outwardly and to all practical purposes converted to Christianity, but at the subconscious and unconscious level, the shamistic frenzy survived, more or less latent, only awaiting the necessary stimulus to break out into the open." Today there is a renewed appreciation for the Sámi traditional way of life, which has led to a revival of "Noaidevuohta". Some Norwegian and Sámi celebrities are reported to visit shamans for guidance.

Norway was awarded first place according to the UN's Human Development Index (HDI) for 2013. In the 1800s, by contrast, poverty and communicable diseases dominated in Norway together with famines and epidemics. From the 1900s, improvements in public health occurred as a result of development in several areas such as social and living conditions, changes in disease and medical outbreaks, establishment of the health care system, and emphasis on public health matters. Vaccination and increased treatment opportunities with antibiotics resulted in great improvements within the Norwegian population. Improved hygiene and better nutrition were factors that contributed to improved health.

The disease pattern in Norway changed from communicable diseases to non-communicable diseases and chronic diseases as cardiovascular disease. Inequalities and social differences are still present in public health in Norway today.

In 2013 the infant mortality rate was 2.5 per 1,000 live births among children under the age of one. For girls it was 2.7 and for boys 2.3, which is the lowest infant mortality rate for boys ever recorded in Norway.

Higher education in Norway is offered by a range of seven universities, five specialised colleges, 25 university colleges as well as a range of private colleges. Education follows the Bologna Process involving Bachelor (3 years), Master (2 years) and PhD (3 years) degrees. Acceptance is offered after finishing upper secondary school with general study competence.

Public education is virtually free, regardless of nationality. The academic year has two semesters, from August to December and from January to June. The ultimate responsibility for the education lies with the Norwegian Ministry of Education and Research.

Norwegian and Sámi are the two official languages of Norway.

The North Germanic Norwegian language has two official written forms, "Bokmål" and "Nynorsk". Both are used in public administration, schools, churches, and media. Bokmål is the written language used by a large majority of about 80–85%. Around 95% of the population speak Norwegian as their first or native language, although many speak dialects that may differ significantly from the written languages. All Norwegian dialects are mutually intelligible, although listeners with limited exposure to dialects other than their own may struggle to understand certain phrases and pronunciations in some other dialects.

Several Uralic Sámi languages are spoken and written throughout the country, especially in the north, by some members of the Sámi people. (Estimates suggest that about one third of the Norwegian Sámi speak a Sámi language.) Speakers have a right to be educated and to receive communication from the government in their own language in a special "forvaltningsområde" (administrative area) for Sámi languages. The Kven minority historically spoke the Uralic Kven language (considered a separate language in Norway, but generally perceived as a Finnish dialect in Finland). Today the majority of ethnic Kven have little or no knowledge of the language. According to the Kainun institutti, "The typical modern Kven is a Norwegian-speaking Norwegian who knows his genealogy." As Norway has ratified the European Charter for Regional or Minority Languages (ECRML) the Kven language together with Romani and Scandoromani language has become officially recognised minority languages.

Some supporters have also advocated making Norwegian Sign Language an official language of the country.

In the 19th and 20th centuries, the Norwegian language was subject to strong political and cultural controversies. This led to the development of Nynorsk in the 19th century and to the formation of alternative spelling standards in the 20th century.

Norwegian is similar to its neighbour Scandinavian languages; Swedish and Danish. All three languages are to a degree mutually intelligible and can be, and commonly are, employed in communication among inhabitants of the Scandinavian countries. As a result of the co-operation within the Nordic Council, inhabitants of all Nordic countries, including Iceland and Finland, have the right to communicate with Norwegian authorities in their own language.

Students who are children of immigrant parents are encouraged to learn the Norwegian language. The Norwegian government offers language instructional courses for immigrants wishing to obtain Norwegian citizenship. With increasing concern about assimilating immigrants, since 1 September 2008, the government has required that an applicant for Norwegian citizenship give evidence of proficiency in either Norwegian or in one of the Sámi languages, or give proof of having attended classes in Norwegian for 300 hours, or meet the language requirements for university studies in Norway (that is, by being proficient in one of the Scandinavian languages).

The primary foreign language taught in Norwegian schools is English, considered an international language since the post-WWII era. The majority of the population is fairly fluent in English, especially those born after World War II. German, French and Spanish are also commonly taught as second or, more often, third languages. Russian, Japanese, Italian, Latin, and rarely Chinese (Mandarin) are offered in some schools, mostly in the cities. Traditionally, English, German and French were considered the main foreign languages in Norway. These languages, for instance, were used on Norwegian passports until the 1990s, and university students have a general right to use these languages when submitting their theses.

The Norwegian farm culture continues to play a role in contemporary Norwegian culture. In the 19th century, it inspired a strong romantic nationalistic movement, which is still visible in the Norwegian language and . Norwegian culture blossomed with nationalist efforts to achieve an independent identity in the areas of literature, art and music. This continues today in the performing arts and as a result of government support for exhibitions, cultural projects and artwork.

Norway has been considered a progressive country, which has adopted legislation and policies to support women's rights, minority rights, and LGBT rights. As early as 1884, 171 of the leading figures, among them five Prime Ministers for the Liberal Party and the Conservative Party, co-founded the Norwegian Association for Women's Rights. They successfully campaigned for women's right to education, women's suffrage, the right to work, and other gender equality policies. From the 1970s, gender equality also came high on the state agenda, with the establishment of a public body to promote gender equality, which evolved into the Gender Equality and Anti-Discrimination Ombud. Civil society organisations also continue to play an important role, and the women's rights organisations are today organised in the Norwegian Women's Lobby umbrella organisation.

In 1990, the Norwegian constitution was amended to grant absolute primogeniture to the Norwegian throne, meaning that the eldest child, regardless of gender, takes precedence in the line of succession. As it was not retroactive, the current successor to the throne is the eldest son of the King, rather than his eldest child. The Norwegian constitution Article 6 states that "For those born before the year 1990 it shall...be the case that a male shall take precedence over a female."
The Sámi people have for centuries been the subject of discrimination and abuse by the dominant cultures in Scandinavia and Russia, those countries claiming possession of Sámi lands. The Sámi people have never been a single community in a single region of Sápmi. Norway has been greatly criticised by the international community for the politics of Norwegianization of and discrimination against the indigenous population of the country. Nevertheless, Norway was, in 1990, the first country to recognise ILO-convention 169 on indigenous people recommended by the UN.

In regard to LGBT rights, Norway was the first country in the world to enact an anti-discrimination law protecting the rights of gays and lesbians. In 1993, Norway became the second country to legalise civil union partnerships for same-sex couples, and on 1 January 2009 Norway became the sixth country to grant full marriage equality to same-sex couples. As a promoter of human rights, Norway has held the annual Oslo Freedom Forum conference, a gathering described by "The Economist" as "on its way to becoming a human-rights equivalent of the Davos economic forum."

The Norwegian cinema has received international recognition. The documentary film "Kon-Tiki" (1950) won an Academy Award. In 1959, Arne Skouen's "Nine Lives" was nominated, but failed to win. Another notable film is "Flåklypa Grand Prix" (English: "Pinchcliffe Grand Prix"), an animated feature film directed by Ivo Caprino. The film was released in 1975 and is based on characters from Norwegian cartoonist Kjell Aukrust. It is the most widely seen Norwegian film of all time.

Nils Gaup's "Pathfinder" (1987), the story of the Sámi, was nominated for an Oscar. Berit Nesheim's "The Other Side of Sunday" was nominated for an Oscar in 1997.

Since the 1990s, the film industry has thrived, producing up to 20 feature films each year. Particular successes were "Kristin Lavransdatter", based on a novel by a Nobel Prize winner; "The Telegraphist" and "Gurin with the Foxtail". Knut Erik Jensen was among the more successful new directors, together with Erik Skjoldbjærg, who is remembered for "Insomnia".

The country has also been used as filming location for several Hollywood and other international productions, including "The Empire Strikes Back" (1980), for which the producers used Hardangerjøkulen glacier as a filming location for scenes of the ice planet Hoth. It included a memorable battle in the snow. The films "Die Another Day", "The Golden Compass", "Spies Like Us" and "Heroes of Telemark," as well as the TV series "Lilyhammer" and "Vikings" also had scenes set in Norway. A short film, "The Spirit of Norway" was featured at Maelstrom at Norway Pavilion at Epcot located within Walt Disney World Resort in Florida in the United States. The attraction and the film ceased their operations on 5 October 2014.

 The classical music of the romantic composers Edvard Grieg, Rikard Nordraak and Johan Svendsen is internationally known, as is the modern music of Arne Nordheim. Norway's classical performers include Leif Ove Andsnes, one of the world's more famous pianists; Truls Mørk, an outstanding cellist; and the great Wagnerian soprano Kirsten Flagstad.

Norwegian black metal, a form of rock music in Norway, has been an influence in world music since the late 20th century. Since the 1990s, Norway's export of black metal, a lo-fi, dark and raw form of heavy metal, has been developed by such bands as Emperor, Darkthrone, Gorgoroth, Mayhem, Burzum and Immortal. More recently, bands such as Enslaved, Kvelertak, Dimmu Borgir and Satyricon have evolved the genre into the present day while still garnering worldwide fans. Controversial events associated with the black metal movement in the early 1990s included several church burnings and two prominent murder cases.

The jazz scene in Norway is thriving. Jan Garbarek, Terje Rypdal, Mari Boine, Arild Andersen and Bugge Wesseltoft are internationally recognised while Paal Nilssen-Love, Supersilent, Jaga Jazzist and Wibutee are becoming world-class artists of the younger generation.

Other internationally recognised bands are A-ha, Röyksopp and Ylvis. A-ha initially rose to global fame during the mid-1980s. In the 1990s and 2000s, the group maintained its popularity domestically, and has remained successful outside Norway, especially in Germany, Switzerland, France, and Brazil.

Some of the most memorable female solo artists from Norway are Susanne Sundfør, Sigrid, Astrid S, Adelén, Julie Bergan, Maria Mena, Tone Damli, Margaret Berger, Lene Marlin, Christel Alsos, Maria Arredondo, Marion Raven and Marit Larsen (both former members of the defunct pop-rock group M2M), Lene Nystrøm (vocalist of the Danish eurodance group Aqua) and Anni-Frid Lyngstad (vocalist of the Swedish pop group ABBA).

In recent years, various Norwegian songwriters and production teams have contributed to the music of other international artists. The Norwegian production team Stargate has produced songs for Rihanna, Beyoncé, Shakira, Jennifer Lopez and Lionel Richie, among others. Espen Lind has written and produced songs for Beyoncé, Lionel Richie and Leona Lewis, among others. Lene Marlin has written songs for Rihanna and Lovebugs. Ina Wroldsen has written songs for artists such as Demi Lovato, Shakira, Inna, Sophie Ellis-Bextor, One Direction and The Saturdays, among others.

Norway enjoys many music festivals throughout the year, all over the country. Norway is the host of one of the world's biggest extreme sport festivals with music, Ekstremsportveko—a festival held annually in Voss. Oslo is the host of many festivals, such as Øyafestivalen and . Oslo used to have a summer parade similar to the German Love Parade. In 1992, the city of Oslo wanted to adopt the French music festival "Fête de la Musique". Fredrik Carl Størmer established the festival. Even in its first year, "Musikkens Dag" gathered thousands of people and artists in the streets of Oslo. "Musikkens Dag" is now renamed "Musikkfest Oslo".

The history of Norwegian literature starts with the pagan Eddaic poems and skaldic verse of the 9th and 10th centuries, with poets such as Bragi Boddason and Eyvindr skáldaspillir. The arrival of Christianity around the year 1000 brought Norway into contact with European medieval learning, hagiography and history writing. Merged with native oral tradition and Icelandic influence, this influenced the literature written in the late 12th and early 13th centuries. Major works of that period include "Historia Norwegiæ", "Þiðrekssaga" and "Konungs skuggsjá".

Little Norwegian literature came out of the period of the Scandinavian Union and the subsequent Dano-Norwegian union (1387–1814), with some notable exceptions such as Petter Dass and Ludvig Holberg. In his play "Peer Gynt", Ibsen characterised this period as "Twice two hundred years of darkness/brooded o'er the race of monkeys." The first line of this couplet is frequently quoted. During the union with Denmark, the government imposed using only written Danish, which decreased the writing of Norwegian literature.

Two major events precipitated a major resurgence in Norwegian literature: in 1811 a Norwegian university was established in Christiania. Secondly, seized by the spirit of revolution following the American and French revolutions, the Norwegians created their first Constitution in 1814. Strong authors were inspired who became recognised first in Scandinavia, and then worldwide; among them were Henrik Wergeland, Peter Christen Asbjørnsen, Jørgen Moe and Camilla Collett.

By the late 19th century, in the Golden Age of Norwegian literature, the so-called "Great Four" emerged: Henrik Ibsen, Bjørnstjerne Bjørnson, Alexander Kielland, and Jonas Lie. Bjørnson's "peasant novels", such as "Ein glad gut" (A Happy Boy) and "Synnøve Solbakken", are typical of the Norwegian romantic nationalism of their day. Kielland's novels and short stories are mostly naturalistic. Although an important contributor to early romantic nationalism, (especially "Peer Gynt"), Henrik Ibsen is better known for his pioneering realistic dramas such as "The Wild Duck" and "A Doll's House." They caused an uproar because of his candid portrayals of the middle classes, complete with infidelity, unhappy marriages, and corrupt businessmen.

In the 20th century, three Norwegian novelists were awarded the Nobel Prize in Literature: Bjørnstjerne Bjørnson in 1903, Knut Hamsun for the book "Markens grøde" ("Growth of the Soil") in 1920, and Sigrid Undset (known for "Kristinlavransdatter") in 1928. Writers such as the following also made important contributions: Dag Solstad, Jon Fosse, Cora Sandel, Olav Duun, Olav H. Hauge, Gunvor Hofmo, Stein Mehren, Kjell Askildsen, Hans Herbjørnsrud, Aksel Sandemose, Bergljot Hobæk Haff, Jostein Gaarder, Erik Fosnes Hansen, Jens Bjørneboe, Kjartan Fløgstad, Lars Saabye Christensen, Johan Borgen, Herbjørg Wassmo, Jan Erik Vold, Rolf Jacobsen, Olaf Bull, Jan Kjærstad, Georg Johannesen, Tarjei Vesaas, Sigurd Hoel, Arnulf Øverland, Karl Ove Knausgård and Johan Falkberget.

Internationally recognised Norwegian scientists include the mathematicians Niels Henrik Abel, Sophus Lie and Atle Selberg, physical chemist Lars Onsager, physicist Ivar Giaever, chemists Odd Hassel, Peter Waage, and Cato Maximilian Guldberg.

In the 20th century, Norwegian academics have been pioneering in many social sciences, including criminology, sociology and peace and conflict studies. Prominent academics include Arne Næss, a philosopher and founder of deep ecology; Johan Galtung, the founder of peace studies; Nils Christie and Thomas Mathiesen, criminologists; Fredrik Barth, a social anthropologist; Vilhelm Aubert, Harriet Holter and Erik Grønseth, sociologists; Tove Stang Dahl, a pioneer of women's law; Stein Rokkan, a political scientist; and economists Ragnar Frisch, Trygve Haavelmo, and Finn E. Kydland.

In 2014, the two Norwegian scientists May-Britt Moser and Edvard Moser won the Nobel Prize in Physiology or Medicine along with John O'Keefe. They won the prize for their groundbreaking work identifying the cells that make up a positioning system in the human brain, our "in-built GPS".

With expansive forests, Norway has long had a tradition of building in wood. Many of today's most interesting new buildings are made of wood, reflecting the strong appeal that this material continues to hold for Norwegian designers and builders.

With Norway's conversion to Christianity some 1,000 years ago, churches were built. Stonework architecture was introduced from Europe for the most important structures, beginning with the construction of Nidaros Cathedral in Trondheim. In the early Middle Ages, wooden stave churches were constructed throughout Norway. Some of them have survived; they represent Norway's most unusual contribution to architectural history. A fine example, Urnes Stave Church in inner Sognefjord, is on UNESCO's World Heritage List. Another notable example of wooden architecture is the buildings at Bryggen Wharf in Bergen, also on the list for World Cultural Heritage sites, consisting of a row of tall, narrow wooden structures along the quayside.

In the 17th century, under the Danish monarchy, cities and villages such as Kongsberg and Røros were established. The city Kongsberg had a church built in the Baroque style. Traditional wooden buildings that were constructed in Røros have survived.

After Norway's union with Denmark was dissolved in 1814, Oslo became the capital. The architect Christian H. Grosch designed the earliest parts of the University of Oslo, the Oslo Stock Exchange, and many other buildings and churches constructed in that early national period.

At the beginning of the 20th century, the city of Ålesund was rebuilt in the Art Nouveau style, influenced by styles of France. The 1930s, when functionalism dominated, became a strong period for Norwegian architecture. It is only since the late 20th century that Norwegian architects have achieved international renown. One of the most striking modern buildings in Norway is the Sámi Parliament in Kárášjohka, designed by Stein Halvorson and Christian Sundby. Its debating chamber, in timber, is an abstract version of a "lavvo," the traditional tent used by the nomadic Sámi people.

For an extended period, the Norwegian art scene was dominated by artwork from Germany and Holland as well as by the influence of Copenhagen. It was in the 19th century that a truly Norwegian era began, first with portraits, later with impressive landscapes. Johan Christian Dahl (1788–1857), originally from the Dresden school, eventually returned to paint the landscapes of western Norway, defining Norwegian painting for the first time."

Norway's newly found independence from Denmark encouraged painters to develop their Norwegian identity, especially with landscape painting by artists such as Kitty Kielland, a female painter who studied under Hans Gude, and Harriet Backer, another pioneer among female artists, influenced by impressionism. Frits Thaulow, an impressionist, was influenced by the art scene in Paris as was Christian Krohg, a realist painter, famous for his paintings of prostitutes.

Of particular note is Edvard Munch, a symbolist/expressionist painter who became world-famous for "The Scream" which is said to represent the anxiety of modern man.

Other artists of note include Harald Sohlberg, a neo-romantic painter remembered for his paintings of Røros, and Odd Nerdrum, a figurative painter who maintains that his work is not art, but kitsch.

 show the influence of long seafaring and farming traditions, with salmon (fresh and cured), herring (pickled or marinated), trout, codfish, and other seafood, balanced by cheeses (such as brunost), dairy products, and breads (predominantly dark/darker).

Lefse is a Norwegian potato flatbread, usually topped with large amounts of butter and sugar, most common around Christmas. Some traditional Norwegian dishes include lutefisk, smalahove, pinnekjøtt, raspeball, and fårikål. Some quirky Norwegian speciality is rakefisk, which is a fermented trout, consumed with thin flatbread (flatbrød, not lefse) and sour cream. And the most popular pastry among all population is vaffel. It is different from Belgian in taste and consistency and is served with sour cream, brown cheese, butter and sugar, or strawberry or raspberry jam, which can all be mixed or eaten separately.

Sports are a central part of Norwegian culture, and popular sports include association football, handball, biathlon, cross-country skiing, ski jumping, speed skating, and, to a lesser degree, ice hockey.

Association football is the most popular sport in Norway in terms of active membership. In 2014–2015 polling, football ranked far behind biathlon and cross-country skiing in terms of popularity as spectator sports. Ice hockey is the biggest indoor sport. The women's handball national team has won several titles, including two Summer Olympics championships (2008, 2012), three World Championships (1999, 2011, 2015), and six European Championship (1998, 2004, 2006, 2008, 2010, 2014).

In association football, the women's national team has won the FIFA Women's World Cup in 1995 and the Olympic Football Tournament in 2000. The women's team also has two UEFA European Women's Championship titles (1987, 1993). The men's national football team has participated three times in the FIFA World Cup (1938, 1994, and 1998), and once in the European Championship (2000). The highest FIFA ranking Norway has achieved is 2nd, a position it has held twice, in 1993 and in 1995.

Chess is also gaining popularity in Norway. Magnus Carlsen is the current world champion. There are about 10 Grandmasters and 29 International Masters in Norway.

Norwegian players in the National Football League (NFL) include Halvor Hagen, Bill Irgens, Leif Olve Dolonen Larsen, Mike Mock, and Jan Stenerud.

Bandy is a traditional sport in Norway and the country is one of the four founders of Federation of International Bandy. In terms of licensed athletes, it is the second biggest winter sport in the world. As of January 2018, the men's national team has captured one silver and one bronze, while the women's national team has managed five bronzes at the World Championships.
Norway first participated at the Olympic Games in 1900, and has sent athletes to compete in every Games since then, except for the sparsely attended 1904 Games and the 1980 Summer Olympics in Moscow when they participated in the American-led boycott. Norway leads the overall medal tables at the Winter Olympic Games by a considerable margin. Famous Norwegian winter sport athletes includes biathlete Ole Einar Bjørndalen, speed skaters Johan Olav Koss and Hjalmar Andersen, figure skater Sonja Henie and cross-country skiers Marit Bjørgen and Bjørn Dæhlie.

Norway has hosted the Games on two occasions:
It also hosted the 2016 Winter Youth Olympics in Lillehammer, making Norway the first country to host both Winter regular and Youth Olympics.

As of 2008, Norway ranks 17th in the World Economic Forum's Travel and Tourism Competitiveness Report. Tourism in Norway contributed to 4.2% of the gross domestic product as reported in 2016. Every one in fifteen people throughout the country work in the tourism industry. Tourism is seasonal in Norway, with more than half of total tourists visiting between the months of May and August.

The main attractions of Norway are the varied landscapes that extend across the Arctic Circle. It is famous for its fjord-indented coastline and its mountains, ski resorts, lakes and woods. Popular tourist destinations in Norway include Oslo, Ålesund, Bergen, Stavanger, Trondheim and Tromsø. Much of the nature of Norway remains unspoiled, and thus attracts numerous hikers and skiers. The fjords, mountains and waterfalls in Western and Northern Norway attract several hundred thousand foreign tourists each year. In the cities, cultural idiosyncrasies such as the Holmenkollen ski jump attract many visitors, as do landmarks such as Bergen's Bryggen and Oslo's Vigeland Sculpture Park.




</doc>
<doc id="21242" url="https://en.wikipedia.org/wiki?curid=21242" title="Nokia">
Nokia

Nokia Corporation (natively Nokia Oyj, referred to as Nokia; , , ) is a Finnish multinational telecommunications, information technology, and consumer electronics company, founded in 1865. Nokia's headquarters are in Espoo, Finland, in the greater Helsinki metropolitan area. In 2018, Nokia employed approximately 103,000 people across over 100 countries, did business in more than 130 countries, and reported annual revenues of around €23 billion. Nokia is a public limited company listed on the Helsinki Stock Exchange and New York Stock Exchange. It is the world's 415th-largest company measured by 2016 revenues according to the "Fortune Global 500," having peaked at 85th place in 2009. It is a component of the Euro Stoxx 50 stock market index.

The company has operated in various industries over the past 150 years. It was founded as a pulp mill and had long been associated with rubber and cables, but since the 1990s has focused on large-scale telecommunications infrastructures, technology development, and licensing. Nokia is a major contributor to the mobile telephony industry, having assisted in the development of the GSM, 3G and LTE standards (and currently in 5G), and was once the largest worldwide vendor of mobile phones and smartphones. After a partnership with Microsoft and subsequent market struggles, its mobile phone business was bought by Microsoft, creating Microsoft Mobile as its successor in 2014. After the sale, Nokia began to focus more extensively on its telecommunications infrastructure business and on Internet of things technologies, marked by the divestiture of its Here mapping division and the acquisition of Alcatel-Lucent, including its Bell Labs research organization. The company then also experimented with virtual reality and digital health, the latter through the purchase of Withings. The Nokia brand has since returned to the mobile and smartphone market through a licensing arrangement with HMD Global. Nokia continues to be a major patent licensor for most large mobile phone vendors. As of 2018, Nokia is the world's third-largest network equipment manufacturer.

The company was viewed with national pride by Finns, as its mobile phone business made it by far the largest worldwide company and brand from Finland. At its peak in 2000, during the telecoms bubble, Nokia alone accounted for 4% of the country's GDP, 21% of total exports, and 70% of the Helsinki Stock Exchange market capital.

Nokia's history dates to 1865, when Finnish-Swede mining engineer Fredrik Idestam established a pulp mill near the town of Tampere, Finland (then in the Russian Empire). A second pulp mill was opened in 1868 near the neighboring town of Nokia, offering better hydropower resources. In 1871, Idestam, together with friend Leo Mechelin, formed a shared company from it and called it "Nokia Ab" (in Swedish, "Nokia Company" being the English equivalent), after the site of the second pulp mill.

Idestam retired in 1896, making Mechelin the company's chairman. Mechelin expanded into electricity generation by 1902 which Idestam had opposed. In 1904 Suomen Gummitehdas ("Finnish Rubber Works"), a rubber business founded by Eduard Polón, established a factory near the town of Nokia and used its name.

In 1922, Nokia Ab entered into a partnership with Finnish Rubber Works and Kaapelitehdas ("the Cable Factory"), all now jointly under the leadership of Polón. Finnish Rubber Works company grew rapidly when it moved to the Nokia region in the 1930s to take advantage of the electrical power supply, and the cable company soon did too.

Nokia at the time also made respirators for both civilian and military use, from the 1930s well into the early 1990s.

In 1967, the three companies – Nokia, Kaapelitehdas and Finnish Rubber Works – merged and created a new Nokia Corporation, restructured into four major businesses: forestry, cable, rubber and electronics. In the early 1970s, it entered the networking and radio industry. Nokia also started making military equipment for Finland's defence forces ("Puolustusvoimat"), such as the Sanomalaite M/90 communicator in 1983, and the M61 gas mask first developed in the 1960s. Nokia was now also making professional mobile radios, telephone switches, capacitors and chemicals.

After Finland's trade agreement with the Soviet Union in the 1960s, Nokia expanded into the Soviet market. It soon widened trade, ranging from automatic telephone exchanges to robotics among others; by the late 1970s the Soviet Union became a major market for Nokia, helping to yield high profits. Nokia also co-operated on scientific technology with the Soviet Union. The U.S. government became increasingly suspicious of that co-operation after the end of the Cold War détente in the early 1980s. Nokia imported many US-made components and used them for the Soviets, and according to U.S. Deputy Minister of Defence, Richard Perle, Nokia had a secret co-operation with The Pentagon that allowed the U.S. to keep track of technology developments in the Soviet Union through trading with Nokia. This was a demonstration of Finland trading with both sides, as it was neutral during the Cold War.

In 1977, Kari Kairamo became CEO and he transformed the company's businesses. By this time, Finland was becoming what has been called "Nordic Japan". Under his leadership Nokia acquired many companies including television maker Salora in 1984, followed by Swedish electronics and computer maker Luxor AB in 1985, and French television maker Oceanic in 1987. This made Nokia the third-largest television manufacturer of Europe (behind Philips and Thomson). The existing brands continued to be used until the end of the television business in 1996.

In 1987, Nokia acquired Schaub-Lorenz, the consumer operations of Germany's Standard Elektrik Lorenz (SEL), which included its "Schaub-Lorenz" and "Graetz" brands. It was originally part of American conglomerate International Telephone & Telegraph (ITT), and after the acquisition products were sold under the "ITT Nokia" brand, despite SEL's sale to Compagnie Générale d'Electricité (CGE), the predecessor of Alcatel, in 1986.

On 1 April 1988, Nokia bought the computer division of Ericsson's Information Systems, which originated as a computer division of Swedish aircraft and car manufacturer Saab called Datasaab. Ericsson Information Systems made Alfaskop terminals, typewriters, minicomputers and Ericsson-branded IBM compatible PCs. The merger with Nokia's Information Systems division—which since 1981 had a line of personal computers called MikroMikko—resulted in the name Nokia Data.

Nokia also acquired Mobira, a mobile telephony company, which was the foundation of its future mobile phones business. In 1981, Mobira launched the Nordic Mobile Telephone (NMT) service, the world's first international cellular network and the first to allow international roaming. In 1982, Mobira launched the Mobira Senator car phone, Nokia's first mobile phone. At that time, the company had no interest in producing mobile phones, which the executive board regarded as akin to James Bond's gadgets: improbably futuristic and niche devices. After all these acquisitions, Nokia's revenue base became US$2.7 billion. CEO Kairamo committed suicide on 11 December 1988.

In 1987, Kaapelitehdas discontinued production of cables at its Helsinki factory after 44 years, effectively shutting down the sub-company.

Following Simo Vuorilehto's appointment as CEO, a major restructuring was planned. With 11 groups within the company, Vuorilehto divested industrial units he deemed as un-strategic. Nokian Tyres ("Nokian Renkaat"), a tyre producer originally formed as a division of Finnish Rubber Works in 1932, split away from Nokia Corporation in 1988. Two years later, in 1990, Finnish Rubber Works followed suit. In 1991 Nokia sold its computer division, Nokia Data, to UK-based International Computers Limited (ICL), the precursor of Fujitsu Siemens. Investors thought of this as financial trouble and Nokia's stock price sank as a result. Finland was now also experiencing its worst recession in living memory, and the collapse of the Soviet Union, a major customer, made matters worse.

Vuorilehto quit in January 1992 and was replaced by Jorma Ollila, who had been the head of the mobile phone business from 1990 and advised against selling that division. Ollila decided to turn Nokia into a 'telecom-oriented' company, and he eventually got rid of divisions like the power business. This strategy proved to be very successful and the company grew rapidly in the following years. Nokia's operating profit went from negative in 1991 to $1 billion in 1995 and almost $4 billion by 1999.

Nokia's first fully portable mobile phone after the Mobira Senator was the Mobira Cityman 900 in 1987. Nokia assisted in the development of the GSM mobile standard in the 1980s, and developed the first GSM network with Siemens, the predecessor to Nokia Siemens Network. The world's first GSM call was made by Finnish prime minister Harri Holkeri on 1 July 1991, using Nokia equipment on the 900 MHz band network built by Nokia and operated by Radiolinja. In November 1992, the Nokia 1011 launched, making it the first commercially available GSM mobile phone.

Salora Oy as a Nokia subsidiary ended in 1989 when the division was merged into Nokia-Mobira Oy. The brand continued to be used for televisions until 1995.

On 12 June 1996, Nokia announced the sale of its television business to Canada/Hong Kong-based Semi-Tech Corporation. The television manufacturing plant in Germany closed down in September 1996. The sale included a factory in Turku, and the rights to use the Nokia, Finlux, Luxor, Salora, Schaub-Lorenz and Oceanic brands until the end of 1999. Some of these brands were later sold to other companies.

Nokia was the first to launch digital satellite receivers in the UK, announced in March 1997. In August 1997 Nokia introduced the first digital satellite receiver with Common Interface (CI) support. In 1998 Nokia became the chosen supplier to produce the world's first digital terrestrial television set-top boxes by British Digital Broadcasting (BDB), which was eventually launched as ONdigital.
In October 1998, Nokia overtook Motorola to become the best-selling mobile phone brand, and in December manufactured its 100 millionth mobile phone. A major reason why Nokia grew against its main competitors Motorola and Ericsson was that it managed to cater to the consumer youth market and fashion-oriented consumers, most significantly with the Nokia 5110 and 3210 handsets which featured a large range of colourful and replaceable back-covers called Xpress-on. One of the earliest fashion phones in 1992, from Swiss watchmaker Swatch, was based on Nokia's 101 handset. The company would also form the Vertu division, creating luxury mobile handsets.

Nokia claimed in April 1996 its 447Xav and 447K monitors to be the first with stereo speakers and a sub-woofer. In May 1999 Nokia introduced their first wireless LAN products. In January 2000 ViewSonic acquired Nokia Display Products, the division making displays for personal computers. On 26 April 2001 Nokia partnered with Telefonica to supply DSL modems and routers in Spain.

In 1998, Nokia co-founded Symbian Ltd. led by Psion to create a new operating system for PDAs and smart mobile phones as a successor of EPOC32. They released the Nokia 9210 Communicator running Symbian OS in 2001 and later that year created the Symbian Series 60 platform, later introducing it with their first camera phone, the Nokia 7650. Both Nokia and Symbian eventually became the largest smartphone hardware and software maker respectively, and in February 2004 Nokia became the largest shareholder of Symbian Ltd. Nokia acquired the entire company in June 2008 and then formed the Symbian Foundation as its successor.

In 1998 alone, the company had sales revenue of $20 billion making $2.6 billion profit. By 2000 Nokia employed over 55,000 people, and had a market share of 30% in the mobile phone market, almost twice as large as its nearest competitor, Motorola. The company was operating in 140 countries as of 1999. It was reported at the time that some people believed Nokia to be a Japanese company. Between 1996 and 2001, Nokia's turnover increased fivefold, from €6.5 billion to €31 billion.

The company would then be known as a successful and innovative maker of camera phones. The Nokia 3600/3650 was the first camera phone on sale in North America in 2003. In April 2005 Nokia partnered with German camera optics maker Carl Zeiss AG. That same month Nokia introduced the Nseries, which would become its flagship line of smart phones for the next six years. The Nokia N95 introduced in September 2006 became highly successful and was also awarded as "best mobile imaging device" in Europe in 2007. Its successor the N82 featured a xenon flash, which helped it win the award of "best mobile imaging" device in Europe in 2008. The N93 in 2006 was known for its specialized camcorder and the twistable design that switches between clamshell and a camcorder-like position. They were also well known for the N8 with a high resolution 12-megapixel sensor in 2010; the 808 PureView in 2012 with a 41-megapixel sensor; and the Lumia 920 flagship in 2012 which implemented advanced PureView technologies.

Nokia was one of the pioneers of mobile gaming due to the popularity of "Snake", which came pre-loaded on many products. In 2002, Nokia attempted to break into the handheld gaming market with the N-Gage. Nokia's head of entertainment and media, Ilkka Raiskinen, once quoted "Game Boy is for 10-year-olds", stating that N-Gage is more suited to a mature audience. However, the device was a failure, unable to challenge the dominant market leader Nintendo. Nokia attempted to revive N-Gage as a platform for their S60 smartphones, which eventually launched in 2008.

In Q1 2004, Nokia's mobile phone handset market share steeply dropped to 28.9%, down from 34.6% a year earlier. However, by 2006 the company was steadily gaining again and in Q4 2007 reached its all-time high figure of 40.4%. Its smartphone market share in that quarter was 51%. Nokia was the largest vendor at the time in all regions bar North America.

Nokia launched mobile TV trials in 2005 in Finland with content provided by public broadcaster Yle. The services are based on the DVB-H standard. It could be viewed with the widescreen Nokia 7710 smartphone with a special accessory enabling it to receive DVB-H signals. Nokia partnered with Arqiva and O2 to launch trials in the UK in September 2005.

In 2005 Nokia developed a Linux-based operating system called Maemo, which shipped that year on the Nokia 770 Internet Tablet.

On 1 June 2006, Jorma Ollila became the company's chairman and retired as CEO, replaced by Olli-Pekka Kallasvuo.

In August 2007, Nokia introduced Ovi, an umbrella name for the company's new Internet services which included the N-Gage platform and the Nokia Music Store. The Ovi Store faced stiff competition against Apple's App Store when it was introduced in 2008.

In October 2008 Nokia announced the Nokia 5800 XpressMusic, the first device to ship with the new touch-centric S60 5th Edition, also known as Symbian^1, the first iteration of the platform since the creation of the Symbian Foundation. In November 2008 Nokia announced it would end mobile phone sales in Japan because of low market share. Nokia's global mobile phone market share peaked in 2008 at 38.6 percent. The same year, Nokia announced the acquisition of Trolltech and its Qt software development. Qt was a central part of Nokia's strategy until 2011, and it was eventually sold in 2012.

Nokia briefly returned to the computer market with the Booklet 3G netbook in August 2009.

In late 2009 and in 2010, the music-focused Xseries and consumer-focused Cseries were introduced respectively. In April 2010 Nokia introduced its next flagship mobile device, the Nokia N8, which would be the first to run on Symbian^3. However it was delayed for many months which tarnished the company's image, especially after the failure of its previous flagship N97 and tougher competition from Apple and the rising Google. On 10 September 2010, Olli-Pekka Kallasvuo was fired as CEO and it was announced that Stephen Elop from Microsoft would take Nokia's CEO position, becoming the first non-Finnish director in Nokia's history. It was claimed that investors pressed Nokia's board to recruit an outsider to shake up management and break from the traditional "Nokia way". Ollila had also announced that he would step down as Nokia chairman by 2012. On 11 March 2011 Nokia announced that it had paid Elop a $6 million signing bonus as "compensation for lost income from his prior employer", on top of his $1.4 million annual salary.
The old Symbian OS became completely open source in February 2010. However, in November 2010 it was announced that the Symbian Foundation was closing and that Nokia would take back control of the Symbian operating system under closed licensing. By now Nokia was the only remaining company using the platform, along with carrier NTT DoCoMo in Japan, after both Samsung and Sony Ericsson moved to Android. Meanwhile, in 2010 for Nokia's Linux ambitions, Nokia collaborated with Intel to form the MeeGo project, after the merger of Nokia's own Maemo and Intel's Moblin.

Nokia's Symbian platform that had been the leading smartphone platform in Europe and Asia for many years was quickly becoming outdated and difficult for developers after the advent of iOS and Android. To counter this, Nokia planned to make their MeeGo Linux operating system, under development, the company's flagship on smartphones. Shortly after Elop's CEO tenure began, the Nokia board green-lit him the ability to change the company's mobile phones strategy, including changing operating systems. Veteran Anssi Vanjoki, head of the smartphones division, left the company around this time. His final appearance was at Nokia World 2010 when the Nokia E7 and other Symbian^3 devices were introduced.

On 11 February 2011, Nokia announced a "strategic partnership" with Microsoft, under which it would adopt Windows Phone 7 as its primary operating system on smartphones, and integrate its services and platforms with its own, including Bing as search engine, and integration of Nokia Maps data into Bing Maps. Elop stated that Nokia chose not to use Android because of an apparent inability to "differentiate" its offerings, with critics also noting that his past ties to Microsoft may have also influenced the decision. Although the MeeGo "Harmattan"-based N9 was met with a highly positive reception in 2011, Nokia had already decided to end development on MeeGo and solely focus on its Microsoft partnership, although the CEO said that the N9's "innovations" will live on in the future, which eventually made their way on the Asha platform in 2013. After the announcement of the Microsoft partnership, Nokia's market share deteriorated; this was due to demand for Symbian dropping when consumers realized Nokia's focus and attention would be elsewhere.

The company posted a large loss for the second quarter of 2011 – only their second quarterly loss in 19 years. Nokia's first Windows Phone flagship was the Lumia 800, which arrived in November 2011. Falling sales in 2011, which were not being improved significantly with the Lumia line in 2012, led to consecutive quarters of huge losses. By mid-2012 the company's stock price fell below $2. CEO Elop announced cost-cutting measures in June by shedding 10,000 employees by the end of the year and the closure of the Salo manufacturing plant. The Finnish prime minister also announced that the government won't save the company from an emergency state fund. Around this time Nokia started a new project codenamed "Meltemi", a platform for low-end smartphones. With the Microsoft alliance and under Elop's management, Nokia also had a renewed focus on the North American market where Nokia phones were, in stark contrast to the rest of the world, almost irrelevant for many years. This strategy began in January 2012 with the introduction of the Nokia Lumia 900 smartphone in partnership with U.S. carrier AT&T.

In March 2011, Nokia introduced a new corporate typeface called "Pure". On 1 August 2011, Nokia announced that it would adopt a new three-digit naming system for mobile phone products and stop using letters, effectively ending the Nseries, Eseries, and short-lived Cseries. That same day the Nokia 500 was introduced with the new system. Nokia last used three-digit names on analogue phones in the 1990s.

When the Lumia 920 was announced in September 2012, it was seen by the press as the first high-end Windows Phone that could challenge rivals due to its advanced feature set. Elop said that the positive reaction to it had created a sense of hope and optimism in the company. The company was also making gains in developing countries with its Asha series, which were selling strongly. Although Nokia's smartphone sales and market share greatly increased throughout 2013, including in the North American market, it was still not enough to avoid financial losses. Ollila stepped down as chairman on 4 May 2012 and was replaced by Risto Siilasmaa.
In September 2013 Nokia announced the sale of its mobile and devices division to Microsoft. The sale was positive for Nokia to avoid further negative financial figures, as well as for Microsoft's CEO Steve Ballmer, who wanted Microsoft to produce more hardware and turn it into a devices and services company. The Nokia chairman, Risto Siilasmaa, described the deal as rationally correct (in the best interests of Nokia shareholders), but emotionally difficult – experts agree that Nokia would have been in a cash crisis had it not sold the division to Microsoft. Analysts believe that Ballmer pushed for the buyout because of fears that Nokia was close to adopting Android and abandoning their alliance with Microsoft. Indeed, in January 2014 the Nokia X was introduced which ran on a customised version of Android. It was a surprising and somewhat odd launch coming just weeks away from the finalisation of the Microsoft buyout. Others, including Ballmer's successor Satya Nadella, felt that Microsoft thought merging their software teams with Nokia's hardware engineering and designs would "accelerate" growth of Windows Phone. The sale was completed in April 2014, with Microsoft Mobile becoming the successor to Nokia's mobile devices division. Nokia also moved from its headquarters to another building complex located at Karaportti. At the time, Ballmer himself was retiring as Microsoft CEO and was replaced by Satya Nadella, who opposed the Nokia mobile phones purchase, along with chairman Bill Gates. The purchased assets from Nokia were eventually written-off by Microsoft in 2015.

By 2014, Nokia's global brand value according to Interbrand fell to 98th place, a sharp slide from the 5th place it was in 2009. Nokia's downfall in the mobile phone market has had different explanations from analysts, with many split about the CEO's decision to abandon its in-house operating system and adopting Windows Phone in 2011. Many researchers have concluded that Nokia suffered from deep internal rivalries within the management. Former employees claimed that the management became so swollen by the early success that they grew complacent over time. Some from the Symbian developing team have claimed that the company's upper management rejected hundreds of potential innovations during the 2000s that they proposed, including entirely rewriting Symbian's code. One former Nokia employee claimed that the company was run as a "Soviet-style bureaucracy".

In July 2013, Nokia bought Siemens' stake in the Nokia Siemens Networks joint venture for $2.2 billion, turning it into a wholly owned subsidiary called Nokia Solutions and Networks, until being rebranded as Nokia Networks soon after. During Nokia's financial struggles, its profitable networking division with Siemens provided much of its income; thus, the purchase proved to be positive, particularly after the sale of its mobile devices unit.

After the sale of its mobile devices division, Nokia focused on network equipment through Nokia Networks.

In October 2014, Nokia and China Mobile signed a US$970 million framework deal for delivery between 2014 and 2015.

On 17 November 2014, Nokia Technologies head Ramzi Haidamus disclosed that the company planned to re-enter the consumer electronics business as an original design manufacturer, licensing in-house hardware designs and technologies to third-party manufacturers. Haidamus stated that the Nokia brand was "valuable" but "is diminishing in value, and that's why it is important that we reverse that trend very quickly, imminently". The next day, Nokia unveiled the N1, an Android tablet manufactured by Foxconn, as its first product following the Microsoft sale. Haidamus emphasized that devices released under these licensing agreements would be held to high standards in production quality, and would "look and feel just like Nokia built it". Nokia CEO Rajeev Suri stated that the company planned to re-enter the mobile phone business in this manner in 2016, following the expiration of its non-compete clause with Microsoft.

According to Robert Morlino, the spokesman of Nokia Technologies, Nokia planned to follow the brand-licensing model rather than direct marketing of mobile devices due to the sale of its mobile devices division to Microsoft. The company took aggressive steps to revitalize itself, evident through its hiring of software experts, testing of new products and seeking of sales partners. On 14 July 2015, CEO Rajeev Suri confirmed that the company would make a return to the mobile phones market in 2016.

On 28 July 2015, Nokia announced OZO, a 360-degrees virtual reality camera, with eight 2K optical image sensors. The division behind the product, Nokia Technologies, claimed that OZO would be the most advanced VR film-making platform. Nokia's press release stated that OZO would be "the first in a planned portfolio of digital media solutions," with more technologic products expected in the future. OZO was fully unveiled on 30 November in Los Angeles. The OZO, designed for professional use, was intended for retail for US$60,000; however, its price was decreased by $15,000 prior to release, and is listed on its official website as $40,000.

On 14 April 2015, Nokia confirmed that it was in talks with the French telecommunications equipment company Alcatel-Lucent regarding a potential merger. The next day, Nokia announced that it had agreed to purchase Alcatel-Lucent for €15.6 billion in an all-stock deal. CEO Rajeev Suri felt that the purchase would give Nokia a strategic advantage in the development of 5G wireless technologies. The acquisition created a stronger competitor to the rival firms Ericsson and Huawei, whom Nokia and Alcatel-Lucent had surpassed in terms of total combined revenue in 2014. Nokia shareholders hold 66.5% of the new combined company, while Alcatel-Lucent shareholders hold 33.5%. The Bell Labs division was to be maintained, but the Alcatel-Lucent brand would be replaced by Nokia. In October 2015, following approval of the deal by China's Ministry of Commerce, the merger awaited approval by French regulators. Despite the initial intent of selling the submarine cable division separately, Alcatel-Lucent later declared that it would not. The merger closed on 14 January 2016, but was not complete until 3 November 2016. From the acquisition Nokia is now also the owner of the Alcatel mobile phone brand, which continues to be licensed to TCL Corporation.

On 3 August 2015, Nokia announced that it had reached a deal to sell its Here digital maps division to a consortium of BMW, Daimler AG and Volkswagen Group for €2.8 billion. The deal closed on 3 December 2015.

On 26 April 2016, Nokia announced its intent to acquire French connected health device maker Withings for US$191 million. The company was integrated into a new Digital Health unit of Nokia Technologies. Nokia later wrote off the cost of the acquisition and in May 2018 the health unit was sold back to Éric Carreel, a Withings co-founder and former CEO.
On 18 May 2016, Microsoft Mobile sold its Nokia-branded feature phone business to HMD Global, a new company founded by former Nokia executive Jean-Francois Baril, and an associated factory in Vietnam to Foxconn's FIH Mobile subsidiary. Nokia subsequently entered into a long-term licensing deal to make HMD the exclusive manufacturer of Nokia-branded phones and tablets outside Japan, operating in conjunction with Foxconn. The deal also granted HMD the right to essential patents and featurephone software. HMD subsequently announced the Android-based Nokia 6 smartphone in January 2017. At Mobile World Congress, HMD additionally unveiled the Nokia 3 and Nokia 5 smartphones, as well as a re-imagining of Nokia's classic 3310 feature phone. While Nokia has no investment in the company, they do have some input in the new devices.

On 28 June 2016 Nokia demonstrated for the first time a 5G-ready network. In February 2017 Nokia carried out a 5G connection in Oulu, Finland using the 5GTF standard, backed by Verizon, on Intel architecture-based equipment.

On 5 July 2017, Nokia and Xiaomi announced that they have signed a business collaboration agreement and a multi-year patent agreement, including a cross license to each company's cellular standard essential patents.

In 2017, Nokia's brand value jumped 147 places to 188th place compared to 2016 in the Brand Finance ranking. Its rise was attributed to its health portfolio and new mobile phones developed by HMD Global.

On 19 January 2018, Nokia signed a deal with NTT Docomo, Japan's largest mobile operator, to provide 5G wireless radio base stations in the country by 2020.

On 29 January 2018, Nokia introduced the ReefShark line of 5G chipsets, claiming that it triples bandwidth to 84 Gbit/s. It will be released by Q3 2018. It also incorporates artificial intelligence technologies from Bell Labs.

On 13 March 2018, Solidium, the investment arm of the Finnish government, purchased a 3.3% stake in Nokia valued at €844 million.

On 7 May 2018, Nokia announced that it has acquired a California-based IoT startup, SpaceTime Insight.

In January 2019, the Canadian government announced that it will provide C$40 million to support Nokia's research on 5G technology.

A 2019 study revealed that Nokia phones performed far better than rivals Samsung, LG, Xiaomi, and Huawei in updating to the latest version of Android. The study, made by Counterpoint Research, found that 96 percent of Nokia phones were either sent with or updated to the latest Android version since Pie was released in 2018. Nokia's competitors were found to be all around roughly the 80 percent range.

On March 2, 2020, Nokia announced Pekka Lundmark as its new CEO. 

On March 25, 2020, Nokia completed the acquisition of Elenion Technologies, a U.S.-based company focusing on silicon photonics technology to improve economics of advanced optical connectivity solutions.

On May 27, 2020, Sari Baldauf succeeded Risto Siilasmaa as chairwoman of the board of directors. Kari Stadigh was appointed vice chair.

On June 29, 2020, Nokia announced that it had won a 5G contract worth approximately $450 million from Taiwan Mobile to build out the telecom operator's next-generation network as the sole supplier.
Nokia is a julkinen osakeyhtiö (public joint-stock company) listed on the Nasdaq Nordic/Helsinki and New York stock exchanges. Nokia has played a very large role in the economy of Finland, and it is an important employer in the country, working with multiple local partners and subcontractors. Nokia contributed 1.6% to Finland's GDP and accounted for about 16% of the country's exports in 2006.

Nokia comprises two business groups along with further subsidiaries and affiliated firms.

Nokia Networks is Nokia Corporation's largest division. It is a multinational data networking and telecommunications equipment company headquartered in Espoo, Finland, and is the world's third-largest telecoms equipment manufacturer, measured by 2017 revenues (after Huawei and Cisco). In USA it competes with Ericsson on building 5G networks for operators, while Huawei Technologies and ZTE Corporation were effectively banned.

It has operations in around 150 countries.

Nokia Networks provides wireless and fixed network infrastructure, communications and networks service platforms and professional services to operators and service providers. It focuses on GSM, EDGE, 3G/W-CDMA, LTE and WiMAX radio access networks, supporting core networks with increasing IP and multiaccess capabilities and services.

The Nokia Siemens Networks (NSN) brand identity was launched at the 3GSM World Congress in Barcelona in February 2007 as a joint venture between Nokia (50.1%) and Siemens (49.9%), although it is now wholly owned by Nokia. In July 2013, Nokia bought back all shares in Nokia Siemens Networks for a sum of US$2.21 billion and renamed it to Nokia Solutions and Networks, shortly thereafter changed to simply Nokia Networks.

Nokia Technologies is a division of Nokia that develops consumer products and licenses technology including the "Nokia" brand. Its focuses are imaging, sensing, wireless connectivity, power management and materials, and other areas such as the IP licensing program. It consists of three labs: Radio Systems Lab, in areas of radio access, wireless local connectivity and radio implementation; Media Technologies Lab, in areas of multimedia and interaction; and Sensor and Material Technologies Lab, in areas of advanced sensing solutions, interaction methods, nanotechnologies and quantum technologies. Nokia Technologies also provides public participation in its development through the "Invent with Nokia" program. It was created in 2014 following a restructuring of Nokia Corporation.

In November 2014, Nokia Technologies launched its first product, the Nokia N1 tablet computer. In July 2015, Nokia Technologies introduced a VR camera called OZO, designed for professional content creators and developed in Tampere, Finland. With its 8 synchronized shutter sensors and 8 microphones, the product can capture stereoscopic 3D video and spatial audio.

On 31 August 2016, Ramzi Haidamus announced he would be stepping down from his position as president of Nokia Technologies. Brad Rodrigues, previously head of strategy and business development, assumed the role of interim president. On 30 June 2017, Gregory Lee, previously CEO of Samsung Electronics in North America, was appointed Nokia Technologies CEO and president.

Nokia Bell Labs is a research and scientific development firm that was once the R&D arm of the American Bell System. It became a subsidiary of Nokia Corporation after the takeover of Alcatel-Lucent in 2016.

NGP Capital (formerly Nokia Growth Partners) is a global venture capital firm, focusing in investments on growth stage "Internet of things" (IoT) and mobile technology companies. NGP holds investments throughout the U.S., Europe, China and India. Their portfolio consists of companies in mobile technology including the sectors Connected Enterprise, Digital Health, Consumer IoT and Connected Car. Following a $350 million funding for IoT companies in 2016, NGP manages $1 billion worth of assets.

Nokia had previously promoted innovation through venture sponsorships dating back to 1998 with Nokia Venture Partners, which was renamed BlueRun Ventures and spun off in 2005. Nokia Growth Partners (NGP) was founded in 2005 as a growth stage venture fund as a continuation of the early successes of Nokia Venture Partners. In 2017, the company was renamed to NGP Capital.

NGP's largest exits include GanJi, UCWeb, Whistle, Rocket Fuel, Swype, Summit Microelectronics and Netmagic.

Nuage Networks is a venture providing software-defined networking solutions. It was formed by Alcatel-Lucent in 2013 to develop a software overlay for automating and orchestrating hybrid clouds. It has been part of Nokia following their acquisition of Alcatel-Lucent in 2016. Throughout 2017 Nuage sealed deals with Vodafone and Telefonica to provide its SD-WAN architecture to their servers. BT had already been a client since 2016. A deal with China Mobile in January 2017 also used Nuage's software-defined networking technology for 2,000 public cloud servers at existing data centers in China, and another in October 2017 with China Pacific Insurance Company.

The company is based in Mountain View, California and the CEO is Sunil Khandekar.

Alcatel Mobile is a mobile phone brand owned by Nokia since 2016. It has been licensed since 2005 to Chinese company TCL when it was under the ownership of Alcatel (later Alcatel-Lucent) in a contract until 2024.

HMD Global is a mobile phone company based in Espoo, Finland. The Nokia brand has been licensed by former Nokia employees who founded HMD Global and introduced Nokia-branded Android-based devices to the market in 2017. Nokia has no investment in the company but retains some input in the development of its devices.

Alcatel Submarine Networks (ASN) is a provider of turnkey undersea network solutions. The business unit develops technology and offers installation services for optical submarine cable network links across the world's oceans.

The control and management of Nokia is divided among the shareholders at a general meeting and the Nokia Group Leadership Team (left), under the direction of the board of directors (right). The chairman and the rest of the Nokia Leadership Team members are appointed by the board of directors. Only the chairman of the Nokia Leadership Team can belong to both the board of directors and the Nokia Group Leadership Team. The Board of Directors' committees consist of the Audit Committee, the Personnel Committee, and the Corporate Governance and Nomination Committee.

The operations of the company are managed within the framework set by the Finnish Companies Act, Nokia's Articles of Association, and Corporate Governance Guidelines, supplemented by the board of directors' adopted charters. On 25 November 2019, Nokia announced that it would discontinue the role of Chief Operating Officer (COO) and distribute its functions to other company leaders. As a result, Chief Operating Officer Joerg Erlemeier decided to step down, effective 1 January 2020.

Nokia is a public limited liability company and is the oldest company listed under the same name on the Helsinki Stock Exchange, beginning in 1915. Nokia has had a secondary listing on the New York Stock Exchange since 1994. Nokia shares were delisted from the London Stock Exchange in 2003, the Paris Stock Exchange in 2004, the Stockholm Stock Exchange in 2007 and the Frankfurt Stock Exchange in 2012. Due to the acquisition of Alcatel-Lucent in 2015, Nokia listed its shares again on the Paris Stock Exchange and was included in the CAC 40 index on 6 January 2016 but later removed on 18 September 2017.

In 2007, Nokia had a market capitalization of €110 billion; by 17 July 2012 this had fallen to €6.28 billion, and by 23 February 2015, it increased to €26.07 billion.

Nokia's official corporate culture manifesto since the 1990s is called "The Nokia Way". It emphasizes the speed and flexibility of decision-making in a flat, networked organization.

The official business language of Nokia is English. All documentation is written in English, and is used in official intra-company communication.

In 1992, Nokia adopted values that were defined with the key words "respect", "achievement", "renewal" and "challenge". In May 2007, the company redefined its values after initiating a series of discussion across its worldwide branches regarding what the new values of the company should be. Based on the employee suggestions, the new values were defined as: "Engaging You", "Achieving Together", "Passion for Innovation" and "Very Human". In August 2014, Nokia redefined its values again after the sale of its Devices business, using the original 1992 values again.

Nokia are based at Karaportti in Espoo, Finland, just outside capital Helsinki. It has been their head office since 2014 after moving from the purpose-built Nokia House in Espoo as part of the sale of the mobile phone business to Microsoft. The building in Karaportti was previously the headquarters of NSN (now Nokia Networks).

In 2018, Nokia received the Leading Lights award for most innovative cable/video product and was named to Ethisphere's 2018 world's most ethical companies list.

In 2008, Nokia Siemens Networks, a joint venture between Nokia and Siemens AG, reportedly provided Iran's monopoly telecom company with technology that allowed it to intercept the Internet communications of its citizens. The technology reportedly allowed Iran to use deep packet inspection to read and change the content of emails, social media, and online phone calls. The technology "enables authorities to not only block communication but to monitor it to gather information about individuals, as well as alter it for disinformation purposes".

During the post-election protests in Iran in June 2009, Iran's Internet access was reported to have slowed to less than a tenth of its normal speeds, which experts suspected was due to use of deep packet inspection.

In July 2009, Nokia began to experience a boycott of their products and services in Iran. The boycott was led by consumers sympathetic to the post-election protest movement and targeted companies deemed to be collaborating with the regime. Demand for handsets fell and users began shunning SMS messaging.

Nokia Siemens Networks asserted in a press release that it provided Iran only with a "lawful intercept capability solely for monitoring of local voice calls" and that it "has not provided any deep packet inspection, web censorship, or Internet filtering capability to Iran".

In 2009, Nokia heavily supported a law in Finland that allows companies to monitor their employees' electronic communications in cases of suspected information leaking. Nokia denied rumors that the company had considered moving its head office out of Finland if laws on electronic surveillance were not changed. The Finnish media dubbed the law "Lex Nokia" because it was implemented as a result of Nokia's pressure.

The law was enacted, but with strict requirements for implementation of its provisions. No company had used its provisions prior to 25 February 2013, when the Office of Data Protection Ombudsman confirmed that city of Hämeenlinna had recently given the required notice.

In October 2009, Nokia filed a lawsuit against Apple Inc. in the U.S. District Court of Delaware claiming that Apple infringed on 10 of its patents related to wireless communication including data transfer. Apple was quick to respond with a countersuit filed in December 2009 accusing Nokia of 11 patent infringements. Apple's general counsel, Bruce Sewell went a step further by stating, "Other companies must compete with us by inventing their own technologies, not just by stealing ours." This resulted in a legal battle between the two telecom majors with Nokia filing another suit, this time with the U.S. International Trade Commission (ITC), alleging Apple of infringing its patents in "virtually all of its mobile phones, portable music players and computers". Nokia went on to ask the court to ban all U.S. imports of the Apple products, including the iPhone, Macintosh and iPod. Apple countersued by filing a complaint with the ITC in January 2010.

In June 2011, Apple settled with Nokia and agreed to an estimated one time payment of $600 million and royalties to Nokia. The two companies also agreed on a cross-licensing patents for some of their patented technologies.

Nokia's Indian subsidiary was charged in January 2013 with non-payment of Indian Tax Deducted at Source and transgressing transfer pricing norms in India. The unpaid TDS of 30 billion, accrued during a course of six years, was due to royalty paid by the Indian subsidiary to its parent company.




</doc>
<doc id="21243" url="https://en.wikipedia.org/wiki?curid=21243" title="Nortel">
Nortel

Nortel Networks Corporation (Nortel), formerly commonly known as Northern Electric and Northern Telecom, was a multinational telecommunications and data networking equipment manufacturer headquartered in Mississauga, Ontario, Canada. It was founded in Montreal, Quebec, in 1895 as the Northern Electric and Manufacturing Company. Until an antitrust settlement in 1949, Northern Electric was owned principally by Bell Canada and the Western Electric Company of the Bell System, producing large volumes of telecommunication equipment based on licensed Western Electric designs.

At its height, Nortel accounted for more than a third of the total valuation of all companies listed on the Toronto Stock Exchange (TSX), employing 94,500 people worldwide.

In 2009, Nortel filed for bankruptcy protection in Canada and the United States, triggering a 79% decline of its corporate stock. The bankruptcy case was the largest in Canadian history, and left pensioners, shareholders and former employees with enormous losses. By 2016 Nortel had sold billions of dollars' worth of assets. Courts in the U.S. and Canada approved a negotiated settlement of bankruptcy proceedings in 2017.

Alexander Graham Bell conceived the technical aspects of the telephone and invented it in July 1874, while residing with his parents at their farm in Tutela Heights, on the outskirts of Brantford, Ontario. He later refined its design at Brantford after producing his first working prototype in Boston. Canada's first telephone factory, created by James Cowherd of Brantford, was a three-story brick building that soon started manufacturing telephones for the Bell System, leading to the city's style as "The Telephone City".

After Cowherd's death in 1881 which resulted in the closure of his Brantford factory, a mechanical production department was created within the Bell Telephone Company of Canada and production of Canadian telephone equipment was transferred to Montreal in 1882, to compensate the restrictions on importing telephone equipment from the United States. In addition to telephones, four years later, the department started manufacturing switchboards, at first the 50-line Standard Magneto Switchboard. The small manufacturing department expanded yearly with the growth and popularity of the telephone to 50 employees in 1888. By 1890 it had been transformed into its own branch of operations with 200 employees, and a new factory was under construction.

As the manufacturing branch expanded, its production ability increased beyond the demand for telephones, and it faced closure for several months a year without manufacturing other products. The Bell Telephone Company of Canada's (later renamed to Bell Canada) charter prohibited the company to build other products. In 1895, the Bell Telephone of Canada spun off its manufacturing arm to build telephones for sale to other companies, as well as other products, such as fire alarm boxes, police street call boxes, and fire department call equipment. This company was incorporated as the Northern Electric and Manufacturing Company Limited.

Northern Electric and Manufacturing Company Limited was incorporated on December 7, 1895, by the following corporate members (or Board of Directors): Charles Fleetford Sise Sr., President of Bell Canada – Provisional Director; Robert Mackay, merchant – Provisional Director; Hugh Paton, manager of the Shedden Company – Provisional Director; The Hon. Joseph Rosaire Thibaudeau, Senator – Provisional Director; Robert Archer, gentleman – Provisional Director; Charles P. Sclater, secretary – Provisional Director; Lewis B. McFarlane, manager, all of the city and district of Montreal, Quebec.

The initial stock capital was $50,000 at $100 per share, with 93 percent held by the Bell Telephone Company of Canada and the remainder held by the seven corporate members above. The first general stockholders meeting was held on March 24, 1896.

In December 1899, The Bell Telephone Company of Canada bought a cabling company for $500,000; a Canadian charter named it "The Wire and Cable Company". Northern Electric and Manufacturing further expanded its product line in 1900, manufacturing the first Canadian wind-up gramophones that played flat discs. In 1911 the Wire and Cable company changed its name to the "Imperial Wire and Cable Company".

The construction of a new manufacturing plant started in 1913 at Shearer Street in Montreal, Quebec, Canada, as preparations began for the two manufacturing companies' integration. Then, in January 1914, the Northern Electric and Manufacturing Company and the Imperial Wire and Cable Company merged into the "Northern Electric Company", commonly known simply as Northern Electric, and the new company opened the doors on a new manufacturing plant in January 1915. This facility at Shearer Street was the primary manufacturing centre until the mid-1950s. Edward Fleetford Sise was the president and his brother Paul Fleetford Sise was the vice-president and general manager.
During the First World War Northern Electric manufactured the Portable Commutator, a one-wire telegraphic switchboard for military operations in the field. In 1922, Northern started to produce, for $5, the "Peanut" vacuum tube, which required only a single dry-cell battery. The use of alternating current was still under development during this time. The "Northern Electric Peanut tube was the smallest tube made, and drew only one-tenth of an ampere and was the most remarkable radio frequency amplifier ever made." During the 1920s Northern Electric made kettles, toasters, cigar lighters, electric stoves, and washing machines. In January 1923, Northern Electric started to operate an AM radio station with call letters CHYC, in the Shearer Street plant, and much of the programming was religious services for the Northern Electric employees and families in the community. In July 1923, CHYC-AM was the first radio station to provide entertainment to the riders of the transcontinental train, in a parlor car fitted with a radio set to receive the broadcast as it left Montreal and traveled west. Later in the 1920s, Northern created the first talking movie sound system in the British Empire for a theater in Montreal.

During the Great Depression in the 1930s, Northern Electric was affected, like most other companies. From the beginning of 1930 through the end of 1933, sales dropped from $34 million to $8.2 million, and the number of employees dropped from 6,100 to 2,400.

In 1949, an antitrust suit in the U.S. forced AT&T/Western Electric to sell its stake in Northern Electric to Bell Canada. AT&T spun off Northern Electric in 1956. Deprived of its Western Electric tie, Northern began developing its own products. In 1953, Northern Electric produced its first television sets using tubes made by RCA.
Bell Canada acquired 100 percent of Northern Electric in 1964; through public stock offerings starting in 1973, Bell's ownership of Northern Electric and its successors would be reduced, though it continued to have majority control.

In 1966, the Northern Electric research lab, Northern Electric Laboratories (the predecessor to Bell-Northern Research), started looking into the possibilities of fiber optic cable, and in 1969, began work on digitizing telephone communications. Also in 1969, Northern began making inroads into the U.S. market with its switching systems. In 1972, it opened its first factory in the U.S. in Michigan. In 1975, Northern began shipping its first digital switching systems, one of the earliest such systems to be sold.

Northern Telecom was, with Bell-Northern Research, in the early 1970s a part owner of MicroSystems International, a semiconductor manufacturer based in Nepean, outside Ottawa.

In March 1976, the company name was changed to Northern Telecom Limited, and management announced its intention to concentrate the company's efforts on digital technology. Northern Telecom was the first company in its industry to announce and to deliver a complete line of fully digital telecommunications products. The product line was branded "Digital World" and included the well known DMS-100, a fully digital central office switch serving as many as 100,000 lines, which was a key contributor to the company's revenue for close to 15 years.

Starting in 1977, Nortel grew rapidly after the introduction of its DMS line of digital central office telephone switches, especially after the AT&T breakup in 1984. Northern Telecom became a significant supplier in Europe and China and was the first non-Japanese supplier to Nippon Telegraph and Telephone.

In 1983, due to deregulation, Bell Canada Enterprises (later shortened to BCE) was formed as the parent company to Bell Canada and Northern Telecom. Bell-Northern Research was jointly owned 50–50 by Bell Canada and Northern Telecom. The combined three companies were referred to as the tricorporate.

As Nortel, the streamlined identity it adopted for its 100th anniversary in 1995, the company set out to dominate the burgeoning global market for public and private networks.

In 1998, with the acquisition of Bay Networks, the company's name was changed to Nortel Networks to emphasize its ability to provide complete solutions for multiprotocol, multiservice, global networking over the Internet and other communications networks. As a consequence of the stock transaction used to purchase Bay Networks, BCE ceased to be the majority shareholder of Nortel.

In 1999, Nortel outsourced several of its manufacturing operations to North American contractors.

In 2000, BCE spun out Nortel, distributing its holdings of Nortel to its shareholders. Bell-Northern Research was gradually absorbed into Nortel, as it first acquired a majority share in BNR, and eventually acquired the entire company.

In the late 1990s, stock market speculators, hoping that Nortel would reap increasingly lucrative profits from the sale of fibre optic network gear, began pushing up the company's share price to unheard-of levels despite the company's repeated failure to turn a profit. Under the leadership of chief executive officer (CEO) John Roth, sales of optical equipment had been robust in the late 1990s, but the market was soon saturated. When the speculative telecom bubble of the late 1990s reached its pinnacle late in the year 2000, Nortel was to become one of the most spectacular casualties. Nortel's revenues would be dented by a saturated market and the failure of WorldCom, which was a major customer. 

At its height, Nortel accounted for more than a third of the total valuation of all the companies listed on the Toronto Stock Exchange (TSX), employing 94,500 worldwide, with 25,900 in Canada alone. Nortel's market capitalization fell from C$398 billion in September 2000 to less than C$5 billion in August 2002, as Nortel's stock price plunged from C$124 to C$0.47. When Nortel's stock crashed, it took with it a wide swath of Canadian investors and pension funds and left 60,000 Nortel employees unemployed. Roth was criticized after it was revealed that he cashed in his own stock options for a personal gain of C$135 million in 2000 alone.

CEO John Roth retired in 2001. His planned successor and chief operating officer (COO), Clarence Chandran, already on sick leave due to complications following his 1997 stabbing in Singapore,
decided to quit, however. Chief financial officer (CFO) Frank Dunn was eventually chosen as Roth's permanent replacement.

Frank Dunn presided over a dramatic restructuring of Nortel, which included laying off two-thirds of its workforce (60,000 staff) and writedowns of nearly US$16 billion in 2001 alone. This had some initial perceived success in turning the company around, with an unexpected return to profitability reported in the first quarter of 2003. The black ink triggered a total of $70 million in bonuses to the top 43 managers, with $7.8 million going to Dunn alone, $3 million to chief financial officer Douglas Beatty, and $2 million to controller Michael Gollogly. Independent auditor Deloitte & Touche advised audit committee chairman John Cleghorn and board chairman "Red" Wilson to look into the suspicious results, who promptly hired the law firm WilmerHale to vet the financial statements. In late October 2003, Nortel announced that it intended to restate approximately $900 million of liabilities carried on its previously reported balance sheet as of June 30, 2003, following a comprehensive internal review of these liabilities. The company stated that the restatement's principal effects would be a reduction in previously reported net losses for 2000, 2001, and 2002 and an increase in shareholders’ equity and net assets previously reported on its balance sheet. A dozen of the company's most senior executives returned $8.6 million of bonuses they were paid based on the erroneous accounting. Investigators ultimately found about $3 billion in revenue had been booked improperly in 1998, 1999, and 2000. More than $2 billion was moved into later years, about $750 million was pushed forward beyond 2003 and about $250 million was wiped away completely. The accounting scandal hurt both Nortel's reputation and finances, as Nortel spent an estimated US$400 million on outside auditors and management consultants to retrain staff.

To improve its liquidity, in 2003 Nortel arranged a US$750 million credit support facility with Export Development Canada. Walter Robinson of the Canadian Taxpayers Federation denounced the line of credit, calling it "corporate welfare at its worst."

On April 28, 2004 amidst the accounting scandal, three of Nortel's top lieutenants—Douglas Beatty, CEO Frank Dunn and Michael Gollogly—were fired for financial mismanagement. They were later charged with fraud by the RCMP. The trial began on January 16, 2012, ending with acquittals for all three.

The United States Securities and Exchange Commission (SEC) also filed charges against them and four vice-presidents for civil fraud. On December 19, 2014, remaining civil charges from the Ontario Securities Commission and SEC were simultaneously dropped.

After Dunn's firing, retired United States Admiral Bill Owens – at the time a member of the board of directors – was appointed interim CEO. Nortel Networks subsequently returned to using the Nortel name for branding purposes only (the official company name was not changed). Nortel acquired PEC Solutions, a provider of information technology and telecommunications services to various government agencies and departments, in June 2005 and renamed it Nortel Government Solutions Incorporated (NGS). LG Electronics and Nortel formed a joint venture in August, with Nortel owning 50% plus one share, to offer telecom and networking solutions in the wireline, optical, wireless and enterprise areas for South Korean and global customers.

Peter W. Currie, previously the Chief Financial Officer (CFO) of the Royal Bank of Canada, was named CFO of Nortel in 2005, having previously served as Northern Telecom's CFO in the 1990s. Gary Daichendt, the former Chief Operating Officer of Cisco Systems, was hired as President and COO, and was expected to succeed Owens as CEO. Shortly afterward, Daichendt appointed ex-Cisco Chief Science Officer Gary Kunis as Chief Technology Officer (CTO). Both Garys were concerned about the overall direction of Nortel, especially when compared to Cisco, their previous employer. Just three months later, Daichendt resigned after both his restructuring plan and his suggestion that Owens and Currie leave the company immediately were rejected by the board of directors. Kunis quit shortly thereafter. At the year's end, directors Lynton "Red" Wilson and John Cleghorn retired from the board.

In 2004 Nortel discovered that hackers they believed to be in China had had free rein within the Nortel network for more than a decade before their collapse. The fall of Nortel coincided with the rise of Huawei.

Mike S. Zafirovski, who had served as President and CEO of GE Lighting and then as Motorola President and COO, succeeded Owens as president and CEO on November 15, 2005. Motorola filed a suit against Zafirovski's hiring, alleging that his new position would break the terms of the non-disclosure agreement he had signed. Nortel agreed to pay $11.5 million on his behalf to settle the lawsuit. Nortel also paid out US$575 million and 629 million common shares in 2006 to settle a class-action lawsuit that accused the company of misleading investors about the company's health.

Currie stepped down as Executive Vice President and CFO in early 2007. In February 2007, Nortel announced its plans to reduce its workforce by 2,000 employees, and to transfer an additional 1,000 jobs to lower-cost job sites. The Securities and Exchange Commission filed civil fraud charges against Nortel for accounting fraud from 2000 to 2003; the fraud was allegedly to close gaps between its true performance, its internal targets and Wall Street expectations. Nortel settled the case, paying $35 million, which the Commission distributed to affected shareholders, and reported periodically to the Commission on remedial measures to improve its financial accounting.

Nortel announced plans in February 2008 to eliminate 2,100 jobs, and to transfer another 1,000 jobs to lower-cost centres. As part of the reductions, Nortel shut down its Calgary campus in 2009.

During its reporting of third quarter 2008 results, Nortel announced it would restructure into three vertically-integrated business units: Enterprise, Carrier Networks, and Metro Ethernet Networks. As part of the decentralization of its organization, four executive positions were eliminated, effective January 1, 2009: Chief Marketing Officer - Lauren Flaherty; Chief Technology Officer - John Roese; Global Services President - Dietmar Wendt; and Executive Vice President Global Sales - Bill Nelson. A net reduction of 1,300 jobs was also announced. As its stock price dropped below $1, the New York Stock Exchange notified Nortel that it would be delisted if its common shares failed to rise above $1 per share within 6 months. Rumours continued to persist of Nortel's poor financial health, amid the late 2000s recession, and its bids for government funds were turned down.

On January 14, 2009, Nortel filed for protection from creditors, in the United States under Chapter 11 of the United States Bankruptcy Code, in Canada under the Companies' Creditors Arrangement Act, and in the United Kingdom under the Insolvency Act 1986. Nortel was the first major technology company to seek bankruptcy protection in this global downturn. Nortel had an interest payment of $107 million due the next day, approximately 4.6% of its cash reserves of approximately $2.3 billion. After the announcement, the share price fell more than 79% on the Toronto Stock Exchange. Export Development Canada agreed to provide up to C$30 million in short-term financing through its existing credit support facility with Nortel. The Canadian government resisted characterizing its position on Nortel as a bailout.

Nortel initially hoped to re-emerge from bankruptcy, implementing a retention bonus plan in an effort to retain its top executives during the restructuring period. These bonuses, totaling US$45 million, were targeted at 1,000 executive positions. At the end of January 2009, Nortel announced that it would be discontinuing its WiMAX business and its agreement with Alvarion. Nortel subsequently sold its Layer 4–7 application delivery business to Israeli technology firm Radware for $18 million, after Radware had initially placed a stalking horse bid. Nortel had acquired the application switch product line in October 2000 when it purchased Alteon WebSystems.

With the worsening recession and stock market decline deterring potential companies from bidding for Nortel's assets, and many of Nortel's major customers reconsidering their relationships with the restructuring company, in June Nortel announced that it no longer planned to emerge from bankruptcy protection, and would seek buyers for all of its business units. After announcing it planned to sell off all of its assets, Nortel shares were delisted from the Toronto Stock Exchange on June 26, 2009 at a price of $0.185 per share, down from its high in 2000 when it comprised a third of the S&P/TSX composite index. Mike Zafirovski subsequently resigned in August, and Nortel's board of directors was reorganized with three members instead of nine. Nortel handed out $14.2 million in cash compensation to seven executives in 2009. Nortel also paid out $1.4 million to 10 former and current directors, and paid $140 million to lawyers, pension, human resources and financial experts helping to oversee the company's bankruptcy proceedings.

Nokia Siemens Networks made a stalking horse bid to purchase Nortel's CDMA and LTE assets for $650 million. By the July 21 deadline for additional bids, MatlinPatterson and Ericsson had made offers, and Ericsson emerged as the victor in the following auction, with a purchase price of $1.13 billion. Avaya won an auction for Nortel's Enterprise Solutions business, including Nortel's stake in Nortel Government Solutions and DiamondWare, for $900 million, after having placed a stalking horse bid of $475 million. In November, Nortel sold its MEN (Metro Ethernet Networks) unit to Ciena Corporation for US$530 million in cash and US$239 million in convertible notes,
and its GSM business at auction to Ericsson and Kapsch for US$103 million. Hitachi purchased the Next Generation Packet Core assets. As insurance against judgments in class action lawsuits filed by former employees, John Roth filed in December 2009 for a US$1 billion indemnification from Nortel, joining the list of U.S. creditors.

In February 2010, Ernst & Young, the court-appointed monitor of Nortel's Canadian bankruptcy proceedings, reported that the assets of Nortel's Health and Welfare Trust had a shortfall of $37 million in its net assets as of December 31, 2008. The trust supports pensioners' medical, dental and life insurance benefits, as well as income support for some groups such as long-term disability recipients. Also in February, Nortel negotiated a $57-million deal to wind up the health care and other benefits provided to former Canadian employees. Shortly afterwards, Nortel proposed spending $92.3M on retention bonuses for 1,475 employees in its Nortel Business Services and Corporate groups, with $2.5 million in incentives going to Christopher Ricaute, president of Nortel Business Services; $27 million allocated for Canadian employees; and $55 million allocated for U.S. employees. The proposed plan was later extended by an additional $27 million. Claiming that the retention bonuses proposal was extraordinary, acting US trustee Roberta DeAngelis objected to the payment of $55.6 million to 866 employees. However, court-appointed representatives for Nortel's former employees, who are creditors in the Ontario bankruptcy court, have signed an agreement to not oppose any employee incentive program.

Genband purchased the Carrier VoIP and Application Solutions (CVAS) unit in May 2010, as Nortel accepted its stalking horse bid of $282 million, with adjustments that decreased the net sale price to about $100 million, without a formal bidding process. Ericsson purchased Nortel's share in its joint venture with LG Electronics for US$242 million, forming LG-Ericsson, in June 2010. Ericsson also purchased Nortel's final operating unit, the Multi-Service Switch division, in September 2010 for US$65 million. Nortel's Ottawa campus on Carling Avenue was purchased by Public Works and Government Services Canada (PWGSC) in October 2010 for a cash purchase price of CDN$208 million, to serve as the new home of Canada's National Defence Headquarters.

Nortel's 53.13% stake in Turkish company Nortel Netaş was acquired by One Equity Partners (OEP) and Rhea Investments for $68 million in December 2010.

The last major asset of Nortel, approximately 6,000 patents and patent applications encompassing technologies such as wireless, wireless 4G, data networking, optical, voice, Internet, and semiconductors, was sold for $4.5 billion to a consortium including Apple, EMC, Ericsson, Microsoft, BlackBerry Limited, and Sony, pending American and Canadian court approval. (Google had placed the initial stalking horse bid of $900 million and later upped the bid to $1,902,160,540, then $2,614,972,128, and eventually $3.14159 billion, which are references to Brun's constant, Meissel–Mertens constant, and pi.) Bankruptcy filings state that Nortel owes former Canadian engineers $285,000 for patent awards that were not paid.

In October 2011, the administrators of Nortel's British subsidiary lost their appeal to overturn a court order requiring them to pay £2.1 billion into Nortel's underfunded pension plan.

Nortel Networks U.S. retirement income plan is now managed by PBGC Pension Benefit Guaranty Corporation

January 2014, a pact between U.S. and European divisions of Nortel Networks is approved by a U.S. court.

However, litigation continued. In April 2016, Nortel Networks Corp. went back to court for a fresh round of legal arguments in a seven-year-old bankruptcy which has cost creditors about $2 billion including attorney fees.

Courts in the U.S. and Canada approved a negotiated settlement among competing creditors in January 2017.

Nortel made telecommunications, computer network equipment and software. It served both general businesses and communications carriers (landline telephone, mobile phone, and cable TV carriers). Technologies included telephonic (voice) equipment, fiber optics, local wireless, and multimedia.

Past products included:

In 2016 the Canadian Broadcasting Corporation, reported that lawyers and accountants received $2.5 billion from Nortel's estate

After bankruptcy, Nortel's estate faced $300 million in claims to pay for environmental damage at various sites, including Belleville, Brockville, Kingston and London. 

In September 1991, Julian Assange was discovered in the act of hacking into the Melbourne master terminal of Nortel.

In 2012, "The Wall Street Journal" reported that in 2004, Nortel discovered that crackers gained almost-complete access to Nortel's systems. Beginning in at least 2000 they accessed documents including emails, technical papers, research, development reports, and business plans. The breach was not properly addressed by the time the company filed for bankruptcy in 2009. Hackers working from Chinese IP addresses had allegedly used seven passwords of Nortel executives, including a former CEO, to penetrate networks owned by the company.

Brian Shields, a former senior systems security advisor for Nortel, led an internal investigation into the breach and exposed rootkit software on at least two machines in 2009 that allowed hackers to control them remotely and monitor email. Despite the original discovery in 2004 and the subsequent investigation that led to the rootkit detection in 2009, Nortel allegedly ignored the problem and failed to disclose it to potential buyers of its business. Avaya and Genband both acquired parts of Nortel, and some employees used old Nortel machines connected to the new companies' networks. Although Avaya says it has dealt with the issue, Shields says "it's despicable that Nortel didn't say anything", leaving it up to him to inform the new company of his investigation. Nortel refused to comment on The Wall Street Journal report, but former CEO Mike Zafirovski, in charge between 2005 and 2009, claimed the company "did not believe it was a real issue".
Shields alleged that the hacking may have benefited Chinese competitors such as Huawei and ZTE. While unable to offer conclusive proof Shields stated that "When 2000 came along, then it was a downward slide. And that coincidentally is the year when Huawei started selling on the international market. How coincidental."

On February 16, 2003, the Winnipeg Sun published an article criticising the Canadian Federal government for propping up "mega-loser Nortel" through Export Development Canada (EDC). The article interviewed Walter Robinson of the Canadian Taxpayers Federation who termed this EDC support as "corporate welfare at its worst.". Mr Robinson was appalled that Canadians who already lost billions on Nortel on the stock market would be asked for even more money through their taxes to support Nortel.

EDC had agreed to provide up to $30 million in short-term financing through an existing bonding facility. This money was previously available to Nortel, and no special funding was made available. The Canadian government resisted characterizing its position on Nortel as a bailout.

There have been reports of financial irregularities at Nortel's Health and Welfare Trust. Diane Urquhart, a financial analyst, testified before a parliamentary committee that $100 million is missing from the HWT and that a $37 million loan to the corporation has not been paid back. The HWT was an unregistered trust maintained by Nortel to provide medical, dental, life insurance, long-term disability and survivor income and pension transition benefits.
Until 2005 Nortel fully funded the disability insurance in its HWT. However, it is alleged that since then, the HWT Governance Committees and third party trustee, Northern Trust, breached their fiduciary duties to protect Nortel's disabled employees and survivors of deceased employees by allowing Nortel to misdirect over $100 million from the HWT for purposes inconsistent with the terms of the HWT.
As of March 1, 2012, Northern Trust continues to act as the paying agent for Canadian Nortel pensioners.

In 2007, both the U.S. Securities and Exchange Commission and the Ontario Securities Commission laid charges against former senior financial officials from Nortel including Frank Dunn who was fired from Nortel in 2004. Frank Dunn was promoted from chief financial officer to replace John A. Roth as CEO in November 2001. According to the SEC, Dunn and three other financial officers began to fudge revenue by misusing "bill and hold" transactions starting "no later than September, 2000". The SEC said that at least a year's worth of the alleged book-keeping took place while John Roth was still CEO of Nortel, even though no charges were laid against him.

On June 23, 2010, the News and Observer published a story criticizing treatment pensioners have been receiving from their former employer, Nortel. According to the article, Nortel has asked a federal court to terminate medical coverage, prescription drug coverage, long-term disability, and life insurance of 4,000 retirees and dependents, claiming the benefits are costing the company $2 million per month. Nortel blamed the company's creditors for this decision.

In the middle of the decade several class-action lawsuits were filed against John Roth and others, by former employees who felt that their 401K company plans were depleted due to misrepresentation by the defendants. They claimed they were duped into investing in Nortel stock, when those who encouraged them to do so allegedly knew that the company was ailing. John Roth left Nortel in 2001 with more than $130 million.

In 2009, Mr. Roth filed a claim for $1 billion, aiming to become a creditor to the assets of Nortel along with all other Nortel employees, in case the class action lawsuits against him succeeded.

During Nortel's 2002 annual shareholders' meeting held in Halifax, Nova Scotia, several shareholders (including Robert Verdun) complained about non-arms-length relationships with service providers such as director Yves Fortier, who provided legal services to Nortel while sitting on its board, and Nortel's auditors, Deloitte & Touche LLP, who were paid $15 million for non-auditing services.

In 2013 workers preparing the former Nortel headquarters for the Department of National Defence discovered electronic eavesdropping devices. The bugs found were older and non-operational leading Canadian intelligence to draw the conclusion that the former tenant Nortel and not the future tenant MND was the target.

In 2001 Nortel identified knockoff products circulating in the Chinese market, in which they did not compete. Nortel management chose not to press the issue.

Nortel's current headquarters is located at 5945 Airport Road in Mississauga, Ontario. Previous locations of its head offices include Brampton, Ontario (sold to Rogers Communications in 2006 and now known as Rogers Park, Brampton) and 195 The West Mall in Toronto (now used by SNC-Lavalin).

Nortel expanded into the U.S. in 1971. The company eventually had employees in over 100 locations in the U.S. with R&D, software engineering, and sales centres in many states including California, Florida, Georgia, Illinois, Maryland, Massachusetts, North Carolina, Texas, and Virginia. Nortel's full-service R&D centres were located in Ottawa (its R&D headquarters), Beijing, and Guangzhou. In Canada, Nortel also has R&D sites in Montreal, Belleville, and Calgary. In the United States, Nortel's major R&D sites were in Research Triangle Park (North Carolina), Richardson (Texas), Billerica (Massachusetts), and Santa Clara.

Nortel had a significant presence in Europe, Middle East, Africa, the Caribbean, and Latin America. Nortel delivered network infrastructure and communication services to customers across Asia in (mainland) China, Hong Kong, Taiwan, South Korea, Japan, Singapore, Thailand, Malaysia, India, Pakistan, Australia, New Zealand, and Turkey (Nortel owned 53.17% of Nortel Netaş, originally established as a joint venture with Turkish PTT in 1967).
In addition, the company had three joint ventures in the People's Republic of China, including Guangdong Nortel Telecommunications Equipment (GDNT), which operated Nortel's full service R&D centres in China.

At the start of 2010, based on membership in Nortel's benefit plan, there were 1,637 employees working for Nortel Networks and 982 working for Nortel Technology in Canada.
In February 2008, Nortel employed approximately 32,550 people worldwide, including 6,800 employees in Canada and 11,900 in the United States. Nortel operations were divided into the following segments:

Nortel's board of directors resigned and the board disbanded effective October 3, 2012. All remaining executive officers also resigned effective this date. As part of the wind-down process, a court order was issued providing Ernst & Young Inc., the court-appointed monitor in Nortel's creditor protection proceedings, the ability to exercise any powers which may be properly exercised by a board of directors of Nortel.






</doc>
<doc id="21244" url="https://en.wikipedia.org/wiki?curid=21244" title="Nile">
Nile

The Nile (, , Bohairic , Nobiin: Áman Dawū) is a major north-flowing river in northeastern Africa, and is the longest river in Africa and the disputed longest river in the world, as the Brazilian government says that the Amazon River is longer than the Nile. The Nile is about long and its drainage basin covers eleven countries: Tanzania, Uganda, Rwanda, Burundi, the Democratic Republic of the Congo, Kenya, Ethiopia, Eritrea, South Sudan, Republic of the Sudan, and Egypt. In particular, the Nile is the primary water source of Egypt and Sudan.

The Nile has two major tributaries – the White Nile and the Blue Nile. The White Nile is considered to be the headwaters and primary stream of the Nile itself. The Blue Nile, however, is the source of most of the water, containing 80% of the water and silt. The White Nile is longer and rises in the Great Lakes region of central Africa, with the most distant source still undetermined but located in either Rwanda or Burundi. It flows north through Tanzania, Lake Victoria, Uganda and South Sudan. The Blue Nile begins at Lake Tana in Ethiopia and flows into Sudan from the southeast. The two rivers meet just north of the Sudanese capital of Khartoum.

The northern section of the river flows north almost entirely through the Sudanese desert to Egypt, then ends in a large delta and flows into the Mediterranean Sea. Egyptian civilization and Sudanese kingdoms have depended on the river since ancient times. Most of the population and cities of Egypt lie along those parts of the Nile valley north of Aswan, and nearly all the cultural and historical sites of Ancient Egypt are found along river banks.

The standard English names "White Nile" and "Blue Nile", to refer to the river's source, derive from Arabic names formerly applied only to the Sudanese stretches which meet at Khartoum.

In the ancient Egyptian language, the Nile is called "Ḥ'pī" (Hapy) or "Iteru", meaning "river". In Coptic, the word ⲫⲓⲁⲣⲟ, pronounced "piaro" (Sahidic) or "phiaro" (Bohairic), means "the river" (lit. p(h).iar-o "the.canal-great"), and comes from the same ancient name.

In Nobiin the river is called Áman Dawū, meaning "the great water".

In Egyptian Arabic, the Nile is called "en-Nīl" while in Standard Arabic it is called "an-Nīl". In Biblical Hebrew: , "Ha-Ye'or" or , "Ha-Shiḥor".

The English name "Nile" and the Arabic names "en-Nîl" and "an-Nîl" both derive from the Latin ' and the Ancient Greek . Beyond that, however, the etymology is disputed. Hesiod at his "Theogony" refers that Nilus (Νεῖλος) was one of the Potamoi (river gods), son of Oceanus and Tethys. Another derivation of "Nile" might be related to the term "Nil" (; ), which refers to "Indigofera tinctoria", one of the original sources of indigo dye; or "Nymphaea caerulea", known as "The Sacred Blue Lily of the Nile", which was found scattered over Tutankhamen's corpse when it was located in 1922.

Another possible etymology derives it from a Semitic "Nahal", meaning "river".

With a total length of about between the region of Lake Victoria and the Mediterranean Sea, the Nile is the longest river on Earth. The drainage basin of the Nile covers , about 10% of the area of Africa. Compared to other major rivers, though, the Nile carries little water (5% of the Congo's river, for example). The Nile basin is complex, and because of this, the discharge at any given point along the mainstem depends on many factors including weather, diversions, evaporation and evapotranspiration, and groundwater flow.

Above Khartoum, the Nile is also known as the White Nile, a term also used in a limited sense to describe the section between Lake No and Khartoum. At Khartoum the river is joined by the Blue Nile. The White Nile starts in equatorial East Africa, and the Blue Nile begins in Ethiopia. Both branches are on the western flanks of the East African Rift.

The source of the Nile is sometimes considered to be Lake Victoria, but the lake has feeder rivers of considerable size. The Kagera River, which flows into Lake Victoria near the Tanzanian town of Bukoba, is the longest feeder, although sources do not agree on which is the longest tributary of the Kagera and hence the most distant source of the Nile itself. It is either the Ruvyironza, which emerges in Bururi Province, Burundi, or the Nyabarongo, which flows from Nyungwe Forest in Rwanda. The two feeder rivers meet near Rusumo Falls on the Rwanda-Tanzania border.

In 2010, an exploration party went to a place described as the source of the Rukarara tributary, and by hacking a path up steep jungle-choked mountain slopes in the Nyungwe forest found (in the dry season) an appreciable incoming surface flow for many kilometres upstream, and found a new source, giving the Nile a length of .

Gish Abay is reportedly the place where the "holy water" of the first drops of the Blue Nile develop.

The Nile leaves Lake Nalubaale (Victoria) at Ripon Falls near Jinja, Uganda, as the Victoria Nile. It flows north for some , to Lake Kyoga. The last part of the approximately river section starts from the western shores of the lake and flows at first to the west until just south of Masindi Port, where the river turns north, then makes a great half circle to the east and north until Karuma Falls. For the remaining part it flows merely westerly through the Murchison Falls until it reaches the very northern shores of Lake Albert where it forms a significant river delta. The lake itself is on the border of DR Congo, but the Nile is not a border river at this point. After leaving Lake Albert, the river continues north through Uganda and is known as the Albert Nile.

The Nile river flows into South Sudan just south of Nimule, where it is known as the Bahr al Jabal ("Mountain River"). Just south of the town it has the confluence with the Achwa River. The Bahr al Ghazal, itself long, joins the Bahr al Jabal at a small lagoon called Lake No, after which the Nile becomes known as the "Bahr al Abyad", or the White Nile, from the whitish clay suspended in its waters. When the Nile floods it leaves a rich silty deposit which fertilizes the soil. The Nile no longer floods in Egypt since the completion of the Aswan Dam in 1970. An anabranch river, the Bahr el Zeraf, flows out of the Nile's Bahr al Jabal section and rejoins the White Nile.

The flow rate of the Bahr al Jabal at Mongalla, South Sudan is almost constant throughout the year and averages . After Mongalla, the Bahr Al Jabal enters the enormous swamps of the Sudd region of South Sudan. More than half of the Nile's water is lost in this swamp to evaporation and transpiration. The average flow rate of the White Nile at the tails of the swamps is about . From here it soon meets with the Sobat River at Malakal. On an annual basis, the White Nile upstream of Malakal contributes about fifteen percent of the total outflow of the Nile.

The average flow of the White Nile at Lake Kawaki Malakal, just below the Sobat River, is ; the peak flow is approximately in October and minimum flow is about in April. This fluctuation is due to the substantial variation in the flow of the Sobat, which has a minimum flow of about in March and a peak flow of over in October. During the dry season (January to June) the White Nile contributes between 70 percent and 90 percent of the total discharge from the Nile.

Below Renk the White Nile enters Sudan, it flows north to Khartoum and meets the Blue Nile.

The course of the Nile in Sudan is distinctive. It flows over six groups of cataracts, from the sixth at Sabaloka just north of Khartoum northward to Abu Hamed. Due to the tectonic uplift of the Nubian Swell, the river is then diverted to flow for over 300 km south-west following the structure of the Central African Shear Zone embracing the Bayuda Desert. At Al Dabbah it resumes its northward course towards the first Cataract at Aswan forming the 'S'-shaped Great Bend of the Nile already mentioned by Eratosthenes.

In the north of Sudan the river enters Lake Nasser (known in Sudan as Lake Nubia), the larger part of which is in Egypt.

Below the Aswan High Dam, at the northern limit of Lake Nasser, the Nile resumes its historic course.

North of Cairo, the Nile splits into two branches (or distributaries) that feed the Mediterranean: the Rosetta Branch to the west and the Damietta to the east, forming the Nile Delta.

The annual sediment transport by the Nile in Egypt has been quantified.

Below the confluence with the Blue Nile the only major tributary is the Atbara River, roughly halfway to the sea, which originates in Ethiopia north of Lake Tana, and is around long. The Atbara flows only while there is rain in Ethiopia and dries very rapidly. During the dry period of January to June, it typically dries up north of Khartoum.

The Blue Nile (, "ʿĀbay") springs from Lake Tana in the Ethiopian Highlands. The Blue Nile flows about 1,400 kilometres to Khartoum, where the Blue Nile and White Nile join to form the Nile. Ninety percent of the water and ninety-six percent of the transported sediment carried by the Nile originates in Ethiopia, with fifty-nine percent of the water from the Blue Nile (the rest being from the Tekezé, Atbarah, Sobat, and small tributaries). The erosion and transportation of silt only occurs during the Ethiopian rainy season in the summer, however, when rainfall is especially high on the Ethiopian Plateau; the rest of the year, the great rivers draining Ethiopia into the Nile (Sobat, Blue Nile, Tekezé, and Atbarah) have a weaker flow. In harsh and arid seasons and droughts the Blue Nile dries out completely.

The flow of the Blue Nile varies considerably over its yearly cycle and is the main contribution to the large natural variation of the Nile flow. During the dry season the natural discharge of the Blue Nile can be as low as , although upstream dams regulate the flow of the river. During the wet season the peak flow of the Blue Nile often exceeds in late August (a difference of a factor of 50).

Before the placement of dams on the river the yearly discharge varied by a factor of 15 at Aswan. Peak flows of over occurred during late August and early September, and minimum flows of about occurred during late April and early May.

The Bahr al Ghazal and the Sobat River are the two most important tributaries of the White Nile in terms of discharge.

The Bahr al Ghazal's drainage basin is the largest of any of the Nile's sub-basins, measuring in size, but it contributes a relatively small amount of water, about annually, due to tremendous volumes of water being lost in the Sudd wetlands.

The Sobat River, which joins the Nile a short distance below Lake No, drains about half as much land, , but contributes annually to the Nile. When in flood the Sobat carries a large amount of sediment, adding greatly to the White Nile's color.

The Yellow Nile is a former tributary that connected the Ouaddaï Highlands of eastern Chad to the Nile River Valley c. 8000 to c. 1000 BCE. Its remains are known as the Wadi Howar. The wadi passes through Gharb Darfur near the northern border with Chad and meets up with the Nile near the southern point of the Great Bend.

The Nile ("iteru" in Ancient Egyptian) has been the lifeline of civilization in Egypt since the Stone Age, with most of the population and all of the cities of Egypt resting along those parts of the Nile valley lying north of Aswan. However, the Nile used to run much more westerly through what is now Wadi Hamim and Wadi al Maqar in Libya and flow into the Gulf of Sidra. As sea level rose at the end of the most recent ice age, the stream which is now the northern Nile pirated the ancestral Nile near Asyut, this change in climate also led to the creation of the current Sahara desert, around 3400 BC.

The present Nile is at least the fifth river that has flowed north from the Ethiopian Highlands. Satellite imagery was used to identify dry watercourses in the desert to the west of the Nile. A canyon, now filled by surface drift, represents an ancestral Nile called the Eonile that flowed during the later Miocene (23–5.3 million years before present). The Eonile transported clastic sediments to the Mediterranean; several natural gas fields have been discovered within these sediments.

During the late-Miocene Messinian salinity crisis, when the Mediterranean Sea was a closed basin and evaporated to the point of being empty or nearly so, the Nile cut its course down to the new base level until it was several hundred metres below world ocean level at Aswan and below Cairo. This created a very long and deep canyon which was filled with sediment when the Mediterranean was recreated. At some point the sediments raised the riverbed sufficiently for the river to overflow westward into a depression to create Lake Moeris.

Lake Tanganyika drained northwards into the Nile until the Virunga Volcanoes blocked its course in Rwanda. The Nile was much longer at that time, with its furthest headwaters in northern Zambia.

There are two theories about the age of the integrated Nile. One is that the integrated drainage of the Nile is of young age and that the Nile basin was formerly broken into series of separate basins, only the most northerly of which fed a river following the present course of the Nile in Egypt and Sudan. Rushdi Said postulated that Egypt itself supplied most of the waters of the Nile during the early part of its history.

The other theory is that the drainage from Ethiopia via rivers equivalent to the Blue Nile, the Atbara and the Takazze flowed to the Mediterranean via the Egyptian Nile since well back into Tertiary times.

Salama suggested that during the Paleogene and Neogene Periods (66 million to 2.588 million years ago) a series of separate closed continental basins each occupied one of the major parts of the Sudanese Rift System: Mellut rift, White Nile rift, Blue Nile rift, Atbara rift and Sag El Naam rift.
The Mellut Rift Basin is nearly deep at its central part. This rift is possibly still active, with reported tectonic activity in its northern and southern boundaries. The Sudd swamps which form the central part of the basin may still be subsiding. The White Nile Rift System, although shallower than the Bahr el Arab rift, is about deep. Geophysical exploration of the Blue Nile Rift System estimated the depth of the sediments to be . These basins were not interconnected until their subsidence ceased, and the rate of sediment deposition was enough to fill and connect them. The Egyptian Nile connected to the Sudanese Nile, which captures the Ethiopian and Equatorial headwaters during the current stages of tectonic activity in the Eastern, Central and Sudanese Rift Systems. The connection of the different Niles occurred during cyclic wet periods. The River Atbara overflowed its closed basin during the wet periods that occurred about 100,000 to 120,000 years ago. The Blue Nile connected to the main Nile during the 70,000–80,000 years B.P. wet period. The White Nile system in Bahr El Arab and White Nile Rifts remained a closed lake until the connection of the Victoria Nile to the main system some 12,500 years ago during the African humid period.

The Greek historian Herodotus wrote that "Egypt was the gift of the Nile". An unending source of sustenance, it played a crucial role in the development of Egyptian civilization. Because the river overflowed its banks annually and deposited new layers of silt, the surrounding land was very fertile. The Ancient Egyptians cultivated and traded wheat, flax, papyrus and other crops around the Nile. Wheat was a crucial crop in the famine-plagued Middle East. This trading system secured Egypt's diplomatic relationships with other countries, and contributed to economic stability. Far-reaching trade has been carried on along the Nile since ancient times. A tune, Hymn to the Nile, was created and sung by the ancient Egyptian peoples about the flooding of the Nile River and all of the miracles it brought to Ancient Egyptian civilization.

Water buffalo were introduced from Asia and the Assyrians introduced camels in the 7th century BC. These animals were killed for meat, and were domesticated and used for ploughing—or in the camels' case, carriage. Water was vital to both people and livestock. The Nile was also a convenient and efficient means of transportation for people and goods.

The Nile was also an important part of ancient Egyptian spiritual life. Hapi was the god of the annual floods, and both he and the pharaoh were thought to control the flooding. The Nile was considered to be a causeway from life to death and the afterlife. The east was thought of as a place of birth and growth, and the west was considered the place of death, as the god Ra, the Sun, underwent birth, death, and resurrection each day as he crossed the sky. Thus, all tombs were west of the Nile, because the Egyptians believed that in order to enter the afterlife, they had to be buried on the side that symbolized death.

As the Nile was such an important factor in Egyptian life, the ancient calendar was even based on the three cycles of the Nile. These seasons, each consisting of four months of thirty days each, were called Akhet, Peret, and Shemu. Akhet, which means inundation, was the time of the year when the Nile flooded, leaving several layers of fertile soil behind, aiding in agricultural growth. Peret was the growing season, and Shemu, the last season, was the harvest season when there were no rains.

Owing to their failure to penetrate the sudd wetlands of South Sudan, the upper reaches of the White Nile remained largely unknown to the ancient Greeks and Romans. Various expeditions failed to determine the river's source. Agatharcides records that in the time of Ptolemy II Philadelphus, a military expedition had penetrated far enough along the course of the Blue Nile to determine that the summer floods were caused by heavy seasonal rainstorms in the Ethiopian Highlands, but no European of antiquity is known to have reached Lake Tana.

The "Tabula Rogeriana" depicted the source as three lakes in 1154.

Europeans began to learn about the origins of the Nile in the fourteenth century when the Pope sent monks as emissaries to Mongolia who passed India, the Middle East and Africa, and described being told of the source of the Nile in Abyssinia (Ethiopia) Later in the fifteenth and sixteenth centuries, travelers to Ethiopia visited Lake Tana and the source of the Blue Nile in the mountains south of the lake. Although James Bruce claimed to be the first European to have visited the headwaters, modern writers give the credit to the Jesuit Pedro Páez. Páez's account of the source of the Nile is a long and vivid account of Ethiopia. It was published in full only in the early twentieth century, although it was featured in works of Páez's contemporaries, including Baltazar Téllez, Athanasius Kircher and by Johann Michael Vansleb.

Europeans had been resident in Ethiopia since the late fifteenth century, and one of them may have visited the headwaters even earlier without leaving a written trace. The Portuguese João Bermudes published the first description of the Tis Issat Falls in his 1565 memoirs, compared them to the Nile Falls alluded to in Cicero's "De Republica". Jerónimo Lobo describes the source of the Blue Nile, visiting shortly after Pedro Páez. Telles also used his account.

The White Nile was even less understood. The ancients mistakenly believed that the Niger River represented the upper reaches of the White Nile. For example, Pliny the Elder wrote that the Nile had its origins "in a mountain of lower Mauretania", flowed above ground for "many days" distance, then went underground, reappeared as a large lake in the territories of the Masaesyli, then sank again below the desert to flow underground "for a distance of 20 days' journey till it reaches the nearest Ethiopians." A merchant named Diogenes reported that the Nile's water attracted game such as buffalo.
Modern exploration of the Nile basin began with the conquest of the northern and central Sudan by the Ottoman viceroy of Egypt, Muḥammad Ali, and his sons from 1821 onward. As a result of this, the Blue Nile was known as far as its exit from the Ethiopian foothills and the White Nile as far as the mouth of the Sobat River. Three expeditions under a Turkish officer, Selim Bimbashi, were made between 1839 and 1842, and two got to the point about 20 miles (32 km) beyond the present port of Juba, where the country rises and rapids make navigation very difficult. 

Lake Victoria was first sighted by Europeans in 1858 when British explorer John Hanning Speke reached its southern shore while traveling with Richard Francis Burton to explore central Africa and locate the great lakes. Believing he had found the source of the Nile on seeing this "vast expanse of open water" for the first time, Speke named the lake after the then Queen of the United Kingdom. Burton, recovering from illness and resting further south on the shores of Lake Tanganyika, was outraged that Speke claimed to have proved his discovery to be the true source of the Nile when Burton regarded this as still unsettled. A very public quarrel ensued, which sparked a great deal of intense debate within the scientific community and interest by other explorers keen to either confirm or refute Speke's discovery. British explorer and missionary David Livingstone pushed too far west and entered the Congo River system instead. It was ultimately Welsh-American explorer Henry Morton Stanley who confirmed Speke's discovery, circumnavigating Lake Victoria and reporting the great outflow at Ripon Falls on the lake's northern shore.

European involvement in Egypt goes back to the time of Napoleon. Laird Shipyard of Liverpool sent an iron steamer to the Nile in the 1830s. With the completion of the Suez Canal and the British takeover of Egypt in the 1882, more British river steamers followed.

The Nile is the area's natural navigation channel, giving access to Khartoum and Sudan by steamer. The Siege of Khartoum was broken with purpose-built sternwheelers shipped from England and steamed up the river to retake the city. After this came regular steam navigation of the river. With British Forces in Egypt in the First World War and the inter-war years, river steamers provided both security and sightseeing to the Pyramids and Thebes. Steam navigation remained integral to the two countries as late as 1962. Sudan steamer traffic was a lifeline as few railways or roads were built in that country. Most paddle steamers have been retired to shorefront service, but modern diesel tourist boats remain on the river.

The Nile has long been used to transport goods along its length. Winter winds blow south, up river, so ships could sail up river, and down river using the flow of the river.
While most Egyptians still live in the Nile valley, the 1970 completion of the Aswan High Dam ended the summer floods and their renewal of the fertile soil, fundamentally changing farming practices. The Nile supports much of the population living along its banks, enabling Egyptians to live in otherwise inhospitable regions of the Sahara. The river's flow is disturbed at several points by the Cataracts of the Nile, which are sections of faster-flowing water with many small islands, shallow water, and rocks, which form an obstacle to navigation by boats. The Sudd wetlands in Sudan also forms a formidable navigation obstacle and impede water flow, to the extent that Sudan had once attempted to canalize (the Jonglei Canal) to bypass the swamps.

Nile cities include Khartoum, Aswan, Luxor (Thebes), and the GizaCairo conurbation. The first cataract, the closest to the mouth of the river, is at Aswan, north of the Aswan Dam. This part of the river is a regular tourist route, with cruise ships and traditional wooden sailing boats known as feluccas. Many cruise ships ply the route between Luxor and Aswan, stopping at Edfu and Kom Ombo along the way. Security concerns have limited cruising on the northernmost portion for many years.

A computer simulation study to plan the economic development of the Nile was directed by H.A.W. Morrice and W.N. Allan, for the Ministry of Hydro-power of the Republic of the Sudan, during 1955–1957 Morrice was their Hydrological Adviser, and Allan his predecessor. M.P. Barnett directed the software development and computer operations. The calculations were enabled by accurate monthly inflow data collected for 50 years. The underlying principle was the use of over-year storage, to conserve water from rainy years for use in dry years. Irrigation, navigation and other needs were considered. Each computer run postulated a set of reservoirs and operating equations for the release of water as a function of the month and the levels upstream. The behavior that would have resulted given the inflow data was modeled. Over 600 models were run. Recommendations were made to the Sudanese authorities. The calculations were run on an IBM 650 computer. Simulation studies to design water resources are discussed further in the article on hydrology transport models, that have been used since the 1980s to analyze water quality.

Despite the development of many reservoirs, drought during the 1980s led to widespread starvation in Ethiopia and Sudan, but Egypt was nourished by water impounded in Lake Nasser. Drought has proven to be a major cause of fatality in the Nile river basin. According to a report by the Strategic Foresight Group around 170 million people have been affected by droughts in the last century with half a million lives lost. From the 70 incidents of drought which took place between 1900 and 2012, 55 incidents took place in Ethiopia, Sudan, South Sudan, Kenya and Tanzania.

The Nile's water has affected the politics of East Africa and the Horn of Africa for many decades. The dispute between Egypt and Ethiopia over the $4.5 billion Grand Ethiopian Renaissance Dam — Africa's largest, with a reservoir about the size of London – has become a national preoccupation in both countries, stoking patriotism, deep-seated fears and even murmurs of war. Countries including Uganda, Sudan, Ethiopia and Kenya have complained about Egyptian domination of its water resources. The Nile Basin Initiative promotes a peaceful cooperation among those states.

Several attempts have been made to establish agreements between the countries sharing the Nile waters. On 14 May 2010 at Entebbe, Ethiopia, Rwanda, Tanzania and Uganda signed a new agreement on sharing the Nile water even though this agreement raised strong opposition from Egypt and Sudan. Ideally, such international agreements should promote equitable and efficient usage of the Nile basin's water resources. Without a better understanding about the availability of the future water resources of the Nile, it is possible that conflicts could arise between these countries relying on the Nile for their water supply, economic and social developments.

In 1951, the American John Goddard together with two French explorers became the first to successfully navigate the entire Nile river from its source in Burundi at the potential headsprings of the Kagera River in Burundi to its mouth on the Mediterranean Sea, a journey of approximately . Their 9-month journey is described in the book "Kayaks down the Nile".

The White Nile Expedition, led by South African national Hendrik Coetzee, navigated the White Nile's entire length of approximately . The expedition began at the White Nile's beginning at Lake Victoria in Uganda, on 17 January 2004 and arrived safely at the Mediterranean in Rosetta, four and a half months later.

The Blue Nile Expedition, led by geologist Pasquale Scaturro and his partner, kayaker and documentary filmmaker Gordon Brown became the first known people to descend the entire Blue Nile, from Lake Tana in Ethiopia to the beaches of Alexandria on the Mediterranean. Their approximately journey took 114 days, from 25 December 2003 to 28 April 2004. Though their expedition included others, Brown and Scaturro were the only ones to complete the entire journey. Although they descended whitewater manually the team used outboard motors for much of their journey.

On 29 January 2005 Canadian Les Jickling and New Zealander Mark Tanner completed the first human powered transit of Ethiopia's Blue Nile. Their journey of over took five months. They recount that they paddled through two war zones, regions notorious for bandits, and were arrested at gunpoint.

The following bridges cross the Blue Nile and connect Khartoum to Khartoum North:

The following bridges cross the White Nile and connect Khartoum to Omdurman:

the following bridges cross from Omdurman: to Khartoum North:

The following bridges cross to Tuti from Khartoum states three cities

Other bridges

The following is an annotated bibliography of key written documents for the Western exploration of the Nile.

17th century


18th century

1800–1850


1850–1900








</doc>
<doc id="21245" url="https://en.wikipedia.org/wiki?curid=21245" title="Neuroscience">
Neuroscience

Neuroscience (or neurobiology) is the scientific study of the nervous system. It combines physiology, anatomy, molecular biology, developmental biology, cytology, mathematical modeling, and psychology to understand the fundamental and emergent properties of neurons and neural circuits. The understanding of the biological basis of learning, memory, behavior, perception, and consciousness has been described by Eric Kandel as the "ultimate challenge" of the biological sciences.

The scope of neuroscience has broadened over time to include different approaches used to study the nervous system at different scales and the techniques used by neuroscientists have expanded enormously, from molecular and cellular studies of individual neurons to imaging of sensory, motor and cognitive tasks in the brain.

The earliest study of the nervous system dates to ancient Egypt. Trepanation, the surgical practice of either drilling or scraping a hole into the skull for the purpose of curing head injuries or mental disorders, or relieving cranial pressure, was first recorded during the Neolithic period. Manuscripts dating to 1700 BC indicate that the Egyptians had some knowledge about symptoms of brain damage.

Early views on the function of the brain regarded it to be a "cranial stuffing" of sorts. In Egypt, from the late Middle Kingdom onwards, the brain was regularly removed in preparation for mummification. It was believed at the time that the heart was the seat of intelligence. According to Herodotus, the first step of mummification was to "take a crooked piece of iron, and with it draw out the brain through the nostrils, thus getting rid of a portion, while the skull is cleared of the rest by rinsing with drugs."

The view that the heart was the source of consciousness was not challenged until the time of the Greek physician Hippocrates. He believed that the brain was not only involved with sensation—since most specialized organs (e.g., eyes, ears, tongue) are located in the head near the brain—but was also the seat of intelligence. Plato also speculated that the brain was the seat of the rational part of the soul. Aristotle, however, believed the heart was the center of intelligence and that the brain regulated the amount of heat from the heart. This view was generally accepted until the Roman physician Galen, a follower of Hippocrates and physician to Roman gladiators, observed that his patients lost their mental faculties when they had sustained damage to their brains.

Abulcasis, Averroes, Avicenna, Avenzoar, and Maimonides, active in the Medieval Muslim world, described a number of medical problems related to the brain. In Renaissance Europe, Vesalius (1514–1564), René Descartes (1596–1650), Thomas Willis (1621–1675) and Jan Swammerdam (1637–1680) also made several contributions to neuroscience.
Luigi Galvani's pioneering work in the late 1700s set the stage for studying the electrical excitability of muscles and neurons. In the first half of the 19th century, Jean Pierre Flourens pioneered the experimental method of carrying out localized lesions of the brain in living animals describing their effects on motricity, sensibility and behavior. In 1843 Emil du Bois-Reymond demonstrated the electrical nature of the nerve signal, whose speed Hermann von Helmholtz proceeded to measure, and in 1875 Richard Caton found electrical phenomena in the cerebral hemispheres of rabbits and monkeys. Adolf Beck published in 1890 similar observations of spontaneous electrical activity of the brain of rabbits and dogs. Studies of the brain became more sophisticated after the invention of the microscope and the development of a staining procedure by Camillo Golgi during the late 1890s. The procedure used a silver chromate salt to reveal the intricate structures of individual neurons. His technique was used by Santiago Ramón y Cajal and led to the formation of the neuron doctrine, the hypothesis that the functional unit of the brain is the neuron. Golgi and Ramón y Cajal shared the Nobel Prize in Physiology or Medicine in 1906 for their extensive observations, descriptions, and categorizations of neurons throughout the brain.

In parallel with this research, work with brain-damaged patients by Paul Broca suggested that certain regions of the brain were responsible for certain functions. At the time, Broca's findings were seen as a confirmation of Franz Joseph Gall's theory that language was localized and that certain psychological functions were localized in specific areas of the cerebral cortex. The localization of function hypothesis was supported by observations of epileptic patients conducted by John Hughlings Jackson, who correctly inferred the organization of the motor cortex by watching the progression of seizures through the body. Carl Wernicke further developed the theory of the specialization of specific brain structures in language comprehension and production. Modern research through neuroimaging techniques, still uses the Brodmann cerebral cytoarchitectonic map (referring to study of cell structure) anatomical definitions from this era in continuing to show that distinct areas of the cortex are activated in the execution of specific tasks.

During the 20th century, neuroscience began to be recognized as a distinct academic discipline in its own right, rather than as studies of the nervous system within other disciplines. Eric Kandel and collaborators have cited David Rioch, Francis O. Schmitt, and Stephen Kuffler as having played critical roles in establishing the field. Rioch originated the integration of basic anatomical and physiological research with clinical psychiatry at the Walter Reed Army Institute of Research, starting in the 1950s. During the same period, Schmitt established a neuroscience research program within the Biology Department at the Massachusetts Institute of Technology, bringing together biology, chemistry, physics, and mathematics. The first freestanding neuroscience department (then called Psychobiology) was founded in 1964 at the University of California, Irvine by James L. McGaugh. This was followed by the Department of Neurobiology at Harvard Medical School, which was founded in 1966 by Stephen Kuffler.

The understanding of neurons and of nervous system function became increasingly precise and molecular during the 20th century. For example, in 1952, Alan Lloyd Hodgkin and Andrew Huxley presented a mathematical model for transmission of electrical signals in neurons of the giant axon of a squid, which they called "action potentials", and how they are initiated and propagated, known as the Hodgkin–Huxley model. In 1961–1962, Richard FitzHugh and J. Nagumo simplified Hodgkin–Huxley, in what is called the FitzHugh–Nagumo model. In 1962, Bernard Katz modeled neurotransmission across the space between neurons known as synapses. Beginning in 1966, Eric Kandel and collaborators examined biochemical changes in neurons associated with learning and memory storage in "Aplysia". In 1981 Catherine Morris and Harold Lecar combined these models in the Morris–Lecar model. Such increasingly quantitative work gave rise to numerous biological neuron models and models of neural computation.

As a result of the increasing interest about the nervous system, several prominent neuroscience organizations have been formed to provide a forum to all neuroscientist during the 20th century. For example, the International Brain Research Organization was founded in 1961, the International Society for Neurochemistry in 1963, the European Brain and Behaviour Society in 1968, and the Society for Neuroscience in 1969. Recently, the application of neuroscience research results has also given rise to applied disciplines as neuroeconomics, neuroeducation, neuroethics, and neurolaw.

Over time, brain research has gone through philosophical, experimental, and theoretical phases, with work on brain simulation predicted to be important in the future.

The scientific study of the nervous system increased significantly during the second half of the twentieth century, principally due to advances in molecular biology, electrophysiology, and computational neuroscience. This has allowed neuroscientists to study the nervous system in all its aspects: how it is structured, how it works, how it develops, how it malfunctions, and how it can be changed.

For example, it has become possible to understand, in much detail, the complex processes occurring within a single neuron. Neurons are cells specialized for communication. They are able to communicate with neurons and other cell types through specialized junctions called synapses, at which electrical or electrochemical signals can be transmitted from one cell to another. Many neurons extrude a long thin filament of axoplasm called an axon, which may extend to distant parts of the body and are capable of rapidly carrying electrical signals, influencing the activity of other neurons, muscles, or glands at their termination points. A nervous "system" emerges from the assemblage of neurons that are connected to each other.

The vertebrate nervous system can be split into two parts: the central nervous system (defined as the brain and spinal cord), and the peripheral nervous system. In many species — including all vertebrates — the nervous system is the most complex organ system in the body, with most of the complexity residing in the brain. The human brain alone contains around one hundred billion neurons and one hundred trillion synapses; it consists of thousands of distinguishable substructures, connected to each other in synaptic networks whose intricacies have only begun to be unraveled. At least one out of three of the approximately 20,000 genes belonging to the human genome is expressed mainly in the brain.

Due to the high degree of plasticity of the human brain, the structure of its synapses and their resulting functions change throughout life.

Making sense of the nervous system's dynamic complexity is a formidable research challenge. Ultimately, neuroscientists would like to understand every aspect of the nervous system, including how it works, how it develops, how it malfunctions, and how it can be altered or repaired. Analysis of the nervous system is therefore performed at multiple levels, ranging from the molecular and cellular levels to the systems and cognitive levels. The specific topics that form the main foci of research change over time, driven by an ever-expanding base of knowledge and the availability of increasingly sophisticated technical methods. Improvements in technology have been the primary drivers of progress. Developments in electron microscopy, computer science, electronics, functional neuroimaging, and genetics and genomics have all been major drivers of progress.

Basic questions addressed in molecular neuroscience include the mechanisms by which neurons express and respond to molecular signals and how axons form complex connectivity patterns. At this level, tools from molecular biology and genetics are used to understand how neurons develop and how genetic changes affect biological functions. The morphology, molecular identity, and physiological characteristics of neurons and how they relate to different types of behavior are also of considerable interest.

Questions addressed in cellular neuroscience include the mechanisms of how neurons process signals physiologically and electrochemically. These questions include how signals are processed by neurites and somas and how neurotransmitters and electrical signals are used to process information in a neuron. Neurites are thin extensions from a neuronal cell body, consisting of dendrites (specialized to receive synaptic inputs from other neurons) and axons (specialized to conduct nerve impulses called action potentials). Somas are the cell bodies of the neurons and contain the nucleus.

Another major area of cellular neuroscience is the investigation of the development of the nervous system. Questions include the patterning and regionalization of the nervous system, neural stem cells, differentiation of neurons and glia (neurogenesis and gliogenesis), neuronal migration, axonal and dendritic development, trophic interactions, and synapse formation.

Computational neurogenetic modeling is concerned with the development of dynamic neuronal models for modeling brain functions with respect to genes and dynamic interactions between genes.

Questions in systems neuroscience include how neural circuits are formed and used anatomically and physiologically to produce functions such as reflexes, multisensory integration, motor coordination, circadian rhythms, emotional responses, learning, and memory. In other words, they address how these neural circuits function in large-scale brain networks, and the mechanisms through which behaviors are generated. For example, systems level analysis addresses questions concerning specific sensory and motor modalities: how does vision work? How do songbirds learn new songs and bats localize with ultrasound? How does the somatosensory system process tactile information? The related fields of neuroethology and neuropsychology address the question of how neural substrates underlie specific animal and human behaviors. Neuroendocrinology and psychoneuroimmunology examine interactions between the nervous system and the endocrine and immune systems, respectively. Despite many advancements, the way that networks of neurons perform complex cognitive processes and behaviors is still poorly understood.

Cognitive neuroscience addresses the questions of how psychological functions are produced by neural circuitry. The emergence of powerful new measurement techniques such as neuroimaging (e.g., fMRI, PET, SPECT), EEG, MEG, electrophysiology, optogenetics and human genetic analysis combined with sophisticated experimental techniques from cognitive psychology allows neuroscientists and psychologists to address abstract questions such as how cognition and emotion are mapped to specific neural substrates. Although many studies still hold a reductionist stance looking for the neurobiological basis of cognitive phenomena, recent research shows that there is an interesting interplay between neuroscientific findings and conceptual research, soliciting and integrating both perspectives. For example, the neuroscience research on empathy solicited an interesting interdisciplinary debate involving philosophy, psychology and psychopathology. Moreover, the neuroscientific identification of multiple memory systems related to different brain areas has challenged the idea of memory as a literal reproduction of the past, supporting a view of memory as a generative, constructive and dynamic process.

Neuroscience is also allied with the social and behavioral sciences as well as nascent interdisciplinary fields such as neuroeconomics, decision theory, social neuroscience, and neuromarketing to address complex questions about interactions of the brain with its environment. A study into consumer responses for example uses EEG to investigate neural correlates associated with narrative transportation into stories about energy efficiency.

Questions in computational neuroscience can span a wide range of levels of traditional analysis, such as development, structure, and cognitive functions of the brain. Research in this field utilizes mathematical models, theoretical analysis, and computer simulation to describe and verify biologically plausible neurons and nervous systems. For example, biological neuron models are mathematical descriptions of spiking neurons which can be used to describe both the behavior of single neurons as well as the dynamics of neural networks. Computational neuroscience is often referred to as theoretical neuroscience.

Nanoparticles in medicine are versatile in treating neurological disorders showing promising results in mediating drug transport across the blood brain barrier.<ref name="10.1016/j.lfs.2017.06.001"></ref> Implementing nanoparticles in antiepileptic drugs enhances their medical efficacy by increasing bioavailability in the bloodstream, as well as offering a measure of control in release time concentration. Although nanoparticles can assist therapeutic drugs by adjusting physical properties to achieve desirable effects, inadvertent increases in toxicity often occur in preliminary drug trials. Furthermore, production of nanomedicine for drug trials is economically consuming, hindering progress in their implementation. Computational models in nanoneuroscience provide alternatives to study the efficacy of nanotechnology-based medicines in neurological disorders while mitigating potential side effects and development costs.

Nanomaterials often operate at length scales between classical and quantum regimes. Due to the associated uncertainties at the length scales that nanomaterials operate, it is difficult to predict their behavior prior to in vivo studies. Classically, the physical processes which occur throughout neurons are analogous to electrical circuits. Designers focus on such analogies and model brain activity as a neural circuit. Success in computational modeling of neurons have led to the development of stereochemical models that accurately predict acetylcholine receptor-based synapses operating at microsecond time scales.

Ultrafine nanoneedles for cellular manipulations are thinner than the smallest single walled carbon nanotubes. Computational quantum chemistry is used to design ultrafine nanomaterials with highly symmetrical structures to optimize geometry, reactivity and stability.

Behavior of nanomaterials are dominated by long ranged non-bonding interactions. Electrochemical processes that occur throughout the brain generate an electric field which can inadvertently affect the behavior of some nanomaterials. Molecular dynamics simulations can mitigate the development phase of nanomaterials as well as prevent neural toxicity of nanomaterials following in vivo clinical trials. Testing nanomaterials using molecular dynamics optimizes nano characteristics for therapeutic purposes by testing different environment conditions, nanomaterial shape fabrications, nanomaterial surface properties, etc without the need for in vivo experimentation. Flexibility in molecular dynamic simulations allows medical practitioners to personalize treatment. Nanoparticle related data from translational nanoinformatics links neurological patient specific data to predict treatment response.

The visualization of neuronal activity is of key importance in the study of neurology. Nano-imaging tools with nanoscale resolution help in these areas. These optical imaging tools are PALM and STORM which helps visualize nanoscale objects within cells. Pampaloni states that, so far, these imaging tools revealed the dynamic behavior and organization of the actin cytoskeleton inside the cells, which will assist in understanding how neurons probe their involvement during neuronal outgrowth and in response to injury, and how they differentiate axonal processes and characterization of receptor clustering and stoichiometry at the plasma inside the synapses, which are critical for understanding how synapses respond to changes in neuronal activity. These past works focused on devices for stimulation or inhibition of neural activity, but the crucial aspect is the ability for the device to simultaneously monitor neural activity. The major aspect that is to be improved in the nano imaging tools is the effective collection of the light as a major problem is that biological tissue are dispersive media that do not allow a straightforward propagation and control of light. These devices use nanoneedle and nanowire (NWs) for probing and stimulation.

NWs are artificial nano- or micro-sized “needles” that can provide high-fidelity electrophysiological recordings if used as microscopic electrodes for neuronal recordings. NWs are an attractive as they are highly functional structures that offer unique electronic properties that are affected by biological/chemical species adsorbed on their surface; mostly the conductivity. This conductivity variance depending on chemical species present allows enhanced sensing performances. NWs are also able to act as non-invasive and highly local probes. These versatility of NWs makes it optimal for interfacing with neurons due to the fact that the contact length along the axon (or the dendrite projection crossing a NW) is just about 20 nm.

Neurology, psychiatry, neurosurgery, psychosurgery, anesthesiology and pain medicine, neuropathology, neuroradiology, ophthalmology, otolaryngology, clinical neurophysiology, addiction medicine, and sleep medicine are some medical specialties that specifically address the diseases of the nervous system. These terms also refer to clinical disciplines involving diagnosis and treatment of these diseases.

Neurology works with diseases of the central and peripheral nervous systems, such as amyotrophic lateral sclerosis (ALS) and stroke, and their medical treatment. Psychiatry focuses on affective, behavioral, cognitive, and perceptual disorders. Anesthesiology focuses on perception of pain, and pharmacologic alteration of consciousness. Neuropathology focuses upon the classification and underlying pathogenic mechanisms of central and peripheral nervous system and muscle diseases, with an emphasis on morphologic, microscopic, and chemically observable alterations. Neurosurgery and psychosurgery work primarily with surgical treatment of diseases of the central and peripheral nervous systems.

Recently, the boundaries between various specialties have blurred, as they are all influenced by basic research in neuroscience. For example, brain imaging enables objective biological insight into mental illnesses, which can lead to faster diagnosis, more accurate prognosis, and improved monitoring of patient progress over time.

Integrative neuroscience describes the effort to combine models and information from multiple levels of research to develop a coherent model of the nervous system. For example, brain imaging coupled with physiological numerical models and theories of fundamental mechanisms may shed light on psychiatric disorders.

One of the main goals of nanoneuroscience is to gain a detailed understanding of how the nervous system operates and, thus, how neurons organize themselves in the brain. Consequently, creating drugs and devices that are able to cross the blood brain barrier (BBB) are essential to allow for detailed imaging and diagnoses. The blood brain barrier functions as a highly specialized semipermeable membrane surrounding the brain, preventing harmful molecules that may be dissolved in the circulation blood from entering the central nervous system.

The main two hurdles for drug-delivering molecules to access the brain are size (must have a molecular weight < 400 Da) and lipid solubility. Physicians hope to circumvent difficulties in accessing the central nervous system through viral gene therapy. This often involves direct injection into the patient’s brain or cerebral spinal fluid. The drawback of this therapy is that it is invasive and carries a high risk factor due to the necessity of surgery for the treatment to be administered. Because of this, only 3.6% of clinical trials in this field have progressed to stage III since the concept of gene therapy was developed in the 1980s.
Another proposed way to cross the BBB is through temporary intentional disruption of the barrier. This method was first inspired by certain pathological conditions that were discovered to break down this barrier by themselves, such as Alzheimer’s disease, Parkinson’s disease, stroke, and seizure conditions.

Nanoparticles are unique from macromolecules because their surface properties are dependent on their size, allowing for strategic manipulation of these properties (or, “programming”) by scientists that would not be possible otherwise. Likewise, nanoparticle shape can also be varied to give a different set of characteristics based on the surface area to volume ratio of the particle.

Nanoparticles have promising therapeutic effects when treating neurodegenerative diseases. Oxygen reactive polymer (ORP) is a nano-platform programmed to react with oxygen and has been shown to detect and reduce the presence of reactive oxygen species (ROS) formed immediately after traumatic brain injuries. Nanoparticles have also been employed as a “neuroprotective” measure, as is the case with Alzheimer’s disease and stroke models. Alzheimer’s disease results in toxic aggregates of the amyloid beta protein formed in the brain. In one study, gold nanoparticles were programmed to attach themselves to these aggregates and were successful in breaking them up. Likewise, with ischemic stroke models, cells in the affected region of the brain undergo apoptosis, dramatically reducing blood flow to important parts of the brain and often resulting in death or severe mental and physical changes. Platinum nanoparticles have been shown to act as ROS, serving as “biological antioxidants” and significantly reducing oxidation in the brain as a result of stroke. Nanoparticles can also lead to neurotoxicity and cause permanent BBB damage either from brain oedema or from unrelated molecules crossing the BBB and causing brain damage. This proves further long term in vivo studies are needed to gain enough understanding to allow for successful clinical trials.

One of the most common nano-based drug delivery platforms is liposome-based delivery. They are both lipid-soluble and nano-scale and thus are permitted through a fully functioning BBB. Additionally, lipids themselves are biological molecules, making them highly biocompatible, which in turn lowers the risk of cell toxicity. The bilayer that is formed allows the molecule to fully encapsulate any drug, protecting it while it is travelling through the body. One drawback to shielding the drug from the outside cells is that it no longer has specificity, and requires coupling to extra antibodies to be able to target a biological site. Due to their low stability, liposome-based nanoparticles for drug delivery have a short shelf life.

Targeted therapy using magnetic nanoparticles (MNPs) is also a popular topic of research and has led to several stage III clinical trials. Invasiveness is not an issue here because a magnetic force can be applied from the outside of a patient’s body to interact and direct the MNPs. This strategy has been proven successful in delivering Brain-derived neurotropic factor, a naturally occurring gene thought to promote neurorehabilitation, across the BBB.

Modern neuroscience education and research activities can be very roughly categorized into the following major branches, based on the subject and scale of the system in examination as well as distinct experimental or curricular approaches. Individual neuroscientists, however, often work on questions that span several distinct subfields.

The largest professional neuroscience organization is the Society for Neuroscience (SFN), which is based in the United States but includes many members from other countries. Since its founding in 1969 the SFN has grown steadily: as of 2010 it recorded 40,290 members from 83 different countries. Annual meetings, held each year in a different American city, draw attendance from researchers, postdoctoral fellows, graduate students, and undergraduates, as well as educational institutions, funding agencies, publishers, and hundreds of businesses that supply products used in research.

Other major organizations devoted to neuroscience include the International Brain Research Organization (IBRO), which holds its meetings in a country from a different part of the world each year, and the Federation of European Neuroscience Societies (FENS), which holds a meeting in a different European city every two years. FENS comprises a set of 32 national-level organizations, including the British Neuroscience Association, the German Neuroscience Society ("Neurowissenschaftliche Gesellschaft"), and the French "Société des Neurosciences". The first National Honor Society in Neuroscience, Nu Rho Psi, was founded in 2006.

In 2013, the BRAIN Initiative was announced in the US. An International Brain Initiative was created in 2017, currently integrated by more than seven national-level brain research initiatives (US, Europe, Allen Institute, Japan, China, Australia, Canada, Korea, Israel) spanning four continents.

In addition to conducting traditional research in laboratory settings, neuroscientists have also been involved in the promotion of awareness and knowledge about the nervous system among the general public and government officials. Such promotions have been done by both individual neuroscientists and large organizations. For example, individual neuroscientists have promoted neuroscience education among young students by organizing the International Brain Bee, which is an academic competition for high school or secondary school students worldwide. In the United States, large organizations such as the Society for Neuroscience have promoted neuroscience education by developing a primer called Brain Facts, collaborating with public school teachers to develop Neuroscience Core Concepts for K-12 teachers and students, and cosponsoring a campaign with the Dana Foundation called Brain Awareness Week to increase public awareness about the progress and benefits of brain research. In Canada, the CIHR Canadian National Brain Bee is held annually at McMaster University.

Neuroscience educators formed Faculty for Undergraduate Neuroscience (FUN) in 1992 to share best practices and provide travel awards for undergraduates presenting at Society for Neuroscience meetings.

Finally, neuroscientists have also collaborated with other education experts to study and refine educational techniques to optimize learning among students, an emerging field called educational neuroscience. Federal agencies in the United States, such as the National Institute of Health (NIH) and National Science Foundation (NSF), have also funded research that pertains to best practices in teaching and learning of neuroscience concepts.




</doc>
<doc id="21247" url="https://en.wikipedia.org/wiki?curid=21247" title="Neil Armstrong">
Neil Armstrong

Neil Alden Armstrong (August 5, 1930 – August 25, 2012) was an American astronaut and aeronautical engineer and the first person to walk on the Moon. He was also a naval aviator, test pilot, and university professor.

A graduate of Purdue University, Armstrong studied aeronautical engineering; his college tuition was paid for by the U.S. Navy under the Holloway Plan. He became a midshipman in 1949 and a naval aviator the following year. He saw action in the Korean War, flying the Grumman F9F Panther from the aircraft carrier . In September 1951, while making a low bombing run, Armstrong's aircraft was damaged when it collided with an anti-aircraft cable, strung across a valley, which cut off a large portion of one wing. Armstrong was forced to bail out. After the war, he completed his bachelor's degree at Purdue and became a test pilot at the National Advisory Committee for Aeronautics (NACA) High-Speed Flight Station at Edwards Air Force Base in California. He was the project pilot on Century Series fighters and flew the North American X-15 seven times. He was also a participant in the U.S. Air Force's Man in Space Soonest and X-20 Dyna-Soar human spaceflight programs.

Armstrong joined the NASA Astronaut Corps in the second group, which was selected in 1962. He made his first spaceflight as command pilot of Gemini 8 in March 1966, becoming NASA's first civilian astronaut to fly in space. During this mission with pilot David Scott, he performed the first docking of two spacecraft; the mission was aborted after Armstrong used some of his re-entry control fuel to stabilize a dangerous roll caused by a stuck thruster. During training for Armstrong's second and last spaceflight as commander of Apollo 11, he had to eject from the Lunar Landing Research Vehicle moments before a crash.

On July 20, 1969, Armstrong and Apollo 11 Lunar Module (LM) pilot Buzz Aldrin became the first people to land on the Moon, and the next day they spent two and a half hours outside the Lunar Module "Eagle" spacecraft while Michael Collins remained in lunar orbit in the Apollo Command Module "Columbia". When Armstrong stepped onto the lunar surface, he famously said: "That's one small step for [a] man, one giant leap for mankind." Along with Collins and Aldrin, Armstrong was awarded the Presidential Medal of Freedom by President Richard Nixon. President Jimmy Carter presented Armstrong with the Congressional Space Medal of Honor in 1978, and Armstrong and his former crewmates received a Congressional Gold Medal in 2009.

After he resigned from NASA in 1971, Armstrong taught in the Department of Aerospace Engineering at the University of Cincinnati until 1979. He served on the Apollo 13 accident investigation and on the Rogers Commission, which investigated the Space Shuttle "Challenger" disaster. He acted as a spokesman for several businesses and appeared in advertising for the automotive brand Chrysler starting in January 1979.

Armstrong was born on August 5, 1930, near Wapakoneta, Ohio to Stephen Koenig Armstrong and Viola Louise Engel. He was of German, Scottish, and Scots-Irish ancestry, and had a younger sister, June, and a younger brother, Dean. His father worked as an auditor for the Ohio state government, and the family moved around the state repeatedly, living in sixteen towns over the next fourteen years. Armstrong's love for flying grew during this time, having started early when his father took his two-year-old son to the Cleveland Air Races. When he was five or six, he experienced his first airplane flight in Warren, Ohio when he and his father took a ride in a Ford Trimotor, also known as the "Tin Goose".

His father's last move was in 1944, back to Wapakoneta. Armstrong attended Blume High School and took flying lessons at the grassy Wapakoneta airfield. He earned a student flight certificate on his sixteenth birthday, then soloed in August, all before he had a driver license. He was active in the Boy Scouts and earned the rank of Eagle Scout. As an adult, he was recognized by the Boy Scouts of America with its Distinguished Eagle Scout Award and Silver Buffalo Award. On July 18, 1969, while flying toward the Moon, Armstrong greeted the Scouts holding their quadrennial National Jamboree in Idaho. Among the few personal items that he carried with him to the Moon and back was a World Scout Badge.

At age 17 in 1947, Armstrong began studying aeronautical engineering at Purdue University. He was the second person in his family to attend college. He was also accepted to the Massachusetts Institute of Technology (MIT), but after watching a football game between the Purdue Boilermakers and the Ohio State Buckeyes at the Ohio Stadium in 1945, in which the Boilermakers, led by quarterback Bob DeMoss, soundly defeated the highly regarded Buckeyes, he resolved to go to Purdue. An uncle who had attended MIT advised him that it was not necessary to go all the way to Cambridge, Massachusetts, for a good education. His college tuition was paid for under the Holloway Plan. Successful applicants committed to two years of study, followed by two years of flight training and one year of service in the U.S. Navy as an aviator, then completion of the final two years of their bachelor's degree. Armstrong did not take courses in naval science, nor did he join the Naval Reserve Officers Training Corps.

Armstrong's call-up from the Navy arrived on January 26, 1949, requiring him to report to Naval Air Station Pensacola in Florida for flight training with class 5-49. After passing the medical examinations, he became a midshipman on February 24, 1949. Flight training was conducted in a North American SNJ trainer, in which he soloed on September 9, 1949. On March 2, 1950, he made his first aircraft carrier landing on , an achievement he considered comparable to his first solo flight. He was then sent to Naval Air Station Corpus Christi in Texas for training on the Grumman F8F Bearcat, culminating in a carrier landing on . On August 16, 1950, Armstrong was informed by letter that he was a fully qualified naval aviator. His mother and sister attended his graduation ceremony on August 23, 1950.

Armstrong was assigned to Fleet Aircraft Service Squadron7 (FASRON 7) at NAS San Diego (now known as NAS North Island). On November 27, 1950, he was assigned to VF-51, an all-jet squadron, becoming its youngest officer, and made his first flight in a jet, a Grumman F9F Panther, on January 5, 1951. He was promoted to ensign on June 5, 1951, and made his first jet carrier landing on two days later. On June 28, 1951, "Essex" had set sail for Korea, with VF-51 aboard to act as ground-attack aircraft. VF-51 flew ahead to Naval Air Station Barbers Point in Hawaii, where it conducted fighter-bomber training before rejoining the ship at the end of July.

On August 29, 1951, Armstrong saw action in the Korean War as an escort for a photo reconnaissance plane over Songjin. Five days later, on September 3, he flew armed reconnaissance over the primary transportation and storage facilities south of the village of Majon-ni, west of Wonsan. An initial report to the commanding officer of "Essex" said that while attacking a target, Armstrong's F9F Panther was hit by anti-aircraft fire. The report indicated he was trying to regain control and collided with a pole, which sliced off of the Panther's right wing. Further perversions of the story by different authors added that he was only from the ground and that of his wing was sheared off.

According to Armstrong, he was making a low bombing run at when of his wing was torn off after it collided with a cable that was strung across the hills as a booby trap. He was flying above the ground when he hit it. While there was heavy anti-aircraft fire in the area, none hit Armstrong's aircraft.
Armstrong flew the plane back to friendly territory, but due to the loss of the aileron, ejection was his only safe option. He intended to eject over water and await rescue by Navy helicopters, but his parachute was blown back over land. A jeep driven by a roommate from flight school picked him up; it is unknown what happened to the wreckage of his aircraft, F9F-2 BuNo "125122".

In all, Armstrong flew 78missions over Korea for a total of 121hours in the air, a third of them in January 1952, with the final mission on March 5, 1952. Of 492 U.S. Navy personnel killed in the Korean War, 27 of them were from "Essex" on this war cruise. Armstrong received the Air Medal for 20 combat missions, two gold stars for the next 40, the Korean Service Medal and Engagement Star, the National Defense Service Medal, and the United Nations Korea Medal.

Armstrong's regular commission was terminated on February 25, 1952, and he became an ensign in the United States Navy Reserve. On completion of his combat tour with "Essex", he was assigned to a transport squadron, VR-32, in May 1952. He was released from active duty on August 23, 1952, but remained in the reserve, and was promoted to lieutenant (junior grade) on May 9, 1953. As a reservist, he continued to fly, with VF-724 at Naval Air Station Glenview in Illinois, and then, after moving to California, with VF-773 at Naval Air Station Los Alamitos. He remained in the reserve for eight years, before resigning his commission on October 21, 1960.

After his service with the Navy, Armstrong returned to Purdue. His previously earned good but not outstanding grades now improved, lifting his final Grade Point Average (GPA) to a respectable but not outstanding 4.8 out of 6.0. He pledged the Phi Delta Theta fraternity, and lived in its fraternity house. He wrote and co-directed two musicals as part of the all-student revue. The first was a version of "Snow White and the Seven Dwarves", co-directed with his girlfriend Joanne Alford from the Alpha Chi Omega sorority, with songs from the Walt Disney film, including "Someday My Prince Will Come"; the second was titled "The Land of Egelloc" ("college" spelled backwards), with music from Gilbert and Sullivan but new lyrics. He was chairman of the Purdue Aero Flying Club, and flew the club's aircraft, an Aeronca and a couple of Pipers, which were kept at nearby Aretz Airport in Lafayette, Indiana. Flying the Aeronca to Wapakoneta in 1954, he damaged it in a rough landing in a farmer's field, and it had to be hauled back to Lafayette on a trailer. He was a baritone player in the Purdue All-American Marching Band. Ten years later he was made an honorary member of Kappa Kappa Psi national band honorary fraternity. Armstrong graduated with a Bachelor of Science degree in Aeronautical Engineering in January 1955. In 1970, he completed his Master of Science degree in Aerospace Engineering at the University of Southern California (USC). He would eventually be awarded honorary doctorates by several universities.

Armstrong met Janet Elizabeth Shearon, who was majoring in home economics, at a party hosted by Alpha Chi Omega. According to the couple, there was no real courtship, and neither could remember the exact circumstances of their engagement. They were married on January 28, 1956, at the Congregational Church in Wilmette, Illinois. When he moved to Edwards Air Force Base, he lived in the bachelor quarters of the base, while Janet lived in the Westwood district of Los Angeles. After one semester, they moved into a house in Antelope Valley, near Edwards AFB. Janet did not finish her degree, a fact she regretted later in life. The couple had three children: Eric, Karen, and Mark. In June 1961, Karen was diagnosed with a diffuse intrinsic pontine glioma, a malignant tumor of the middle part of her brain stem. X-ray treatment slowed its growth, but her health deteriorated to the point where she could no longer walk or talk. She died of pneumonia, related to her weakened health, on January 28, 1962, aged two.

Following his graduation from Purdue, Armstrong became an experimental research test pilot. He applied at the National Advisory Committee for Aeronautics (NACA) High-Speed Flight Station at Edwards Air Force Base. NACA had no open positions, and forwarded his application to the Lewis Flight Propulsion Laboratory in Cleveland, where Armstrong made his first test flight on March 1, 1955. Armstrong's stint at Cleveland lasted only a couple of months before a position at the High-Speed Flight Station became available, and he reported for work there on July 11, 1955.
On his first day, Armstrong was tasked with piloting chase planes during releases of experimental aircraft from modified bombers. He also flew the modified bombers, and on one of these missions had his first flight incident at Edwards. On March 22, 1956, he was in a Boeing B-29 Superfortress, which was to air-drop a Douglas D-558-2 Skyrocket. He sat in the right-hand pilot seat while the left-hand seat commander, Stan Butchart, flew the B-29.

As they climbed to , the number-four engine stopped and the propeller began windmilling (rotating freely) in the airstream. Hitting the switch that would stop the propeller's spinning, Butchart found it slowed but then started spinning again, this time even faster than the others; if it spun too fast, it would break apart. Their aircraft needed to hold an airspeed of to launch its Skyrocket payload, and the B-29 could not land with the Skyrocket attached to its belly. Armstrong and Butchart brought the aircraft into a nose-down attitude to increase speed, then launched the Skyrocket. At the instant of launch, the number-four engine propeller disintegrated. Pieces of it damaged the number-three engine and hit the number-two engine. Butchart and Armstrong were forced to shut down the damaged number-three engine, along with the number-one engine, due to the torque it created. They made a slow, circling descent from using only the number-two engine, and landed safely.

Armstrong served as project pilot on Century Series fighters, including the North American F-100 Super Sabre A and C variants, the McDonnell F-101 Voodoo, the Lockheed F-104 Starfighter, the Republic F-105 Thunderchief and the Convair F-106 Delta Dart. He also flew the Douglas DC-3, Lockheed T-33 Shooting Star, North American F-86 Sabre, McDonnell Douglas F-4 Phantom II, Douglas F5D-1 Skylancer, Boeing B-29 Superfortress, Boeing B-47 Stratojet and Boeing KC-135 Stratotanker, and was one of eight elite pilots involved in the Parasev paraglider research vehicle program. Over his career, he flew more than 200 different models of aircraft. His first flight in a rocket-powered aircraft was on August 15, 1957, in the Bell X-1B, to an altitude of . On landing, the poorly designed nose landing gear failed, as had happened on about a dozen previous flights of the Bell X-1B. He flew the North American X-15 seven times, including the first flight with the Q-ball system, the first flight of the number3 X-15 airframe, and the first flight of the MH-96 adaptive flight control system. He became an employee of the National Aeronautics and Space Administration (NASA) when it was established on October 1, 1958, absorbing NACA.

Armstrong was involved in several incidents that went down in Edwards folklore or were chronicled in the memoirs of colleagues. During his sixth X-15 flight on April 20, 1962, Armstrong was testing the MH-96 control system when he flew to a height of over (the highest he flew before Gemini 8). He held up the aircraft nose for too long during its descent to demonstrate the MH-96's g-limiting performance, and the X-15 ballooned back up to around . He flew past the landing field at Mach3 at over in altitude, and ended up south of Edwards. After sufficient descent, he turned back toward the landing area, and landed, just missing Joshua trees at the south end. It was the longest X-15 flight in both flight time and length of the ground track.

Many of the test pilots at Edwards praised Armstrong's engineering ability. Milt Thompson said he was "the most technically capable of the early X-15 pilots". Bill Dana said Armstrong "had a mind that absorbed things like a sponge". Those who flew for the Air Force tended to have a different opinion, especially people like Chuck Yeager and Pete Knight, who did not have engineering degrees. Knight said that pilot-engineers flew in a way that was "more mechanical than it is flying", and gave this as the reason why some pilot-engineers got into trouble: their flying skills did not come naturally. Armstrong made seven flights in the X-15 between November 30, 1960, and July 26, 1962. He reached a top speed of Mach 5.74 () in the X-15-1, and left the Flight Research Center with a total of 2,400 flying hours.

On April 24, 1962, Armstrong flew for the only time with Chuck Yeager. Their job, flying a T-33, was to evaluate Smith Ranch Dry Lake in Nevada for use as an emergency landing site for the X-15. In his autobiography, Yeager wrote that he knew the lake bed was unsuitable for landings after recent rains, but Armstrong insisted on flying out anyway. As they attempted a touch-and-go, the wheels became stuck and they had to wait for rescue. As Armstrong told the story, Yeager never tried to talk him out of it and they made a first successful landing on the east side of the lake. Then Yeager told him to try again, this time a bit slower. On the second landing, they became stuck, provoking Yeager to fits of laughter.

On May 21, 1962, Armstrong was involved in the "Nellis Affair". He was sent in an F-104 to inspect Delamar Dry Lake in southern Nevada, again for emergency landings. He misjudged his altitude and did not realize that the landing gear had not fully extended. As he touched down, the landing gear began to retract; Armstrong applied full power to abort the landing, but the ventral fin and landing gear door struck the ground, damaging the radio and releasing hydraulic fluid. Without radio communication, Armstrong flew south to Nellis Air Force Base, past the control tower, and waggled his wings, the signal for a no-radio approach. The loss of hydraulic fluid caused the tailhook to release, and upon landing, he caught the arresting wire attached to an anchor chain, and dragged the chain along the runway.

It took thirty minutes to clear the runway and rig another arresting cable. Armstrong telephoned Edwards and asked for someone to collect him. Milt Thompson was sent in an F-104B, the only two-seater available, but a plane Thompson had never flown. With great difficulty, Thompson made it to Nellis, where a strong crosswind caused a hard landing and the left main tire suffered a blowout. The runway was again closed to clear it, and Bill Dana was sent to Nellis in a T-33, but he almost landed long. The Nellis base operations office then decided that to avoid any further problems, it would be best to find the three NASA pilots ground transport back to Edwards.

In June 1958, Armstrong was selected for the U.S. Air Force's Man In Space Soonest program, but the Advanced Research Projects Agency (ARPA) canceled its funding on August 1, 1958, and on November 5, 1958, it was superseded by Project Mercury, a civilian project run by NASA. As a NASA civilian test pilot, Armstrong was ineligible to become one of its astronauts at this time, as selection was restricted to military test pilots. In November 1960, he was chosen as part of the pilot consultant group for the X-20 Dyna-Soar, a military space plane under development by Boeing for the U.S. Air Force, and on March 15, 1962, he was selected by the U.S. Air Force as one of seven pilot-engineers who would fly the X-20 when it got off the design board.

In April 1962, NASA announced that applications were being sought for the second group of NASA astronauts for Project Gemini, a proposed two-man spacecraft. This time, selection was open to qualified civilian test pilots. Armstrong visited the Seattle World's Fair in May 1962, and attended a conference there on space exploration that was co-sponsored by NASA. After he returned from Seattle on June 4, he applied to become an astronaut. His application arrived about a week past the June 1, 1962, deadline, but Dick Day, a flight simulator expert with whom Armstrong had worked closely at Edwards, saw the late arrival of the application and slipped it into the pile before anyone noticed. At Brooks Air Force Base at the end of June, Armstrong underwent a medical exam that many of the applicants described as painful and at times seemingly pointless.

NASA's Director of Flight Crew Operations, Deke Slayton, called Armstrong on September 13, 1962, and asked whether he would be interested in joining the NASA Astronaut Corps as part of what the press dubbed "the New Nine"; without hesitation, Armstrong said yes. The selections were kept secret until three days later, although newspaper reports had circulated since earlier that year that he would be selected as the "first civilian astronaut". Armstrong was one of two civilian pilots selected for this group; the other was Elliot See, another former naval aviator. NASA announced the selection of the second group at a press conference on September 17, 1962. Compared with the Mercury Seven astronauts, they were younger, and had more impressive academic credentials.

On February 8, 1965, Armstrong and Elliot See were announced as the backup crew for Gemini 5, with Armstrong as commander, supporting the prime crew of Gordon Cooper and Pete Conrad. The mission's purpose was to practice space rendezvous and to develop procedures and equipment for a seven-day flight, all of which would be required for a mission to the Moon. With two other flights (Gemini 3 and Gemini 4) in preparation, six crews were competing for simulator time, so Gemini5 was postponed. It finally lifted off on August 21. Armstrong and See watched the launch at Cape Kennedy, then flew to the Manned Spacecraft Center (MSC) in Houston. The mission was generally successful, despite a problem with the fuel cells that prevented a rendezvous. Cooper and Conrad practiced a "phantom rendezvous", carrying out the maneuver without a target.

The crew assignments for Gemini8 were announced on September 20, 1965. Under the normal rotation system, the backup crew for one mission became the prime crew for the third mission after, but Slayton designated David Scott as the pilot of Gemini8. Scott was the first member of the third group of astronauts, whose selection was announced on October 18, 1963, to receive a prime crew assignment. See was designated to command Gemini 9. Henceforth, each Gemini mission was commanded by a member of Armstrong's group, with a member of Scott's group as the pilot. Conrad would be Armstrong's backup this time, and Richard F. Gordon Jr. his pilot. Armstrong became the first American civilian in space. (Valentina Tereshkova of the Soviet Union had become the first civilian—and first woman—nearly three years earlier aboard Vostok 6 when it launched on June 16, 1963.) Armstrong would also be the last of his group to fly in space, as See died in a T-38 crash on February 28, 1966, that also took the life of crewmate Charles Bassett. They were replaced by the backup crew of Tom Stafford and Gene Cernan, while Jim Lovell and Buzz Aldrin moved up from the backup crew of Gemini 10 to become the backup for Gemini 9, and would eventually fly Gemini 12.

Gemini 8 launched on March 16, 1966. It was the most complex mission yet, with a rendezvous and docking with an uncrewed Agena target vehicle, and the planned second American space walk (EVA) by Scott. The mission was planned to last 75hours and 55orbits. After the Agena lifted off at 10:00:00 EST, the Titan II rocket carrying Armstrong and Scott ignited at 11:41:02 EST, putting them into an orbit from which they chased the Agena. They achieved the first-ever docking between two spacecraft. Contact with the crew was intermittent due to the lack of tracking stations covering their entire orbits. While out of contact with the ground, the docked spacecraft began to roll, and Armstrong attempted to correct this with the Gemini's Orbit Attitude and Maneuvering System (OAMS). Following the earlier advice of Mission Control, they undocked, but the roll increased dramatically until they were turning about once per second, indicating a problem with Gemini's attitude control. Armstrong engaged the Reentry Control System (RCS) and turned off the OAMS. Mission rules dictated that once this system was turned on, the spacecraft had to reenter at the next possible opportunity. It was later thought that damaged wiring caused one of the thrusters to stick in the on position.
A few people in the Astronaut Office, including Walter Cunningham, felt that Armstrong and Scott "had botched their first mission". There was speculation that Armstrong could have salvaged the mission if he had turned on only one of the two RCS rings, saving the other for mission objectives. These criticisms were unfounded; no malfunction procedures had been written, and it was possible to turn on only both RCS rings, not one or the other. Gene Kranz wrote, "The crew reacted as they were trained, and they reacted wrong because we trained them wrong." The mission planners and controllers had failed to realize that when two spacecraft were docked, they must be considered one spacecraft. Kranz considered this the mission's most important lesson. Armstrong was depressed that the mission was cut short, canceling most mission objectives and robbing Scott of his EVA. The Agena was later reused as a docking target by Gemini 10. Armstrong and Scott received the NASA Exceptional Service Medal, and the Air Force awarded Scott the Distinguished Flying Cross as well. Scott was promoted to lieutenant colonel, and Armstrong received a $678 raise in pay to $21,653 a year (), making him NASA's highest-paid astronaut.

In Armstrong's final assignment in the Gemini program, he was the back-up Command Pilot for Gemini 11; this was announced two days after the landing of Gemini 8. Having trained for two flights, Armstrong was quite knowledgeable about the systems and took on a teaching role for the rookie backup Pilot, William Anders. The launch was on September 12, 1966, with Conrad and Gordon on board, who successfully completed the mission objectives, while Armstrong served as a capsule communicator (CAPCOM).

Following the flight, President Lyndon B. Johnson asked Armstrong and his wife to take part in a 24-day goodwill tour of South America. Also on the tour, which took in 11countries and 14major cities, were Dick Gordon, George Low, their wives, and other government officials. In Paraguay, Armstrong greeted dignitaries in their local language, Guarani; in Brazil he talked about the exploits of the Brazilian-born Alberto Santos-Dumont.

On January 27, 1967—the day of the Apollo 1 fire—Armstrong was in Washington, D.C. with Cooper, Gordon, Lovell and Scott Carpenter for the signing of the United Nations Outer Space Treaty. The astronauts chatted with the assembled dignitaries until 18:45, when Carpenter went to the airport, and the others returned to the Georgetown Inn, where they each found messages to phone the MSC. During these calls, they learned of the deaths of Gus Grissom, Ed White and Roger Chaffee in the fire. Armstrong and the group spent the rest of the night drinking scotch and discussing what had happened.

On April 5, 1967, the same day the Apollo1 investigation released its final report, Armstrong and 17 other astronauts gathered for a meeting with Slayton. The first thing Slayton said was, "The guys who are going to fly the first lunar missions are the guys in this room." According to Cernan, only Armstrong showed no reaction to the statement. To Armstrong it came as no surprise—the room was full of veterans of Project Gemini, the only people who could fly the lunar missions. Slayton talked about the planned missions and named Armstrong to the backup crew for Apollo 9, which at that stage was planned as a medium Earth orbit test of the combined lunar module and command and service module.

The crew assignment was officially announced November 20, 1967. For crewmates, Armstrong was assigned Lovell and Aldrin, from Gemini 12. After design and manufacturing delays of the lunar module (LM), Apollo 8 and9 swapped prime and backup crews. Based on the normal crew rotation, Armstrong would command Apollo 11, with one change: Mike Collins on the Apollo8 crew began experiencing trouble with his legs. Doctors diagnosed the problem as a bony growth between his fifth and sixth vertebrae, requiring surgery. Lovell took his place on the Apollo8 crew, and, when Collins recovered, he joined Armstrong's crew.

To give the astronauts practice piloting the LM on its descent, NASA commissioned Bell Aircraft to build two Lunar Landing Research Vehicles (LLRV), later augmented with three Lunar Landing Training Vehicles (LLTV). Nicknamed the "Flying Bedsteads", they simulated the Moon's one-sixth gravity using a turbofan engine to support five-sixths of the craft's weight. On May 6, 1968, above the ground, Armstrong's controls started to degrade and the LLRV began rolling. He ejected safely before the vehicle struck the ground and burst into flames. Later analysis suggested that if he had ejected half a second later, his parachute would not have opened in time. His only injury was from biting his tongue. The LLRV was completely destroyed. Even though he was nearly killed, Armstrong maintained that without the LLRV and LLTV, the lunar landings would not have been successful, as they gave commanders essential experience in piloting the lunar landing craft.

In addition to the LLRV training, NASA began lunar landing simulator training after Apollo 10 was completed. Aldrin and Armstrong trained for a variety of scenarios that could develop during a real lunar landing. They also received briefings from geologists at NASA.

After Armstrong served as backup commander for Apollo8, Slayton offered him the post of commander of Apollo 11 on December 23, 1968, as Apollo8 orbited the Moon. According to Armstrong's 2005 biography, Slayton told him that although the planned crew was Commander Armstrong, Lunar Module Pilot Buzz Aldrin, and Command Module Pilot Michael Collins, he was offering Armstrong the chance to replace Aldrin with Jim Lovell. After thinking it over for a day, Armstrong told Slayton he would stick with Aldrin, as he had no difficulty working with him and thought Lovell deserved his own command. Replacing Aldrin with Lovell would have made Lovell the lunar module pilot, unofficially the lowest ranked member, and Armstrong could not justify placing Lovell, the commander of Gemini 12, in the number3 position of the crew. The crew of Apollo 11 was officially announced on January 9, 1969, as Armstrong, Collins, and Aldrin, with Lovell, Anders, and Fred Haise as the backup crew.

According to Chris Kraft, a March 1969 meeting among Slayton, George Low, Bob Gilruth, and Kraft determined that Armstrong would be the first person on the Moon, in part because NASA management saw him as a person who did not have a large ego. A press conference on April 14, 1969, gave the design of the LM cabin as the reason for Armstrong's being first; the hatch opened inwards and to the right, making it difficult for the LM pilot, on the right-hand side, to exit first. At the time of their meeting, the four men did not know about the hatch consideration. The first knowledge of the meeting outside the small group came when Kraft wrote his book. Methods of circumventing this difficulty existed, but it is not known if these were considered at the time. Slayton added, "Secondly, just on a pure protocol basis, I figured the commander ought to be the first guy out... I changed it as soon as I found they had the time line that showed that. Bob Gilruth approved my decision."

A Saturn V rocket launched Apollo 11 from Launch Complex 39A at the Kennedy Space Center on July 16, 1969, at 13:32:00 UTC (09:32:00 EDT local time). Armstrong's wife Janet and two sons watched from a yacht moored on the Banana River. During the launch, Armstrong's heart rate peaked at 110beats per minute. He found the first stage the loudest, much noisier than the Gemini8 Titan II launch. The Apollo command module was relatively roomy compared with the Gemini spacecraft. None of the Apollo 11 crew suffered space sickness, as some members of previous crews had. Armstrong was especially glad about this, as he had been prone to motion sickness as a child and could experience nausea after long periods of aerobatics.
Apollo 11's objective was to land safely on the Moon, rather than to touch down at a precise location. Three minutes into the lunar descent, Armstrong noted that craters were passing about two seconds too early, which meant the Lunar Module "Eagle" would probably touch down several miles (kilometres) beyond the planned landing zone. As the "Eagle"s landing radar acquired the surface, several computer error alarms sounded. The first was a code 1202 alarm, and even with their extensive training, neither Armstrong nor Aldrin knew what this code meant. They promptly received word from CAPCOM Charles Duke in Houston that the alarms were not a concern; the 1202 and 1201 alarms were caused by executive overflows in the lunar module guidance computer. In 2007, Aldrin said the overflows were caused by his own counter-checklist choice of leaving the docking radar on during the landing process, causing the computer to process unnecessary radar data. When it did not have enough time to execute all tasks, the computer dropped the lower-priority ones, triggering the alarms. Aldrin said he decided to leave the radar on in case an abort was necessary when re-docking with the Apollo command module; he did not realize it would cause the processing overflows.

When Armstrong noticed they were heading toward a landing area that seemed unsafe, he took manual control of the LM and attempted to find a safer area. This took longer than expected, and longer than most simulations had taken. For this reason, Mission Control was concerned that the LM was running low on fuel. On landing, Aldrin and Armstrong believed they had 40seconds of fuel left, including the 20seconds' worth which had to be saved in the event of an abort. During training, Armstrong had, on several occasions, landed with fewer than 15seconds of fuel; he was also confident the LM could survive a fall of up to . Post-mission analysis showed that at touchdown there were 45 to 50seconds of propellant burn time left.

The landing on the surface of the Moon occurred several seconds after 20:17:40 UTC on July 20, 1969. One of three probes attached to three of the LM's four legs made contact with the surface, a panel light in the LM illuminated, and Aldrin called out, "Contact light." Armstrong shut the engine off and said, "Shutdown." As the LM settled onto the surface, Aldrin said, "Okay, engine stop"; then they both called out some post-landing checklist items. After a 10-second pause, Duke acknowledged the landing with, "We copy you down, "Eagle"." Armstrong announced the landing to Mission Control and the world with the words, "Houston, Tranquility Base here. The "Eagle" has landed." Aldrin and Armstrong celebrated with a brisk handshake and pat on the back. They then returned to the checklist of contingency tasks, should an emergency liftoff become necessary. After Armstrong confirmed touch down, Duke re-acknowledged, adding a comment about the flight crew's relief: "Roger, Tranquility. We copy you on the ground. You got a bunch of guys about to turn blue. We're breathing again. Thanks a lot." During the landing, Armstrong's heart rate ranged from 100 to 150beats per minute.

The flight plan called for a crew rest period before leaving the module, but Armstrong asked for this be moved to earlier in the evening, Houston time. When he and Aldrin were ready to go outside, "Eagle" was depressurized, the hatch was opened, and Armstrong made his way down the ladder. At the bottom of the ladder Armstrong said, "I'm going to step off the LM [lunar module] now". He turned and set his left boot on the lunar surface at 02:56 UTC July 21, 1969, then said, "That's one small step for [a] man, one giant leap for mankind." The exact timing of Armstrong's first step on the Moon is unclear.

Armstrong prepared his famous epigram on his own. In a post-flight press conference, he said that he chose the words "just prior to leaving the LM." In a 1983 interview in "Esquire" magazine, he explained to George Plimpton: "I always knew there was a good chance of being able to return to Earth, but I thought the chances of a successful touch down on the moon surface were about even money—fifty–fifty... Most people don't realize how difficult the mission was. So it didn't seem to me there was much point in thinking of something to say if we'd have to abort landing." In 2012, his brother Dean Armstrong said that Neil showed him a draft of the line months before the launch. Historian Andrew Chaikin, who interviewed Armstrong in 1988 for his book "A Man on the Moon", disputed that Armstrong claimed to have conceived the line during the mission.

Recordings of Armstrong's transmission do not provide evidence for the indefinite article "a" before "man", though NASA and Armstrong insisted for years that static obscured it. Armstrong stated he would never make such a mistake, but after repeated listenings to recordings, he eventually conceded he must have dropped the "a". He later said he "would hope that history would grant me leeway for dropping the syllable and understand that it was certainly intended, even if it was not said—although it might actually have been". There have since been claims and counter-claims about whether acoustic analysis of the recording reveals the presence of the missing "a"; Peter Shann Ford, an Australian computer programmer, conducted a digital audio analysis and claims that Armstrong did say "a man", but the "a" was inaudible due to the limitations of communications technology of the time. Ford and James R. Hansen, Armstrong's authorized biographer, presented these findings to Armstrong and NASA representatives, who conducted their own analysis. Armstrong found Ford's analysis "persuasive." Linguists David Beaver and Mark Liberman wrote of their skepticism of Ford's claims on the blog Language Log. A 2016 peer-reviewed study again concluded Armstrong had included the article. NASA's transcript continues to show the "a" in parentheses.

When Armstrong made his proclamation, Voice of America was rebroadcast live by the BBC and many other stations worldwide. An estimated 530million people viewed the event, 20percent out of a world population of approximately 3.6billion.
About 19minutes after Armstrong's first step, Aldrin joined him on the surface, becoming the second human to walk on the Moon. They began their tasks of investigating how easily a person could operate on the lunar surface. Armstrong unveiled a plaque commemorating the flight, and with Aldrin, planted the flag of the United States. Although Armstrong had wanted the flag to be draped on the flagpole, it was decided to use a metal rod to hold it horizontally. However, the rod did not fully extend, leaving the flag with a slightly wavy appearance, as if there were a breeze. Shortly after the flag planting, President Richard Nixon spoke to them by telephone from his office. He spoke for about a minute, after which Armstrong responded for about thirty seconds. In the Apollo 11 photographic record, there are only five images of Armstrong partly shown or reflected. The mission was planned to the minute, with the majority of photographic tasks performed by Armstrong with the single Hasselblad camera.

After helping to set up the Early Apollo Scientific Experiment Package, Armstrong went for a walk to what is now known as East Crater, east of the LM, the greatest distance traveled from the LM on the mission. His final task was to remind Aldrin to leave a small package of memorial items to Soviet cosmonauts Yuri Gagarin and Vladimir Komarov, and Apollo1 astronauts Grissom, White and Chaffee. The Apollo 11 EVA lasted two and a half hours. Each of the subsequent five landings was allotted a progressively longer EVA period; the crew of Apollo 17 spent over 22hours exploring the lunar surface. In a 2010 interview, Armstrong explained that NASA limited their Moon walk because they were unsure how the space suits would cope with the Moon's extremely high temperature.

After they re-entered the LM, the hatch was closed and sealed. While preparing for liftoff, Armstrong and Aldrin discovered that, in their bulky space suits, they had broken the ignition switch for the ascent engine; using part of a pen, they pushed in the circuit breaker to start the launch sequence. The "Eagle" then continued to its rendezvous in lunar orbit, where it docked with "Columbia", the command and service module. The three astronauts returned to Earth and splashed down in the Pacific Ocean, to be picked up by the .

After being released from an 18-day quarantine to ensure that they had not picked up any infections or diseases from the Moon, the crew was feted across the United States and around the world as part of a 38-day "Giant Leap" tour. 
The tour began on August 13, when the three astronauts spoke and rode in ticker-tape parades in their honor in New York and Chicago, with an estimated six million attendees. On the same evening an official state dinner was held in Los Angeles to celebrate the flight, attended by members of Congress, 44governors, the Chief Justice of the United States, and ambassadors from 83nations. President Nixon and Vice President Agnew presented each astronaut with a Presidential Medal of Freedom.

After the tour Armstrong took part in Bob Hope's 1969 USO show, primarily to Vietnam. In May 1970, Armstrong traveled to the Soviet Union to present a talk at the 13th annual conference of the International Committee on Space Research; after arriving in Leningrad from Poland, he traveled to Moscow where he met Premier Alexei Kosygin. Armstrong was the first westerner to see the supersonic Tupolev Tu-144 and was given a tour of the Yuri Gagarin Cosmonaut Training Center, which he described as "a bit Victorian in nature". At the end of the day, he was surprised to view a delayed video of the launch of Soyuz 9 as it had not occurred to Armstrong that the mission was taking place, even though Valentina Tereshkova had been his host and her husband, Andriyan Nikolayev, was on board.

Shortly after Apollo 11, Armstrong announced that he did not plan to fly in space again. He was appointed Deputy Associate Administrator for Aeronautics for the Office of Advanced Research and Technology at ARPA, served in the position for a year, then resigned from it and NASA in 1971. He accepted a teaching position in the Department of Aerospace Engineering at the University of Cincinnati, having chosen Cincinnati over other universities, including his "alma mater" Purdue, because Cincinnati had a small aerospace department, and said he hoped the faculty there would not be annoyed that he came straight into a professorship with only a USC master's degree. He began his master's degree while stationed at Edwards years before, and completed it after Apollo 11 by presenting a report on various aspects of Apollo, instead of a thesis on the simulation of hypersonic flight.

At Cincinnati, Armstrong was University Professor of Aerospace Engineering. He took a heavy teaching load, taught core classes, and created two graduate-level classes: aircraft design and experimental flight mechanics. He was considered a good teacher, and a tough grader. His research activities during this time did not involve his work at NASA, as he did not want to give the appearance of favoritism; he later regretted the decision. After teaching for eight years, Armstrong resigned in 1980. When the university changed from an independent municipal university to a state school, bureaucracy increased. He did not want to be a part of the faculty collective bargaining group, so he decided to teach half-time. According to Armstrong, he had the same amount of work but received half his salary. In 1979, less than 10% of his income came from his university salary. Employees at the university did not know why he left.

In 1970, after an explosion aboard Apollo 13 aborted its lunar landing, Armstrong was part of Edgar Cortright's investigation of the mission. He produced a detailed chronology of the flight. He determined that a 28-volt thermostat switch in an oxygen tank, which was supposed to have been replaced with a 65-volt version, led to the explosion. Cortright's report recommended the entire tank be redesigned at a cost of $40million. Many NASA managers, including Armstrong, opposed the recommendation, since only the thermostat switch had caused the problem. They lost the argument and the tanks were redesigned.

In 1986, President Ronald Reagan asked Armstrong to join the Rogers Commission investigating the destruction of the Space Shuttle "Challenger". Armstrong was made vice chairman of the commission, and held private interviews with contacts he had developed over the years to help determine the cause of the disaster. He helped limit the committee's recommendations to nine, believing that if there were too many, NASA would not act on them.
Armstrong was appointed to a fourteen-member commission by President Reagan to develop a plan for American civilian spaceflight in the 21st century. The commission was chaired by former NASA administrator Dr. Thomas O. Paine, with whom Armstrong had worked during the Apollo program. The group published a book titled "Pioneering the Space Frontier: The Report on the National Commission on Space", recommending a permanent lunar base by 2006, and sending people to Mars by 2015. The recommendations were largely ignored, overshadowed by the "Challenger" disaster.

Armstrong and his wife attended the memorial service for the victims of the Space Shuttle "Columbia" disaster in 2003, at the invitation of President George W. Bush.

After Armstrong retired from NASA in 1971, he acted as a spokesman for several businesses. The first company to successfully approach him was Chrysler, for whom he appeared in advertising starting in January 1979. Armstrong thought they had a strong engineering division, and they were in financial difficulty. He later acted as a spokesman for other American companies, including General Time Corporation and the Bankers Association of America. He acted as a spokesman for only American companies.

In addition to his duties as a spokesman, he also served on the board of directors of several companies. The first company board Armstrong joined was Gates Learjet, chairing their technical committee. He flew their new and experimental jets and even set a climb and altitude record for business jets. Armstrong became a member of Cincinnati Gas & Electric Company's board in 1973. They were interested in nuclear power and wanted to increase the company's technical competence. He served on the board of Taft Broadcasting, also based in Cincinnati. Armstrong joined Thiokol's board in 1989, after he was vice-chair of the Rogers Commission; the Space Shuttle "Challenger" was destroyed due to a problem with the Thiokol-manufactured solid rocket boosters. When Armstrong left the University of Cincinnati, he became the chairman of Cardwell International Ltd., a company that manufactured drilling rigs. He served on additional aerospace boards, first United Airlines in 1978, and later Eaton Corporation in 1980. He was asked to chair the board of directors for a subsidiary of Eaton, AIL Systems. He chaired the board through the company's 2000 merger with EDO Corporation, until his retirement in 2002.
In 1985, professional expedition leader Mike Dunn organized a trip to take men he deemed the "greatest explorers" to the North Pole. The group included Armstrong, Edmund Hillary, Hillary's son Peter, Steve Fossett, and Patrick Morrow. They arrived at the Pole on April 6, 1985. Armstrong said he was curious to see what it looked like from the ground, as he had seen it only from the Moon. He did not inform the media of the trip, preferring to keep it private.

Armstrong's family described him as a "reluctant American hero". He kept a low profile later in his life, leading to the belief that he was a recluse. Recalling Armstrong's humility, John Glenn, the first American to orbit Earth, told CNN: "[Armstrong] didn't feel that he should be out huckstering himself. He was a humble person, and that's the way he remained after his lunar flight, as well as before." Armstrong turned down most requests for interviews and public appearances. Michael Collins said in his book "Carrying the Fire" that when Armstrong moved to a dairy farm to become a college professor, it was like he "retreated to his castle and pulled up the drawbridge". Armstrong found this amusing, and said, "...those of us that live out in the hinterlands think that people that live inside the Beltway are the ones that have the problems."

Andrew Chaikin says in "A Man on the Moon" that Armstrong kept a low profile but was not a recluse, citing his participation in interviews, advertisements for Chrysler, and hosting a cable television series. Between 1991 and 1993, he hosted "First Flights with Neil Armstrong", an aviation history documentary series on A&E. In 2010, Armstrong voiced the character of Dr. Jack Morrow in "", an animated educational sci-fi adventure film initiated by JPL/NASA through a grant from Jet Propulsion Lab.

Armstrong guarded the use of his name, image, and famous quote. When it was launched in 1981, MTV wanted to use his quote in its station identification, with the American flag replaced with the MTV logo, but he refused the use of his voice and likeness. He sued Hallmark Cards in 1994, when they used his name, and a recording of the "one small step" quote, in a Christmas ornament without his permission. The lawsuit was settled out of court for an undisclosed sum, which Armstrong donated to Purdue.

For many years, he wrote letters congratulating new Eagle Scouts on their accomplishment, but decided to quit the practice in the 1990s because he felt the letters should be written by people who knew the scout. (In 2003, he received 950congratulation requests.) This contributed to the myth of his reclusiveness. Armstrong used to autograph everything except first day covers. Around 1993, he found out his signatures were being sold online, and that most of them were forgeries, and stopped giving autographs.

Some former astronauts, including Glenn and Harrison Schmitt, sought political careers after leaving NASA. Armstrong was approached by groups from both political parties but he declined the offers. He described his political leanings as favoring states' rights and opposing the United States acting as the "world's policeman".

When Armstrong applied at a local Methodist church to lead a Boy Scout troop in the late 1950s, he gave his religious affiliation as "deist". His mother later said that Armstrong's religious views caused her grief and distress in later life, as she was more religious. Upon his return from the Moon, Armstrong gave a speech in front of the U.S. Congress in which he thanked them for giving him the opportunity to see some of the grandest views of the Creator. In the early 1980s, Armstrong was the subject of a hoax saying that he converted to Islam after hearing the "adhan", the Muslim call to prayer, while walking on the Moon. The Indonesian singer Suhaemi wrote a song called "Gema Suara Adzan di Bulan" ("The Resonant Sound of the Call to Prayer on the Moon"), describing Armstrong's conversion; the song was discussed widely in Jakarta news outlets in 1983. Similar hoax stories were seen in Egypt and Malaysia. In March 1983, the State Department responded by issuing a message to embassies and consulates in Muslim countries saying that Armstrong "has not converted to Islam". The hoax surfaced occasionally for the next three decades. Part of the confusion arose from the similarity between the names of Armstrong's American residence in Lebanon, Ohio, and the country of Lebanon, which has a majority Muslim population.

In 1972, Armstrong visited the town of Langholm, Scotland, the traditional seat of Clan Armstrong. He was made the first freeman of the burgh, and happily declared the town his home. The Justice of the Peace read from an unrepealed 400-year-old law that required him to hang any Armstrong found in the town.

Armstrong flew light aircraft for pleasure. He enjoyed gliders and before the moon flight had earned a gold badge with two diamonds from the International Gliding Commission. Well into his 70s he continued to fly engineless aircraft.

While working at his farm near Lebanon, Ohio, in November 1978, Armstrong jumped off the back of his grain truck and his wedding ring was caught in the wheel, tearing off the tip of his left hand's ring finger. He collected the severed digit and packed it in ice, and surgeons reattached it at the Jewish Hospital in Louisville, Kentucky. In February 1991, a year after his father died, and nine months after the death of his mother, Armstrong suffered a mild heart attack while skiing with friends at Aspen, Colorado.

Armstrong and his first wife, Janet, separated in 1990, and divorced in 1994, after 38 years of marriage. He met his second wife, Carol Held Knight, at a golf tournament in 1992, when they were seated together at breakfast. She said little to Armstrong, but two weeks later he called her to ask what she was doing. She replied that she was cutting down a cherry tree, and 35minutes later Armstrong was at her house to help. They were married in Ohio on June 12, 1994, and had a second ceremony at San Ysidro Ranch in California. He lived in Indian Hill, Ohio. Through his marriage to Carol, Armstrong was the father-in-law to current New York Mets general manager Brodie Van Wagenen.

In May 2005, Armstrong became involved in a legal dispute with his barber of 20years, Mark Sizemore. After cutting Armstrong's hair, Sizemore sold some of it to a collector for $3,000 without Armstrong's knowledge. Armstrong threatened legal action against Sizemore unless he returned the hair or donated the proceeds to a charity of Armstrong's choosing. Sizemore, unable to retrieve the hair, donated the proceeds to charity.

Armstrong underwent bypass surgery on August 7, 2012, to relieve coronary artery disease. Although he was reportedly recovering well, he developed complications in the hospital and died on August 25, in Cincinnati, Ohio, aged 82. The White House released a statement in which President Obama described Armstrong as "among the greatest of American heroes—not just of his time, but of all time". It went on to say that Armstrong had carried the aspirations of the United States' citizens and had delivered "a moment of human achievement that will never be forgotten."
Armstrong's family released a statement describing him as a "reluctant American hero [who had] served his nation proudly, as a navy fighter pilot, test pilot, and astronaut ... While we mourn the loss of a very good man, we also celebrate his remarkable life and hope that it serves as an example to young people around the world to work hard to make their dreams come true, to be willing to explore and push the limits, and to selflessly serve a cause greater than themselves. For those who may ask what they can do to honor Neil, we have a simple request. Honor his example of service, accomplishment and modesty, and the next time you walk outside on a clear night and see the moon smiling down at you, think of Neil Armstrong and give him a wink." It prompted many responses, including the Twitter hashtag "#WinkAtTheMoon".

Buzz Aldrin called Armstrong "a true American hero and the best pilot I ever knew", and said he was disappointed that they would not be able to celebrate the 50th anniversary of the Moon landing together in 2019. Michael Collins said, "He was the best, and I will miss him terribly." NASA Administrator Charles F. Bolden, Jr. said, "As long as there are history books, Neil Armstrong will be included in them, remembered for taking humankind's first small step on a world beyond our own".

A tribute was held for Armstrong on September 13, at Washington National Cathedral, whose Space Window depicts the Apollo 11 mission and holds a sliver of Moon rock amid its stained-glass panels. In attendance were Armstrong's Apollo 11 crewmates, Collins and Aldrin; Gene Cernan, the Apollo 17 mission commander and last man to walk on the Moon; and former senator and astronaut John Glenn, the first American to orbit the Earth. In his eulogy, Charles Bolden praised Armstrong's "courage, grace, and humility". Cernan recalled Armstrong's low-fuel approach to the Moon: "When the gauge says empty, we all know there's a gallon or two left in the tank!" Diana Krall sang the song "Fly Me to the Moon". Collins led prayers. Scott spoke, possibly for the first time, about an incident during their Gemini 8 mission: minutes before the hatch was to be sealed, a small chip of dried glue fell into the latch of his harness and prevented it from being buckled, threatening to abort the mission. Armstrong then called on Conrad to solve the problem, which he did, and the mission proceeded. "That happened because Neil Armstrong was a team player—he always worked on behalf of the team." Congressman Bill Johnson from Armstrong's home state of Ohio led calls for President Barack Obama to authorize a state funeral in Washington D.C. Throughout his lifetime, Armstrong shunned publicity and rarely gave interviews. Mindful that Armstrong would have objected to a state funeral, his family opted to have a private funeral in Cincinnati. On September 14, Armstrong's cremated remains were scattered in the Atlantic Ocean from the . Flags were flown at half-staff on the day of Armstrong's funeral.

In July 2019, after observations of the 50th anniversary of the Moon landing, "The New York Times" reported on details of a medical malpractice suit Armstrong's family had filed against Mercy Health–Fairfield Hospital, where he died. When Armstrong appeared to be recovering from his bypass surgery, nurses removed the wires connected to his temporary pacemaker. He began to bleed internally and his blood pressure dropped. Doctors took him to the hospital's catheterization laboratory, and only later began operating. Two of the three physicians who reviewed the medical files during the lawsuit called this a serious error, saying surgery should have begun immediately; experts the "Times" talked to, while qualifying their judgement by noting that they were unable to review the specific records in the case, said that taking a patient in those circumstances to the operating room generally gave them the highest chance of survival.

The family ultimately settled for $6 million in 2014. Letters included with the 93 pages of documents sent to the "Times" by an unknown individual show that his sons intimated to the hospital, through their lawyers, that they might discuss what happened to their father publicly at the 45th anniversary observances in 2014. The hospital, fearing the bad publicity that would result from being accused of negligently causing the death of a revered figure such as Armstrong, agreed to pay as long as the family never spoke about the suit or the settlement. Armstrong's wife, Carol, was not a party to the lawsuit. She reportedly felt that her husband would have been opposed to taking legal action.

Armstrong received many honors and awards, including the Presidential Medal of Freedom (with distinction) from President Nixon, the Cullum Geographical Medal from the American Geographical Society, and the Collier Trophy from the National Aeronautic Association (1969); the NASA Distinguished Service Medal and the Dr. Robert H. Goddard Memorial Trophy (1970); the Sylvanus Thayer Award by the United States Military Academy (1971); the Congressional Space Medal of Honor from President Jimmy Carter (1978); the Wright Brothers Memorial Trophy from the National Aeronautic Association (2001); and a Congressional Gold Medal (2011).

Armstrong and his Apollo 11 crewmates were the 1999 recipients of the Langley Gold Medal from the Smithsonian Institution. On April 18, 2006, he received NASA's Ambassador of Exploration Award. The Space Foundation named Armstrong as a recipient of its 2013 General James E. Hill Lifetime Space Achievement Award. Armstrong was also inducted into the Aerospace Walk of Honor, the International Space Hall of Fame, National Aviation Hall of Fame, and the United States Astronaut Hall of Fame. He was awarded his Naval Astronaut badge in a ceremony on board the aircraft carrier on March 10, 2010, in a ceremony attended by Lovell and Cernan.

The lunar crater Armstrong, from the Apollo 11 landing site, and asteroid 6469 Armstrong are named in his honor. There are more than a dozen elementary, middle and high schools named for Armstrong in the United States, and many places around the world have streets, buildings, schools, and other places named for him and/or Apollo. The Armstrong Air and Space Museum, in Armstrong's hometown of Wapakoneta, and the Neil Armstrong Airport in New Knoxville, Ohio, are named after him.

Purdue University announced in October 2004, that its new engineering building would be named Neil Armstrong Hall of Engineering; the building was dedicated on October 27, 2007, during a ceremony at which Armstrong was joined by fourteen other Purdue astronauts. The NASA Dryden Flight Research Center was renamed the NASA Neil A. Armstrong Flight Research Center in 2014. In September 2012, the U.S. Navy announced that the first "Armstrong"-class vessel would be named . Delivered to the Navy on September 23, 2015, it is a modern oceanographic research platform supporting a wide range of activities by academic groups. In 2019, the College of Engineering at Purdue University celebrated the 50th anniversary of Neil Armstrong's walk on the Moon by launching the Neil Armstrong Distinguished Visiting Fellows Program, which brings highly accomplished scholars and practitioners to the college to catalyze collaborations with faculty and students.

Armstrong's authorized biography, "", was published in 2005. For many years, he turned down biography offers from authors such as Stephen Ambrose and James A. Michener, but agreed to work with James R. Hansen after reading one of Hansen's other biographies. He recalled his initial concerns about the Apollo 11 mission, when he had believed there was only a 50% chance of landing on the Moon. "I was elated, ecstatic and extremely surprised that we were successful". A film adaptation of the book, starring Ryan Gosling and directed by Damien Chazelle, was released in October 2018.

In July 2018, Armstrong's sons put his collection of memorabilia up for sale, including his Boy Scout cap, and various flags and medals flown on his space missions. A series of auctions was held on November 1 to 3, 2018, that realized $5,276,320. , the auction sales have totaled $16.7million. Two fragments of wood from the propeller and four pieces of fabric from the wing from the 1903 "Wright Flyer" that Armstrong took to the Moon fetched between $112,500 and $275,000 each. Armstrong's wife, Carol, has not put any of his memorabilia up for sale.

Armstrong donated his papers to Purdue. Along with posthumous donations by his widow Carol, the collection consists of over 450boxes of material. In May 2019, she donated two pieces of fabric from the "Wright Flyer", along with his correspondence related to them.

In a 2010 "Space Foundation" survey, Armstrong was ranked as the #1 most popular space hero; and in 2013, "Flying" magazine ranked him on its list of 51 Heroes of Aviation. The press often asked Armstrong for his views on the future of spaceflight. In 2005, he said that a human mission to Mars would be easier than the lunar challenge of the 1960s. In 2010, he made a rare public criticism of the decision to cancel the Ares I launch vehicle and the Constellation Moon landing program. In an open letter also signed by fellow Apollo veterans Lovell and Cernan, he noted, "For The United States, the leading space faring nation for nearly half a century, to be without carriage to low Earth orbit and with no human exploration capability to go beyond Earth orbit for an indeterminate time into the future, destines our nation to become one of second or even third rate stature". On November 18, 2010, aged 80, he said in a speech during the "Science & Technology Summit" in the Hague, Netherlands, that he would offer his services as commander on a mission to Mars if he were asked.

The planetarium at Altoona Area High School in Altoona, Pennsylvania is named after Neil Armstrong and is home to a Space Race museum.

Armstrong was named the class exemplar for the Class of 2019 at the U.S. Air Force Academy. 



</doc>
<doc id="21255" url="https://en.wikipedia.org/wiki?curid=21255" title="North Korea">
North Korea

North Korea (Korean: , MR: "Chosŏn"; literally , MR: "Pukchosŏn", or /, RR: "Bukhan" in South Korean usage), officially the Democratic People's Republic of Korea (DPRK or DPR Korea; Korean: , "Chosŏn Minjujuŭi Inmin Konghwaguk"), is a country in East Asia constituting the northern part of the Korean Peninsula. The country is bordered to the north by China and by Russia along the Amnok (known as the Yalu in Chinese) and Tumen rivers, and to the south by South Korea, with the heavily fortified Korean Demilitarized Zone (DMZ) separating the two. North Korea, like its southern counterpart, claims to be the legitimate government of the entire peninsula and adjacent islands. Pyongyang is the country's capital and largest city.

In 1910, Korea was annexed by Imperial Japan. At the Japanese surrender at the end of World War II in 1945, Korea was divided into two zones, with the north occupied by the Soviet Union and the south occupied by the United States. Negotiations on reunification failed, and in 1948, separate governments were formed: the socialist Democratic People's Republic of Korea in the north, and the capitalist Republic of Korea in the south. An invasion initiated by North Korea led to the Korean War (1950–1953). The Korean Armistice Agreement brought about a ceasefire, but no peace treaty was signed.

According to article 1 of the constitution of North Korea, the DPRK is an "independent socialist State". North Korea holds elections, though they have been described by independent observers as sham elections. North Korea is generally viewed as a totalitarian Stalinist dictatorship, particularly noting the elaborate cult of personality around the Kim dynasty. The Workers' Party of Korea (WPK), led by a member of the ruling family, holds absolute power in the state and leads the Democratic Front for the Reunification of the Fatherland of which all political officers are required to be members. According to article 3 of the constitution of the DPRK, "Kimilsungism-Kimjongilism" is the North Korean official ideology. The means of production are owned by the state through state-run enterprises and collectivized farms. Most services—such as healthcare, education, housing and food production—are subsidized or state-funded. From 1994 to 1998, North Korea suffered a famine that resulted in the deaths of between 240,000 and 420,000 people, and the population continues to suffer malnutrition. North Korea follows "Songun", or "military-first" policy. It is the country with the highest number of military and paramilitary personnel, with a total of 9,495,000 active, reserve and paramilitary personnel, or approximately of its population. Its active duty army of 1.21 million is the fourth-largest in the world, after China, the United States and India; consisting of of its population. It possesses nuclear weapons. In addition to being a member of the United Nations since 1991, North Korea is also a member of the Non-Aligned Movement, G77 and the ASEAN Regional Forum.

A 2014 UN inquiry into abuses of human rights in North Korea concluded that, "the gravity, scale and nature of these violations reveal a state that does not have any parallel in the contemporary world," with Amnesty International and Human Rights Watch holding similar views. The North Korean government denies these abuses.

The name "Korea" derives from the name "Goryeo" (also spelled "Koryŏ"). The name "Goryeo" itself was first used by the ancient kingdom of Goguryeo (Koguryŏ) which was one of the great powers in East Asia during its time, ruling most of the Korean Peninsula, Manchuria, parts of the Russian Far East and parts of Inner Mongolia, under Gwanggaeto the Great. The 10th-century kingdom of Goryeo succeeded Goguryeo, and thus inherited its name, which was pronounced by visiting Persian merchants as "Korea". The modern spelling of Korea first appeared in the late 17th century in the travel writings of the Dutch East India Company's Hendrick Hamel.

After the division of the country into North and South Korea, the two sides used different terms to refer to Korea: "Chosun" or "Joseon" (조선) in North Korea, and "Hanguk" (한국) in South Korea. In 1948, North Korea adopted "Democratic People's Republic of Korea" (, "Chosŏn Minjujuŭi Inmin Konghwaguk"; ) as its new legal name. In the wider world, because the government controls the northern part of the Korean Peninsula, it is commonly called North Korea to distinguish it from South Korea, which is officially called the "Republic of Korea" in English. Both governments consider themselves to be the legitimate government of the whole of Korea. For this reason, the people do not consider themselves as 'North Koreans' but as Koreans in the same divided country as their compatriots in the South and foreign visitors are discouraged from using the former term.

After the First Sino-Japanese War and the Russo-Japanese War, Korea was occupied by Japan from 1910 to 1945. Korean resistance groups known as Dongnipgun (Liberation Army) operated along the Sino-Korean border, fighting guerrilla warfare against Japanese forces. Some of them took part in allied action in China and parts of South East Asia. One of the guerrilla leaders was the communist Kim Il-sung, who later became the first leader of North Korea.

After the Japanese surrender at the end of World War II in 1945, the Korean Peninsula was divided into two zones along the 38th parallel, with the northern half of the peninsula occupied by the Soviet Union and the southern half by the United States. Negotiations on reunification failed. Soviet general Terentii Shtykov recommended the establishment of the Soviet Civil Authority in October 1945, and supported Kim Il-sung as chairman of the Provisional People's Committee for North Korea, established in February 1946. In September 1946, South Korean citizens rose up against the Allied Military Government. In April 1948, an uprising of the Jeju islanders was violently crushed. The South declared its statehood in May 1948 and two months later the ardent anti-communist Syngman Rhee became its ruler. The Democratic People's Republic of Korea was established in the North on 9 September 1948. Shtykov served as the first Soviet ambassador, while Kim Il-sung became premier.

Soviet forces withdrew from the North in 1948, and most American forces withdrew from the South in 1949. Ambassador Shtykov suspected Rhee was planning to invade the North and was sympathetic to Kim's goal of Korean unification under socialism. The two successfully lobbied Joseph Stalin to support a quick war against the South, which culminated in the outbreak of the Korean War.

The military of North Korea invaded the South on 25 June 1950, and swiftly overran most of the country. A United Nations force, led by the United States, intervened to defend the South, and rapidly advanced into North Korea. As they neared the border with China, Chinese forces intervened on behalf of North Korea, shifting the balance of the war again. Fighting ended on 27 July 1953, with an armistice that approximately restored the original boundaries between North and South Korea, but no peace treaty was signed. Approximately 3 million people died in the Korean War, with a higher proportional civilian death toll than World War II or the Vietnam War, making it perhaps the deadliest conflict of the Cold War-era. In both per capita and absolute terms, North Korea was the country most devastated by the war, which resulted in the death of an estimated 12–15% of the North Korean population ( 10 million), "a figure close to or surpassing the proportion of Soviet citizens killed in World War II," according to Charles K. Armstrong. As a result of the war, almost every substantial building in North Korea was destroyed. Some have referred to the conflict as a civil war, with other factors involved.

A heavily guarded demilitarized zone (DMZ) still divides the peninsula, and an anti-communist and anti-North Korea sentiment remains in South Korea. Since the war, the United States has maintained a strong military presence in the South which is depicted by the North Korean government as an imperialist occupation force. It claims that the Korean War was caused by the United States and South Korea.

The relative peace between the South and the North following the armistice was interrupted by border skirmishes, celebrity abductions, and assassination attempts. The North failed in several assassination attempts on South Korean leaders, such as in 1968, 1974, and the Rangoon bombing in 1983; tunnels were found under the DMZ and tensions flared over the axe murder incident at Panmunjom in 1976. For almost two decades after the war, the two states did not seek to negotiate with one another. In 1971, secret, high-level contacts began to be conducted culminating in the 1972 July 4th North–South Joint Statement that established principles of working toward peaceful reunification. The talks ultimately failed because in 1973, South Korea declared its preference that the two Koreas should seek separate memberships in international organizations.

During the 1956 August Faction Incident, Kim Il-sung successfully resisted efforts by the Soviet Union and China to depose him in favor of Soviet Koreans or the pro-Chinese Yan'an faction. The last Chinese troops withdrew from the country in October 1958, which is the consensus as the latest date when North Korea became effectively independent. Some scholars believe that the 1956 August incident demonstrated independence. North Korea remained closely aligned with China and the Soviet Union, and the Sino-Soviet split allowed Kim to play the powers off each other. North Korea sought to become a leader of the Non-Aligned Movement, and emphasized the ideology of "Juche" to distinguish it from both the Soviet Union and China. In United States policymaking, North Korea was considered among the Captive Nations.
Recovery from the war was quick—by 1957 industrial production reached 1949 levels. In 1959, relations with Japan had improved somewhat, and North Korea began allowing the repatriation of Japanese citizens in the country. The same year, North Korea revalued the North Korean won, which held greater value than its South Korean counterpart. Until the 1960s, economic growth was higher than in South Korea, and North Korean GDP per capita was equal to that of its southern neighbor as late as 1976. However, by the 1980s, the economy had begun to stagnate; it started its long decline in 1987 and almost completely collapsed after the dissolution of the Soviet Union in 1991, when all Soviet aid was suddenly halted.

In 1992, as Kim Il-sung's health began deteriorating, Kim Jong-il slowly began taking over various state tasks. Kim Il-sung died of a heart attack in 1994, with Kim Jong-il declaring a three-year period of national mourning before officially announcing his position as the new leader afterwards.

North Korea promised to halt its development of nuclear weapons under the Agreed Framework, negotiated with U.S. president Bill Clinton and signed in 1994. Building on Nordpolitik, South Korea began to engage with the North as part of its Sunshine Policy.

Kim Jong-il instituted a policy called "Songun", or "military first". There is much speculation about this policy being used as a strategy to strengthen the military while discouraging coup attempts.

Flooding in the mid-1990s exacerbated the economic crisis, severely damaging crops and infrastructure and led to widespread famine which the government proved incapable of curtailing, resulting in the deaths of between 240,000 and 420,000 people. In 1996, the government accepted UN food aid.

The international environment changed with the election of U.S. president George W. Bush in 2001. His administration rejected South Korea's Sunshine Policy and the Agreed Framework. The U.S. government treated North Korea as a rogue state, while North Korea redoubled its efforts to acquire nuclear weapons to avoid the fate of Iraq. On 9 October 2006, North Korea announced it had conducted its first nuclear weapons test.

U.S. President Barack Obama adopted a policy of "strategic patience", resisting making deals with North Korea. Tensions with South Korea and the United States increased in 2010 with the sinking of the South Korean warship "Cheonan" and North Korea's shelling of Yeonpyeong Island.

On 17 December 2011, Kim Jong-il died from a heart attack. His youngest son Kim Jong-un was announced as his successor. In the face of international condemnation, North Korea continued to develop its nuclear arsenal, possibly including a hydrogen bomb and a missile capable of reaching the United States.

Throughout 2017, following Donald Trump's assumption of the US presidency, tensions between the United States and North Korea increased, and there was heightened rhetoric between the two, with Trump threatening "fire and fury" and North Korea threatening to test missiles that would land near Guam. The tensions substantially decreased in 2018, and a détente developed. A series of summits took place between Kim Jong-un of North Korea, President Moon Jae-in of South Korea, and President Trump.
It has been since North Korea's last ICBM test.

North Korea occupies the northern portion of the Korean Peninsula, lying between latitudes 37° and 43°N, and longitudes 124° and 131°E. It covers an area of . North Korea is bordered by China and by Russia along the Amnok (known as the Yalu in Chinese) and Tumen rivers and borders South Korea along the Korean Demilitarized Zone. To its west are the Yellow Sea and Korea Bay, and to its east lies Japan across the Sea of Japan (East Sea of Korea).
Early European visitors to Korea remarked that the country resembled "a sea in a heavy gale" because of the many successive mountain ranges that crisscross the peninsula. Some 80 percent of North Korea is composed of mountains and uplands, separated by deep and narrow valleys. All of the Korean Peninsula's mountains with elevations of or more are located in North Korea. The highest point in North Korea is Paektu Mountain, a volcanic mountain with an elevation of above sea level. Considered a sacred place by North Koreans, Mount Paektu holds significance in Korean culture and has been incorporated in the elaborate folklore and cult personality around the Kim dynasty. For example, the song, "We Will Go To Mount Paektu" sings in praise of Kim Jong-un and describes a symbolic trek to the mountain. Other prominent ranges are the Hamgyong Range in the extreme northeast and the Rangrim Mountains, which are located in the north-central part of North Korea. Mount Kumgang in the Taebaek Range, which extends into South Korea, is famous for its scenic beauty.

The coastal plains are wide in the west and discontinuous in the east. A great majority of the population lives in the plains and lowlands. According to a United Nations Environmental Programme report in 2003, forest covers over 70 percent of the country, mostly on steep slopes. The longest river is the Amnok (Yalu) River which flows for .

North Korea experiences a combination of continental climate and an oceanic climate, but most of the country experiences a humid continental climate within the Köppen climate classification scheme. Winters bring clear weather interspersed with snow storms as a result of northern and northwestern winds that blow from Siberia. Summer tends to be by far the hottest, most humid, and rainiest time of year because of the southern and southeastern monsoon winds that carry moist air from the Pacific Ocean. Approximately 60 percent of all precipitation occurs from June to September. Spring and autumn are transitional seasons between summer and winter. The daily average high and low temperatures for Pyongyang are in January and in August.

North Korea functions as a highly centralized, one-party state. According to its 2016 constitution, it is a self-described revolutionary and socialist state "guided in its activities by the Juche idea and the Songun idea". In addition to the constitution, North Korea is governed by the Ten Principles for the Establishment of a Monolithic Ideological System (also known as the "Ten Principles of the One-Ideology System") which establishes standards for governance and a guide for the behaviors of North Koreans. The Workers' Party of Korea (WPK), led by a member of the Kim dynasty, has an estimated 3,000,000 members and dominates every aspect of North Korean politics. It has two satellite organizations, the Korean Social Democratic Party and the Chondoist Chongu Party which participate in the WPK-led Democratic Front for the Reunification of the Fatherland of which all political officers are required to be members.

Kim Jong-un of the Kim dynasty is the current Supreme Leader or "Suryeong" of North Korea. He heads all major governing structures: he is Chairman of the Workers' Party of Korea, Chairman of the State Affairs Commission of North Korea, and Supreme Commander of the Korean People's Army. His grandfather Kim Il-sung, the founder and leader of North Korea until his death in 1994, is the country's "eternal President", while his father Kim Jong-il who succeeded Kim Il-sung as the leader was announced "Eternal General Secretary" and "Eternal Chairman of the National Defence Commission" after his death in 2011.

According to the Constitution of North Korea, there are officially three main branches of government. The first of these is the State Affairs Commission of North Korea, which acts as "the supreme national guidance organ of state sovereignty". Its role is to deliberate and decide the work on defense building of the State, including major policies of the State; and to carry out the directions of the Chairman of the commission, Kim Jong-Un.

Legislative power is held by the unicameral Supreme People's Assembly (SPA). Its 687 members are elected every five years by universal suffrage, though they have been described by outside observers as sham elections. Supreme People's Assembly sessions are convened by the SPA Presidium, whose president (Choe Ryong-hae since 2019) represents the state in relations with foreign countries. Deputies formally elect the President, the vice-presidents and members of the Presidium and take part in the constitutionally appointed activities of the legislature: pass laws, establish domestic and foreign policies, appoint members of the cabinet, review and approve the state economic plan, among others. The SPA itself cannot initiate any legislation independently of party or state organs. It is unknown whether it has ever criticized or amended bills placed before it, and the elections are based around a single list of WPK-approved candidates who stand without opposition.

Executive power is vested in the Cabinet of North Korea, which has been headed by Premier Kim Dok-hun since 14 August 2020. The Premier represents the government and functions independently. His authority extends over two vice-premiers, 30 ministers, two cabinet commission chairmen, the cabinet chief secretary, the president of the Central Bank, the director of the Central Bureau of Statistics and the president of the Academy of Sciences. A 31st ministry, the Ministry of People's Armed Forces, is under the jurisdiction of the State Affairs Commission.

North Korea, like its southern counterpart, claims to be the legitimate government of the entire Korean peninsula and adjacent islands. Despite its official title as the "Democratic People's Republic of Korea", some observers have described North Korea's political system as an absolute monarchy or a "hereditary dictatorship". It has also been described as a Stalinist dictatorship.

The "Juche" ideology is the cornerstone of party works and government operations. It is viewed by the official North Korean line as an embodiment of Kim Il-sung's wisdom, an expression of his leadership, and an idea which provides "a complete answer to any question that arises in the struggle for national liberation". "Juche" was pronounced in December 1955 in order to emphasize a Korea-centered revolution. Its core tenets are economic self-sufficiency, military self-reliance and an independent foreign policy. The roots of "Juche" were made up of a complex mixture of factors, including the cult of personality centered on Kim Il-sung, the conflict with pro-Soviet and pro-Chinese dissenters, and Korea's centuries-long struggle for independence. "Juche" was introduced into the constitution in 1972.

"Juche" was initially promoted as a "creative application" of Marxism–Leninism, but in the mid-1970s, it was described by state propaganda as "the only scientific thought... and most effective revolutionary theoretical structure that leads to the future of communist society". "Juche" eventually replaced Marxism–Leninism entirely by the 1980s, and in 1992 references to the latter were omitted from the constitution. The 2009 constitution dropped references to communism and elevated the "Songun" military-first policy while explicitly confirming the position of Kim Jong-il. However, the constitution retains references to socialism. "Juche"s concepts of self-reliance have evolved with time and circumstances, but still provide the groundwork for the spartan austerity, sacrifice and discipline demanded by the party. Scholar Brian Reynolds Myers views North Korea's actual ideology as a Korean ethnic nationalism similar to statism in Shōwa Japan and European fascism.

North Korea is ruled by the Kim dynasty, which in North Korea is referred to as the "Mount Paektu Bloodline". It is a three-generation lineage descending from the country's first leader, Kim Il-sung, since 1948. Kim developed a cult of personality closely tied to the state philosophy of "Juche", which was later passed on to his successors: his son Kim Jong-il and grandson Kim Jong-un. In 2013, this lineage was made explicit when Clause 2 of Article 10 of the new edited Ten Fundamental Principles of the Korean Workers' Party stated that the party and revolution must be carried "eternally" by the "Baekdu bloodline". In order to solidify "Mount Paektu Bloodline", Kim Il-sung and Kim Jong-il have recalled all the family genealogy books under the pretext that familyism and regionalism are the hotbeds of the revolution. In 1958, North Korea declared its ideology to be socialism and took away all of people's private property and dismantled family groups that had been living in the center of genealogy and ancestors. They later moved the entire population from the northern 38th parallel. Hence, in North Korea there is no bon-gwan in people's names.

According to "New Focus International", the cult of personality, particularly surrounding Kim Il-sung, has been crucial for legitimizing the family's hereditary succession. The control the North Korean government exercises over many aspects of the nation's culture is used to perpetuate the cult of personality surrounding Kim Il-sung, and Kim Jong-il. While visiting North Korea in 1979, journalist Bradley Martin wrote that nearly all music, art, and sculpture that he observed glorified "Great Leader" Kim Il-sung, whose personality cult was then being extended to his son, "Dear Leader" Kim Jong-il.

Claims that the dynasty has been deified are contested by North Korea researcher B. R. Myers: "Divine powers have never been attributed to either of the two Kims. In fact, the propaganda apparatus in Pyongyang has generally been careful "not" to make claims that run directly counter to citizens' experience or common sense." He further explains that the state propaganda painted Kim Jong-il as someone whose expertise lay in military matters and that the famine of the 1990s was partially caused by natural disasters out of Kim Jong-il's control.
The song "No Motherland Without You", sung by the North Korean army choir, was created especially for Kim Jong-il and is one of the most popular tunes in the country. Kim Il-sung is still officially revered as the nation's "Eternal President". Several landmarks in North Korea are named for Kim Il-sung, including Kim Il-sung University, Kim Il-sung Stadium, and Kim Il-sung Square. Defectors have been quoted as saying that North Korean schools deify both father and son. Kim Il-sung rejected the notion that he had created a cult around himself, and accused those who suggested this of "factionalism". Following the death of Kim Il-sung, North Koreans were prostrating and weeping to a bronze statue of him in an organized event; similar scenes were broadcast by state television following the death of Kim Jong-il.

Critics maintain that Kim Jong-il's personality cult was inherited from his father. Kim Jong-il was often the center of attention throughout ordinary life. His birthday is one of the most important public holidays in the country. On his 60th birthday (based on his official date of birth), mass celebrations occurred throughout the country. Kim Jong-il's personality cult, although significant, was not as extensive as his father's. One point of view is that Kim Jong-il's cult of personality was solely out of respect for Kim Il-sung or out of fear of punishment for failure to pay homage, while North Korean government sources consider it genuine hero worship.

The extent of the cult of personality surrounding Kim Jong-il and Kim Il-sung was illustrated on 11 June 2012 when a 14-year-old North Korean schoolgirl drowned while attempting to rescue portraits of the two from a flood.

As a result of its isolation, North Korea is sometimes known as the "hermit kingdom", a term that originally referred to the isolationism in the latter part of the Joseon Dynasty. Initially, North Korea had diplomatic ties only with other communist countries, and even today, most of the foreign embassies accredited to North Korea are located in Beijing rather than in Pyongyang. In the 1960s and 1970s, it pursued an independent foreign policy, established relations with many developing countries, and joined the Non-Aligned Movement. In the late 1980s and the 1990s its foreign policy was thrown into turmoil with the collapse of the Soviet bloc. Suffering an economic crisis, it closed a number of its embassies. At the same time, North Korea sought to build relations with developed free market countries.

North Korea joined the United Nations in 1991 together with South Korea. North Korea is also a member of the Non-Aligned Movement, G77 and the ASEAN Regional Forum.

North Korea enjoys a close relationship with China which is often called North Korea's closest ally. The relations were strained in the last few years because of China's concerns about North Korea's nuclear program. However, the relations have started to improve again and been increasingly close especially after Xi Jinping, General Secretary of the Communist Party of China visited North Korea in April 2019.

, North Korea had diplomatic relations with 166 countries and embassies in 47 countries. However, owing to the human rights and political situation, North Korea does not have diplomatic relations with Argentina, Botswana, Estonia, France, Iraq, Israel, Japan, Taiwan, and the United States. As of September 2017, France and Estonia are the last two European countries that do not have an official relationship with North Korea. North Korea continues to have strong ties with its socialist southeast Asian allies in Vietnam and Laos, as well as with Cambodia.
North Korea was previously designated a state sponsor of terrorism because of its alleged involvement in the 1983 Rangoon bombing and the 1987 bombing of a South Korean airliner. On 11 October 2008, the United States removed North Korea from its list of states that sponsor terrorism after Pyongyang agreed to cooperate on issues related to its nuclear program. North Korea was re-designated a state sponsor of terrorism by the U.S. under the Trump administration on 20 November 2017. The kidnapping of at least 13 Japanese citizens by North Korean agents in the 1970s and the 1980s has affected North Korea's relationship with Japan.

US President Donald Trump met with Kim in Singapore on 12 June 2018. An agreement was signed between the two countries endorsing the 2017 Panmunjom Declaration signed by North and South Korea, pledging to work towards denuclearizing the Korean Peninsula. They met in Hanoi from 27 to 28 February 2019, but failed to achieve an agreement. On 30 June 2019, Trump met with Kim along with Moon Jae-in at the Korean DMZ.

The Korean Demilitarized Zone with South Korea remains the most heavily fortified border in the world. Inter-Korean relations are at the core of North Korean diplomacy and have seen numerous shifts in the last few decades. North Korea's policy is to seek reunification without what it sees as outside interference, through a federal structure retaining each side's leadership and systems. In 1972, the two Koreas agreed in principle to achieve reunification through peaceful means and without foreign interference. On 10 October 1980, then North Korean president Kim Il-sung proposed a federation between North and South Korea named the Democratic Federal Republic of Korea in which the respective political systems would initially remain. However, relations remained cool well until the early 1990s, with a brief period in the early 1980s when North Korea offered to provide flood relief to its southern neighbor. Although the offer was initially welcomed, talks over how to deliver the relief goods broke down and none of the promised aid ever crossed the border.
The two countries also organized a reunion of 92 separated families.
The Sunshine Policy instituted by South Korean president Kim Dae-jung in 1998 was a watershed in inter-Korean relations. It encouraged other countries to engage with the North, which allowed Pyongyang to normalize relations with a number of European Union states and contributed to the establishment of joint North-South economic projects. The culmination of the Sunshine Policy was the 2000 Inter-Korean summit, when Kim Dae-jung visited Kim Jong-il in Pyongyang. Both North and South Korea signed the June 15th North–South Joint Declaration, in which both sides promised to seek peaceful reunification. On 4 October 2007, South Korean president Roh Moo-hyun and Kim Jong-il signed an eight-point peace agreement.
However, relations worsened when South Korean president Lee Myung-bak adopted a more hard-line approach and suspended aid deliveries pending the de-nuclearization of the North. In 2009, North Korea responded by ending all of its previous agreements with the South. It deployed additional ballistic missiles and placed its military on full combat alert after South Korea, Japan and the United States threatened to intercept a Unha-2 space launch vehicle. The next few years witnessed a string of hostilities, including the alleged North Korean involvement in the sinking of South Korean warship "Cheonan", mutual ending of diplomatic ties, a North Korean artillery attack on Yeonpyeong Island, and growing international concern over North Korea's nuclear program.

In May 2017, Moon Jae-in was elected President of South Korea with a promise to return to the Sunshine Policy. In February 2018, a détente developed at the Winter Olympics held in South Korea. In April, South Korean President Moon Jae-in and Kim Jong-un met at the DMZ, and, in the Panmunjom Declaration, pledged to work for peace and nuclear disarmament. In September, at a joint news conference in Pyongyang, Moon and Kim agreed upon turning the Korean Peninsula into a "land of peace without nuclear weapons and nuclear threats".

North Korea is widely accused of having perhaps the worst human rights record in the world. A 2014 UN inquiry into human rights in North Korea concluded that, "The gravity, scale and nature of these violations reveal a state that does not have any parallel in the contemporary world". North Koreans have been referred to as "some of the world's most brutalized people" by Human Rights Watch, because of the severe restrictions placed on their political and economic freedoms. The North Korean population is strictly managed by the state and all aspects of daily life are subordinated to party and state planning. Employment is managed by the party on the basis of political reliability, and travel is tightly controlled by the Ministry of People's Security.

Amnesty International reports of severe restrictions on the freedom of association, expression and movement, arbitrary detention, torture and other ill-treatment resulting in death, and executions.

The State Security Department extrajudicially apprehends and imprisons those accused of political crimes without due process. People perceived as hostile to the government, such as Christians or critics of the leadership, are deported to labor camps without trial, often with their whole family and mostly without any chance of being released.

Based on satellite images and defector testimonies, Amnesty International estimates that around 200,000 prisoners are held in six large political prison camps, where they are forced to work in conditions approaching slavery. Supporters of the government who deviate from the government line are subject to reeducation in sections of labor camps set aside for that purpose. Those who are deemed politically rehabilitated may reassume responsible government positions on their release.

North Korean defectors have provided detailed testimonies on the existence of the total control zones where abuses such as torture, starvation, rape, murder, medical experimentation, forced labor, and forced abortions have been reported. On the basis of these abuses, as well as persecution on political, religious, racial and gender grounds, forcible transfer of populations, enforced disappearance of persons and forced starvation, the United Nations Commission of Inquiry has accused North Korea of crimes against humanity. The International Coalition to Stop Crimes Against Humanity in North Korea (ICNK) estimates that over 10,000 people die in North Korean prison camps every year.

According to Human Rights Watch, which cites interviews with defectors, North Korean women are routinely subjected to sexual violence, unwanted sexual contact, and rape. Men in positions of power, including police, high-ranking officials, market supervisors, and guards can abuse women at will and are not prosecuted for it. It happens so often that it is accepted as a routine part of life. Women assume they can't do anything about it. The only ones with protection are those whose husbands or fathers are themselves in positions of power.

The North Korean government rejects the human rights abuse claims, calling them "a smear campaign" and a "human rights racket" aimed at government change. In a 2014 report to the UN, North Korea dismissed accusations of atrocities as "wild rumors". The official state media, KCNA, responded with an article that included homophobic insults against the author of the human rights report, Michael Kirby, calling him "a disgusting old lecher with a 40-odd-year-long career of homosexuality ... This practice can never be found in the DPRK boasting of the sound mentality and good morals ... In fact, it is ridiculous for such gay to sponsor dealing with others' human rights issue." The government, however, admitted some human rights issues related to living conditions and stated that it is working to improve them.

According to Amnesty International, citizens in North Korea are denied freedom of movement including the right to leave the country at will and its government denies access to international human rights observers.

North Korea has a civil law system based on the Prussian model and influenced by Japanese traditions and communist legal theory. Judiciary procedures are handled by the Supreme Court (the highest court of appeal), provincial or special city-level courts, people's courts and special courts. People's courts are at the lowest level of the system and operate in cities, counties and urban districts, while different kinds of special courts handle cases related to military, railroad or maritime matters.

Judges are theoretically elected by their respective local people's assemblies, but in practice they are appointed by the Workers' Party of Korea. The penal code is based on the principle of "nullum crimen sine lege" (no crime without a law), but remains a tool for political control despite several amendments reducing ideological influence. Courts carry out legal procedures related to not only criminal and civil matters, but also political cases as well. Political prisoners are sent to labor camps, while criminal offenders are incarcerated in a separate system.

The Ministry of People's Security (MPS) maintains most law enforcement activities. It is one of the most powerful state institutions in North Korea and oversees the national police force, investigates criminal cases and manages non-political correctional facilities. It handles other aspects of domestic security like civil registration, traffic control, fire departments and railroad security. The State Security Department was separated from the MPS in 1973 to conduct domestic and foreign intelligence, counterintelligence and manage the political prison system. Political camps can be short-term reeducation zones or "kwalliso" (total control zones) for lifetime detention. Camp 15 in Yodok and Camp 18 in Bukchang have been described in detailed testimonies.

The security apparatus is very extensive, exerting strict control over residence, travel, employment, clothing, food and family life. Security forces employ mass surveillance. It is believed they tightly monitor cellular and digital communications.

The Korean People's Army (KPA) has 1,106,000 active and 8,389,000 reserve and paramilitary troops, making it the largest military institution in the world. With an active duty army of 1.21 million, consisting of of its population, the KPA is the fourth largest military force in the world after China, the United States and India. About 20 percent of men aged 17–54 serve in the regular armed forces, and approximately one in every 25 citizens is an enlisted soldier. The KPA has five branches: Ground Force, Navy, Air Force, Special Operations Force, and Rocket Force. Command of the Korean People's Army lies in both the Central Military Commission of the Workers' Party of Korea and the independent State Affairs Commission. The Ministry of People's Armed Forces is subordinated to the latter.

Of all KPA branches, the Ground Force is the largest. It has approximately one million personnel divided into 80 infantry divisions, 30 artillery brigades, 25 special warfare brigades, 20 mechanized brigades, 10 tank brigades and seven tank regiments. They are equipped with 3,700 tanks, 2,100 armored personnel carriers and infantry fighting vehicles, 17,900 artillery pieces, 11,000 anti-aircraft guns and some 10,000 MANPADS and anti-tank guided missiles. Other equipment includes 1,600 aircraft in the Air Force and 1,000 vessels in the Navy. North Korea has the largest special forces and the largest submarine fleet in the world.
North Korea possesses nuclear weapons, but the strength of its arsenal is uncertain. In January 2018, estimates of North Korea's nuclear arsenal ranged between 15 and 60 bombs, probably including hydrogen bombs. Delivery capabilities are provided by the Rocket Force, which has some 1,000 ballistic missiles with a range of up to .

According to a 2004 South Korean assessment, North Korea possesses a stockpile of chemical weapons estimated to amount to 2,500–5,000 tons, including nerve, blister, blood, and vomiting agents, as well as the ability to cultivate and produce biological weapons including anthrax, smallpox, and cholera. Because of its nuclear and missile tests, North Korea has been sanctioned under United Nations Security Council resolutions 1695 of July 2006, 1718 of October 2006, 1874 of June 2009, 2087 of January 2013, and 2397 in December 2017.

The military faces some issues limiting its conventional capabilities, including obsolete equipment, insufficient fuel supplies and a shortage of digital command and control assets due to other countries being banned from selling weapons to it by the UN sanctions. To compensate for these deficiencies, the KPA has deployed a wide range of asymmetric warfare technologies like anti-personnel blinding lasers, GPS jammers, midget submarines and human torpedoes, stealth paint, and cyberwarfare units. In 2015, North Korea was estimated as having 6,000 sophisticated computer security personnel. KPA units have allegedly attempted to jam South Korean military satellites.

Much of the equipment is engineered and produced by a domestic defense industry. Weapons are manufactured in roughly 1,800 underground defense industry plants scattered throughout the country, most of them located in Chagang Province. The defense industry is capable of producing a full range of individual and crew-served weapons, artillery, armored vehicles, tanks, missiles, helicopters, surface combatants, submarines, landing and infiltration craft, Yak-18 trainers and possibly co-production of jet aircraft. According to official North Korean media, military expenditures for 2010 amount to 15.8 percent of the state budget. The U.S. State Department has estimated that North Korea's military spending averaged 23% of its GDP from 2004 to 2014, the highest level in the world.

According to Military Watch Magazine's military strength list, DPR Korea has the sixth most powerful military, placing it in the "Tier 2" military powers group.

With the exception of a small Chinese community and a few ethnic Japanese, North Korea's people are ethnically homogeneous. Demographic experts in the 20th century estimated that the population would grow to 25.5 million by 2000 and 28 million by 2010, but this increase never occurred due to the North Korean famine. It began in 1995, lasted for three years and resulted in the deaths of between 240,000 and 420,000 North Koreans.

International donors led by the United States initiated shipments of food through the World Food Program in 1997 to combat the famine. Despite a drastic reduction of aid under the George W. Bush administration, the situation gradually improved: the number of malnourished children declined from 60% in 1998 to 37% in 2006 and 28% in 2013. Domestic food production almost recovered to the recommended annual level of 5.37 million tons of cereal equivalent in 2013, but the World Food Program reported a continuing lack of dietary diversity and access to fats and proteins.

The famine had a significant impact on the population growth rate, which declined to 0.9% annually in 2002. It was 0.5% in 2014. Late marriages after military service, limited housing space and long hours of work or political studies further exhaust the population and reduce growth. The national birth rate is 14.5 births per year per 1,000 population. Two-thirds of households consist of extended families mostly living in two-room units. Marriage is virtually universal and divorce is extremely rare.

North Korea had a life expectancy of 72.1 years in 2018, according to World Bank. While North Korea is classified as a low-income country, the structure of North Korea's causes of death (2013) is unlike that of other low-income countries. Instead, it is closer to worldwide averages, with non-communicable diseases—such as cardiovascular disease and cancers—accounting for 84 percent of the total deaths in 2016.

According to the World Bank report of 2016 (based on WHO's estimate), only 9.5% of the total deaths recorded in North Korea are attributed to communicable diseases and maternal, prenatal and nutrition conditions, a figure which is slightly lower than that of South Korea (10.1%) and one fifth of other low-income countries (50.1%) but higher than that of high income countries (6.7%). Only one out of ten leading causes of overall deaths in North Korea is attributed to communicable diseases (lower respiratory infection), a disease which is reported to have declined by six percent since 2007.

In 2013, cardiovascular disease as a single disease group was reported as the largest cause of death in North Korea. The three major causes of death in North Korea are stroke, COPD and Ischaemic heart disease. Non-communicable diseases risk factors in North Korea include high rates of urbanization, an aging society, and high rates of smoking and alcohol consumption amongst men.

Maternal mortality is lower than other low-income countries, but significantly higher than South Korea and other high income countries, at 89 per 100,000 live births.

Healthcare Access and Quality Index, calculated by IHME, was reported to stand at 62.3, much lower than that of South Korea.

According to a 2003 report by the United States Department of State, almost 100% of the population has access to water and sanitation. 80% of the population had access to improved sanitation facilities in 2015.

North Korea has the highest number of doctors per capita amongst low-income countries, with 3.7 physicians per 1,000 people, a figure which is also significantly higher than that of South Korea, according to WHO's data.

A free universal insurance system is in place. Quality of medical care varies significantly by region and is often low, with severe shortages of equipment, drugs and anesthetics. According to WHO, expenditure on health per capita is one of the lowest in the world. Preventive medicine is emphasized through physical exercise and sports, nationwide monthly checkups and routine spraying of public places against disease. Every individual has a lifetime health card which contains a full medical record.

The 2008 census listed the entire population as literate. An 11-year free, compulsory cycle of primary and secondary education is provided in more than 27,000 nursery schools, 14,000 kindergartens, 4,800 four-year primary and 4,700 six-year secondary schools. 77% of males and 79% of females aged 30–34 have finished secondary school. An additional 300 universities and colleges offer higher education.

Most graduates from the compulsory program do not attend university but begin their obligatory military service or proceed to work in farms or factories instead. The main deficiencies of higher education are the heavy presence of ideological subjects, which comprise 50% of courses in social studies and 20% in sciences, and the imbalances in curriculum. The study of natural sciences is greatly emphasized while social sciences are neglected. Heuristics is actively applied to develop the independence and creativity of students throughout the system. The study of Russian and English was made compulsory in upper middle schools in 1978.

North Korea shares the Korean language with South Korea, although some dialectal differences exist within both Koreas. North Koreans refer to their Pyongyang dialect as "munhwaŏ" ("cultured language") as opposed to the dialects of South Korea, especially the Seoul dialect or "p'yojun'ŏ" ("standard language"), which are viewed as decadent because of its use of loanwords from Chinese and European languages (particularly English). Words of Chinese, Manchu or Western origin have been eliminated from "munhwa" along with the usage of Chinese hancha characters. Written language uses only the chosŏn'gŭl (Hangul) phonetic alphabet, developed under Sejong the Great (1418–1450).

Officially, North Korea is an atheist state. There are no known official statistics of religions in North Korea. According to Religious Intelligence, 64% of the population are irreligious, 16% practice Korean shamanism, 14% practice Chondoism, 4% are Buddhist, and 2% are Christian. Freedom of religion and the right to religious ceremonies are constitutionally guaranteed, but religions are restricted by the government. Amnesty International has expressed concerns about religious persecution in North Korea.

The influence of Buddhism and Confucianism still has an effect on cultural life. Chondoism ("Heavenly Way") is an indigenous syncretic belief combining elements of Korean shamanism, Buddhism, Taoism and Catholicism that is officially represented by the WPK-controlled Chondoist Chongu Party.

The Open Doors mission, a Protestant-group based in the United States and founded during the Cold War-era, claims the most severe persecution of Christians in the world occurs in North Korea. Four state-sanctioned churches exist, but critics claim these are showcases for foreigners.

According to North Korean documents and refugee testimonies, all North Koreans are sorted into groups according to their Songbun, an ascribed status system based on a citizen's assessed loyalty to the government. Based on their own behavior and the political, social, and economic background of their family for three generations as well as behavior by relatives within that range, Songbun is allegedly used to determine whether an individual is trusted with responsibility, given opportunities, or even receives adequate food.

Songbun allegedly affects access to educational and employment opportunities and particularly whether a person is eligible to join North Korea's ruling party. There are 3 main classifications and about 50 sub-classifications. According to Kim Il-sung, speaking in 1958, the loyal "core class" constituted 25% of the North Korean population, the "wavering class" 55%, and the "hostile class" 20%. The highest status is accorded to individuals descended from those who participated with Kim Il-sung in the resistance against Japanese occupation during and before World War II and to those who were factory workers, laborers, or peasants in 1950.

While some analysts believe private commerce recently changed the Songbun system to some extent, most North Korean refugees say it remains a commanding presence in everyday life. The North Korean government claims all citizens are equal and denies any discrimination on the basis of family background.

North Korea has maintained one of the most closed and centralized economies in the world since the 1940s. For several decades, it followed the Soviet pattern of five-year plans with the ultimate goal of achieving self-sufficiency. Extensive Soviet and Chinese support allowed North Korea to rapidly recover from the Korean War and register very high growth rates. Systematic inefficiency began to arise around 1960, when the economy shifted from the extensive to the intensive development stage. The shortage of skilled labor, energy, arable land and transportation significantly impeded long-term growth and resulted in consistent failure to meet planning objectives. The major slowdown of the economy contrasted with South Korea, which surpassed the North in terms of absolute GDP and per capita income by the 1980s. North Korea declared the last seven-year plan unsuccessful in December 1993 and thereafter stopped announcing plans.

The loss of Eastern Bloc trading partners and a series of natural disasters throughout the 1990s caused severe hardships, including widespread famine. By 2000, the situation improved owing to a massive international food assistance effort, but the economy continues to suffer from food shortages, dilapidated infrastructure and a critically low energy supply. In an attempt to recover from the collapse, the government began structural reforms in 1998 that formally legalized private ownership of assets and decentralized control over production. A second round of reforms in 2002 led to an expansion of market activities, partial monetization, flexible prices and salaries, and the introduction of incentives and accountability techniques. Despite these changes, North Korea remains a command economy where the state owns almost all means of production and development priorities are defined by the government.

North Korea has the structural profile of a relatively industrialized country where nearly half of the Gross Domestic Product is generated by industry and human development is at medium levels. Purchasing power parity (PPP) GDP is estimated at $40 billion, with a very low per capita value of $1,800. In 2012, Gross national income per capita was $1,523, compared to $28,430 in South Korea. The North Korean won is the national currency, issued by the Central Bank of the Democratic People's Republic of Korea.

The economy is heavily nationalized. Food and housing are extensively subsidized by the state; education and healthcare are free; and the payment of taxes was officially abolished in 1974. A variety of goods are available in department stores and supermarkets in Pyongyang, though most of the population relies on small-scale "jangmadang" markets. In 2009, the government attempted to stem the expanding free market by banning jangmadang and the use of foreign currency, heavily devaluing the won and restricting the convertibility of savings in the old currency, but the resulting inflation spike and rare public protests caused a reversal of these policies. Private trade is dominated by women because most men are required to be present at their workplace, even though many state-owned enterprises are non-operational.

Industry and services employ 65% of North Korea's 12.6 million labor force. Major industries include machine building, military equipment, chemicals, mining, metallurgy, textiles, food processing and tourism. Iron ore and coal production are among the few sectors where North Korea performs significantly better than its southern neighbor—it produces about 10 times larger amounts of each resource. Using ex-Romanian drilling rigs, several oil exploration companies have confirmed significant oil reserves in the North Korean shelf of the Sea of Japan, and in areas south of Pyongyang. The agricultural sector was shattered by the natural disasters of the 1990s. Its 3,500 cooperatives and state farms were among the most productive and successful in the world around 1980 but now experience chronic fertilizer and equipment shortages. Rice, corn, soybeans and potatoes are some of the primary crops. A significant contribution to the food supply comes from commercial fishing and aquaculture. Tourism has been a growing sector for the past decade. North Korea has been aiming to increase the number of foreign visitors through projects like the Masikryong Ski Resort.

Foreign trade surpassed pre-crisis levels in 2005 and continues to expand. North Korea has a number of special economic zones (SEZs) and Special Administrative Regions where foreign companies can operate with tax and tariff incentives while North Korean establishments gain access to improved technology. Initially four such zones existed, but they yielded little overall success. The SEZ system was overhauled in 2013 when 14 new zones were opened and the Rason Special Economic Zone was reformed as a joint Chinese-North Korean project. The Kaesong Industrial Region is a special economic zone where more than 100 South Korean companies employ some 52,000 North Korean workers. , China is the biggest trading partner of North Korea outside inter-Korean trade, accounting for more than 84% of the total external trade ($5.3 billion) followed by India at 3.3% share ($205 million). In 2014, Russia wrote off 90% of North Korea's debt and the two countries agreed to conduct all transactions in rubles. Overall, external trade in 2013 reached a total of $7.3 billion (the highest amount since 1990), while inter-Korean trade dropped to an eight-year low of $1.1 billion.

North Korea's energy infrastructure is obsolete and in disrepair. Power shortages are chronic and would not be alleviated even by electricity imports because the poorly maintained grid causes significant losses during transmission. Coal accounts for 70% of primary energy production, followed by hydroelectric power with 17%. The government under Kim Jong-un has increased emphasis on renewable energy projects like wind farms, solar parks, solar heating and biomass. A set of legal regulations adopted in 2014 stressed the development of geothermal, wind and solar energy along with recycling and environmental conservation. North Korea's long-term objective is to curb fossil fuel usage and reach an output of 5 million kilowatts from renewable sources by 2044, up from its current total of 430,000 kilowatts from all sources. Wind power is projected to satisfy 15% of the country's total energy demand under this strategy.

North Korea also strives to develop its own civilian nuclear program. These efforts are under much international dispute due to their military applications and concerns about safety.

Transport infrastructure includes railways, highways, water and air routes, but rail transport is by far the most widespread. North Korea has some 5,200 kilometers of railways mostly in standard gauge which carry 80% of annual passenger traffic and 86% of freight, but electricity shortages undermine their efficiency. Construction of a high-speed railway connecting Kaesong, Pyongyang and Sinuiju with speeds exceeding 200 km/h was approved in 2013. North Korea connects with the Trans-Siberian Railway through Rajin.

Road transport is very limited—only 724 kilometers of the 25,554 kilometer road network are paved, and maintenance on most roads is poor. Only 2% of the freight capacity is supported by river and sea transport, and air traffic is negligible. All port facilities are ice-free and host a merchant fleet of 158 vessels. Eighty-two airports and 23 helipads are operational and the largest serve the state-run airline, Air Koryo. Cars are relatively rare, but bicycles are common.

R&D efforts are concentrated at the State Academy of Sciences, which runs 40 research institutes, 200 smaller research centers, a scientific equipment factory and six publishing houses. The government considers science and technology to be directly linked to economic development. A five-year scientific plan emphasizing IT, biotechnology, nanotechnology, marine and plasma research was carried out in the early 2000s. A 2010 report by the South Korean Science and Technology Policy Institute identified polymer chemistry, single carbon materials, nanoscience, mathematics, software, nuclear technology and rocketry as potential areas of inter-Korean scientific cooperation. North Korean institutes are strong in these fields of research, although their engineers require additional training and laboratories need equipment upgrades.

Under its "constructing a powerful knowledge economy" slogan, the state has launched a project to concentrate education, scientific research and production into a number of "high-tech development zones". International sanctions remain a significant obstacle to their development. The "Miraewon" network of electronic libraries was established in 2014 under similar slogans.

Significant resources have been allocated to the national space program, which is managed by the National Aerospace Development Administration (formerly managed by the Korean Committee of Space Technology until April 2013) Domestically produced launch vehicles and the Kwangmyŏngsŏng satellite class are launched from two spaceports, the Tonghae Satellite Launching Ground and the Sohae Satellite Launching Station. After four failed attempts, North Korea became the tenth spacefaring nation with the launch of Kwangmyŏngsŏng-3 Unit 2 in December 2012, which successfully reached orbit but was believed to be crippled and non-operational. It joined the Outer Space Treaty in 2009 and has stated its intentions to undertake manned and Moon missions. The government insists the space program is for peaceful purposes, but the United States, Japan, South Korea and other countries maintain that it serves to advance military ballistic missile programs.

On 7 February 2016, North Korea successfully launched a long-range rocket, supposedly to place a satellite into orbit. Critics believe that the real purpose of the launch was to test a ballistic missile. The launch was strongly condemned by the UN Security Council. A statement broadcast on Korean Central Television said that a new Earth observation satellite, Kwangmyongsong-4, had successfully been put into orbit less than 10 minutes after lift-off from the Sohae space center in North Phyongan province.

Usage of communication technology is controlled by the Ministry of Post and Telecommunications. An adequate nationwide fiber-optic telephone system with 1.18 million fixed lines and expanding mobile coverage is in place. Most phones are installed for senior government officials and installation requires written explanation why the user needs a telephone and how it will be paid for. Cellular coverage is available with a 3G network operated by Koryolink, a joint venture with Orascom Telecom Holding. The number of subscribers has increased from 3,000 in 2002 to almost two million in 2013. International calls through either fixed or cellular service are restricted, and mobile Internet is not available.

Internet access itself is limited to a handful of elite users and scientists. Instead, North Korea has a walled garden intranet system called Kwangmyong, which is maintained and monitored by the Korea Computer Center. Its content is limited to state media, chat services, message boards, an e-mail service and an estimated 1,000–5,500 websites. Computers employ the Red Star OS, an operating system derived from Linux, with a user shell visually similar to that of OS X. On 19 September 2016, a TLDR project noticed the North Korean Internet DNS data and top-level domain was left open which allowed global DNS zone transfers. A dump of the data discovered was shared on GitHub.

On 8 July 2020, the "CNN" reported that satellite imagery showed activity at a North Korean facility, which was suspected by researchers of being utilized for building nuclear warheads. The images were captured by Planet Labs and analyzed by experts at the Middlebury Institute of International Studies.

Despite a historically strong Chinese influence, Korean culture has shaped its own unique identity. It came under attack during the Japanese rule from 1910 to 1945, when Japan enforced a cultural assimilation policy. Koreans were forced to learn and speak Japanese, adopt the Japanese family name system and Shinto religion, and were forbidden to write or speak the Korean language in schools, businesses, or public places.

After the peninsula was divided in 1945, two distinct cultures formed out of the common Korean heritage. North Koreans have little exposure to foreign influence. The revolutionary struggle and the brilliance of the leadership are some of the main themes in art. "Reactionary" elements from traditional culture have been discarded and cultural forms with a "folk" spirit have been reintroduced.

Korean heritage is protected and maintained by the state. Over 190 historical sites and objects of national significance are cataloged as National Treasures of North Korea, while some 1,800 less valuable artifacts are included in a list of Cultural Assets. The Historic Sites and Monuments in Kaesong and the Complex of Goguryeo Tombs are UNESCO World Heritage Sites.

Visual arts are generally produced in the esthetics of Socialist realism. North Korean painting combines the influence of Soviet and Japanese visual expression to instill a sentimental loyalty to the system. All artists in North Korea are required to join the Artists' Union, and the best among them can receive an official license to portray the leaders. Portraits and sculptures depicting Kim Il-sung, Kim Jong-il and Kim Jong-un are classed as "Number One works".

Most aspects of art have been dominated by Mansudae Art Studio since its establishment in 1959. It employs around 1,000 artists in what is likely the biggest art factory in the world where paintings, murals, posters and monuments are designed and produced. The studio has commercialized its activity and sells its works to collectors in a variety of countries including China, where it is in high demand. Mansudae Overseas Projects is a subdivision of Mansudae Art Studio that carries out construction of large-scale monuments for international customers. Some of the projects include the African Renaissance Monument in Senegal, and the Heroes' Acre in Namibia.

In the Democratic People's Republic of Korea, the Goguryeo tumulus is registered on the World Heritage list of UNESCO. These remains were registered as the first World Heritage property of North Korea in the UNESCO World Heritage Committee (WHC) in July 2004. There are 63 burial mounds in the tomb group, with clear murals preserved. The burial customs of the Goguryeo culture have influenced Asian civilizations beyond Korea, including Japan.

The government emphasized optimistic folk-based tunes and revolutionary music throughout most of the 20th century. Ideological messages are conveyed through massive orchestral pieces like the "Five Great Revolutionary Operas" based on traditional Korean "ch'angguk". Revolutionary operas differ from their Western counterparts by adding traditional instruments to the orchestra and avoiding recitative segments. "Sea of Blood" is the most widely performed of the Five Great Operas: since its premiere in 1971, it has been played over 1,500 times, and its 2010 tour in China was a major success. Western classical music by Brahms, Tchaikovsky, Stravinsky and other composers is performed both by the State Symphony Orchestra and student orchestras.

Pop music appeared in the 1980s with the Pochonbo Electronic Ensemble and Wangjaesan Light Music Band. Improved relations with South Korea following the 2000 inter-Korean summit caused a decline in direct ideological messages in pop songs, but themes like comradeship, nostalgia and the construction of a powerful country remained. In 2014, the all-girl Moranbong Band was described as the most popular group in the country. North Koreans also listen to K-pop which spreads through illegal markets.

All publishing houses are owned by the government or the WPK because they are considered an important tool for propaganda and agitation. The Workers' Party of Korea Publishing House is the most authoritative among them and publishes all works of Kim Il-sung, ideological education materials and party policy documents. The availability of foreign literature is limited, examples being North Korean editions of Indian, German, Chinese and Russian fairy tales, "Tales from Shakespeare", some works of Bertolt Brecht and Erich Kästner, and the Harry Potter series.

Kim Il-sung's personal works are considered "classical masterpieces" while the ones created under his instruction are labeled "models of "Juche" literature". These include "The Fate of a Self-Defense Corps Man", "The Song of Korea" and "Immortal History", a series of historical novels depicting the suffering of Koreans under Japanese occupation. More than four million literary works were published between the 1980s and the early 2000s, but almost all of them belong to a narrow variety of political genres like "army-first revolutionary literature".

Science fiction is considered a secondary genre because it somewhat departs from the traditional standards of detailed descriptions and metaphors of the leader. The exotic settings of the stories give authors more freedom to depict cyberwarfare, violence, sexual abuse and crime, which are absent in other genres. Sci-fi works glorify technology and promote the Juche concept of anthropocentric existence through depictions of robotics, space exploration and immortality.

Government policies towards film are no different than those applied to other arts—motion pictures serve to fulfill the targets of "social education". Some of the most influential films are based on historic events ("An Jung-geun shoots Itō Hirobumi") or folk tales ("Hong Gildong"). Most movies have predictable propaganda story lines which make cinema an unpopular entertainment; viewers only see films that feature their favorite actors. Western productions are only available at private showings to high-ranking Party members, although the 1997 film "Titanic" is frequently shown to university students as an example of Western culture. Access to foreign media products is available through smuggled DVDs and television or radio broadcasts in border areas. Western films like "The Interview", "Titanic", and "Charlie's Angels" are just a few films that have been smuggled across the borders of North Korea, allowing for access to the North Korean citizens.

North Korean media are under some of the strictest government control in the world. The censorship in North Korea encompasses all the information produced by the media. Monitored heavily by government officials, the media is strictly used to reinforce ideals approved by the government. There is no freedom of press in North Korea as all the media is controlled and filtered through governmental censors. Freedom of the press in 2017 was 180th out of 180 countries in Reporters Without Borders' annual Press Freedom Index. According to Freedom House, all media outlets serve as government mouthpieces, all journalists are party members and listening to foreign broadcasts carries the threat of a death penalty. The main news provider is the Korean Central News Agency. All 12 major newspapers and 20 periodicals, including "Rodong Sinmun", are published in the capital.

There are three state-owned TV stations. Two of them broadcast only on weekends and the Korean Central Television is on air every day in the evenings. Uriminzokkiri and its associated YouTube and Twitter accounts distribute imagery, news and video issued by government media. The Associated Press opened the first Western all-format, full-time bureau in Pyongyang in 2012.

Media coverage of North Korea has often been inadequate as a result of the country's isolation. Stories like Kim Jong-un undergoing surgery to look like his grandfather, executing his ex-girlfriend or feeding his uncle to a pack of hungry dogs have been circulated by foreign media as truth despite the lack of a credible source. Many of the claims originate from the South Korean right-wing newspaper "The Chosun Ilbo". Max Fisher of "The Washington Post" has written that "almost any story [on North Korea] is treated as broadly credible, no matter how outlandish or thinly sourced". Occasional deliberate disinformation on the part of North Korean establishments further complicates the issue.

Korean cuisine has evolved through centuries of social and political change. Originating from ancient agricultural and nomadic traditions in southern Manchuria and the Korean Peninsula, it has gone through a complex interaction of the natural environment and different cultural trends. Rice dishes and kimchi are staple Korean food. In a traditional meal, they accompany both side dishes ("panch'an") and main courses like "juk", "pulgogi" or noodles. "Soju" liquor is the best-known traditional Korean spirit.

North Korea's most famous restaurant, Okryu-gwan, located in Pyongyang, is known for its "raengmyeon" cold noodles. Other dishes served there include gray mullet soup with boiled rice, beef rib soup, green bean pancake, "sinsollo" and dishes made from terrapin. Okryu-gwan sends research teams into the countryside to collect data on Korean cuisine and introduce new recipes. Some Asian cities host branches of the Pyongyang restaurant chain where waitresses perform music and dance.

Most schools have daily practice in association football, basketball, table tennis, gymnastics, boxing and others. The DPR Korea League is popular inside the country and its games are often televised. The national football team, "Chollima", competed in the FIFA World Cup in 2010, when it lost all three matches against Brazil, Portugal and Ivory Coast. Its 1966 appearance was much more successful, seeing a surprise 1–0 victory over Italy and a quarter final loss to Portugal by 3–5. A national team represents the nation in international basketball competitions as well. In December 2013, former American basketball professional Dennis Rodman visited North Korea to help train the national team after he developed a friendship with Kim Jong-un.

North Korea's first appearance in the Olympics came in 1964. The 1972 Olympics saw its summer games debut and five medals, including one gold. With the exception of the boycotted Los Angeles and Seoul Olympics, North Korean athletes have won medals in all summer games since then. Weightlifter Kim Un-guk broke the world record of the Men's 62 kg category at the 2012 Summer Olympics in London. Successful Olympians receive luxury apartments from the state in recognition for their achievements.

The Arirang Festival has been recognized by the Guinness World Records as the biggest choreographic event in the world. Some 100,000 athletes perform rhythmic gymnastics and dances while another 40,000 participants create a vast animated screen in the background. The event is an artistic representation of the country's history and pays homage to Kim Il-sung and Kim Jong-il. Rungrado 1st of May Stadium, the largest stadium in the world with its capacity of 150,000, hosts the Festival. The Pyongyang Marathon is another notable sports event. It is an IAAF Bronze Label Race where amateur runners from around the world can participate.

Between 2010 and 2019, North Korea has imported 138 purebred horses from Russia at cost of over $584,000.





</doc>
<doc id="21256" url="https://en.wikipedia.org/wiki?curid=21256" title="History of North Korea">
History of North Korea

The history of North Korea began at the end of World War II in 1945. The surrender of Japan led to the division of Korea at the 38th parallel, with the Soviet Union occupying the north, and the United States occupying the south. The Soviet Union and the United States failed to agree on a way to unify the country, and in 1948 they established two separate governments – the Soviet-aligned Democratic People's Republic of Korea and the Western-aligned Republic of Korea – each claiming to be the legitimate government of all of Korea. 

In 1950 the Korean War broke out. After much destruction, the war ended with a stalemate. The division at the 38th parallel was replaced by the Korean Demilitarized Zone. Tension between the two sides continued. Out of the rubble North Korea built an industrialized command economy.

Kim Il-sung led North Korea until his death in 1994. He developed a pervasive personality cult and steered the country on an independent course in accordance with the principle of "Juche" (self-reliance). However, with natural disasters and the collapse of the Soviet Bloc in 1991, North Korea went into a severe economic crisis. Kim Il-sung's son, Kim Jong-il, succeeded him, and was in turn succeeded by his son, Kim Jong-un. Amid international alarm, North Korea developed nuclear missiles. In 2018, Kim Jong-un made a sudden peace overture towards South Korea and the United States.

From 1910 to the end of World War II in 1945, Korea was under Japanese rule. Most Koreans were peasants engaged in subsistence farming. In the 1930s, Japan developed mines, hydro-electric dams, steel mills, and manufacturing plants in northern Korea and neighboring Manchuria. The Korean industrial working class expanded rapidly, and many Koreans went to work in Manchuria. As a result, 65% of Korea's heavy industry was located in the north, but, due to the rugged terrain, only 37% of its agriculture.

A Korean guerrilla movement emerged in the mountainous interior and in Manchuria, harassing the Japanese imperial authorities. One of the most prominent guerrilla leaders was the Communist Kim Il-sung.

Northern Korea had little exposure to modern, Western ideas. One partial exception was the penetration of religion. Since the arrival of missionaries in the late nineteenth century, the northwest of Korea, and Pyongyang in particular, had been a stronghold of Christianity. As a result, Pyongyang was called the "Jerusalem of the East".

At the Tehran Conference in November 1943 and the Yalta Conference in February 1945, the Soviet Union promised to join its allies in the Pacific War within three months of victory in Europe. On August 8, 1945, after three months to the day, the Soviet Union declared war on Japan. Soviet troops advanced rapidly, and the US government became anxious that they would occupy the whole of Korea. On August 10, the US government decided to propose the 38th parallel as the dividing line between a Soviet occupation zone in the north and a US occupation zone in the south. The parallel was chosen as it would place the capital Seoul under American control. To the surprise of the Americans, the Soviet Union immediately accepted the division. The agreement was incorporated into General Order No. 1 (approved on 17 August 1945) for the surrender of Japan. The division placed sixteen million Koreans in the American zone and nine million in the Soviet zone. 

Soviet forces began amphibious landings in Korea by August 14 and rapidly took over the northeast, and on August 16 they landed at Wonsan. On August 24, the Red Army reached Pyongyang. US forces did not arrive in the south until September 8.

During August, People's Committees sprang up across Korea, affiliated with the Committee for the Preparation of Korean Independence, which in September founded the People's Republic of Korea. When Soviet troops entered Pyongyang, they found a local People's Committee established there, led by veteran Christian nationalist Cho Man-sik. Unlike their American counterparts, the Soviet authorities recognized and worked with the People's Committees. By some accounts, Cho Man-sik was the Soviet government's first choice to lead North Korea.

On September 19, Kim Il-sung and 66 other Korean Red Army officers arrived in Wonsan. They had fought the Japanese in Manchuria in the 1930s but had lived in the USSR and trained in the Red Army since 1941. On October 14, Soviet authorities introduced Kim to the North Korean public as a guerrilla hero.

In December 1945, at the Moscow Conference, the Soviet Union agreed to a US proposal for a trusteeship over Korea for up to five years in the lead-up to independence. Most Koreans demanded independence immediately, but Kim and the other Communists supported the trusteeship under pressure from the Soviet government. Cho Man-sik opposed the proposal at a public meeting on January 4, 1946, and disappeared into house arrest. On February 8, 1946, the People's Committees were reorganized as Interim People's Committees dominated by Communists. The new regime instituted popular policies of land redistribution, industry nationalization, labor law reform, and equality for women.

Meanwhile, existing Communist groups were reconstituted as a party under Kim Il-sung's leadership. On December 18, 1945, local Communist Party committees were combined into the North Korean Communist Party. In August 1946, this party merged with the New People's Party to form the Workers' Party of North Korea. In December, a popular front led by the Workers' Party dominated elections in the North. In 1949, the Workers' Party of North Korea merged with its southern counterpart to become the Workers' Party of Korea with Kim as party chairman.
Kim established the Korean People's Army (KPA) aligned with the Communists, formed from a cadre of guerrillas and former soldiers who had gained combat experience in battles against the Japanese and later Nationalist Chinese troops. From their ranks, using Soviet advisers and equipment, Kim constructed a large army skilled in infiltration tactics and guerrilla warfare. Before the outbreak of the Korean War, Joseph Stalin equipped the KPA with modern medium tanks, trucks, artillery, and small arms. Kim also formed an air force, equipped at first with ex-Soviet propeller-driven fighter and attack aircraft. Later, North Korean pilot candidates were sent to the Soviet Union and China to train in MiG-15 jet aircraft at secret bases.

In 1946, a sweeping series of laws transformed North Korea on Soviet-style Communist lines. The "land to the tiller" reform redistributed the bulk of agricultural land to the poor and landless peasant population, effectively breaking the power of the landed class. This was followed by a "Labor Law", a "Sexual Equality Law", and a "Nationalisation of Industry, Transport, Communications and Banks Law".

As negotiations with the Soviet Union on the future of Korea failed to make progress, the US took the issue to the United Nations in September 1947. In response, the UN established the United Nations Temporary Commission on Korea to hold elections in Korea. The Soviet Union opposed this move. In the absence of Soviet cooperation, it was decided to hold UN-supervised elections in the south only. In April 1948, a conference of organizations from the North and the South met in Pyongyang, but the conference produced no results. The southern politicians Kim Koo and Kim Kyu-sik attended the conference and boycotted the elections in the South. Both men were posthumously awarded the National Reunification Prize by North Korea. The elections were held in South Korea on May 10, 1948. On August 15, the Republic of Korea formally came into existence. A parallel process occurred in North Korea. A new Supreme People's Assembly was elected in August 1948, and on September 3 a new constitution was promulgated. The Democratic People's Republic of Korea (DPRK) was proclaimed on September 9, with Kim as Premier. On December 12, 1948, the United Nations General Assembly accepted the report of UNTCOK and declared the Republic of Korea to be the "only lawful government in Korea".

By 1949, North Korea was a full-fledged Communist state. All parties and mass organizations joined the Democratic Front for the Reunification of the Fatherland, ostensibly a popular front but in reality dominated by the Communists. The government moved rapidly to establish a political system that was partly styled on the Soviet system, with political power monopolised by the Workers' Party of Korea (WPK).

The consolidation of Syngman Rhee's government in the South with American military support and the suppression of the October 1948 insurrection ended North Korean hopes that a revolution in the South could reunify Korea, and from early 1949 Kim Il-sung sought Soviet and Chinese support for a military campaign to reunify the country by force. The withdrawal of most U.S. forces from South Korea in June 1949 left the southern government defended only by a weak and inexperienced South Korean army. The southern régime also had to deal with a citizenry of uncertain loyalty. The North Korean army, by contrast, had benefited from the Soviet Union's WWII-era equipment, and had a core of hardened veterans who had fought either as anti-Japanese guerrillas or alongside the Chinese Communists. In 1949 and 1950 Kim traveled to Moscow with the South Korean Communist leader Pak Hon-yong to raise support for a war of reunification.

Initially Joseph Stalin rejected Kim Il-sung's requests for permission to invade the South, but in late 1949 the Communist victory in China and the development of Soviet nuclear weapons made him re-consider Kim's proposal. In January 1950, after China's Mao Zedong indicated that the People's Republic of China would send troops and other support to Kim, Stalin approved an invasion. The Soviets provided limited support in the form of advisers who helped the North Koreans as they planned the operation, and Soviet military instructors to train some of the Korean units. However, from the very beginning Stalin made it clear that the Soviet Union would avoid a direct confrontation with the U.S. over Korea and would not commit ground forces even in case of major military crisis. The stage was set for a civil war between the two rival régimes on the Korean peninsula.

For over a year before the outbreak of war, the two sides had engaged in a series of bloody clashes along the 38th parallel, especially in the Ongjin area on the west coast. On June 25, 1950, claiming to be responding to a South Korean assault on Ongjin, the Northern forces launched an amphibious offensive all along the parallel. Due to a combination of surprise and military superiority, the Northern forces quickly captured the capital Seoul, forcing Syngman Rhee and his government to flee. By mid-July North Korean troops had overwhelmed the South Korean and allied American units and forced them back to a defensive line in south-east South Korea known as the Pusan Perimeter. During its brief occupation of southern Korea, the DPRK regime initiated radical social change, which included the nationalisation of industry, land reform, and the restoration of the People's Committees. According to the captured US General William F. Dean, "the civilian attitude seemed to vary between enthusiasm and passive acceptance".

The United Nations condemned North Korea's actions and approved an American-led intervention force to defend South Korea. In September, UN forces landed at Inchon and retook Seoul. Under the leadership of US General Douglas MacArthur, UN forces pushed north, reaching the Chinese border. According to Bruce Cumings, the North Korean forces were not routed, but managed a strategic retreat into the mountainous interior and into neighboring Manchuria. Kim Il-sung's government re-established itself in a stronghold in Chagang Province. In late November, Chinese forces entered the war and pushed the UN forces back, retaking Pyongyang in December 1950 and Seoul in January 1951. According to American historian Bruce Cumings, the Korean People's Army played an equal part in this counterattack. UN forces managed to retake Seoul for South Korea. The war essentially became a bloody stalemate for the next two years. American bombing included the use of napalm against populated areas and the destruction of dams and dykes, which caused devastating floods. China and North Korea also alleged the US was deploying biological weapons. As a result of the bombing, almost every substantial building and much of the infrastructure in North Korea was destroyed. The North Koreans responded by building homes, schools, hospitals, and factories underground. Economic output in 1953 had fallen by 75-90% compared with 1949.

While the bombing continued, armistice negotiations, which had commenced in July 1951, wore on. North Korea's lead negotiator was General Nam Il. The Korean Armistice Agreement was signed on July 27, 1953. A ceasefire followed, but there was no peace treaty, and hostilities continued at a lower intensity.

Kim began gradually consolidating his power. Up to this time, North Korean politics were represented by four factions: the Yan'an faction, made up of returnees from China; the "Soviet Koreans" who were ethnic Koreans from the USSR; native Korean communists led by Pak Hon-yong; and Kim's Kapsan group who had fought guerrilla actions against Japan in the 1930s.

When the Workers' Party Central Committee plenum opened on 30 August 1953, Choe Chang-ik made a speech attacking Kim for concentrating the power of the party and the state in his own hands as well as criticising the party line on industrialisation which ignored widespread starvation among the North Korean people. However, Kim neutralised the attack on him by promising to moderate the regime, promises which were never kept. The majority in the Central Committee voted to support Kim and also voted in favor of expelling Choe and Pak Hon-yong from the Central Committee. Eleven of Kim's opponents were convicted in a show trial. It is believed that all were executed. A major purge of the KWP followed, with members originating from South Korea being expelled.

Pak Hon-yong, party vice chairman and Foreign Minister of the DPRK, was blamed for the failure of the southern population to support North Korea during the war, was dismissed from his positions in 1953, and was executed after a show trial in 1955.

The Party Congress in 1956 indicated the transformation that the party had undergone. Most members of other factions had lost their positions of influence. More than half the delegates had joined after 1950, most were under 40 years old, and most had limited formal education.

In February 1956, Soviet leader Nikita Khrushchev made a sweeping denunciation of Stalin, which sent shock waves throughout the Communist world. Encouraged by this, members of the party leadership in North Korea began to criticize Kim's dictatorial leadership, personality cult, and Stalinist economic policies. They were defeated by Kim at the August Plenum of the party. By 1960, 70 per cent of the members of the 1956 Central Committee were no longer in politics.

Kim Il-sung had initially been criticized by the Soviets during a previous 1955 visit to Moscow for practicing Stalinism and a cult of personality, which was already growing enormous. The Korean ambassador to the USSR, Li Sangjo, a member of the Yan'an faction, reported that it had become a criminal offense to so much as write on Kim's picture in a newspaper and that he had been elevated to the status of Marx, Lenin, Mao, and Stalin in the communist pantheon. He also charged Kim with rewriting history to appear as if his guerrilla faction had single-handedly liberated Korea from the Japanese, completely ignoring the assistance of the Chinese People's Volunteers. In addition, Li stated that in the process of agricultural collectivization, grain was being forcibly confiscated from the peasants, leading to "at least 300 suicides" and that Kim made nearly all major policy decisions and appointments himself. Li reported that over 30,000 people were in prison for completely unjust and arbitrary reasons as trivial as not printing Kim Il-sung's portrait on sufficient quality paper or using newspapers with his picture to wrap parcels. Grain confiscation and tax collection were also conducted forcibly with violence, beatings, and imprisonment.

In late 1968, known military opponents of North Korea's "Juche" (or self-reliance) ideology such as Kim Chang-bong (minister of National Security), Huh Bong-hak (chief of the Division for Southern Intelligence) and Lee Young-ho (commander in chief of the DPRK Navy) were purged as anti-party/counter-revolutionary elements, despite their credentials as anti-Japanese guerrilla fighters in the past.

Kim's personality cult was modeled on Stalinism and his regime originally acknowledged Stalin as the supreme leader. After Stalin's death in 1953, however, Kim was described as the "Great Leader" or "Suryong". As his personality cult grew, the doctrine of "Juche" began to displace Marxism–Leninism. At the same time the cult extended beyond Kim himself to include his family in a revolutionary blood line.
In 1972, to celebrate Kim Il-sung's birthday, the Mansu Hill Grand Monument was unveiled, including a 22-meter bronze statue of him.

Like Mao in China, Kim Il-sung refused to accept Nikita Khrushchev's denunciation of Stalin and continued to model his regime on Stalinist norms. At the same time, he increasingly stressed Korean independence, as embodied in the concept of "Juche". Kim told Alexei Kosygin in 1965 that he was not anyone's puppet and "We...implement the purest Marxism and condemn as false both the Chinese admixtures and the errors of the CPSU".

Relations with China had worsened during the war. Mao Zedong criticized Kim for having started the whole "idiotic war" and for being an incompetent military commander who should have been removed from power. PLA commander Peng Dehuai was equally contemptuous of Kim's skills at waging war.

By some analysis, Kim Il-sung remained in power partially because the Soviets turned their attention to the Hungarian Revolution of 1956 that fall. The Soviets and Chinese were unable to stop the inevitable purge of Kim's domestic opponents or his move towards a one-man Stalinist autocracy and relations with both countries deteriorated in the former's case because of the elimination of the pro-Soviet Koreans and the latter because of the regime's refusal to acknowledge Chinese assistance in either liberation from the Japanese or the war in 1950–53.

Tensions between North and South escalated in the late 1960s with a series of low-level armed clashes known as the Korean DMZ Conflict. In 1966, Kim declared "liberation of the south" to be a "national duty". In 1968, North Korean commandos launched the Blue House Raid, an unsuccessful attempt to assassinate the South Korean President Park Chung-hee. Shortly after, the US spy ship Pueblo was captured by the North Korean navy. The crew were held captive throughout the year despite American protests that the vessel was in international waters, and they were finally released in December after a formal US apology was issued. In April 1969 a North Korean fighter jet shot down an EC-121 aircraft, killing all 31 crewmen on board. The Nixon administration found itself unable to react at all, since the US was heavily committed in the Vietnam War and had no troops to spare if the situation in Korea escalated. However, the "Pueblo" capture and EC-121 shootdown did not find approval in Moscow, as the Soviet Union did not want a second major war to erupt in Asia. China's response to the USS "Pueblo" crisis is less clear.

After Khrushchev was replaced by Leonid Brezhnev as Soviet Leader in 1964, and with the incentive of Soviet aid, North Korea strengthened its ties with the USSR. Kim condemned China's Cultural Revolution as "unbelievable idiocy". In turn, China's Red Guards labelled him a "fat revisionist".

In 1972, the first formal summit meeting between Pyongyang and Seoul was held, but the cautious talks did not lead to a lasting change in the relationship.

With the fall of South Vietnam to the North Vietnamese on April 30, 1975, Kim Il-sung felt that the US had shown its weakness and that reunification of Korea under his regime was possible. Kim visited Beijing in May 1975 in the hope of gaining political and military support for this plan to invade South Korea again, but Mao Zedong refused. Despite public proclamations of support, Mao privately told Kim that China would be unable to assist North Korea because of the lingering after-effects of the Cultural Revolution throughout China, and because Mao had recently decided to restore diplomatic relations with the US.

Meanwhile, North Korea emphasized its independent orientation by joining the Non-Aligned Movement in 1975. It promoted "Juche" as a model for developing countries to follow. It developed strong ties with the regimes of Bokassa in the Central African Republic, Macias Nguema in Equatorial Guinea, Idi Amin in Uganda, Pol Pot in Cambodia, Gaddafi in Libya, and Ceausescu in Romania.

Reconstruction of the country after the war proceeded with extensive Chinese and Soviet assistance. Koreans with experience in Japanese industries also played a significant part. Land was collectivized between 1953 and 1958. Resistance appears to have been minimal as landlords had been eliminated by the earlier reforms or during the war.

Although developmental debates took place within the Workers' Party of Korea in the 1950s, North Korea, like all the postwar communist states, undertook massive state investment in heavy industry, state infrastructure and military strength, neglecting the production of consumer goods.

The first Three Year Plan (1954–1956) introduced the concept of "Juche" or self-reliance. The first Five Year Plan (1957-1961) consolidated the collectivization of agriculture and initiated mass mobilizations campaigns: the Chollima Movement, the Chongsan-ni system in agriculture and the Taean Work System in industry. The Chollima Movement was influenced by China's Great Leap Forward, but did not have its disastrous results. Industry was fully nationalized by 1959. Taxation on agricultural income was abolished in 1966.

North Korea was placed on a semi-war footing, with equal emphasis being given to the civilian and military economies. This was expressed in the 1962 Party Plenum by the slogan, "Arms in one hand and a hammer and sickle in the other!" At a special party conference in 1966, members of the leadership who opposed the military build-up were removed.

On the ruins left by the war, North Korea had built an industrialized command economy. Che Guevara, then a Cuban government minister, visited North Korea in 1960, and proclaimed it a model for Cuba to follow. In 1965, the British economist Joan Robinson described North Korea's economic development as a "miracle". As late as the 1970s, its GDP per capita was estimated to be equivalent to South Korea's. By 1968, all homes had electricity, though the supply was unreliable. By 1972, all children from age 5 to 16 were enrolled in school, and over 200 universities and specialized colleges had been established. By the early 1980s, 60–70% of the population was urbanized.

In the 1970s, expansion of North Korea's economy, with the accompanying rise in living standards, came to an end. Compounding this was a decision to borrow foreign capital and invest heavily in military industries. North Korea's desire to lessen its dependence on aid from China and the Soviet Union prompted the expansion of its military power, which had begun in the second half of the 1960s. The government believed such expenditures could be covered by foreign borrowing and increased sales of its mineral wealth in the international market. North Korea invested heavily in its mining industries and purchased a large quantity of mineral extraction infrastructure from abroad. It also purchased entire petrochemical, textile, concrete, steel, pulp and paper manufacturing plants from the developed capitalist world. This included a Japanese-Danish venture that provided North Korea with the largest cement factory in the world. However, following the world 1973 oil crisis, international prices for many of North Korea's native minerals fell, leaving the country with large debts and an inability to pay them off and still provide a high level of social welfare to its people. North Korea began to default in 1974 and halted almost all repayments in 1985. As a result, it was unable to pay for foreign technology.

Worsening this already poor situation, the centrally planned economy, which emphasized heavy industry, had reached the limits of its productive potential in North Korea. "Juche"s repeated demands that North Koreans learn to build and innovate domestically had run its course as had the ability of North Koreans to keep technological pace with other industrialized nations. By the mid to late-1970s some parts of the capitalist world, including South Korea, were creating new industries based around computers, electronics, and other advanced technology in contrast to North Korea's Stalinist economy of mining and steel production. Migration to urban areas stalled.

Despite the emerging economic problems, the regime invested heavily in prestigious projects, such as the "Juche" Tower, the Nampo Dam, and the Ryugyong Hotel. In 1989, as a response to the 1988 Seoul Olympics, it held the 13th World Festival of Youth and Students in Pyongyang. In fact, the grandiosity associated with the regime and its personality cult, as expressed in monuments, museums, and events, has been identified as a factor in the economic decline.

In 1984, Kim visited Moscow during a grand tour of the USSR where he met Soviet leader Konstantin Chernenko. Kim also made public visits to East Germany, Czechoslovakia, Poland, Hungary, Romania, Bulgaria and Yugoslavia. Soviet involvement in the North Korean economy increased, until 1988 when bilateral trade peaked at US$2.8 billion. In 1986, Kim met the incoming Soviet leader Mikhail Gorbachev in Moscow and received a pledge of support.

However, Gorbachev's reforms and diplomatic initiatives, the Chinese economic reforms starting in 1979, and the collapse of the Eastern Bloc from 1989 to 1991 increased North Korea's isolation. The leadership in Pyongyang responded by proclaiming that the collapse of the Eastern Bloc communist governments demonstrated the correctness of the policy of "Juche".

The collapse of the Soviet Union in 1991 deprived North Korea of its main source of economic aid, leaving China as the isolated regime's only major ally. Without Soviet aid, North Korea's economy went into a free-fall. By this time in the early 1990s, Kim Jong-il was already conducting most of the day-to-day activities of running of the state. Meanwhile, international tensions were rising over North Korea's quest for nuclear weapons. Former US president Jimmy Carter made a visit to Pyongyang in June 1994 in which he met with Kim, and returned proclaiming that he had resolved the crisis.

Kim Il-sung died from a sudden heart attack on July 8, 1994, three weeks after the Carter visit. His son, Kim Jong-il, who had already assumed key positions in the government, succeeded as General Secretary of the Korean Workers' Party. At that time, North Korea had no secretary-general in the party nor a president. What minimal legal procedure had been established was summarily ignored. Although a new constitution appeared to end the wartime political system, it did not completely terminate the transitional military rule. Rather it legitimized and institutionalized military rule by making the National Defense Commission (NDC) the most important state organization and its chairman the highest authority. After three years of consolidating his power, Kim Jong-il became Chairman of the NDC on October 8, 1997, a position described by the NDC as the nation's "highest administrative authority", and thus North Korea's "de facto" head of state. His succession had been foreshadowed in 1980, when he was introduced to the public at the Sixth Party Congress. In 1982, Kim Jong-il had established himself as a leading theoretician with the publication of "On the Juche Idea". In 1984, he had been officially confirmed as his father's successor.

Although the succession of Kim Jong-il coincided with much societal upheaval, and the succession is conventionally seen as a turning point of North Korean history, the change in leadership hardly had direct consequences. The politics in the last years of Kim Il-sung closely resemble those of the beginning of the Kim Jong-il era. The economy was in steep decline. In 1990–1995, foreign trade was cut in half, with the loss of subsidized Soviet oil being particularly keenly felt. The crisis came to a head in 1995 with widespread flooding that destroyed crops and infrastructure, leading to a famine that lasted until 1998. At the same time, there appeared to be little significant internal opposition to the regime. Indeed, a great many of the North Koreans fleeing to China because of famine still showed significant support for the government as well as pride in their homeland. Many of these people reportedly returned to North Korea after earning sufficient money.

In 1998, the government announced a new policy called "Songun", or "Military First". Some analysts suggested that this meant the Korean People's Army was now more powerful than the Workers' Party.

President Kim Dae-jung of South Korea actively attempted to reduce tensions between the two Koreas under the Sunshine Policy. After the election of George W. Bush as the President of the United States in 2000, North Korea faced renewed pressure over its nuclear program.

On October 9, 2006, North Korea announced that it had successfully detonated a nuclear bomb underground. An official at South Korea's seismic monitoring center confirmed that a magnitude-3.6 tremor felt at the time was not a natural occurrence.

Additionally, North Korea was running a missile development program. In 1998, North Korea tested a Taepodong-1 Space Launch Vehicle, which successfully launched but failed to reach orbit. On July 5, 2006, it tested a Taepodong-2 ICBM that reportedly could reach the west coast of the U.S. in the 2-stage version, or the entire U.S. with a third stage. However, the missile failed shortly after launch.

On February 13, 2007, North Korea signed into an agreement with South Korea, the United States, Russia, China, and Japan, which stipulated North Korea would shut down its Yongbyon nuclear reactor in exchange for economic and energy assistance. However, in 2009 the North continued its nuclear test program.

In 2010, the sinking of a South Korean naval ship, the Cheonan, allegedly by a North Korean torpedo, and North Korea's shelling of Yeonpyeong Island escalated tensions between North and South.

Kim Jong-il died on December 17, 2011 and was succeeded by his son, Kim Jong-un. In late 2013, Kim Jong Un's uncle Jang Song-thaek was arrested and executed after a trial. According to the South Korean spy agency, Kim may have purged some 300 people after taking power. In 2014, the United Nations Commission of Inquiry accused the government of crimes against humanity.

In 2015, North Korea adopted Pyongyang Standard Time (UTC+08.30), reversing the change to Japan Standard Time (UTC+9.00) which had been imposed by the Japanese Empire when it annexed Korea. As a result, North Korea was in a different time zone to South Korea. In 2016, 7th Congress of the Workers' Party of Korea was held in Pyongyang, the first party congress since 1980.

In 2017, North Korea tested the Hwasong-15, an intercontinental ballistic missile capable of striking anywhere in the United States of America. Estimates of North Korea's nuclear arsenal at that time ranged between 15 and 60 bombs, probably including hydrogen bombs.

In February 2018, North Korea sent an unprecedented high-level delegation to the Winter Olympics in South Korea, headed by Kim Yo-jong, sister of Kim Jong-un, and President Kim Yong-nam, which passed on an invitation to South Korean President Moon to visit the North. In April the two Korean leaders met at the Joint Security Area where they announced their governments would work towards a denuclearized Korean Peninsula and formalize peace between the two states. North Korea announced it would change its time zone to realign with the South.

On June 12, 2018, Kim met American President Donald Trump at a summit in Singapore and signed a declaration, again affirming a commitment to peace and denuclearization. Trump announced that he would halt military exercises with South Korea and foreshadowed withdrawing American troops entirely. In September, South Korean President Moon visited Pyongyang for a summit with Kim. In February 2019 in Hanoi, a second summit between Kim and Trump broke down without an agreement. On June 30, 2019, Trump, Moon, and Kim met at the DMZ. Talks in Stockholm began in October between US and North Korean negotiating teams, but broke down after one day.





</doc>
<doc id="21257" url="https://en.wikipedia.org/wiki?curid=21257" title="Geography of North Korea">
Geography of North Korea

North Korea is located in East Asia on the Northern half of the Korean Peninsula.

North Korea shares a border with three countries; China along the Amnok River, Russia along the Tumen River, and South Korea along the Korean Demilitarized Zone (DMZ). The Yellow Sea and the Korea Bay are off the west coast and the Sea of Japan (East Sea of Korea) is off the east coast.

Most of North Korea is a series of medium-sized to large-sized mountain ranges and large hills, separated by deep, narrow valleys. The highest peak, Paektu-san on the volcanic Baekdu Mountain, is located on its northern border with China, and rises 9,002 ft. (2,744 m). Along the west coast there are wide coastal plains, while along the East Sea coastline (North Korea's lowest point at 0 m), narrow plains rise into mountains. Similar to South Korea, dozens of small islands dot the western coastline. North Korea's longest river is the Yulu (Yalu). Other large rivers include the Tumen, Taedong and Imjin.

The terrain consists mostly of hills and mountains separated by deep, narrow valleys. The coastal plains are wide in the west and discontinuous in the east.

Early European visitors to Korea remarked that the country resembled "a sea in a heavy gale" because of the many successive mountain ranges that crisscross the peninsula. Some 80 percent of North Korea's land area is composed of mountains and uplands, with all of the peninsula's mountains with elevations of or more located in North Korea. The great majority of the population lives in the plains and lowlands.

Paektu Mountain, the highest point in North Korea at 2,743 m (9,003 ft), is a volcanic mountain near Manchuria with basalt lava plateau with elevations between and above sea level. The Hamgyong Range, located in the extreme northeastern part of the peninsula, has many high peaks, including Kwanmobong at approximately .

Other major ranges include the Rangrim Mountains, which are located in the north-central part of North Korea and run in a north-south direction, making communication between the eastern and western parts of the country rather difficult; and the Kangnam Range, which runs along the North Korea–China border. Geumgangsan, often written Mt Kumgang, or Diamond Mountain, (approximately ) in the Thaebaek Range, which extends into South Korea, is famous for its scenic beauty.

For the most part, the plains are small. The most extensive are the Pyongyang and Chaeryŏng plains, each covering about 500 km. Because the mountains on the east coast drop abruptly to the sea, the plains are even smaller there than on the west coast.

The mountain ranges in the northern and eastern parts of North Korea form the watershed for most of its rivers, which run in a westerly direction and empty into the Yellow Sea and Korea Bay. The longest is the Amnok River, which is navigable for 678 km of its . The Tuman River, one of the few major rivers to flow into the Sea of Japan, is the second longest at but is navigable for only because of the mountainous topography.

The third longest river, the Taedong River, flows through Pyongyang and is navigable for 245 of its 397 km. Lakes tend to be small because of the lack of glacial activity and the stability of the Earth's crust in the region. Unlike neighboring Japan or northern China, North Korea experiences few severe earthquakes. The country has a number of natural spas and hot springs, which number 124 according to one North Korean source.

North Korea has a combination of a continental climate and an oceanic climate, with four distinct seasons. Most of North Korea is classified as being of a humid continental climate within the Köppen climate classification scheme, with warm summers and cold, dry winters. In summer, there is a short rainy season called "changma".

Long winters bring bitter cold and clear weather interspersed with snow storms as a result of northern and northwestern winds that blow from Siberia. The daily average high and low temperatures for Pyongyang in January are . On average, it snows thirty-seven days during the winter. Winter can be particularly harsh in the northern, mountainous regions.

Summer tends to be short, hot, humid, and rainy because of the southern and southeastern monsoon winds that bring moist air from the Pacific Ocean. Spring and autumn are transitional seasons marked by mild temperatures and variable winds and bring the most pleasant weather. The daily average high and low temperatures for Pyongyang in August are .

On average, approximately 60% of all precipitation occurs from June to September. Natural hazards include late spring droughts which are often followed by severe flooding. Typhoons affect the peninsula on an average of at least once every summer or early autumn. The drought that started in June 2015, according to the Korean Central News Agency, has been the worst seen in 100 years.

The environment of North Korea is diverse, encompassing alpine, forest, farmland, freshwater, and marine ecosystems.

In recent years, the environment has been reported to be in a state of "crisis", "catastrophe", or "collapse".

Cultivation, logging, and natural disasters have all put pressure on North Korea's forests. During the economic crisis of the 1990s, deforestation accelerated, as people turned to the woodlands to provide firewood and food. This in turn has led to soil erosion, soil depletion, and increased risk of flooding. In response, the government has promoted a tree planting program. Based on satellite imagery, it has been estimated that 40 percent of forest cover has been lost since 1985.

North Korea has an area of 120,538 km², of which 120,408 km² is land and 130 km² is water. It has of land boundaries; of these, are with China, are with South Korea, and are with Russia.

The Korean Peninsula extends about southward from the northeast Asian continental landmass. The coastline of Korea is highly irregular, and North Korea accounts for of this, roughly one-third. Some 3579 islands lie adjacent to the Korean Peninsula, mostly along the south and west coasts.

The southern stretch of its east coast forms the northern side of the East Korea Bay. At the headland Musu Dan, this ends and the coast turns sharply northward.

The North Korean government claims territorial waters extending from shore. It also claims an exclusive economic zone from shore. In addition, a maritime military boundary that lies offshore in the Sea of Japan (East Sea of Korea) and offshore in the Yellow Sea demarcates the waters and airspace into which foreign ships and planes are prohibited from entering without permission.

Waters of the Yellow Sea are demarcated between North Korea and South Korea by the disputed Northern Limit Line drawn by the United Nations Command (Korea) in early 1950s and not officially recognized by North Korea. Disputes between North and South Korean naval vessels have occurred in this area. A total of five disputes were noteworthy enough to have been reported in the news (three in 2009 and two in 2010).

Natural resources include coal, petroleum, lead, tungsten, zinc, graphite, magnesite, iron ore, copper, gold, pyrites, salt, fluorspar and hydropower.





Lists:




</doc>
<doc id="21258" url="https://en.wikipedia.org/wiki?curid=21258" title="Demographics of North Korea">
Demographics of North Korea

The demographics of North Korea are known through national censuses and international estimates. The Central Bureau of Statistics of North Korea conducted the most recent census in 2008, where the population reached 24 million inhabitants. The population density is 199.54 inhabitants per square kilometre, and the 2014 estimated life expectancy is 69.81 years. In 1980, the population rose at a near consistent, but low, rate (0.84% from the two censuses). Since 2000, North Korea's birth rate has exceeded its death rate; the natural growth is positive. In terms of age structure, the population is dominated by the 15–64-year-old segment (68.09%). The median age of the population is 32.9 years, and the gender ratio is 0.95 males to 1.00 female. Since the early 1990s, the birth rate has been fairly stable, with an average of 2 children per woman, down from an average of 3 in the early 1980s.

According to "The World Factbook", North Korea is racially homogeneous and contains a small Chinese community and a few ethnic Japanese. The 2008 census listed two nationalities: Korean (%) and Other (%). Korea was annexed by the Empire of Japan in 1910, in which the Korean Peninsula was occupied by Japanese. In 1945, when Japan was defeated in World War II, Korea was divided into two occupied zones: North occupied by the Soviet Union and the South by the United States. Negotiations on unification failed, and in 1948 two separate countries were formed: North and South Korea.

Korean is the official language of North Korea. "The World Factbook" states "traditionally Buddhist and Confucianist, some Christian and syncretic Chondogyo" in regards to religion, but also states "autonomous religious activities now almost nonexistent; government-sponsored religious groups exist to provide illusion of religious freedom". , 8.86% of the population older than 5 years old have attained academic degrees. In 2000, North Korea spent 38.2% of its expenditures on education, social insurance, and social security. Estimates show that, in 2012, gross domestic product (GDP) per capita was $1,800. The most significant sources of employment were machine building and manufacturing of metallurgical products, military products, and textiles. In 2006, the unemployment rate was between 14.7% and 36.5%. The 2008 census enumerated 5,887,471 households, averaging 3.9 persons per house. Average urbanization rate was 60.3% in 2011.

During the North Korean famine of 1994-1998 somewhere between 240,000 and 3,500,000 North Koreans died from starvation or hunger-related illnesses, with the deaths peaking in 1997. A 2011 U.S. Census Bureau report put the likely number of excess deaths during 1993 to 2000 at from 500,000 to 600,000.

Until the release of official data in 1989, the 1963 edition of the North Korea Central Yearbook was the last official publication to disclose population figures. After 1963 demographers used varying methods to estimate the population. They either totaled the number of delegates elected to the Supreme People's Assembly (each delegate representing 50,000 people before 1962 and 30,000 people afterward) or relied on official statements that a certain number of persons, or percentage of the population, was engaged in a particular activity. Thus, on the basis of remarks made by President Kim Il-sung in 1977 concerning school attendance, the population that year was calculated at 17.2 million persons. During the 1980s, health statistics, including life expectancy and causes of mortality, were gradually made available to the outside world.

In 1989 the Central Bureau of Statistics released demographic data to the United Nations Fund for Population Activities (UNFPA) to secure the UNFPA's assistance in holding North Korea's first nationwide census since the establishment of the DPRK in 1946. Although the figures given to the United Nations (UN) might have been purposely distorted, it appears that in line with other attempts to open itself to the outside world, the North Korean regime has also opened somewhat in the demographic realm. Although the country lacks trained demographers, accurate data on household registration, migration, and births and deaths are available to North Korean authorities.

According to the United States scholar Nicholas Eberstadt and demographer Judith Banister, vital statistics and personal information on residents are kept by agencies on the ri, or ni (, : village, the local administrative unit) level in rural areas and the dong (, : district or block) level in urban areas.

The next census is scheduled for 2018.

In their 1992 monograph, "The Population of North Korea", Eberstadt and Banister use the data given to the UNFPA and make their own assessments. They place the total population at 21.4 million persons in mid-1990, consisting of 10.6 million males and 10.8 million females. This figure is close to an estimate of 21.9 million persons for mid-1988 cited in the 1990 edition of the "Demographic Yearbook" published by the UN. "Korean Review", a book by Pan Hwan Ju published by the Pyongyang Foreign Languages Press in 1987, gives a figure of 19.1 million persons for 1986.

The figures disclosed by the government reveal an unusually low proportion of males to females: in 1980 and 1987, the male-to-female ratios were 86.2 to 100, and 84.2 to 100, respectively. Low male-to-female ratios are usually the result of a war, but these figures were lower than the sex ratio of 88.3 males per 100 females recorded for 1953, the last year of the Korean War. The male-to-female ratio would be expected to rise to a normal level with the passage of years, as happened between 1953 and 1970, when the figure was 95.1 males per 100 females. After 1970, however, the ratio declined. Eberstadt and Banister suggest that before 1970 male and female population figures included the whole population, yielding ratios in the ninetieth percentile, but that after that time the male military population was excluded from population figures.

Based on the figures provided by the Central Statistics Bureau, Eberstadt and Banister estimate that the actual size of the "hidden" male North Korean military had reached 1.2 million by 1986 and that the actual male-to-female ratio was 97.1 males to 100 females in 1990. If their estimates are correct, 6.1 percent of North Korea's total population was in the military, numerically the world's fifth largest military force, in the late 1980s (fourth largest ).

A survey in 2017 found that the famine had skewed North Korea's demography, impacting particularly on male infants. Women aged 20-24 made up 4% of the population, while men in the same age group made up only 2.5%.

The annual population growth rate in 1960 was 2.7 percent, rising to a high of 3.6 percent in 1970, and falling to 1.9 percent in 1975. This fall reflected a dramatic decline in the fertility rate: the average number of children born to women decreased from 6.5 in 1966 to 2.5 in 1988. Assuming the data is reliable, reasons for falling growth rates and fertility rates probably include late marriage, urbanization, limited housing space, and the expectation that women would participate equally in work hours in the labor force. The experience of other socialist countries suggests that widespread labor force participation by women often goes hand-in-hand with more traditional role expectations; in other words, they are still responsible for housework and childrearing. The high percentage of males age 17 to 26 may have contributed to the low fertility rate.

According to Eberstadt and Banister's data, the annual population growth rate in 1991 was 1.9 percent. However, the CIA World Factbook estimated that North Korea's annual population growth rate was 1.0% in 1991 and that it has since declined to 0.4% by 2009.

The North Korean government seems to perceive its population as too small in relation to that of South Korea. In its public pronouncements, Pyongyang has called for accelerated population growth and encouraged large families. According to one Korean American scholar who visited North Korea in the early 1980s, the country has no birth control policies; parents are encouraged to have as many as six children. The state provides "t'agaso" (nurseries) to lessen the burden of childrearing for parents and offers a 77-day paid leave after childbirth.

Eberstadt and Banister suggest, however, that authorities at the local level make contraceptive information readily available to parents and that intrauterine devices are the most commonly adopted birth control method. An interview with a former North Korean resident in the early 1990s revealed that such devices are distributed free at clinics.

Demographers determine the age structure of a given population by dividing it into five-year age-groups and arranging them chronologically in a pyramidlike structure that "bulges" or recedes in relation to the number of persons in a given age cohort. Many poor, developing countries have a broad base and steadily tapering higher levels, which reflects a large number of births and young children but much smaller age cohorts in later years as a result of relatively short life expectancies. North Korea does not entirely fit this pattern; data reveal a "bulge" in the lower ranges of adulthood. In 1991, life expectancy at birth was approximately 66 years for males, almost 73 for females.

It is likely that annual population growth rates will increase, as well as difficulties in employing the many young men and women entering the labor force in a socialist economy already suffering from stagnant growth. Eberstadt and Banister project that the population will stabilize (that is, cease to grow) at 34 million persons in 2045 and will then experience a gradual decline.

North Korea's population is concentrated in the plains and lowlands. The least populated regions are the mountainous Chagang and Yanggang provinces adjacent to the Chinese border. The largest concentrations of population are in North P'yŏngan and South P'yŏngan provinces, in the municipal district of Pyongyang, and in South Hamgyŏng Province, which includes the Hamhŭng-Hŭngnam urban area. Eberstadt and Banister calculate the average population density at 167 persons per square kilometer, ranging from 1,178 persons per square kilometer in Pyongyang Municipality to 44 persons per square kilometer in Yanggang Province. By contrast, South Korea had an average population density of 425 persons per square kilometer in 1989.

Like South Korea, North Korea has experienced significant urban migration since the end of the Korean War. Official statistics reveal that 59.6 percent of the total population was classified as urban in 1987. This figures compares with only 17.7 percent in 1953. It is not entirely clear, however, what standards are used to define urban populations. Eberstadt and Banister suggest that although South Korean statisticians do not classify settlements of under 50,000 as urban, their North Korean counterparts include settlements as small as 20,000 in this category. And, in North Korea, people who engage in agricultural pursuits inside municipalities sometimes are not counted as urban.

Urbanization in North Korea seems to have proceeded most rapidly between 1953 and 1960, when the urban population grew between 12 and 20 percent annually. Subsequently, the increase slowed to about 6 percent annually in the 1960s and between 1 and 3 percent from 1970 to 1987.

In 1987, North Korea's largest cities were Pyongyang, with approximately 2.3 million inhabitants; Hamhŭng, 701,000; Ch'ŏngjin, 520,000; Namp'o, 370,000; Sunch'ŏn, 356,000; and Sinŭiju, 289,000. In 1987, the total national population living in Pyongyang was 11.5 percent. The government restricts and monitors migration to cities and ensures a relatively balanced distribution of population in provincial centers in relation to Pyongyang.

Source: Population Division of the United Nations Department of Economic and Social Affairs
Births and deaths
Life expectancy

Average life expectancy at age 0 of the total population.
Source: Central Bureau of Statistics

Large-scale emigration from Korea began around 1904 and continued until the end of World War II. During the Japanese colonial occupation (1910–45), many Koreans emigrated to Northeast China, other parts of China, the Soviet Union, Hawaii, and the contiguous United States. People from Korea's northern provinces went primarily to Manchuria, China, and Siberia; many of those from the southern provinces went to Japan. Most emigrants left for economic reasons because employment opportunities were scarce; many Korean farmers had lost their land after the Japanese colonial government introduced a system of private land tenure, imposed higher land taxes, and promoted the growth of an absentee landlord class charging exorbitant rents.

In the 1980s, more than 4 million ethnic Koreans lived outside the peninsula. The largest group, about 1.7 million people, lived in China (see Koreans in China); most had assumed Chinese citizenship. Approximately 1 million Koreans, almost exclusively from South Korea, lived in North America (see Korean Americans). About 389,000 ethnic Koreans resided in the former Soviet Union (see Koryosaram and Sakhalin Koreans). One observer noted that Koreans have been so successful in running collective farms in Soviet Central Asia that being Korean is often associated by other citizens with being rich. As a result, there is growing antagonism against Koreans. Smaller groups of Koreans are found in Central America and South America (85,000), the Middle East (62,000), Europe (40,000), Asia (27,000), and Africa (25,000).

Many of Japan's approximately 680,000 Koreans have below average standards of living. This is partly because of discrimination by the Japanese. Many resident Koreans, loyal to North Korea, remain separate from, and often hostile to, the Japanese social mainstream. The pro-North Korean Chongryon (General Association of Korean Residents in Japan, known as Chosen Soren or Chosoren in Japanese) initially was more successful than the pro-South Korean Mindan (Association for Korean Residents in Japan) in attracting adherents. However, the widening disparity between the political and economic conditions of the two Koreas has since made Mindan the larger and certainly the less politically controversial faction. In addition, third- and fourth-generation Zainichi Chosenjin have largely given up active participation or loyalty to the Chongryon ideology. Reasons stated for this increased disassociation include widespread mainstream tolerance of Koreans by Japanese in recent years, greatly reducing the need to rely on Chongryon and the increasing unpopularity of Kim Jong Il even among loyal members of Chongryon.

Between 1959 and 1982, Chongryon encouraged the repatriation of Korean residents in Japan to North Korea. More than 93,000 Koreans left Japan, the majority (80,000 persons) in 1960 and 1961. Thereafter, the number of repatriates declined, apparently because of reports of hardships suffered by their compatriots. Approximately 6,637 Japanese wives accompanied their husbands to North Korea, of whom about 1,828 retained Japanese citizenship in the early 1990s. Pyongyang had originally promised that the wives could return home every two or three years to visit their relatives. In fact, however, they are not allowed to do so, and few have had contact with their families in Japan. In normalization talks between North Korean and Japanese officials in the early 1990s, the latter urged unsuccessfully that the wives be allowed to make home visits. According to a defector, himself a former returnee, many petitioned to be returned to Japan and in response were sent to political prison camps. Japanese research puts the number of Zainichi Korean returnees condemned to prison camps at around 10,000.

The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.

Population: 25,115,311 (July 2016 est.)

Age structure<br>
"0–14 years:"
20.97% (male 2,678,638/female 2,588,744)
"15–24 years:"
15.88% (male 2,009,360/female 1,977,942)
"25–54 years:"
44.22% (male 5,567,682/female 5,537,077)
"55–64 years:"
9.19% (male 1,090,739/female 1,218,406)
"65 years and over:"
9.74% (male 840,003/female 1,606,720) (2016 est.)

Population growth rate<br>
1.02% (1991 est.)<br>
0.31% (1996 est.)<br>
0.87% (2006 est.)<br>
0.42% (2009 est.)<br>
0.53% (2016 est.)

Birth rate<br>
20.01 births/1,000 population (1991 est.)<br>
17.58 births/1,000 population (1996 est.)<br>
14.61 births/1,000 population (2006 est.)<br>
14.61 births/1,000 population (2008 est.)<br>
14.60 births/1,000 population (2016 est.)

Death rate<br>
8.94 deaths/1,000 population (1991 est.)<br>
9.52 deaths/1,000 population (1996 est.)<br>
7.29 deaths/1,000 population (2006 est.)<br>
7.29 deaths/1,000 population (2008 est.)<br>
9.30 deaths/1,000 population (2016 est.)

Net migration rate:
0 migrant(s)/1,000 population (2016 est.)

Sex ratio<br>
"at birth:"
1.05 male(s)/female
<br>"0–14 years:"
1.03 male(s)/female
<br>"15–24 years:"
1.02 male(s)/female
<br>"25–54 years:"
1.01 male(s)/female
<br>"55–64 years:"
0.9 male(s)/female
<br>"65 years and over:"
0.53 male(s)/female
<br>"total population:"
0.94 male(s)/female (2016 est.)

Infant mortality rate<br>
total: 22.9 deaths/1,000 live births (2016 est.)

Life expectancy at birth<br>
"total population:"
70.4 years
<br>"male:"
66.6 years
<br>"female:"
74.5 years (2016 est.)

Total fertility rate<br>
2.09 children born/woman (2006 est.)<br>
1.94 children born/woman (2010 est.)<br>
1.96 children born/woman (2016 est.)

Nationality<br>
"noun:" Korean(s)<br>
"adjective:" Korean

Ethnic groups<br>
racially homogeneous: Koreans; small Chinese community, a few ethnic Japanese, Singaporeans, ethnic Thai, ethnic Indian, ethnic African, Americans and ethnic Vietnamese

Religion:
no statistics available; predominantly Cheondoism (see "religion in North Korea")

Language: Korean

Literacy<br>
"definition:"
age 15 and over can read and write Korean using the Korean script Hangul
<br>"total population:" 100%
<br>"male:" 100%
<br>"female:" 100% (2015 est.)





</doc>
<doc id="21259" url="https://en.wikipedia.org/wiki?curid=21259" title="Politics of North Korea">
Politics of North Korea

The politics of North Korea (officially the Democratic People's Republic of Korea or DPRK) takes place within the framework of the official state philosophy, "Juche", a concept created by Hwang Jang-yop and later attributed to Kim Il-sung. The "Juche" theory is the belief that only through self-reliance and a strong independent state, can true socialism be achieved.

North Korea's political system is built upon the principle of centralization. While the North Korean constitution formally guarantees protection of human rights, in practice there are severe limits on freedom of expression, and the government closely supervises the lives of North Korean citizens. The constitution defines North Korea as "a dictatorship of people's democracy" under the leadership of the Workers' Party of Korea (WPK), which is given legal supremacy over other political parties.

The WPK is the ruling party of North Korea. It has been in power since its creation in 1948. Two minor political parties also exist, but are legally bound to accept the ruling role of the WPK. They, with the WPK, comprise a popular front, known as the Democratic Front for the Reunification of the Fatherland (DFRF). Elections occur only in single-candidate races where the candidate is effectively selected beforehand by the WPK.

In addition to the parties, there are over 100 mass organizations controlled by the WPK. Those who are not WPK members are required to join one of these organizations. Of these, the most important ones are the Kimilsungist-Kimjongilist Youth League, Socialist Women's Union of Korea, General Federation of Trade Unions of Korea, and Union of Agricultural Workers of Korea. These four organizations are also DFRF members.

Kim Il-sung ruled the country from 1948 until his death in July 1994, holding the offices of General Secretary of the WPK from 1949 to 1994 (titled as Chairman from 1949 to 1972), Premier of North Korea from 1948 to 1972 and President from 1972 to 1994. He was succeeded by his son, Kim Jong-il. While the younger Kim had been his father's designated successor since the 1980s, it took him three years to consolidate his power. He was named to his father's old post of General Secretary in 1997, and in 1998 became chairman of the National Defence Commission (NDC), which gave him command of the armed forces. The constitution was amended to make the NDC chairmanship "the highest post in the state." At the same time, the presidential post was written out of the constitution, and Kim Il-sung was designated "Eternal President of the Republic" in order to honor his memory forever. Most analysts believe the title to be a product of the cult of personality he cultivated during his life.

Outside observers generally view North Korea as a Stalinist dictatorship particularly noting the elaborate cult of personality around Kim Il-sung and his family. The Workers' Party of Korea (WPK), led by a member of the ruling family, holds power in the state and leads the Democratic Front for the Reunification of the Fatherland of which all political officers are required to be members. The government has formally replaced all references to Marxism–Leninism in its constitution with the locally developed concept of "Juche", or self-reliance. In recent years, there has been great emphasis on the "Songun" or "military-first" philosophy. All references to communism were removed from the North Korean constitution in 2009.

The status of the military has been enhanced, and it appears to occupy the center of the North Korean political system; all the social sectors are forced to follow the military spirit and adopt military methods. Kim Jong-il's public activity focused heavily on "on-the-spot guidance" of places and events related to the military. The enhanced status of the military and military-centered political system was confirmed at the first session of the 10th Supreme People's Assembly (SPA) by the promotion of NDC members into the official power hierarchy. All ten NDC members were ranked within the top twenty on 5 September, and all but one occupied the top twenty at the fiftieth anniversary of the Day of the Foundation of the Republic on 9 September.

According to the Constitution of North Korea, the country is a democratic republic and the Supreme People's Assembly (SPA) and Provincial People's Assemblies (PPA) are elected by direct universal suffrage and secret ballot. Suffrage is guaranteed to all citizens aged 17 and over. In reality, elections in North Korea are for show and feature single-candidate races only. Those who want to vote against the sole candidate on the ballot must go to a special booth - in the presence of an electoral official - to cross out the candidate's name before dropping it into the ballot box—an act which, according to many North Korean defectors, is far too risky to even contemplate.

All elected candidates are members of the Democratic Front for the Reunification of the Fatherland (DFRF), a popular front dominated by the ruling Workers' Party of Korea (WPK). The two minor parties in the coalition are the Chondoist Chongu Party and the Korean Social Democratic Party; they also have a few elected officials. The WPK exercises direct control over the candidates selected for election by members of the other two parties. In the past, elections were contested by other minor parties as well, including the Korea Buddhist Federation, Democratic Independent Party, Dongro People's Party, Gonmin People's Alliance, and People's Republic Party.

Originally a close ally of Joseph Stalin's Soviet Union, North Korea has increasingly emphasized "Juche", an adoption of socialist self-reliance, which roots from Marxism–Leninism, its adoption of a certain ideological form of Marxism-Leninism is specific to the conditions of North Korea. "Juche" was enshrined as the official ideology when the country adopted a new constitution in 1972. In 2009, the constitution was amended again, quietly removing the brief references to communism (). However, North Korea continues to see itself as part of a worldwide leftist movement. The Workers' Party maintains a relationship with other leftist parties, sending a delegation to the International Meeting of Communist and Workers' Parties. North Korea has a strong relationship with Cuba; in 2016, the North Korean government declared three days of mourning period for Fidel Castro's death.

For much of its history, North Korean politics have been dominated by its adversarial relationship with South Korea. During the Cold War, North Korea aligned with the Soviet Union and the People's Republic of China. The North Korean government invested heavily in its military, hoping to develop the capability to reunify Korea by force if possible and also preparing to repel any attack by South Korea or the United States. Following the doctrine of "Juche", North Korea aimed for a high degree of economic independence and the mobilization of all the resources of the nation to defend Korean sovereignty against foreign powers.

In the wake of the collapse of the Soviet Union in the early 1990s and the loss of Soviet aid, North Korea faced a long period of economic crisis, including severe agricultural and industrial shortages. North Korea's main political issue has been to find a way to sustain its economy without compromising the internal stability of its government or its ability to respond to perceived external threats. Recently, North Korean efforts to improve relations with South Korea to increase trade and to receive development assistance have been mildly successful. North Korea has tried to improve its relations with South Korea by participating in the Pyeongchang Olympics (North Korea at the 2018 Winter Olympics), when Kim Jong-un sent his band and a few officials to visit South Korea. But North Korea's determination to develop nuclear weapons and ballistic missiles has prevented stable relations with both South Korea and the United States. North Korea has also experimented with market economics in some sectors of its economy, but these have had limited impact.

Although there are occasional reports of signs of opposition to the government, these appear to be isolated, and there is no evidence of major internal threats to the current government. Some foreign analysts have pointed to widespread starvation, increased emigration through North Korea-China border, and new sources of information about the outside world for ordinary North Koreans as factors pointing to an imminent collapse of the regime. However, North Korea has remained stable in spite of more than a decade of such predictions. The Workers' Party of Korea maintains a monopoly on political power and Kim Jong-il remained the leader of the country until 2011, ever since he first gained power following the death of his father.

After the death of Kim Il-Sung in 1994, his son, Kim Jong-Il reigned as the new leader, which marked the closure of one chapter of North Korean politics. Combined with external shocks and less charismatic personality of Kim Jong-Il, the transition of the leadership caused North Korea toward less centralized control. There are three key institutions: the Korean People's Army (KPA), the Korean Workers’ Party (KWP), and the cabinet. Rather than dominate a unified system as his father had, each party has their own enduring goals, therefore providing checks and balances to the government. No one party could claim victory and power over the other ones. With changing internal situation, combined with external pressure, the cabinet started to endorse policies it had rejected for years. North Korea politics is gradually becoming more open and negotiable with foreign countries. The fact that the leader of North Korea is willing to talk with other leaders shows a huge step towards peace and negotiation.

According to Cheong Seong-chang of Sejong Institute, speaking on 25 June 2012, there is some possibility that the new leader Kim Jong-un, who has greater visible interest in the welfare of his people and engages in greater interaction with them than his father did, will consider economic reforms and normalization of international relations.

In June 2011, it was reported that the government had ordered universities to cancel most classes until April 2012, sending students to work on construction projects, presumably for fear of similar developments as in North Africa. In the previous months, the regime had ordered anti-riot gear from China. However, "as soon as universities were reopened, graffiti appeared again. Perhaps the succession is not the real reason, but greater awareness among North Koreans could lead to changes." 

After the death of Kim Jong-il on December 17, 2011, his son, Kim Jong-un inherited the political leadership of the DPRK. The succession of power was immediate: Kim Jong-un became Supreme Commander of the Korean People's Army on December 30, 2011, was appointed secretary of the Korean Workers Party (KWP) on April 11, 2012, and was entitled chairman of the National Defense Commission (NDC) two days later. To gain complete political power, he became the rank of marshal of the KPA.

Up until his death, Kim Jong-il maintained a strong national military-first political system that equated stability with military power. Kim Jong-un continues to carry on the militarized political style of his father, but with less commitment to complete military rule. Since he took power, Kim Jong-un has attempted to move political power away from the KPA and has divided it among the WPK and the cabinet. Because of his political lobbying, the WPK's Central Committee has vastly shifted power in April 2012: out of 17 members and 15 alternates of the Committee, only five members and six alternates derive from military and security sectors. Ever since, the economic power of the WPK, the cabinet, and the KPA has been in a tense balance. The KPA has lost a significant amount of economic influence because of the current regime, which continually shifts from what Kim Jong-il built his regime on, and may cause later internal issues.





</doc>
<doc id="21260" url="https://en.wikipedia.org/wiki?curid=21260" title="Economy of North Korea">
Economy of North Korea

The economy of North Korea is a centrally planned system, where the role of market allocation schemes is limited, although increasing. , North Korea continues its basic adherence to a centralized command economy. There has been some economic liberalization, particularly after Kim Jong-un assumed the leadership in 2012, but reports conflict over particular legislation and enactment. According to economic freedom ranking by Heritage Foundation, North Korea's economic freedom score is 5.9, making it the least free of the 180 economies measured in the 2019 Index.

The collapse of the Eastern Bloc from 1989 to 1991, particularly North Korea's principal source of support, the Soviet Union, forced the North Korean economy to realign its foreign economic relations, including increased economic exchanges with South Korea. China is North Korea's largest trading partner. North Korea's ideology of Juche has resulted in the country pursuing autarky in an environment of international sanctions. While the current North Korean economy is still dominated by state-owned industry and collective farms, foreign investment and corporate autonomy have slightly increased.

North Korea had a similar GDP per capita to its neighbor South Korea from the aftermath of the Korean War until the mid-1970s, but had a GDP per capita of less than $2,000 in the late 1990s and early 21st century. For 2018 the Bank of Korea estimated the GDP growth as −4.1%.

In 2019, North Korea was ranked 172nd in the Transparency International Corruption Perceptions Index with a score of 17 out of 100.

Estimating gross national product in North Korea is a difficult task because of a dearth of economic data and the problem of choosing an appropriate rate of exchange for the North Korean won, the nonconvertible North Korean currency. The South Korean government's estimate placed North Korea's GNP in 1991 at US$22.9 billion, or US$1,038 per capita. In contrast, South Korea posted US$237.9 billion of GNP and a per capita income of US$5,569 in 1991. North Korea's GNP in 1991 showed a 5.2% decline from 1989, and preliminary indications were that the decline would continue. South Korea's GNP, by contrast, expanded by 9.3% and 8.4%, respectively, in 1990 and 1991.

It is estimated that North Korea's GNP nearly halved between 1990 and 1999. North Korean annual budget reports suggest state income roughly tripled between 2000 and 2014. By about 2010 external trade had returned to 1990 levels.

The South Korea-based Bank of Korea estimated that over 2000 to 2013 average growth was 1.4% per year. It estimated that the real GDP of North Korea in 2015 was 30,805 billion South Korean won. It has published the following estimates of North Korea's GDP growth:
This analysis converts production volume estimates into South Korean prices, so is subject to price changes over time of South Korean goods. According to analyst Andrei Lankov, writing in 2017, a significant number of observers believe that the Bank of Korea is too conservative and the real growth rate is North Korea reported that the government budget has been increasing at between 5% and 10% annually from 2007 to 2015. Reported planned capital expenditure, mainly on roads and public buildings, increased by 4.3% in 2014, 8.7% in 2015 to 13.7% in 2016. According to a North Korea economist, the growth rate was 3.7% in 2017, lifting GDP to $29.6 billion in 2018. The Australian government estimated 1.3% growth in 2017, while the South Korean government estimated -3.5%.

In 2018, North Korea's government budget revenue plan overfulfilled 1.4%, an increase of 4.6% over 2017 year.

Beginning in the mid-1920s, the Japanese colonial administration in Korea concentrated its industrial-development efforts in the comparatively under-populated and resource-rich northern portion of the country, resulting in a considerable movement of people northward from the agrarian southern provinces of the Korean Peninsula.

This trend did not reverse until after the end (1945) of World War II, when more than 2 million Koreans moved from North to South following the division of Korea into Soviet and American military zones of administration. This southward exodus continued after the establishment of the Democratic People's Republic of Korea (North Korea) in 1948 and during the 1950–53 Korean War. The North Korean population as of October 2008 was given as 24 million.

The post-World War II division of the Korean Peninsula resulted in imbalances of natural and human resources, with disadvantages for both the North and the South. In 1945, about 80% of Korean heavy industry was in the North but only 31% of light industry, 37% of agriculture, and 18% of the peninsula's total commerce.

North and South Korea both suffered from the massive destruction caused during the Korean War. Historian Charles K. Armstrong stated that "North Korea had been virtually destroyed as an industrial society". In the years immediately after the war, North Korea mobilized its labour force and natural resources in an effort to achieve rapid economic development. Large amounts of aid from other communist countries, notably the Soviet Union and the People's Republic of China, helped the country achieve a high growth-rate in the immediate postwar period.

In 1961 an ambitious seven-year plan was launched to continue industrial expansion and increase living standards, but within three years it became clear this was failing and the plan period was extended to 1970. The failure was due to reduced support from the Soviet Union when North Korea aligned more with China, and military pressure from the U.S. leading to increased defence spending. In 1965 South Korea's rate of economic growth first exceeded North Korea's in most industrial areas, though South Korea's per capita GNP remained lower than North Korea's.

In 1979, North Korea renegotiated much of its international debt, but in 1980 it defaulted on its loans except those from Japan. By the end of 1986, hard-currency debt had reached more than US$1 billion. It also owed nearly $2 billion to communist creditors, principally the Soviet Union. The Japanese declared North Korea in default. By 2000, taking into account penalties and accrued interest, North Korea's debt was estimated at $10–12 billion. By 2012, North Korea's external debt had grown to an estimated US$20 billion despite Russia reportedly writing off about $8 billion of debt in exchange for participation in natural resources development. Besides Russia, major creditors included Hungary, the Czech Republic and Iran.

Largely because of these debt problems and because of a prolonged drought and mismanagement, North Korea's industrial growth slowed, and per capita GNP fell below that of the South. By the end of 1979 per capita GNP in North Korea was about one-third of that in the South. The causes for this relatively poor performance are complex, but a major factor is the disproportionately large percentage of GNP (possibly as much as 25%) that North Korea devotes to the military.

There were minor efforts toward relaxing central control of the economy in the 1980s that involve industrial enterprises. Encouraged by Kim Jong-il's call to strengthen the implementation of the independent accounting system (, "tongnip ch'aesanje") of enterprises in March 1984, interest in enterprise management and the independent accounting system increased, as evidenced by increasing coverage of the topic in North Korean journals. Under the system, factory managers still are assigned output targets but are given more discretion in decisions about labour, equipment, materials, and funds.

In addition to fixed capital, each enterprise is allocated a minimum of working capital from the state through the Central Bank and is required to meet operating expenses with the proceeds from sales of its output. Up to 50% of the "profit" is taxed, the remaining half being kept by the enterprise for purchase of equipment, introduction of new technology, welfare benefits, and bonuses. As such, the system provides some built-in incentives and a degree of micro-level autonomy, unlike the budget allocation system, under which any surplus is turned over to the government in its entirety.

Another innovation, the August Third People's Consumer Goods Production Movement, is centred on consumer goods production. This measure was so named after Kim Jong-il made an inspection tour of an exhibition of light industrial products held in Pyongyang on August 3, 1984. The movement charges workers to use locally available resources and production facilities to produce needed consumer goods. On the surface, the movement does not appear to differ much from the local industry programs in existence since the 1960s, although some degree of local autonomy is allowed. However, a major departure places output, pricing, and purchases outside central planning. In addition, direct sales stores were established to distribute goods produced under the movement directly to consumers. The movement is characterized as a third sector in the production of consumer goods, alongside centrally controlled light industry and locally controlled traditional light industry. Moreover, there were some reports in the mid-1980s of increasing encouragement of small-scale private handicrafts and farm markets. As of 1992, however, no move was reported to expand the size of private garden plots.

All these measures appear to be minor stop-gap measures to alleviate severe shortages of consumer goods by infusing some degree of incentives. In mid-1993, no significant moves signalling a fundamental deviation from the existing system had occurred. The reluctance to initiate reform appears to be largely political. This concern is based on the belief that economic reform will produce new interests that will demand political expression and that demands for the institutionalization of such pluralism eventually will lead to political liberalization.

Beginning in the mid-1980s and particularly around the end of the decade, North Korea slowly began to modify its rigid self-reliant policy. The changes, popularly identified as the open-door policy, included an increasing emphasis on foreign trade, a readiness to accept direct foreign investment by enacting a joint venture law, the decision to open the country to international tourism, and economic cooperation with South Korea.

The main targets of the Third Seven-Year Plan of 1987–1993 were to achieve the "Ten Long-Range Major Goals of the 1980s for the Construction of the Socialist Economy". These goals, conceived in 1980, were to be fulfilled by the end of the decade. The fact that these targets were rolled over to the end of the Third Seven-Year Plan is another indication of the disappointing economic performance during the Second Seven-Year Plan. The three policy goals of self-reliance, modernization, and scientification were repeated. Economic growth was set at 7.9% annually, lower than the previous plan. Although achieving the ten major goals of the 1980s was the main thrust of the Third Seven-Year Plan, some substantial changes have been made in specific quantitative targets. For example, the target for the annual output of steel was reduced by a third: from 15 million tons to 10 million tons. The output targets of cement and non-ferrous metals—two major export items—have been increased significantly. The June 1989 introduction of the Three-Year Plan for Light Industry as part of the Third Seven-Year Plan is intended to boost the standard of living by addressing consumer needs.

The Third Seven-Year Plan gave a great deal of attention to developing foreign trade and joint ventures, the first time a plan has addressed these issues. By the end of 1991, however, two years before the termination of the plan, no quantitative plan targets were made public, an indication that the plan has not fared well. The diversion of resources to build highways, theatres, hotels, airports, and other facilities to host the Thirteenth World Festival of Youth and Students in July 1989 must have had a negative impact on industrial and agricultural development, although the expansion and improvement of social infrastructure have resulted in some long-term economic benefits.

Although general economic policy objectives are decided by the Central People's Committee (CPC), it is the task of the State Planning Committee to translate the broad goals into specific annual and long-term development plans and quantitative targets for the economy as a whole, as well as for each industrial sector and enterprise. Under the basic tenets of the 1964 reforms, the planning process is guided by the principles of "unified planning" (, "ilwŏnhwa") and of "detailed planning" (, "saebunhwa").

Under "unified planning", regional committees are established in each province, city, and county to systematically coordinate planning work. These committees do not belong to any regional organization and are directly supervised by the State Planning Committee. As a result of a reorganization in 1969, they are separated into provincial planning committees, city/county committees, and enterprise committees (for large-scale enterprises).

The planning committees, under the auspices of the State Planning Committee, coordinate their work with the planning offices of the economy-related government organizations the corresponding regional and local areas. The system attempts to enable the regional planning staff to better coordinate with economic establishments in their areas, which are directly responsible to them in planning, as well as communicating directly with staff at the CPC. "Detailed planning" seeks to construct plans with precision and scientific methods based on concrete assessment of the resources, labour, funds, plant capacities, and other necessary information.

There are four stages in drafting the final national economic plan.

The plan then becomes legal and compulsory. Frequent directives from the central government contain changes in the plan targets or incentives for meeting the plan objectives.

Although the central government is most clearly involved in the formulation and evaluation of the yearly and long-term plans, it also reviews summaries of quarterly or monthly progress. Individual enterprises divide the production time into daily, weekly, ten-day, monthly, quarterly, and annual periods. In general, the monthly plan is the basic factory planning period.

The success of an economic plan depends on the quality and detail of information received, the establishment of realistic targets, coordination among sectors, and correct implementation. High initial growth during the Three-Year Plan and, to a lesser extent, during the Five-Year Plan contributed to a false sense of confidence among the planners. Statistical over reporting—an inherent tendency in an economy where rewards lie in fulfilling the quantitative targets, particularly when the plan target year approaches—leads to overestimation of economic potential, poor product quality, and eventually to plan errors. Inefficient use of plants, equipment, and raw materials add to planning errors. Lack of coordination in planning and production competition among sectors and regions cause imbalances and disrupt input-output relationships. The planning reforms in 1964 were supposed to solve these problems, but the need for correct and detailed planning and strict implementation of plans was so great that their importance was emphasized in the report unveiling the Second Seven-Year Plan, indicating that planning problems persisted in the 1980s.

In the mid-1990s North Korea abandoned firm directive planning, and multi-year plans became more of a long-term economic strategy.

The "Ch'ŏngsan-ni Method" () of management was born out of Kim Il-sung's February 1960 visit to the Ch'ŏngsan-ni Cooperative Farm in South P'yŏngan Province. Influenced by Mao Zedong's Great Leap Forward Policy, Kim and other members of the KWP Central Committee offered "on-the-spot guidance" (, "hyŏnji chido") and spent two months instructing and interacting with the workers. The avowed objective of this new method is to combat "bureaucratism" and "formalism" in the farm management system.

The leadership claimed that farm workers were unhappy and produced low output because low-ranking functionaries of the Workers' Party of Korea (who expounded abstract Marxist theories and slogans) were using tactics that failed to motivate. To correct this, the leadership recommended that the workers receive specific guidance in solving production problems and be promised readily available material incentives. The Ch'ŏngsan-ni Method called for high-ranking party officials, party cadres, and administrative officials to emulate Kim Il-sung by making field inspections. The system provided opportunities for farmers to present their grievances and ideas to leading cadres and managers.

Perhaps more important than involving administrative personnel in on-site inspections was the increased use of material incentives, such as paid vacations, special bonuses, honorific titles, and monetary rewards. In fact, the Ch'ŏngsan-ni Method appeared to accommodate almost any expedient to spur production. The method, subsequently, was undercut by heavy-handed efforts to increase farm production and amalgamate farms into ever-larger units. Actual improvement in the agricultural sector began with the adoption of the subteam contract system as a means of increasing peasant productivity by adjusting individual incentives to those of the immediate, small working group. Thus the increasing scale of collective farms was somewhat offset by the reduction in the size of the working unit. "On-the-spot guidance" by high government functionaries, however, continued in the early 1990s, as exemplified by Kim Il-sung's visits to such places as the Wangjaesan Cooperative Farm in Onsŏng County and the Kyŏngsŏn Branch Experimental Farm of the Academy of Agricultural Sciences between August 20 and 30, 1991. Kim Jong-il carried on the tradition, despite having refused to do so before, and even expanded it to the Korean People's Army. Today Kim Jong-un continues the practices of the method.

The industrial management system developed in three distinct stages. The first was a period of enterprise autonomy that lasted until December 1946. The second stage was a transitional system based on local autonomy, with each enterprise managed by the enterprise management committee under the direction of the local people's committee. This system was replaced by the "one-man management system" (), with management patterned along Soviet lines as large enterprises were nationalized and came under central control. The third stage, the "Taean" Work System (, "Taeanŭi saŏpch'e"), was introduced in December 1961 as an application and refinement of agricultural management techniques to industry. The Taean industrial management system grew out of the "Ch'ŏngsan-ni" Method.

The highest managerial authority under the Taean system is the party committee. Each committee has approximately 25 to 35 members elected from the ranks of managers, workers, engineers, and the leadership of "working people's organizations" at the factory. A smaller "executive committee", about one-quarter the size of the regular committee, has practical responsibility for day-to-day plant operations and major factory decisions. The most important staff members, including the party committee secretary, factory manager, and chief engineer, make up its membership. The system focuses on co-operation among workers, technicians, and party functionaries at the factory level.

Each factory has two major lines of administration, one headed by the manager, the other by the party committee secretary. A chief engineer and his or her assistants direct a general staff in charge of all aspects of production, planning, and technical guidance. Depending on the size of the factory, varying numbers of deputies oversee factory logistics, marketing, and workers' services. The supply of materials includes securing, storing, and distributing all materials for factory use, as well as storing finished products and shipping them from the factory.

Deputies are assigned workers to their units and handle factory accounts and payroll. Providing workers' services requires directing any farming done on factory lands, stocking factory retail shops, and taking care of all staff amenities. Deputies in charge of workers' services are encouraged to meet as many of the factory's needs as possible using nearby agricultural cooperatives and local industries.

The secretary of the party committee organizes all political activities in each of the factory party cells and attempts to ensure loyalty to the party's production targets and management goals. According to official claims, all management decisions are arrived at by consensus among the members of the party committee. Given the overwhelming importance of the party in the country's affairs, it seems likely that the party secretary has the last say in any major factory disputes.

The Taean system heralded a more rational approach to industrial management than that practised previously. Although party functionaries and workers became more important to management under the new system, engineers and technical staff received more responsibility in areas where their expertise could contribute the most. The system recognizes the importance of material as well as "politico-moral" incentives for managing the factory workers. The "internal accounting system", a spin-off of the "independent accounting system", grants bonuses to work teams and workshops that use raw materials and equipment most efficiently. These financial rewards come out of enterprise profits.

A measure of the success of the Taean work system is its longevity and its continued endorsement by the leadership. In his 1991 New Year's address marking the 13th anniversary of the creation of the system, Kim Il-sung said that the

Taean work system is the best system of economic management. It enables the producer masses to fulfill their responsibility and role as masters and to manage the economy in a scientific and rational manner by implementing the mass line in economic management, and by combining party leadership organically with administrative, economic, and technical guidance.

Parallel to management techniques such as the Ch'ŏngsan-ni Method and the Taean work system, which were designed to increase output in more normalized and regularized operations of farms and enterprises, the leadership continuously resorts to exhortations and mass campaigns to motivate the workers to meet output targets. The earliest and the most pervasive mass production campaign was the Ch'ŏllima Movement. Introduced in 1958 and fashioned after China's Great Leap Forward (1958–1960), the Ch'ŏllima Movement organized the labour force into work teams and brigades to compete at increasing production. The campaign was aimed at industrial and agricultural workers and at organizations in education, science, sanitation and health, and culture.

In addition to work teams, units eligible for Ch'ŏllima citations included entire factories, factory workshops, and such self-contained units as a ship or a railroad station. The "socialist emulation" among the industrial sectors, enterprises, farms, and work teams under the Ch'ŏllima Movement frantically sought to complete the First Five-Year Plan (1957–1960) but instead created chaotic disruptions in the economy. This made it necessary to set aside 1959 as a "buffer year" to restore balance in the economy.

Although the Ch'ŏllima Movement was replaced in the early 1960s by the Ch'ŏngsan-ni Method and the Taean Work System, the regime's reliance on mass campaigns continued into the early 1990s. Campaigns conducted after the Ch'ŏllima to speed battles toward the end of a period (such as a month, a year, or an economic plan) to reach production targets to carry out the economic goals of the decade.

Following the collapse of the Soviet Union in 1991, the principal source of external support, North Korea announced in December 1993 a three-year transitional economic policy placing primary emphasis on agriculture, light industry, and foreign trade. However, lack of fertilizer, natural disasters, and poor storage and transportation practices the country fell more than a million tons per year short of grain self-sufficiency. Moreover, lack of foreign exchange to purchase spare parts and oil for electricity generation left many factories idle.

The shortage of foreign exchange because of a chronic trade deficit, a large foreign debt, and dwindling foreign aid has constrained economic development. In addition, North Korea has been diverting scarce resources from developmental projects to defence; it spent more than 20% of GNP on defence toward the end of the 1980s, a proportion among the highest in the world. These negative factors, compounded by the declining efficiency of the central planning system and the failure to modernize the economy, have slowed the pace of growth since the 1960s. The demise of the communist regimes in the Soviet Union and East European countries—North Korea's traditional trade partners and benefactors—has compounded the economic difficulties in the early 1990s.

Economically, the collapse of the Soviet Union and the end of Soviet support to North Korean industries caused a contraction of the North Korea's economy by 25% during the 1990s. While, by some accounts, North Korea had a higher per capita income than South Korea in the 1970s, by 2006 its per capita income was estimated to be only $1108, one seventeenth that of South Korea.

Experimentation in small scale entrepreneurship took place from 2009 to 2013, and although there continue to be legal uncertainties this has developed into a significant sector. By 2016 economic liberalisation had progressed to the extent that both locally-responsible and state industrial enterprises gave the state 20% to 50% of their output, selling the remainder to buy raw materials with market-based prices in akin to a free market.

In 2014 the Enterprise Act was amended to allow state-owned enterprise managers to engage in foreign trade and joint ventures, and to accept investment from non-government domestic sources. Under the new rules the enterprise director became more like the western chief executive officer, and the chief engineer had an operational role more like a western chief operating officer. As of 2017 it was unclear if the Taean Work System (described above) still in practice operated to give local people's committees much influence.

In 2017 Dr. Mitsuhiro Mimura, Senior Research Fellow at Japan's Economic Research Institute for Northeast Asia, who has visited North Korea 45 times, described it as the "poorest advanced economy in the world", in that while having comparatively low GDP, it had built a sophisticated production environment. He described the recent rise of entrepreneurial groups through "socialist cooperation", where groups of individuals could start small enterprises as cooperative groups. Managers in state-owned industries or farms were also free to sell or trade production beyond state plan targets, providing incentives to increase production. Managers could also find investment for expansion of successful operations, in a process he called "socialist competition". A state plan was still the basis for production, but was more realistic leaving room for excess production.

The state budget is a major government instrument in carrying out the country's economic goals. Expenditures represented about three-quarters of GNP in the mid-1980s, the allocation of which reflected the priorities assigned to different economic sectors. Taxes were abolished in 1974 as "remnants of an antiquated society". This action, however, was not expected to have any significant effect on state revenue because the overwhelming proportion of government funds—an average of 98.1% during 1961–1970—was from turnover (sales) taxes, deductions from profits paid by state enterprises, and various user fees on machinery and equipment, irrigation facilities, television sets, and water.

In order to provide a certain degree of local autonomy as well as to lessen the financial burden of the central government, a "local budget system" was introduced in 1973. Under this system, provincial authorities are responsible for the operating costs of institutions and enterprises not under direct central government control, such as schools, hospitals, shops, and local consumer goods production. In return, they are expected to organize as many profitable ventures as possible and to turn over profits to the central government.

Around December of every year, the state budget for the following calendar year is drafted, subject to revision around March. Typically, total revenue exceeds expenditure by a small margin, with the surplus carried over to the following year. The largest share of state expenditures goes to the "people's economy", which averaged 67.3% of total expenditures between 1987 and 1990, followed in magnitude by "socio-cultural", "defense", and "administration".

Defense spending, as a share of total expenditures, has increased significantly since the 1960s: from 3.7% in 1959 to 19% in 1960, and, after averaging 19.8% between 1961 and 1966, to 30.4% in 1967. After remaining around 30% until 1971, the defense share decreased abruptly to 17% in 1972, and continued to decline throughout the 1980s. Officially, in both 1989 and 1990 the defense share remained at 12%, and for 1991 it was 12.3% with 11.6% planned for 1992. The declining trend was consistent with the government's announced intentions to stimulate economic development and increase the social benefits. However, Western experts have estimated that actual military expenditures are higher than budget figures indicate.

In the 1999 budget, expenditures for the farming and power sectors were increased by 15% and 11%, respectively, compared with those of 1998.

In the 2007 budget, it was estimated an increase in revenue at 433.2bn won ($3.072bn, $1 = 141 won). In 2006, 5.9% were considered the public revenue, whereas this year, this figure was raised to 7.1%.

North Korea claims that it is the only state in the world that does not levy taxes. Taxes were abolished beginning on April 1, 1974.

Since 2003, North Korean authorities issue government bonds called The "People's Life Bonds", and promoted the slogan "Buying bonds is patriotic".

North Korea sold bonds internationally in the late 1970s for 680 million Deutsche marks and 455 million Swiss francs. North Korea defaulted on these bonds by 1984, although the bonds remain traded internationally on speculation that the country would eventually perform on the obligations.

"The Sydney Morning Herald" reported that Kim’s previous propaganda was changed into patriotism and economy, and in improving the relationship between China, South Korea, and the United States. The state-run television promoted a song of praise to the National flag by airing videos with images that included the flag being raised September 2018, during mass games events, marking North Korea's 70th anniversary. In the video, brief images of troops, fighter jets releasing blue, red, and white smoke, scattered pictures of civilians, new high-rise apartments in the capital, fireworks displays, and even students in their school uniforms can all be seen at the same event.

The "South China Morning Post", in a 2019 article, stated that already there is also some economical and cultural revolution happening recently within North Korea itself. It started in earnest in February 2018, during the Pyeongchang Winter Olympic Games, when top musicians from North Korea were sent to perform in South Korea. This included a female quintet who performed in black shorts and red tops. After two months, Supreme Leader Kim Jong-un saw the performance of South Korean girl group, Red Velvet. This is the first ever K-Pop show to be held in Pyongyang. The North Korean musicians that performed in South Korea were highly praised for their performance that leader Kim decided to send them to Beijing for another goodwill tour in January, 2019. Part of the revolution was the introduction of other cultures, including Western, which was previously believed to be vulgar and quite corrupt in the past, but is now slowly making its way to the North Korean people. Second-hand Harry Potter books can now be read at the National Library, and Bollywood films like the "Three Idiots" had just had a run in their cinemas. The changes have also found their way to the economic sector with factories that are also producing products that are associated more with the West, like Air Jordan shoes, for national consumption.
Per the amendments made to the Constitution in 2019, the former economic methods of management, Ch'ŏngsan-ni in agriculture and Taean in the industries, were now phased out altogether.

North Korea also implements planned economy in industry. The government will provide fuel and materials for the factory, and the factory will manufacture the required products and quantities according to the government's requirements.

North Korea's self-reliant development strategy assigned top priority to developing heavy industry, with parallel development in agriculture and light industry. This policy was achieved mainly by giving heavy industry preferential allocation of state investment funds. More than 50% of state investment went to the industrial sector during the 1954–1976 period (47.6%, 51.3%, 57.0%, and 49.0%, respectively, during the Three-Year Plan, Five-Year Plan, First Seven-Year Plan, and Six-Year Plan). As a result, gross industrial output grew rapidly.

As was the case with the growth in national output, the pace of growth has slowed markedly since the 1960s. The rate declined from 41.7% and 36.6% a year during the Three-Year Plan and Five-Year Plan, respectively, to 12.8%, 16.3%, and 12.2%, respectively, during the First Seven Year Plan, Six-Year Plan, and Second Seven-Year Plan. As a result of faster growth in industry, that sector's share in total national output increased from 16.8% in 1946 to 57.3% in 1970. Since the 1970s, industry's share in national output has remained relatively stable. From all indications, the pace of industrialization during the Third Seven-Year Plan up to 1991 is far below the planned rate of 9.6%. In 1990 it was estimated that the industrial sector's share of national output was 56%.

Industry's share of the combined total of gross agricultural and industrial output climbed from 28% in 1946 to well over 90% in 1980. Heavy industry received more than 80% of the total state investment in industry between 1954 and 1976 (81.1%, 82.6%, 80%, and 83%, respectively, during the Three-Year Plan, Five-Year Plan, First Seven-Year Plan, and Six-Year Plan), and was overwhelmingly favored over light industry.

North Korea claims to have fulfilled the Second Seven-Year Plan (1978–1984) target of raising the industrial output in 1984 to 120% of the 1977 target, equivalent to an average annual growth rate of 12.2%. Judging from the production of major commodities that form the greater part of industrial output, however, it is unlikely that this happened. For example, the increase during the 1978–1984 plan period for electric power, coal, steel, metal-cutting machines, tractors, passenger cars, chemical fertilizers, chemical fibers, cement, and textiles, respectively, was 78%, 50%, 85%, 67%, 50%, 20%, 56%, 80%, 78%, and 45%.
Raw materials were in short supply and so were energy and hard currency. Infrastructure decayed and machinery became obsolete. Unlike other socialist countries in the Eastern Europe, North Korea kept planning in a highly centralized manner and refused to liberalize economic management.
In the mid-1980s, the speculation that North Korea would emulate China in establishing Chinese-style special economic zones was flatly denied by then deputy chairman of the Economic Policy Commission Yun Ki-pok (Yun became chairman as of June 1989). China's special economic zones typically are coastal areas established to promote economic development and the introduction of advanced technology through foreign investment. Investors are offered preferential tax terms and facilities. The zones, which allow greater reliance on market forces, have more decision making power in economic activities than do provincial-level units. Over the years, China has tried to convince the North Korean leadership of the advantages of these zones by giving tours of the various zones and explaining their values to visiting high-level officials.

In April 1982, Kim Il-sung announced a new economic policy giving priority to increased agricultural production through land reclamation, development of the country's infrastructure—especially power plants and transportation facilities—and reliance on domestically produced equipment. There also was more emphasis on trade.

In September 1984, North Korea promulgated a joint venture law to attract foreign capital and technology. The new emphasis on expanding trade and acquiring technology was not, however, accompanied by a shift in priorities away from support of the military. In 1991, North Korea announced the creation of a Special Economic Zone (SEZ) in the northeast regions of Rason (Rason Special Economic Zone) and Ch'ŏngjin. Investment in this SEZ has been slow in coming. Problems with infrastructure, bureaucracy, uncertainties about the security of investments, and viability have hindered growth and development. Nevertheless, thousands of small Chinese businesses had set up profitable operations in North Korea by 2011.

A government research center, the Korea Computer Center, was set up in 1990, starting the slow development of an information technology industry.

In 2013 and 2014, the State Economic Development Administration announced a number of smaller special economic zones covering export handling, mineral processing, high technology, gaming and tourism.

The most successful export industry is the garment industry. Production is by a North Korean firm for a European or other foreign partner, by a Chinese firm operating in North Korea with a North Korean partner, or by North Korean workers working in Chinese or other foreign factories. Wages are the lowest in northeastern Asia.

The North Korean motor vehicle production establishes military, industrial and construction goals, with private car ownership by citizens remaining on low demand. Having Soviet origins (the subsequent practice of cloning foreign specimens, and a recent automobile joint-venture), North Korea has developed a wide-range automotive industry with production of all types of vehicles. The basis for production is in urban and off-road minis; luxury cars; SUV cars; small, medium, heavy, and super-heavy cargo; construction and off-road trucks; minibuses/minivans, coach buses, civilian and articulated buses, trolleybuses, and trams. However, North Korea produces far fewer vehicles than its production capability due to the ongoing economic crisis. North Korea has not joined or collaborated with the OICA, or with any other automotive organization, so any critical information about its motor vehicle industry is limited.

The energy sector is one of the most serious bottlenecks in the North Korean economy. Since 1990, the supply of oil, coal, and electricity declined steadily, and seriously affected all sectors of the economy. Crude oil was formerly imported by pipeline at "friendship prices" from the former Soviet Union or China, but the withdrawal of Russian concessions and the reduction of imports from China brought down annual imports from about in 1988 to less than by 1997. As the imported oil was refined for fuels for transportation and agricultural machinery, a serious cutback in oil imports caused critical problems in transportation and agriculture.

According to statistics compiled by the South Korean agency Statistics Korea based on International Energy Agency (IEA) data, per capita electricity consumption fell from its peak in 1990 of 1247 kilowatt hours to a low of 712 kilowatt hours in 2000. It slowly rose since then to 819 kilowatt hours in 2008, a level below that of 1970.

North Korea has no coking coal, but has substantial reserves of anthracite in Anju, Aoji (Ŭndŏk), and other areas. Coal production peaked at 43 million tons in 1989 and steadily declined to 18.6 million tons in 1998. Major causes of coal shortages include mine flooding, and outdated mining technology. As coal was used mainly for industry and electricity generation, decrease in coal production caused serious problems in industrial production and electricity generation. Coal production may not necessarily increase significantly until North Korea imports modern mining technology.

Electricity generation of North Korea peaked in 1989 at about 30 TWh. There were seven large hydroelectric plants in the 1980s. Four were along the Yalu River, built with Chinese aid, and supplying power to both countries. In 1989, 60% of electricity generation was hydroelectric and 40% fossil fueled, mostly coal-fired.

In 1997, coal accounted for more than 80% of primary energy consumption and hydro power more than 10%. Net imports of coal represented only about 3% of coal consumption. Hydroelectric power plants generated about 65% of North Korea's electricity and coal-fired thermal plants about 35% in 1997. However, with only 20% of the per capita electricity generation of Japan, North Korea suffered from chronic supply shortages. Coal exports to China currently account for a major portion of North Korea's revenue.

Some hydroelectric facilities were believed to be out of operation due to damage from major flooding in 1995. Coal-fired power plants were running well under capacity, due in part to a serious decline in coal supply and in part to problems with transportation of coal. The electricity supply steadily declined and was 17 TWh in 1998. Since electricity generated needed to be doubled just to return to the 1989 level, power shortages continued until coal production could increase substantially and generating equipment is refurbished. Transmission losses were reported to be around 30%.

Construction has been an active sector in North Korea. This was demonstrated not only through large housing programmes, of which most were visible in the high-rise apartment blocks in Pyongyang, but also in the smaller modern apartment complexes widespread even in the countryside. These are dwarfed in every sense by "grand monumental edifices". The same may apply even to apparently economically useful projects such as the Nampo Dam, which cost US$4bn.

The years of economic contraction in the 1990s slowed this sector as it did others; the shell of the 105-story Ryugyŏng Hotel towered unfinished on Pyongyang's skyline for over a decade. The Bank of Korea claims that construction's share of GDP fell by almost one-third between 1992 and 1994, from 9.1% to 6.3%. This accords with a rare official figure of 6% for 1993, when the sector was said to have employed 4.2% of the labour force. However, the latter figure excludes the Korean People's Army, which visibly does much of the country's construction work.

Since about 2012, when 18 tower blocks were built in Pyongyang, a construction boom has taken place in Pyongyang. Major projects include the Mansudae People's Theatre (2012), Munsu Water Park (2013), the modernisation of Pyongyang Sunan International Airport (2015) and the Science and Technology Center (2015).

The Central Bank of North Korea, under the Ministry of Finance, has a network of 227 local branches. Several reissues of banknotes in recent years suggest that citizens are inclined to hoard rather than bank any savings that they make from their incomes; reportedly they now also prefer foreign currency. At least two foreign aid agencies have recently set up microcredit schemes, lending to farmers and small businesses.

In late 2009, North Korea revalued its currency, effectively confiscating all privately held money above the equivalent of US$35 per person. The revaluation effectively wiped out the savings of many North Koreans. Days after the revaluation the won dropped 96% against the United States dollar. Pak Nam-gi, the director of the Planning and Finance Department of North Korea's ruling Workers' Party, was blamed for the disaster and later executed in 2010.
In 2004 and 2006 laws were passed to codify rules for savings and commercial banking. However it was not until 2012 that North Korean banks started to seriously compete for retail customers. Competing electronic cash cards have become widely accepted in Pyongyang and other cities, but are generally not linked to bank accounts. North Korean banks have introduced retail products which permit a mobile phone app to make payments and top-ups.

As of May 2013, the Chinese banks, China Merchants Bank, Industrial and Commercial Bank of China, China Construction Bank, and Agricultural Bank of China, stopped "all cross-border cash transfers, regardless of the nature of the business" with North Korea. The Bank of China, the China's primary institution for foreign exchange transactions, said, on May 14, 2013, that "it had closed the account of Foreign Trade Bank, North Korea's main foreign exchange bank". However, "smaller banks based in northeastern China across the border from North Korea said it was still handling large-scale cross-border transfers." For example, the Bank of Dalian branch in Dandong was still doing transfers to North Korea.

Until the early 2000s the official retail sector was mainly state-controlled, under the direction of the People's Services Committee. Consumer goods were few and of poor quality, with most provided on a ration basis. There were state-run stores and direct factory outlets for the masses, and special shops with luxuries for the elite—as well as a chain of hard-currency stores (a joint venture with the association of pro-Pyongyang Korean residents in Japan, the Ch'ongryŏn), with branches in large cities.

In 2002 and in 2010, private markets were progressively legalized, mostly for food sales. As of 2013, urban and farmer markets were held every 10 days, and most urban residents lived within 2 km of a market.

In 2012, the third large shopping mall in Pyongyang, the Kwangbok Area Shopping Center, opened. In 2014 the construction of another large shopping mall started. As of 2017, these malls sold competing brands of goods, for example at least ten different kinds of toothpaste were being sold.

In 2017, the Korea Institute for National Unification estimated there were 440 government-approved markets employing about 1.1 million people.

North Korea's sparse agricultural resources limit agricultural production. Climate, terrain, and soil conditions are not particularly favorable for farming, with a relatively short cropping season. Only about 17% of the total landmass, or approximately , is arable, of which is well suited for cereal cultivation; the major portion of the country is rugged mountain terrain.

The weather varies markedly according to elevation, and lack of precipitation, along with infertile soil, makes land at elevations higher than 400 meters unsuitable for purposes other than grazing. Precipitation is geographically and seasonally irregular, and in most parts of the country as much as half the annual rainfall occurs in the three summer months. This pattern favors the cultivation of paddy rice in warmer regions that are outfitted with irrigation and flood control networks. Rice yields are 5.3 tonnes per hectare, close to international norms. In 2005, North Korea was ranked by the FAO as an estimated 10th in the production of fresh fruit and as an estimated 19th in the production of apples.

Farming is concentrated in the flatlands of the four west coast provinces, where a longer growing season, level land, adequate rainfall, and good irrigated soil permit the most intensive cultivation of crops. A narrow strip of similarly fertile land runs through the eastern seaboard Hamgyŏng provinces and Kangwŏn Province, but the interior provinces of Chagang and Ryanggang are too mountainous, cold, and dry to allow much farming. The mountains contain the bulk of North Korea's forest reserves while the foothills within and between the major agricultural regions provide lands for livestock grazing and fruit tree cultivation.

Since self-sufficiency remains an important pillar of North Korean ideology, self-sufficiency in food production is deemed a worthy goal. Another aim of government policies—to reduce the gap between urban and rural living standards—requires continued investment in the agricultural sector. The stability of the country depends on steady, if not rapid, increases in the availability of food items at reasonable prices. In the early 1990s, there were severe food shortages.
The most far-reaching statement on agricultural policy is embodied in Kim Il-sung's 1964 "Theses on the Socialist Agrarian Question in Our Country", which underscores the government's concern for agricultural development. Kim emphasized technological and educational progress in the countryside as well as collective forms of ownership and management. As industrialization progressed, the share of agriculture, forestry, and fisheries in the total national output declined from 63.5% and 31.4%, respectively, in 1945 and 1946, to a low of 26.8% in 1990. Their share in the labor force also declined from 57.6% in 1960 to 34.4% in 1989.

In the 1990s, the decreasing ability to carry out mechanized operations (including the pumping of water for irrigation), as well as lack of chemical inputs, was clearly contributing to reduced yields and increased harvesting and post-harvest losses.

Incremental improvements in agricultural production have been made since the late 1990s, bringing North Korea close to self-sufficiency in staple foods by 2013. In particular, rice yields have steadily improved, though yields on other crops have generally not improved. The production of protein foods remains inadequate. Access to chemical fertilizer has declined, but the use of compost and other organic fertilizer has been encouraged.

North Korean fisheries export seafood, primarily crab, to Dandong, Liaoning, illicitly. Crabs, clams and conches from the Yellow Sea waters of North Korea are popular in China, possibly because the less salty water improves taste.

Since the 1950s, a majority of North Koreans have received their food through the public distribution system (PDS). The PDS requires farmers in agricultural regions to hand over a portion of their production to the government and then reallocates the surplus to urban regions, which cannot grow their own foods. About 70% of the North Korean population, including the entire urban population, receives food through this government-run system.

Before the floods, recipients were generally allotted 600–700 grams per day while high officials, military men, heavy laborers, and public security personnel were allotted slightly larger portions of 700–800 grams per day. As of 2013, the target average distribution was 573 grams of cereal equivalent per person per day, but varied according to age, occupation, and whether rations are received elsewhere (such as school meals). However, as of 2019, this number has been reduced to 312 grams per day according to an investigation conducted by the United Nations between March 29 and April 12.

Decreases in production affected the quantity of food available through the public distribution system. Shortages were compounded when the North Korean government imposed further restrictions on collective farmers. When farmers, who had never been covered by the PDS, were mandated by the government to reduce their own food allotments from 167 kilograms to 107 kilograms of grain per person each year, they responded by withholding portions of the required amount of grain. Famine refugees reported that the government decreased PDS rations to 150 grams in 1994 and to as low as 30 grams by 1997. It was further reported that the PDS failed to provide any food from April to August 1998 (the "lean" season) as well as from March to June 1999. In January 1998, the North Korean government publicly announced that the PDS would no longer distribute rations and that families needed to somehow procure their own food supplies. By 2005, the PDS was only supplying households with approximately one half of an absolute minimum caloric need. By 2008, the system had significantly recovered, and, from 2009 to 2013, daily per person rations averaged at 400 grams per day for much of the year, though in 2011 it dropped to 200 grams per day from May to September.

It is estimated that in the early 2000s, the average North Korean family drew some 80% of its income from small businesses that were technically illegal (though unenforced) in North Korea. In 2002 and in 2010, private markets were progressively legalized. As of 2013, urban and farmer markets were held every 10 days, and most urban residents lived within 2 km of a market, with markets having an increasing role in obtaining food.

From 1994 to 1998, North Korea suffered a famine. Since North Korea is a closed country, the number of specific deaths in the incident is difficult to know. According to different literature, the starved or malnourished death toll is estimated to be between 240,000 and 480,000. Since 1998 there has been a gradual recovery in agriculture production, which by 2013 brought North Korea back close to self-sufficiency in staple foods. However, as of 2013, most households have borderline or poor food consumption, and consumption of protein remains inadequate.

In the 1990s, the North Korean economy saw stagnation turning into crisis. Economic assistance received from the Soviet Union and China was an important factor of its economic growth. Upon its collapse in 1991, the Soviet Union withdrew its support and demanded payment in hard currency for imports. China stepped in to provide some assistance and supplied food and oil, most of it reportedly at concessionary prices. The North Korean economy was undermined and its industrial output began to decline in 1990. Deprived of industrial inputs, including fertilizers, pesticides, and electricity for irrigation, agricultural output also started to decrease even before North Korea had a series of natural disasters in the mid-1990s. This evolution, combined with a series of natural disasters including record floods in 1995, caused one of the worst economic crises in North Korea's history. Other causes of this crisis were high defense spending (about 25% of GDP) and bad governance.

In December 1991, North Korea established a "zone of free economy and trade" to include the northeastern port cities of Unggi (Sŏnbong), Ch'ŏngjin, and Najin. The establishment of this zone also had ramifications on the questions of how far North Korea would go in opening its economy to the West and to South Korea, the future of the development scheme for the Tumen River area, and, more important, how much North Korea would reform its economic system.

North Korea announced in December 1993 a three-year transitional economic policy placing primary emphasis on agriculture, light industry, and foreign trade. However, lack of fertilizer, natural disasters, and poor storage and transportation practices have left the country more than a million tons per year short of grain self-sufficiency. Moreover, lack of foreign exchange to purchase spare parts and oil for electricity generation left many factories idle.

The 1990s famine paralyzed many of the Stalinist economic institutions. The government pursued Kim Jong-il's "Songun" policy, under which the military is deployed to direct production and infrastructure projects. As a consequence of the government's policy of establishing economic self-sufficiency, the North Korean economy has become increasingly isolated from that of the rest of the world, and its industrial development and structure do not reflect its international competitiveness. Domestic firms are shielded from international as well as domestic competition; the result is chronic inefficiency, poor quality, limited product diversity, and underutilization of plants. This protectionism also limits the size of the market for North Korean producers, which prevents taking advantage of economies of scale.

The food shortage was primarily precipitated by the loss of fuel and other raw materials imports from China and the Soviet Union which had been essential to support an energy intensive and energy inefficient farming system. Following the collapse of the Soviet Union, the former concessional trade relationships which benefited the North Korea were not available. The three flood and drought years between 1994 and 1996 only served to complete the collapse of the agriculture sector.
In 2004, more than half (57%) of the population did not have enough food to stay healthy. 37% of children had their growth stunted and of mothers severely lacked nutrition.

In 2006, the World Food Program (WFP) and FAO estimated a requirement of 5.3 to 6.5 million tons of grain when domestic production fulfilled only 3.825 million tons. The country also faces land degradation after forests stripped for agriculture resulted in soil erosion. In 2008, a decade after the worst years of the famine, total production was 3.34 million tons (grain equivalent) compared with a need of 5.98 million tons. Thirty seven percent of the population was deemed to be insecure in food access. Weather continued to pose challenges every year, but overall food production grew gradually, and by 2013, production had increased to the highest level since the crisis, to 5.03 million tons cereal equivalent, against a minimum requirement of 5.37 MMT.

In 2014 North Korea had an exceptionally good harvest, 5.08 million tonnes of cereal equivalent, almost sufficient to feed the entire population. While food production had recovered significantly since the hardest years of 1996 and 1997, the recovery was fragile, subject to adverse weather and year to year economic shortages. Distribution was uneven with the Public Distribution System largely ineffective. Any shortfall between production and need could be easily met by government funded imports, should the decision to make those purchases be made. North Korea now has in most years lower malnutrition levels than in some richer Asian countries.

According to a 2012 report by South Korea-based North Korea Resource Institute (NKRI), North Korea has substantial reserves of iron ore, coal, limestone, and magnesite. In addition, North Korea is thought to have tremendous potential rare metal resources, which have been valued in excess of US$6 trillion.

It is the world's 18th largest producer of iron and zinc, and has the 22nd largest coal reserves in the world. It is also the 15th largest fluorite producer and 12th largest producer of copper and salt in Asia. Other major natural resources in production include lead, tungsten, graphite, magnesite, gold, pyrites, fluorspar, and hydropower.

In 2015, North Korea exported 19.7 million tonnes of coal, worth $1.06 billion, much of it to China.
In 2016 it was estimated that coal shipments to China accounted for about 40% of exports.

However, starting from February 2017 China suspended all North Korean coal imports, although according to China overall trade with North Korea increased.

North Korea has a proficient information technology industry. In 2018, a technological exhibition unveiled a new wi-fi service called Mirae ("Future"), which allowed mobile devices to access the intranet network in Pyongyang. The exhibition also showcased a home automation system using speech recognition in Korean.

North Korea's cartoon animation studios such as SEK Studio sub-contract work from South Korean animation studios. Mansudae Overseas Projects builds monuments around the world.

North Korea's economy has been unique in its elimination of markets. By the 1960s, market elements had been suppressed almost completely. Almost all items, from food to clothes, have traditionally been handed out through a public distribution system, with money only having a symbolic meaning. Ratios of food depend on hierarchy in the system, wherein the positions seem to be semi-hereditary. Until the late 1980s, peasants were not allowed to cultivate private garden plots.

Since the government is the dominant force in the development and management of the economy, bureaus and departments have proliferated at all administrative levels. There are fifteen committees—such as the agricultural and state planning committees—one bureau, and twenty departments under the supervision of the Cabinet; of these, twelve committees—one bureau, and sixteen departments are involved in economic management. In the early 1990s, several vice premiers of the then State Administration Council supervised economic affairs. Organizations undergo frequent reorganization. Many of these agencies have their own separate branches at lower levels of government while others maintain control over subordinate sections in provincial and county administrative agencies.

Around 1990, with the collapse of the Soviet Union, restrictions on private sales, including grain, ceased to be enforced. It is estimated that in the early 2000s, the average North Korean family drew some 80% of its income from small businesses that were technically illegal (though unenforced) in North Korea. In 2002, and in 2010, private markets were progressively legalized. As of 2013, urban and farmer markets were held every 10 days, and most urban residents lived within 2 km of a market.

In 2014, North Korea announced the "May 30th measures". These planned to give more freedom to farmers, allowing them to keep 60% of their produce. Also enterprise managers would be allowed to hire and fire workers, and decide whom they do business with and where they buy raw materials and spare parts. Some reports suggest that these measures would allow nominally state-run enterprises to be run on capitalist lines like those on China.

North Korea, one of the world's most centrally planned and isolated economies, faces desperate economic conditions. Industrial capital stock is nearly beyond repair as a result of years of underinvestment and shortages of spare parts. Industrial and power output have declined in parallel.
During what North Korea called the "peaceful construction" period before the Korean War, the fundamental task of the economy was to overtake the level of output and efficiency attained toward the end of the Japanese occupation; to restructure and develop a viable economy reoriented toward the communist-bloc countries; and to begin the process of socializing the economy. Nationalization of key industrial enterprises and land reform, both of which were carried out in 1946, laid the groundwork for two successive one-year plans in 1947 and 1948, respectively, and the Two-Year Plan of 1949–50. It was during this period that the piece-rate wage system and the independent accounting system began to be applied and that the commercial network increasingly came under state and cooperative ownership.

The basic goal of the Three-Year Plan, officially named "The Three-Year Post-war Reconstruction Plan of 1954–56", was to reconstruct an economy torn by the Korean War. The plan stressed more than merely regaining the prewar output levels. The Soviet Union, other East European countries and China provided reconstruction assistance. The highest priority was developing heavy industry, but an earnest effort to collectivize farming also was begun. At the end of 1957, output of most industrial commodities had returned to 1949 levels, except for a few items such as chemical fertilizers, carbides, and sulfuric acid, whose recovery took longer.

Having basically completed the task of reconstruction, the state planned to lay a solid foundation for industrialization while completing the socialization process and solving the basic problems of food and shelter during the Five-Year Plan of 1957–1960. The socialization process was completed by 1958 in all sectors of the economy, and the Ch'ŏllima Movement was introduced. Although growth rates reportedly were high, there were serious imbalances among the different economic sectors. Because rewards were given to individuals and enterprises that met production quotas, frantic efforts to fulfill plan targets in competition with other enterprises and industries caused disproportionate growth among various enterprises, between industry and agriculture and between light and heavy industries. Because resources were limited and the transportation system suffered bottlenecks, resources were diverted to politically well-connected enterprises or those whose managers complained the loudest. An enterprise or industry that performed better than others often did so at the expense of others. Such disruptions intensified as the target year of the plan approached.

Until the 1960s, North Korea's economy grew much faster than South Korea's. Although North Korea was behind in total national output, it was ahead of South Korea in per capita national output, because of its smaller population relative to South Korea. For example, in 1960 North Korea's population was slightly over 10 million people, while South Korea's population was almost 25 million people. Annual economic growth rates of 30% and 21% during the Three-Year Plan of 1954–1956 and the Five-Year Plan of 1957–1960, respectively, were reported. After claiming early fulfillment of the Five-Year Plan in 1959, North Korea officially designated 1960 a "buffer year"—a year of adjustment to restore balances among sectors before the next plan became effective in 1961. Not surprisingly the same phenomenon recurred in subsequent plans. Because the Five-Year Plan was fulfilled early, it became a de facto four-year plan. Beginning in the early 1960s, however, North Korea's economic growth slowed until it was stagnant at the beginning of the 1990s.

Various factors explain the very high rate of economic development of the country in the 1950s and the general slowdown since the 1960s. During the reconstruction period after the Korean War, there were opportunities for extensive economic growth—attainable through the communist regime's ability to marshall idle resources and labor and to impose a low rate of consumption. This general pattern of initially high growth resulting in a high rate of capital formation was mirrored in other Soviet-type economies. Toward the end of the 1950s, as reconstruction work was completed and idle capacity began to diminish, the economy had to shift from the extensive to the intensive stage, where the simple communist discipline of marshaling underutilized resources became less effective. In the new stage, inefficiency arising from emerging bottlenecks led to diminishing returns. Further growth would only be attained by increasing efficiency and technological progress.

Beginning in the early 1960s, a series of serious bottlenecks began to impede development. Bottlenecks were pervasive and generally were created by the lack of arable land, skilled labor, energy, and transportation, and deficiencies in the extractive industries. Moreover, both land and marine transportation lacked modern equipment and modes of transportation. The inability of the energy and extractive industries as well as of the transportation network to supply power and raw materials as rapidly as the manufacturing plants could absorb them began to slow industrial growth.

The First Seven-Year Plan (initially 1961–1967) built on the groundwork of the earlier plans but changed the focus of industrialization. Heavy industry, with the machine tool industry as its linchpin, was given continuing priority. During the plan, however, the economy experienced widespread slowdowns and reverses for the first time, in sharp contrast to the rapid and uninterrupted growth during previous plans. Disappointing performance forced the planners to extend the plan three more years, until 1970. During the last part of the "de facto" ten-year plan, emphasis shifted to pursuing parallel development of the economy and of defense capabilities. This shift was prompted by concern over the military takeover in South Korea by General Park Chung-hee (1961–1979), escalation of the United States involvement in Vietnam, and the widening Sino-Soviet split. It was thought that stimulating a technological revolution in the munitions industry was one means to achieve these parallel goals. In the end, the necessity to divert resources to defense became the official explanation for the plan's failure.

The Six-Year Plan of 1971–1976 followed immediately in 1971. In the aftermath of the poor performance of the preceding plan, growth targets of the Six-Year Plan were scaled down substantially. Because some of the proposed targets in the First Seven-Year Plan had not been attained even by 1970, the Six-Year Plan did not deviate much from its predecessor in basic goals. The Six-Year Plan placed more emphasis on technological advance, self-sufficiency ("Juche") in industrial raw materials, improving product quality, correcting imbalances among different sectors, and developing the power and extractive industries; the last of these had been deemed largely responsible for slowdowns during the First Seven-Year Plan. The plan called for attaining a self- sufficiency rate of 60–70% in all industrial sectors by substituting domestic raw materials wherever possible and by organizing and renovating technical processes to make the substitution feasible. Improving transport capacity was seen as one of the urgent tasks in accelerating economic development—it was one of the major bottlenecks of the Six-Year Plan.

North Korea claimed to have fulfilled the Six-Year Plan by the end of August 1975, a full year and four months ahead of schedule. Under the circumstances, it was expected that the next plan would start without delay in 1976, a year early, as was the case when the First Seven-Year Plan was instituted in 1961. Even if the Six-Year Plan had been completed on schedule, the next plan should have started in 1977. However, it was not until nearly two years and four months later that the long-awaited plan was unveiled—1977 had become a "buffer year".

The inability of the planners to continuously formulate and institute economic plans reveals as much about the inefficacy of planning itself as the extent of the economic difficulties and administrative disruptions facing the country. For example, targets for successive plans have to be based on the accomplishments of preceding plans. If these targets are underfulfilled, all targets of the next plan—initially based on satisfaction of the plan—have to be reformulated and adjusted. Aside from underfulfillment of the targets, widespread disruptions and imbalances among various sectors of the economy further complicate plan formulation.

The basic thrust of the Second Seven-Year Plan (1978–1984) was to achieve the three-pronged goals of self-reliance, modernization, and "scientification". Although the emphasis on self-reliance was not new, it had not previously been the explicit focus of an economic plan. This new emphasis might have been a reaction to mounting foreign debt originating from large-scale imports of Western machinery and equipment in the mid-1970s. Through modernization North Korea hoped to increase mechanization and automation in all sectors of the economy. "Scientification" means the adoption of up-to-date production and management techniques. The specific objectives of the economic plan were to strengthen the fuel, energy, and resource bases of industry through priority development of the energy and extractive industries; to modernize industry; to substitute domestic resources for certain imported raw materials; to expand freight-carrying capacity in railroad, road, and marine transportation systems; to centralize and containerize the transportation system; and to accelerate a technical revolution in agriculture.

In order to meet the manpower and technology requirements of an expanding economy, the education sector also was targeted for improvements. The quality of the comprehensive eleven-year compulsory education system was to be enhanced to train more technicians and specialists, and to expand the training of specialists, particularly in the fields of fuel, mechanical, electronic, and automation engineering.

Successful fulfillment of the so-called nature-remaking projects also was part of the Second Seven-Year Plan. These projects referred to the five-point program for nature transformation unveiled by Kim Il-sung in 1976: completing the irrigation of non-paddy fields; reclaiming 1,000 square kilometres of new land; building 1,500 to 2,000 km of terraced fields; carrying out afforestation and water conservation work; and reclaiming tidal land.

From all indications, the Second Seven-Year Plan was not successful. North Korea generally downplayed the accomplishments of the plan, and no other plan received less official fanfare. It was officially claimed that the economy had grown at an annual rate of 8.8% during the plan, somewhat below the planned rate of 9.6%. The reliability of this aggregate measure, however, is questionable. During the plan, the target annual output of 10 million tons of grains (cereals and pulses) was attained. However, by official admission, the targets of only five other commodities were fulfilled. Judging from the growth rates announced for some twelve industrial products, it is highly unlikely that the total industrial output increased at an average rate of 12.2% as claimed. After the plan concluded, there was no new economic plan for two years, indications of both the plan's failure and the severity of the economic and planning problems confronting the economy in the mid-1980s. From 1998 to 2003, the government implemented a plan for scientific and technical development, which focused on the nation's IT and electronic industry.

Growth and changes in the structure and ownership pattern of the economy also have changed the labor force. By 1958 individual private farmers, who once constituted more than 70% of the labor force, had been transformed into or replaced by state or collective farmers. Private artisans, merchants, and entrepreneurs had joined state or cooperative enterprises. In the industrial sector in 1963, the last year for which such data are available, there were 2,295 state enterprises and 642 cooperative enterprises. The size and importance of the state enterprises can be surmised by the fact that state enterprises, which constituted 78% of the total number of industrial enterprises, contributed 91% of total industrial output.

Labor force (12.6 million)—by occupation:

Statistics from North Korea's trade partners is collected by international organizations like the United Nations and the International Monetary Fund, and by the South Korean Ministry of Unification.

It has also been estimated that imports of arms from the Soviet Union in the period 1988 to 1990 accounted for around 30% of the North Korea's total imports, and that between 1981 and 1989 North Korea earned approximately $4 billion from the export of arms, approximately 30% of North Korea's total exports in that period. The nominal dollar value of arms exports from North Korea in 1996 was estimated to have been around $50 million.

North Korea's foreign trade deteriorated in the 1990s. After hitting the bottom of $1.4 billion in 1998, it recovered slightly. North Korea's trade total in 2002 was $2.7 billion: only about 50% of $5.2 billion in 1988, even in nominal US dollars. These figures exclude intra-Korean trade, deemed internal, which rose in 2002 to $641 million. During the late 2000s trade grew strongly, almost tripling between 2007 and 2011 to $5.6 billion, with much of the growth being with China. By about 2010 external trade had returned to 1990 levels, and by 2014 was near double 1990 levels, with trade with China increasing from 50% of total trade in 2005 to near 90% in 2014. In 2015, it was estimated that exports to China were $2.3 billion—83% of total exports of $2.83 billion.

In addition to Kaesŏng and Kŭmgang-san, other special economic areas were established at Sinŭiju in the northwest (on the border with China), and at Rasŏn in the northeast (on the border with China and Russia).

International sanctions impeded international trade to some degree, many related to North Korea's development of weapons of mass destruction. United States President Barack Obama approved an executive order in April 2011 that declared "the importation into the United States, directly or indirectly, of any goods, services, or technology from North Korea is prohibited". Operational sanctions included United Nations Security Council Resolutions 1695, 1718, 1874, 1928, 2087, and 2094. Reports in 2018 indicated that trade sanctions (bans on almost all exports and the freezing of overseas accounts) were seriously affecting the economy. The main paper Rodong Sinmun was running short of paper and was publishing only a third of its normal print run, two energy plants supplying electricity to Pyongyang had to be shut down intermittently due to lack of coal, causing blackouts, coal mines were operating under capacity due to lack of fuel, coal could not be transported due to lack of fuel and food rations had been cut by half.

The Taep'oong International Investment Group of Korea is the official company that manages oversea investments to North Korea.

North and South Korea's economic ties have fluctuated greatly over the past 30 years or so. In the late 1990s and most of the 2000s, North-South relations warmed under the Sunshine Policy of President Kim Dae-jung. Many firms agreed to invest in North Korea, encouraged by the South Korean government's commitment to cover their losses, should investment projects in the north fail to become profitable.

Following a 1988 decision by the South Korean Government to allow trade with the North (see Reunification efforts since 1971), South Korean firms began to import North Korean goods. Direct trade with the South began in the fall of 1990 after the unprecedented September 1990 meeting of the two Korean Prime Ministers. Trade between the countries increased from $18.8 million in 1989 to $333.4 million in 1999, much of it processing or assembly work undertaken in the North.

During this decade, the chairman of the South Korean company Daewoo visited North Korea and reached agreement on building a light industrial complex at Namp'o. In other negotiations, Hyundai Asan obtained permission to bring tour groups by sea to Kŭmgang-san on the North Korea's southeast coast (see Kŭmgang-san Tourist Region), and more recently to construct the Kaesŏng Industrial Park, near the Korean Demilitarized Zone (DMZ), at a cost of more than $1 billion.

In response to the summit between Kim Jong-il and Kim Dae-jung in 2000, North and South Korea agreed in August 2000 to reconnect the section of the Seoul–Pyongyang Gyeongui Railway Line across the DMZ. In addition, the two governments said they would build a four-lane highway bypassing the truce village at Panmunjeom.

TV commercials for Samsung's Anycall cell phone featuring North Korean dancer Cho Myong-ae and South Korea's Lee Hyo-ri were first broadcast on June 11, 2006.

Trade with South Korea declined after Lee Myung-bak was elected President of South Korea in 2008, who reduced trade to put pressure on North Korea over nuclear matters. Trade with South Korea fell from $1.8 billion to $1.1 billion between 2007 and 2013, most of the remaining trade being through the Kaesŏng Industrial Park. The Park has been subject to frequent shutdowns due to political tensions.

With the collapse of the Soviet Union, China has been North Korea's primary trading partner. Bilateral trade rose sharply after 2007. In 2007 trade between the two countries was $1.97 billion (₩1.7 trillion). By 2011 trade had increased to $5.6 billion (₩5.04 trillion). Trade with China represented 57% of North Korea's imports and 42% of exports.

Chinese statistics for 2013 indicate that North Korean exports to China were nearly $3 billion, with imports of about $3.6 billion. Exports to China in 2015 were estimated at $2.3 billion.

Some South Korean companies launched joint ventures in areas like animation and computer software, and Chinese traders have done a booming business back and forth across the China–North Korea border. In a 2007 survey of 250 Chinese operations in North Korea, a majority reported paying bribes. Robert Suter, who headed the Seoul office of Swedish-Swiss power generation company ABB, says ABB was staking out a position in North Korea, "It is the same as it was in China years ago. You had to be there and you had to build trust." A number of South Korean enterprises were mainly active in a specially developed industrial zone in Kaesong Industrial Region and Chinese enterprises were known to be involved in a variety of activities in trade and manufacturing in North Korea. European enterprises founded in 2005 the European Business Association (EBA), Pyongyang, a "de facto" chamber of commerce representing a number of European-invested joint ventures and other businesses. Ch'ongryŏn, the pro-North Korean General Association of Korean Residents in Japan, broadcast on their TV channel in 2008 a TV film in three parts featuring foreign investment and business in North Korea. This film was put on a YouTube channel called "BusinessNK" and could be watched together with a number of other videos on foreign joint ventures as well as other investment and business activities in North Korea.

Though no international banks operated in the isolated socialist state in 2013, foreign companies were said to be increasingly interested in dealing with North Korea.

A flat LCD television factory in North Korea was funded by the Ch'ongryŏn in 2010.

The Rason Special Economic Zone was established in the early 1990s, in the northeastern corner of the country bordering China and Russia. In June 2011, an agreement with China was made to establish a joint free trade area on North Korea's Hwanggumpyong and Wihwa Islands and China's border area near Dandong.
North Korea designated over a dozen new special economic zones in 2013 and 2014.





</doc>
<doc id="21261" url="https://en.wikipedia.org/wiki?curid=21261" title="Telecommunications in North Korea">
Telecommunications in North Korea

Telecommunications in North Korea refers to the communication services available in North Korea. North Korea has not fully adopted mainstream Internet technology due to its isolationist policies.

North Korea has an adequate telephone system, with 1.18 million fixed lines available in 2008. However, most phones are only installed for senior government officials. Someone wanting a phone installed must fill out a form indicating their rank, why he/she wants a phone, and how he/she will pay for it. Most of these are installed in government offices, collective farms, and state-owned enterprises (SOEs), with only perhaps 10 percent controlled by individuals or households.
By 1970 automatic switching facilities were in use in Pyongyang, Sinŭiju, Hamhŭng, and Hyesan. A few public telephone booths were beginning to appear in Pyongyang around 1990. In the mid-1990s, an automated exchange system based on an E-10A
system produced by Alcatel joint-venture factories in China was installed in Pyongyang. North Koreans announced in
1997 that automated switching had replaced manual switching in Pyongyang and 70 other locales.
North Korean press reported in 2000 that fiber-optic cable had been extended to the port of Nampho and that North Pyong'an Province had been connected with fiber-optic cable.

In November 2002, mobile phones were introduced to North Korea and by November 2003, 20,000 North Koreans had bought mobile phones.

There was a ban on cell phones from 2004–2008.

In December 2008, a new mobile phone service was launched in Pyongyang, operated by Egyptian company Orascom, with current plans to expand coverage to all parts of the country. The official name of the 3G mobile phone service in North Korea is called Koryolink, and is a joint venture between Orascom and the state-owned Korea Post and Telecommunications Corporation (KPTC). There has been a large demand for the service since it was launched.

In May 2010, more than 120,000 North Koreans owned mobile phones; this number had increased to 301,000 by September 2010, 660,000 by August 2011, and 900,000 by December 2011. Orascom reported 432,000 North Korean subscribers after two years of operation (December 2010), increasing to 809,000 by September 2011, and exceeding one million by February 2012. By April 2013 subscriber numbers neared two million. By 2015 the figure had grown to three million.

In 2011, 60% of Pyongyang's citizens between the age of 20 and 50 had a cellphone. On June 15, 2011, StatCounter.com confirmed that some North Koreans use Apple's iPhones, as well as Nokia's and Samsung's smartphones.

In November 2011, no mobile phones could dial into or out of the country, and there was no Internet connection. A 3G network covered 94 percent of the population, but only 14 percent of the territory. 

Koryolink has no international roaming agreements. Pre-paid SIM cards can be purchased by visitors to North Korea to make international (but not domestic) calls. Prior to January 2013, foreigners had to surrender their phones at the border crossing or airport before entering the country, but with the availability of local SIM cards this policy is no longer in place. Internet access, however, is only available to resident foreigners and not tourists.

North Korean mobile phones use a digital signature system to prevent access to unsanctioned files, and log usage information that can be physically inspected.

A survey in 2017 found that 69% of households had a mobile phone.

In September 2019 a previously unknown company Kwangya Trading Company (광야무역회사의) announced the release of a cell phone for North Korean consumer use called the Kimtongmu. Although state-run media reports that the phone was developed by North Korean outlets it is likely sourced rather from a Chinese OEM manufacturer and outfitted with North Korean software.

North Korea has had a varying number of connections to other nations. Currently, international fixed line connections consist of a network connecting Pyongyang to Beijing and Moscow, and Chongjin to Vladivostok. Communications were opened with South Korea in 2000. On May 2006 TransTeleCom Company and North Korea's Ministry of Communications have signed an agreement for the construction and joint operation of a fiber-optic transmission line in the section of the Khasan–Tumangang railway checkpoint in the North Korea-Russia border. This is the first direct land link between Russia and North Korea. TTC's partner in the design, construction, and connection of the communication line from the Korean side to the junction was Korea Communication Company of North Korea's Ministry of Communications. The technology transfer was built around STM-1 level digital equipment with the possibility of further increasing bandwidth. The construction was completed in 2007.

Since joining Intersputnik in 1984, North Korea has operated 22 lines of frequency-division multiplexing and 10 lines of single channel per carrier for communication with Eastern Europe. and in late 1989 international direct dialing service through microwave link was introduced from Hong Kong. A satellite ground station near Pyongyang provides direct international communications using the International Telecommunications Satellite Corporation (Intelsat) Indian Ocean satellite. A satellite communications center was installed in Pyongyang in 1986 with French technical support. An agreement to share in Japan's telecommunications satellites was reached in 1990. North Korea joined the Universal Postal Union in 1974 but has direct postal arrangements with only a select group of countries.

Following the agreement with UNDP, the Pyongyang Fiber Optic Cable Factory was
built in April 1992 and the country's first optical fiber cable network consisting of 480
Pulse Code Modulation (PCM) lines and 6 automatic exchange stations from Pyongyang
to Hamhung (300 kilometers) was installed in September 1995. Moreover, the nationwide land leveling and rezoning campaign initiated by Kim Jong-il in Kangwon province in May 1998 and in North Pyongan province in January 2000 facilitated the construction of provincial and county fiber optic lines, which were laid by tens of thousands of Korean People's Army (KPA) soldier-builders and provincial shock brigade members mobilized for the large-scale public works projects designed to rehabilitate the hundreds of thousands of hectares of arable lands devastated by the natural disasters in the late 1990s.

Broadcasting in North Korea is tightly controlled by the state and is used as a propaganda arm of the ruling Korean Workers' Party. The Korean Central Television station is located in Pyongyang, and there also are stations in major cities, including Chŏngjin, Kaesŏng, Hamhŭng, Haeju, and Sinŭiju. There are three channels in Pyongyang but only one channel in other cities. Imported Japanese-made color televisions have a North Korean brand name superimposed, but nineteen-inch black-and-white sets have been produced locally since 1980. One estimate placed the total number of television sets in use in the early 1990s at 250,000 sets. A study in 2017 found that 98% of households had a TV set.

Visitors are not allowed to bring a radio. As part of the government's information blockade policy, North Korean radios and televisions must be modified to receive only government stations. These modified radios and televisions should be registered at special state department. They are also subject to inspection at random. The removal of the official seal is punishable by law. In order to buy a TV-set or a radio, North Korean citizens are required to get special permission from officials at their places of residence or employment.

North Korea has two AM radio broadcasting networks, (Voice of Korea) and Korean Central Broadcasting Station, and one FM network, . All three networks have stations in major cities that offer local programming. There also is a powerful shortwave transmitter for overseas broadcasts in several languages.

The official government station is the Korean Central Broadcasting Station (KCBS), which broadcasts in Korean. In 1997 there were 3.36 million radio sets.

Kwangmyong is a North Korean "walled garden" national intranet opened in 2000. It is accessible from within North Korea's major cities, counties, as well as universities and major industrial and commercial organizations. Kwangmyong has 24-hour unlimited access by dial-up telephone line. A survey in 2017 found that 19% of households had a computer, but that only 1% nationally and 5% in Pyongyang had access to the intranet.

In August 2016, it was reported that North Korea had launched a state-approved video streaming service which has been likened to Netflix. The service, known as "Manbang" (meaning everyone) uses a set-top box to stream live TV, on-demand video and newspaper articles (from the state newspaper Rodong Sinmun")" over the internet. The service is only available to citizens in Pyongyang, Siniju and Sariwon. The state TV channel Korean Central Television (KCTV) described the service as a "respite from radio interference".

In 2018, North Korea unveiled a new wi-fi service called Mirae ("Future"), which allowed mobile devices to access the intranet network in Pyongyang.

North Korea's main connection to the international Internet is through a fiber-optic cable connecting Pyongyang with
Dandong, China, crossing the China–North Korea border at Sinuiju. Internet access is provided by China Unicom. Before the fiber connection, international Internet access was limited to government-approved dial-up over land lines to China. In 2003 a joint venture between businessman Jan Holterman in Berlin and the North Korean government called KCC Europe brought the commercial Internet to North Korea. The connection was established through an Intelsat satellite link from North Korea to servers located in Germany. This link ended the need to dial ISPs in China.

In 2007 North Korea successfully applied at ICANN for the .kp country code top-level domain (ccTLD). KCC Europe administered the domain from Berlin, and also hosted a large number of websites .

In 2009 Internet service provider Star Joint Venture Co., a joint venture between the North Korean government's Post and Telecommunications Corporation and Thailand-based Loxley Pacific, took control of North Korea's Internet and address allocation. The satellite link was phased out in favour of the fiber connection and is currently only used as a backup line.

In October 2017 a large scale DDoS attack on the main China connection led to a second Internet connection taken into service. This connects North Korea through a fiber optic cable with Vladivostok, crossing the Russia-North Korea border at Tumangang. Internet access is provided by TransTelekom, a subsidiary of Russian national railway operator Russian Railways.

North Korea's first Internet café opened in 2002 as a joint venture with South Korean Internet company Hoonnet. It is connected via a land line to China. Foreign visitors can link their computers to the Internet through international phone lines available in a few hotels in Pyongyang. In 2005 a new Internet café opened in Pyongyang, connected not through China, but through the North Korean satellite link. Content is most likely filtered by North Korean government agencies.

Since February 2013, foreigners have been able to access the internet using the 3G phone network.

"A Quiet Opening: North Koreans in a Changing Media Environment" a study commissioned by the U.S. State Department and conducted by Intermedia and released May 10, 2012 shows that despite extremely strict regulations and draconian penalties North Koreans, particularly elite elements, have increasing access to news and other media outside the state-controlled media authorized by the government. While access to the Internet is tightly controlled, radio and DVDs are common media accessed, and in border areas, television.

As of 2011, USB flash drives were selling well in North Korea, primarily used for watching South Korean dramas and films on personal computers.




</doc>
<doc id="21262" url="https://en.wikipedia.org/wiki?curid=21262" title="Transport in North Korea">
Transport in North Korea

Transport in North Korea is constrained by economic problems and government restrictions. Public transport predominates, and most of it is electrified.

Travel to North Korea is tightly controlled. The standard route to and from North Korea is by plane or train via Beijing. Transport directly to and from South Korea was possible on a limited scale from 2003 until 2008, when a road was opened (bus tours, no private cars). Freedom of movement in North Korea is also limited, as citizens are not allowed to move around freely inside their country. On October 14, 2018, North and South Korea agreed to restore inter-Korean rail and road transportation. On November 22, 2018, North and South Korea reopened a road on the Korean border which had been closed since 2004. On November 30, 2018, inter-Korean rail transportation resumed when a South Korean train crossed into North Korea for the first time since November 2008. On December 8, 2018, a South Korean bus crossed into North Korea.

Fuel constraints and the near absence of private automobiles have relegated road transportation to a secondary role. The road network was estimated to be around in 1999, up from between and in 1990, of which only , 7.5%, are paved. However, "The World Factbook" (published by the US Central Intelligence Agency) lists of roads with only paved as of 2006. As for the road quality, drivers will often swerve and change lanes to evade potholes, and this includes going into opposite-direction lanes at times. Likewise, sections under repair may not be properly signalled, so oncoming traffic should always be expected even on a divided motorway.

There are three major multilane highways: a expressway connecting Pyongyang and Wonsan on the east coast, a expressway connecting Pyongyang and its port, Nampo, and a four-lane motorway linking Pyongyang and Kaesong. The overwhelming majority of the estimated 264,000 vehicles in use in 1990 were for the military. Rural bus service connects all villages, and cities have bus and tram services. Since 1945/1946, there is right-hand traffic on roads. In cities, driving speeds are set by which lane a driver is in. The speed limits are , , and for the first, second, and subsequent (if existing) lanes "from the right", respectively. A white-on-blue sign informs about this. The leftmost lane, if it is number 3 from the right or higher and is not a turning lane, is often left vacant, even by tourist buses, while the second-from-right lane is generally used to overtake vehicles from lane one, such as public transport buses and trams.

Besides the blue in-city sign, all other occasions, such as motorways and roads outside cities, use the more widely known red-circle-with-number-inside sign to post speed limits. On motorways, the typical limit is and for lanes from the right, respectively, as posted on the Pyongyang-Kaesong highway, for example. The rightmost lane of a motorway is sometimes, as seen on the Pyongyang–Myohyang highway, limited to near on-ramp joining points.

Automobile transportation is further restricted by a series of regulations. According to North Korean exile Kim Ji-ho, unless a driver receives a special permit it is forbidden to drive alone (the driver must carry passengers). Other permits are a military mobilization permit (to transport soldiers in times of war), a certificate of driver training (to be renewed every year), a fuel validity document (a certificate confirming that the fuel was purchased from an authorized source), and a mechanical certificate (to prove that the car is in working order).

Although it drives on the right, North Korea has imported various used right-hand drive RHD vehicles from Japan (through Russia), from tourist buses to Toyota Land Cruisers and HiAces.

As of 2017, electric bicycles are becoming popular in Pyongyang; about 5% of bicycles are electric. Both locally produced and Chinese electric bicycles were available.

As of 2016 there is of road which is 25% of South Korea's road system in length.

There is a mix of locally built and imported trolleybuses and trams in the major urban centres of North Korea. Earlier fleets were obtained from Europe and China.

The Korean State Railway is the only rail operator in North Korea. It has a network of over of standard gauge and of narrow gauge () lines; as of 2007, over of the standard gauge (well over 80%), along with of the narrow gauge lines are electrified. The narrow gauge segment runs in the Haeju peninsula.

Because of lack of maintenance on the rail infrastructure and vehicles, the travel time by rail is increasing. It has been reported that the trip from Pyongyang to Kaesong can take up to six hours.

Water transport on the major rivers and along the coasts plays a growing role in freight and passenger traffic. Except for the Yalu and Taedong rivers, most of the inland waterways, totaling , are navigable only by small boats. Coastal traffic is heaviest on the eastern seaboard, whose deeper waters can accommodate larger vessels. The major ports are Nampho on the west coast and Rajin, Chongjin, Wonsan, and Hamhung on the east coast. The country's harbor loading capacity in the 1990s was estimated at almost 35 million tons a year. There is a continuing investment in upgrading and expanding port facilities, developing transportation—particularly on the Taedong River—and increasing the share of international cargo by domestic vessels.

In the early 1990s, North Korea possessed an oceangoing merchant fleet, largely domestically produced, of 68 ships (of at least 1,000 gross-registered tons), totalling 465,801 gross-registered tons (), which included 58 cargo ships and two tankers. As of 2008, this has increased to a total of 167 vessels consisting mainly of cargo and tanker ships.

North Korea maintains the "Man Gyong Bong 92", a ferry connecting Rajin and Vladivostok, Russia.

North Korea's international air connections are limited in frequency and numbers. As of 2011, scheduled flights operate only from Pyongyang's Pyongyang Sunan International Airport to Beijing, Dalian, Shenyang, Shanghai, Bangkok, Kuala Lumpur, Singapore, Moscow, Khabarovsk, Vladivostok, and Kuwait International Airport. Charters to other destinations operate as per demand. Prior to 1995 many routes to Eastern Europe were operated including services to Sofia, Belgrade, Prague, and Budapest, along with others.

Air Koryo is the country's national airline. , Air China also operates flights between Beijing and Pyongyang. In 2013, MIAT Mongolian Airlines began operating direct charter services from Ulaanbattar to Pyongyang with Boeing 737-800 aircraft.

Internal flights are available between Pyongyang, Hamhung, Haeju (HAE), Hungnam (HGM), Kaesong (KSN), Kanggye, Kilju, Najin (NJN), Nampo (NAM), Sinuiju (SII), Samjiyon, Wonsan (WON), Songjin (SON), and Chongjin (CHO). All civil aircraft are operated by Air Koryo, which has a fleet of 19 passenger and cargo aircraft, all of which are Soviet or more modern Russian types.

As of 2013, the CIA estimates that North Korea has 82 usable airports, 39 of which have permanent-surface runways.

It was reported that North Korean air traffic controllers had been cut off from the international global satellite communications network in 2017 because North Korea had not made the required payments. Traffic controllers at Pyongyang Sunan International Airport had to use conventional telephone lines to inform their counterparts at Incheon International airport that the flight containing North Korean delegates to the 2018 Winter Olympic Games in South Korea had taken off.

Road vehicles in North Korea bear distance stars. These are paint markings which display how far the particular vehicle has traveled without incident. Each star represents travelled without an accident. The bus in this example has three stars, indicating that it has traveled over without a crash.

The DPRK licence plate background color denotes the vehicle type;

Blue - State vehicle

Black - Military vehicle

Yellow - Private vehicle - permitted persons who have contributed greatly to DPRK

Green - Foreign Non-governmental Organizations (NGO)

Red - Diplomatic





</doc>
<doc id="21263" url="https://en.wikipedia.org/wiki?curid=21263" title="Korean People's Army">
Korean People's Army

The Korean People's Army (KPA; Korean: 조선인민군; Hanja: 朝鮮人民軍; "Chosŏn-inmin'gun"; ) is the "de facto" military forces of North Korea and the armed wing of the Workers' Party of Korea. Under the "Songun" policy, it is the central institution of North Korean society. Kim Jong-un serves as Supreme Commander and the chairman of the Central Military Commission. The KPA consists of five branches: the Ground Force, the Naval Force, the Air and Anti-Air Force, the Strategic Rocket Forces, and the Special Operation Force.
The KPA considers its primary adversaries to be the South Korean military and United States Forces Korea, across the Korean Demilitarized Zone, as it has since the Armistice Agreement of July 1953. , with 5,889,000 paramilitary personnel, it is the largest paramilitary organisation in the world. This number serves as 25% of the North Korean population.

Kim Il-sung's anti-Japanese guerrilla army, the Korean People's Revolutionary Army, was established on 25 April 1932. This revolutionary army was transformed into the regular army on 8 February 1948. Both these are celebrated as army days, with decennial anniversaries treated as major celebrations, except from 1978 to 2014 when only the 1932 anniversary was celebrated.

In 1939, the Korean Volunteer Army (KVA), was formed in Yan'an, China. The two individuals responsible for the army were Kim Tu-bong and Mu Chong. At the same time, a school was established near Yan'an for training military and political leaders for a future independent Korea. By 1945, the KVA had grown to approximately 1,000 men, mostly Korean deserters from the Imperial Japanese Army. During this period, the KVA fought alongside the Chinese communist forces from which it drew its arms and ammunition. After the defeat of the Japanese, the KVA accompanied the Chinese communist forces into eastern Jilin, intending to gain recruits from ethnic Koreans in China, particularly from Yanbian, and then enter Korea.

Just after World War II and during the Soviet Union's occupation of the part of Korea north of the 38th Parallel, the Soviet 25th Army headquarters in Pyongyang issued a statement ordering all armed resistance groups in the northern part of the peninsula to disband on 12 October 1945. Two thousand Koreans with previous experience in the Soviet army were sent to various locations around the country to organise constabulary forces with permission from Soviet military headquarters, and the force was created on 21 October 1945.
The headquarters felt a need for a separate unit for security around railways, and the formation of the unit was announced on 11 January 1946. That unit was activated on 15 August of the same year to supervise existing security forces and creation of the national armed forces.

Military institutes such as the Pyongyang Academy (became No. 2 KPA Officers School in Jan. 1949) and the Central Constabulary Academy (became KPA Military Academy in Dec. 1948) soon followed for the education of political and military officers for the new armed forces.

After the military was organised and facilities to educate its new recruits were constructed, the Constabulary Discipline Corps was reorganised into the Korean People's Army General Headquarters. The previously semi-official units became military regulars with the distribution of Soviet uniforms, badges, and weapons that followed the inception of the headquarters.

The State Security Department, a forerunner to the Ministry of People's Defense, was created as part of the Interim People's Committee on 4 February 1948. The formal creation of the Korean People's Army was announced on four days later on 8 February, the day after the Fourth Plenary Session of the People's Assembly approved the plan to separate the roles of the military and those of the police, seven months before the government of the Democratic People's Republic of Korea was proclaimed on 9 September 1948. In addition, the Ministry of State for the People's Armed Forces was established, which controlled a central guard battalion, two divisions, and an independent mixed and combined arms brigade.

Before the outbreak of the Korean War, Joseph Stalin equipped the KPA with modern tanks, trucks, artillery, and small arms (at the time, the South Korean Army had nothing remotely comparable either in numbers of troops or equipment). During the opening phases of the Korean War in 1950, the KPA quickly drove South Korean forces south and captured Seoul, only to lose 70,000 of their 100,000-strong army in the autumn after U.S. amphibious landings at the Battle of Incheon and a subsequent drive to the Yalu River. On 4 November, China openly staged a military intervention. On 7 December, Kim Il-sung was deprived of the right of command of KPA by China. The KPA subsequently played a secondary minor role to Chinese forces in the remainder of the conflict. By the time of the Armistice in 1953, the KPA had sustained 290,000 casualties and lost 90,000 men as POWs.
In 1953, the Military Armistice Commission (MAC) was able to oversee and enforce the terms of the armistice. The Neutral Nations Supervisory Commission (NNSC), made up of delegations from Czechoslovakia, Poland, Sweden and Switzerland, carried out inspections to ensure implementation of the terms of the Armistice that prevented reinforcements or new weapons being brought into Korea.
Soviet thinking on the strategic scale was replaced since December 1962 with a people's war concept. The Soviet idea of direct warfare was replaced with a Maoist war of attrition strategy. Along with the mechanisation of some infantry units, more emphasis was put on light weapons, high-angle indirect fire, night fighting, and sea denial.

Until 1977, original Korean People's Army's official date of establishment was 8 February 1948. But in 1978, changed to 25 April 1932, Kim Il-sung's anti-Japanese guerrilla army – Joseon People's Revolutionary Army, considered the predecessor of the Korean People's Army, was formed on 25 April 1932. But Officially, date of establishment was back to 8 February 1948 in 2018.

The primary path for command and control of the KPA extends through the State Affairs Commission which was led by its chairman Kim Jong-il until 2011, to the Ministry of People's Armed Forces and its General Staff Department. From there on, command and control flows to the various bureaus and operational units. A secondary path, to ensure political control of the military establishment, extends through the Workers' Party of Korea's Central Military Commission of the Workers' Party of Korea.

Since 1990, numerous and dramatic transformations within the have led to the current command and control structure. The details of the majority of these changes are simply unknown to the world. What little is known indicates that many changes were the natural result of the deaths of the aging leadership including Kim Il-sung (July 1994), Minister of People's Armed Forces O Chin-u (February 1995) and Minister of People's Armed Forces Choi Kwang (February 1997).

The vast majority of changes were undertaken to secure the power and position of Kim Jong-il. Formerly the State Affairs Commission, from its founding in 1972 (originally the National Defence Commission), was part of the Central People's Committee (CPC) while the Ministry of the People's Armed Forces, from 1982 onward, was under direct presidential control. At the Eighteenth session of the sixth Central People's Committee, held on 23 May 1990, the SAC became established as its own independent commission, rising to the same status as the CPC (now the Cabinet of North Korea) and not subordinated to it, as was the case before. Concurrent with this, Kim Jong-il was appointed first vice-chairman of the State Affairs Commission. The following year, on 24 December 1991, Kim Jong-il was appointed Supreme Commander of the Korean People's Army. Four months later, on 20 April 1992, Kim Jong-il was awarded the rank of Marshal and his father, in virtue of being the KPA's founding commander in chief, became Grand Marshal as a result and one year later he became the Chairman of the State Affairs Commission, by now under Supreme People's Assembly control under the then 1992 constitution as amended.

Almost all officers of the KPA began their military careers as privates; only very few people are admitted to a military academy without prior service. The results is an egalitarian military system where officers are familiar with the life of a military private and "military nobility" is all but nonexistent.

Within the KPA, between December 1991 and December 1995, nearly 800 high officers (out of approximately 1,200) received promotions and preferential assignments. Three days after Kim Jong-il became Marshal, eight generals were appointed to the rank of Vice-Marshal. In April 1997, on the 85th anniversary of Kim Il-sung's birthday, Kim Jong-il promoted 127 general and admiral grade officers. The following April he ordered the promotions of another 22 generals and flag officers. Along with these changes, many KPA officers were appointed to influential positions within the Korean Workers' Party. These promotions continue today, simultaneous with the celebration of Kim Il-sung's birthday and the KPA anniversary celebrations every April and since recently in July to honour the end of the Korean War. Under Kim Jong-il's leadership, political officers dispatched from the party monitored every move of a general's daily life, according to analysts similar to the work of Soviet political commissars during the early and middle years of the military establishment.

Today the KPA exercises full control of both the Politburo and the Central Military Commission of the WPK, the KPA General Political and General Staff Departments and the Ministry of the People's Armed Forces, all having KPA representatives with a minimum general officer rank. Following changes made during the 4th session of the 13th Supreme People's Assembly on 29 June 2016, the State Affairs Commission has overseen the Ministry of the People's Armed Forces as part of its systemic responsibilities. All members of the State Affairs Commission have membership status (regular or alternate) on the WPK Political Bureau.


North Korea has universal conscription for males and selective conscription for females with many pre- and post-service requirements. Article 86 of the North Korean Constitution states: "National defence is the supreme duty and honour of citizens.
Citizens shall defend the country and serve in the armed forces as
required by law."

KPA soldiers serve three years of military service in the KPA, which also runs its own factories, farms and trading arms.

The Young Red Guards are the youth cadet corps of the KPA for secondary level and university level students. Every Saturday, they hold mandatory 4-hour military training drills, and have training activities on and off campus to prepare them for military service when they turn 18 or after graduation, as well as for contingency measures in peacetime.

Under the Ministry of Social Security (North Korea) and the wartime control of the Ministry of People's Armed Forces, and formerly the Korean People's Security Forces, the Korean People's Internal Security Forces forms the national gendarmerie and civil defence force of the KPA. The KPISF has its units in various fields like civil defence, traffic management, civil disturbance control, and local security. It has its own special forces units. The service shares the ranks of the KPA (with the exception of Marshals) but wears different uniforms.

The KPA's annual budget is approximately US$6 billion. In 2009, the U.S. Institute for Science and International Security reported that North Korea may possess fissile material for around two to nine nuclear warheads. The North Korean Songun ("Military First") policy elevates the KPA to the primary position in the government and society.

According to North Korea's state news agency, military expenditures for 2010 made up 15.8 percent of the state budget. Most analyses of North Korea's defence sector, however, estimate that defence spending constitutes between one-quarter and one-third of all government spending. As of 2003, according to the International Institute of Strategic Studies, North Korea's defence budget consumed some 25 percent of central government spending. In the mid-1970s and early 1980s, according to figures released by the Polish Arms Control and Disarmament Agency, between 32 and 38 percent of central government expenditures went towards defence.
North Korea sells missiles and military equipment to many countries worldwide. In April 2009, the United Nations named the Korea Mining and Development Trading Corporation (KOMID) as North Korea's primary arms dealer and main exporter of equipment related to ballistic missiles and conventional weapons. It also named Korea Ryonbong as a supporter of North Korea's military related sales.

Historically, North Korea has assisted a vast number of revolutionary, insurgent and terrorist groups in more than 62 countries. A cumulative total of more than 5,000 foreign personnel have been trained in North Korea, and over 7,000 military advisers, primarily from the Reconnaissance General Bureau, have been dispatched to some forty-seven countries. Some of the organisations which received North Korean aid include the Polisario Front, Janatha Vimukthi Peramuna, the Communist Party of Thailand, the Palestine Liberation Organization and the Islamic Revolutionary Guard Corps. The Zimbabwean Fifth Brigade received its initial training from KPA instructors. North Korean troops allegedly saw combat during the Libyan–Egyptian War and the Angolan Civil War. Up to 200 KPAF pilots took part in the Vietnam War, scoring several kills against US aircraft. Two KPA anti-aircraft artillery regiments were sent to North Vietnam as well.

North Korean instructors trained Hezbollah fighters in guerrilla warfare tactics around 2004, prior to the Second Lebanon War. During the Syrian Civil War, Arabic-speaking KPA officers may have assisted the Syrian Arab Army in military operations planning and have supervised artillery bombardments in the Aleppo area.

The Korean People's Army Ground Force (KPAGF) is the main branch of the Korean People's Army responsible for land-based military operations. It is the "de facto" army of North Korea.

The Korean People's Army Naval Force (KPANF) is organized into two fleets which are not able to support each other. The East Fleet is headquartered at T'oejo-dong and the West Fleet at Nampho. A number of training, shipbuilding and maintenance units and a naval air wing report directly to Naval Command Headquarters at Pyongyang. The majority of the Navy's ships are assigned to the East Fleet. Due to the short range of most ships, the two fleets are not known to have ever conducted joint operations or shared vessels.

The Korean People's Army Air and Anti-Air Force (KPAAF) is also responsible for North Korea's air defence forces through the use of anti-aircraft artillery and surface-to-air missiles (SAM). While much of the equipment is outdated, the high saturation of multilayered, overlapping, mutually supporting air defence sites provides a formidable challenge to enemy air attacks.

The Korean People's Army Strategic Rocket Force (KPASRF) is a major division of the KPA that controls the DPRK's nuclear and conventional strategic missiles. It is mainly equipped with surface-to-surface missiles of Soviet and Chinese design, as well as locally developed long-range missiles.

The Korean People's Army Special Operation Force (KPASOF) is an asymmetric force with a total troop size of 200,000. Since the Korean War (North Korea: the Korean War of Liberation), it has continued to play a role of concentrating infiltration of troops into the territory of the Republic of South Korea and conducting sabotage.

After the Korean War, North Korea maintained a powerful, but smaller military force than that of South Korea. In 1967 the KPA forces of about 345,000 were much smaller than the South Korean ground forces of about 585,000. North Korea's relative isolation and economic plight starting from the 1980s has now tipped the balance of military power into the hands of the better-equipped South Korean military. In response to this predicament, North Korea relies on asymmetric warfare techniques and unconventional weaponry to achieve parity against high-tech enemy forces. North Korea is reported to have developed a wide range of technologies towards this end, such as stealth paint to conceal ground targets, midget submarines and human torpedoes, blinding laser weapons, and probably has a chemical weapons program and is likely to possess a stockpile of chemical weapons. The Korean People's Army operates ZM-87 anti-personnel lasers, which are banned under the United Nations Protocol on Blinding Laser Weapons.

Since the 1980s, North Korea has also been actively developing its own cyber warfare capabilities. As of 2014, the secretive Bureau 121 – the elite North Korean cyber warfare unit – comprises approximately 1,800 highly trained hackers. In December 2014, the Bureau was accused of hacking Sony and making threats, leading to the cancellation of "The Interview", a political satire comedy film based on the assassination of Kim Jong-un. The Korean People's Army has also made advances in electronic warfare by developing GPS jammers. Current models include vehicle-mounted jammers with a range of -. Jammers with a range of more than 100 km are being developed, along with electromagnetic pulse bombs. The Korean People's Army has also made attempts to jam South Korean military satellites. North Korea does not have satellites capable of obtaining satellite imagery useful for military purposes, and appears to use imagery from foreign commercial platforms.

Despite the general fuel and ammunition shortages for training, it is estimated that the wartime strategic reserves of food for the army are sufficient to feed the regular troops for 500 days, while fuel and ammunition – amounting to 1.5 million and 1.7 million tonnes respectively – are sufficient to wage a full-scale war for 100 days.

The KPA does not operate aircraft carriers, but has other means of power projection. Korean People's Air Force Il-76MD aircraft provide a strategic airlift capacity of 6,000 troops, while the Navy's sea lift capacity amounts to 15,000 troops. The Strategic Rocket Forces operate more than 1,000 ballistic missiles according to South Korean officials in 2010, although the U.S. Department of Defense reported in 2012 that North Korea has fewer than 200 missile launchers. North Korea acquired 12 Foxtrot class and Golf-II class missile submarines as scrap in 1993. Some analysts suggest that these have either been refurbished with the help of Russian experts or their launch tubes have been reverse-engineered and externally fitted to regular submarines or cargo ships. However GlobalSecurity reports that the submarines were rust-eaten hulks with the launch tubes inactivated under Russian observation before delivery, and the U.S. Department of Defense does not list them as active.

A photograph of Kim Jong-un receiving a briefing from his top generals on 29 March 2013 showed a list that purported to show that the military had a minimum of 40 submarines, 13 landing ships, 6 minesweepers, 27 support vessels and 1,852 aircraft.

The Korean People's Army operates a very large amount of equipment, including 4,100 tanks, 2,100 APCs, 8,500 field artillery pieces, 5,100 multiple rocket launchers, 11,000 air defence guns and some 10,000 MANPADS and anti-tank guided missiles in the Ground force; about 500 vessels in the Navy and 730 combat aircraft in the Air Force, of which 478 are fighters and 180 are bombers. North Korea also has the largest special forces in the world, as well as the largest submarine fleet. The equipment is a mixture of World War II vintage vehicles and small arms, widely proliferated Cold War technology, and more modern Soviet or locally produced weapons.

North Korea possesses a vast array of long range artillery in shelters just north of the Korean Demilitarized Zone. It has been a long-standing cause for concern that a preemptive strike or retaliatory strike on Seoul using this arsenal of artillery north of the Demilitarized Zone would lead to a massive loss of life in Seoul. Estimates on how many people would die in an attack on Seoul vary. When the Clinton administration mobilised forces over the reactor at Yongbyon in 1994, planners concluded that retaliation by North Korea against Seoul could kill 40,000 people. Other estimates projects hundreds of thousands or possibly millions of fatalities if North Korea uses chemical munitions.

The KPA possess a variety of Chinese and Soviet sourced equipment and weaponry, as well as locally produced versions and improvements of the former. Soldiers are mostly armed with indigenous Kalashnikov-type rifles as the standard issue weapon. Front line troops are issued the Type 88, while the older Type 58 assault rifle and Type 68A/B have been shifted to rear echelon or home guard units.
A rifle of unknown nomenclature was seen during the 2017 'Day of the Sun' military parade, appearing to consist of a grenade launcher and a standard assault rifle, similar to the U.S OICW or South Korean S&T Daewoo K11.
North Korea generally designates rifles as "Type XX", similar to the Chinese naming system. On 15 November 2018, North Korea successfully tested a "newly developed ultramodern tactical weapon". Leader Kim Jong Un observed the test at the Academy of Defense Science and called it a "decisive turn" in bolstering the combat power of the North Korean army.

The U.S. Department of Defense believes North Korea probably has a chemical weapons program and is likely to possess a stockpile of such weapons.

North Korea has tested a series of different missiles, including short-, medium-, intermediate-, and intercontinental- range, and submarine-launched ballistic missiles. Estimates of the country's nuclear stockpile vary: some experts believe Pyongyang has between fifteen and twenty nuclear weapons, while U.S. intelligence believes the number to be between thirty and sixty bombs. The regime conducted two tests of an intercontinental ballistic missile (ICBM) capable of carrying a large nuclear warhead in July 2017. The Pentagon confirmed North Korea's ICBM tests, and analysts estimate that the new missile has a potential range of and, if fired on a flatter trajectory, could be capable of reaching mainland U.S. territory.

On 9 October 2006, the North Korean government announced that it had unsuccessfully attempted a nuclear test for the first time. Experts at the United States Geological Survey and Japanese seismological authorities detected an earthquake with a preliminary estimated magnitude of 4.3 from the site in North Korea, proving the official claims to be true.

North Korea also went on to claim that it had developed a nuclear weapon in 2009. It is widely believed to possess a stockpile of relatively simple nuclear weapons. The IAEA has met Ri Je Son, The Director General of the General Department of Atomic Energy (GDAE) of the DPRK, to discuss nuclear matters. Ri Je Son was also mentioned in this role in 2002 in a United Nations article.

On 3 September 2017, the North Korean leadership announced that it had conducted a nuclear test with what it claimed to be its first hydrogen bomb detonation. The detonation took place at an underground location at the Punggye-Ri nuclear test site in North Hamgyong Province at 12:00 pm local time. South Korean officials claimed the test yielded 50 kilotons of explosive force, with many international observers claiming the test likely involved some form of a thermonuclear reaction.








</doc>
<doc id="21264" url="https://en.wikipedia.org/wiki?curid=21264" title="Foreign relations of North Korea">
Foreign relations of North Korea

The foreign relations of North Korea – officially the Democratic People's Republic of Korea (DPRK) – have been shaped by its conflict with capitalist countries like South Korea and its historical ties with world communism. Both the government of North Korea and the government of South Korea (officially the Republic of Korea) claim to be the sole legitimate government of the whole of Korea. The Korean War in the 1950s failed to resolve the issue, leaving North Korea locked in a military confrontation with South Korea and the United States Forces Korea across the Demilitarized Zone.

At the start of the Cold War, North Korea only had diplomatic recognition by Communist countries. Over the following decades, it established relations with developing countries and joined the Non-Aligned Movement. When the Eastern Bloc collapsed in the years 1989–1992, North Korea made efforts to improve its diplomatic relations with developed capitalist countries. At the same time, there were international efforts to resolve the confrontation on the Korean peninsula (known as the Korean conflict).

When North Korea acquired nuclear weapons after the demise of the Soviet Union, its main economic backer, resolving the crisis became a more important issue to much of the international community. North Korea is considered a rogue state, and is not signatory to the Non-proliferation treaty (NPT)—in fact, it was formerly an acceder to the treaty, which it had violated, but withdrew in 2003 after banishing the International Atomic Energy Agency. Its nuclear program is seen as part of North Korea's strategy of "nuclear coercion", which analysts have posed in terms of North Korea's regime survival.

In 2018, North Korean leader Kim Jong-un made a sudden peace overture towards South Korea and the United States. This led to the first face-to-face discussion between the State Chairman of North Korea and a sitting President of the United States. This is known as the 2018 Korean peace process.

The Constitution of North Korea establishes the country's foreign policy. While Article 2 of the constitution describes the country as a "revolutionary state," Article 9 says that the country will work to achieve Korean reunification, maintain state sovereignty and political independence, and "national unity."

Many articles specifically outline the country's foreign policy. Article 15 says that the country will "protect the democratic national rights of Korean compatriots overseas and their legitimate rights and interests as recognized by international law" and Article 17 explicates the basic ideals of the country's foreign policy:

Other parts of the constitution explicate other foreign policies. Article 36 says that foreign trade by the DPRK will be conducted "by state organs, enterprises, and social, cooperative organizations" while the country will "develop foreign trade on the principles of complete equality and mutual benefit." Article 37 adds that the country will encourage "institutions, enterprises and organizations in the country to conduct equity or contractual joint ventures with foreign corporations and individuals, and to establish and operate enterprises of various kinds in special economic zones." Furthermore, Article 38 says that the DPRK will implement a protectionist tariff policy "to protect the independent national economy" while Article 59 says the country's armed forces will "carry out the military-first revolutionary line." In terms of other foreign policy, Article 80 says that the country will grant asylum to foreign nationals who have been persecuted "for struggling for peace and democracy, national independence and socialism or for the freedom of scientific and cultural pursuits."

Ultimately, however, as explicated in Articles 100–103 and 109, the chairman of the National Defense Commission (NDC) is the supreme leader of the country, with a term that is the same as members of the Supreme People's Assembly or SPA (five years), as is established in article 90, directing the country's armed forces, and guiding overall state affairs, but is not determined by him alone since he is still accountable to the SPA. Rather, the NDC chairman works to defend the state from external actors. Currently, Kim Jong-un, is the Chairman of the Workers' Party of Korea (WPK), State Chairman of North Korea, and holder of numerous other leadership positions. The Constitution also delineates, in article 117, that the President of SPA Presidium, which can convene this assembly, represents the state and receives "credentials and letters of recall from envoys accredited by other countries." Additionally, the cabinet of the DPRK has the authority to "conclude treaties with foreign countries and conduct external affairs" as noted in Article 125.

North Korea is one of the few countries in which the giving of presents still plays a significant role in diplomatic protocol, with Korean Central News Agency (KCNA) reporting from time to time the country's leader received a floral basket or other gift from a foreign leader or organization. During a 2000 visit to Pyongyang, US Secretary of State Madeleine Albright gave North Korean leader Kim Jong-il a basketball signed by Michael Jordan, as he took an interest in NBA basketball. During the 2000 inter-Korean summit, Kim Jong-il made a gift of two Pungsan dogs (associated with the North) to South Korean president Kim Dae-jung. In return, Kim Dae-jung gave two Jindo dogs (associated with the South) to Kim Jong-il. At their Pyongyang summit in 2018, North Korean leader Kim Jong-un gave two Pungsan dogs to South Korean President, Moon Jae-in.

North Korea takes its defense seriously, confronting countries they see as threatening their sovereignty, and restricts the activities of foreign diplomats.

After 1945, the Soviet Union supplied the economic and military aid that enabled North Korea to mount its invasion of South Korea in 1950. Soviet aid and influence continued at a high level during the Korean war. This was only the beginning of North Korea as governed by the faction which had its roots in an anti-Japanese Korean nationalist movement based in Manchuria and China, with Kim Il-sung participating in this movement and later forming the Workers' Party of Korea (WPK).

The assistance of Chinese troops, after 1950, during the war and their presence in the country until 1958 gave China some degree of influence in North Korea.

In 1961, North Korea concluded formal mutual security treaties with the Soviet Union and China, which have not been formally ended. In the case of China, Kim Il-sung and Chou En-Lai signed the Sino-North Korean Mutual Aid and Cooperation Friendship Treaty, whereby Communist China pledged to immediately render military and other assistance by all means to its ally against any outside attack. The treaty says, in short that:

THE Chairman of the People's Republic of China and the Presidium of the Supreme People's Assembly of the Democratic People's Republic of Korea, determined, in accordance with Marxism–Leninism and the principle of proletarian internationalism and on the basis of mutual respect for state sovereignty and territorial integrity, mutual non-aggression, non-interference in each other's internal affairs, equality and mutual benefit, and mutual assistance and support, to make every effort to further strengthen and develop the fraternal relations of friendship, co-operation and mutual assistance between the People's Republic of China and the Democratic People's Republic of Korea, to jointly guard the security of the two peoples, and to safeguard and consolidate the peace of Asia and the world...[Article II:]The Contracting Parties will continue to make every effort to safeguard the peace of Asia and the world and the security of all peoples...[Article II:] In the event of one of the Contracting Parties being subjected to the armed attack by any state or several states jointly and thus being involved in a state of war, the other Contracting Party shall immediately render military and other assistance by all means at its disposal...[Article V:] The Contracting Parties, on the principles of mutual respect for sovereignty, non-interference in each other's internal affairs, equality and mutual benefit and in the spirit of friendly co-operation, will continue to render each other every possible economic and technical aid in the cause of socialist construction of the two countries and will continue to consolidate and develop economic, cultural, and scientific and technical co-operation between the two countries...[Article VI:] The Contracting Parties hold that the unification of Korea must be realized along peaceful and democratic lines and that such a solution accords exactly with the national interests of the Korean people and the aim of preserving peace in the Far East.

This treaty was prolonged twice, in 1981 and 2001, with a validity until 2021.

For most of the Cold War, North Korea avoided taking sides in the Sino-Soviet split, but was originally only recognized by countries in the Communist Bloc until 1958 when Algeria recognized it.

East Germany was an important source of economic cooperation for North Korea. The East German leader, Erich Honecker, who visited in 1977, was one of Kim Il-sung's closest foreign friends. In 1986, the two countries signed an agreement on military co-operation. Kim was also close to maverick Communist leaders, Josip Broz Tito of Yugoslavia, and Nicolae Ceaușescu of Romania. North Korea began to play a part in the global radical movement, forging ties with such diverse groups as the Black Panther Party of the US, the Workers Party of Ireland, and the African National Congress. As it increasingly emphasized its independence, North Korea began to promote the doctrine of "Juche" ("self-reliance") as an alternative to orthodox Marxism-Leninism and as a model for developing countries to follow.

When North-South dialogue started in 1972, North Korea began to receive diplomatic recognition from countries outside the Communist bloc. Within four years, North Korea was recognized by 93 countries, on par with South Korea's 96. North Korea gained entry into the World Health Organization and, as a result, sent its first permanent observer missions to the United Nations (UN). In 1975, it joined the Non-Aligned Movement.

During the 1980s, the pace of North Korea's establishment of new diplomatic relations slowed considerably. Following Kim Il-sung's 1984 visit to Moscow, there was a dramatic improvement in Soviet-DPRK relations, resulting in renewed deliveries of advanced Soviet weaponry to North Korea and increases in economic aid. In 1989, as a response to the 1988 Seoul Olympics, North Korea hosted the 13th World Festival of Youth and Students in Pyongyang.

South Korea established diplomatic relations with the Soviet Union in 1990 and the People's Republic of China in 1992, which put a serious strain on relations between North Korea and its traditional allies. Moreover, the demise of Communist states in Eastern Europe in 1989 and the disintegration of the Soviet Union in 1991 had resulted in a significant drop in communist aid to North Korea, resulting in largely decreased relations with Russia. Subsequently, South Korea developed the "sunshine policy" towards North Korea, aiming for peaceful Korean reunification. This policy ended in 2009.

In September 1991, North Korea became a member of the UN. In July 2000, it began participating in the ASEAN Regional Forum (ARF), as Foreign Minister Paek Nam-sun attended the ARF ministerial meeting in Bangkok 26–27 July. North Korea also expanded its bilateral diplomatic ties in that year, establishing diplomatic relations with Italy, Australia and the Philippines. The United Kingdom established diplomatic relations with North Korea on 13 December 2000, as did Canada in February 2001, followed by Germany and New Zealand on 1 March 2001.
In 2006, North Korea test-fired a series of ballistic missiles, after Chinese officials had advised North Korean authorities not to do so. As a result, Chinese authorities publicly rebuked what the west perceives as China's closest ally, and supported the UN Security Council Resolution 1718, which imposed sanctions on North Korea. At other times however, China has blocked United Nations resolutions threatening sanctions against North Korea. In January, 2009, China's paramount leader Hu Jintao and North Korea's supreme leader Kim Jong-il exchanged greetings and declared 2009 as the "year of China-DPRK friendship", marking 60 years of diplomatic relations between the two countries.

On 28 November 2010, as part of the United States diplomatic cables leak, WikiLeaks and media partners such as "The Guardian" published details of communications in which Chinese officials referred to North Korea as a "spoiled child" and its nuclear program as "a threat to the whole world's security" while two anonymous Chinese officials claimed there was growing support in Beijing for Korean reunification under the South's government.

In August 1971, both North and South Korea agreed to hold talks through their respective Red Cross societies with the aim of reuniting the many Korean families separated following the division of Korea after the Korean War. After a series of secret meetings, both sides announced on 4 July 1972, an agreement to work toward peaceful reunification and an end to the hostile atmosphere prevailing on the peninsula. Dialogue was renewed on several fronts in September 1984, when South Korea accepted the North's offer to provide relief goods to victims of severe flooding in South Korea.

In a major initiative in July 1988, South Korean President Roh Tae-woo called for new efforts to promote North-South exchanges, family reunification, inter-Korean trade and contact in international forums. Roh followed up this initiative in a UN General Assembly speech in which South Korea offered to discuss security matters with the North for the first time. In September 1990, the first of eight prime minister-level meetings between officials of North Korea and South Korea took place in Seoul, beginning an especially fruitful period of dialogue. The prime ministerial talks resulted in two major agreements: the Agreement on Reconciliation, Nonaggression, Exchanges, and Cooperation (the "Basic Agreement") and the Declaration on the Denuclearization of the Korean Peninsula (the "Joint Declaration"). The "Joint Declaration" on denuclearization was initiated on 13 December 1991. It forbade both sides to test, manufacture, produce, receive, possess, store, deploy, or use nuclear weapons and forbade the possession of nuclear reprocessing and uranium enrichment facilities. On 30 January 1992, North Korea also signed a nuclear safeguards agreement with the IAEA, as it had pledged to do in 1985 when acceding to the nuclear Non-Proliferation Treaty. This safeguards agreement allowed IAEA inspections to begin in June 1992.

As the 1990s progressed, concern over the North's nuclear program became a major issue in North-South relations and between North Korea and the US. By 1998, South Korean President Kim Dae-jung announced a Sunshine Policy towards North Korea. This led in June 2000 to the first Inter-Korean summit, between Kim Dae-jung and Kim Jong-il. In September 2000, the North and South Korean teams marched together at the Sydney Olympics. Trade increased to the point where South Korea became North Korea's largest trading partner. Starting in 1998, the Mount Kumgang Tourist Region was developed as a joint venture between the government of North Korea and Hyundai. In 2003, the Kaesong Industrial Region was established to allow South Korean businesses to invest in the North.

In 2007, South Korean President Roh Moo-hyun held talks with Kim Jong-il in Pyongyang. On October 4, 2007, South Korean President Roh and Kim signed a peace declaration. The document called for international talks to replace the which ended the Korean War with a permanent peace treaty. The Sunshine Policy was formally abandoned by subsequent South Korean President Lee Myung-bak in 2010.

The Kaesong Industrial Park was closed in 2013, amid tensions about North Korea's nuclear weapons program. It reopened the same year but closed again in 2016.

In 2017 Moon Jae-in was elected President of South Korea with promises to return to the Sunshine Policy. In his New Year address for 2018, North Korean leader Kim Jong-un proposed sending a delegation to the upcoming Winter Olympics in South Korea. The Seoul–Pyongyang hotline was reopened after almost two years. North and South Korea marched together in the Olympics opening ceremony and fielded a united women's ice hockey team. North Korea sent an unprecedented high-level delegation, headed by Kim Yo-jong, sister of Kim Jong-un, and President Kim Yong-nam, as well as athletes and performers.

On 27 April, the 2018 inter-Korean summit took place between President Moon Jae-in and Kim Jong-un on the South Korean side of the Joint Security Area. It was also the first time since the Korean War that a North Korean leader had entered South Korean territory. The summit ended with both countries pledging to work towards complete denuclearization of the Korean Peninsula. They agreed to work to remove all nuclear weapons from the Korean Peninsula and, within the year, to declare an official end to the Korean War. As part of the Panmunjom Declaration which was signed by leaders of both countries, both sides also called for the end of longstanding military activities in the region of the Korean border and a reunification of Korea. Also, the leaders of the region's two divided states have agreed to work together to connect and modernise their border railways.

Moon and Kim met the second time on 26 May. Their second summit was unannounced, held in the North Korean portion of Joint Security Area and concerned Kim's upcoming summit with US President Donald Trump. Trump and Kim met on 12 June 2018 in Singapore and endorsed the Panmunjom Declaration. On June 30, 2019, Kim and Moon met again at the Korean DMZ, this time joined by Trump. During 2019, North Korea conducted a series of short–range missile tests, while the US and South Korea took part in joint military drills in August. On 16 August 2019, North Korea's ruling party made a statement criticizing the South for participating in the drills and for buying US military hardware, calling it a "grave provocation" and saying there would be no more negotiation.

North Korea's nuclear research program started with Soviet help in the 1960s, on condition that it joined the Nuclear Non-Proliferation Treaty (NPT). In the 1980s an indigenous nuclear reactor development program started with a small experimental 5 MWe gas-cooled reactor in Yongbyon, with a 50 MWe and 200 MWe reactor to follow. Concerns that North Korea had non-civilian nuclear ambitions were first raised in the late 1980s and almost resulted in their withdrawal from the NPT in 1994. However, the Agreed Framework and the Korean Peninsula Energy Development Organization (KEDO) temporarily resolved this crisis by having the US and several other countries agree that in exchange for dismantling its nuclear program, two light-water reactors (LWRs) would be provided with moves toward normalization of political and economic relations. This agreement started to break down from 2001 because of slow progress on the KEDO light water reactor project and U.S. President George W. Bush's Axis of Evil speech. After continued allegations from the United States, North Korea declared the existence of uranium enrichment programs during a private meeting with American military officials. North Korea withdrew from the Nuclear Non-Proliferation Treaty on 10 January 2003. In 2006, North Korea conducted its first nuclear test.

In the third (and last) phase of the fifth round of six-party talks were held on 8 February 2007, and implementation of the agreement reached at the end of the round has been successful according to the requirements of steps to be taken by all six parties within 30 days, and within 60 days after the agreement, including normalization of US-North Korea and Japanese-North Korean diplomatic ties, but on the condition that North Korea ceases to operate its Yongbyon nuclear research centre.

North Korea conducted further nuclear tests in 2009, 2013, January and September 2016, and 2017. In 2018, North Korea ceased conducting nuclear and missile tests. Kim Jong-un signed the Panmunjom Declaration committing to "denuclearisation of the Korean Peninsula" and affirmed the same commitment in a subsequent meeting with US President Donald Trump.

North Korea is often perceived as the "Hermit kingdom", completely isolated from the rest of the world, but North Korea maintains diplomatic relations with 164 independent states.

The country also has bilateral relations with the State of Palestine, the Sahrawi Arab Democratic Republic, and the European Union.

North Korea is a member of the following international organizations:






</doc>
<doc id="21265" url="https://en.wikipedia.org/wiki?curid=21265" title="Northern Ireland">
Northern Ireland

Northern Ireland ( ; Ulster-Scots: "") is variously described as a country, province or region which is part of the United Kingdom. Located in the northeast of the island of Ireland, Northern Ireland shares a border to the south and west with the Republic of Ireland. In 2011, its population was 1,810,863, constituting about 30% of the island's total population and about 3% of the UK's population. Established by the Northern Ireland Act 1998 as part of the Good Friday Agreement, the Northern Ireland Assembly (colloquially referred to as Stormont after its location) holds responsibility for a range of devolved policy matters, while other areas are reserved for the British government. Northern Ireland co-operates with the Republic of Ireland in several areas, and the Agreement granted the Republic the ability to "put forward views and proposals" with "determined efforts to resolve disagreements between the two governments".

On 11 January 2020, legislators in Northern Ireland formed a government for the first time since the Executive of the 5th Northern Ireland Assembly collapsed in January 2017, following the Renewable Heat Incentive scandal.

Northern Ireland was created in 1921, when Ireland was partitioned between Northern Ireland and Southern Ireland by the Government of Ireland Act 1920. Unlike Southern Ireland, which would become the Irish Free State in 1922, the majority of Northern Ireland's population were unionists, who wanted to remain within the United Kingdom. Most of these were the Protestant descendants of colonists from Great Britain. However, a significant minority, mostly Catholics, were nationalists who wanted a united Ireland independent of British rule. Today, the former generally see themselves as British and the latter generally see themselves as Irish, while a distinct Northern Irish or Ulster identity is claimed both by a large minority of Catholics and Protestants and by many of those who are non-aligned.

For most of the 20th century, when it came into existence, Northern Ireland was marked by discrimination and hostility between these two sides in what First Minister of Northern Ireland, David Trimble, called a "cold house" for Catholics. In the late 1960s, conflict between state forces and chiefly Protestant unionists on the one hand, and chiefly Catholic nationalists on the other, erupted into three decades of violence known as the Troubles, which claimed over 3,500 lives and injured over 50,000 others. The 1998 Good Friday Agreement was a major step in the peace process, including the decommissioning of weapons and security normalisation, although sectarianism and religious segregation still remain major social problems, and sporadic violence has continued.

The economy of Northern Ireland was the most industrialised of Ireland, declining as a result of the political and social turmoil of the Troubles, but economically growing significantly since the late 1990s. The initial growth came from the "peace dividend" and the links which increased trade with the Republic of Ireland, continuing with a significant increase in tourism, investment and business from around the world. Unemployment in Northern Ireland peaked at 17.2% in 1986, dropping to 6.1% and down by 1.2 percentage points over the year, similar to the UK figure of 6.2%. More than 58% of those unemployed had been unemployed for over a year.

Cultural links between Northern Ireland, the rest of Ireland, and the rest of the UK are complex, with Northern Ireland sharing both the culture of Ireland and the culture of the United Kingdom. In many sports, the island of Ireland fields a single team, a notable exception being association football. Northern Ireland competes separately at the Commonwealth Games, and people from Northern Ireland may compete for either Great Britain or Ireland at the Olympic Games.

The region that is now Northern Ireland was the bedrock of the Irish war of resistance against English programmes of colonialism in the late 16th century. The English-controlled Kingdom of Ireland had been declared by the English king Henry VIII in 1542, but Irish resistance made English control fragmentary. Following Irish defeat at the Battle of Kinsale, though, the region's Gaelic, Roman Catholic aristocracy fled to continental Europe in 1607 and the region became subject to major programmes of colonialism by Protestant English (mainly Anglican) and Scottish (mainly Presbyterian) settlers. A rebellion in 1641 by Irish aristocrats against English rule resulted in a massacre of settlers in Ulster in the context of a war breaking out between England, Scotland and Ireland fuelled by religious intolerance in government. Victories by English forces in that war and further Protestant victories in the Williamite War in Ireland (1688–1691) toward the close of the 17th century solidified Anglican rule in Ireland. In Northern Ireland, the victories of the Siege of Derry (1689) and the Battle of the Boyne (1690) in this latter war are still celebrated by some Protestants (both Anglican and Presbyterian).
Popes Innocent XI and Alexander VIII had supported William of Orange instead of his maternal uncle and father-in-law James II, despite William being Protestant and James a Catholic, due to William's participation in alliance with both Protestant and Catholic powers in Europe in wars against Louis XIV (the "Sun King"), the powerful King of France who had been in conflict with the papacy for decades. In 1693, however, Pope Innocent XII recognised James as continuing King of Great Britain and Ireland in place of William, after reconciliation with Louis. In 1695, and contrary to the terms of the Treaty of Limerick (October 1691), a series of penal laws were passed by the Anglican ruling class in Ireland in intense anger at the Pope's recognition of James over William, which was felt to be a betrayal. The intention of the laws was to materially disadvantage the Catholic community and, to a lesser extent, the Presbyterian community. In the context of open institutional discrimination, the 18th century saw secret, militant societies develop in communities in the region and act on sectarian tensions in violent attacks. These events escalated at the end of the century following an event known as the Battle of the Diamond, which saw the supremacy of the Anglican and Presbyterian Peep o'Day Boys over the Catholic Defenders and leading to the formation of the Anglican Orange Order. A rebellion in 1798 led by the cross-community Belfast-based Society of the United Irishmen and inspired by the French Revolution sought to break the constitutional ties between Ireland and Britain and unite Irish people of all religions. Following this, in an attempt to quell sectarianism and force the removal of discriminatory laws (and to prevent the spread of French-style republicanism to Ireland), the government of the Kingdom of Great Britain pushed for the two kingdoms to be merged. The new state, formed in 1801, the United Kingdom of Great Britain and Ireland, was governed from a single government and parliament based in London.

Some 250,000 people from Ulster emigrated to the British North American colonies between 1717 and 1775. It is estimated that there are more than 27 million Scotch-Irish Americans now living in the United States, along with many Scotch-Irish Canadians in Canada.

During the 19th century, legal reforms started in the late 18th century continued to remove statutory discrimination against Catholics, and progressive programmes enabled tenant farmers to buy land from landlords. By the close of the century, a large and disciplined cohort of Irish Nationalist MPs at Westminster committed the Liberal Party to autonomy—"Home Rule"—for Ireland, a prospect bitterly opposed by Irish Unionists. In 1912, after decades of obstruction from the House of Lords, and with a Liberal government dependent on Nationalist support, Home Rule became a near-certainty. A clash between the House of Commons and House of Lords over a controversial budget produced the Parliament Act 1911, which enabled the veto of the Lords to be overturned. The House of Lords veto had been the unionists' main guarantee that Home Rule would not be enacted because the majority of members of the House of Lords were unionists. In response, opponents to Home Rule, from Conservative and Unionist Party leaders such as Bonar Law and Dublin-based barrister Sir Edward Carson to militant working class unionists in Ireland, threatened the use of violence. In 1914, they smuggled thousands of rifles and rounds of ammunition from Imperial Germany for use by the Ulster Volunteers (UVF), a paramilitary organisation opposed to the implementation of Home Rule.

Unionists were in a minority in Ireland as a whole, but in the northern province of Ulster they were a very large majority in County Antrim and County Down, small majorities in County Armagh and County Londonderry and a substantial minority in Ulster's five other counties. The four counties named, along with County Fermanagh and County Tyrone, would later constitute Northern Ireland. Most of the remaining 26 counties which later became the Republic of Ireland were overwhelmingly majority-nationalist.

During the Home Rule Crisis, the possibility was discussed of a "temporary" partition of these six counties from the rest of Ireland. In 1914, the Third Home Rule Bill received Royal Assent as the Government of Ireland Act 1914. However, its implementation was suspended before it came into effect because of the outbreak of the First World War, and the Amending Bill to partition Ireland was abandoned. The war was expected to last only a few weeks but in fact, lasted four years. By the end of the war (during which the 1916 Easter Rising had taken place), the Act was seen as unimplementable. Public opinion among nationalists had shifted during the war from a demand for home rule to one for full independence. In 1919, David Lloyd George proposed a new bill be established by the cabinet's "Walter Long Committee" on Ireland, which by adopting findings of his (Lloyd George's) inconclusive 1917-18 Irish Convention would divide Ireland into two Home Rule areas: twenty-six counties being ruled from Dublin and six being ruled from Belfast. Straddling these two areas would be a shared Lord Lieutenant of Ireland who would appoint both governments and a Council of Ireland, which Lloyd George believed would evolve into an all-Ireland parliament.

Events overtook the government. The pro-independence Sinn Féin won 73 of the 105 parliamentary seats in Ireland at the general election of 1918, and unilaterally established the First Dáil, an extrajudicial parliament in Ireland. Ireland was partitioned between Northern Ireland and Southern Ireland in 1921, under the terms of Lloyd George's Government of Ireland Act 1920, during the Anglo-Irish War between Irish republican and British forces. A truce was established on 11 July; the war ended on 6 December 1921 with the signing of the Anglo-Irish Treaty, which created the Irish Free State. Under the terms of the treaty, Northern Ireland would become part of the Free State unless the government opted out by presenting an address to the king, although in practice partition remained in place.
As expected, the Houses of the Parliament of Northern Ireland resolved on 7 December 1922 (the day after the establishment of the Irish Free State) to exercise its right to opt out of the Free State by making an address to the King. The text of the address was: Shortly afterwards, the Boundary Commission was established to decide on the territorial boundaries between the Irish Free State and Northern Ireland. Owing to the outbreak of civil war in the Free State, the work of the commission was delayed until 1925. Leaders in Dublin expected a substantial reduction in the territory of Northern Ireland, with nationalist areas moving to the Free State. However, the commission's report recommended only that some small portions of land should be ceded from Northern Ireland to the Free State and even that a small amount of land should be ceded from the Free State to Northern Ireland. To prevent argument, this report was suppressed and, in exchange for a waiver to the Free State's obligations to the UK's public debt and the dissolution of the Council of Ireland (sought by the Government of Northern Ireland), the initial six-county border was maintained with no changes.

In June 1940, to encourage the neutral Irish state to join with the Allies, British Prime Minister Winston Churchill indicated to the Taoiseach Éamon de Valera that the United Kingdom would push for Irish unity, but believing that Churchill could not deliver, de Valera declined the offer. The British did not inform the Government of Northern Ireland that they had made the offer to the Dublin government, and De Valera's rejection was not publicised until 1970.

The Ireland Act 1949 gave the first legal guarantee that the region would not cease to be part of the United Kingdom without the consent of the Parliament of Northern Ireland.

The Troubles, which started in the late 1960s, consisted of about 30 years of recurring acts of intense violence during which 3,254 people were killed with over 50,000 casualties. From 1969 to 2003 there were over 36,900 shooting incidents and over 16,200 bombings or attempted bombings associated with The Troubles. The conflict was caused by the disputed status of Northern Ireland within the United Kingdom and the discrimination against the Irish nationalist minority by the dominant unionist majority. From 1967 to 1972 the Northern Ireland Civil Rights Association (NICRA), which modelled itself on the US civil rights movement, led a campaign of civil resistance to anti-Catholic discrimination in housing, employment, policing, and electoral procedures. The franchise for local government elections included only rate-payers and their spouses, and so excluded over a quarter of the electorate. While the majority of disenfranchised electors were Protestant, Catholics were over-represented since they were poorer and had more adults still living in the family home.

NICRA's campaign, seen by many unionists as an Irish republican front, and the violent reaction to it, proved to be a precursor to a more violent period. As early as 1969, armed campaigns of paramilitary groups began, including the Provisional IRA campaign of 1969–1997 which was aimed at the end of British rule in Northern Ireland and the creation of a United Ireland, and the Ulster Volunteer Force, formed in 1966 in response to the perceived erosion of both the British character and unionist domination of Northern Ireland. The state security forces – the British Army and the police (the Royal Ulster Constabulary) – were also involved in the violence. The British government's position is that its forces were neutral in the conflict, trying to uphold law and order in Northern Ireland and the right of the people of Northern Ireland to democratic self-determination. Republicans regarded the state forces as combatants in the conflict, pointing to the collusion between the state forces and the loyalist paramilitaries as proof of this. The "Ballast" investigation by the Police Ombudsman has confirmed that British forces, and in particular the RUC, did collude with loyalist paramilitaries, were involved in murder, and did obstruct the course of justice when such claims had been investigated, although the extent to which such collusion occurred is still hotly disputed.

As a consequence of the worsening security situation, autonomous regional government for Northern Ireland was suspended in 1972. Alongside the violence, there was a political deadlock between the major political parties in Northern Ireland, including those who condemned violence, over the future status of Northern Ireland and the form of government there should be within Northern Ireland. In 1973, Northern Ireland held a referendum to determine if it should remain in the United Kingdom, or be part of a united Ireland. The vote went heavily in favour (98.9%) of maintaining the status quo. Approximately 57.5% of the total electorate voted in support, but only 1% of Catholics voted following a boycott organised by the Social Democratic and Labour Party (SDLP).

The Troubles were brought to an uneasy end by a peace process which included the declaration of ceasefires by most paramilitary organisations and the complete decommissioning of their weapons, the reform of the police, and the corresponding withdrawal of army troops from the streets and from sensitive border areas such as South Armagh and Fermanagh, as agreed by the signatories to the Belfast Agreement (commonly known as the "Good Friday Agreement"). This reiterated the long-held British position, which had never before been fully acknowledged by successive Irish governments, that Northern Ireland will remain within the United Kingdom until a majority of voters in Northern Ireland decides otherwise. The Constitution of Ireland was amended in 1999 to remove a claim of the "Irish nation" to sovereignty over the entire island (in Article 2).
The new Articles 2 and 3, added to the Constitution to replace the earlier articles, implicitly acknowledge that the status of Northern Ireland, and its relationships within the rest of the United Kingdom and with the Republic of Ireland, would only be changed with the agreement of a majority of voters in each jurisdiction. This aspect was also central to the Belfast Agreement which was signed in 1998 and ratified by referendums held simultaneously in both Northern Ireland and the Republic. At the same time, the British Government recognised for the first time, as part of the prospective, the so-called "Irish dimension": the principle that the people of the island of Ireland as a whole have the right, without any outside interference, to solve the issues between North and South by mutual consent. The latter statement was key to winning support for the agreement from nationalists. It established a devolved power-sharing government within Northern Ireland, which must consist of both unionist and nationalist parties. These institutions were suspended by the British Government in 2002 after Police Service of Northern Ireland (PSNI) allegations of spying by people working for Sinn Féin at the Assembly (Stormontgate). The resulting case against the accused Sinn Féin member collapsed.

On 28 July 2005, the Provisional IRA declared an end to its campaign and has since decommissioned what is thought to be all of its arsenal. This final act of decommissioning was performed under the watch of the Independent International Commission on Decommissioning (IICD) and two external church witnesses. Many unionists, however, remained sceptical. The IICD later confirmed that the main loyalist paramilitary groups, the Ulster Defence Association, UVF and the Red Hand Commando, had decommissioned what is thought to be all of their arsenals, witnessed by former archbishop Robin Eames and a former top civil servant.

Politicians elected to the Assembly at the 2003 Assembly election were called together on 15 May 2006 under the Northern Ireland Act 2006 for the purpose of electing a First Minister and deputy First Minister of Northern Ireland and choosing the members of an Executive (before 25 November 2006) as a preliminary step to the restoration of devolved government.

Following the election held on 7 March 2007, devolved government returned on 8 May 2007 with Democratic Unionist Party (DUP) leader Ian Paisley and Sinn Féin deputy leader Martin McGuinness taking office as First Minister and deputy First Minister, respectively. In its white paper on Brexit the United Kingdom government reiterated its commitment to the Belfast Agreement. With regard to Northern Ireland's status, it said that the UK Government's "clearly-stated preference is to retain Northern Ireland’s current constitutional position: as part of the UK, but with strong links to Ireland".

The main political divide in Northern Ireland is between unionists, who wish to see Northern Ireland continue as part of the United Kingdom, and nationalists, who wish to see Northern Ireland unified with the Republic of Ireland, independent from the United Kingdom. These two opposing views are linked to deeper cultural divisions. Unionists are predominantly Ulster Protestant, descendants of mainly Scottish, English, and Huguenot settlers as well as Gaels who converted to one of the Protestant denominations. Nationalists are overwhelmingly Catholic and descend from the population predating the settlement, with a minority from the Scottish Highlands as well as some converts from Protestantism. Discrimination against nationalists under the Stormont government (1921–1972) gave rise to the civil rights movement in the 1960s.

While some unionists argue that discrimination was not just due to religious or political bigotry, but also the result of more complex socio-economic, socio-political and geographical factors, its existence, and the manner in which nationalist anger at it was handled, were a major contributing factor to the Troubles. The political unrest went through its most violent phase between 1968 and 1994.

In 2007, 36% of the population defined themselves as unionist, 24% as nationalist and 40% defined themselves as neither. According to a 2015 opinion poll, 70% express a long-term preference of the maintenance of Northern Ireland's membership of the United Kingdom (either directly ruled or with devolved government), while 14% express a preference for membership of a united Ireland. This discrepancy can be explained by the overwhelming preference among Protestants to remain a part of the UK (93%), while Catholic preferences are spread across a number of solutions to the constitutional question including remaining a part of the UK (47%), a united Ireland (32%), Northern Ireland becoming an independent state (4%), and those who "don't know" (16%).

Official voting figures, which reflect views on the "national question" along with issues of candidate, geography, personal loyalty and historic voting patterns, show 54% of Northern Ireland voters vote for unionist parties, 42% vote for nationalist parties and 4% vote "other". Opinion polls consistently show that the election results are not necessarily an indication of the electorate's stance regarding the constitutional status of Northern Ireland. Most of the population of Northern Ireland are at least nominally Christian, mostly Roman Catholic and Protestant denominations. Many voters (regardless of religious affiliation) are attracted to unionism's conservative policies, while other voters are instead attracted to the traditionally leftist Sinn Féin and SDLP and their respective party platforms for democratic socialism and social democracy.

For the most part, Protestants feel a strong connection with Great Britain and wish for Northern Ireland to remain part of the United Kingdom. Many Catholics however, generally aspire to a United Ireland or are less certain about how to solve the constitutional question. In the 2015 survey by Northern Ireland Life and Times, 47% of Northern Irish Catholics supported Northern Ireland remaining a part of the United Kingdom, either by direct rule (6%) or devolved government (41%).

Protestants have a slight majority in Northern Ireland, according to the latest Northern Ireland Census. The make-up of the Northern Ireland Assembly reflects the appeals of the various parties within the population. Of the 108 Members of the Legislative Assembly (MLAs), 56 are unionists and 40 are nationalists (the remaining 12 are classified as "other").

Since 1998, Northern Ireland has had devolved government within the United Kingdom, presided over by the Northern Ireland Assembly and a cross-community government (the Northern Ireland Executive). The UK Government and UK Parliament are responsible for reserved and excepted matters. Reserved matters comprise listed policy areas (such as civil aviation, units of measurement, and human genetics) that Parliament may devolve to the Assembly some time in the future. Excepted matters (such as international relations, taxation and elections) are never expected to be considered for devolution. On all other governmental matters, the Executive together with the 90-member Assembly may legislate for and govern Northern Ireland. Devolution in Northern Ireland is dependent upon participation by members of the Northern Ireland executive in the North/South Ministerial Council, which coordinates areas of co-operation (such as agriculture, education and health) between Northern Ireland and the Republic of Ireland. Additionally, "in recognition of the Irish Government's special interest in Northern Ireland", the Government of Ireland and Government of the United Kingdom co-operate closely on non-devolved matters through the British-Irish Intergovernmental Conference.

Elections to the Northern Ireland Assembly are by single transferable vote with five Members of the Legislative Assembly (MLAs) elected from each of 18 parliamentary constituencies. In addition, eighteen representatives (Members of Parliament, MPs) are elected to the lower house of the UK parliament from the same constituencies using the first-past-the-post system. However, not all of those elected take their seats. Sinn Féin MPs, currently seven, refuse to take the oath to serve the Queen that is required before MPs are allowed to take their seats. In addition, the upper house of the UK parliament, the House of Lords, currently has some 25 appointed members from Northern Ireland.

The Northern Ireland Office represents the UK government in Northern Ireland on reserved matters and represents Northern Ireland's interests within the UK Government. Additionally, the Republic's government also has the right to "put forward views and proposals" on non-devolved matters in relation to Northern Ireland. The Northern Ireland Office is led by the Secretary of State for Northern Ireland, who sits in the Cabinet of the United Kingdom.
Northern Ireland is a distinct legal jurisdiction, separate from the two other jurisdictions in the United Kingdom (England and Wales, and Scotland). Northern Ireland law developed from Irish law that existed before the partition of Ireland in 1921. Northern Ireland is a common law jurisdiction and its common law is similar to that in England and Wales. However, there are important differences in law and procedure between Northern Ireland and England and Wales. The body of statute law affecting Northern Ireland reflects the history of Northern Ireland, including Acts of the Parliament of the United Kingdom, the Northern Ireland Assembly, the former Parliament of Northern Ireland and the Parliament of Ireland, along with some Acts of the Parliament of England and of the Parliament of Great Britain that were extended to Ireland under Poynings' Law between 1494 and 1782.

There is no generally accepted term to describe what Northern Ireland is: province, region, country or something else. The choice of term can be controversial and can reveal the writer's political preferences. This has been noted as a problem by several writers on Northern Ireland, with no generally recommended solution.

Owing in part to the way in which the United Kingdom, and Northern Ireland, came into being, there is no legally defined term to describe what Northern Ireland 'is'. There is also no uniform or guiding way to refer to Northern Ireland amongst the agencies of the UK government. For example, the websites of the Office of the Prime Minister of the United Kingdom and the UK Statistics Authority describe the United Kingdom as being made up of four countries, one of these being Northern Ireland. Other pages on the same websites refer to Northern Ireland specifically as a "province" as do publications of the UK Statistics Authority. The website of the Northern Ireland Statistics and Research Agency also refers to Northern Ireland as being a province as does the website of the Office of Public Sector Information and other agencies within Northern Ireland. Publications of HM Treasury and the Department of Finance and Personnel of the Northern Ireland Executive, on the other hand, describe Northern Ireland as being a "region of the UK". The UK's submission to the 2007 United Nations Conference on the Standardization of Geographical Names defines the UK as being made up of two countries (England and Scotland), one principality (Wales) and one province (Northern Ireland).

Unlike England, Scotland and Wales, Northern Ireland has no history of being an independent country or of being a nation in its own right. Some writers describe the United Kingdom as being made up of three countries and one province or point out the difficulties with calling Northern Ireland a country. Authors writing specifically about Northern Ireland dismiss the idea that Northern Ireland is a "country" in general terms, and draw contrasts in this respect with England, Scotland and Wales. Even for the period covering the first 50 years of Northern Ireland's existence, the term "country" is considered inappropriate by some political scientists on the basis that many decisions were still made in London. The absence of a distinct nation of Northern Ireland, separate within the island of Ireland, is also pointed out as being a problem with using the term and is in contrast to England, Scotland, and Wales.

Many commentators prefer to use the term "province", although that is also not without problems. It can arouse irritation, particularly among nationalists, for whom the title province is properly reserved for the traditional province of Ulster, of which Northern Ireland comprises six out of nine counties. The BBC style guide is to refer to Northern Ireland as a province, and use of the term is common in literature and newspaper reports on Northern Ireland and the United Kingdom. Some authors have described the meaning of this term as being equivocal: referring to Northern Ireland as being a province both of the United Kingdom and of the traditional country of Ireland.

"Region" is used by several UK government agencies and the European Union. Some authors choose this word but note that it is "unsatisfactory". Northern Ireland can also be simply described as "part of the UK", including by UK government offices.

Many people inside and outside Northern Ireland use other names for Northern Ireland, depending on their point of view. Disagreement on names, and the reading of political symbolism into the use or non-use of a word, also attaches itself to some urban centres. The most notable example is whether Northern Ireland's second city should be called "Derry" or "Londonderry".

Choice of language and nomenclature in Northern Ireland often reveals the cultural, ethnic and religious identity of the speaker. Those who do not belong to any group but lean towards one side often tend to use the language of that group. Supporters of unionism in the British media (notably "The Daily Telegraph" and the "Daily Express") regularly call Northern Ireland "Ulster". Some media outlets in the Republic use "North of Ireland", "the North", or (less often) the "Six Counties".

Government and cultural organisations in Northern Ireland often use the word "Ulster" in their title; for example, the University of Ulster, the Ulster Museum, the Ulster Orchestra, and BBC Radio Ulster.

Although some news bulletins since the 1990s have opted to avoid all contentious terms and use the official name, Northern Ireland, the term "the North" remains commonly used by broadcast media in the Republic. 




Northern Ireland was covered by an ice sheet for most of the last ice age and on numerous previous occasions, the legacy of which can be seen in the extensive coverage of drumlins in Counties Fermanagh, Armagh, Antrim and particularly Down.

The centrepiece of Northern Ireland's geography is Lough Neagh, at the largest freshwater lake both on the island of Ireland and in the British Isles. A second extensive lake system is centred on Lower and Upper Lough Erne in Fermanagh. The largest island of Northern Ireland is Rathlin, off the north Antrim coast. Strangford Lough is the largest inlet in the British Isles, covering .
There are substantial uplands in the Sperrin Mountains (an extension of the Caledonian mountain belt) with extensive gold deposits, granite Mourne Mountains and basalt Antrim Plateau, as well as smaller ranges in South Armagh and along the Fermanagh–Tyrone border. None of the hills are especially high, with Slieve Donard in the dramatic Mournes reaching , Northern Ireland's highest point. Belfast's most prominent peak is Cavehill.
The volcanic activity which created the Antrim Plateau also formed the eerily geometric pillars of the Giant's Causeway on the north Antrim coast. Also in north Antrim are the Carrick-a-Rede Rope Bridge, Mussenden Temple and the Glens of Antrim.
The Lower and Upper River Bann, River Foyle and River Blackwater form extensive fertile lowlands, with excellent arable land also found in North and East Down, although much of the hill country is marginal and suitable largely for animal husbandry.

The valley of the River Lagan is dominated by Belfast, whose metropolitan area includes over a third of the population of Northern Ireland, with heavy urbanisation and industrialisation along the Lagan Valley and both shores of Belfast Lough.

The vast majority of Northern Ireland has a temperate maritime climate, ("Cfb" in the Koeppen climate classification) rather wetter in the west than the east, although cloud cover is very common across the region. The weather is unpredictable at all times of the year, and although the seasons are distinct, they are considerably less pronounced than in interior Europe or the eastern seaboard of North America. Average daytime maximums in Belfast are in January and in July. The highest maximum temperature recorded was at Knockarevan, near Garrison, County Fermanagh on 30 June 1976 and at Belfast on 12 July 1983. The lowest minimum temperature recorded was at Castlederg, County Tyrone on 23 December 2010.

Northern Ireland is the least forested part of the United Kingdom and Ireland, and one of the least forested parts of Europe. Until the end of the Middle Ages, the land was heavily forested with native trees such as oak, ash, hazel, birch, alder, willow, aspen, elm, rowan, yew and Scots pine. Today, only 8% of Northern Ireland is woodland, and most of this is non-native conifer plantations.

Northern Ireland consists of six historic counties: County Antrim, County Armagh, County Down, County Fermanagh, County Londonderry, County Tyrone.

These counties are no longer used for local government purposes; instead, there are eleven districts of Northern Ireland which have different geographical extents. These were created in 2015, replacing the twenty-six districts which previously existed.
Although counties are no longer used for local governmental purposes, they remain a popular means of describing where places are. They are officially used while applying for an Irish passport, which requires one to state one's county of birth. The name of that county then appears in both Irish and English on the passport's information page, as opposed to the town or city of birth on the United Kingdom passport. The Gaelic Athletic Association still uses the counties as its primary means of organisation and fields representative teams of each GAA county. The original system of car registration numbers largely based on counties still remains in use. In 2000, the telephone numbering system was restructured into an 8 digit scheme with (except for Belfast) the first digit approximately reflecting the county.

The county boundaries still appear on Ordnance Survey of Northern Ireland Maps and the Phillips Street Atlases, among others. With their decline in official use, there is often confusion surrounding towns and cities which lie near county boundaries, such as Belfast and Lisburn, which are split between counties Down and Antrim (the majorities of both cities, however, are in Antrim).

In March 2018, "The Sunday Times" published its list of Best Places to Live in Britain, including the following places in Northern Ireland: Ballyhackamore near Belfast (overall best for Northern Ireland), Holywood, County Down, Newcastle, County Down, Portrush, County Antrim, Strangford, County Down.

Northern Ireland has traditionally had an industrial economy, most notably in shipbuilding, rope manufacture and textiles, but most heavy industry has since been replaced by services, primarily the public sector.

Seventy percent of the economy's revenue comes from the service sector. Apart from the public sector, another important service sector is tourism, which rose to account for over 1% of the economy's revenue in 2004. Tourism has been a major growth area since the end of the Troubles. Key tourism attractions include the historic cities of Derry, Belfast and Armagh and the many castles in Northern Ireland. These large firms are attracted by government subsidies and the skilled workforce in Northern Ireland.

The local economy has seen contraction during the Great Recession. In response, the Northern Ireland Assembly has sent trade missions abroad. The Executive wishes to gain taxation powers from London, to align Northern Ireland's corporation tax rate with the unusually low rate of the Republic of Ireland.

Northern Ireland has underdeveloped transport infrastructure, with most infrastructure concentrated around Greater Belfast, Greater Derry and Craigavon. Northern Ireland is served by three airports – Belfast International near Antrim, George Best Belfast City integrated into the railway network at Sydenham in East Belfast, and City of Derry in County Londonderry.

Major seaports at Larne and Belfast carry passengers and freight between Great Britain and Northern Ireland.

Passenger railways are operated by Northern Ireland Railways. With Iarnród Éireann (Irish Rail), Northern Ireland Railways co-operates in providing the joint Enterprise service between Dublin Connolly and Lanyon Place. The whole of Ireland has a mainline railway network with a gauge of, which is unique in Europe and has resulted in distinct rolling stock designs. The only preserved line of this gauge is the Downpatrick and County Down Railway, which operates steam and diesel locomotives. Main railway lines linking to and from Belfast Great Victoria Street railway station and Lanyon Place railway station are:


Main motorways are:

The cross-border road connecting the ports of Larne in Northern Ireland and Rosslare Harbour in the Republic of Ireland is being upgraded as part of an EU-funded scheme. European route E01 runs from Larne through the island of Ireland, Spain and Portugal to Seville.

The population of Northern Ireland has risen yearly since 1978. The population in 2011 was 1.8 million, having grown 7.5% over the previous decade from just under 1.7 million in 2001. This constitutes just under 3% of the population of the UK (62 million) and just over 28% of the population of the island of Ireland (6.3 million).

The population of Northern Ireland is almost entirely white (98.2%). In 2011, 88.8% of the population were born in Northern Ireland, with 4.5% born elsewhere in Britain, and 2.9% born in the Republic of Ireland. 4.3% were born elsewhere; triple the amount there were in 2001. Most are from Eastern Europe and Lithuania and Latvia. The largest non-white ethnic groups were Chinese (6,300) and Indian (6,200). Black people of various origins made up 0.2% of the 2011 population and people of mixed ethnicity made up 0.2%.

At the 2011 census, 41.5% of the population identified as Protestant/non-Roman Catholic Christian, 41% as Roman Catholic, and 0.8% as non-Christian, while 17% identified with no religion or did not state one. The biggest of the Protestant/non-Roman Catholic Christian denominations were the Presbyterian Church (19%), the Church of Ireland (14%) and the Methodist Church (3%). In terms of community background (i.e. religion or religion brought up in), 48% of the population came from a Protestant background, 45% from a Catholic background, 0.9% from non-Christian backgrounds, and 5.6% from non-religious backgrounds.

In the 2011 census in Northern Ireland respondents gave their national identity as follows.

Several studies and surveys carried out between 1971 and 2006 have indicated that, in general, most Protestants in Northern Ireland see themselves primarily as British, whereas a majority of Roman Catholics regard themselves primarily as Irish. This does not, however, account for the complex identities within Northern Ireland, given that many of the population regard themselves as "Ulster" or "Northern Irish", either as a primary or secondary identity. Overall, the Catholic population is somewhat more ethnically diverse than the more homogeneous Protestant population. 83.1% of Protestants identified as "British" or with a British ethnic group (English, Scottish, or Welsh) in the 2011 Census, whereas only 3.9% identified as "Irish". Meanwhile, 13.7% of Catholics identified as "British" or with a British ethnic group. A further 4.4% identified as "all other", which are largely immigrants, for example from Poland.

A 2008 survey found that 57% of Protestants described themselves as British, while 32% identified as Northern Irish, 6% as Ulster and 4% as Irish. Compared to a similar survey carried out in 1998, this shows a fall in the percentage of Protestants identifying as British and Ulster and a rise in those identifying as Northern Irish. The 2008 survey found that 61% of Catholics described themselves as Irish, with 25% identifying as Northern Irish, 8% as British and 1% as Ulster. These figures were largely unchanged from the 1998 results.

People born in Northern Ireland are, with some exceptions, deemed by UK law to be citizens of the United Kingdom. They are also, with similar exceptions, entitled to be citizens of Ireland. This entitlement was reaffirmed in the 1998 Good Friday Agreement between the British and Irish governments, which provides that:

...it is the birthright of all the people of Northern Ireland to identify themselves and be accepted as Irish or British, or both, as they may so choose, and accordingly [the two governments] confirm that their right to hold both British and Irish citizenship is accepted by both Governments and would not be affected by any future change in the status of Northern Ireland.
As a result of the Agreement, the Constitution of the Republic of Ireland was amended. The current wording provides that people born in Northern Ireland are entitled to be Irish citizens on the same basis as people from any other part of the island.

Neither government, however, extends its citizenship to all persons born in Northern Ireland. Both governments exclude some people born in Northern Ireland, in particular persons born without one parent who is a British or Irish citizen. The Irish restriction was given effect by the twenty-seventh amendment to the Irish Constitution in 2004. The position in UK nationality law is that most of those born in Northern Ireland are UK nationals, whether or not they so choose. Renunciation of British citizenship requires the payment of a fee, currently £372.

In the 2011 census in Northern Ireland respondents stated that they held the following passports.

English is spoken as a first language by almost all of the Northern Ireland population. It is the "de facto" official language and the Administration of Justice (Language) Act (Ireland) 1737 prohibits the use of languages other than English in legal proceedings.

Under the Good Friday Agreement, Irish and Ulster Scots (an Ulster dialect of the Scots language, sometimes known as "Ullans"), are recognised as "part of the cultural wealth of Northern Ireland". Two all-island bodies for the promotion of these were created under the Agreement: "Foras na Gaeilge", which promotes the Irish language, and the Ulster Scots Agency, which promotes the Ulster Scots dialect and culture. These operate separately under the aegis of the North/South Language Body, which reports to the North/South Ministerial Council.

The British government in 2001 ratified the European Charter for Regional or Minority Languages. Irish (in Northern Ireland) was specified under Part III of the Charter, with a range of specific undertakings in relation to education, translation of statutes, interaction with public authorities, the use of placenames, media access, support for cultural activities and other matters. A lower level of recognition was accorded to Ulster Scots, under Part II of the Charter.

The dialect of English spoken in Northern Ireland shows influence from the lowland Scots language. There are supposedly some minute differences in pronunciation between Protestants and Catholics, for instance; the name of the letter "h", which Protestants tend to pronounce as "aitch", as in British English, and Catholics tend to pronounce as "haitch", as in Hiberno-English. However, geography is a much more important determinant of dialect than religious background.

The Irish language (), or "Gaelic", is a native language of Ireland. It was spoken predominantly throughout what is now Northern Ireland before the Ulster Plantations in the 17th century and most place names in Northern Ireland are anglicised versions of a Gaelic name. Today, the language is often associated with Irish nationalism (and thus with Catholics). However, in the 19th century, the language was seen as a common heritage, with Ulster Protestants playing a leading role in the Gaelic revival.

In the 2011 census, 11% of the population of Northern Ireland claimed "some knowledge of Irish" and 3.7% reported being able to "speak, read, write and understand" Irish. In another survey, from 1999, 1% of respondents said they spoke it as their main language at home.

The dialect spoken in Northern Ireland, Ulster Irish, has two main types, East Ulster Irish and Donegal Irish (or West Ulster Irish), is the one closest to Scottish Gaelic (which developed into a separate language from Irish Gaelic in the 17th century). Some words and phrases are shared with Scots Gaelic, and the dialects of east Ulster – those of Rathlin Island and the Glens of Antrim – were very similar to the dialect of Argyll, the part of Scotland nearest to Ireland. And those dialects of Armagh and Down were also very similar to the dialects of Galloway.

Use of the Irish language in Northern Ireland today is politically sensitive. The erection by some district councils of bilingual street names in both English and Irish, invariably in predominantly nationalist districts, is resisted by unionists who claim that it creates a "chill factor" and thus harms community relationships. Efforts by members of the Northern Ireland Assembly to legislate for some official uses of the language have failed to achieve the required cross-community support, and the UK government has declined to legislate. There has recently been an increase in interest in the language among unionists in East Belfast.

Ulster Scots comprises varieties of the Scots language spoken in Northern Ireland. For a native English speaker, "[Ulster Scots] is comparatively accessible, and even at its most intense can be understood fairly easily with the help of a glossary."

Along with the Irish language, the Good Friday Agreement recognised the dialect as part of Northern Ireland's unique culture and the St Andrews Agreement recognised the need to "enhance and develop the Ulster Scots language, heritage and culture".

Approximately 2% of the population claim to speak Ulster Scots. However, the number speaking it as their main language in their home is negligible, with only 0.9% of 2011 census respondents claiming to be able to speak, read, write and understand Ulster-Scots. 8.1% professed to have "some ability" however.

The most common sign language in Northern Ireland is Northern Ireland Sign Language (NISL). However, because in the past Catholic families tended to send their deaf children to schools in Dublin where Irish Sign Language (ISL) is commonly used, ISL is still common among many older deaf people from Catholic families.

Irish Sign Language (ISL) has some influence from the French family of sign language, which includes American Sign Language (ASL). NISL takes a large component from the British family of sign language (which also includes Auslan) with many borrowings from ASL. It is described as being related to Irish Sign Language at the syntactic level while much of the lexicon is based on British Sign Language (BSL).

Northern Ireland shares both the culture of Ireland and the culture of the United Kingdom.

Parades are a prominent feature of Northern Ireland society, more so than in the rest of Ireland or in Britain. Most are held by Protestant fraternities such as the Orange Order, and Ulster loyalist marching bands. Each summer, during the "marching season", these groups have hundreds of parades, deck streets with British flags, bunting and specially-made arches, and light large towering bonfires. The biggest parades are held on 12 July (The Twelfth). There is often tension when these activities take place near Catholic neighbourhoods, which sometimes leads to violence.

Since the end of the Troubles, Northern Ireland has witnessed rising numbers of tourists. Attractions include cultural festivals, musical and artistic traditions, countryside and geographical sites of interest, public houses, welcoming hospitality and sports (especially golf and fishing). Since 1987 public houses have been allowed to open on Sundays, despite some opposition.

The Ulster Cycle is a large body of prose and verse centring on the traditional heroes of the Ulaid in what is now eastern Ulster. This is one of the four major cycles of Irish mythology. The cycle centres on the reign of Conchobar mac Nessa, who is said to have been king of Ulster around the 1st century. He ruled from Emain Macha (now Navan Fort near Armagh), and had a fierce rivalry with queen Medb and king Ailill of Connacht and their ally, Fergus mac Róich, former king of Ulster. The foremost hero of the cycle is Conchobar's nephew Cúchulainn, who features in the epic prose/poem "An Táin Bó Cúailnge" (The Cattle Raid of Cooley, a "cassus belli" between Ulster and Connaught).

Northern Ireland comprises a patchwork of communities whose national loyalties are represented in some areas by flags flown from flagpoles or lamp posts. The Union Jack and the former Northern Ireland flag are flown in many loyalist areas, and the Tricolour, adopted by republicans as the flag of Ireland in 1916, is flown in some republican areas. Even kerbstones in some areas are painted red-white-blue or green-white-orange, depending on whether local people express unionist/loyalist or nationalist/republican sympathies.

The official flag is that of the state having sovereignty over the territory, i.e. the Union Flag. The former Northern Ireland flag, also known as the "Ulster Banner" or "Red Hand Flag", is a banner derived from the coat of arms of the Government of Northern Ireland until 1972. Since 1972, it has had no official status. The Union Flag and the Ulster Banner are used exclusively by unionists. UK flags policy states that in Northern Ireland, "The Ulster flag and the Cross of St Patrick have no official status and, under the Flags Regulations, are not permitted to be flown from Government Buildings."

The Irish Rugby Football Union and the Church of Ireland have used the Saint Patrick's Saltire or "Cross of St Patrick". This red saltire on a white field was used to represent Ireland in the flag of the United Kingdom. It is still used by some British army regiments. Foreign flags are also found, such as the Palestinian flags in some nationalist areas and Israeli flags in some unionist areas.

The United Kingdom national anthem of "God Save the Queen" is often played at state events in Northern Ireland. At the Commonwealth Games and some other sporting events, the Northern Ireland team uses the Ulster Banner as its flag—notwithstanding its lack of official status—and the "Londonderry Air" (usually set to lyrics as "Danny Boy"), which also has no official status, as its national anthem. The national football team also uses the Ulster Banner as its flag but uses "God Save The Queen" as its anthem.
Major Gaelic Athletic Association matches are opened by the national anthem of the Republic of Ireland, "Amhrán na bhFiann (The Soldier's Song)", which is also used by most other all-Ireland sporting organisations.
Since 1995, the Ireland rugby union team has used a specially commissioned song, "Ireland's Call" as the team's anthem. The Irish national anthem is also played at Dublin home matches, being the anthem of the host country.

Northern Irish murals have become well-known features of Northern Ireland, depicting past and present events and documenting peace and cultural diversity. Almost 2,000 murals have been documented in Northern Ireland since the 1970s.

In Northern Ireland, sport is popular and important in the lives of many people. Sports tend to be organised on an all-Ireland basis, with a single team for the whole island. The most notable exception is association football, which has separate governing bodies for each jurisdiction.

The Irish Football Association (IFA) serves as the organising body for association football in Northern Ireland, with the Northern Ireland Football League (NIFL) responsible for the independent administration of the three divisions of national domestic football, as well as the Northern Ireland Football League Cup.

The highest level of competition within Northern Ireland are the NIFL Premiership and the NIFL Championship. However, many players from Northern Ireland compete with clubs in England and Scotland.

NIFL clubs are semi-professional or Intermediate.NIFL Premiership clubs are also eligible to compete in the UEFA Champions League and UEFA Europa League with the league champions entering the Champions league second qualifying round and the 2nd placed league finisher, the European play-off winners and the Irish Cup winners entering the Europa League second qualifying round. No clubs have ever reached the group stage.

Despite Northern Ireland's small population, the national team qualified for the World Cup in 1958, 1982 and 1986, making it to the quarter-finals in 1958 and 1982 and made it the first knockout round in the European Championships in 2016.

The six counties of Northern Ireland are among the nine governed by the Ulster branch of the Irish Rugby Football Union, the governing body of rugby union in Ireland. Ulster is one of the four professional provincial teams in Ireland and competes in the Pro14 and European Cup. It won the European Cup in 1999.

In international competitions, the Ireland national rugby union team's recent successes include four Triple Crowns between 2004 and 2009 and a Grand Slam in 2009 in the Six Nations Championship.

Northern Ireland plays as the Ireland cricket team which represents both Northern Ireland and Republic of Ireland. The Ireland Cricket team is a full member of the International Cricket Council, having been granted Test status and full membership (along with Afghanistan) by the ICC in June 2017. They are currently able to compete in Test cricket, the highest level of competitive cricket in the international arena and they are one of the twelve full-member countries under the ICC.

Ireland is the current champion of the ICC Intercontinental Cup. One of Ireland's regular international venues is Stormont in Belfast.

Gaelic games include Gaelic football, hurling (and camogie), handball and rounders. Of the four, football is the most popular in Northern Ireland. Players play for local clubs with the best being selected for their county teams. The Ulster GAA is the branch of the Gaelic Athletic Association that is responsible for the nine counties of Ulster, which include the six of Northern Ireland.

These nine county teams participate in the Ulster Senior Football Championship, Ulster Senior Hurling Championship, All-Ireland Senior Football Championship and All-Ireland Senior Hurling Championship.

Recent successes for Northern Ireland teams include Armagh's 2002 All-Ireland Senior Football Championship win and Tyrone's wins in 2003, 2005 and 2008.

Perhaps Northern Ireland's most notable successes in professional sport have come in golf. Northern Ireland has contributed more major champions in the modern era than any other European country, with three in the space of just 14 months from the U.S. Open in 2010 to The Open Championship in 2011. Notable golfers include Fred Daly (winner of The Open in 1947), Ryder Cup players Ronan Rafferty and David Feherty, leading European Tour professionals David Jones, Michael Hoey (a five-time winner on the tour) and Gareth Maybin, as well as three recent major winners Graeme McDowell (winner of the U.S. Open in 2010, the first European to do so since 1970), Rory McIlroy (winner of four majors) and Darren Clarke (winner of The Open in 2011). Northern Ireland has also contributed several players to the Great Britain and Ireland Walker Cup team, including Alan Dunbar and Paul Cutler who played on the victorious 2011 team in Scotland. Dunbar also won The Amateur Championship in 2012, at Royal Troon.

The Golfing Union of Ireland, the governing body for men's and boy's amateur golf throughout Ireland and the oldest golfing union in the world, was founded in Belfast in 1891. Northern Ireland's golf courses include the Royal Belfast Golf Club (the earliest, formed in 1881), Royal Portrush Golf Club, which is the only course outside Great Britain to have hosted The Open Championship, and Royal County Down Golf Club ("Golf Digest" magazine's top-rated course outside the United States).

Northern Ireland has produced two world snooker champions; Alex Higgins, who won the title in 1972 and 1982, and Dennis Taylor, who won in 1985. The highest-ranked Northern Ireland professional on the world circuit presently is Mark Allen from Antrim. The sport is governed locally by the Northern Ireland Billiards and Snooker Association who run regular ranking tournaments and competitions.

Motorcycle racing is a particularly popular sport during the summer months, with the main meetings of the season attracting some of the largest crowds to any outdoor sporting event in the whole of Ireland. Two of the three major international road race meetings are held in Northern Ireland, these being the North West 200 and the Ulster Grand Prix. In addition racing on purpose built circuits take place at Kirkistown and Bishop's Court, whilst smaller road race meetings are held such as the Cookstown 100, the Armoy Road Races and the Tandragee 100 all of which form part of the Irish National Road Race Championships and which have produced some of the greatest motorcycle racers in the history of the sport, notably Joey Dunlop.

Although Northern Ireland lacks an international automobile racecourse, two Northern Irish drivers have finished inside the top two of Formula One, with John Watson achieving the feat in 1982 and Eddie Irvine doing the same in 1999. The largest course and the only MSA-licensed track for UK-wide competition is Kirkistown.

The Ireland national rugby league team has participated in the Emerging Nations Tournament (1995), the Super League World Nines (1996), the World Cup (2000 and 2008), European Nations Cup (since 2003) and Victory Cup (2004).

The Ireland A rugby league team compete annually in the Amateur Four Nations competition (since 2002) and the St Patrick's Day Challenge (since 1995).

The Belfast Giants have competed in the Elite Ice Hockey League since the 2000-01 season and are the sole Northern Irish team in the league. The team's roster has featured Northern Irish born players such as Mark Morrison, Graeme Walton and Gareth Roberts among others.

Geraldine Heaney, an Olympic gold medalist and one of the first women inducted into the IIHF Hall of Fame, competed internationally for Canada but was born in Northern Ireland.

Owen Nolan, (born 12 February 1972) is a Canadian former professional ice hockey player born in Northern Ireland. He was drafted 1st overall in the 1990 NHL Draft by the Quebec Nordiques. 

In 2007, after the closure of UCW (Ulster Championship Wrestling) which was a wrestling promotion, PWU formed, standing for Pro Wrestling Ulster. The wrestling promotion features championships, former WWE superstars and local independent wrestlers. Events and IPPV's throughout Northern Ireland.

Unlike most areas of the United Kingdom, in the last year of primary school, many children sit entrance examinations for grammar schools.

Integrated schools, which attempt to ensure a balance in enrolment between pupils of Protestant, Roman Catholic and other faiths (or none), are becoming increasingly popular, although Northern Ireland still has a primarily "de facto" religiously segregated education system. In the primary school sector, 40 schools (8.9% of the total number) are integrated schools and 32 (7.2% of the total number) are Irish language-medium schools.

The main universities in Northern Ireland are Queen's University Belfast and Ulster University, and the distance learning Open University which has a regional office in Belfast.

356 species of marine algae have been recorded in the north-east of Ireland. As Counties Londonderry, Antrim and Down are the only three counties of Northern Ireland with a shoreline this will apply to all Northern Ireland. 77 species are considered rare having been recorded rarely.

The BBC has a division called BBC Northern Ireland with headquarters in Belfast. As well as broadcasting standard UK-wide programmes, BBC NI produces local content, including a news break-out called BBC Newsline. The ITV franchise in Northern Ireland is Ulster Television (UTV). The state-owned Channel 4 and the privately owned Channel 5 also broadcast in Northern Ireland. Access is available to satellite and cable services. All Northern Ireland viewers must obtain a UK TV licence to watch live television transmissions.

RTÉ, the national broadcaster of the Republic of Ireland, is available over the air to most parts of Northern Ireland via reception overspill and via satellite and cable. Since the digital TV switchover, RTÉ One, RTÉ2 and the Irish-language channel TG4, are now available over the air on the UK's Freeview system from transmitters within Northern Ireland. Although they are transmitted in standard definition, a Freeview HD box or television is required for reception.

As well as the standard UK-wide radio stations from the BBC, Northern Ireland is home to many local radio stations, such as Cool FM, CityBeat, and Q102.9. The BBC has two regional radio stations which broadcast in Northern Ireland, BBC Radio Ulster and BBC Radio Foyle.

Besides the UK and Irish national newspapers, there are three main regional newspapers published in Northern Ireland. These are the "Belfast Telegraph", the "Irish News" and the "News Letter". According to the Audit Bureau of Circulations (UK) the average daily circulation for these three titles in 2018 was:

Northern Ireland uses the same telecommunications and postal services as the rest of the United Kingdom at standard domestic rates and there are no mobile roaming charges between Great Britain and Northern Ireland. People in Northern Ireland who live close to the border with the Republic of Ireland may inadvertently switch over to the Irish mobile networks, causing international roaming fees to be applied. Calls from landlines in Northern Ireland to numbers in the Republic of Ireland are charged at the same rate as those to numbers in Great Britain, while landline numbers in Northern Ireland can similarly be called from the Republic of Ireland at domestic rates, using the 048 prefix.





</doc>
<doc id="21267" url="https://en.wikipedia.org/wiki?curid=21267" title="Nasjonal Samling">
Nasjonal Samling

Nasjonal Samling (, NS; literally "National Rally") was a Norwegian far-right party active from 1933 to 1945. It was the only legal party of Norway from 1942 to 1945. It was founded by former minister of defence Vidkun Quisling and a group of supporters such as Johan Bernhard Hjortwho led the party's paramilitary wing ("Hirden") for a short time before leaving the party in 1937 after various internal conflicts. The party celebrated its founding on 17 May, Norway's national holiday, but was founded on 13 May 1933.

The party never gained direct political influence, but it made its mark on Norwegian politics nonetheless. Despite the fact that it never managed to get more than 2.5% of the vote and failed to elect even one candidate to the Storting, it became a factor by polarising the political scene. The established parties in Norway viewed it as a Norwegian version of the German Nazis, and generally refused to cooperate with it in any way. Several of its marches and rallies before the war were either banned, or marred by violence when communists and socialists clashed with the Hird.

A significant trait of the party throughout its existence was a relatively high level of internal conflict. Antisemitism, anti-Masonry and differing views on religion, as well as the party's association with the Nazis and Germany, were hotly debated, and factioned the party. By the time the Second World War broke out, the party had been reduced to a political sect with hardly any real activity.

Strong belief in Norse Paganism, Romantic nationalism and totalitarianism dominated NS ideology. It also relied heavily on Nordic symbolism, using Vikings, pre-Christian religion and runes in its propaganda and speeches. It asserted that its symbol (shown at the head of this article), a golden sun cross on a red background (colours of the coat of arms of Norway), had been the symbol of St. Olaf, painted on his shield.

When Germany invaded Norway in April 1940, Quisling marched into the Norwegian Broadcasting Corporation studios in Oslo and made a radio broadcast proclaiming himself Prime Minister and ordering all anti-German resistance to end immediately. However, King Haakon VII, in unoccupied territory along with the legitimate government, let it be known he would abdicate rather than appoint any government headed by Quisling. The existing government refused to step down in Quisling's favour or serve under him, and confirmed that resistance was to be continued. With no popular support, the German forces of occupation quickly thrust Quisling aside. In April 1940 the party probably only had a few hundred members, but membership rose to 22,000 in December the same year, and peaked with 43,400 in November 1943.

After a brief period with a civilian caretaker government ("Administrasjonsrådet") appointed by the Supreme Court, the Germans took control through Reichskommissar Josef Terboven. He appointed a government responsible to himself, with most ministers from the ranks of Nasjonal Samling. However, the party leader, Quisling, was controversial in Norway as well as among the occupiers, and was denied a formal position until 1 February 1942, when he became "minister president" of the "national government". Other important ministers were Jonas Lie (also head of the Norwegian wing of the SS from 1941) as minister of police, Gulbrand Lunde as minister of "popular enlightenment and propaganda", and the opera singer Albert Viljam Hagelin, who was Minister of Home Affairs. The NS administration had a certain amount of autonomy in purely civilian matters, but it was in reality controlled by the Reichskommissar as "head of state", subordinate only to Adolf Hitler.

The post-war authorities proscribed the party and prosecuted its members as collaborators. Nearly 50,000 were brought to trial, approximately half of whom received prison sentences. The authorities executed Quisling for treason as well as a few other high-profile NS members, and prominent German officials in Norway, for war crimes. The sentences' lawfulness has been questioned, however, as Norway did not have capital punishment in peace-time, and the Norwegian constitution at the time stipulated that capital punishment for war crimes had to be carried out during actual wartime.

Another issue of post-war treatment has been the ongoing Hamsun debate in Norway. The internationally renowned author Knut Hamsun, although never a member, was a well-known NS sympathiser. After the war, Hamsun was, however, deemed mentally unfit to stand trial, and the issue of his links to the party has never been properly resolved. Hamsun's status as a Nobel Prize laureate and probably the best-known Norwegian author next to Henrik Ibsen also results in his ties to NS being a touchy subject, as many feel the valuation of Hamsun's literature should not be marred by constant debate about whether or not he was a fascist.



</doc>
<doc id="21269" url="https://en.wikipedia.org/wiki?curid=21269" title="Nasreddin">
Nasreddin

Nasreddin or Nasreddin Hodja or Mullah Nasreddin Hooja () or Mullah Nasruddin was a Seljuq satirist, born in Hortu Village in Sivrihisar, Eskişehir Province, present-day Turkey and died in 13th century in Akşehir, near Konya, a capital of the Seljuk Sultanate of Rum, in today's Turkey. He is considered a populist philosopher, Sufi and wise man, remembered for his funny stories and anecdotes. He appears in thousands of stories, sometimes witty, sometimes wise, but often, too, a fool or the butt of a joke. A Nasreddin story usually has a subtle humour and a pedagogic nature. The International Nasreddin Hodja festival is celebrated between 5 and 10 July in his hometown every year.

Claims about his origin are made by many ethnic groups. Many sources give the birthplace of Nasreddin as Hortu Village in Sivrihisar, Eskişehir Province, present-day Turkey, in the 13th century, after which he settled in Akşehir, and later in Konya under the Seljuq rule, where he died in 1275/6 or 1285/6 CE. The alleged tomb of Nasreddin is in Akşehir and the ""International Nasreddin Hodja Festival"" is held annually in Akşehir between 5–10 July.

According to Prof. Mikail Bayram who made an extensive research on Nasreddin, his full name is Nasir ud-din Mahmood al-Khoyi, his title Ahi Evran (as being the leader of the ahi organization). According to him, Nasreddin was born in the city of Khoy in West Azerbaijan Province of Iran, had his education in Khorasan and became the pupil of famous Quran mufassir Fakhr al-Din al-Razi in Herat. He was sent to Anatolia by the Khalif in Baghdad to organize resistance and uprising against the Mongol invasion. He served as a kadı (an Islamic judge and ombudsman) in Kayseri. This explains why he addresses judicial problems in the jokes not only religious ones. During the turmoil of the Mongol invasion he became a political opponent of Persian Rumi. He was addressed in Masnavi by juha anecdotes for this reason. He became the vazir at the court of Kaykaus II. Having lived in numerous cities in vast area and being steadfastly against the Mongol invasion as well as having his witty character, he was embraced by various nations and cultures from Turkey to Arabia, from Persia to Afghanistan, and from Russia to China, most of which suffered from those invasions.

The Arabic version of the character, known as ""juha"" (), is the oldest attested version of the character and the most divergent, being mentioned in Al-Jahiz's book ""Saying on Mules""— —, according Al-Dhahabi's book Al-Dhahabi's book "", his full name was ""Abu al-Ghusn Dujayn al-Fizari"", he lived under the Umayyads in Kufa, his mother was said to be a servant to Anas ibn Malik, thus he was one of the Tabi'un in Sunni tradition.

As generations have gone by, new stories have been added to the Nasreddin corpus, others have been modified, and he and his tales have spread to many regions. The themes in the tales have become part of the folklore of a number of nations and express the national imaginations of a variety of cultures. Although most of them depict Nasreddin in an early small-village setting, the tales deal with concepts that have a certain timelessness. They purvey a pithy folk wisdom that triumphs over all trials and tribulations. The oldest manuscript of Nasreddin dates to 1571.

Today, Nasreddin stories are told in a wide variety of regions, especially across the Muslim world and have been translated into many languages. Some regions independently developed a character similar to Nasreddin, and the stories have become part of a larger whole. In many regions, Nasreddin is a major part of the culture, and is quoted or alluded to frequently in daily life. Since there are thousands of different Nasreddin stories, one can be found to fit almost any occasion. Nasreddin often appears as a whimsical character of a large Turkish, Persian, Albanian, Armenian, Azerbaijani, Bengali, Bosnian, Bulgarian, Chinese, Greek, Gujarati, Hindi, Judeo-Spanish, Kurdish, Romanian, Serbian, Russian, and Urdu folk tradition of vignettes, not entirely different from zen koans.

1996–1997 was declared International Nasreddin Year by UNESCO.

Many peoples of the Near, Middle East, South Asia and Central Asia claim Nasreddin as their own ("e.g.", Turks, Afghans, Iranians, and Uzbeks). His name is spelt in a wide variety of ways: "Nasrudeen", "Nasrudin", "Nasruddin", "Nasr ud-Din", "Nasredin", "Nasiruddin," "Naseeruddin", "Nasr Eddin", "Nastradhin", "Nasreddine", "Nastratin", "Nusrettin", "Nasrettin", "Nostradin", "Nastradin" (lit.: Victory of the Deen) and "Nazaruddin". It is sometimes preceded or followed by a title or honorific used in the corresponding cultures: "Hoxha", "Khwaje", "Hodja", "Hoja", "Hojja", "Hodscha", "Hodža", "Hoca", "Hocca","Hooka",
"Hogea", "Mullah", "Mulla", "Mula", "Molla", "Efendi", "Afandi", "Ependi" ( "'afandī"), "Hajji". In several cultures he is named by the title alone.

In Arabic-speaking countries this character is known as "Juha", "Djoha", "Djuha", "Dschuha", "Chotzas", "Goha" ( "juḥā"). Juha was originally a separate folk character found in Arabic literature as early as the 9th century, and was widely popular by the 11th century. Lore of the two characters became amalgamated in the 19th century when collections were translated from Arabic into Turkish and Persian.

In Sicily and Southern Italy he is known as "Giufà", derived from the Arabic character Juha.

In the Swahili and Indonesian culture, many of his stories are being told under the name of "Abunuwasi" or "Abunawas", though this confuses Nasreddin with an entirely different man – the poet Abu Nuwas, known for homoerotic verse.

In China, where stories of him are well known, he is known by the various transliterations from his Uyghur name, 阿凡提 (Āfántí) and 阿方提 (Āfāngtí). The Uyghurs believe that he was from Xinjiang, while the Uzbeks believe he was from Bukhara. Shanghai Animation Film Studio produced a 13-episode Nasreddin related animation called 'The Story of Afanti'/ 阿凡提 in 1979, which became one of the most influential animations in China's history. The musical Nasirdin Apandim features the legend of Nasreddin effendi ("sir, lord"), largely sourced from Uyghur folklore.

In Central Asia, he is commonly known as "Afandi". The Central Asian peoples also claim his local origin, as do Uyghurs.

The Nasreddin stories are known throughout the Middle East and have touched cultures around the world. Superficially, most of the Nasreddin stories may be told as jokes or humorous anecdotes. They are told and retold endlessly in the teahouses and caravanserais of Asia and can be heard in homes and on the radio. But it is inherent in a Nasreddin story that it may be understood at many levels. There is the joke, followed by a moral and usually the little extra which brings the consciousness of the potential mystic a little further on the way to realization.

Nasreddin was the main character in a magazine, called simply "Molla Nasraddin", published in Azerbaijan and "read across the Muslim world from Morocco to Iran". The eight-page Azerbaijani satirical periodical was published in Tiflis (from 1906 to 1917), Tabriz (in 1921) and Baku (from 1922 to 1931) in the Azeri and occasionally Russian languages. Founded by Jalil Mammadguluzadeh, it depicted inequality, cultural assimilation, and corruption and ridiculed the backward lifestyles and values of clergy and religious fanatics. The magazine was frequently banned but had a lasting influence on Azerbaijani and Iranian literature.

Some Nasreddin tales also appear in collections of Aesop's fables. "The miller, his son and the donkey" is one example. Others are "The Ass with a Burden of Salt" (Perry Index 180) and "The Satyr and the Traveller."

In some Bulgarian folk tales that originated during the Ottoman period, the name appears as an antagonist to a local wise man, named "Sly Peter". In Sicily the same tales involve a man named "Giufà". In Sephardic culture, spread throughout the Ottoman Empire, a character that appears in many folk tales is named "Djohá".

In Romanian, the existing stories come from an 1853 verse compilation edited by Anton Pann, a philologist and poet renowned for authoring the current Romanian anthem.

Nasreddin is mostly known as a character from short tales; whole novels and stories have later been written and production began on a never-completed animated feature film. In Russia, Nasreddin is known mostly because of the Russian work "Возмутитель спокойствия" by Leonid Solovyov (English translations: "The Beggar in the Harem: Impudent Adventures in Old Bukhara", 1956, and "The Tale of Hodja Nasreddin: Disturber of the Peace", 2009). The composer Shostakovich celebrated Nasreddin, among other figures, in the second movement ("Yumor", "Humor") of his Symphony No. 13. The text, by Yevgeny Yevtushenko, portrays humor as a weapon against dictatorship and tyranny. Shostakovich's music shares many of the "foolish yet profound" qualities of Nasreddin's sayings listed above.

The Graeco-Armenian mystic G. I. Gurdjieff often referred to "our own dear Mullah Nasr Eddin", also
calling him an "incomparable teacher", particularly in his book "Beelzebub's Tales". Sufi philosopher Idries Shah published several collections of Nasruddin stories in English, and emphasized their teaching value.

He is known as "Mullah Nasruddin" in South Asian children's books. A TV serial on him was aired in India as "Mulla Nasiruddin" and was widely watched in India and Pakistan.

For Uzbek people, Nasreddin is one of their own; he is said to have lived and been born in Bukhara. In gatherings, family meetings, and parties they tell each other stories about him that are called "latifa" of "afandi".
There are at least two collections of stories related to Nasriddin Afandi.

Books on him:


In 1943, the Soviet film "Nasreddin in Bukhara" was directed by Yakov Protazanov based on Solovyov's book, followed in 1947 by a film called "The Adventures of Nasreddin", directed by Nabi Ganiyev and also set in the Uzbekistan SSR.



</doc>
<doc id="21272" url="https://en.wikipedia.org/wiki?curid=21272" title="Neutron">
Neutron

The neutron is a subatomic particle, symbol or , with no electric charge and a mass slightly greater than that of a proton. Protons and neutrons constitute the nuclei of atoms. Since protons and neutrons behave similarly within the nucleus, and each has a mass of approximately one atomic mass unit, they are both referred to as nucleons. Their properties and interactions are described by nuclear physics.
The chemical properties of an atom are mostly determined by the configuration of electrons that orbit the atom's heavy nucleus. The electron configuration is determined by the charge of the nucleus, set by the number of protons, or atomic number. Neutrons do not affect the electron configuration, but the sum of atomic number and the number of neutrons, or neutron number, is the mass of the nucleus.

Atoms of a chemical element that differ only in neutron number are called isotopes. For example, carbon, with atomic number 6, has an abundant isotope carbon-12 with 6 neutrons and a rare isotope carbon-13 with 7 neutrons. Some elements occur in nature with only one stable isotope, such as fluorine. Other elements occur with many stable isotopes, such as tin with ten stable isotopes. 

The properties of an atomic nucleus are dependent on both atomic and neutron numbers. With their positive charge, the protons within the nucleus are repelled by the long-range electromagnetic force, but the much stronger, but short-range, nuclear force binds the nucleons closely together. Neutrons are required for the stability of nuclei, with the exception of the single-proton hydrogen nucleus. Neutrons are produced copiously in nuclear fission and fusion. They are a primary contributor to the nucleosynthesis of chemical elements within stars through fission, fusion, and neutron capture processes.

The neutron is essential to the production of nuclear power. In the decade after the neutron was discovered by James Chadwick in 1932, neutrons were used to induce many different types of nuclear transmutations. With the discovery of nuclear fission in 1938, it was quickly realized that, if a fission event produced neutrons, each of these neutrons might cause further fission events, in a cascade known as a nuclear chain reaction. These events and findings led to the first self-sustaining nuclear reactor (Chicago Pile-1, 1942) and the first nuclear weapon (Trinity, 1945).

Free neutrons, while not directly ionizing atoms, cause ionizing radiation. As such they can be a biological hazard, depending upon dose. A small natural "neutron background" flux of free neutrons exists on Earth, caused by cosmic ray showers, and by the natural radioactivity of spontaneously fissionable elements in the Earth's crust. Dedicated neutron sources like neutron generators, research reactors and spallation sources produce free neutrons for use in irradiation and in neutron scattering experiments.

An atomic nucleus is formed by a number of protons, "Z" (the atomic number), and a number of neutrons, "N" (the neutron number), bound together by the nuclear force. The atomic number determines the chemical properties of the atom, and the neutron number determines the isotope or nuclide. The terms isotope and nuclide are often used synonymously, but they refer to chemical and nuclear properties, respectively. Isotopes are nuclides with the same atomic number, but different neutron number. Nuclides with the same neutron number, but different atomic number, are called isotones. The atomic mass number, "A", is equal to the sum of atomic and neutron numbers. Nuclides with the same atomic mass number, but different atomic and neutron numbers, are called isobars. 

The nucleus of the most common isotope of the hydrogen atom (with the chemical symbol H) is a lone proton. The nuclei of the heavy hydrogen isotopes deuterium (D or H) and tritium (T or H) contain one proton bound to one and two neutrons, respectively. All other types of atomic nuclei are composed of two or more protons and various numbers of neutrons. The most common nuclide of the common chemical element lead, Pb, has 82 protons and 126 neutrons, for example. The table of nuclides comprises all the known nuclides. Even though it is not a chemical element, the neutron is included in this table.

The free neutron has a mass of 939,565,413.3 eV/c, or , or . The neutron has a mean square radius of about , or 0.8 fm, and it is a spin-½ fermion.
The neutron has no measurable electric charge. With its positive electric charge, the proton is directly influenced by electric fields, whereas the neutron is unaffected by electric fields. The neutron has a magnetic moment, however, so the neutron is influenced by magnetic fields. The neutron's magnetic moment has a negative value, because its orientation is opposite to the neutron's spin.

A free neutron is unstable, decaying to a proton, electron and antineutrino with a mean lifetime of just under 15 minutes (). This radioactive decay, known as beta decay, is possible because the mass of the neutron is slightly greater than the proton. The free proton is stable. Neutrons or protons bound in a nucleus can be stable or unstable, however, depending on the nuclide. Beta decay, in which neutrons decay to protons, or vice versa, is governed by the weak force, and it requires the emission or absorption of electrons and neutrinos, or their antiparticles.
Protons and neutrons behave almost identically under the influence of the nuclear force within the nucleus. The concept of isospin, in which the proton and neutron are viewed as two quantum states of the same particle, is used to model the interactions of nucleons by the nuclear or weak forces. Because of the strength of the nuclear force at short distances, the binding energy of nucleons is more than seven orders of magnitude larger than the electromagnetic energy binding electrons in atoms. Nuclear reactions (such as nuclear fission) therefore have an energy density that is more than ten million times that of chemical reactions. Because of the mass–energy equivalence, nuclear binding energies reduce the mass of nuclei. Ultimately, the ability of the nuclear force to store energy arising from the electromagnetic repulsion of nuclear components is the basis for most of the energy that makes nuclear reactors or bombs possible. In nuclear fission, the absorption of a neutron by a heavy nuclide (e.g., uranium-235) causes the nuclide to become unstable and break into light nuclides and additional neutrons. The positively charged light nuclides then repel, releasing electromagnetic potential energy.

The neutron is classified as a "hadron", because it is a composite particle made of quarks. The neutron is also classified as a "baryon", because it is composed of three valence quarks. The finite size of the neutron and its magnetic moment both indicate that the neutron is a composite, rather than elementary, particle. A neutron contains two down quarks with charge − "e" and one up quark with charge + "e".

Like protons, the quarks of the neutron are held together by the strong force, mediated by gluons. The nuclear force results from secondary effects of the more fundamental strong force.

The story of the discovery of the neutron and its properties is central to the extraordinary developments in atomic physics that occurred in the first half of the 20th century, leading ultimately to the atomic bomb in 1945. In the 1911 Rutherford model, the atom consisted of a small positively charged massive nucleus surrounded by a much larger cloud of negatively charged electrons. In 1920, Rutherford suggested that the nucleus consisted of positive protons and neutrally-charged particles, suggested to be a proton and an electron bound in some way. Electrons were assumed to reside within the nucleus because it was known that beta radiation consisted of electrons emitted from the nucleus. Rutherford called these uncharged particles "neutrons", by the Latin root for "neutralis" (neuter) and the Greek suffix "-on" (a suffix used in the names of subatomic particles, i.e. "electron" and "proton"). References to the word "neutron" in connection with the atom can be found in the literature as early as 1899, however.

Throughout the 1920s, physicists assumed that the atomic nucleus was composed of protons and "nuclear electrons" but there were obvious problems. It was difficult to reconcile the proton–electron model for nuclei with the Heisenberg uncertainty relation of quantum mechanics. The Klein paradox, discovered by Oskar Klein in 1928, presented further quantum mechanical objections to the notion of an electron confined within a nucleus. Observed properties of atoms and molecules were inconsistent with the nuclear spin expected from the proton–electron hypothesis. Both protons and electrons carry an intrinsic spin of ½ "ħ". Isotopes of the same species (i.e. having the same number of protons) can have both integer or fractional spin, i.e. the neutron spin must be also fractional (½ "ħ"). However, there is no way to arrange the spins of an electron and a proton (supposed to bond to form a neutron) to get the fractional spin of a neutron.

In 1931, Walther Bothe and Herbert Becker found that if alpha particle radiation from polonium fell on beryllium, boron, or lithium, an unusually penetrating radiation was produced. The radiation was not influenced by an electric field, so Bothe and Becker assumed it was gamma radiation. The following year Irène Joliot-Curie and Frédéric Joliot-Curie in Paris showed that if this "gamma" radiation fell on paraffin, or any other hydrogen-containing compound, it ejected protons of very high energy. Neither Rutherford nor James Chadwick at the Cavendish Laboratory in Cambridge were convinced by the gamma ray interpretation. Chadwick quickly performed a series of experiments that showed that the new radiation consisted of uncharged particles with about the same mass as the proton. These particles were neutrons. Chadwick won the 1935 Nobel Prize in Physics for this discovery.

Models for atomic nucleus consisting of protons and neutrons were quickly developed by Werner Heisenberg and others. The proton–neutron model explained the puzzle of nuclear spins. The origins of beta radiation were explained by Enrico Fermi in 1934 by the process of beta decay, in which the neutron decays to a proton by "creating" an electron and a (as yet undiscovered) neutrino. In 1935, Chadwick and his doctoral student Maurice Goldhaber reported the first accurate measurement of the mass of the neutron.

By 1934, Fermi had bombarded heavier elements with neutrons to induce radioactivity in elements of high atomic number. In 1938, Fermi received the Nobel Prize in Physics ""for his demonstrations of the existence of new radioactive elements produced by neutron irradiation, and for his related discovery of nuclear reactions brought about by slow neutrons"". In 1938 Otto Hahn, Lise Meitner, and Fritz Strassmann discovered nuclear fission, or the fractionation of uranium nuclei into light elements, induced by neutron bombardment. In 1945 Hahn received the 1944 Nobel Prize in Chemistry ""for his discovery of the fission of heavy atomic nuclei."" The discovery of nuclear fission would lead to the development of nuclear power and the atomic bomb by the end of World War II.

Since interacting protons have a mutual electromagnetic repulsion that is stronger than their attractive nuclear interaction, neutrons are a necessary constituent of any atomic nucleus that contains more than one proton (see diproton and neutron–proton ratio). Neutrons bind with protons and one another in the nucleus via the nuclear force, effectively moderating the repulsive forces between the protons and stabilizing the nucleus.

The neutrons and protons bound in a nucleus form a quantum mechanical system wherein each nucleon is bound in a particular, hierarchical quantum state. Protons can decay to neutrons, or vice-versa, within the nucleus. This process, called beta decay, requires the emission of an electron or positron and an associated neutrino. These emitted particles carry away the energy excess as a nucleon falls from one quantum state to a lower energy state, while the proton (or neutron) changes to a neutron (or proton). Such decay processes can occur only if allowed by basic energy conservation and quantum mechanical constraints. The stability of nuclei depends on these constraints.

Outside the nucleus, free neutrons are unstable and have a mean lifetime of (about 14 minutes, 40 seconds); therefore the half-life for this process (which differs from the mean lifetime by a factor of ) is (about 10 minutes, 10 seconds). This decay is only possible because the mass of the proton is less than that of the neutron. By the mass-energy equivalence, when a neutron decays to a proton this way it attains a lower energy state. Beta decay of the neutron, described above, can be denoted by the radioactive decay:

where , , and denote the proton, electron and electron antineutrino, respectively.
For the free neutron the decay energy for this process (based on the masses of the neutron, proton, and electron) is 0.782343 MeV. The maximal energy of the beta decay electron (in the process wherein the neutrino receives a vanishingly small amount of kinetic energy) has been measured at 0.782 ± 0.013 MeV. The latter number is not well-enough measured to determine the comparatively tiny rest mass of the neutrino (which must in theory be subtracted from the maximal electron kinetic energy) as well as neutrino mass is constrained by many other methods.

A small fraction (about one in 1000) of free neutrons decay with the same products, but add an extra particle in the form of an emitted gamma ray:

This gamma ray may be thought of as an "internal bremsstrahlung" that arises from the electromagnetic interaction of the emitted beta particle with the proton. Internal bremsstrahlung gamma ray production is also a minor feature of beta decays of bound neutrons (as discussed below).

A very small minority of neutron decays (about four per million) are so-called "two-body (neutron) decays", in which a proton, electron and antineutrino are produced as usual, but the electron fails to gain the 13.6 eV necessary energy to escape the proton (the ionization energy of hydrogen), and therefore simply remains bound to it, as a neutral hydrogen atom (one of the "two bodies"). In this type of free neutron decay, almost all of the neutron decay energy is carried off by the antineutrino (the other "body"). (The hydrogen atom recoils with a speed of only about (decay energy)/(hydrogen rest energy) times the speed of light, or 250 km/s.)

The transformation of a free proton to a neutron (plus a positron and a neutrino) is energetically impossible, since a free neutron has a greater mass than a free proton. But a high-energy collision of a proton and an electron or neutrino can result in a neutron.

While a free neutron has a half life of about 10.2 min, most neutrons within nuclei are stable. According to the nuclear shell model, the protons and neutrons of a nuclide are a quantum mechanical system organized into discrete energy levels with unique quantum numbers. For a neutron to decay, the resulting proton requires an available state at lower energy than the initial neutron state. In stable nuclei the possible lower energy states are all filled, meaning they are each occupied by two protons with spin up and spin down. The Pauli exclusion principle therefore disallows the decay of a neutron to a proton within stable nuclei. The situation is similar to electrons of an atom, where electrons have distinct atomic orbitals and are prevented from decaying to lower energy states, with the emission of a photon, by the exclusion principle.

Neutrons in unstable nuclei can decay by beta decay as described above. In this case, an energetically allowed quantum state is available for the proton resulting from the decay. One example of this decay is carbon-14 (6 protons, 8 neutrons) that decays to nitrogen-14 (7 protons, 7 neutrons) with a half-life of about 5,730 years.

Inside a nucleus, a proton can transform into a neutron via inverse beta decay, if an energetically allowed quantum state is available for the neutron. This transformation occurs by emission of a positron and an electron neutrino:

The transformation of a proton to a neutron inside of a nucleus is also possible through electron capture:
Positron capture by neutrons in nuclei that contain an excess of neutrons is also possible, but is hindered because positrons are repelled by the positive nucleus, and quickly annihilate when they encounter electrons.

Three types of beta decay in competition are illustrated by the single isotope copper-64 (29 protons, 35 neutrons), which has a half-life of about 12.7 hours. This isotope has one unpaired proton and one unpaired neutron, so either the proton or the neutron can decay. This particular nuclide is almost equally likely to undergo proton decay (by positron emission, 18% or by electron capture, 43%) or neutron decay (by electron emission, 39%).

Within the theoretical framework of Standard Model for particle physics, the neutron is composed of two down quarks and an up quark. The only possible decay mode for the neutron that conserves baryon number is for one of the neutron's quarks to change flavour via the weak interaction. The decay of one of the neutron's down quarks into a lighter up quark can be achieved by the emission of a W boson. By this process, the Standard Model description of beta decay, the neutron decays into a proton (which contains one down and two up quarks), an electron, and an electron antineutrino.

The decay of the proton to a neutron occurs similarly through the electroweak force. The decay of one of the proton's up quarks into a down quark can be achieved by the emission of a W boson. The proton decays into a neutron, a positron, and an electron neutrino. This reaction can only occur within an atomic nucleus which has a quantum state at lower energy available for the created neutron.

The mass of a neutron cannot be directly determined by mass spectrometry due to lack of electric charge. However, since the masses of a proton and of a deuteron can be measured with a mass spectrometer, the mass of a neutron can be deduced by subtracting proton mass from deuteron mass, with the difference being the mass of the neutron plus the binding energy of deuterium (expressed as a positive emitted energy). The latter can be directly measured by measuring the energy (formula_1) of the single gamma photon emitted when neutrons are captured by protons (this is exothermic and happens with zero-energy neutrons), plus the small recoil kinetic energy (formula_2) of the deuteron (about 0.06% of the total energy).

The energy of the gamma ray can be measured to high precision by X-ray diffraction techniques, as was first done by Bell and Elliot in 1948. The best modern (1986) values for neutron mass by this technique are provided by Greene, et al. These give a neutron mass of:

The value for the neutron mass in MeV is less accurately known, due to less accuracy in the known conversion of u to MeV:

Another method to determine the mass of a neutron starts from the beta decay of the neutron, when the momenta of the resulting proton and electron are measured.

The total electric charge of the neutron is . This zero value has been tested experimentally, and the present experimental limit for the charge of the neutron is , or . This value is consistent with zero, given the experimental uncertainties (indicated in parentheses). By comparison, the charge of the proton is .

Even though the neutron is a neutral particle, the magnetic moment of a neutron is not zero. The neutron is not affected by electric fields, but it is affected by magnetic fields. The magnetic moment of the neutron is an indication of its quark substructure and internal charge distribution.
The value for the neutron's magnetic moment was first directly measured by Luis Alvarez and Felix Bloch at Berkeley, California, in 1940. Alvarez and Bloch determined the magnetic moment of the neutron to be , where "μ" is the nuclear magneton.

In the quark model for hadrons, the neutron is composed of one up quark (charge +2/3 "e") and two down quarks (charge −1/3 "e"). The magnetic moment of the neutron can be modeled as a sum of the magnetic moments of the constituent quarks. The calculation assumes that the quarks behave like pointlike Dirac particles, each having their own magnetic moment. Simplistically, the magnetic moment of the neutron can be viewed as resulting from the vector sum of the three quark magnetic moments, plus the orbital magnetic moments caused by the movement of the three charged quarks within the neutron.

In one of the early successes of the Standard Model (SU(6) theory, now understood in terms of quark behavior), in 1964 Mirza A.B. Beg, Benjamin W. Lee, and Abraham Pais theoretically calculated the ratio of proton to neutron magnetic moments to be −3/2, which agrees with the experimental value to within 3%. The measured value for this ratio is . A contradiction of the quantum mechanical basis of this calculation with the Pauli exclusion principle, led to the discovery of the color charge for quarks by Oscar W. Greenberg in 1964.

The above treatment compares neutrons with protons, allowing the complex behavior of quarks to be subtracted out between models, and merely exploring what the effects would be of differing quark charges (or quark type). Such calculations are enough to show that the interior of neutrons is very much like that of protons, save for the difference in quark composition with a down quark in the neutron replacing an up quark in the proton.

The neutron magnetic moment can be roughly computed by assuming a simple nonrelativistic, quantum mechanical wavefunction for baryons composed of three quarks. A straightforward calculation gives fairly accurate estimates for the magnetic moments of neutrons, protons, and other baryons. For a neutron, the end result of this calculation is that the magnetic moment of the neutron is given by , where "μ" and "μ" are the magnetic moments for the down and up quarks, respectively. This result combines the intrinsic magnetic moments of the quarks with their orbital magnetic moments, and assumes the three quarks are in a particular, dominant quantum state.

The results of this calculation are encouraging, but the masses of the up or down quarks were assumed to be 1/3 the mass of a nucleon. The masses of the quarks are actually only about 1% that of a nucleon. The discrepancy stems from the complexity of the Standard Model for nucleons, where most of their mass originates in the gluon fields, virtual particles, and their associated energy that are essential aspects of the strong force. Furthermore, the complex system of quarks and gluons that constitute a neutron requires a relativistic treatment. The nucleon magnetic moment has been successfully computed numerically from first principles, however, including all the effects mentioned and using more realistic values for the quark masses. The calculation gave results that were in fair agreement with measurement, but it required significant computing resources.

The neutron is a spin 1/2 particle, that is, it is a fermion with intrinsic angular momentum equal to 1/2 , where is the reduced Planck constant. For many years after the discovery of the neutron, its exact spin was ambiguous. Although it was assumed to be a spin 1/2 Dirac particle, the possibility that the neutron was a spin 3/2 particle lingered. The interactions of the neutron's magnetic moment with an external magnetic field were exploited to finally determine the spin of the neutron. In 1949, Hughes and Burgy measured neutrons reflected from a ferromagnetic mirror and found that the angular distribution of the reflections was consistent with spin 1/2. In 1954, Sherwood, Stephenson, and Bernstein employed neutrons in a Stern–Gerlach experiment that used a magnetic field to separate the neutron spin states. They recorded two such spin states, consistent with a spin 1/2 particle.

As a fermion, the neutron is subject to the Pauli exclusion principle; two neutrons cannot have the same quantum numbers. This is the source of the degeneracy pressure which makes neutron stars possible.
An article published in 2007 featuring a model-independent analysis concluded that the neutron has a negatively charged exterior, a positively charged middle, and a negative core. In a simplified classical view, the negative "skin" of the neutron assists it to be attracted to the protons with which it interacts in the nucleus. (However, the main attraction between neutrons and protons is via the nuclear force, which does not involve electric charge.)

The simplified classical view of the neutron's charge distribution also "explains" the fact that the neutron magnetic dipole points in the opposite direction from its spin angular momentum vector (as compared to the proton). This gives the neutron, in effect, a magnetic moment which resembles a negatively charged particle. This can be reconciled classically with a neutral neutron composed of a charge distribution in which the negative sub-parts of the neutron have a larger average radius of distribution, and therefore contribute more to the particle's magnetic dipole moment, than do the positive parts that are, on average, nearer the core.

The Standard Model of particle physics predicts a tiny separation of positive and negative charge within the neutron leading to a permanent electric dipole moment. The predicted value is, however, well below the current sensitivity of experiments. From several unsolved puzzles in particle physics, it is clear that the Standard Model is not the final and full description of all particles and their interactions. New theories going beyond the Standard Model generally lead to much larger predictions for the electric dipole moment of the neutron. Currently, there are at least four experiments trying to measure for the first time a finite neutron electric dipole moment, including:

The antineutron is the antiparticle of the neutron. It was discovered by Bruce Cork in 1956, a year after the antiproton was discovered. CPT-symmetry puts strong constraints on the relative properties of particles and antiparticles, so studying antineutrons provides stringent tests on CPT-symmetry. The fractional difference in the masses of the neutron and antineutron is . Since the difference is only about two standard deviations away from zero, this does not give any convincing evidence of CPT-violation.

The existence of stable clusters of 4 neutrons, or tetraneutrons, has been hypothesised by a team led by Francisco-Miguel Marqués at the CNRS Laboratory for Nuclear Physics based on observations of the disintegration of beryllium-14 nuclei. This is particularly interesting because current theory suggests that these clusters should not be stable.

In February 2016, Japanese physicist Susumu Shimoura of the University of Tokyo and co-workers reported they had observed the purported tetraneutrons for the first time experimentally. Nuclear physicists around the world say this discovery, if confirmed, would be a milestone in the field of nuclear physics and certainly would deepen our understanding of the nuclear forces.

The dineutron is another hypothetical particle. In 2012, Artemis Spyrou from Michigan State University and coworkers reported that they observed, for the first time, the dineutron emission in the decay of Be. The dineutron character is evidenced by a small emission angle between the two neutrons. The authors measured the two-neutron separation energy to be 1.35(10) MeV, in good agreement with shell model calculations, using standard interactions for this mass region.

At extremely high pressures and temperatures, nucleons and electrons are believed to collapse into bulk neutronic matter, called neutronium. This is presumed to happen in neutron stars.

The extreme pressure inside a neutron star may deform the neutrons into a cubic symmetry, allowing tighter packing of neutrons.

The common means of detecting a charged particle by looking for a track of ionization (such as in a cloud chamber) does not work for neutrons directly. Neutrons that elastically scatter off atoms can create an ionization track that is detectable, but the experiments are not as simple to carry out; other means for detecting neutrons, consisting of allowing them to interact with atomic nuclei, are more commonly used. The commonly used methods to detect neutrons can therefore be categorized according to the nuclear processes relied upon, mainly neutron capture or elastic scattering.

A common method for detecting neutrons involves converting the energy released from neutron capture reactions into electrical signals. Certain nuclides have a high neutron capture cross section, which is the probability of absorbing a neutron. Upon neutron capture, the compound nucleus emits more easily detectable radiation, for example an alpha particle, which is then detected. The nuclides , , , , , , and are useful for this purpose. 

Neutrons can elastically scatter off nuclei, causing the struck nucleus to recoil. Kinematically, a neutron can transfer more energy to a light nucleus such as hydrogen or helium than to a heavier nucleus. Detectors relying on elastic scattering are called fast neutron detectors. Recoiling nuclei can ionize and excite further atoms through collisions. Charge and/or scintillation light produced in this way can be collected to produce a detected signal. A major challenge in fast neutron detection is discerning such signals from erroneous signals produced by gamma radiation in the same detector. Methods such as pulse shape discrimination can be used in distinguishing neutron signals from gamma-ray signals, although certain inorganic scintillator-based detectors have been developed to selectively detect neutrons in mixed radiation fields inherently without any additional techniques.

Fast neutron detectors have the advantage of not requiring a moderator, and are therefore capable of measuring the neutron's energy, time of arrival, and in certain cases direction of incidence.

Free neutrons are unstable, although they have the longest half-life of any unstable subatomic particle by several orders of magnitude. Their half-life is still only about 10 minutes, however, so they can be obtained only from sources that produce them continuously.

Natural neutron background. A small natural background flux of free neutrons exists everywhere on Earth. In the atmosphere and deep into the ocean, the "neutron background" is caused by muons produced by cosmic ray interaction with the atmosphere. These high-energy muons are capable of penetration to considerable depths in water and soil. There, in striking atomic nuclei, among other reactions they induce spallation reactions in which a neutron is liberated from the nucleus. Within the Earth's crust a second source is neutrons produced primarily by spontaneous fission of uranium and thorium present in crustal minerals. The neutron background is not strong enough to be a biological hazard, but it is of importance to very high resolution particle detectors that are looking for very rare events, such as (hypothesized) interactions that might be caused by particles of dark matter. Recent research has shown that even thunderstorms can produce neutrons with energies of up to several tens of MeV. Recent research has shown that the fluence of these neutrons lies between 10 and 10 per ms and per m depending on the detection altitude. The energy of most of these neutrons, even with initial energies of 20 MeV, decreases down to the keV range within 1 ms.

Even stronger neutron background radiation is produced at the surface of Mars, where the atmosphere is thick enough to generate neutrons from cosmic ray muon production and neutron-spallation, but not thick enough to provide significant protection from the neutrons produced. These neutrons not only produce a Martian surface neutron radiation hazard from direct downward-going neutron radiation but may also produce a significant hazard from reflection of neutrons from the Martian surface, which will produce reflected neutron radiation penetrating upward into a Martian craft or habitat from the floor.

Sources of neutrons for research. These include certain types of radioactive decay (spontaneous fission and neutron emission), and from certain nuclear reactions. Convenient nuclear reactions include tabletop reactions such as natural alpha and gamma bombardment of certain nuclides, often beryllium or deuterium, and induced nuclear fission, such as occurs in nuclear reactors. In addition, high-energy nuclear reactions (such as occur in cosmic radiation showers or accelerator collisions) also produce neutrons from disintegration of target nuclei. Small (tabletop) particle accelerators optimized to produce free neutrons in this way, are called neutron generators.

In practice, the most commonly used small laboratory sources of neutrons use radioactive decay to power neutron production. One noted neutron-producing radioisotope, californium-252 decays (half-life 2.65 years) by spontaneous fission 3% of the time with production of 3.7 neutrons per fission, and is used alone as a neutron source from this process. Nuclear reaction sources (that involve two materials) powered by radioisotopes use an alpha decay source plus a beryllium target, or else a source of high-energy gamma radiation from a source that undergoes beta decay followed by gamma decay, which produces photoneutrons on interaction of the high-energy gamma ray with ordinary stable beryllium, or else with the deuterium in heavy water. A popular source of the latter type is radioactive antimony-124 plus beryllium, a system with a half-life of 60.9 days, which can be constructed from natural antimony (which is 42.8% stable antimony-123) by activating it with neutrons in a nuclear reactor, then transported to where the neutron source is needed.

Nuclear fission reactors naturally produce free neutrons; their role is to sustain the energy-producing chain reaction. The intense neutron radiation can also be used to produce various radioisotopes through the process of neutron activation, which is a type of neutron capture.

Experimental nuclear fusion reactors produce free neutrons as a waste product. However, it is these neutrons that possess most of the energy, and converting that energy to a useful form has proved a difficult engineering challenge. Fusion reactors that generate neutrons are likely to create radioactive waste, but the waste is composed of neutron-activated lighter isotopes, which have relatively short (50–100 years) decay periods as compared to typical half-lives of 10,000 years for fission waste, which is long due primarily to the long half-life of alpha-emitting transuranic actinides.

Free neutron beams are obtained from neutron sources by neutron transport. For access to intense neutron sources, researchers must go to a specialized neutron facility that operates a research reactor or a spallation source.

The neutron's lack of total electric charge makes it difficult to steer or accelerate them. Charged particles can be accelerated, decelerated, or deflected by electric or magnetic fields. These methods have little effect on neutrons. However, some effects may be attained by use of inhomogeneous magnetic fields because of the neutron's magnetic moment. Neutrons can be controlled by methods that include moderation, reflection, and velocity selection. Thermal neutrons can be polarized by transmission through magnetic materials in a method analogous to the Faraday effect for photons. Cold neutrons of wavelengths of 6–7 angstroms can be produced in beams of a high degree of polarization, by use of magnetic mirrors and magnetized interference filters.

The neutron plays an important role in many nuclear reactions. For example, neutron capture often results in neutron activation, inducing radioactivity. In particular, knowledge of neutrons and their behavior has been important in the development of nuclear reactors and nuclear weapons. The fissioning of elements like uranium-235 and plutonium-239 is caused by their absorption of neutrons.

"Cold", "thermal", and "hot" neutron radiation is commonly employed in neutron scattering facilities, where the radiation is used in a similar way one uses X-rays for the analysis of condensed matter. Neutrons are complementary to the latter in terms of atomic contrasts by different scattering cross sections; sensitivity to magnetism; energy range for inelastic neutron spectroscopy; and deep penetration into matter.

The development of "neutron lenses" based on total internal reflection within hollow glass capillary tubes or by reflection from dimpled aluminum plates has driven ongoing research into neutron microscopy and neutron/gamma ray tomography.

A major use of neutrons is to excite delayed and prompt gamma rays from elements in materials. This forms the basis of neutron activation analysis (NAA) and prompt gamma neutron activation analysis (PGNAA). NAA is most often used to analyze small samples of materials in a nuclear reactor whilst PGNAA is most often used to analyze subterranean rocks around bore holes and industrial bulk materials on conveyor belts.

Another use of neutron emitters is the detection of light nuclei, in particular the hydrogen found in water molecules. When a fast neutron collides with a light nucleus, it loses a large fraction of its energy. By measuring the rate at which slow neutrons return to the probe after reflecting off of hydrogen nuclei, a neutron probe may determine the water content in soil.

Because neutron radiation is both penetrating and ionizing, it can be exploited for medical treatments. Neutron radiation can have the unfortunate side-effect of leaving the affected area radioactive, however. Neutron tomography is therefore not a viable medical application.

Fast neutron therapy utilizes high-energy neutrons typically greater than 20 MeV to treat cancer. Radiation therapy of cancers is based upon the biological response of cells to ionizing radiation. If radiation is delivered in small sessions to damage cancerous areas, normal tissue will have time to repair itself, while tumor cells often cannot. Neutron radiation can deliver energy to a cancerous region at a rate an order of magnitude larger than gamma radiation.

Beams of low-energy neutrons are used in boron capture therapy to treat cancer. In boron capture therapy, the patient is given a drug that contains boron and that preferentially accumulates in the tumor to be targeted. The tumor is then bombarded with very low-energy neutrons (although often higher than thermal energy) which are captured by the boron-10 isotope in the boron, which produces an excited state of boron-11 that then decays to produce lithium-7 and an alpha particle that have sufficient energy to kill the malignant cell, but insufficient range to damage nearby cells. For such a therapy to be applied to the treatment of cancer, a neutron source having an intensity of the order of a thousand million (10) neutrons per second per cm is preferred. Such fluxes require a research nuclear reactor.

Exposure to free neutrons can be hazardous, since the interaction of neutrons with molecules in the body can cause disruption to molecules and atoms, and can also cause reactions that give rise to other forms of radiation (such as protons). The normal precautions of radiation protection apply: Avoid exposure, stay as far from the source as possible, and keep exposure time to a minimum. Some particular thought must be given to how to protect from neutron exposure, however. For other types of radiation, e.g., alpha particles, beta particles, or gamma rays, material of a high atomic number and with high density makes for good shielding; frequently, lead is used. However, this approach will not work with neutrons, since the absorption of neutrons does not increase straightforwardly with atomic number, as it does with alpha, beta, and gamma radiation. Instead one needs to look at the particular interactions neutrons have with matter (see the section on detection above). For example, hydrogen-rich materials are often used to shield against neutrons, since ordinary hydrogen both scatters and slows neutrons. This often means that simple concrete blocks or even paraffin-loaded plastic blocks afford better protection from neutrons than do far more dense materials. After slowing, neutrons may then be absorbed with an isotope that has high affinity for slow neutrons without causing secondary capture radiation, such as lithium-6.

Hydrogen-rich ordinary water affects neutron absorption in nuclear fission reactors: Usually, neutrons are so strongly absorbed by normal water that fuel enrichment with fissionable isotope is required. The deuterium in heavy water has a very much lower absorption affinity for neutrons than does protium (normal light hydrogen). Deuterium is, therefore, used in CANDU-type reactors, in order to slow (moderate) neutron velocity, to increase the probability of nuclear fission compared to neutron capture.

"Thermal neutrons" are free neutrons whose energies have a Maxwell–Boltzmann distribution with kT =  () at room temperature. This gives characteristic (not average, or median) speed of 2.2 km/s. The name 'thermal' comes from their energy being that of the room temperature gas or material they are permeating. (see "kinetic theory" for energies and speeds of molecules). After a number of collisions (often in the range of 10–20) with nuclei, neutrons arrive at this energy level, provided that they are not absorbed.

In many substances, thermal neutron reactions show a much larger effective cross-section than reactions involving faster neutrons, and thermal neutrons can therefore be absorbed more readily (i.e., with higher probability) by any atomic nuclei that they collide with, creating a heavier – and often unstable – isotope of the chemical element as a result.

Most fission reactors use a neutron moderator to slow down, or "thermalize" the neutrons that are emitted by nuclear fission so that they are more easily captured, causing further fission. Others, called fast breeder reactors, use fission energy neutrons directly.

"Cold neutrons" are thermal neutrons that have been equilibrated in a very cold substance such as liquid deuterium. Such a "cold source" is placed in the moderator of a research reactor or spallation source. Cold neutrons are particularly valuable for neutron scattering experiments.

Ultracold neutrons are produced by inelastic scattering of cold neutrons in substances with a low neutron absorption cross section at a temperature of a few kelvins, such as solid deuterium or superfluid helium. An alternative production method is the mechanical deceleration of cold neutrons exploiting the Doppler shift.

A "fast neutron" is a free neutron with a kinetic energy level close to (), hence a speed of ~ (~5% of the speed of light). They are named "fission energy" or "fast" neutrons to distinguish them from lower-energy thermal neutrons, and high-energy neutrons produced in cosmic showers or accelerators. Fast neutrons are produced by nuclear processes such as nuclear fission. Neutrons produced in fission, as noted above, have a Maxwell–Boltzmann distribution of kinetic energies from 0 to ~14 MeV, a mean energy of 2 MeV (for U fission neutrons), and a mode of only 0.75 MeV, which means that more than half of them do not qualify as fast (and thus have almost no chance of initiating fission in fertile materials, such as U and Th).

Fast neutrons can be made into thermal neutrons via a process called moderation. This is done with a neutron moderator. In reactors, typically heavy water, light water, or graphite are used to moderate neutrons.

D–T (deuterium–tritium) fusion is the fusion reaction that produces the most energetic neutrons, with 14.1 MeV of kinetic energy and traveling at 17% of the speed of light. D–T fusion is also the easiest fusion reaction to ignite, reaching near-peak rates even when the deuterium and tritium nuclei have only a thousandth as much kinetic energy as the 14.1 MeV that will be produced.

14.1 MeV neutrons have about 10 times as much energy as fission neutrons, and are very effective at fissioning even non-fissile heavy nuclei, and these high-energy fissions produce more neutrons on average than fissions by lower-energy neutrons. This makes D–T fusion neutron sources such as proposed tokamak power reactors useful for transmutation of transuranic waste. 14.1 MeV neutrons can also produce neutrons by knocking them loose from nuclei.

On the other hand, these very high-energy neutrons are less likely to simply be captured without causing fission or spallation. For these reasons, nuclear weapon design extensively utilizes D–T fusion 14.1 MeV neutrons to cause more fission. Fusion neutrons are able to cause fission in ordinarily non-fissile materials, such as depleted uranium (uranium-238), and these materials have been used in the jackets of thermonuclear weapons. Fusion neutrons also can cause fission in substances that are unsuitable or difficult to make into primary fission bombs, such as reactor grade plutonium. This physical fact thus causes ordinary non-weapons grade materials to become of concern in certain nuclear proliferation discussions and treaties.

Other fusion reactions produce much less energetic neutrons. D–D fusion produces a 2.45 MeV neutron and helium-3 half of the time, and produces tritium and a proton but no neutron the rest of the time. D–He fusion produces no neutron.

A fission energy neutron that has slowed down but not yet reached thermal energies is called an epithermal neutron.

Cross sections for both capture and fission reactions often have multiple resonance peaks at specific energies in the epithermal energy range.
These are of less significance in a fast neutron reactor, where most neutrons are absorbed before slowing down to this range, or in a well-moderated thermal reactor, where epithermal neutrons interact mostly with moderator nuclei, not with either fissile or fertile actinide nuclides.
However, in a partially moderated reactor with more interactions of epithermal neutrons with heavy metal nuclei, there are greater possibilities for transient changes in reactivity that might make reactor control more difficult.

Ratios of capture reactions to fission reactions are also worse (more captures without fission) in most nuclear fuels such as plutonium-239, making epithermal-spectrum reactors using these fuels less desirable, as captures not only waste the one neutron captured but also usually result in a nuclide that is not fissile with thermal or epithermal neutrons, though still fissionable with fast neutrons. The exception is uranium-233 of the thorium cycle, which has good capture-fission ratios at all neutron energies.

High-energy neutrons have much more energy than fission energy neutrons and are generated as secondary particles by particle accelerators or in the atmosphere from cosmic rays. These high-energy neutrons are extremely efficient at ionization and far more likely to cause cell death than X-rays or protons.






</doc>
<doc id="21273" url="https://en.wikipedia.org/wiki?curid=21273" title="Neon">
Neon

Neon is a chemical element with the symbol Ne and atomic number 10. It is a noble gas. Neon is a colorless, odorless, inert monatomic gas under standard conditions, with about two-thirds the density of air. It was discovered (along with krypton and xenon) in 1898 as one of the three residual rare inert elements remaining in dry air, after nitrogen, oxygen, argon and carbon dioxide were removed. Neon was the second of these three rare gases to be discovered and was immediately recognized as a new element from its bright red emission spectrum. The name neon is derived from the Greek word, , neuter singular form of ("neos"), meaning new. Neon is chemically inert, and no uncharged neon compounds are known. The compounds of neon currently known include ionic molecules, molecules held together by van der Waals forces and clathrates.

During cosmic nucleogenesis of the elements, large amounts of neon are built up from the alpha-capture fusion process in stars. Although neon is a very common element in the universe and solar system (it is fifth in cosmic abundance after hydrogen, helium, oxygen and carbon), it is rare on Earth. It composes about 18.2 ppm of air by volume (this is about the same as the molecular or mole fraction) and a smaller fraction in Earth's crust. The reason for neon's relative scarcity on Earth and the inner (terrestrial) planets is that neon is highly volatile and forms no compounds to fix it to solids. As a result, it escaped from the planetesimals under the warmth of the newly ignited Sun in the early Solar System. Even the outer atmosphere of Jupiter is somewhat depleted of neon, although for a different reason.

Neon gives a distinct reddish-orange glow when used in low-voltage neon glow lamps, high-voltage discharge tubes and neon advertising signs. The red emission line from neon also causes the well known red light of helium–neon lasers. Neon is used in some plasma tube and refrigerant applications but has few other commercial uses. It is commercially extracted by the fractional distillation of liquid air. Since air is the only source, it is considerably more expensive than helium.

Neon was discovered in 1898 by the British chemists Sir William Ramsay (1852–1916) and Morris W. Travers (1872–1961) in London. Neon was discovered when Ramsay chilled a sample of air until it became a liquid, then warmed the liquid and captured the gases as they boiled off. The gases nitrogen, oxygen, and argon had been identified, but the remaining gases were isolated in roughly their order of abundance, in a six-week period beginning at the end of May 1898. First to be identified was krypton. The next, after krypton had been removed, was a gas which gave a brilliant red light under spectroscopic discharge. This gas, identified in June, was named "neon", the Greek analogue of the Latin "novum" ('new') suggested by Ramsay's son. The characteristic brilliant red-orange color emitted by gaseous neon when excited electrically was noted immediately. Travers later wrote: "the blaze of crimson light from the tube told its own story and was a sight to dwell upon and never forget."

A second gas was also reported along with neon, having approximately the same density as argon but with a different spectrum – Ramsay and Travers named it "metargon". However, subsequent spectroscopic analysis revealed it to be argon contaminated with carbon monoxide. Finally, the same team discovered xenon by the same process, in September 1898.

Neon's scarcity precluded its prompt application for lighting along the lines of Moore tubes, which used nitrogen and which were commercialized in the early 1900s. After 1902, Georges Claude's company Air Liquide produced industrial quantities of neon as a byproduct of his air-liquefaction business. In December 1910 Claude demonstrated modern neon lighting based on a sealed tube of neon. Claude tried briefly to sell neon tubes for indoor domestic lighting, due to their intensity, but the market failed because homeowners objected to the color. In 1912, Claude's associate began selling neon discharge tubes as eye-catching advertising signs and was instantly more successful. Neon tubes were introduced to the U.S. in 1923 with two large neon signs bought by a Los Angeles Packard car dealership. The glow and arresting red color made neon advertising completely different from the competition. The intense color and vibrancy of neon equated with American society at the time, suggesting a "century of progress" and transforming cities into sensational new environments filled with radiating advertisements and "electro-graphic architecture".

Neon played a role in the basic understanding of the nature of atoms in 1913, when J. J. Thomson, as part of his exploration into the composition of canal rays, channeled streams of neon ions through a magnetic and an electric field and measured the deflection of the streams with a photographic plate. Thomson observed two separate patches of light on the photographic plate (see image), which suggested two different parabolas of deflection. Thomson eventually concluded that some of the atoms in the neon gas were of higher mass than the rest. Though not understood at the time by Thomson, this was the first discovery of isotopes of stable atoms. Thomson's device was a crude version of the instrument we now term a mass spectrometer.

Neon is the second lightest inert gas. Neon has three stable isotopes: Ne (90.48%), Ne (0.27%) and Ne (9.25%). Ne and Ne are partly primordial and partly nucleogenic (i.e. made by nuclear reactions of other nuclides with neutrons or other particles in the environment) and their variations in natural abundance are well understood. In contrast, Ne (the chief primordial isotope made in stellar nucleosynthesis) is not known to be nucleogenic or radiogenic. The causes of the variation of Ne in the Earth have thus been hotly debated.

The principal nuclear reactions generating nucleogenic neon isotopes start from Mg and Mg, which produce Ne and Ne respectively, after neutron capture and immediate emission of an alpha particle. The neutrons that produce the reactions are mostly produced by secondary spallation reactions from alpha particles, in turn derived from uranium-series decay chains. The net result yields a trend towards lower Ne/Ne and higher Ne/Ne ratios observed in uranium-rich rocks such as granites. Ne may also be produced in a nucleogenic reaction, when Ne absorbs a neutron from various natural terrestrial neutron sources.

In addition, isotopic analysis of exposed terrestrial rocks has demonstrated the cosmogenic (cosmic ray) production of Ne. This isotope is generated by spallation reactions on magnesium, sodium, silicon, and aluminium. By analyzing all three isotopes, the cosmogenic component can be resolved from magmatic neon and nucleogenic neon. This suggests that neon will be a useful tool in determining cosmic exposure ages of surface rocks and meteorites.

Similar to xenon, neon content observed in samples of volcanic gases is enriched in Ne and nucleogenic Ne relative to Ne content. The neon isotopic content of these mantle-derived samples represents a non-atmospheric source of neon. The Ne-enriched components are attributed to exotic primordial rare-gas components in the Earth, possibly representing solar neon. Elevated Ne abundances are found in diamonds, further suggesting a solar-neon reservoir in the Earth.

Neon is the second-lightest noble gas, after helium. It glows reddish-orange in a vacuum discharge tube. Also, neon has the narrowest liquid range of any element: from 24.55 to 27.05 K (−248.45 °C to −245.95 °C, or −415.21 °F to −410.71 °F). It has over 40 times the refrigerating capacity (per unit volume) of liquid helium and three times that of liquid hydrogen. In most applications it is a less expensive refrigerant than helium.
Neon plasma has the most intense light discharge at normal voltages and currents of all the noble gases. The average color of this light to the human eye is red-orange due to many lines in this range; it also contains a strong green line, which is hidden, unless the visual components are dispersed by a spectroscope.

Two quite different kinds of neon lighting are in common use. Neon glow lamps are generally tiny, with most operating between 100 and 250 volts. They have been widely used as power-on indicators and in circuit-testing equipment, but light-emitting diodes (LEDs) now dominate in those applications. These simple neon devices were the forerunners of plasma displays and plasma television screens. Neon signs typically operate at much higher voltages (2–15 kilovolts), and the luminous tubes are commonly meters long. The glass tubing is often formed into shapes and letters for signage, as well as architectural and artistic applications.

Stable isotopes of neon are produced in stars. Neon's most abundant isotope Ne (90.48%) is created by the nuclear fusion of carbon and carbon in the carbon-burning process of stellar nucleosynthesis. This requires temperatures above 500 megakelvins, which occur in the cores of stars of more than 8 solar masses.

Neon is abundant on a universal scale; it is the fifth most abundant chemical element in the universe by mass, after hydrogen, helium, oxygen, and carbon (see chemical element). Its relative rarity on Earth, like that of helium, is due to its relative lightness, high vapor pressure at very low temperatures, and chemical inertness, all properties which tend to keep it from being trapped in the condensing gas and dust clouds that formed the smaller and warmer solid planets like Earth.

Neon is monatomic, making it lighter than the molecules of diatomic nitrogen and oxygen which form the bulk of Earth's atmosphere; a balloon filled with neon will rise in air, albeit more slowly than a helium balloon.

Neon's abundance in the universe is about 1 part in 750; in the Sun and presumably in the proto-solar system nebula, about 1 part in 600. The Galileo spacecraft atmospheric entry probe found that even in the upper atmosphere of Jupiter, the abundance of neon is reduced (depleted) by about a factor of 10, to a level of 1 part in 6,000 by mass. This may indicate that even the ice-planetesimals which brought neon into Jupiter from the outer solar system, formed in a region which was too warm to retain the neon atmospheric component (abundances of heavier inert gases on Jupiter are several times that found in the Sun).

Neon comprises 1 part in 55,000 in the Earth's atmosphere, or 18.2 ppm by volume (this is about the same as the molecule or mole fraction), or 1 part in 79,000 of air by mass. It comprises a smaller fraction in the crust. It is industrially produced by cryogenic fractional distillation of liquefied air.

On 17 August 2015, based on studies with the Lunar Atmosphere and Dust Environment Explorer (LADEE) spacecraft, NASA scientists reported the detection of neon in the exosphere of the moon.

Neon is the first p-block noble gas, and the first element with a true octet of electrons. It is inert: as is the case with its lighter analogue, helium, no strongly bound neutral molecules containing neon have been identified. The ions [NeAr], [NeH], and [HeNe] have been observed from optical and mass spectrometric studies. Solid neon clathrate hydrate was produced from water ice and neon gas at pressures 0.35–0.48 GPa and temperatures about −30 °C. Ne atoms are not bonded to water and can freely move through this material. They can be extracted by placing the clathrate into a vacuum chamber for several days, yielding ice XVI, the least dense crystalline form of water.

The familiar Pauling electronegativity scale relies upon chemical bond energies, but such values have obviously not been measured for inert helium and neon. The Allen electronegativity scale, which relies only upon (measurable) atomic energies, identifies neon as the most electronegative element, closely followed by fluorine and helium.

Neon is often used in signs and produces an unmistakable bright reddish-orange light. Although tube lights with other colors are often called "neon", they use different noble gases or varied colors of fluorescent lighting.

Neon is used in vacuum tubes, high-voltage indicators, lightning arresters, wavemeter tubes, television tubes, and helium–neon lasers. Liquefied neon is commercially used as a cryogenic refrigerant in applications not requiring the lower temperature range attainable with more extreme liquid-helium refrigeration.

Neon, as liquid or gas, is relatively expensive – for small quantities, the price of liquid neon can be more than 55 times that of liquid helium. Driving neon's expense is the rarity of neon, which, unlike helium, can only be obtained from air.

The triple point temperature of neon (24.5561 K) is a defining fixed point in the International Temperature Scale of 1990.



</doc>
<doc id="21274" url="https://en.wikipedia.org/wiki?curid=21274" title="Nickel">
Nickel

Nickel is a chemical element with the symbol Ni and atomic number 28. It is a silvery-white lustrous metal with a slight golden tinge. Nickel belongs to the transition metals and is hard and ductile. Pure nickel, powdered to maximize the reactive surface area, shows a significant chemical activity, but larger pieces are slow to react with air under standard conditions because an oxide layer forms on the surface and prevents further corrosion (passivation). Even so, pure native nickel is found in Earth's crust only in tiny amounts, usually in ultramafic rocks, and in the interiors of larger nickel–iron meteorites that were not exposed to oxygen when outside Earth's atmosphere.

Meteoric nickel is found in combination with iron, a reflection of the origin of those elements as major end products of supernova nucleosynthesis. An iron–nickel mixture is thought to compose Earth's outer and inner cores.

Use of nickel (as a natural meteoric nickel–iron alloy) has been traced as far back as 3500 BCE. Nickel was first isolated and classified as a chemical element in 1751 by Axel Fredrik Cronstedt, who initially mistook the ore for a copper mineral, in the cobalt mines of Los, Hälsingland, Sweden. The element's name comes from a mischievous sprite of German miner mythology, Nickel (similar to Old Nick), who personified the fact that copper-nickel ores resisted refinement into copper. An economically important source of nickel is the iron ore limonite, which often contains 1–2% nickel. Nickel's other important ore minerals include pentlandite and a mixture of Ni-rich natural silicates known as garnierite. Major production sites include the Sudbury region in Canada (which is thought to be of meteoric origin), New Caledonia in the Pacific, and Norilsk in Russia.

Nickel is slowly oxidized by air at room temperature and is considered corrosion-resistant. Historically, it has been used for plating iron and brass, coating chemistry equipment, and manufacturing certain alloys that retain a high silvery polish, such as German silver. About 9% of world nickel production is still used for corrosion-resistant nickel plating. Nickel-plated objects sometimes provoke nickel allergy. Nickel has been widely used in coins, though its rising price has led to some replacement with cheaper metals in recent years.

Nickel is one of four elements (the others are iron, cobalt, and gadolinium) that are ferromagnetic at approximately room temperature. Alnico permanent magnets based partly on nickel are of intermediate strength between iron-based permanent magnets and rare-earth magnets. The metal is valuable in modern times chiefly in alloys; about 68% of world production is used in stainless steel. A further 10% is used for nickel-based and copper-based alloys, 7% for alloy steels, 3% in foundries, 9% in plating and 4% in other applications, including the fast-growing battery sector. As a compound, nickel has a number of niche chemical manufacturing uses, such as a catalyst for hydrogenation, cathodes for batteries, pigments and metal surface treatments. Nickel is an essential nutrient for some microorganisms and plants that have enzymes with nickel as an active site.

Nickel is a silvery-white metal with a slight golden tinge that takes a high polish. It is one of only four elements that are magnetic at or near room temperature, the others being iron, cobalt and gadolinium. Its Curie temperature is , meaning that bulk nickel is non-magnetic above this temperature. The unit cell of nickel is a face-centered cube with the lattice parameter of 0.352 nm, giving an atomic radius of 0.124 nm. This crystal structure is stable to pressures of at least 70 GPa. Nickel belongs to the transition metals. It is hard, malleable and ductile, and has a relatively high electrical and thermal conductivity for transition metals. The high compressive strength of 34 GPa, predicted for ideal crystals, is never obtained in the real bulk material due to the formation and movement of dislocations; however, it has been reached in Ni nanoparticles.

The nickel atom has two electron configurations, [Ar] 3d 4s and [Ar] 3d 4s, which are very close in energy – the symbol [Ar] refers to the argon-like core structure. There is some disagreement on which configuration has the lowest energy. Chemistry textbooks quote the electron configuration of nickel as [Ar] 4s 3d, which can also be written [Ar] 3d 4s. This configuration agrees with the Madelung energy ordering rule, which predicts that 4s is filled before 3d. It is supported by the experimental fact that the lowest energy state of the nickel atom is a 3d 4s energy level, specifically the 3d(F) 4s F, "J" = 4 level.

However, each of these two configurations splits into several energy levels due to fine structure, and the two sets of energy levels overlap. The average energy of states with configuration [Ar] 3d 4s is actually lower than the average energy of states with configuration [Ar] 3d 4s. For this reason, the research literature on atomic calculations quotes the ground state configuration of nickel as [Ar] 3d 4s.

The isotopes of nickel range in atomic weight from 48 u () to 78 u ().

Naturally occurring nickel is composed of five stable isotopes; , , , and , with being the most abundant (68.077% natural abundance). Isotopes heavier than cannot be formed by nuclear fusion without losing energy.

Nickel-62 has the highest mean nuclear binding energy per nucleon of any nuclide, at 8.7946 MeV/nucleon. Its binding energy is greater than both and , more abundant elements often incorrectly cited as having the most tightly-bound nuclides. Although this would seem to predict nickel-62 as the most abundant heavy element in the universe, the relatively high rate of photodisintegration of nickel in stellar interiors causes iron to be by far the most abundant.

The stable isotope nickel-60 is the daughter product of the extinct radionuclide , which decays with a half-life of 2.6 million years. Because has such a long half-life, its persistence in materials in the solar system may generate observable variations in the isotopic composition of . Therefore, the abundance of present in extraterrestrial material may provide insight into the origin of the solar system and its early history.

At least 26 nickel radioisotopes have been characterised, the most stable being with a half-life of 76,000 years, with 100 years, and with 6 days. All of the remaining radioactive isotopes have half-lives that are less than 60 hours and the majority of these have half-lives that are less than 30 seconds. This element also has one meta state.

Radioactive nickel-56 is produced by the silicon burning process and later set free in large quantities during type Ia supernovae. The shape of the light curve of these supernovae at intermediate to late-times corresponds to the decay via electron capture of nickel-56 to cobalt-56 and ultimately to iron-56. Nickel-59 is a long-lived cosmogenic radionuclide with a half-life of 76,000 years. has found many applications in isotope geology. has been used to date the terrestrial age of meteorites and to determine abundances of extraterrestrial dust in ice and sediment. Nickel-78's half-life was recently measured at 110 milliseconds, and is believed an important isotope in supernova nucleosynthesis of elements heavier than iron. The nuclide Ni, discovered in 1999, is the most proton-rich heavy element isotope known. With 28 protons and 20 neutrons, Ni is "doubly magic", as is with 28 protons and 50 neutrons. Both are therefore unusually stable for nuclides with so large a proton–neutron imbalance.

On Earth, nickel occurs most often in combination with sulfur and iron in pentlandite, with sulfur in millerite, with arsenic in the mineral nickeline, and with arsenic and sulfur in nickel galena. Nickel is commonly found in iron meteorites as the alloys kamacite and taenite. The presence of nickel in meteorites was first detected in 1799 by Joseph-Louis Proust, a French chemist who then worked in Spain. Proust analyzed samples of the meteorite from Campo del Cielo (Argentina), which had been obtained in 1783 by Miguel Rubín de Celis, discovering the presence in them of nickel (about 10%) along with iron.

The bulk of the nickel is mined from two types of ore deposits. The first is laterite, where the principal ore mineral mixtures are nickeliferous limonite, (Fe,Ni)O(OH), and garnierite (a mixture of various hydrous nickel and nickel-rich silicates). The second is magmatic sulfide deposits, where the principal ore mineral is pentlandite: .

Indonesia and Australia have the biggest estimated reserves, at 46% of world's total.

Identified land-based resources throughout the world averaging 1% nickel or greater comprise at least 130 million tons of nickel (about the double of known reserves). About 60% is in laterites and 40% in sulfide deposits.

On geophysical evidence, most of the nickel on Earth is believed to be in the Earth's outer and inner cores. Kamacite and taenite are naturally occurring alloys of iron and nickel. For kamacite, the alloy is usually in the proportion of 90:10 to 95:5, although impurities (such as cobalt or carbon) may be present, while for taenite the nickel content is between 20% and 65%. Kamacite and taenite are also found in nickel iron meteorites.

The most common oxidation state of nickel is +2, but compounds of Ni, Ni, and Ni are well known, and the exotic oxidation states Ni, Ni, and Ni have been produced and studied.

Nickel tetracarbonyl ), discovered by Ludwig Mond, is a volatile, highly toxic liquid at room temperature. On heating, the complex decomposes back to nickel and carbon monoxide:
This behavior is exploited in the Mond process for purifying nickel, as described above. The related nickel(0) complex bis(cyclooctadiene)nickel(0) is a useful catalyst in organonickel chemistry because the cyclooctadiene (or "cod") ligands are easily displaced.

Nickel(I) complexes are uncommon, but one example is the tetrahedral complex NiBr(PPh). Many nickel(I) complexes feature Ni-Ni bonding, such as the dark red diamagnetic prepared by reduction of with sodium amalgam. This compound is oxidised in water, liberating .

It is thought that the nickel(I) oxidation state is important to nickel-containing enzymes, such as [NiFe]-hydrogenase, which catalyzes the reversible reduction of protons to .

Nickel(II) forms compounds with all common anions, including sulfide, sulfate, carbonate, hydroxide, carboxylates, and halides. Nickel(II) sulfate is produced in large quantities by dissolving nickel metal or oxides in sulfuric acid, forming both a hexa- and heptahydrates useful for electroplating nickel. Common salts of nickel, such as chloride, nitrate, and sulfate, dissolve in water to give green solutions of the metal aquo complex .

The four halides form nickel compounds, which are solids with molecules that feature octahedral Ni centres. Nickel(II) chloride is most common, and its behavior is illustrative of the other halides. Nickel(II) chloride is produced by dissolving nickel or its oxide in hydrochloric acid. It is usually encountered as the green hexahydrate, the formula of which is usually written NiCl•6HO. When dissolved in water, this salt forms the metal aquo complex . Dehydration of NiCl•6HO gives the yellow anhydrous .

Some tetracoordinate nickel(II) complexes, e.g. bis(triphenylphosphine)nickel chloride, exist both in tetrahedral and square planar geometries. The tetrahedral complexes are paramagnetic, whereas the square planar complexes are diamagnetic. In having properties of magnetic equilibrium and formation of octahedral complexes, they contrast with the divalent complexes of the heavier group 10 metals, palladium(II) and platinum(II), which form only square-planar geometry.

Nickelocene is known; it has an electron count of 20, making it relatively unstable.

Numerous Ni(III) compounds are known, with the first such examples being Nickel(III) trihalophosphines (Ni(PPh)X). Further, Ni(III) forms simple salts with fluoride or oxide ions. Ni(III) can be stabilized by σ-donor ligands such as thiols and phosphines.

Ni(IV) is present in the mixed oxide , while Ni(III) is present in nickel oxide hydroxide, which is used as the cathode in many rechargeable batteries, including nickel-cadmium, nickel-iron, nickel hydrogen, and nickel-metal hydride, and used by certain manufacturers in Li-ion batteries. Ni(IV) remains a rare oxidation state of nickel and very few compounds are known to date.
Because the ores of nickel are easily mistaken for ores of silver, understanding of this metal and its use dates to relatively recent times. However, the unintentional use of nickel is ancient, and can be traced back as far as 3500 BCE. Bronzes from what is now Syria have been found to contain as much as 2% nickel. Some ancient Chinese manuscripts suggest that "white copper" (cupronickel, known as "baitong") was used there between 1700 and 1400 BCE. This Paktong white copper was exported to Britain as early as the 17th century, but the nickel content of this alloy was not discovered until 1822. Coins of nickel-copper alloy were minted by the Bactrian kings Agathocles, Euthydemus II and Pantaleon in the 2nd century BCE, possibly out of the Chinese cupronickel.

In medieval Germany, a red mineral was found in the Erzgebirge (Ore Mountains) that resembled copper ore. However, when miners were unable to extract any copper from it, they blamed a mischievous sprite of German mythology, Nickel (similar to "Old Nick"), for besetting the copper. They called this ore "Kupfernickel" from the German "Kupfer" for copper. This ore is now known to be nickeline, a nickel arsenide. In 1751, Baron Axel Fredrik Cronstedt tried to extract copper from kupfernickel at a cobalt mine in the Swedish village of Los, and instead produced a white metal that he named after the spirit that had given its name to the mineral, nickel. In modern German, Kupfernickel or Kupfer-Nickel designates the alloy cupronickel.

Originally, the only source for nickel was the rare Kupfernickel. Beginning in 1824, nickel was obtained as a byproduct of cobalt blue production. The first large-scale smelting of nickel began in Norway in 1848 from nickel-rich pyrrhotite. The introduction of nickel in steel production in 1889 increased the demand for nickel, and the nickel deposits of New Caledonia, discovered in 1865, provided most of the world's supply between 1875 and 1915. The discovery of the large deposits in the Sudbury Basin, Canada in 1883, in Norilsk-Talnakh, Russia in 1920, and in the Merensky Reef, South Africa in 1924, made large-scale production of nickel possible.

Aside from the aforementioned Bactrian coins, nickel was not a component of coins until the mid-19th century.

99.9% nickel five-cent coins were struck in Canada (the world's largest nickel producer at the time) during non-war years from 1922 to 1981; the metal content made these coins magnetic. During the wartime period 1942–45, most or all nickel was removed from Canadian and US coins to save it for manufacturing armor. Canada used 99.9% nickel from 1968 in its higher-value coins until 2000.

Coins of nearly pure nickel were first used in 1881 in Switzerland.

Birmingham forged nickel coins in for trading in Malaya.

In the United States, the term "nickel" or "nick" originally applied to the copper-nickel Flying Eagle cent, which replaced copper with 12% nickel 1857–58, then the Indian Head cent of the same alloy from 1859 to 1864. Still later, in 1865, the term designated the three-cent nickel, with nickel increased to 25%. In 1866, the five-cent shield nickel (25% nickel, 75% copper) appropriated the designation. Along with the alloy proportion, this term has been used to the present in the United States.

In the 21st century, the high price of nickel has led to some replacement of the metal in coins around the world. Coins still made with nickel alloys include one- and two-euro coins, 5¢, 10¢, 25¢, and 50¢ U.S. coins, and 20p, 50p, £1, and £2 UK coins. Nickel-alloy in 5p and 10p UK coins was replaced with nickel-plated steel began in 2012, causing allergy problems for some people and public controversy.

More than 2.7 million tonnes (t) of nickel per year are estimated to be mined worldwide, with Indonesia (800,000 t), the Philippines (420,000 t), Russia (270,000 t), New Caledonia (220,000 t), Australia (180,000 t) and Canada (180,000 t) being the largest producers as of 2019. The largest deposits of nickel in non-Russian Europe are located in Finland and Greece. Identified land-based resources averaging 1% nickel or greater contain at least 130 million tonnes of nickel. Approximately 60% is in laterites and 40% is in sulfide deposits. In addition, extensive deep-sea resources of nickel are in manganese crusts and nodules covering large areas of the ocean floor, particularly in the Pacific Ocean.

The one locality in the United States where nickel has been profitably mined is Riddle, Oregon, where several square miles of nickel-bearing garnierite surface deposits are located. The mine closed in 1987. The Eagle mine project is a new nickel mine in Michigan's upper peninsula. Construction was completed in 2013, and operations began in the third quarter of 2014. In the first full year of operation, the Eagle Mine produced 18,000 t.

Nickel is obtained through extractive metallurgy: it is extracted from the ore by conventional roasting and reduction processes that yield a metal of greater than 75% purity. In many stainless steel applications, 75% pure nickel can be used without further purification, depending on the impurities.

Traditionally, most sulfide ores have been processed using pyrometallurgical techniques to produce a matte for further refining. Recent advances in hydrometallurgical techniques resulted in significantly purer metallic nickel product. Most sulfide deposits have traditionally been processed by concentration through a froth flotation process followed by pyrometallurgical extraction. In hydrometallurgical processes, nickel sulfide ores are concentrated with flotation (differential flotation if Ni/Fe ratio is too low) and then smelted. The nickel matte is further processed with the Sherritt-Gordon process. First, copper is removed by adding hydrogen sulfide, leaving a concentrate of cobalt and nickel. Then, solvent extraction is used to separate the cobalt and nickel, with the final nickel content greater than 99%.
A second common refining process is leaching the metal matte into a nickel salt solution, followed by the electro-winning of the nickel from solution by plating it onto a cathode as electrolytic nickel.

The purest metal is obtained from nickel oxide by the Mond process, which achieves a purity of greater than 99.99%. The process was patented by Ludwig Mond and has been in industrial use since before the beginning of the 20th century. In this process, nickel is reacted with carbon monoxide in the presence of a sulfur catalyst at around 40–80 °C to form nickel carbonyl. Iron gives iron pentacarbonyl, too, but this reaction is slow. If necessary, the nickel may be separated by distillation. Dicobalt octacarbonyl is also formed in nickel distillation as a by-product, but it decomposes to tetracobalt dodecacarbonyl at the reaction temperature to give a non-volatile solid.

Nickel is obtained from nickel carbonyl by one of two processes. It may be passed through a large chamber at high temperatures in which tens of thousands of nickel spheres, called pellets, are constantly stirred. The carbonyl decomposes and deposits pure nickel onto the nickel spheres. In the alternate process, nickel carbonyl is decomposed in a smaller chamber at 230 °C to create a fine nickel powder. The byproduct carbon monoxide is recirculated and reused. The highly pure nickel product is known as "carbonyl nickel".

The market price of nickel surged throughout 2006 and the early months of 2007; as of April 5, 2007, the metal was trading at US$52,300/tonne or $1.47/oz. The price subsequently fell dramatically, and as of September 2017, the metal was trading at $11,000/tonne, or $0.31/oz.

The US nickel coin contains of nickel, which at the April 2007 price was worth 6.5 cents, along with 3.75 grams of copper worth about 3 cents, with a total metal value of more than 9 cents. Since the face value of a nickel is 5 cents, this made it an attractive target for melting by people wanting to sell the metals at a profit. However, the United States Mint, in anticipation of this practice, implemented new interim rules on December 14, 2006, subject to public comment for 30 days, which criminalized the melting and export of cents and nickels. Violators can be punished with a fine of up to $10,000 and/or imprisoned for a maximum of five years.

As of September 19, 2013, the melt value of a US nickel (copper and nickel included) is $0.045, which is 90% of the face value.

The global production of nickel is presently used as follows: 68% in stainless steel; 10% in nonferrous alloys; 9% in electroplating; 7% in alloy steel; 3% in foundries; and 4% other uses (including batteries).

Nickel is used in many specific and recognizable industrial and consumer products, including stainless steel, alnico magnets, coinage, rechargeable batteries, electric guitar strings, microphone capsules, plating on plumbing fixtures, and special alloys such as permalloy, elinvar, and invar. It is used for plating and as a green tint in glass. Nickel is preeminently an alloy metal, and its chief use is in nickel steels and nickel cast irons, in which it typically increases the tensile strength, toughness, and elastic limit. It is widely used in many other alloys, including nickel brasses and bronzes and alloys with copper, chromium, aluminium, lead, cobalt, silver, and gold (Inconel, Incoloy, Monel, Nimonic).

Because it is resistant to corrosion, nickel was occasionally used as a substitute for decorative silver. Nickel was also occasionally used in some countries after 1859 as a cheap coinage metal (see above), but in the later years of the 20th century, it was replaced by cheaper stainless steel (i.e. iron) alloys, except in the United States and Canada.

Nickel is an excellent alloying agent for certain precious metals and is used in the fire assay as a collector of platinum group elements (PGE). As such, nickel is capable of fully collecting all six PGE elements from ores, and of partially collecting gold. High-throughput nickel mines may also engage in PGE recovery (primarily platinum and palladium); examples are Norilsk in Russia and the Sudbury Basin in Canada.

Nickel foam or nickel mesh is used in gas diffusion electrodes for alkaline fuel cells.

Nickel and its alloys are frequently used as catalysts for hydrogenation reactions. Raney nickel, a finely divided nickel-aluminium alloy, is one common form, though related catalysts are also used, including Raney-type catalysts.

Nickel is a naturally magnetostrictive material, meaning that, in the presence of a magnetic field, the material undergoes a small change in length. The magnetostriction of nickel is on the order of 50 ppm and is negative, indicating that it contracts.

Nickel is used as a binder in the cemented tungsten carbide or hardmetal industry and used in proportions of 6% to 12% by weight. Nickel makes the tungsten carbide magnetic and adds corrosion-resistance to the cemented parts, although the hardness is less than those with a cobalt binder.

, with its half-life of 100.1 years, is useful in krytron devices as a beta particle (high-speed electron) emitter to make ionization by the keep-alive electrode more reliable.

Around 27% of all nickel production is destined for engineering, 10% for building and construction, 14% for tubular products, 20% for metal goods, 14% for transport, 11% for electronic goods, and 5% for other uses.

Raney nickel is widely used for hydrogenation of unsaturated oils to make margarine, and substandard margarine and leftover oil may contain nickel as contaminant. Forte et al. found that type 2 diabetic patients have 0.89 ng/ml of Ni in the blood relative to 0.77 ng/ml in the control subjects.

Although it was not recognized until the 1970s, nickel is known to play an important role in the biology of some plants, eubacteria, archaebacteria, and fungi. Nickel enzymes such as urease are considered virulence factors in some organisms. Urease catalyzes the hydrolysis of urea to form ammonia and carbamate. The NiFe hydrogenases can catalyze the oxidation of to form protons and electrons, and can also catalyze the reverse reaction, the reduction of protons to form hydrogen gas. A nickel-tetrapyrrole coenzyme, cofactor F430, is present in methyl coenzyme M reductase, which can catalyze the formation of methane, or the reverse reaction, in methanogenic archaea (in +1 oxidation state). One of the carbon monoxide dehydrogenase enzymes consists of an Fe-Ni-S cluster. Other nickel-bearing enzymes include a rare bacterial class of superoxide dismutase and glyoxalase I enzymes in bacteria and several parasitic eukaryotic trypanosomal parasites (in higher organisms, including yeast and mammals, this enzyme contains divalent Zn).

Dietary nickel may affect human health through infections by nickel-dependent bacteria, but it is also possible that nickel is an essential nutrient for bacteria residing in the large intestine, in effect functioning as a prebiotic. The US Institute of Medicine has not confirmed that nickel is an essential nutrient for humans, so neither a Recommended Dietary Allowance (RDA) nor an Adequate Intake have been established. The Tolerable Upper Intake Level of dietary nickel is 1000 µg/day as soluble nickel salts. Dietary intake is estimated at 70 to 100 µg/day, with less than 10% absorbed. What is absorbed is excreted in urine. Relatively large amounts of nickel – comparable to the estimated average ingestion above – leach into food cooked in stainless steel. For example, the amount of nickel leached after 10 cooking cycles into one serving of tomato sauce averages 88 µg.

Nickel released from Siberian Traps volcanic eruptions is suspected of assisting the growth of "Methanosarcina", a genus of euryarchaeote archaea that produced methane during the Permian–Triassic extinction event, the biggest extinction event on record.

The major source of nickel exposure is oral consumption, as nickel is essential to plants. Nickel is found naturally in both food and water, and may be increased by human pollution. For example, nickel-plated faucets may contaminate water and soil; mining and smelting may dump nickel into waste-water; nickel–steel alloy cookware and nickel-pigmented dishes may release nickel into food. The atmosphere may be polluted by nickel ore refining and fossil fuel combustion. Humans may absorb nickel directly from tobacco smoke and skin contact with jewelry, shampoos, detergents, and coins. A less-common form of chronic exposure is through hemodialysis as traces of nickel ions may be absorbed into the plasma from the chelating action of albumin.

The average daily exposure does not pose a threat to human health. Most of the nickel absorbed every day by humans is removed by the kidneys and passed out of the body through urine or is eliminated through the gastrointestinal tract without being absorbed. Nickel is not a cumulative poison, but larger doses or chronic inhalation exposure may be toxic, even carcinogenic, and constitute an occupational hazard.

Nickel compounds are classified as human carcinogens<ref name="Reg 1272/2008">Regulation (EC) No 1272/2008 of the European Parliament and of the Council of 16 December 2008 on Classification, Labelling and Packaging of Substances and Mixtures, Amending and Repealing Directives 67/548/EEC and 1999/45/EC and amending Regulation (EC) No 1907/2006 [OJ L 353, 31.12.2008, p. 1]. Annex VI . Accessed July 13, 2017.</ref> based on increased respiratory cancer risks observed in epidemiological studies of sulfidic ore refinery workers. This is supported by the positive results of the NTP bioassays with Ni sub-sulfide and Ni oxide in rats and mice. The human and animal data consistently indicate a lack of carcinogenicity via the oral route of exposure and limit the carcinogenicity of nickel compounds to respiratory tumours after inhalation. Nickel metal is classified as a suspect carcinogen; there is consistency between the absence of increased respiratory cancer risks in workers predominantly exposed to metallic nickel and the lack of respiratory tumours in a rat lifetime inhalation carcinogenicity study with nickel metal powder. In the rodent inhalation studies with various nickel compounds and nickel metal, increased lung inflammations with and without bronchial lymph node hyperplasia or fibrosis were observed. In rat studies, oral ingestion of water-soluble nickel salts can trigger perinatal mortality effects in pregnant animals. Whether these effects are relevant to humans is unclear as epidemiological studies of highly exposed female workers have not shown adverse developmental toxicity effects.

People can be exposed to nickel in the workplace by inhalation, ingestion, and contact with skin or eye. The Occupational Safety and Health Administration (OSHA) has set the legal limit (permissible exposure limit) for the workplace at 1 mg/m per 8-hour workday, excluding nickel carbonyl. The National Institute for Occupational Safety and Health (NIOSH) specifies the recommended exposure limit (REL) of 0.015 mg/m per 8-hour workday. At 10 mg/m, nickel is immediately dangerous to life and health. Nickel carbonyl [] is an extremely toxic gas. The toxicity of metal carbonyls is a function of both the toxicity of the metal and the off-gassing of carbon monoxide from the carbonyl functional groups; nickel carbonyl is also explosive in air.

Sensitized individuals may show a skin contact allergy to nickel known as a contact dermatitis. Highly sensitized individuals may also react to foods with high nickel content. Sensitivity to nickel may also be present in patients with pompholyx. Nickel is the top confirmed contact allergen worldwide, partly due to its use in jewelry for pierced ears. Nickel allergies affecting pierced ears are often marked by itchy, red skin. Many earrings are now made without nickel or with low-release nickel to address this problem. The amount allowed in products that contact human skin is now regulated by the European Union. In 2002, researchers found that the nickel released by 1 and 2 Euro coins was far in excess of those standards. This is believed to be the result of a galvanic reaction. Nickel was voted Allergen of the Year in 2008 by the American Contact Dermatitis Society. In August 2015, the American Academy of Dermatology adopted a position statement on the safety of nickel: "Estimates suggest that contact dermatitis, which includes nickel sensitization, accounts for approximately $1.918 billion and affects nearly 72.29 million people."

Reports show that both the nickel-induced activation of hypoxia-inducible factor (HIF-1) and the up-regulation of hypoxia-inducible genes are caused by depletion of intracellular ascorbate. The addition of ascorbate to the culture medium increased the intracellular ascorbate level and reversed both the metal-induced stabilization of HIF-1- and HIF-1α-dependent gene expression.



</doc>
<doc id="21275" url="https://en.wikipedia.org/wiki?curid=21275" title="Niobium">
Niobium

Niobium, also known as columbium, is a chemical element with the symbol Nb (formerly Cb) and atomic number 41. Niobium is a light grey, crystalline, and ductile transition metal. Pure niobium has a Mohs hardness rating similar to that of pure titanium, and it has similar ductility to iron. Niobium oxidizes in the earth's atmosphere very slowly, hence its application in jewelry as a hypoallergenic alternative to nickel. Niobium is often found in the minerals pyrochlore and columbite, hence the former name "columbium". Its name comes from Greek mythology, specifically Niobe, who was the daughter of Tantalus, the namesake of tantalum. The name reflects the great similarity between the two elements in their physical and chemical properties, making them difficult to distinguish.

The English chemist Charles Hatchett reported a new element similar to tantalum in 1801 and named it columbium. In 1809, the English chemist William Hyde Wollaston wrongly concluded that tantalum and columbium were identical. The German chemist Heinrich Rose determined in 1846 that tantalum ores contain a second element, which he named niobium. In 1864 and 1865, a series of scientific findings clarified that niobium and columbium were the same element (as distinguished from tantalum), and for a century both names were used interchangeably. Niobium was officially adopted as the name of the element in 1949, but the name columbium remains in current use in metallurgy in the United States.

It was not until the early 20th century that niobium was first used commercially. Brazil is the leading producer of niobium and ferroniobium, an alloy of 60–70% niobium with iron. Niobium is used mostly in alloys, the largest part in special steel such as that used in gas pipelines. Although these alloys contain a maximum of 0.1%, the small percentage of niobium enhances the strength of the steel. The temperature stability of niobium-containing superalloys is important for its use in jet and rocket engines.

Niobium is used in various superconducting materials. These superconducting alloys, also containing titanium and tin, are widely used in the superconducting magnets of MRI scanners. Other applications of niobium include welding, nuclear industries, electronics, optics, numismatics, and jewelry. In the last two applications, the low toxicity and iridescence produced by anodization are highly desired properties. Niobium is considered a technology-critical element.

Niobium was identified by English chemist Charles Hatchett in 1801. He found a new element in a mineral sample that had been sent to England from Connecticut, United States in 1734 by John Winthrop F.R.S. (grandson of John Winthrop the Younger) and named the mineral "columbite" and the new element "columbium" after "Columbia", the poetical name for the United States. The "columbium" discovered by Hatchett was probably a mixture of the new element with tantalum.

Subsequently, there was considerable confusion over the difference between columbium (niobium) and the closely related tantalum. In 1809, English chemist William Hyde Wollaston compared the oxides derived from both columbium—columbite, with a density 5.918 g/cm, and tantalum—tantalite, with a density over 8 g/cm, and concluded that the two oxides, despite the significant difference in density, were identical; thus he kept the name tantalum. This conclusion was disputed in 1846 by German chemist Heinrich Rose, who argued that there were two different elements in the tantalite sample, and named them after children of Tantalus: "niobium" (from Niobe) and "pelopium" (from Pelops). This confusion arose from the minimal observed differences between tantalum and niobium. The claimed new elements "pelopium", "ilmenium", and "dianium" were in fact identical to niobium or mixtures of niobium and tantalum.

The differences between tantalum and niobium were unequivocally demonstrated in 1864 by Christian Wilhelm Blomstrand and Henri Etienne Sainte-Claire Deville, as well as Louis J. Troost, who determined the formulas of some of the compounds in 1865 and finally by Swiss chemist Jean Charles Galissard de Marignac in 1866, who all proved that there were only two elements. Articles on "ilmenium" continued to appear until 1871.

De Marignac was the first to prepare the metal in 1864, when he reduced niobium chloride by heating it in an atmosphere of hydrogen. Although de Marignac was able to produce tantalum-free niobium on a larger scale by 1866, it was not until the early 20th century that niobium was used in incandescent lamp filaments, the first commercial application. This use quickly became obsolete through the replacement of niobium with tungsten, which has a higher melting point. That niobium improves the strength of steel was first discovered in the 1920s, and this application remains its predominant use. In 1961, the American physicist Eugene Kunzler and coworkers at Bell Labs discovered that niobium-tin continues to exhibit superconductivity in the presence of strong electric currents and magnetic fields, making it the first material to support the high currents and fields necessary for useful high-power magnets and electrical power machinery. This discovery enabled – two decades later – the production of long multi-strand cables wound into coils to create large, powerful electromagnets for rotating machinery, particle accelerators, and particle detectors.

"Columbium" (symbol "Cb") was the name originally bestowed by Hatchett upon his discovery of the metal in 1801. The name reflected that the type specimen of the ore came from America (Columbia). This name remained in use in American journals—the last paper published by American Chemical Society with "columbium" in its title dates from 1953—while "niobium" was used in Europe. To end this confusion, the name "niobium" was chosen for element 41 at the 15th Conference of the Union of Chemistry in Amsterdam in 1949. A year later this name was officially adopted by the International Union of Pure and Applied Chemistry (IUPAC) after 100 years of controversy, despite the chronological precedence of the name "columbium". This was a compromise of sorts; the IUPAC accepted tungsten instead of wolfram in deference to North American usage; and "niobium" instead of "columbium" in deference to European usage. While many US chemical societies and government organizations typically use the official IUPAC name, some metallurgists and metal societies still use the original American name, ""columbium"".

Niobium is a lustrous, grey, ductile, paramagnetic metal in group 5 of the periodic table (see table), with an electron configuration in the outermost shells atypical for group 5. (This can be observed in the neighborhood of ruthenium (44), rhodium (45), and palladium (46).)

Although it is thought to have a body-centered cubic crystal structure from absolute zero to its melting point, high-resolution measurements of the thermal expansion along the three crystallographic axes reveal anisotropies which are inconsistent with a cubic structure. Therefore, further research and discovery in this area is expected.

Niobium becomes a superconductor at cryogenic temperatures. At atmospheric pressure, it has the highest critical temperature of the elemental superconductors at 9.2 K. Niobium has the greatest magnetic penetration depth of any element. In addition, it is one of the three elemental Type II superconductors, along with vanadium and technetium. The superconductive properties are strongly dependent on the purity of the niobium metal.

When very pure, it is comparatively soft and ductile, but impurities make it harder.

The metal has a low capture cross-section for thermal neutrons; thus it is used in the nuclear industries where neutron transparent structures are desired.

The metal takes on a bluish tinge when exposed to air at room temperature for extended periods. Despite a high melting point in elemental form (2,468 °C), it has a lower density than other refractory metals. Furthermore, it is corrosion-resistant, exhibits superconductivity properties, and forms dielectric oxide layers.

Niobium is slightly less electropositive and more compact than its predecessor in the periodic table, zirconium, whereas it is virtually identical in size to the heavier tantalum atoms, as a result of the lanthanide contraction. As a result, niobium's chemical properties are very similar to those for tantalum, which appears directly below niobium in the periodic table. Although its corrosion resistance is not as outstanding as that of tantalum, the lower price and greater availability make niobium attractive for less demanding applications, such as vat linings in chemical plants.

Niobium in the Earth's crust comprises one stable isotope, Nb. By 2003, at least 32 radioisotopes had been synthesized, ranging in atomic mass from 81 to 113. The most stable of these is Nb with a half-life of 34.7 million years. One of the least stable is Nb, with an estimated half-life of 30 milliseconds. Isotopes that are lighter than the stable Nb tend to decay by β decay, and those that are heavier tend to decay by β decay, with some exceptions. Nb, Nb, and Nb have minor β delayed proton emission decay paths, Nb decays by electron capture and positron emission, and Nb decays by both β and β decay.

At least 25 nuclear isomers have been described, ranging in atomic mass from 84 to 104. Within this range, only Nb, Nb, and Nb do not have isomers. The most stable of niobium's isomers is Nb with a half-life of 16.13 years. The least stable isomer is Nb with a half-life of 103 ns. All of niobium's isomers decay by isomeric transition or beta decay except Nb, which has a minor electron capture branch.

Niobium is estimated to be the 34th most common element in the Earth's crust, with 20 ppm. Some think that the abundance on Earth is much greater, and that the element's high density has concentrated it in the Earth's core. The free element is not found in nature, but niobium occurs in combination with other elements in minerals. Minerals that contain niobium often also contain tantalum. Examples include columbite ((Fe,Mn)(Nb,Ta)O) and columbite–tantalite (or "coltan", (Fe,Mn)(Ta,Nb)O). Columbite–tantalite minerals (the most common species being columbite-(Fe) and tantalite-(Fe), where "-(Fe)" is the Levinson suffix informing about the prevailence of iron over other elements like manganese) are most usually found as accessory minerals in pegmatite intrusions, and in alkaline intrusive rocks. Less common are the niobates of calcium, uranium, thorium and the rare earth elements. Examples of such niobates are pyrochlore ((Na,Ca)NbO(OH,F)) (now a group name, with a relatively common example being, e.g., fluorcalciopyrochlore) and euxenite (correctly named euxenite-(Y)) ((Y,Ca,Ce,U,Th)(Nb,Ta,Ti)O). These large deposits of niobium have been found associated with carbonatites (carbonate-silicate igneous rocks) and as a constituent of pyrochlore. 

The three largest currently mined deposits of pyrochlore, two in Brazil and one in Canada, were found in the 1950s, and are still the major producers of niobium mineral concentrates. The largest deposit is hosted within a carbonatite intrusion in Araxá, state of Minas Gerais, Brazil, owned by CBMM (Companhia Brasileira de Metalurgia e Mineração); the other active Brazilian deposit is located near Catalão, state of Goiás, and owned by China Molybdenum, also hosted within a carbonatite intrusion. Together, those two mines produce about 88% of the world's supply. Brazil also has a large but still unexploited deposit near São Gabriel da Cachoeira, state of Amazonas, as well as a few smaller deposits, notably in the state of Roraima.

The third largest producer of niobium is the carbonatite-hosted Niobec mine, in Saint-Honoré, near Chicoutimi, Quebec, Canada, owned by Magris Resources. It produces between 7% and 10% of the world's supply.

After the separation from the other minerals, the mixed oxides of tantalum TaO and niobium NbO are obtained. The first step in the processing is the reaction of the oxides with hydrofluoric acid:

The first industrial scale separation, developed by de Marignac, exploits the differing solubilities of the complex niobium and tantalum fluorides, dipotassium oxypentafluoroniobate monohydrate (K<nowiki>[</nowiki>NbOF<nowiki>]</nowiki>·HO) and dipotassium heptafluorotantalate (K<nowiki>[</nowiki>TaF<nowiki>]</nowiki>) in water. Newer processes use the liquid extraction of the fluorides from aqueous solution by organic solvents like cyclohexanone. The complex niobium and tantalum fluorides are extracted separately from the organic solvent with water and either precipitated by the addition of potassium fluoride to produce a potassium fluoride complex, or precipitated with ammonia as the pentoxide:

Followed by:

Several methods are used for the reduction to metallic niobium. The electrolysis of a molten mixture of K<nowiki>[</nowiki>NbOF<nowiki>]</nowiki> and sodium chloride is one; the other is the reduction of the fluoride with sodium. With this method, a relatively high purity niobium can be obtained. In large scale production, NbO is reduced with hydrogen or carbon. In the aluminothermic reaction, a mixture of iron oxide and niobium oxide is reacted with aluminium:

Small amounts of oxidizers like sodium nitrate are added to enhance the reaction. The result is aluminium oxide and ferroniobium, an alloy of iron and niobium used in steel production. Ferroniobium contains between 60 and 70% niobium. Without iron oxide, the aluminothermic process is used to produce niobium. Further purification is necessary to reach the grade for superconductive alloys. Electron beam melting under vacuum is the method used by the two major distributors of niobium.

, CBMM from Brazil controlled 85 percent of the world's niobium production. The United States Geological Survey estimates that the production increased from 38,700 tonnes in 2005 to 44,500 tonnes in 2006. Worldwide resources are estimated to be 4,400,000 tonnes. During the ten-year period between 1995 and 2005, the production more than doubled, starting from 17,800 tonnes in 1995. Between 2009 and 2011, production was stable at 63,000 tonnes per year, with a slight decrease in 2012 to only 50,000 tonnes per year.

Lesser amounts are found in Malawi's Kanyika Deposit (Kanyika mine).

In many ways, niobium is similar to tantalum and zirconium. It reacts with most nonmetals at high temperatures; with fluorine at room temperature; with chlorine at 150 °C and hydrogen at 200 °C; and with nitrogen at 400 °C, with products that are frequently interstitial and nonstoichiometric. The metal begins to oxidize in air at 200 °C. It resists corrosion by fused alkalis and by acids, including aqua regia, hydrochloric, sulfuric, nitric and phosphoric acids. Niobium is attacked by hydrofluoric acid and hydrofluoric/nitric acid mixtures.

Although niobium exhibits all of the formal oxidation states from +5 to −1, the most common compounds have niobium in the +5 state. Characteristically, compounds in oxidation states less than 5+ display Nb–Nb bonding. In aqueous solutions, niobium only exhibit the +5 oxidation state. It is also readily prone to hydrolysis and is barely soluble in dilute solutions of hydrochloric, sulfuric, nitric and phosphoric acids due to the precipitation of hydrous Nb oxide. Nb(V) is also slightly soluble in alkaline media due to the formation of soluble polyoxoniobate species.

Niobium forms oxides in the oxidation states +5 (NbO), +4 (NbO), +3 (), and the rarer oxidation state, +2 (NbO). Most common is the pentoxide, precursor to almost all niobium compounds and alloys. Niobates are generated by dissolving the pentoxide in basic hydroxide solutions or by melting it in alkali metal oxides. Examples are lithium niobate (LiNbO) and lanthanum niobate (LaNbO). In the lithium niobate is a trigonally distorted perovskite-like structure, whereas the lanthanum niobate contains lone ions. The layered niobium sulfide (NbS) is also known.

Materials can be coated with a thin film of niobium(V) oxide chemical vapor deposition or atomic layer deposition processes, produced by the thermal decomposition of niobium(V) ethoxide above 350 °C.

Niobium forms halides in the oxidation states of +5 and +4 as well as diverse substoichiometric compounds. The pentahalides () feature octahedral Nb centres. Niobium pentafluoride (NbF) is a white solid with a melting point of 79.0 °C and niobium pentachloride (NbCl) is yellow (see image at left) with a melting point of 203.4 °C. Both are hydrolyzed to give oxides and oxyhalides, such as NbOCl. The pentachloride is a versatile reagent used to generate the organometallic compounds, such as niobocene dichloride (). The tetrahalides () are dark-coloured polymers with Nb-Nb bonds; for example, the black hygroscopic niobium tetrafluoride (NbF) and brown niobium tetrachloride (NbCl).

Anionic halide compounds of niobium are well known, owing in part to the Lewis acidity of the pentahalides. The most important is [NbF], an intermediate in the separation of Nb and Ta from the ores. This heptafluoride tends to form the oxopentafluoride more readily than does the tantalum compound. Other halide complexes include octahedral [NbCl]:

As with other metals with low atomic numbers, a variety of reduced halide cluster ions is known, the prime example being [NbCl].

Other binary compounds of niobium include niobium nitride (NbN), which becomes a superconductor at low temperatures and is used in detectors for infrared light. The main niobium carbide is NbC, an extremely hard, refractory, ceramic material, commercially used in cutting tool bits.

Out of 44,500 tonnes of niobium mined in 2006, an estimated 90% was used in high-grade structural steel. The second largest application is superalloys. Niobium alloy superconductors and electronic components account for a very small share of the world production.

Niobium is an effective microalloying element for steel, within which it forms niobium carbide and niobium nitride. These compounds improve the grain refining, and retard recrystallization and precipitation hardening. These effects in turn increase the toughness, strength, formability, and weldability. Within microalloyed stainless steels, the niobium content is a small (less than 0.1%) but important addition to high strength low alloy steels that are widely used structurally in modern automobiles. Niobium is sometimes used in considerably higher quantities for highly wear-resistant machine components and knives, as high as 3% in Crucible CPM S110V stainless steel.

These same niobium alloys are often used in pipeline construction.

Quantities of niobium are used in nickel-, cobalt-, and iron-based superalloys in proportions as great as 6.5% for such applications as jet engine components, gas turbines, rocket subassemblies, turbo charger systems, heat resisting, and combustion equipment. Niobium precipitates a hardening γ<nowiki>"</nowiki>-phase within the grain structure of the superalloy.

One example superalloy is Inconel 718, consisting of roughly 50% nickel, 18.6% chromium, 18.5% iron, 5% niobium, 3.1% molybdenum, 0.9% titanium, and 0.4% aluminium. These superalloys were used, for example, in advanced air frame systems for the Gemini program. Another niobium alloy was used for the nozzle of the Apollo Service Module. Because niobium is oxidized at temperatures above 400 °C, a protective coating is necessary for these applications to prevent the alloy from becoming brittle.

C-103 alloy was developed in the early 1960s jointly by the Wah Chang Corporation and Boeing Co. DuPont, Union Carbide Corp., General Electric Co. and several other companies were developing Nb-base alloys simultaneously, largely driven by the Cold War and Space Race. It is composed of 89% niobium, 10% hafnium and 1% titanium and is used for liquid rocket thruster nozzles, such as the main engine of the Apollo Lunar Modules.

The nozzle of the Merlin Vacuum series of engines developed by SpaceX for the upper stage of its Falcon 9 rocket is made from a niobium alloy.

The reactivity of niobium with oxygen requires it to be worked in a vacuum or inert atmosphere, which significantly increases the cost and difficulty of production. Vacuum arc remelting (VAR) and electron beam melting (EBM), novel processes at the time, enabled the development of niobium and other reactive metals. The project that yielded C-103 began in 1959 with as many as 256 experimental niobium alloys in the "C-series" (possibly from columbium) that could be melted as buttons and rolled into sheet. Wah Chang had an inventory of hafnium, refined from nuclear-grade zirconium alloys, that it wanted to put to commercial use. The 103rd experimental composition of the C-series alloys, Nb-10Hf-1Ti, had the best combination of formability and high-temperature properties. Wah Chang fabricated the first 500-lb heat of C-103 in 1961, ingot to sheet, using EBM and VAR. The intended applications included turbine engines and liquid metal heat exchangers. Competing niobium alloys from that era included FS85 (Nb-10W-28Ta-1Zr) from Fansteel Metallurgical Corp., Cb129Y (Nb-10W-10Hf-0.2Y) from Wah Chang and Boeing, Cb752 (Nb-10W-2.5Zr) from Union Carbide, and Nb1Zr from Superior Tube Co.

Niobium-germanium (), niobium-tin (), as well as the niobium-titanium alloys are used as a type II superconductor wire for superconducting magnets. These superconducting magnets are used in magnetic resonance imaging and nuclear magnetic resonance instruments as well as in particle accelerators. For example, the Large Hadron Collider uses 600 tons of superconducting strands, while the International Thermonuclear Experimental Reactor uses an estimated 600 tonnes of NbSn strands and 250 tonnes of NbTi strands. In 1992 alone, more than US$1 billion worth of clinical magnetic resonance imaging systems were constructed with niobium-titanium wire.

The superconducting radio frequency (SRF) cavities used in the free-electron lasers FLASH (result of the cancelled TESLA linear accelerator project) and XFEL are made from pure niobium. A cryomodule team at Fermilab used the same SRF technology from the FLASH project to develop 1.3 GHz nine-cell SRF cavities made from pure niobium. The cavities will be used in the linear particle accelerator of the International Linear Collider. The same technology will be used in LCLS-II at SLAC National Accelerator Laboratory and PIP-II at Fermilab.

The high sensitivity of superconducting niobium nitride bolometers make them an ideal detector for electromagnetic radiation in the THz frequency band. These detectors were tested at the Submillimeter Telescope, the South Pole Telescope, the Receiver Lab Telescope, and at APEX, and are now used in the HIFI instrument on board the Herschel Space Observatory.

Lithium niobate, which is a ferroelectric, is used extensively in mobile telephones and optical modulators, and for the manufacture of surface acoustic wave devices. It belongs to the ABO structure ferroelectrics like lithium tantalate and barium titanate. Niobium capacitors are available as alternative to tantalum capacitors, but tantalum capacitors still predominate. Niobium is added to glass to obtain a higher refractive index, making possible thinner and lighter corrective glasses.

Niobium and some niobium alloys are physiologically inert and hypoallergenic. For this reason, niobium is used in prosthetics and implant devices, such as pacemakers. Niobium treated with sodium hydroxide forms a porous layer that aids osseointegration.

Like titanium, tantalum, and aluminium, niobium can be heated and anodized ("reactive metal anodization") to produce a wide array of iridescent colours for jewelry, where its hypoallergenic property is highly desirable.

Niobium is used as a precious metal in commemorative coins, often with silver or gold. For example, Austria produced a series of silver niobium euro coins starting in 2003; the colour in these coins is created by the diffraction of light by a thin anodized oxide layer. In 2012, ten coins are available showing a broad variety of colours in the centre of the coin: blue, green, brown, purple, violet, or yellow. Two more examples are the 2004 Austrian €25 150 Years Semmering Alpine Railway commemorative coin, and the 2006 Austrian €25 European Satellite Navigation commemorative coin.
The Austrian mint produced for Latvia a similar series of coins starting in 2004,
with one following in 2007.
In 2011, the Royal Canadian Mint started production of a $5 sterling silver and niobium coin named "Hunter's Moon"
in which the niobium was selectively oxidized, thus creating unique finishes where no two coins are exactly alike.

The arc-tube seals of high pressure sodium vapor lamps are made from niobium, sometimes alloyed with 1% of zirconium; niobium has a very similar coefficient of thermal expansion, matching the sintered alumina arc tube ceramic, a translucent material which resists chemical attack or reduction by the hot liquid sodium and sodium vapour contained inside the operating lamp.

Niobium is used in arc welding rods for some stabilized grades of stainless steel and in anodes for cathodic protection systems on some water tanks, which are then usually plated with platinum.

Niobium is an important component of high-performance heterogeneous catalysts for the production of acrylic acid by selective oxidation of propane.

Niobium is used to make the high voltage wire of the solar corona particles receptor module of the Parker Solar Probe.

Niobium has no known biological role. While niobium dust is an eye and skin irritant and a potential fire hazard, elemental niobium on a larger scale is physiologically inert (and thus hypoallergenic) and harmless. It is frequently used in jewelry and has been tested for use in some medical implants.

Niobium-containing compounds are rarely encountered by most people, but some are toxic and should be treated with care. The short- and long-term exposure to niobates and niobium chloride, two chemicals that are water-soluble, have been tested in rats. Rats treated with a single injection of niobium pentachloride or niobates show a median lethal dose (LD) between 10 and 100 mg/kg. For oral administration the toxicity is lower; a study with rats yielded a LD after seven days of 940 mg/kg.



</doc>
<doc id="21276" url="https://en.wikipedia.org/wiki?curid=21276" title="Neodymium">
Neodymium

Neodymium is a chemical element with the symbol Nd and atomic number 60. Neodymium belongs to the lanthanide series and is a rare-earth element. It is a hard, slightly malleable silvery metal that quickly tarnishes in air and moisture. When oxidized, neodymium reacts quickly to produce pink, purple/blue and yellow compounds in the +2, +3 and +4 oxidation states. Neodymium was discovered in 1885 by the Austrian chemist Carl Auer von Welsbach. It is present in significant quantities in the ore minerals monazite and bastnäsite. Neodymium is not found naturally in metallic form or unmixed with other lanthanides, and it is usually refined for general use. Although neodymium is classed as a rare-earth element, it is fairly common, no rarer than cobalt, nickel, or copper, and is widely distributed in the Earth's crust. Most of the world's commercial neodymium is mined in China.

Neodymium compounds were first commercially used as glass dyes in 1927, and they remain a popular additive in glasses. The color of neodymium compounds is due to the Nd ion and is often a reddish-purple, but it changes with the type of lighting, because of the interaction of the sharp light absorption bands of neodymium with ambient light enriched with the sharp visible emission bands of mercury, trivalent europium or terbium. Some neodymium-doped glasses are used in lasers that emit infrared with wavelengths between 1047 and 1062 nanometers. These have been used in extremely-high-power applications, such as experiments in inertial confinement fusion. Neodymium is also used with various other substrate crystals, such as yttrium aluminium garnet in the .

Another important use of neodymium is as a component in the alloys used to make high-strength neodymium magnets—powerful permanent magnets. These magnets are widely used in such products as microphones, professional loudspeakers, in-ear headphones, high performance hobby DC electric motors, and computer hard disks, where low magnet mass (or volume) or strong magnetic fields are required. Larger neodymium magnets are used in high-power-versus-weight electric motors (for example in hybrid cars) and generators (for example aircraft and wind turbine electric generators).

Neodymium, a rare-earth metal, was present in the classical mischmetal at a concentration of about 18%. Metallic neodymium has a bright, silvery metallic luster. Neodymium commonly exists in two allotropic forms, with a transformation from a double hexagonal to a body-centered cubic structure taking place at about 863 °C. Neodymium is paramagnetic at room temperature and becomes an antiferromagnet upon cooling to . In order to make the neodymium magnets it is alloyed with iron, which is a ferromagnet.

Neodymium metal quickly oxidizes at ambient conditions and readily burns at about 150 °C to form neodymium(III) oxide; the oxide peels off, exposing the bulk metal to the further oxidation:

Neodymium is a quite electropositive element, and it reacts slowly with cold water but quite quickly with hot water to form neodymium(III) hydroxide:

Neodymium metal reacts vigorously with all the halogens:

Neodymium dissolves readily in dilute sulfuric acid to form solutions that contain the lilac Nd(III) ion. These exist as a [Nd(OH)] complexes:

Neodymium compounds include


Some neodymium compounds have colors that vary based upon the type of lighting.

Naturally occurring neodymium is a mixture of five stable isotopes, Nd, Nd, Nd, Nd and Nd, with Nd being the most abundant (27.2% of the natural abundance), and two radioisotopes, Nd and Nd. In all, 31 radioisotopes of neodymium have been detected , with the most stable radioisotopes being the naturally occurring ones: Nd (alpha decay with a half-life ("t") of 2.29×10 years) and Nd (double beta decay, "t" = 7×10 years, approximately). All of the remaining radioactive isotopes have half-lives that are shorter than eleven days, and the majority of these have half-lives that are shorter than 70 seconds. Neodymium also has 13 known meta states, with the most stable one being Nd ("t" = 5.5 hours), Nd ("t" = 5.5 minutes) and Nd ("t" ~70 seconds).

The primary decay modes before the most abundant stable isotope, Nd, are electron capture and positron decay, and the primary mode after is beta minus decay. The primary decay products before Nd are element Pr (praseodymium) isotopes and the primary products after are element Pm (promethium) isotopes.

Neodymium was discovered by Austrian chemist Carl Auer von Welsbach in Vienna in 1885. He separated neodymium, as well as the element praseodymium, from their mixture, called didymium, by means of fractional crystallization of the double ammonium nitrate tetrahydrates from nitric acid. Von Welsbach confirmed the separation by spectroscopic analysis, but the products were of relatively low purity. Didymium was discovered by Carl Gustaf Mosander in 1841, and pure neodymium was isolated from it in 1925. The name neodymium is derived from the Greek words "neos" (νέος), new, and "didymos" (διδύμος), twin.

Double nitrate crystallization was the means of commercial neodymium purification until the 1950s. Lindsay Chemical Division was the first to commercialize large-scale ion-exchange purification of neodymium. Starting in the 1950s, high purity (above 99%) neodymium was primarily obtained through an ion exchange process from monazite, a mineral rich in rare-earth elements. The metal is obtained through electrolysis of its halide salts. Currently, most neodymium is extracted from bastnäsite, (Ce,La,Nd,Pr)COF, and purified by solvent extraction. Ion-exchange purification is reserved for preparing the highest purities (typically >99.99%). The evolving technology, and improved purity of commercially available neodymium oxide, was reflected in the appearance of neodymium glass that resides in collections today. Early neodymium glasses made in the 1930s have a more reddish or orange tinge than modern versions which are more cleanly purple, because of the difficulties in removing the last traces of praseodymium in the era when manufacturing relied upon fractional crystallization technology.

Because of its role in permanent magnets used for direct-drive wind turbines, it has been argued that neodymium will be one of the main objects of geopolitical competition in a world running on renewable energy. This perspective has been criticised for failing to recognise that most wind turbines do not use permanent magnets, and for underestimating the power of economic incentives for expanded production.

Neodymium is rarely found in nature as a free element, but rather it occurs in ores such as monazite and bastnäsite (these are mineral group names rather than single mineral names) that contain small amounts of all rare-earth metals. In these minerals neodymium is rarely dominant (as in the case of lanthanum), with cerium being the most abundant lanthanide; some exceptions include monazite-(Nd) and kozoite-(Nd). The main mining areas are in China, United States, Brazil, India, Sri Lanka, and Australia. The reserves of neodymium are estimated at about eight million tonnes. Although it belongs to the rare-earth metals, neodymium is not rare at all. Its abundance in the Earth's crust is about 38 mg/kg, which is the second highest among rare-earth elements, following cerium. The world's production of neodymium was about 7,000 tonnes in 2004. The bulk of current production is from China. Historically, Chinese government has imposed strategic material controls on the element, causing large fluctuations in prices. The uncertainty of pricing and availability have caused companies (particularly Japanese ones) to create permanent magnets and associated electric motors with fewer rare-earth metals; however, so far they have been unable to eliminate the need for neodymium.

Neodymium is typically 10–18% of the rare-earth content of commercial deposits of the light rare-earth-element minerals bastnäsite and monazite. With neodymium compounds being the most strongly colored for the trivalent lanthanides, it can occasionally dominate the coloration of rare-earth minerals when competing chromophores are absent. It usually gives a pink coloration. Outstanding examples of this include monazite crystals from the tin deposits in Llallagua, Bolivia; ancylite from Mont Saint-Hilaire, Quebec, Canada; or lanthanite from the Saucon Valley, Pennsylvania, United States. As with neodymium glasses, such minerals change their colors under the differing lighting conditions. The absorption bands of neodymium interact with the visible emission spectrum of mercury vapor, with the unfiltered shortwave UV light causing neodymium-containing minerals to reflect a distinctive green color. This can be observed with monazite-containing sands or bastnäsite-containing ore.


Neodymium magnets (actually an alloy, NdFeB) are the strongest permanent magnets known. A neodymium magnet of a few grams can lift a thousand times its own weight. These magnets are cheaper, lighter, and stronger than samarium–cobalt magnets. However, they are not superior in every aspect, as neodymium-based magnets lose their magnetism at lower temperatures and tend to corrode, while samarium–cobalt magnets do not.

Neodymium magnets appear in products such as microphones, professional loudspeakers, in-ear headphones, guitar and bass guitar pick-ups, and computer hard disks where low mass, small volume, or strong magnetic fields are required. Neodymium is used in the electric motors of hybrid and electric automobiles and in the electricity generators of some designs of commercial wind turbines (only wind turbines with "permanent magnet" generators use neodymium). For example, drive electric motors of each Toyota Prius require one kilogram (2.2 pounds) of neodymium per vehicle.

In 2020, physics researchers at Radboud University and Uppsala University announced they had observed a behavior known as "self-induced spin glass" in the atomic structure of neodymium. One of the researchers explained, "…we are specialists in scanning tunneling microscopy. It allows us to see the structure of individual atoms, and we can resolve the north and south poles of the atoms. With this advancement in high-precision imaging, we were able to discover the behavior in neodymium, because we could resolve the incredibly small changes in the magnetic structure." Neodymium behaves in a complex magnetic way that had not been seen before in a periodic table element.

Certain transparent materials with a small concentration of neodymium ions can be used in lasers as gain media for infrared wavelengths (1054–1064 nm), e.g. (yttrium aluminium garnet), Nd:YLF (yttrium lithium fluoride), Nd:YVO (yttrium orthovanadate), and Nd:glass. Neodymium-doped crystals (typically Nd:YVO) generate high-powered infrared laser beams which are converted to green laser light in commercial DPSS hand-held lasers and laser pointers.

The current laser at the UK Atomic Weapons Establishment (AWE), the HELEN (High Energy Laser Embodying Neodymium) 1-terawatt neodymium-glass laser, can access the midpoints of pressure and temperature regions and is used to acquire data for modeling on how density, temperature, and pressure interact inside warheads. HELEN can create plasmas of around 10 K, from which opacity and transmission of radiation are measured.

Neodymium glass solid-state lasers are used in extremely high power (terawatt scale), high energy (megajoules) multiple beam systems for inertial confinement fusion. Nd:glass lasers are usually frequency tripled to the third harmonic at 351 nm in laser fusion devices.

Neodymium glass (Nd:glass) is produced by the inclusion of neodymium oxide (NdO) in the glass melt. Usually in daylight or incandescent light neodymium glass appears lavender, but it appears pale blue under fluorescent lighting. Neodymium may be used to color glass in delicate shades ranging from pure violet through wine-red and warm gray.

The first commercial use of purified neodymium was in glass coloration, starting with experiments by Leo Moser in November 1927. The resulting "Alexandrite" glass remains a signature color of the Moser glassworks to this day. Neodymium glass was widely emulated in the early 1930s by American glasshouses, most notably Heisey, Fostoria ("wisteria"), Cambridge ("heatherbloom"), and Steuben ("wisteria"), and elsewhere (e.g. Lalique, in France, or Murano). Tiffin's "twilight" remained in production from about 1950 to 1980. Current sources include glassmakers in the Czech Republic, the United States, and China.

The sharp absorption bands of neodymium cause the glass color to change under different lighting conditions, being reddish-purple under daylight or yellow incandescent light, but blue under white fluorescent lighting, or greenish under trichromatic lighting. This color-change phenomenon is highly prized by collectors. In combination with gold or selenium, red colors are produced. Since neodymium coloration depends upon "forbidden" f-f transitions deep within the atom, there is relatively little influence on the color from the chemical environment, so the color is impervious to the thermal history of the glass. However, for the best color, iron-containing impurities need to be minimized in the silica used to make the glass. The same forbidden nature of the f-f transitions makes rare-earth colorants less intense than those provided by most d-transition elements, so more has to be used in a glass to achieve the desired color intensity. The original Moser recipe used about 5% of neodymium oxide in the glass melt, a sufficient quantity such that Moser referred to these as being "rare-earth doped" glasses. Being a strong base, that level of neodymium would have affected the melting properties of the glass, and the lime content of the glass might have had to be adjusted accordingly.

Light transmitted through neodymium glasses shows unusually sharp absorption bands; the glass is used in astronomical work to produce sharp bands by which spectral lines may be calibrated. Another application is the creation of selective astronomical filters to reduce the effect of light pollution from sodium and fluorescent lighting while passing other colours, especially dark red hydrogen-alpha emission from nebulae. Neodymium is also used to remove the green color caused by iron contaminants from glass.

Neodymium is a component of "didymium" (referring to mixture of salts of neodymium and praseodymium) used for coloring glass to make welder's and glass-blower's goggles; the sharp absorption bands obliterate the strong sodium emission at 589 nm. The similar absorption of the yellow mercury emission line at 578 nm is the principal cause of the blue color observed for neodymium glass under traditional white-fluorescent lighting. Neodymium and didymium glass are used in color-enhancing filters in indoor photography, particularly in filtering out the yellow hues from incandescent lighting. Similarly, neodymium glass is becoming widely used more directly in incandescent light bulbs. These lamps contain neodymium in the glass to filter out yellow light, resulting in a whiter light which is more like sunlight. Similar to its use in glasses, neodymium salts are used as a colorant for enamels.

Neodymium metal dust is combustible and therefore an explosion hazard. Neodymium compounds, as with all rare-earth metals, are of low to moderate toxicity; however, its toxicity has not been thoroughly investigated. Neodymium dust and salts are very irritating to the eyes and mucous membranes, and moderately irritating to skin. Breathing the dust can cause lung embolisms, and accumulated exposure damages the liver. Neodymium also acts as an anticoagulant, especially when given intravenously.

Neodymium magnets have been tested for medical uses such as magnetic braces and bone repair, but biocompatibility issues have prevented widespread application. Commercially available magnets made from neodymium are exceptionally strong and can attract each other from large distances. If not handled carefully, they come together very quickly and forcefully, causing injuries. For example, there is at least one documented case of a person losing a fingertip when two magnets he was using snapped together from 50 cm away.

Another risk of these powerful magnets is that if more than one magnet is ingested, they can pinch soft tissues in the gastrointestinal tract. This has led to at least 1,700 emergency room visits and necessitated the recall of the Buckyballs line of toys, which were construction sets of small neodymium magnets.



</doc>
<doc id="21277" url="https://en.wikipedia.org/wiki?curid=21277" title="Neptunium">
Neptunium

Neptunium is a chemical element with the symbol Np and atomic number 93. A radioactive actinide metal, neptunium is the first transuranic element. Its position in the periodic table just after uranium, named after the planet Uranus, led to it being named after Neptune, the next planet beyond Uranus. A neptunium atom has 93 protons and 93 electrons, of which seven are valence electrons. Neptunium metal is silvery and tarnishes when exposed to air. The element occurs in three allotropic forms and it normally exhibits five oxidation states, ranging from +3 to +7. It is radioactive, poisonous, pyrophoric, and capable of accumulating in bones, which makes the handling of neptunium dangerous.

Although many false claims of its discovery were made over the years, the element was first synthesized by Edwin McMillan and Philip H. Abelson at the Berkeley Radiation Laboratory in 1940. Since then, most neptunium has been and still is produced by neutron irradiation of uranium in nuclear reactors. The vast majority is generated as a by-product in conventional nuclear power reactors. While neptunium itself has no commercial uses at present, it is used as a precursor for the formation of plutonium-238, used in radioisotope thermal generators to provide electricity for spacecraft. Neptunium has also been used in detectors of high-energy neutrons.

The longest-lived isotope of neptunium, neptunium-237, is a by-product of nuclear reactors and plutonium production. It, and the isotope neptunium-239, are also found in trace amounts in uranium ores due to neutron capture reactions and beta decay.

Neptunium is a hard, silvery, ductile, radioactive actinide metal. In the periodic table, it is located to the right of the actinide uranium, to the left of the actinide plutonium and below the lanthanide promethium. Neptunium is a hard metal, having a bulk modulus of 118 GPa, comparable to that of manganese. Neptunium metal is similar to uranium in terms of physical workability. When exposed to air at normal temperatures, it forms a thin oxide layer. This reaction proceeds more rapidly as the temperature increases. Neptunium has been determined to melt at 639±3 °C: this low melting point, a property the metal shares with the neighboring element plutonium (which has melting point 639.4 °C), is due to the hybridization of the 5f and 6d orbitals and the formation of directional bonds in the metal. The boiling point of neptunium is not empirically known and the usually given value of 4174 °C is extrapolated from the vapor pressure of the element. If accurate, this would give neptunium the largest liquid range of any element (3535 K passes between its melting and boiling points).

Neptunium is found in at least three allotropes. Some claims of a fourth allotrope have been made, but they are so far not proven. This multiplicity of allotropes is common among the actinides. The crystal structures of neptunium, protactinium, uranium, and plutonium do not have clear analogs among the lanthanides and are more similar to those of the 3d transition metals.

α-neptunium takes on an orthorhombic structure, resembling a highly distorted body-centered cubic structure. Each neptunium atom is coordinated to four others and the Np–Np bond lengths are 260 pm. It is the densest of all the actinides and the fifth-densest of all naturally occurring elements, behind only rhenium, platinum, iridium, and osmium. α-neptunium has semimetallic properties, such as strong covalent bonding and a high electrical resistivity, and its metallic physical properties are closer to those of the metalloids than the true metals. Some allotropes of the other actinides also exhibit similar behaviour, though to a lesser degree. The densities of different isotopes of neptunium in the alpha phase are expected to be observably different: α-Np should have density 20.303 g/cm; α-Np, density 20.389 g/cm; α-Np, density 20.476 g/cm.

β-neptunium takes on a distorted tetragonal close-packed structure. Four atoms of neptunium make up a unit cell, and the Np–Np bond lengths are 276 pm. γ-neptunium has a body-centered cubic structure and has Np–Np bond length of 297 pm. The γ form becomes less stable with increased pressure, though the melting point of neptunium also increases with pressure. The β-Np/γ-Np/liquid triple point occurs at 725 °C and 3200 MPa.

Due to the presence of valence 5f electrons, neptunium and its alloys exhibit very interesting magnetic behavior, like many other actinides. These can range from the itinerant band-like character characteristic of the transition metals to the local moment behavior typical of scandium, yttrium, and the lanthanides. This stems from 5f-orbital hybridization with the orbitals of the metal ligands, and the fact that the 5f orbital is relativistically destabilized and extends outwards. For example, pure neptunium is paramagnetic, NpAl is ferromagnetic, NpGe has no magnetic ordering, and NpSn behaves fermionically. Investigations are underway regarding alloys of neptunium with uranium, americium, plutonium, zirconium, and iron, so as to recycle long-lived waste isotopes such as neptunium-237 into shorter-lived isotopes more useful as nuclear fuel.

One neptunium-based superconductor alloy has been discovered with formula NpPdAl. This occurrence in neptunium compounds is somewhat surprising because they often exhibit strong magnetism, which usually destroys superconductivity. The alloy has a tetragonal structure with a superconductivity transition temperature of −268.3 °C (4.9 K).

Neptunium has five ionic oxidation states ranging from +3 to +7 when forming chemical compounds, which can be simultaneously observed in solutions. It is the heaviest actinide that can lose all its valence electrons in a stable compound. The most stable state in solution is +5, but the valence +4 is preferred in solid neptunium compounds. Neptunium metal is very reactive. Ions of neptunium are prone to hydrolysis and formation of coordination compounds.

A neptunium atom has 93 electrons, arranged in the configuration <nowiki>[</nowiki>Rn<nowiki>]</nowiki>5f6d7s. This differs from the configuration expected by the Aufbau principle in that one electron is in the 6d subshell instead of being as expected in the 5f subshell. This is because of the similarity of the electron energies of the 5f, 6d, and 7s subshells. In forming compounds and ions, all the valence electrons may be lost, leaving behind an inert core of inner electrons with the electron configuration of the noble gas radon; more commonly, only some of the valence electrons will be lost. The electron configuration for the tripositive ion Np is [Rn] 5f, with the outermost 7s and 6d electrons lost first: this is exactly analogous to neptunium's lanthanide homolog promethium, and conforms to the trend set by the other actinides with their [Rn] 5f electron configurations in the tripositive state. The first ionization potential of neptunium was measured to be at most in 1974, based on the assumption that the 7s electrons would ionize before 5f and 6d; more recent measurements have refined this to 6.2657 eV.

24 neptunium radioisotopes have been characterized with the most stable being Np with a half-life of 2.14 million years, Np with a half-life of 154,000 years, and Np with a half-life of 396.1 days. All of the remaining radioactive isotopes have half-lives that are less than 4.5 days, and the majority of these have half-lives that are less than 50 minutes. This element also has at least four meta states, with the most stable being Np with a half-life of 22.5 hours.

The isotopes of neptunium range in atomic weight from 219.032 u (Np) to 244.068 u (Np), though Np and Np have not yet been reported. Most of the isotopes that are lighter than the most stable one, Np, decay primarily by electron capture although a sizable number, most notably Np and Np, also exhibit various levels of decay via alpha emission to become protactinium. Np itself, being the beta-stable isobar of mass number 237, decays almost exclusively by alpha emission into Pa, with very rare (occurring only about once in trillions of decays) spontaneous fission and cluster decay (emission of Mg to form Tl). All of the known isotopes except one that are heavier than this decay exclusively via beta emission. The lone exception, Np, exhibits a rare (>0.12%) decay by isomeric transition in addition to the beta emission. Np eventually decays to form bismuth-209 and thallium-205, unlike most other common heavy nuclei which decay into isotopes of lead. This decay chain is known as the neptunium series. This decay chain had long been extinct on Earth due to the short half-lives of all of its isotopes above bismuth-209, but is now being resurrected thanks to artificial production of neptunium on the tonne scale.

The isotopes neptunium-235, -236, and -237 are predicted to be fissile; only neptunium-237's fissionability has been experimentally shown, with the critical mass being about 60 kg, only about 10 kg more than that of the commonly used uranium-235. Calculated values of the critical masses of neptunium-235, -236, and -237 respectively are 66.2 kg, 6.79 kg, and 63.6 kg: the neptunium-236 value is even lower than that of plutonium-239. In particular Np also has a low neutron cross section. Despite this, a neptunium atomic bomb has never been built: uranium and plutonium have lower critical masses than Np and Np, and Np is difficult to purify as it is not found in quantity in spent nuclear fuel and is nearly impossible to separate in any significant quantities from its parent Np.

Since all isotopes of neptunium have half-lives that are many times shorter than the age of the Earth, any primordial neptunium should have decayed by now. After only about 80 million years, the concentration of even the longest lived isotope, Np, would have been reduced to less than one-trillionth (10) of its original amount; and even if the whole Earth had initially been made of pure Np (and ignoring that this would be well over its critical mass of 60 kg), 2100 half-lives would have passed since the formation of the Solar System, and thus all of it would have decayed. Thus neptunium is present in nature only in negligible amounts produced as intermediate decay products of other isotopes.

Trace amounts of the neptunium isotopes neptunium-237 and -239 are found naturally as decay products from transmutation reactions in uranium ores. In particular, Np and Np are the most common of these isotopes; they are directly formed from neutron capture by uranium-238 atoms. These neutrons come from the spontaneous fission of uranium-238, naturally neutron-induced fission of uranium-235, cosmic ray spallation of nuclei, and light elements absorbing alpha particles and emitting a neutron. The half-life of Np is very short, although the detection of its much longer-lived daughter Pu in nature in 1951 definitively established its natural occurrence. In 1952, Np was identified and isolated from concentrates of uranium ore from the Belgian Congo: in these minerals, the ratio of neptunium-237 to uranium is less than or equal to about 10 to 1.

Most neptunium (and plutonium) now encountered in the environment is due to atmospheric nuclear explosions that took place between the detonation of the first atomic bomb in 1945 and the ratification of the Partial Nuclear Test Ban Treaty in 1963. The total amount of neptunium released by these explosions and the few atmospheric tests that have been carried out since 1963 is estimated to be around 2500 kg. The overwhelming majority of this is composed of the long-lived isotopes Np and Np since even the moderately long-lived Np (half-life 396 days) would have decayed to less than one-billionth (10) its original concentration over the intervening decades. An additional very small amount of neptunium, created by neutron irradiation of natural uranium in nuclear reactor cooling water, is released when the water is discharged into rivers or lakes. The concentration of Np in seawater is approximately 6.5 × 10 millibecquerels per liter: this concentration is between 0.1% and 1% that of plutonium.

Once in the environment, neptunium generally oxidizes fairly quickly, usually to the +4 or +5 state. Regardless of its oxidation state, the element exhibits a much greater mobility than the other actinides, largely due to its ability to readily form aqueous solutions with various other elements. In one study comparing the diffusion rates of neptunium(V), plutonium(IV), and americium(III) in sandstone and limestone, neptunium penetrated more than ten times as well as the other elements. Np(V) will also react efficiently in pH levels greater than 5.5 if there are no carbonates present and in these conditions it has also been observed to readily bond with quartz. It has also been observed to bond well with goethite, ferric oxide colloids, and several clays including kaolinite and smectite. Np(V) does not bond as readily to soil particles in mildly acidic conditions as its fellow actinides americium and curium by nearly an order of magnitude. This behavior enables it to migrate rapidly through the soil while in solution without becoming fixed in place, contributing further to its mobility. Np(V) is also readily absorbed by concrete, which because of the element's radioactivity is a consideration that must be addressed when building nuclear waste storage facilities. When absorbed in concrete, it is reduced to Np(IV) in a relatively short period of time. Np(V) is also reduced by humic acid if it is present on the surface of goethite, hematite, and magnetite. Np(IV) is absorbed efficiently by tuff, granodiorite, and bentonite; although uptake by the latter is most pronounced in mildly acidic conditions. It also exhibits a strong tendency to bind to colloidal particulates, an effect that is enhanced when in soil with a high clay content. The behavior provides an additional aid in the element's observed high mobility.

When the first periodic table of the elements was published by Dmitri Mendeleev in the early 1870s, it showed a " — " in place after uranium similar to several other places for then-undiscovered elements. Other subsequent tables of known elements, including a 1913 publication of the known radioactive isotopes by Kasimir Fajans, also show an empty place after uranium, element 92.

Up to and after the discovery of the final component of the atomic nucleus, the neutron in 1932, most scientists did not seriously consider the possibility of elements heavier than uranium. While nuclear theory at the time did not explicitly prohibit their existence, there was little evidence to suggest that they did. However, the discovery of induced radioactivity by Irène and Frédéric Joliot-Curie in late 1933 opened up an entirely new method of researching the elements and inspired a small group of Italian scientists led by Enrico Fermi to begin a series of experiments involving neutron bombardment. Although the Joliot-Curies' experiment involved bombarding a sample of Al with alpha particles to produce the radioactive P, Fermi realized that using neutrons, which have no electrical charge, would most likely produce even better results than the positively charged alpha particles. Accordingly, in March 1934 he began systematically subjecting all of the then-known elements to neutron bombardment to determine whether others could also be induced to radioactivity.

After several months of work, Fermi's group had tentatively determined that lighter elements would disperse the energy of the captured neutron by emitting a proton or alpha particle and heavier elements would generally accomplish the same by emitting a gamma ray. This latter behavior would later result in the beta decay of a neutron into a proton, thus moving the resulting isotope one place up the periodic table. When Fermi's team bombarded uranium, they observed this behavior as well, which strongly suggested that the resulting isotope had an atomic number of 93. Fermi was initially reluctant to publicize such a claim, but after his team observed several unknown half-lives in the uranium bombardment products that did not match those of any known isotope, he published a paper entitled "Possible Production of Elements of Atomic Number Higher than 92" in June 1934. In it he proposed the name ausonium (atomic symbol Ao) for element 93, after the Greek name "Ausonia" (Italy).

Several theoretical objections to the claims of Fermi's paper were quickly raised; in particular, the exact process that took place when an atom captured a neutron was not well understood at the time. This and Fermi's accidental discovery three months later that nuclear reactions could be induced by slow neutrons cast further doubt in the minds of many scientists, notably Aristid von Grosse and Ida Noddack, that the experiment was creating element 93. While von Grosse's claim that Fermi was actually producing protactinium (element 91) was quickly tested and disproved, Noddack's proposal that the uranium had been shattered into two or more much smaller fragments was simply ignored by most because existing nuclear theory did not include a way for this to be possible. Fermi and his team maintained that they were in fact synthesizing a new element, but the issue remained unresolved for several years.
Although the many different and unknown radioactive half-lives in the experiment's results showed that several nuclear reactions were occurring, Fermi's group could not prove that element 93 was being created unless they could isolate it chemically. They and many other scientists attempted to accomplish this, including Otto Hahn and Lise Meitner who were among the best radiochemists in the world at the time and supporters of Fermi's claim, but they all failed. Much later, it was determined that the main reason for this failure was because the predictions of element 93's chemical properties were based on a periodic table which lacked the actinide series. This arrangement placed protactinium below tantalum, uranium below tungsten, and further suggested that element 93, at that point referred to as eka-rhenium, should be similar to the group 7 elements, including manganese and rhenium. Thorium, protactinium, and uranium, with their dominant oxidation states of +4, +5, and +6 respectively, fooled scientists into thinking they belonged below hafnium, tantalum, and tungsten, rather than below the lanthanide series, which was at the time viewed as a fluke, and whose members all have dominant +3 states; neptunium, on the other hand, has a much rarer, more unstable +7 state, with +4 and +5 being the most stable. Upon finding that plutonium and the other transuranic elements also have dominant +3 and +4 states, along with the discovery of the f-block, the actinide series was firmly established.

While the question of whether Fermi's experiment had produced element 93 was stalemated, two additional claims of the discovery of the element appeared, although unlike Fermi, they both claimed to have observed it in nature. The first of these claims was by Czech engineer Odolen Koblic in 1934 when he extracted a small amount of material from the wash water of heated pitchblende. He proposed the name bohemium for the element, but after being analyzed it turned out that the sample was a mixture of tungsten and vanadium. The other claim, in 1938 by Romanian physicist Horia Hulubei and French chemist Yvette Cauchois, claimed to have discovered the new element via spectroscopy in minerals. They named their element sequanium, but the claim was discounted because the prevailing theory at the time was that if it existed at all, element 93 would not exist naturally. However, as neptunium does in fact occur in nature in trace amounts, as demonstrated when it was found in uranium ore in 1952, it is possible that Hulubei and Cauchois did in fact observe neptunium.

Although by 1938 some scientists, including Niels Bohr, were still reluctant to accept that Fermi had actually produced a new element, he was nevertheless awarded the Nobel Prize in Physics in November 1938 ""for his demonstrations of the existence of new radioactive elements produced by neutron irradiation, and for his related discovery of nuclear reactions brought about by slow neutrons"". A month later, the almost totally unexpected discovery of nuclear fission by Hahn, Meitner, and Otto Frisch put an end to the possibility that Fermi had discovered element 93 because most of the unknown half-lives that had been observed by Fermi's team were rapidly identified as those of fission products.

Perhaps the closest of all attempts to produce the missing element 93 was that conducted by the Japanese physicist Yoshio Nishina working with chemist Kenjiro Kimura in 1940, just before the outbreak of the Pacific War in 1941: they bombarded U with fast neutrons. However, while slow neutrons tend to induce neutron capture through a (n, γ) reaction, fast neutrons tend to induce a "knock-out" (n, 2n) reaction, where one neutron is added and two more are removed, resulting in the net loss of a neutron. Nishina and Kimura, having tested this technique on Th and successfully produced the known Th and its long-lived beta decay daughter Pa (both occurring in the natural decay chain of U), therefore correctly assigned the new 6.75-day half-life activity they observed to the new isotope U. They confirmed that this isotope was also a beta emitter and must hence decay to the unknown nuclide 93. They attempted to isolate this nuclide by carrying it with its supposed lighter congener rhenium, but no beta or alpha decay was observed from the rhenium-containing fraction: Nishina and Kimura thus correctly speculated that the half-life of 93, like that of Pa, was very long and hence its activity would be so weak as to be unmeasurable by their equipment, thus concluding the last and closest unsuccessful search for transuranic elements.

As research on nuclear fission progressed in early 1939, Edwin McMillan at the Berkeley Radiation Laboratory of the University of California, Berkeley decided to run an experiment bombarding uranium using the powerful 60-inch (1.52 m) cyclotron that had recently been built at the university. The purpose was to separate the various fission products produced by the bombardment by exploiting the enormous force that the fragments gain from their mutual electrical repulsion after fissioning. Although he did not discover anything of note from this, McMillan did observe two new beta decay half-lives in the uranium trioxide target itself, which meant that whatever was producing the radioactivity had not violently repelled each other like normal fission products. He quickly realized that one of the half-lives closely matched the known 23-minute decay period of uranium-239, but the other half-life of 2.3 days was unknown. McMillan took the results of his experiment to chemist and fellow Berkeley professor Emilio Segrè to attempt to isolate the source of the radioactivity. Both scientists began their work using the prevailing theory that element 93 would have similar chemistry to rhenium, but Segrè rapidly determined that McMillan's sample was not at all similar to rhenium. Instead, when he reacted it with hydrogen fluoride (HF) with a strong oxidizing agent present, it behaved much like members of the rare earths. Since these elements comprise a large percentage of fission products, Segrè and McMillan decided that the half-life must have been simply another fission product, titling the paper "An Unsuccessful Search for Transuranium Elements".
However, as more information about fission became available, the possibility that the fragments of nuclear fission could still have been present in the target became more remote. McMillan and several scientists, including Philip H. Abelson, attempted again to determine what was producing the unknown half-life. In early 1940, McMillan realized that his 1939 experiment with Segrè had failed to test the chemical reactions of the radioactive source with sufficient rigor. In a new experiment, McMillan tried subjecting the unknown substance to HF in the presence of a reducing agent, something he had not done before. This reaction resulted in the sample precipitating with the HF, an action that definitively ruled out the possibility that the unknown substance was a rare earth. 
Shortly after this, Abelson, who had received his graduate degree from the university, visited Berkeley for a short vacation and McMillan asked the more able chemist to assist with the separation of the experiment's results. Abelson very quickly observed that whatever was producing the 2.3-day half-life did not have chemistry like any known element and was actually more similar to uranium than a rare earth. This discovery finally allowed the source to be isolated and later, in 1945, led to the classification of the actinide series. As a final step, McMillan and Abelson prepared a much larger sample of bombarded uranium that had a prominent 23-minute half-life from U and demonstrated conclusively that the unknown 2.3-day half-life increased in strength in concert with a decrease in the 23-minute activity through the following reaction:

This proved that the unknown radioactive source originated from the decay of uranium and, coupled with the previous observation that the source was different chemically from all known elements, proved beyond all doubt that a new element had been discovered. McMillan and Abelson published their results in a paper entitled "Radioactive Element 93" in the "Physical Review" on May 27, 1940. They did not propose a name for the element in the paper, but they soon decided on the name "neptunium" since Neptune is the next planet beyond Uranus in our solar system. McMillan and Abelson's success compared to Nishina and Kimura's near miss can be attributed to the favorable half-life of Np for radiochemical analysis and quick decay of U, in contrast to the slower decay of U and extremely long half-life of Np.

It was also realized that the beta decay of Np must produce an isotope of element 94 (now called plutonium), but the quantities involved in McMillan and Abelson's original experiment were too small to isolate and identify plutonium along with neptunium. The discovery of plutonium had to wait until the end of 1940, when Glenn T. Seaborg and his team identified the isotope plutonium-238.

Neptunium's unique radioactive characteristics allowed it to be traced as it moved through various compounds in chemical reactions, at first this was the only method available to prove that its chemistry was different from other elements. As the first isotope of neptunium to be discovered has such a short half-life, McMillan and Abelson were unable to prepare a sample that was large enough to perform chemical analysis of the new element using the technology that was then available. However, after the discovery of the long-lived Np isotope in 1942 by Glenn Seaborg and Arthur Wahl, forming weighable amounts of neptunium became a realistic endeavor. Its half-life was initially determined to be about 3 million years (later revised to 2.144 million years), confirming the predictions of Nishina and Kimura of a very long half-life.

Early research into the element was somewhat limited because most of the nuclear physicists and chemists in the United States at the time were focused on the massive effort to research the properties of plutonium as part of the Manhattan Project. Research into the element did continue as a minor part of the project and the first bulk sample of neptunium was isolated in 1944.

Much of the research into the properties of neptunium since then has been focused on understanding how to confine it as a portion of nuclear waste. Because it has isotopes with very long half-lives, it is of particular concern in the context of designing confinement facilities that can last for thousands of years. It has found some limited uses as a radioactive tracer and a precursor for various nuclear reactions to produce useful plutonium isotopes. However, most of the neptunium that is produced as a reaction byproduct in nuclear power stations is considered to be a waste product.

The vast majority of the neptunium that currently exists on Earth was produced artificially in nuclear reactions. Neptunium-237 is the most commonly synthesized isotope due to it being the only one that both can be created via neutron capture and also has a half-life long enough to allow weighable quantities to be easily isolated. As such, it is by far the most common isotope to be utilized in chemical studies of the element.


Heavier isotopes of neptunium decay quickly, and lighter isotopes of neptunium cannot be produced by neutron capture, so chemical separation of neptunium from cooled spent nuclear fuel gives nearly pure Np. The short-lived heavier isotopes Np and Np, useful as radioactive tracers, are produced through neutron irradiation of Np and U respectively, while the longer-lived lighter isotopes Np and Np are produced through irradiation of U with protons and deuterons in a cyclotron.

Artificial Np metal is usually isolated through a reaction of NpF with liquid barium or lithium at around 1200 °C and is most often extracted from spent nuclear fuel rods in kilogram amounts as a by-product in plutonium production.

By weight, neptunium-237 discharges are about 5% as great as plutonium discharges and about 0.05% of spent nuclear fuel discharges. However, even this fraction still amounts to more than fifty tons per year globally.

Recovering uranium and plutonium from spent nuclear fuel for reuse is one of the major processes of the nuclear fuel cycle. As it has a long half-life of just over 2 million years, the alpha emitter Np is one of the major isotopes of the minor actinides separated from spent nuclear fuel. Many separation methods have been used to separate out the neptunium, operating on small and large scales. The small-scale purification operations have the goals of preparing pure neptunium as a precursor of metallic neptunium and its compounds, and also to isolate and preconcentrate neptunium in samples for analysis.

Most methods that separate neptunium ions exploit the differing chemical behaviour of the differing oxidation states of neptunium (from +3 to +6 or sometimes even +7) in solution. Among the methods that are or have been used are: solvent extraction (using various extractants, usually multidentate β-diketone derivatives, organophosphorus compounds, and amine compounds), chromatography using various ion-exchange or chelating resins, coprecipitation (possible matrices include LaF, BiPO, BaSO, Fe(OH), and MnO), electrodeposition, and biotechnological methods. Currently, commercial reprocessing plants use the Purex process, involving the solvent extraction of uranium and plutonium with tributyl phosphate.

When it is in an aqueous solution, neptunium can exist in any of its five possible oxidation states (+3 to +7) and each of these show a characteristic color. The stability of each oxidation state is strongly dependent on various factors, such as the presence of oxidizing or reducing agents, pH of the solution, presence of coordination complex-forming ligands, and even the concentration of neptunium in the solution.

In acidic solutions, the neptunium(III) to neptunium(VII) ions exist as Np, Np, , , and . In basic solutions, they exist as the oxides and hydroxides Np(OH), NpO, NpOOH, NpO(OH), and . Not as much work has been done to characterize neptunium in basic solutions. Np and Np can easily be reduced and oxidized to each other, as can and .

Np(III) or Np exists as hydrated complexes in acidic solutions, . It is a dark blue-purple and is analogous to its lighter congener, the pink rare-earth ion Pm. In the presence of oxygen, it is quickly oxidized to Np(IV) unless strong reducing agents are also present. Nevertheless, it is the second-least easily hydrolyzed neptunium ion in water, forming the NpOH ion. Np is the predominant neptunium ion in solutions of pH 4–5.

Np(IV) or Np is pale yellow-green in acidic solutions, where it exists as hydrated complexes (). It is quite unstable to hydrolysis in acidic aqueous solutions at pH 1 and above, forming NpOH. In basic solutions, Np tends to hydrolyze to form the neutral neptunium(IV) hydroxide (Np(OH)) and neptunium(IV) oxide (NpO).

Np(V) or is green-blue in aqueous solution, in which it behaves as a strong Lewis acid. It is a stable ion and is the most common form of neptunium in aqueous solutions. Unlike its neighboring homologues and , does not spontaneously disproportionate except at very low pH and high concentration:

It hydrolyzes in basic solutions to form NpOOH and .

Np(VI) or , the neptunyl ion, shows a light pink or reddish color in an acidic solution and yellow-green otherwise. It is a strong Lewis acid and is the main neptunium ion encountered in solutions of pH 3–4. Though stable in acidic solutions, it is quite easily reduced to the Np(V) ion, and it is not as stable as the homologous hexavalent ions of its neighbours uranium and plutonium (the uranyl and plutonyl ions). It hydrolyzes in basic solutions to form the oxo and hydroxo ions NpOOH, , and .

Np(VII) is dark green in a strongly basic solution. Though its chemical formula in basic solution is frequently cited as , this is a simplification and the real structure is probably closer to a hydroxo species like . Np(VII) was first prepared in basic solution in 1967. In strongly acidic solution, Np(VII) is found as ; water quickly reduces this to Np(VI). Its hydrolysis products are uncharacterized.

The oxides and hydroxides of neptunium are closely related to its ions. In general, Np hydroxides at various oxidation levels are less stable than the actinides before it on the periodic table such as thorium and uranium and more stable than those after it such as plutonium and americium. This phenomenon is because the stability of an ion increases as the ratio of atomic number to the radius of the ion increases. Thus actinides higher on the periodic table will more readily undergo hydrolysis.

Neptunium(III) hydroxide is quite stable in acidic solutions and in environments that lack oxygen, but it will rapidly oxidize to the IV state in the presence of air. It is not soluble in water. Np(IV) hydroxides exist mainly as the electrically neutral Np(OH) and its mild solubility in water is not affected at all by the pH of the solution. This suggests that the other Np(IV) hydroxide, , does not have a significant presence.

Because the Np(V) ion is very stable, it can only form a hydroxide in high acidity levels. When placed in a 0.1 M sodium perchlorate solution, it does not react significantly for a period of months, although a higher molar concentration of 3.0 M will result in it reacting to the solid hydroxide NpOOH almost immediately. Np(VI) hydroxide is more reactive but it is still fairly stable in acidic solutions. It will form the compound NpO· HO in the presence of ozone under various carbon dioxide pressures. Np(VII) has not been well-studied and no neutral hydroxides have been reported. It probably exists mostly as .

Three anhydrous neptunium oxides have been reported, NpO, NpO, and NpO, though some studies have stated that only the first two of these exist, suggesting that claims of NpO are actually the result of mistaken analysis of NpO. However, as the full extent of the reactions that occur between neptunium and oxygen has yet to be researched, it is not certain which of these claims is accurate. Although neptunium oxides have not been produced with neptunium in oxidation states as high as those possible with the adjacent actinide uranium, neptunium oxides are more stable at lower oxidation states. This behavior is illustrated by the fact that NpO can be produced by simply burning neptunium salts of oxyacids in air.

The greenish-brown NpO is very stable over a large range of pressures and temperatures and does not undergo phase transitions at low temperatures. It does show a phase transition from face-centered cubic to orthorhombic at around 33-37GPa, although it returns to is original phase when pressure is released. It remains stable under oxygen pressures up to 2.84 MPa and temperatures up to 400 °C. 
NpO is black-brown in color and monoclinic with a lattice size of 418×658×409 picometres. It is relatively unstable and decomposes to NpO and O at 420-695 °C. Although NpO was initially subject to several studies that claimed to produce it with mutually contradictory methods, it was eventually prepared successfully by heating neptunium peroxide to 300-350 °C for 2–3 hours or by heating it under a layer of water in an ampoule at 180 °C.

Neptunium also forms a large number of oxide compounds with a wide variety of elements, although the neptunate oxides formed with alkali metals and alkaline earth metals have been by far the most studied. Ternary neptunium oxides are generally formed by reacting NpO with the oxide of another element or by precipitating from an alkaline solution. LiNpO has been prepared by reacting LiO and NpO at 400 °C for 16 hours or by reacting LiO with NpO · HO at 400 °C for 16 hours in a quartz tube and flowing oxygen. Alkali neptunate compounds KNpO, CsNpO, and RbNpO are all created by a similar reaction: 

The oxide compounds KNpO, CsNpO, and RbNpO are formed by reacting Np(VII) () with a compound of the alkali metal nitrate and ozone. Additional compounds have been produced by reacting NpO and water with solid alkali and alkaline peroxides at temperatures of 400 - 600 °C for 15–30 hours. Some of these include Ba(NpO), BaNaNpO, and BaLiNpO. Also, a considerable number of hexavelant neptunium oxides are formed by reacting solid-state NpO with various alkali or alkaline earth oxides in an environment of flowing oxygen. Many of the resulting compounds also have an equivalent compound that substitutes uranium for neptunium. Some compounds that have been characterized include NaNpO, NaNpO, NaNpO, and NaNpO. These can be obtained by heating different combinations of NpO and NaO to various temperature thresholds and further heating will also cause these compounds to exhibit different neptunium allotropes. The lithium neptunate oxides LiNpO and LiNpO can be obtained with similar reactions of NpO and LiO.

A large number of additional alkali and alkaline neptunium oxide compounds such as CsNpO and CsNpO have been characterized with various production methods. Neptunium has also been observed to form ternary oxides with many additional elements in groups 3 through 7, although these compounds are much less well studied.

Although neptunium halide compounds have not been nearly as well studied as its oxides, a fairly large number have been successfully characterized. Of these, neptunium fluorides have been the most extensively researched, largely because of their potential use in separating the element from nuclear waste products. Four binary neptunium fluoride compounds, NpF, NpF, NpF, and NpF, have been reported. The first two are fairly stable and were first prepared in 1947 through the following reactions:
Later, NpF was obtained directly by heating NpO to various temperatures in mixtures of either hydrogen fluoride or pure fluorine gas. NpF is much more difficult to create and most known preparation methods involve reacting NpF or NpF compounds with various other fluoride compounds. NpF will decompose into NpF and NpF when heated to around 320 °C.

NpF or neptunium hexafluoride is extremely volatile, as are its adjacent actinide compounds uranium hexafluoride (UF) and plutonium hexafluoride (PuF). This volatility has attracted a large amount of interest to the compound in an attempt to devise a simple method for extracting neptunium from spent nuclear power station fuel rods. NpF was first prepared in 1943 by reacting NpF and gaseous fluorine at very high temperatures and the first bulk quantities were obtained in 1958 by heating NpF and dripping pure fluorine on it in a specially prepared apparatus. Additional methods that have successfully produced neptunium hexafluoride include reacting BrF and BrF with NpF and by reacting several different neptunium oxide and fluoride compounds with anhydrous hydrogen fluorides.

Four neptunium oxyfluoride compounds, NpOF, NpOF, NpOF, and NpOF, have been reported, although none of them have been extensively studied. NpOF is a pinkish solid and can be prepared by reacting NpO · HO and NpF with pure fluorine at around 330 °C. NpOF and NpOF can be produced by reacting neptunium oxides with anhydrous hydrogen fluoride at various temperatures. Neptunium also forms a wide variety of fluoride compounds with various elements. Some of these that have been characterized include CsNpF, RbNpF, NaNpF, and KNpOF.

Two neptunium chlorides, NpCl and NpCl, have been characterized. Although several attempts to create NpCl have been made, they have not been successful. NpCl is created by reducing neptunium dioxide with hydrogen and carbon tetrachloride (CCl) and NpCl by reacting a neptunium oxide with CCl at around 500 °C. Other neptunium chloride compounds have also been reported, including NpOCl, CsNpCl, CsNpOCl, and CsNaNpCl. Neptunium bromides NpBr and NpBr have also been created; the latter by reacting aluminium bromide with NpO at 350 °C and the former in an almost identical procedure but with zinc present. The neptunium iodide NpI has also been prepared by the same method as NpBr.

Neptunium chalcogen and pnictogen compounds have been well studied primarily as part of research into their electronic and magnetic properties and their interactions in the natural environment. Pnictide and carbide compounds have also attracted interest because of their presence in the fuel of several advanced nuclear reactor designs, although the latter group has not had nearly as much research as the former.

A wide variety of neptunium sulfide compounds have been characterized, including the pure sulfide compounds NpS, NpS, NpS, NpS, NpS, and NpS. Of these, NpS, prepared by reacting NpO with hydrogen sulfide and carbon disulfide at around 1000 °C, is the most well-studied and three allotropic forms are known. The α form exists up to around 1230 °C, the β up to 1530 °C, and the γ form, which can also exist as NpS, at higher temperatures. NpS can be created by reacting NpS and neptunium metal at 1600 °C and NpS can be prepared by the decomposition of NpS at 500 °C or by reacting sulfur and neptunium hydride at 650 °C. NpS is made by heating a mixture of NpS and pure sulfur to 500 °C. All of the neptunium sulfides except for the β and γ forms of NpS are isostructural with the equivalent uranium sulfide and several, including NpS, α−NpS, and β−NpS are also isostructural with the equivalent plutonium sulfide. The oxysulfides NpOS, NpOS, and NpOS have also been created, although the latter three have not been well studied. NpOS was first prepared in 1985 by vacuum sealing NpO, NpS, and pure sulfur in a quartz tube and heating it to 900 °C for one week.

Neptunium selenide compounds that have been reported include NpSe, NpSe, NpSe, NpSe, NpSe, and NpSe. All of these have only been obtained by heating neptunium hydride and selenium metal to various temperatures in a vacuum for an extended period of time and NpSe is only known to exist in the γ allotrope at relatively high temperatures. Two neptunium oxyselenide compounds are known, NpOSe and NpOSe, are formed with similar methods by replacing the neptunium hydride with neptunium dioxide. The known neptunium telluride compounds NpTe, NpTe, NpTe, NpTe, and NpOTe are formed by similar procedures to the selenides and NpOTe is isostructural to the equivalent uranium and plutonium compounds. No neptunium−polonium compounds have been reported.

Neptunium nitride (NpN) was first prepared in 1953 by reacting neptunium hydride and ammonia gas at around 750 °C in a quartz capillary tube. Later, it was produced by reacting different mixtures of nitrogen and hydrogen with neptunium metal at various temperatures. It has also been created by the reduction of neptunium dioxide with diatomic nitrogen gas at 1550 °C. NpN is isomorphous with uranium mononitride (UN) and plutonium mononitride (PuN) and has a melting point of 2830 °C under a nitrogen pressure of around 1 MPa. Two neptunium phosphide compounds have been reported, NpP and NpP. The first has a face centered cubic structure and is prepared by converting neptunium metal to a powder and then reacting it with phosphine gas at 350 °C. NpP can be created by reacting neptunium metal with red phosphorus at 740 °C in a vacuum and then allowing any extra phosphorus to sublimate away. The compound is non-reactive with water but will react with nitric acid to produce Np(IV) solution.

Three neptunium arsenide compounds have been prepared, NpAs, NpAs, and NpAs. The first two were first created by heating arsenic and neptunium hydride in a vacuum-sealed tube for about a week. Later, NpAs was also made by confining neptunium metal and arsenic in a vacuum tube, separating them with a quartz membrane, and heating them to just below neptunium's melting point of 639 °C, which is slightly higher than the arsenic's sublimation point of 615 °C. NpAs is prepared by a similar procedure using iodine as a transporting agent. NpAs crystals are brownish gold and NpAs is black. The neptunium antimonide compound NpSb was created in 1971 by placing equal quantities of both elements in a vacuum tube, heating them to the melting point of antimony, and then heating it further to 1000 °C for sixteen days. This procedure also created trace amounts of an additional antimonide compound NpSb. One neptunium-bismuth compound, NpBi, has also been reported.

The neptunium carbides NpC, NpC, and NpC (tentative) have been reported, but have not characterized in detail despite the high importance and utility of actinide carbides as advanced nuclear reactor fuel. NpC is a non-stoichiometric compound, and could be better labelled as NpC (0.82 ≤ "x" ≤ 0.96). It may be obtained from the reaction of neptunium hydride with graphite at 1400 °C or by heating the constituent elements together in an electric arc furnace using a tungsten electrode. It reacts with excess carbon to form pure NpC. NpC is formed from heating NpO in a graphite crucible at 2660–2800 °C.

Neptunium reacts with hydrogen in a similar manner to its neighbor plutonium, forming the hydrides NpH (face-centered cubic) and NpH (hexagonal). These are isostructural with the corresponding plutonium hydrides, although unlike PuH, the lattice parameters of NpH become greater as the hydrogen content ("x") increases. The hydrides require extreme care in handling as they decompose in a vacuum at 300 °C to form finely divided neptunium metal, which is pyrophoric.

Being chemically stable, neptunium phosphates have been investigated for potential use in immobilizing nuclear waste. Neptunium pyrophosphate (α-NpPO), a green solid, has been produced in the reaction between neptunium dioxide and boron phosphate at 1100 °C, though neptunium(IV) phosphate has so far remained elusive. The series of compounds NpM(PO), where M is an alkali metal (Li, Na, K, Rb, or Cs), are all known. Some neptunium sulfates have been characterized, both aqueous and solid and at various oxidation states of neptunium (IV through VI have been observed). Additionally, neptunium carbonates have been investigated to achieve a better understanding of the behavior of neptunium in geological repositories and the environment, where it may come into contact with carbonate and bicarbonate aqueous solutions and form soluble complexes.

A few organoneptunium compounds are known and chemically characterized, although not as many as for uranium due to neptunium's scarcity and radioactivity. The most well known organoneptunium compounds are the cyclopentadienyl and cyclooctatetraenyl compounds and their derivatives. The trivalent cyclopentadienyl compound Np(CH)·THF was obtained in 1972 from reacting Np(CH)Cl with sodium, although the simpler Np(CH) could not be obtained. Tetravalent neptunium cyclopentadienyl, a reddish-brown complex, was synthesized in 1968 by reacting neptunium(IV) chloride with potassium cyclopentadienide:

It is soluble in benzene and THF, and is less sensitive to oxygen and water than Pu(CH) and Am(CH). Other Np(IV) cyclopentadienyl compounds are known for many ligands: they have the general formula (CH)NpL, where L represents a ligand.
Neptunocene, Np(CH), was synthesized in 1970 by reacting neptunium(IV) chloride with K(CH). It is isomorphous to uranocene and plutonocene, and they behave chemically identically: all three compounds are insensitive to water or dilute bases but are sensitive to air, reacting quickly to form oxides, and are only slightly soluble in benzene and toluene. Other known neptunium cyclooctatetraenyl derivatives include Np(RCH) (R = ethanol, butanol) and KNp(CH)·2THF, which is isostructural to the corresponding plutonium compound. In addition, neptunium hydrocarbyls have been prepared, and solvated triiodide complexes of neptunium are a precursor to many organoneptunium and inorganic neptunium compounds.

There is much interest in the coordination chemistry of neptunium, because its five oxidation states all exhibit their own distinctive chemical behavior, and the coordination chemistry of the actinides is heavily influenced by the actinide contraction (the greater-than-expected decrease in ionic radii across the actinide series, analogous to the lanthanide contraction).

Few neptunium(III) coordination compounds are known, because Np(III) is readily oxidized by atmospheric oxygen while in aqueous solution. However, sodium formaldehyde sulfoxylate can reduce Np(IV) to Np(III), stabilizing the lower oxidation state and forming various sparingly soluble Np(III) coordination complexes, such as ·11HO, ·HO, and .

Many neptunium(IV) coordination compounds have been reported, the first one being , which is isostructural with the analogous uranium(IV) coordination compound. Other Np(IV) coordination compounds are known, some involving other metals such as cobalt (·8HO, formed at 400 K) and copper (·6HO, formed at 600 K). Complex nitrate compounds are also known: the experimenters who produced them in 1986 and 1987 produced single crystals by slow evaporation of the Np(IV) solution at ambient temperature in concentrated nitric acid and excess 2,2′-pyrimidine.

The coordination chemistry of neptunium(V) has been extensively researched due to the presence of cation–cation interactions in the solid state, which had been already known for actinyl ions. Some known such compounds include the neptunyl dimer ·8HO and neptunium glycolate, both of which form green crystals.

Neptunium(VI) compounds range from the simple oxalate (which is unstable, usually becoming Np(IV)) to such complicated compounds as the green . Extensive study has been performed on compounds of the form , where M represents a monovalent cation and An is either uranium, neptunium, or plutonium.

Since 1967, when neptunium(VII) was discovered, some coordination compounds with neptunium in the +7 oxidation state have been prepared and studied. The first reported such compound was initially characterized as ·"n"HO in 1968, but was suggested in 1973 to actually have the formula ·2HO based on the fact that Np(VII) occurs as in aqueous solution. This compound forms dark green prismatic crystals with maximum edge length 0.15–0.4 mm.

Most neptunium coordination complexes known in solution involve the element in the +4, +5, and +6 oxidation states: only a few studies have been done on neptunium(III) and (VII) coordination complexes. For the former, NpX and (X = Cl, Br) were obtained in 1966 in concentrated LiCl and LiBr solutions, respectively: for the latter, 1970 experiments discovered that the ion could form sulfate complexes in acidic solutions, such as and ; these were found to have higher stability constants than the neptunyl ion (). A great many complexes for the other neptunium oxidation states are known: the inorganic ligands involved are the halides, iodate, azide, nitride, nitrate, thiocyanate, sulfate, carbonate, chromate, and phosphate. Many organic ligands are known to be able to be used in neptunium coordination complexes: they include acetate, propionate, glycolate, lactate, oxalate, malonate, phthalate, mellitate, and citrate.

Analogously to its neighbours, uranium and plutonium, the order of the neptunium ions in terms of complex formation ability is Np > ≥ Np > . (The relative order of the middle two neptunium ions depends on the ligands and solvents used.) The stability sequence for Np(IV), Np(V), and Np(VI) complexes with monovalent inorganic ligands is F > > SCN > > Cl > ; the order for divalent inorganic ligands is > > . These follow the strengths of the corresponding acids. The divalent ligands are more strongly complexing than the monovalent ones. can also form the complex ions [] (M = Al, Ga, Sc, In, Fe, Cr, Rh) in perchloric acid solution: the strength of interaction between the two cations follows the order Fe > In > Sc > Ga > Al. The neptunyl and uranyl ions can also form a complex together.

An important of use of Np is as a precursor in plutonium production, where it is irradiated with neutrons to create Pu, an alpha emitter for radioisotope thermal generators for spacecraft and military applications. Np will capture a neutron to form Np and beta decay with a half-life of just over two days to Pu.

Pu also exists in sizable quantities in spent nuclear fuel but would have to be separated from other isotopes of plutonium. Irradiating neptunium-237 with electron beams, provoking bremsstrahlung, also produces quite pure samples of the isotope plutonium-236, useful as a tracer to determine plutonium concentration in the environment.

Neptunium is fissionable, and could theoretically be used as fuel in a fast neutron reactor or a nuclear weapon, with a critical mass of around 60 kilograms. In 1992, the U.S. Department of Energy declassified the statement that neptunium-237 "can be used for a nuclear explosive device". It is not believed that an actual weapon has ever been constructed using neptunium. As of 2009, the world production of neptunium-237 by commercial power reactors was over 1000 critical masses a year, but to extract the isotope from irradiated fuel elements would be a major industrial undertaking.

In September 2002, researchers at the Los Alamos National Laboratory briefly created the first known nuclear critical mass using neptunium in combination with shells of enriched uranium (uranium-235), discovering that the critical mass of a bare sphere of neptunium-237 "ranges from kilogram weights in the high fifties to low sixties," showing that it "is about as good a bomb material as [uranium-235]." The United States Federal government made plans in March 2004 to move America's supply of separated neptunium to a nuclear-waste disposal site in Nevada.

Np is used in devices for detecting high-energy (MeV) neutrons.
Neptunium accumulates in commercial household ionization-chamber smoke detectors from decay of the (typically) 0.2 microgram of americium-241 initially present as a source of ionizing radiation. With a half-life of 432 years, the americium-241 in an ionization smoke detector includes about 3% neptunium after 20 years, and about 15% after 100 years.

Neptunium-237 is the most mobile actinide in the deep geological repository environment. This makes it and its predecessors such as americium-241 candidates of interest for destruction by nuclear transmutation. Due to its long half-life, neptunium will become the major contributor of the total radiotoxicity in 10,000 years. As it is unclear what happens to the containment in that long time span, an extraction of the neptunium would minimize the contamination of the environment if the nuclear waste could be mobilized after several thousand years.
Neptunium does not have a biological role, as it has a short half-life and occurs only in small traces naturally. Animal tests showed that it is not absorbed via the digestive tract. When injected it concentrates in the bones, from which it is slowly released.

Finely divided neptunium metal presents a fire hazard because neptunium is pyrophoric; small grains will ignite spontaneously in air at room temperature.





</doc>
<doc id="21278" url="https://en.wikipedia.org/wiki?curid=21278" title="Nobelium">
Nobelium

Nobelium is a synthetic chemical element with the symbol No and atomic number 102. It is named in honor of Alfred Nobel, the inventor of dynamite and benefactor of science. A radioactive metal, it is the tenth transuranic element and is the penultimate member of the actinide series. Like all elements with atomic number over 100, nobelium can only be produced in particle accelerators by bombarding lighter elements with charged particles. A total of twelve nobelium isotopes are known to exist; the most stable is No with a half-life of 58 minutes, but the shorter-lived No (half-life 3.1 minutes) is most commonly used in chemistry because it can be produced on a larger scale.

Chemistry experiments have confirmed that nobelium behaves as a heavier homolog to ytterbium in the periodic table. The chemical properties of nobelium are not completely known: they are mostly only known in aqueous solution. Before nobelium's discovery, it was predicted that it would show a stable +2 oxidation state as well as the +3 state characteristic of the other actinides: these predictions were later confirmed, as the +2 state is much more stable than the +3 state in aqueous solution and it is difficult to keep nobelium in the +3 state.

In the 1950s and 1960s, many claims of the discovery of nobelium were made from laboratories in Sweden, the Soviet Union, and the United States. Although the Swedish scientists soon retracted their claims, the priority of the discovery and therefore the naming of the element was disputed between Soviet and American scientists, and it was not until 1997 that International Union of Pure and Applied Chemistry (IUPAC) credited the Soviet team with the discovery, but retained nobelium, the Swedish proposal, as the name of the element due to its long-standing use in the literature.

The discovery of element 102 was a complicated process and was claimed by groups from Sweden, the United States, and the Soviet Union. The first complete and incontrovertible report of its detection only came in 1966 from the Joint Institute of Nuclear Research at Dubna (then in the Soviet Union).

The first announcement of the discovery of element 102 was announced by physicists at the Nobel Institute in Sweden in 1957. The team reported that they had bombarded a curium target with carbon-13 ions for twenty-five hours in half-hour intervals. Between bombardments, ion-exchange chemistry was performed on the target. Twelve out of the fifty bombardments contained samples emitting (8.5 ± 0.1) MeV alpha particles, which were in drops which eluted earlier than fermium (atomic number "Z" = 100) and californium ("Z" = 98). The half-life reported was 10 minutes and was assigned to either 102 or 102, although the possibility that the alpha particles observed were from a presumably short-lived mendelevium ("Z" = 101) isotope created from the electron capture of element 102 was not excluded. The team proposed the name "nobelium" (No) for the new element, which was immediately approved by IUPAC, a decision which the Dubna group characterized in 1968 as being hasty. The following year, scientists at the Lawrence Berkeley National Laboratory repeated the experiment but were unable to find any 8.5 MeV events which were not background effects.

In 1959, the Swedish team attempted to explain the Berkeley team's inability to detect element 102 in 1958, maintaining that they did discover it. However, later work has shown that no nobelium isotopes lighter than No (no heavier isotopes could have been produced in the Swedish experiments) with a half-life over 3 minutes exist, and that the Swedish team's results are most likely from thorium-225, which has a half-life of 8 minutes and quickly undergoes triple alpha decay to polonium-213, which has a decay energy of 8.53612 MeV. This hypothesis is lent weight by the fact that thorium-225 can easily be produced in the reaction used and would not be separated out by the chemical methods used. Later work on nobelium also showed that the divalent state is more stable than the trivalent one and hence that the samples emitting the alpha particles could not have contained nobelium, as the divalent nobelium would not have eluted with the other trivalent actinides. Thus, the Swedish team later retracted their claim and associated the activity to background effects.

The Berkeley team, consisting of Albert Ghiorso, Glenn T. Seaborg, John R. Walton and Torbjørn Sikkeland, then claimed the synthesis of element 102 in 1958. The team used the new heavy-ion linear accelerator (HILAC) to bombard a curium target (95% Cm and 5% Cm) with C and C ions. They were unable to confirm the 8.5 MeV activity claimed by the Swedes but were instead able to detect decays from fermium-250, supposedly the daughter of 102 (produced from the curium-246), which had an apparent half-life of ~3 s. Later 1963 Dubna work confirmed that 102 could be produced in this reaction, but that its half-life was actually . In 1967, the Berkeley team attempted to defend their work, stating that the isotope found was indeed Fm but the isotope that the half-life measurements actually related to was californium-244, granddaughter of 102, produced from the more abundant curium-244. Energy differences were then attributed to "resolution and drift problems", although these had not been previously reported and should also have influenced other results. 1977 experiments showed that 102 indeed had a 2.3-second half-life. However, 1973 work also showed that the Fm recoil could have also easily been produced from the isomeric transition of Fm (half-life 1.8 s) which could also have been formed in the reaction at the energy used. Given this, it is probable that no nobelium was actually produced in this experiment.

In 1959, the team continued their studies and claimed that they were able to produce an isotope that decayed predominantly by emission of an 8.3 MeV alpha particle, with a half-life of 3 s with an associated 30% spontaneous fission branch. The activity was initially assigned to 102 but later changed to 102. However, they also noted that it was not certain that nobelium had been produced due to difficult conditions. The Berkeley team decided to adopt the proposed name of the Swedish team, "nobelium", for the element.

Meanwhile, in Dubna, experiments were carried out in 1958 and 1960 aiming to synthesize element 102 as well. The first 1958 experiment bombarded plutonium-239 and -241 with oxygen-16 ions. Some alpha decays with energies just over 8.5 MeV were observed, and they were assigned to 102, although the team wrote that formation of isotopes from lead or bismuth impurities (which would not produce nobelium) could not be ruled out. While later 1958 experiments noted that new isotopes could be produced from mercury, thallium, lead, or bismuth impurities, the scientists still stood by their conclusion that element 102 could be produced from this reaction, mentioning a half-life of under 30 seconds and a decay energy of (8.8 ± 0.5) MeV. Later 1960 experiments proved that these were background effects. 1967 experiments also lowered the decay energy to (8.6 ± 0.4) MeV, but both values are too high to possibly match those of No or No. The Dubna team later stated in 1970 and again in 1987 that these results were not conclusive.

In 1961, Berkeley scientists claimed the discovery of element 103 in the reaction of californium with boron and carbon ions. They claimed the production of the isotope 103, and also claimed to have synthesized an alpha decaying isotope of element 102 that had a half-life of 15 s and alpha decay energy 8.2 MeV. They assigned this to 102 without giving a reason for the assignment. The values do not agree with those now known for No, although they do agree with those now known for No, and while this isotope probably played a part in this experiment, its discovery was inconclusive.

Work on element 102 also continued in Dubna, and in 1964, experiments were carried out there to detect alpha-decay daughters of element 102 isotopes by synthesizing element 102 from the reaction of a uranium-238 target with neon ions. The products were carried along a silver catcher foil and purified chemically, and the isotopes Fm and Fm were detected. The yield of Fm was interpreted as evidence that its parent 102 was also synthesized: as it was noted that Fm could also be produced directly in this reaction by the simultaneous emission of an alpha particle with the excess neutrons, steps were taken to ensure that Fm could not go directly to the catcher foil. The half-life detected for 102 was 8 s, which is much higher than the more modern 1967 value of (3.2 ± 0.2) s. Further experiments were conducted in 1966 for 102, using the reactions Am(N,4n)102 and U(Ne,6n)102, finding a half-life of (50 ± 10) s: at that time the discrepancy between this value and the earlier Berkeley value was not understood, although later work proved that the formation of the isomer Fm was less likely in the Dubna experiments than at the Berkeley ones. In hindsight, the Dubna results on 102 were probably correct and can be now considered a conclusive detection of element 102.

One more very convincing experiment from Dubna was published in 1966, again using the same two reactions, which concluded that 102 indeed had a half-life much longer than the 3 seconds claimed by Berkeley. Later work in 1967 at Berkeley and 1971 at the Oak Ridge National Laboratory fully confirmed the discovery of element 102 and clarified earlier observations. In December 1966, the Berkeley group repeated the Dubna experiments and fully confirmed them, and used this data to finally assign correctly the isotopes they had previously synthesized but could not yet identify at the time, and thus claimed to have discovered nobelium in 1958 to 1961.

In 1969, the Dubna team carried out chemical experiments on element 102 and concluded that it behaved as the heavier homologue of ytterbium. The Russian scientists proposed the name "joliotium" (Jo) for the new element after Irène Joliot-Curie, who had recently died, creating an element naming controversy that would not be resolved for several decades, which each group using its own proposed names.

In 1992, the IUPAC-IUPAP Transfermium Working Group (TWG) reassessed the claims of discovery and concluded that only the Dubna work from 1966 correctly detected and assigned decays to nuclei with atomic number 102 at the time. The Dubna team are therefore officially recognised as the discoverers of nobelium although it is possible that it was detected at Berkeley in 1959. This decision was criticized by Berkeley the following year, calling the reopening of the cases of elements 101 to 103 a "futile waste of time", while Dubna agreed with IUPAC's decision.

In 1994, as part of an attempted resolution to the element naming controversy, IUPAC ratified names for elements 101–109. For element 102, it ratified the name "nobelium" (No) on the basis that it had become entrenched in the literature over the course of 30 years and that Alfred Nobel should be commemorated in this fashion. Because of outcry over the 1994 names, which mostly did not respect the choices of the discoverers, a comment period ensued, and in 1995 IUPAC named element 102 "flerovium" (Fl) as part of a new proposal, after either Georgy Flyorov or his eponymous Flerov Laboratory of Nuclear Reactions. This proposal was also not accepted, and in 1997 the name "nobelium" was restored. Today the name "flerovium", with the same symbol, refers to element 114.

In the periodic table, nobelium is located to the right of the actinide mendelevium, to the left of the actinide lawrencium, and below the lanthanide ytterbium. Nobelium metal has not yet been prepared in bulk quantities, and bulk preparation is currently impossible. Nevertheless, a number of predictions and some preliminary experimental results have been done regarding its properties.

The lanthanides and actinides, in the metallic state, can exist as either divalent (such as europium and ytterbium) or trivalent (most other lanthanides) metals. The former have fs configurations, whereas the latter have fds configurations. In 1975, Johansson and Rosengren examined the measured and predicted values for the cohesive energies (enthalpies of crystallization) of the metallic lanthanides and actinides, both as divalent and trivalent metals. The conclusion was that the increased binding energy of the [Rn]5f6d7s configuration over the [Rn]5f7s configuration for nobelium was not enough to compensate for the energy needed to promote one 5f electron to 6d, as is true also for the very late actinides: thus einsteinium, fermium, mendelevium, and nobelium were expected to be divalent metals, although for nobelium this prediction has not yet been confirmed. The increasing predominance of the divalent state well before the actinide series concludes is attributed to the relativistic stabilization of the 5f electrons, which increases with increasing atomic number: an effect of this is that nobelium is predominantly divalent instead of trivalent, unlike all the other lanthanides and actinides. In 1986, nobelium metal was estimated to have an enthalpy of sublimation between 126 kJ/mol, a value close to the values for einsteinium, fermium, and mendelevium and supporting the theory that nobelium would form a divalent metal. Like the other divalent late actinides (except the once again trivalent lawrencium), metallic nobelium should assume a face-centered cubic crystal structure. Divalent nobelium metal should have a metallic radius of around 197 pm. Nobelium's melting point has been predicted to be 827 °C, the same value as that estimated for the neighboring element mendelevium. Its density is predicted to be around 9.9 ± 0.4 g/cm.

The chemistry of nobelium is incompletely characterized and is known only in aqueous solution, in which it can take on the +3 or +2 oxidation states, the latter being more stable. It was largely expected before the discovery of nobelium that in solution, it would behave like the other actinides, with the trivalent state being predominant; however, Seaborg predicted in 1949 that the +2 state would also be relatively stable for nobelium, as the No ion would have the ground-state electron configuration [Rn]5f, including the stable filled 5f shell. It took nineteen years before this prediction was confirmed.

In 1967, experiments were conducted to compare nobelium's chemical behavior to that of terbium, californium, and fermium. All four elements were reacted with chlorine and the resulting chlorides were deposited along a tube, along which they were carried by a gas. It was found that the nobelium chloride produced was strongly adsorbed on solid surfaces, proving that it was not very volatile, like the chlorides of the other three investigated elements. However, both NoCl and NoCl were expected to exhibit nonvolatile behavior and hence this experiment was inconclusive as to what the preferred oxidation state of nobelium was. Determination of nobelium's favoring of the +2 state had to wait until the next year, when cation-exchange chromatography and coprecipitation experiments were carried out on around fifty thousand No atoms, finding that it behaved differently from the other actinides and more like the divalent alkaline earth metals. This proved that in aqueous solution, nobelium is most stable in the divalent state when strong oxidizers are absent. Later experimentation in 1974 showed that nobelium eluted with the alkaline earth metals, between Ca and Sr. Nobelium is the only known f-block element for which the +2 state is the most common and stable one in aqueous solution. This occurs because of the large energy gap between the 5f and 6d orbitals at the end of the actinide series.

It is expected that the relativistic stabilization of the 7s subshell greatly destabilizes nobelium dihydride, NoH, and relativistic stabilisation of the 7p spinor over the 6d spinor mean that excited states in nobelium atoms have 7s and 7p contribution instead of the expected 6d contribution. The long No–H distances in the NoH molecule and the significant charge transfer lead to extreme ionicity with a dipole moment of 5.94 D for this molecule. In this molecule, nobelium is expected to exhibit main-group-like behavior, specifically acting like an alkaline earth metal with its "n"s valence shell configuration and core-like 5f orbitals.

Nobelium's complexing ability with chloride ions is most similar to that of barium, which complexes rather weakly. Its complexing ability with citrate, oxalate, and acetate in an aqueous solution of 0.5 M ammonium nitrate is between that of calcium and strontium, although it is somewhat closer to that of strontium.

The standard reduction potential of the "E"°(No→No) couple was estimated in 1967 to be between +1.4 and +1.5 V; it was later found in 2009 to be only about +0.75 V. The positive value shows that No is more stable than No and that No is a good oxidizing agent. While the quoted values for the "E"°(No→No) and "E"°(No→No) vary among sources, the accepted standard estimates are −2.61 and −1.26 V. It has been predicted that the value for the "E"°(No→No) couple would be +6.5 V. The Gibbs energies of formation for No and No are estimated to be −342 and −480 kJ/mol, respectively.

A nobelium atom has 102 electrons, of which three can act as valence electrons. They are expected to be arranged in the configuration [Rn]5f7s (ground state term symbol S), although experimental verification of this electron configuration had not yet been made as of 2006. In forming compounds, all the three valence electrons may be lost, leaving behind a [Rn]5f core: this conforms to the trend set by the other actinides with their [Rn]5f electron configurations in the tripositive state. Nevertheless, it is more likely that only two valence electrons may be lost, leaving behind a stable [Rn]5f core with a filled 5f shell. The first ionization potential of nobelium was measured to be at most (6.65 ± 0.07) eV in 1974, based on the assumption that the 7s electrons would ionize before the 5f ones; this value has since not yet been refined further due to nobelium's scarcity and high radioactivity. The ionic radius of hexacoordinate and octacoordinate No had been preliminarily estimated in 1978 to be around 90 and 102 pm respectively; the ionic radius of No has been experimentally found to be 100 pm to two significant figures. The enthalpy of hydration of No has been calculated as 1486 kJ/mol.

Twelve isotopes of nobelium are known, with mass numbers 250–260 and 262; all are radioactive. Additionally, nuclear isomers are known for mass numbers 251, 253, and 254. Of these, the longest-lived isotope is No with a half-life of 58 minutes, and the longest-lived isomer is No with a half-life of 1.7 seconds. However, the still undiscovered isotope No is predicted to have a still longer half-life of 170 min. Additionally, the shorter-lived No (half-life 3.1 minutes) is more often used in chemical experimentation because it can be produced in larger quantities from irradiation of californium-249 with carbon-12 ions. After No and No, the next most stable nobelium isotopes are No (half-life 1.62 minutes), No (51 seconds), No (25 seconds), No (2.91 seconds), and No (2.57 seconds). All of the remaining nobelium isotopes have half-lives that are less than a second, and the shortest-lived known nobelium isotope (No) has a half-life of only 0.25 milliseconds. The isotope No is especially interesting theoretically as it is in the middle of a series of prolate nuclei from Pa to Rg, and the formation of its nuclear isomers (of which two are known) is controlled by proton orbitals such as 2f which come just above the spherical proton shell; it can be synthesized in the reaction of Pb with Ca.

The half-lives of nobelium isotopes increase smoothly from No to No. However, a dip appears at No, and beyond this the half-lives of even-even nobelium isotopes drop sharply as spontaneous fission becomes the dominant decay mode. For example, the half-life of No is almost three seconds, but that of No is only 1.2 milliseconds. This shows that at nobelium, the mutual repulsion of protons poses a limit to the region of long-lived nuclei in the actinide series. The even-odd nobelium isotopes mostly continue to have longer half-lives as their mass numbers increase, with a dip in the trend at No.

The isotopes of nobelium are mostly produced by bombarding actinide targets (uranium, plutonium, curium, californium, or einsteinium), with the exception of nobelium-262, which is produced as the daughter of lawrencium-262. The most commonly used isotope, No, can be produced from bombarding curium-248 or californium-249 with carbon-12: the latter method is more common. Irradiating a 350 μg cm target of californium-249 with three trillion (3 × 10) 73 MeV carbon-12 ions per second for ten minutes can produce around 1200 nobelium-255 atoms.

Once the nobelium-255 is produced, it can be separated out in a similar way as used to purify the neighboring actinide mendelevium. The recoil momentum of the produced nobelium-255 atoms is used to bring them physically far away from the target from which they are produced, bringing them onto a thin foil of metal (usually beryllium, aluminium, platinum, or gold) just behind the target in a vacuum: this is usually combined by trapping the nobelium atoms in a gas atmosphere (frequently helium), and carrying them along with a gas jet from a small opening in the reaction chamber. Using a long capillary tube, and including potassium chloride aerosols in the helium gas, the nobelium atoms can be transported over tens of meters. The thin layer of nobelium collected on the foil can then be removed with dilute acid without completely dissolving the foil. The nobelium can then be isolated by exploiting its tendency to form the divalent state, unlike the other trivalent actinides: under typically used elution conditions (bis-(2-ethylhexyl) phosphoric acid (HDEHP) as stationary organic phase and 0.05 M hydrochloric acid as mobile aqueous phase, or using 3 M hydrochloric acid as an eluant from cation-exchange resin columns), nobelium will pass through the column and elute while the other trivalent actinides remain on the column. However, if a direct "catcher" gold foil is used, the process is complicated by the need to separate out the gold using anion-exchange chromatography before isolating the nobelium by elution from chromatographic extraction columns using HDEHP.




</doc>
<doc id="21281" url="https://en.wikipedia.org/wiki?curid=21281" title="Norwegian Sea">
Norwegian Sea

The Norwegian Sea () is a marginal sea in the Arctic Ocean, northwest of Norway between the North Sea and the Greenland Sea, adjoining the Barents Sea to the northeast. In the southwest, it is separated from the Atlantic Ocean by a submarine ridge running between Iceland and the Faroe Islands. To the north, the Jan Mayen Ridge separates it from the Greenland Sea.

Unlike many other seas, most of the bottom of the Norwegian Sea is not part of a continental shelf and therefore lies at a great depth of about two kilometres on average. Rich deposits of oil and natural gas are found under the sea bottom and are being explored commercially, in the areas with sea depths of up to about one kilometre. The coastal zones are rich in fish that visit the Norwegian Sea from the North Atlantic or from the Barents Sea (cod) for spawning. The warm North Atlantic Current ensures relatively stable and high water temperatures, so that unlike the Arctic seas, the Norwegian Sea is ice-free throughout the year. Recent research has concluded that the large volume of water in the Norwegian Sea with its large heat absorption capacity is more important as a source of Norway's mild winters than the Gulf Stream and its extensions.

The International Hydrographic Organization defines the limits of the Norwegian Sea as follows:

The Norwegian Sea was formed about 250 million years ago, when the Eurasian plate of Norway and the North American Plate, including Greenland, started to move apart. The existing narrow shelf sea between Norway and Greenland began to widen and deepen. The present continental slope in the Norwegian Sea marks the border between Norway and Greenland as it stood approximately 250 million years ago. In the north it extends east from Svalbard and on the southwest between Britain and the Faroes. This continental slope contains rich fishing grounds and numerous coral reefs. Settling of the shelf after the separation of the continents has resulted in landslides, such as the Storegga Slide about 8,000 years ago that induced a major tsunami.

The coasts of the Norwegian Sea were shaped during the last Ice Age. Large glaciers several kilometres high pushed into the land, forming fjords, removing the crust into the sea, and thereby extending the continental slopes. This is particularly clear off the Norwegian coast along Helgeland and north to the Lofoten Islands. The Norwegian continental shelf is between 40 and 200 kilometres wide, and has a different shape from the shelves in the North Sea and Barents Sea. It contains numerous trenches and irregular peaks, which usually have an amplitude of less than 100 metres, but can reach up to 400 metres. They are covered with a mixture of gravel, sand, and mud, and the trenches are used by fish as spawning grounds. Deeper into the sea, there are two deep basins separated by a low ridge (its deepest point at 3,000 m) between the Vøring Plateau and Jan Mayen island. The southern basin is larger and deeper, with large areas between 3,500 and 4,000 metres deep. The northern basin is shallower at 3,200–3,300 metres, but contains many individual sites going down to 3,500 metres. Submarine thresholds and continental slopes mark the borders of these basins with the adjacent seas. To the south lies the European continental shelf and the North Sea, to the east is the Eurasian continental shelf with the Barents Sea. To the west, the Scotland-Greenland Ridge separates the Norwegian Sea from the North Atlantic. This ridge is on average only 500 metres deep, only in a few places reaching the depth of 850 metres. To the north lie the Jan Mayen Ridge and Mohns Ridge, which lie at a depth of 2,000 metres, with some trenches reaching depths of about 2,600 meters.

Four major water masses originating in the Atlantic and Arctic oceans meet in the Norwegian Sea, and the associated currents are of fundamental importance for the global climate. The warm, salty North Atlantic Current flows in from the Atlantic Ocean, and the colder and less saline Norwegian Current originates in the North Sea. The so-called East Iceland Current transports cold water south from the Norwegian Sea toward Iceland and then east, along the Arctic Circle; this current occurs in the middle water layer. Deep water flows into the Norwegian Sea from the Greenland Sea. The tides in the sea are semi-diurnal; that is, they rise twice a day, to a height of about 3.3 metres.

The hydrology of the upper water layers is largely determined by the flow from the North Atlantic. It reaches a speed of 10 Sv (1 Sv = million m/s) and its maximum depth is 700 metres at the Lofoten Islands, but normally it is within 500 meters. Part of it comes through the Faroe-Shetland Channel and has a comparatively high salinity of 35.3‰ (parts per thousand). This current originates in the North Atlantic Current and passes along the European continental slope; increased evaporation due to the warm European climate results in the elevated salinity. Another part passes through the Greenland-Scotland trench between the Faroe Islands and Iceland; this water has a mean salinity between 35 and 35.2‰. The flow shows strong seasonal variations and can be twice as high in winter as in summer. While at the Faroe-Shetland Channel it has a temperature of about 9.5 °C; it cools to about 5 °C at Svalbard and releases this energy (about 250 terawatts) to the environment.

The current flowing from the North Sea originates in the Baltic Sea and thus collects most of the drainage from northern Europe; this contribution is however relatively small. The temperature and salinity of this current show strong seasonal and annual fluctuations. Long-term measurements within the top 50 metres near the coast show a maximum temperature of 11.2 °C at the 63° N parallel in September and a minimum of 3.9 °C at the North Cape in March. The salinity varies between 34.3 and 34.6‰ and is lowest in spring owing to the inflow of melted snow from rivers. The largest rivers discharging into the sea are Namsen, Ranelva and Vefsna. They are all relatively short, but have a high discharge rate owing to their steep mountainous nature.

A portion of the warm surface water flows directly, within the West Spitsbergen Current, from the Atlantic Ocean, off the Greenland Sea, to the Arctic Ocean. This current has a speed of 3–5 Sv and has a large impact on the climate. Other surface water (~1 Sv) flows along the Norwegian coast in the direction of the Barents Sea. This water may cool enough in the Norwegian Sea to submerge into the deeper layers; there it displaces water that flows back into the North Atlantic.

Arctic water from the East Iceland Current is mostly found in the southwestern part of the sea, near Greenland. Its properties also show significant annual fluctuations, with long-term average temperature being below 3 °C and salinity between 34.7 and 34.9‰. The fraction of this water on the sea surface depends on the strength of the current, which in turn depends on the pressure difference between the Icelandic Low and Azores High: the larger the difference, the stronger the current.

The Norwegian Sea is connected with the Greenland Sea and the Arctic Ocean by the 2,600-metre deep Fram Strait. The Norwegian Sea Deep Water (NSDW) occurs at depths exceeding 2,000 metres; this homogeneous layer with a salinity of 34.91‰ experiences little exchange with the adjacent seas. Its temperature is below 0 °C and drops to −1 °C at the ocean floor. Compared with the deep waters of the surrounding seas, NSDW has more nutrients but less oxygen and is relatively old.

The weak deep-water exchange with the Atlantic Ocean is due to the small depth of the relatively flat Greenland-Scotland Ridge between Scotland and Greenland, an offshoot of the Mid-Atlantic Ridge. Only four areas of the Greenland-Scotland Ridge are deeper than 500 metres: the Faroe Bank Channel (about 850 metres), some parts of the Iceland-Faroe Ridge (about 600 metres), the Wyville-Thomson Ridge (620 metres), and areas between Greenland and the Denmark Strait (850 meters) – this is much shallower than the Norwegian Sea. Cold deep water flows into the Atlantic through various channels: about 1.9 Sv through the Faroe Bank channel, 1.1 Sv through the Iceland-Faroe channel, and 0.1 Sv via the Wyville-Thomson Ridge. The turbulence that occurs when the deep water falls behind the Greenland-Scotland Ridge into the deep Atlantic basin mixes the adjacent water layers and forms the North Atlantic Deep Water, one of two major deep-sea currents providing the deep ocean with oxygen.

The thermohaline circulation affects the climate in the Norwegian Sea, and the regional climate can significantly deviate from average. There is also a difference of about 10 °C between the sea and the coastline. Temperatures rose between 1920 and 1960, and the frequency of storms decreased in this period. The storminess was relatively high between 1880 and 1910, decreased significantly in 1910–1960, and then recovered to the original level.

In contrast to the Greenland Sea and Arctic seas, the Norwegian Sea is ice-free year round, owing to its warm currents. The convection between the relatively warm water and cold air in the winter plays an important role in the Arctic climate. The 10-degree July isotherm (air temperature line) runs through the northern boundary of the Norwegian Sea and is often taken as the southern boundary of the Arctic. In winter, the Norwegian Sea generally has the lowest air pressure in the entire Arctic and where most Icelandic Low depressions form. The water temperature in most parts of the sea is 2–7 °C in February and 8–12 °C in August.

The Norwegian Sea is a transition zone between boreal and Arctic conditions, and thus contains flora and fauna characteristic of both climatic regions. The southern limit of many Arctic species runs through the North Cape, Iceland, and the center of the Norwegian Sea, while the northern limit of boreal species lies near the borders of the Greenland Sea with the Norwegian Sea and Barents Sea; that is, these areas overlap. Some species like the scallop "Chlamys islandica" and capelin tend to occupy this area between the Atlantic and Arctic oceans.

Most of the aquatic life in the Norwegian Sea is concentrated in the upper layers. Estimates for the entire North Atlantic are that only 2% of biomass is produced at depths below 1,000 metres and only 1.2% occurs near the sea floor.

The blooming of the phytoplankton is dominated by chlorophyll and peaks around 20 May. The major phytoplankton forms are diatoms, in particular the genus "Thalassiosira" and "Chaetoceros". After the spring bloom the haptophytes of the genus "Phaecocystis pouchetti" become dominant.

Zooplankton is mostly represented by the copepods "Calanus finmarchicus" and "Calanus hyperboreus", where the former occurs about four times more often than the latter and is mostly found in the Atlantic streams, whereas "C. hyperboreus" dominates the Arctic waters; they are the main diet of most marine predators. The most important krill species are "Meganyctiphanes norvegica", "Thyssanoessa inermis", and "Thyssanoessa longicaudata". In contrast to the Greenland Sea, there is a significant presence of calcareous plankton (Coccolithophore and Globigerinida) in the Norwegian Sea. Plankton production strongly fluctuates between years. For example, "C. finmarchicus" yield was 28 g/m² (dry weight) in 1995 and only 8 g/m² in 1997; this correspondingly affected the population of all its predators.

Shrimp of the species "Pandalus borealis" play an important role in the diet of fish, particularly cod and blue whiting, and mostly occur at depths between 200 and 300 metres. A special feature of the Norwegian Sea is extensive coral reefs of "Lophelia pertusa", which provide shelter to various fish species. Although these corals are widespread in many peripheral areas of the North Atlantic, they never reach such amounts and concentrations as at the Norwegian continental slopes. However, they are at risk due to increasing trawling, which mechanically destroys the coral reefs.

The Norwegian coastal waters are the most important spawning ground of the herring populations of the North Atlantic, and the hatching occurs in March. The eggs float to the surface and are washed off the coast by the northward current. Whereas a small herring population remains in the fjords and along the northern Norwegian coast, the majority spends the summer in the Barents Sea, where it feeds on the rich plankton. Upon reaching puberty, herring returns to the Norwegian Sea. The herring stock varies greatly between years. It increased in the 1920s owing to the milder climate and then collapsed in the following decades until 1970; the decrease was, however, at least partly caused by overfishing. The biomass of young hatched herring declined from 11 million tonnes in 1956 to almost zero in 1970; that affected the ecosystem not only of the Norwegian Sea but also of the Barents Sea.
Enforcement of environmental and fishing regulations has resulted in partial recovery of the herring populations since 1987. This recovery was accompanied by a decline of capelin and cod stocks. While the capelin benefited from the reduced fishing, the temperature rise in the 1980s and competition for food with the herring resulted in a near disappearance of young capelin from the Norwegian Sea. Meanwhile, the elderly capelin population was quickly fished out. This also reduced the population of cod – a major predator of capelin – as the herring was still too small in numbers to replace the capelin in the cod's diet.

Blue whiting ("Micromesistius poutassou") has benefited from the decline of the herring and capelin stocks as it assumed the role of major predator of plankton. The blue whiting spawns near the British Isles. The sea currents carry their eggs to the Norwegian Sea, and the adults also swim there to benefit from the food supply. The young spend the summer and the winter until February in Norwegian coastal waters and then return to the warmer waters west of Scotland. The Norwegian Arctic cod mostly occurs in the Barents Sea and at the Svalbard Archipelago. In the rest of the Norwegian Sea, it is found only during the reproduction season, at the Lofoten Islands, whereas "Pollachius virens" and haddock spawn in the coastal waters. Mackerel is an important commercial fish. The coral reefs are populated by different species of the genus "Sebastes".

Significant numbers of minke, humpback, sei, and orca whales are present in the Norwegian Sea, and white-beaked dolphins occur in the coastal waters. Orcas and some other whales visit the sea in the summer months for feeding; their population is closely related to the herring stocks, and they follow the herring schools within the sea. With a total population of about 110,000, minke whales are by far the most common whales in the sea. They are hunted by Norway and Iceland, with a quota of about 1,000 per year in Norway. In contrast to the past, nowadays primarily their meat is consumed, rather than fat and oil.

The bowhead whale used to be a major plankton predator, but it almost disappeared from the Norwegian Sea after intense whaling in the 19th century, and was temporarily extinct in the entire North Atlantic. Similarly, the blue whale used to form large groups between Jan Mayen and Spitsbergen, but is hardly present nowadays. Observations of northern bottlenose whales in the Norwegian Sea are rare. Other large animals of the sea are hooded and harp seals and squid.

Important waterfowl species of the Norwegian Sea are puffin, kittiwake and guillemot. Puffins and guillemots also suffered from the collapse of the herring population, especially the puffins on the Lofoten Islands. The latter hardly had an alternative to herring and their population was approximately halved between 1969 and 1987.

Norway, Iceland, and Denmark/Faroe Islands share the territorial waters of the Norwegian Sea, with the largest part belonging to the first. Norway has claimed twelve-mile limit as territorial waters since 2004 and an exclusive economic zone of 200 miles since 1976. Consequently, due to the Norwegian islands of Svalbard and Jan Mayen, the southeast, northeast and northwest edge of the sea fall within Norway. The southwest border is shared between Iceland and Denmark/Faroe Islands.

The largest damage to the Norwegian Sea was caused by extensive fishing, whaling, and pollution. The British nuclear complex of Sellafield is one of the greatest polluters, discharging radioactive waste into the sea. Other contamination is mostly by oil and toxic substances, but also from the great number of ships sunk during the two world wars. The environmental protection of the Norwegian Sea is mainly regulated by the OSPAR Convention.

Fishing has been practised near the Lofoten archipelago for hundreds of years. The coastal waters of the remote Lofoten islands are one of the richest fishing areas in Europe, as most of the Atlantic cod swims to the coastal waters of Lofoten in the winter to spawn. So in the 19th century, dried cod was one of Norway's main exports and by far the most important industry in northern Norway. Strong sea currents, maelstroms, and especially frequent storms made fishing a dangerous occupation: several hundred men died on the "Fatal Monday" in March 1821, 300 of them from a single parish, and about a hundred boats with their crews were lost within a short time in April 1875.

Whaling was also important for the Norwegian Sea. In the early 1600s, the Englishman Stephen Bennet started hunting walrus at Bear Island. In May 1607 the Muscovy Company, while looking for the Northwest Passage and exploring the sea, discovered the large populations of walrus and whales in the Norwegian Sea and started hunting them in 1610 near Spitsbergen. Later in the 17th century, Dutch ships started hunting bowhead whales near Jan Mayen; the bowhead population between Svalbard and Jan Mayen was then about 25,000 individuals. Britons and Dutch were then joined by Germans, Danes, and Norwegians. Between 1615 and 1820, the waters between Jan Mayen, Svalbard, Bear Island, and Greenland, between the Norwegian, Greenland, and Barents Seas, were the most productive whaling area in the world. However, extensive hunting had wiped out the whales in that region by the early 20th century.

For many centuries, the Norwegian Sea was regarded as the edge of the known world. The disappearance of ships there, due to the natural disasters, induced legends of monsters that stopped and sank ships (kraken). As late as in 1845, the "Encyclopædia metropolitana" contained a multi-page review by Erik Pontoppidan (1698–1764) on ship-sinking sea monsters half a mile in size. Many legends might be based on the work "Historia de gentibus septentrionalibus" of 1539 by Olaus Magnus, which described the kraken and maelstroms of the Norwegian Sea. The kraken also appears in Alfred Tennyson's poem of the same name, in Herman Melville's "Moby Dick", and in "Twenty Thousand Leagues Under the Sea" by Jules Verne.

Between the Lofoten islands of Moskenesøya and Værøy, at the tiny Mosken island, lies the Moskenstraumen – a system of tidal eddies and a whirlpool called a maelstrom. With a speed on the order of (the value strongly varies between sources), it is one of the strongest maelstroms in the world. It was described in the 13th century in the Old Norse Poetic Edda and remained an attractive subject for painters and writers, including Edgar Allan Poe, Walter Moers and Jules Verne. The word was introduced into the English language by Poe in his story "A Descent into the Maelström" (1841) describing the Moskenstraumen. The Moskenstraumen is created as a result of a combination of several factors, including the tides, the position of the Lofoten, and the underwater topography; unlike most other whirlpools, it is located in the open sea rather than in a channel or bay. With a diameter of 40–50 metres, it can be dangerous even in modern times to small fishing vessels that might be attracted by the abundant cod feeding on the microorganisms sucked in by the whirlpool.

The fish-rich coastal waters of northern Norway have long been known and attracted skilled sailors from Iceland and Greenland. Thus most settlements in Iceland and Greenland were on the west coasts of the islands, which were also warmer due to the Atlantic currents. The first reasonably reliable map of northern Europe, the Carta marina of 1539, represents the Norwegian Sea as coastal waters and shows nothing north of the North Cape. The Norwegian Sea off the coast regions appeared on the maps in the 17th century as an important part of the then sought Northern Sea Route and a rich whaling ground.

Jan Mayen island was discovered in 1607 and become an important base of Dutch whalers. The Dutchman Willem Barents discovered Bear Island and Svalbard, which was then used by Russian whalers called pomors. The islands on the edge of the Norwegian Sea have been rapidly divided between nations. During the peaks of whaling, some 300 ships with 12,000 crew members were yearly visiting Svalbard.

The first depth measurements of the Norwegian Sea were performed in 1773 by Constantine Phipps aboard HMS "Racehorse", as a part of his North Pole expedition. Systematic oceanographic research in the Norwegian Sea started in the late 19th century, when declines in the yields of cod and herring off the Lofoten prompted the Norwegian government to investigate the matter. The zoologist Georg Ossian Sars and meteorologist Henrik Mohn persuaded the government in 1874 to send out a scientific expedition, and between 1876 and 1878 they explored much of the sea aboard "Vøringen". The data obtained allowed Mohn to establish the first dynamic model of ocean currents, which incorporated winds, pressure differences, sea water temperature, and salinity and agreed well with later measurements. In 2019, deposits of iron, copper, zink and cobalt were found on the Mohn Ridge, likely from hydrothermal vents.

Until the 20th century, the coasts of the Norwegian Sea were sparsely populated and therefore shipping in the sea was mostly focused on fishing, whaling, and occasional coastal transportation. Since the late 19th century, the Norwegian Coastal Express sea line has been established, connecting the more densely populated south with the north of Norway by at least one trip a day. The importance of shipping in the Norwegian Sea also increased with the expansion of the Russian and Soviet navies in the Barents Sea and development of international routes to the Atlantic through the Baltic Sea, Kattegat, Skagerrak, and North Sea.

The Norwegian Sea is ice-free and provides a direct route from the Atlantic to the Russian ports in the Arctic (Murmansk, Archangel, and Kandalaksha), which are directly linked to central Russia. This route was extensively used for supplies during World War II – of 811 US ships, 720 reached Russian ports, bringing some 4 million tonnes of cargo that included about 5,000 tanks and 7,000 aircraft. The Allies lost 18 convoys and 89 merchant ships on this route. The major operations of the German Navy against the convoys included PQ 17 in July 1942, the Battle of the Barents Sea in December 1942, and the Battle of the North Cape in December 1943 and were carried out around the border between the Norwegian Sea and Barents Sea, near the North Cape.

Navigation across the Norwegian Sea declined after World War II and intensified only in the 1960s–70s with the expansion of the Soviet Northern Fleet, which was reflected in major joint naval exercises of the Soviet Northern Baltic fleets in the Norwegian Sea. The sea was the gateway for the Soviet Navy to the Atlantic Ocean and thus to the United States, and the major Soviet port of Murmansk was just behind the border of the Norwegian and Barents Sea. The countermeasures by the NATO countries resulted in a significant naval presence in the Norwegian Sea and intense cat-and-mouse games between Soviet and NATO aircraft, ships, and especially submarines. A relic of the Cold War in the Norwegian Sea, the Soviet nuclear submarine K-278 Komsomolets, sank in 1989 southwest of Bear Island, at the border of the Norwegian and Barents seas, with radioactive material onboard that poses potential danger to flora and fauna.

The Norwegian Sea is part of the Northern Sea Route for ships from European ports to Asia. The travel distance from Rotterdam to Tokyo is via the Suez Canal and only through the Norwegian Sea. Sea ice is a common problem in the Arctic seas, but ice-free conditions along the entire northern route were observed at the end of August 2008. Russia is planning to expand its offshore oil production in the Arctic, which should increase the traffic of tankers through the Norwegian Sea to markets in Europe and America; it is expected that the number of oil shipments through the northern Norwegian Sea will increase from 166 in 2002 to 615 in 2015.

The most important products of the Norwegian Sea are no longer fish, but oil and especially gas found under the ocean floor. Norway started undersea oil production in 1993, followed by development of the Huldra gas field in 2001. The large depth and harsh waters of the Norwegian Sea pose significant technical challenges for offshore drilling. Whereas drilling at depths exceeding 500 meters has been conducted since 1995, only a few deep gas fields have been explored commercially. The most important current project is Ormen Lange (depth 800-1,100 m), where gas production started in 2007. With reserves of 1.4 cubic feet, it is the major Norwegian gas field. It is connected to the Langeled pipeline, currently the world's longest underwater pipeline, and thus to a major European gas pipeline network. Several other gas fields are being developed. A particular challenge is the Kristin field, where the temperature is as high as 170 °C and the gas pressure exceeds 900 bar (900 times the normal pressure).
Further north are Norne and Snøhvit.




</doc>
<doc id="21284" url="https://en.wikipedia.org/wiki?curid=21284" title="NMD">
NMD

NMD may refer to:


</doc>
<doc id="21285" url="https://en.wikipedia.org/wiki?curid=21285" title="Nuclear physics">
Nuclear physics

Nuclear physics is the field of physics that studies atomic nuclei and their constituents and interactions. Other forms of nuclear matter are also studied. Nuclear physics should not be confused with atomic physics, which studies the atom as a whole, including its electrons.

Discoveries in nuclear physics have led to applications in many fields. This includes nuclear power, nuclear weapons, nuclear medicine and magnetic resonance imaging, industrial and agricultural isotopes, ion implantation in materials engineering, and radiocarbon dating in geology and archaeology. Such applications are studied in the field of nuclear engineering.

Particle physics evolved out of nuclear physics and the two fields are typically taught in close association. Nuclear astrophysics, the application of nuclear physics to astrophysics, is crucial in explaining the inner workings of stars and the origin of the chemical elements.

The history of nuclear physics as a discipline distinct from atomic physics starts with the discovery of radioactivity by Henri Becquerel in 1896 while investigating phosphorescence in uranium salts. The discovery of the electron by J. J. Thomson a year later was an indication that the atom had internal structure. At the beginning of the 20th century the accepted model of the atom was J. J. Thomson's "plum pudding" model in which the atom was a positively charged ball with smaller negatively charged electrons embedded inside it.

In the years that followed, radioactivity was extensively investigated, notably by Marie Curie, Pierre Curie, Ernest Rutherford and others. By the turn of the century physicists had also discovered three types of radiation emanating from atoms, which they named alpha, beta, and gamma radiation. Experiments by Otto Hahn in 1911 and by James Chadwick in 1914 discovered that the beta decay spectrum was continuous rather than discrete. That is, electrons were ejected from the atom with a continuous range of energies, rather than the discrete amounts of energy that were observed in gamma and alpha decays. This was a problem for nuclear physics at the time, because it seemed to indicate that energy was not conserved in these decays.

The 1903 Nobel Prize in Physics was awarded jointly to Becquerel, for his discovery and to Marie and Pierre Curie for their subsequent research into radioactivity. Rutherford was awarded the Nobel Prize in Chemistry in 1908 for his "investigations into the disintegration of the elements and the chemistry of radioactive substances".

In 1905, Albert Einstein formulated the idea of mass–energy equivalence. While the work on radioactivity by Becquerel and Marie Curie predates this, an explanation of the source of the energy of radioactivity would have to wait for the discovery that the nucleus itself was composed of smaller constituents, the nucleons.

In 1906, Ernest Rutherford published "Retardation of the α Particle from Radium in passing through matter." Hans Geiger expanded on this work in a communication to the Royal Society with experiments he and Rutherford had done, passing alpha particles through air, aluminum foil and gold leaf. More work was published in 1909 by Geiger and Ernest Marsden, and further greatly expanded work was published in 1910 by Geiger. In 1911–1912 Rutherford went before the Royal Society to explain the experiments and propound the new theory of the atomic nucleus as we now understand it.

The key experiment behind this announcement was performed in 1910, at the University of Manchester: Ernest Rutherford's team performed a remarkable experiment in which Geiger and Marsden under Rutherford's supervision fired alpha particles (helium nuclei) at a thin film of gold foil. The plum pudding model had predicted that the alpha particles should come out of the foil with their trajectories being at most slightly bent. But Rutherford instructed his team to look for something that shocked him to observe: a few particles were scattered through large angles, even completely backwards in some cases. He likened it to firing a bullet at tissue paper and having it bounce off. The discovery, with Rutherford's analysis of the data in 1911, led to the Rutherford model of the atom, in which the atom had a very small, very dense nucleus containing most of its mass, and consisting of heavy positively charged particles with embedded electrons in order to balance out the charge (since the neutron was unknown). As an example, in this model (which is not the modern one) nitrogen-14 consisted of a nucleus with 14 protons and 7 electrons (21 total particles) and the nucleus was surrounded by 7 more orbiting electrons.

Around 1920, Arthur Eddington anticipated the discovery and mechanism of nuclear fusion processes in stars, in his paper "The Internal Constitution of the Stars". At that time, the source of stellar energy was a complete mystery; Eddington correctly speculated that the source was fusion of hydrogen into helium, liberating enormous energy according to Einstein's equation "E = mc". This was a particularly remarkable development since at that time fusion and thermonuclear energy, and even that stars are largely composed of hydrogen (see metallicity), had not yet been discovered.

The Rutherford model worked quite well until studies of nuclear spin were carried out by Franco Rasetti at the California Institute of Technology in 1929. By 1925 it was known that protons and electrons each had a spin of . In the Rutherford model of nitrogen-14, 20 of the total 21 nuclear particles should have paired up to cancel each other's spin, and the final odd particle should have left the nucleus with a net spin of . Rasetti discovered, however, that nitrogen-14 had a spin of 1.

In 1932 Chadwick realized that radiation that had been observed by Walther Bothe, Herbert Becker, Irène and Frédéric Joliot-Curie was actually due to a neutral particle of about the same mass as the proton, that he called the neutron (following a suggestion from Rutherford about the need for such a particle). In the same year Dmitri Ivanenko suggested that there were no electrons in the nucleus — only protons and neutrons — and that neutrons were spin particles, which explained the mass not due to protons. The neutron spin immediately solved the problem of the spin of nitrogen-14, as the one unpaired proton and one unpaired neutron in this model each contributed a spin of in the same direction, giving a final total spin of 1.

With the discovery of the neutron, scientists could at last calculate what fraction of binding energy each nucleus had, by comparing the nuclear mass with that of the protons and neutrons which composed it. Differences between nuclear masses were calculated in this way. When nuclear reactions were measured, these were found to agree with Einstein's calculation of the equivalence of mass and energy to within 1% as of 1934.

Alexandru Proca was the first to develop and report the massive vector boson field equations and a theory of the mesonic field of nuclear forces. Proca's equations were known to Wolfgang Pauli who mentioned the equations in his Nobel address, and they were also known to Yukawa, Wentzel, Taketani, Sakata, Kemmer, Heitler, and Fröhlich who appreciated the content of Proca's equations for developing a theory of the atomic nuclei in Nuclear Physics.

In 1935 Hideki Yukawa proposed the first significant theory of the strong force to explain how the nucleus holds together. In the Yukawa interaction a virtual particle, later called a meson, mediated a force between all nucleons, including protons and neutrons. This force explained why nuclei did not disintegrate under the influence of proton repulsion, and it also gave an explanation of why the attractive strong force had a more limited range than the electromagnetic repulsion between protons. Later, the discovery of the pi meson showed it to have the properties of Yukawa's particle.

With Yukawa's papers, the modern model of the atom was complete. The center of the atom contains a tight ball of neutrons and protons, which is held together by the strong nuclear force, unless it is too large. Unstable nuclei may undergo alpha decay, in which they emit an energetic helium nucleus, or beta decay, in which they eject an electron (or positron). After one of these decays the resultant nucleus may be left in an excited state, and in this case it decays to its ground state by emitting high-energy photons (gamma decay).

The study of the strong and weak nuclear forces (the latter explained by Enrico Fermi via Fermi's interaction in 1934) led physicists to collide nuclei and electrons at ever higher energies. This research became the science of particle physics, the crown jewel of which is the standard model of particle physics, which describes the strong, weak, and electromagnetic forces.

A heavy nucleus can contain hundreds of nucleons. This means that with some approximation it can be treated as a classical system, rather than a quantum-mechanical one. In the resulting liquid-drop model, the nucleus has an energy that arises partly from surface tension and partly from electrical repulsion of the protons. The liquid-drop model is able to reproduce many features of nuclei, including the general trend of binding energy with respect to mass number, as well as the phenomenon of nuclear fission.

Superimposed on this classical picture, however, are quantum-mechanical effects, which can be described using the nuclear shell model, developed in large part by Maria Goeppert Mayer and J. Hans D. Jensen. Nuclei with certain "magic" numbers of neutrons and protons are particularly stable, because their shells are filled.

Other more complicated models for the nucleus have also been proposed, such as the interacting boson model, in which pairs of neutrons and protons interact as bosons, analogously to Cooper pairs of electrons.

Ab initio methods try to solve the nuclear many-body problem from the ground up, starting from the nucleons and their interactions.

Much of current research in nuclear physics relates to the study of nuclei under extreme conditions such as high spin and excitation energy. Nuclei may also have extreme shapes (similar to that of Rugby balls or even pears) or extreme neutron-to-proton ratios. Experimenters can create such nuclei using artificially induced fusion or nucleon transfer reactions, employing ion beams from an accelerator. Beams with even higher energies can be used to create nuclei at very high temperatures, and there are signs that these experiments have produced a phase transition from normal nuclear matter to a new state, the quark–gluon plasma, in which the quarks mingle with one another, rather than being segregated in triplets as they are in neutrons and protons.

Eighty elements have at least one stable isotope which is never observed to decay, amounting to a total of about 252 stable nuclides. However, thousands of isotopes have been characterized as unstable. These "radioisotopes" decay over time scales ranging from fractions of a second to trillions of years. Plotted on a chart as a function of atomic and neutron numbers, the binding energy of the nuclides forms what is known as the valley of stability. Stable nuclides lie along the bottom of this energy valley, while increasingly unstable nuclides lie up the valley walls, that is, have weaker binding energy.

The most stable nuclei fall within certain ranges or balances of composition of neutrons and protons: too few or too many neutrons (in relation to the number of protons) will cause it to decay. For example, in beta decay, a nitrogen-16 atom (7 protons, 9 neutrons) is converted to an oxygen-16 atom (8 protons, 8 neutrons) within a few seconds of being created. In this decay a neutron in the nitrogen nucleus is converted by the weak interaction into a proton, an electron and an antineutrino. The element is transmuted to another element, with a different number of protons.

In alpha decay, which typically occurs in the heaviest nuclei, the radioactive element decays by emitting a helium nucleus (2 protons and 2 neutrons), giving another element, plus helium-4. In many cases this process continues through several steps of this kind, including other types of decays (usually beta decay) until a stable element is formed.

In gamma decay, a nucleus decays from an excited state into a lower energy state, by emitting a gamma ray. The element is not changed to another element in the process (no nuclear transmutation is involved).

Other more exotic decays are possible (see the first main article). For example, in internal conversion decay, the energy from an excited nucleus may eject one of the inner orbital electrons from the atom, in a process which produces high speed electrons but is not beta decay and (unlike beta decay) does not transmute one element to another.

In nuclear fusion, two low-mass nuclei come into very close contact with each other so that the strong force fuses them. It requires a large amount of energy for the strong or nuclear forces to overcome the electrical repulsion between the nuclei in order to fuse them; therefore nuclear fusion can only take place at very high temperatures or high pressures. When nuclei fuse, a very large amount of energy is released and the combined nucleus assumes a lower energy level. The binding energy per nucleon increases with mass number up to nickel-62. Stars like the Sun are powered by the fusion of four protons into a helium nucleus, two positrons, and two neutrinos. The uncontrolled fusion of hydrogen into helium is known as thermonuclear runaway. A frontier in current research at various institutions, for example the Joint European Torus (JET) and ITER, is the development of an economically viable method of using energy from a controlled fusion reaction. Nuclear fusion is the origin of the energy (including in the form of light and other electromagnetic radiation) produced by the core of all stars including our own Sun.

Nuclear fission is the reverse process to fusion. For nuclei heavier than nickel-62 the binding energy per nucleon decreases with the mass number. It is therefore possible for energy to be released if a heavy nucleus breaks apart into two lighter ones.

The process of alpha decay is in essence a special type of spontaneous nuclear fission. It is a highly asymmetrical fission because the four particles which make up the alpha particle are especially tightly bound to each other, making production of this nucleus in fission particularly likely.

From certain of the heaviest nuclei whose fission produces free neutrons, and which also easily absorb neutrons to initiate fission, a self-igniting type of neutron-initiated fission can be obtained, in a chain reaction. Chain reactions were known in chemistry before physics, and in fact many familiar processes like fires and chemical explosions are chemical chain reactions. The fission or "nuclear" chain-reaction, using fission-produced neutrons, is the source of energy for nuclear power plants and fission-type nuclear bombs, such as those detonated in Hiroshima and Nagasaki, Japan, at the end of World War II. Heavy nuclei such as uranium and thorium may also undergo spontaneous fission, but they are much more likely to undergo decay by alpha decay.

For a neutron-initiated chain reaction to occur, there must be a critical mass of the relevant isotope present in a certain space under certain conditions. The conditions for the smallest critical mass require the conservation of the emitted neutrons and also their slowing or moderation so that there is a greater cross-section or probability of them initiating another fission. In two regions of Oklo, Gabon, Africa, natural nuclear fission reactors were active over 1.5 billion years ago. Measurements of natural neutrino emission have demonstrated that around half of the heat emanating from the Earth's core results from radioactive decay. However, it is not known if any of this results from fission chain reactions.

According to the theory, as the Universe cooled after the Big Bang it eventually became possible for common subatomic particles as we know them (neutrons, protons and electrons) to exist. The most common particles created in the Big Bang which are still easily observable to us today were protons and electrons (in equal numbers). The protons would eventually form hydrogen atoms. Almost all the neutrons created in the Big Bang were absorbed into helium-4 in the first three minutes after the Big Bang, and this helium accounts for most of the helium in the universe today (see Big Bang nucleosynthesis).

Some relatively small quantities of elements beyond helium (lithium, beryllium, and perhaps some boron) were created in the Big Bang, as the protons and neutrons collided with each other, but all of the "heavier elements" (carbon, element number 6, and elements of greater atomic number) that we see today, were created inside stars during a series of fusion stages, such as the proton-proton chain, the CNO cycle and the triple-alpha process. Progressively heavier elements are created during the evolution of a star.

Since the binding energy per nucleon peaks around iron (56 nucleons), energy is only released in fusion processes involving smaller atoms than that. Since the creation of heavier nuclei by fusion requires energy, nature resorts to the process of neutron capture. Neutrons (due to their lack of charge) are readily absorbed by a nucleus. The heavy elements are created by either a "slow" neutron capture process (the so-called "s"-process) or the "rapid", or "r"-process. The "s" process occurs in thermally pulsing stars (called AGB, or asymptotic giant branch stars) and takes hundreds to thousands of years to reach the heaviest elements of lead and bismuth. The "r"-process is thought to occur in supernova explosions, which provide the necessary conditions of high temperature, high neutron flux and ejected matter. These stellar conditions make the successive neutron captures very fast, involving very neutron-rich species which then beta-decay to heavier elements, especially at the so-called waiting points that correspond to more stable nuclides with closed neutron shells (magic numbers).





</doc>
<doc id="21287" url="https://en.wikipedia.org/wiki?curid=21287" title="Nuremberg">
Nuremberg

Nuremberg ( ; ; in the local East Franconian dialect: ) is the second-largest city of the German federal state of Bavaria after its capital Munich, and its 511,628 (2016) inhabitants make it the 14th largest city in Germany. On the Pegnitz River (from its confluence with the Rednitz in Fürth onwards: Regnitz, a tributary of the River Main) and the Rhine–Main–Danube Canal, it lies in the Bavarian administrative region of Middle Franconia, and is the largest city and the unofficial capital of Franconia. Nuremberg forms a continuous conurbation with the neighbouring cities of Fürth, Erlangen and Schwabach with a total population of 798,867 (2018), while the larger Nuremberg Metropolitan Region has approximately 3.6 million inhabitants. The city lies about north of Munich. It is the largest city in the East Franconian dialect area (colloquially: "Franconian"; ), Nuremberg was one of the host cities of the 2006 FIFA World Cup.

There are many institutions of higher education in the city, including the University of Erlangen-Nuremberg (). With 39,780 students in 2017, it is Bavaria's third-largest and Germany's 11th-largest university, with campuses in Erlangen and Nuremberg and a university hospital in Erlangen (Universitätsklinikum Erlangen). and are also located within the city. Nuremberg Airport () is the second-busiest airport in Bavaria after Munich Airport, and the tenth-busiest airport in Germany.

Nuremberg was the site of major Nazi rallies, and it provided the site for the Nuremberg trials, which held to account many major Nazi officials.

The first documentary mention of the city, in 1050, mentions Nuremberg as the location of an Imperial castle between the East Franks and the Bavarian March of the Nordgau. From 1050 to 1571 the city expanded and rose dramatically in importance due to its location on key trade-routes. King Conrad III (reigning as King of Germany from 1138 to 1152) established the Burgraviate of Nuremberg, with the first burgraves coming from the Austrian House of Raab. With the extinction of their male line around 1190, the last Raabs count's son-in-law, Frederick I from the House of Hohenzollern, inherited the burgraviate in 1192.

From the late 12th century to the Interregnum (1254–73), however, the power of the burgraves diminished as the Hohenstaufen emperors transferred most non-military powers to a castellan, with the city administration and the municipal courts handed over to an Imperial mayor () from 1173/74. The strained relations between the burgraves and the castellans, with gradual transferral of powers to the latter in the late 14th and early 15th centuries, finally broke out into open enmity, which greatly influenced the history of the city.
Nuremberg is often referred to as the "unofficial capital" of the Holy Roman Empire, particularly because the Imperial Diet ("Reichstag") and courts met at Nuremberg Castle. The Diets of Nuremberg played an important role in the administration of the empire. The increasing demands of the Imperial court and the increasing importance of the city attracted increased trade and commerce in Nuremberg. In 1219 Emperor Frederick II granted the ('Great Letter of Freedom'), including town rights, Imperial immediacy (), the privilege to mint coins, and an independent customs policy - almost wholly removing the city from the purview of the burgraves. Nuremberg soon became, with Augsburg, one of the two great trade-centers on the route from Italy to Northern Europe.

In 1298 the Jews of the town were falsely accused of having desecrated the host, and 698 of them were killed in one of the many Rintfleisch massacres. Behind the massacre of 1298 was also the desire to combine the northern and southern parts of the city, which were divided by the Pegnitz. The Jews of the German lands suffered many massacres during the plague pandemic of the mid-14th century.

In 1349 Nuremberg's Jews suffered a pogrom. They were burned at the stake or expelled, and a marketplace was built over the former Jewish quarter. The plague returned to the city in 1405, 1435, 1437, 1482, 1494, 1520 and 1534.
The largest growth of Nuremberg occurred in the 14th century. Charles IV's Golden Bull of 1356, naming Nuremberg as the city where newly elected kings of Germany must hold their first Imperial Diet, made Nuremberg one of the three most important cities of the Empire. Charles was the patron of the Frauenkirche, built between 1352 and 1362 (the architect was likely Peter Parler), where the Imperial court worshipped during its stays in Nuremberg. The royal and Imperial connection grew stronger in 1423 when the Holy Roman Emperor Sigismund of Luxembourg granted the Imperial regalia to be kept permanently in Nuremberg, where they remained until 1796, when the advance of French troops required their removal to Regensburg and thence to Vienna.

In 1349 the members of the guilds unsuccessfully rebelled against the patricians in a ('Craftsmen's Uprising'), supported by merchants and some by councillors, leading to a ban on any self-organisation of the artisans in the city, abolishing the guilds that were customary elsewhere in Europe; the unions were then dissolved, and the oligarchs remained in power while Nuremberg was a free city (until the early-19th century). Charles IV conferred upon the city the right to conclude alliances independently, thereby placing it upon a politically equal footing with the princes of the Empire. Frequent fights took place with the burgraves - without, however, inflicting lasting damage upon the city. After fire destroyed the castle in 1420 during a feud between Frederick IV (from 1417 Margrave of Brandenburg) and the duke of Bavaria-Ingolstadt, the city purchased the ruins and the forest belonging to the castle (1427), resulting in the city's total sovereignty within its borders.

Through these and other acquisitions the city accumulated considerable territory. The Hussite Wars (1419-1434), a recurrence of the Black Death in 1437, and the First Margrave War (1449-1450) led to a severe fall in population in the mid-15th century. Siding with Albert IV, Duke of Bavaria-Munich, in the Landshut War of Succession of 1503-1505 led the city to gain substantial territory, resulting in lands of , making it one of the largest Imperial cities.

During the Middle Ages, Nuremberg fostered a rich, varied, and influential literary culture.

The cultural flowering of Nuremberg in the 15th and 16th centuries made it the centre of the German Renaissance. In 1525 Nuremberg accepted the Protestant Reformation, and in 1532 the Nuremberg Religious Peace was signed there, preventing war between Lutherans and Catholics for 15 years. During the Princes' 1552 revolution against Charles V, Nuremberg tried to purchase its neutrality, but Margrave Albert Alcibiades, one of the leaders of the revolt, attacked the city without a declaration of war and dictated a disadvantageous peace. At the 1555 Peace of Augsburg, the possessions of the Protestants were confirmed by the Emperor, their religious privileges extended and their independence from the Bishop of Bamberg affirmed, while the 1520s' secularisation of the monasteries was also approved. Families like the Tucher, Imhoff or Haller run trading businesses across Europe, similar to the Fugger and Welser families from Augsburg, although on a slightly smaller scale.
The state of affairs in the early 16th century, increased trade routes elsewhere and the ossification of the social hierarchy and legal structures contributed to the decline in trade. During the Thirty Years' War, frequent quartering of Imperial, Swedish and League soldiers, the financial costs of the war and the cessation of trade caused irreparable damage to the city and a near-halving of the population. In 1632, the city, occupied by the forces of Gustavus Adolphus of Sweden, was besieged by the army of Imperial general Albrecht von Wallenstein. The city declined after the war and recovered its importance only in the 19th century, when it grew as an industrial centre. Even after the Thirty Years' War, however, there was a late flowering of architecture and culture – secular Baroque architecture is exemplified in the layout of the civic gardens built outside the city walls, and in the Protestant city's rebuilding of St. Egidien church, destroyed by fire at the beginning of the 18th century, considered a significant contribution to the baroque church architecture of Middle Franconia.

After the Thirty Years' War, Nuremberg attempted to remain detached from external affairs, but contributions were demanded for the War of the Austrian Succession and the Seven Years' War and restrictions of imports and exports deprived the city of many markets for its manufactures. The Bavarian elector, Charles Theodore, appropriated part of the land obtained by the city during the Landshut War of Succession, to which Bavaria had maintained its claim; Prussia also claimed part of the territory. Realising its weakness, the city asked to be incorporated into Prussia but Frederick William II refused, fearing to offend Austria, Russia and France. At the Imperial diet in 1803, the independence of Nuremberg was affirmed, but on the signing of the Confederation of the Rhine on 12 July 1806, it was agreed to hand the city over to Bavaria from 8 September, with Bavaria guaranteeing the amortisation of the city's 12.5 million guilder public debt.

After the fall of Napoleon, the city's trade and commerce revived; the skill of its inhabitants together with its favourable situation soon made the city prosperous, particularly after its public debt had been acknowledged as a part of the Bavarian national debt. Having been incorporated into a Catholic country, the city was compelled to refrain from further discrimination against Catholics, who had been excluded from the rights of citizenship. Catholic services had been celebrated in the city by the priests of the Teutonic Order, often under great difficulties. After their possessions had been confiscated by the Bavarian government in 1806, they were given the Frauenkirche on the Market in 1809; in 1810 the first Catholic parish was established, which in 1818 numbered 1,010 souls.

In 1817, the city was incorporated into the district of Rezatkreis (named for the river Franconian Rezat), which was renamed to Middle Franconia () on 1 January 1838. The first German railway, the Bavarian Ludwigsbahn, from Nuremberg to nearby Fürth, was opened in 1835. The establishment of railways and the incorporation of Bavaria into Zollverein (the 19th-century German Customs Union), commerce and industry opened the way to greater prosperity. In 1852, there were 53,638 inhabitants: 46,441 Protestants and 6,616 Catholics. It subsequently grew to become the more important industrial city of Southern Germany, one of the most prosperous towns of southern Germany, but after the Austro-Prussian War it was given to the Prussia as part of their telegraph stations they had to give up. In 1905, its population, including several incorporated suburbs, was 291,351: 86,943 Catholics, 196,913 Protestants, 3,738 Jews and 3,766 members of other creeds.

Nuremberg held great significance during the Nazi Germany era. Because of the city's relevance to the Holy Roman Empire and its position in the centre of Germany, the Nazi Party chose the city to be the site of huge Nazi Party conventions — the Nuremberg rallies. The rallies were held in 1927, 1929 and annually from 1933 through 1938. After Adolf Hitler's rise to power in 1933 the Nuremberg rallies became huge Nazi propaganda events, a centre of Nazi ideals. The 1934 rally was filmed by Leni Riefenstahl, and made into a propaganda film called "Triumph des Willens" ("Triumph of the Will").

At the 1935 rally, Hitler specifically ordered the Reichstag to convene at Nuremberg to pass the Nuremberg Laws which revoked German citizenship for all Jews and other non-Aryans. A number of premises were constructed solely for these assemblies, some of which were not finished. Today many examples of Nazi architecture can still be seen in the city. The city was also the home of the Nazi propagandist Julius Streicher, the publisher of "Der Stürmer".
During the Second World War, Nuremberg was the headquarters of "Wehrkreis" (military district) XIII, and an important site for military production, including aircraft, submarines and tank engines. A subcamp of Flossenbürg concentration camp was located here, and extensively used slave labour. The city was severely damaged in Allied strategic bombing from 1943 to 1945. On 29 March 1944, the RAF endured its heaviest losses in the bombing campaign of Germany. Out of more than 700 planes participating, 106 were shot down or crash-landed on the way home to their bases, and more than 700 men were missing, as many as 545 of them dead. More than 160 became prisoners of war.

On 2 January 1945, the medieval city centre was systematically bombed by the Royal Air Force and the U.S. Army Air Forces and about ninety percent of it was destroyed in only one hour, with 1,800 residents killed and roughly 100,000 displaced. In February 1945, additional attacks followed. In total, about 6,000 Nuremberg residents are estimated to have been killed in air raids.

Nuremberg was a heavily fortified city that was captured in a fierce battle lasting from 17 to 21 April 1945 by the U.S. 3rd Infantry Division, 42nd Infantry Division and 45th Infantry Division, which fought house-to-house and street-by-street against determined German resistance, causing further urban devastation to the already bombed and shelled buildings. Despite this intense degree of destruction, the city was rebuilt after the war and was to some extent restored to its pre-war appearance, including the reconstruction of some of its medieval buildings. Much of this reconstructive work and conservation was done by the organisation 'Old Town Friends Nuremberg'. However, over half of the historic look of the center, and especially the northeastern half of the old Imperial Free City was not restored.

Between 1945 and 1946, German officials involved in war crimes and crimes against humanity were brought before an international tribunal in the Nuremberg trials. The Soviet Union had wanted these trials to take place in Berlin. However, Nuremberg was chosen as the site for the trials for specific reasons:

Following the trials, in October 1946, several prominent German Nazi politicians and military leaders were executed in Nuremberg.

The same courtroom in Nuremberg was the venue of the Nuremberg Military Tribunals, organized by the United States as occupying power in the area.

Several old villages now belong to the city, for example Grossgründlach, Kraftshof, Thon, and Neunhof in the north-west; Ziegelstein in the northeast, Altenfurt and Fischbach in the south-east; and Katzwang, Kornburg in the south. Langwasser is a modern suburb.

Nuremberg has an oceanic climate (Köppen: "Cfb") with a certain humid continental influence ("Dfb"), categorized in the latter by the 0 °C isotherm. The city's climate is influenced by its inland position and higher altitude. Winters are changeable, with either mild or cold weather: the average temperature is around to , while summers are generally warm, mostly around at night to in the afternoon. Precipitation is evenly spread throughout the year, although February and April tend to be a bit drier whereas July tends to have more rainfall.

Nuremberg has been a destination for immigrants. 39.5% of the residents had an immigrant background in 2010 (counted with MigraPro).

Nuremberg for many people is still associated with its traditional gingerbread ("Lebkuchen") products, sausages, and handmade toys. Pocket watches — "Nuremberg eggs" — were made here in the 16th century by Peter Henlein. Only one of the districts in the 1797-1801 sample was early industrial; the economic structure of the region around Nuremberg was dominated by metal and glass manufacturing, reflected by a share of nearly 50% handicrafts and workers. In the 19th century Nuremberg became the "industrial heart" of Bavaria with companies such as Siemens and MAN establishing a strong base in the city. Nuremberg is still an important industrial centre with a strong standing in the markets of Central and Eastern Europe. Items manufactured in the area include electrical equipment, mechanical and optical products, motor vehicles, writing and drawing paraphernalia, stationery products and printed materials.

The city is also strong in the fields of automation, energy and medical technology. Siemens is still the largest industrial employer in the Nuremberg region but a good third of German market research agencies are also located in the city.

The Nuremberg International Toy Fair, held at the city's exhibition centre is the largest of its kind in the world.

Nuremberg is Bavaria’s second largest city after Munich, and a popular tourist destination for foreigners and Germans alike. It was a leading city 500 years ago, but 90% of the town was destroyed in 1945 during the war. After World War II, many medieval-style areas of the town were rebuilt.

Beyond its main attractions of the Imperial Castle, St. Lorenz Church, and Nazi Trial grounds, there are 54 different museums for arts and culture, history, science and technology, family and children, and more niche categories, where visitors can see the world's oldest globe (built in 1492), a 500-year-old Madonna, and Renaissance-era German art. There are several types of tours offered in the city, including historic tours, those that are Nazi-focused, underground and night tours, walking tours, sightseeing buses, self guided tours, and an old town tour on a mini train. Nuremberg also offers several parks and green areas, as well as indoor activities such as bowling, rock wall climbing, escape rooms, cart racing, and mini golf, theaters and cinemas, pools and thermal spas. There are also six nearby amusement parks. The city's tourism board sells the Nurnberg Card which allows for free use of public transportation and free entry to all museums and attractions in Nuremberg for a two-day period.

Nuremberg is also a destination for food lovers. Culinary tourists can taste the city's famous lebkuchen, gingerbread, local beer, and Nürnberger Rostbratwürstchen, or Nuremberg sausages. There are hundreds of restaurants for all tastes, including traditional franconian restaurants and beer gardens. Also offers 17 vegan and vegetarian restaurants, seven fully organic restaurants. Nuremberg also boasts a two Michelin Star rated restaurant, Essigbrätlein.

Like many European cities, Nuremberg offers a pedestrian-only zone covering a large portion of the old town, which is a main destination for shopping and specialty retail, including year-round Christmas stores where tourists and locals alike can purchase Christmas ornaments, gifts, decorations, and additions to their toy Christmas villages. The Craftsmen's Courtyard, or Handwerkerhof, is another tourist shopping destination in the style of a medieval village. It houses several local family-run businesses which sell handcrafted items from glass, wood, leather, pottery, and precious metals. The Handwerkerhof is also home to traditional German restaurants and beer gardens.

The Pedestrian zones of Nuremberg host festivals and markets throughout the year, most well known being Christkindlesmarkt, Germany's largest Christmas market and the gingerbread capital of the world. Visitors to the Christmas market can peruse the hundreds of stalls and purchase local wood crafts, nutcrackers, smokers, and prune people, while sampling Christmas sweets and traditional gluhwein.

In 2017, Nuremberg saw a total of 3.3 million overnight stays, a record for the town, and is expected to have surpassed that in 2018, with more growth in tourism anticipated in the coming years. There are over 175 registered places of accommodation in Nuremberg, ranging from hostels to luxury hotels, bed and breakfasts, to multi-hundred room properties. As of 19 April 2019, Nuremberg had 306 AirBnB listings.

Nuremberg was an early centre of humanism, science, printing, and mechanical invention. The city contributed much to the science of astronomy. In 1471 Johannes Mueller of Königsberg (Bavaria), later called Regiomontanus, built an astronomical observatory in Nuremberg and published many important astronomical charts.

In 1515, Albrecht Dürer, a native of Nuremberg, created woodcuts of the first maps of the stars of the northern and southern hemispheres, producing the first printed star charts, which had been ordered by Johannes Stabius. Around 1515 Dürer also published the "Stabiussche Weltkarte", the first perspective drawing of the terrestrial globe.

Printers and publishers have a long history in Nuremberg. Many of these publishers worked with well-known artists of the day to produce books that could also be considered works of art. In 1470 Anton Koberger opened Europe's first print shop in Nuremberg. In 1493, he published the "Nuremberg Chronicles", also known as the "World Chronicles" ("Schedelsche Weltchronik"), an illustrated history of the world from the creation to the present day. It was written in the local Franconian dialect by Hartmann Schedel and had illustrations by Michael Wohlgemuth, Wilhelm Pleydenwurff, and Albrecht Dürer. Others furthered geographical knowledge and travel by map making. Notable among these was navigator and geographer Martin Behaim, who made the first world globe.

Sculptors such as Veit Stoss, Adam Kraft and Peter Vischer are also associated with Nuremberg.

Composed of prosperous artisans, the guilds of the Meistersingers flourished here. Richard Wagner made their most famous member, Hans Sachs, the hero of his opera "Die Meistersinger von Nürnberg". Baroque composer Johann Pachelbel was born here and was organist of St. Sebaldus Church.

The academy of fine arts situated in Nuremberg is the oldest art academy in central Europe and looks back to a tradition of 350 years of artistic education.

Nuremberg is also famous for its Christkindlesmarkt (Christmas market), which draws well over a million shoppers each year. The market is famous for its handmade ornaments and delicacies.


The Nuremberg State Theatre, founded in 1906, is dedicated to all types of opera, ballet and stage theatre. During the season 2009/2010, the theatre presented 651 performances for an audience of 240,000 persons. The State Philharmonic Nuremberg (Staatsphilharmonie Nürnberg) is the orchestra of the State Theatre. Its name was changed in 2011 from its previous name: The Nuremberg Philharmonic ("Nürnberger Philharmoniker"). It is the second-largest opera orchestra in Bavaria. Besides opera performances, it also presents its own subscription concert series in the Meistersingerhalle. Christof Perick was the principal conductor of the orchestra between 2006–2011. Marcus Bosch heads the orchestra since September 2011 .

The Nuremberg Symphony Orchestra ("Nürnberger Symphoniker") performs around 100 concerts a year to a combined annual audience of more than 180,000. The regular subscription concert series are mostly performed in the "Meistersingerhalle" but other venues are used as well, including the new concert hall of the "Kongresshalle" and the "Serenadenhof". Alexander Shelley has been the principal conductor of the orchestra since 2009.

The Nuremberg International Chamber Music Festival ("Internationales Kammermusikfestival Nürnberg") takes place in early September each year, and in 2011 celebrated its tenth anniversary. Concerts take place around the city; opening and closing events are held in the medieval "Burg". The Bardentreffen, an annual folk festival in Nuremberg, has been deemed the largest world music festival in Germany and takes place since 1976. 2014 the Bardentreffen starred 368 artists from 31 nations.

Nuremberg is known for Nürnberger Bratwurst, which is shorter and thinner than other bratwurst sausages.

Another Nuremberg speciality is Nürnberger Lebkuchen, a kind of gingerbread eaten mainly around Christmas time.

Nuremberg offers 51 public and 6 private elementary schools in nearly all of its districts. Secondary education is offered at 23 Mittelschulen, 12 Realschulen, and 17 Gymnasien (state, city, church, and privately owned). There are also several other providers of secondary education such as Berufsschule, Berufsfachschule, Wirtschaftsschule etc.

Nuremberg hosts the joint university Friedrich-Alexander-Universität Erlangen-Nürnberg, two Fachhochschulen (Technische Hochschule Nürnberg and "Evangelische Hochschule Nürnberg"), an art school ("Akademie der Bildenden Künste Nürnberg"), and a music conservatoire (Hochschule für Musik Nürnberg). There are also private schools such as the "Akademie Deutsche POP Nürnberg" offering higher education.


The city's location next to numerous highways, railways, and a waterway has contributed to its rising importance for trade with Eastern Europe.

Nürnberg Hauptbahnhof is a stop for IC and ICE trains on the German long-distance railway network. The Nuremberg–Ingolstadt–Munich high-speed line with operation opened 28 May 2006, and was fully integrated into the rail schedule on 10 December 2006. Travel times to Munich have been reduced to as little as one hour. The Nuremberg–Erfurt high-speed railway opened in December 2017.

The Nuremberg tramway network was opened in 1881. , it extended a total length of , had six lines, and carried 39.152 million passengers annually. The first segment of the Nuremberg U-Bahn metro system was opened in 1972. Nuremberg's trams, buses and metro system are operated by the VAG Nürnberg ("Verkehrsaktiengesellschaft Nürnberg" or Nuremberg Transport Corporation), itself a member of the VGN ("Verkehrsverbund Grossraum Nürnberg" or Greater Nuremberg Transport Network).

There is also a Nuremberg S-Bahn suburban metro railway and a regional train network, both centred on Nürnberg Hauptbahnhof.
Since 2008, Nuremberg has had the first U-Bahn in Germany (U2/U21 and U3) that works without a driver. It also was the first subway system worldwide in which both driver-operated trains and computer-controlled trains shared tracks.

Nuremberg is located at the junction of several important Autobahn routes. The A3 ("Netherlands"–Frankfurt–Würzburg–"Vienna") passes in a south-easterly direction along the north-east of the city. The A9 (Berlin–Munich) passes in a north–south direction on the east of the city. The A6 ("France"–Saarbrücken–"Prague") passes in an east–west direction to the south of the city. Finally, the A73 begins in the south-east of Nuremberg and travels north-west through the city before continuing towards Fürth and Bamberg.

Nuremberg Airport has flights to major German cities and many European destinations. The largest operators are currently Eurowings and TUI fly Deutschland, while the low-cost Ryanair and Wizz Air companies connect the city to various European centres. A significant amount of the airport's traffic flies to and from mainly touristic destinations during the peak winter season. The airport (Flughafen) is the terminus of subway line 2; it is the only airport in Germany served by a subway.

Nuremberg is an important port on the Rhine–Main–Danube Canal.

1. FC Nürnberg, known locally as "Der Club" (English: "The Club"), was founded in 1900 and currently plays in the Bundesliga. The official colours of the association are red and white, but the traditional colours are red and black. They won their first regional title in the Southern German championship in 1916 closely followed by their first national title in 1920. Besides the eleven regional championships they won they were the German championship for a total of seven times. With this they held the record for the most German championship titles until 1986 when the current record holder FC Bayern München surpassed them. The current chairmen are Nils Rossow and Robert Palikuca. They play in Max-Morlock-Stadion which was refurbished for the 2006 FIFA World Cup and accommodates 50,000 spectators.


TuS Bar Kochba is a league that was founded in 1913 as a social-sport club for the Jewish community in Nürnberg. Established as the "Jewish Gymnastics and Sports Club Nuremberg", the league was dissolved by the Nazi party in 1939. It was reformed in 1966. The club plays in the senior A-league of the Bavarian Football Association.

The "SELLBYTEL Baskets Nürnberg" played in the Basketball Bundesliga from 2005 to 2007. Since then, teams from Nuremberg have attempted to return to Germany's elite league. The recently founded Nürnberg Falcons BC have already established themselves as one of the main teams in Germany's second division ProA and aim to take on the heritage of the SELLBYTEL Baskets Nürnberg. The Falcons play their home games at the "Halle im Berufsbildungszentrum (BBZ)".

Nuremberg is twinned with:

Apart from the official twin towns (sister cities), there are a number with which Nuremberg maintains "cordial relations":
Nuremberg also engages in cooperation with various other cities internationally:










</doc>
<doc id="21288" url="https://en.wikipedia.org/wiki?curid=21288" title="Nicholas Lemann">
Nicholas Lemann

Nicholas Berthelot Lemann is the Joseph Pulitzer II and Edith Pulitzer Moore Professor of Journalism and Dean Emeritus of the Faculty of Journalism at the Columbia University Graduate School of Journalism. He has been a staff writer at "The New Yorker" since 1999. 

Nicholas Lemann was born, raised, and educated in a Jewish family in New Orleans. He describes his family's faith as a "kind of super-Reform Judaism" where there were "no kosher laws, no bar mitzvahs, no tallit, no kippot".

Lemann was educated at Metairie Park Country Day School, a private school in New Orleans, from which he graduated in 1972, followed by Harvard College, where he studied American history and literature, and was president of "The Harvard Crimson", where he wrote the "Brass Tacks" column, and from which he graduated "magna cum laude" in 1976.

Lemann began his journalism career as a 17-year-old writer for an alternative weekly newspaper, the "Vieux Carre Courier", in his home city of New Orleans. In 1975, amid reports of mass murder in Cambodia by the Khmer Rouge, Lemann wrote, "I continue to support the Khmer Rouge in its principles and goals but I have to admit that I deplore the way they are going about it." After graduation, he worked at the "Washington Monthly", as an associate editor and then managing editor; at "Texas Monthly", as an associate editor and then executive editor; at "The Washington Post", as a member of the national staff; at "The Atlantic Monthly", as national correspondent; and at "The New Yorker", as staff writer and then Washington correspondent.

On September 1, 2003, Lemann became dean of the Graduate School of Journalism at Columbia University. During Lemann's time as dean, the Journalism School launched and completed its first capital fundraising campaign, added 20 members to its full-time faculty, built a student center, started its first new professional degree program since the 1930s, and launched initiatives in investigative reporting, digital journalism, executive leadership for news organizations, and other areas. He stepped down as dean in 2013, following two five-year terms.

In 2015, Lemann launched "Columbia Global Reports", a university-funded publishing imprint that produces four to six ambitious works of journalism and analysis a year, each on a different underreported story in the world.

Lemann has published five books, most recently "Redemption: The Last Battle of the Civil War" (2006); "The Big Test: The Secret History of the American Meritocracy" (1999); and "The Promised Land: The Great Black Migration and How It Changed America" (1991), which won several book prizes. He has written widely for such publications as "The New York Times", "The New York Review of Books", "The New Republic", and "Slate"; worked in documentary television with Blackside, Inc., "Frontline", the Discovery Channel, and the BBC; and lectured at many universities.

Lemann serves on the boards of directors of the Authors Guild, the National Academy of Sciences’ Division of Behavioral and Social Sciences and Education, and the Academy of Political Science, and is a member of the New York Institute for the Humanities. He was named a fellow of the American Academy of Arts and Sciences in April 2010.

Lemann has been married twice. His first wife was Dominique Alice Browning, who later became an editor in chief of "House & Garden" until 2007; they married on May 20, 1983, have two sons, Alexander and Theodore, and later divorced. His second wife is Judith Anne Shulevitz, who was a columnist for "Slate" "The New York Times Book Review," and "The New Republic" and the cousin of noted independent filmmaker Robert Shulevitz; married on November 7, 1999, they have a son and a daughter.






</doc>
<doc id="21289" url="https://en.wikipedia.org/wiki?curid=21289" title="Nautical mile">
Nautical mile

A nautical mile is a unit of measurement used in air, marine, and space navigation, and for the definition of territorial waters. Historically, it was defined as one minute ( of a degree) of latitude along any line of longitude. Today the international nautical mile is defined as exactly 1852 metres (about 1.15 miles). The derived unit of speed is the knot, one nautical mile per hour.

There is no single internationally agreed symbol, with several symbols in use.

The word mile is from the Latin word for a thousand paces: mille passus. Navigation at sea was done by eye until around 1500 when navigational instruments were developed and cartographers began using a coordinate system with parallels of latitude and meridians of longitude.

By the late 16th century, Englishmen knew that the ratio of distances at sea to degrees were constant along any great circle such as the equator or any meridian, assuming that Earth was a sphere. Robert Hues wrote in 1594 that the distance along a great circle was 60 miles per degree, that is, one nautical mile per arcminute. Edmund Gunter wrote in 1623 that the distance along a great circle was 20 leagues per degree. Thus, Hues explicitly used nautical miles while Gunter did not.

Since the Earth is not a perfect sphere but is an oblate spheroid with slightly flattened poles, a minute of latitude is not constant, but about 1861 metres at the poles and 1843 metres at the Equator. France and other metric countries state that in principle a nautical mile is an arcminute of a meridian at a latitude of 45°, but that is a modern justification for a more mundane calculation that was developed a century earlier. By the mid 19th century France had defined a nautical mile via the original 1791 definition of the metre, one ten-millionth of a quarter meridian. Thus became the metric length for a nautical mile. France made it legal for the French Navy in 1906, and many metric countries voted to sanction it for international use at the 1929 International Hydrographic Conference.

Both the United States and the United Kingdom used an average arcminute, specifically, a minute of arc of a great circle of a sphere having the same surface area as the Clarke 1866 ellipsoid. The "authalic" (equal area) radius of the Clarke 1866 ellipsoid is . The resulting arcminute is . The United States chose five significant digits for its nautical mile, 6080.2 feet, whereas the United Kingdom chose four significant digits for its Admiralty mile, 6080 feet.

In 1929, the international nautical mile was defined by the First International Extraordinary Hydrographic Conference in Monaco as exactly 1,852 metres. The United States did not adopt the international nautical mile until 1954. Britain adopted it in 1970, but legal references to the obsolete unit are now converted to 1853 metres.

The metre was originally defined as of the length of the meridian arc from the North pole to the equator, thus one kilometre of distance corresponds to one centigrad of latitude. The Earth's circumference is therefore approximately 40,000 km. The equatorial circumference is slightly longer than the polar circumference – the measurement based on this ( = 1855.3 metres) is known as the geographical mile.



</doc>
<doc id="21290" url="https://en.wikipedia.org/wiki?curid=21290" title="N">
N

N or n is the fourteenth letter in the modern English alphabet and the ISO basic Latin alphabet. Its name in English is "en" (pronounced ), plural "ens".

One of the most common hieroglyphs, snake, was used in Egyptian writing to stand for a sound like the English , because the Egyptian word for "snake" was "djet". It is speculated by many that Semitic people working in Egypt adapted hieroglyphics to create the first alphabet, and that they used the same snake symbol to represent N, because their word for "snake" may have begun with that sound. However, the name for the letter in the Phoenician, Hebrew, Aramaic and Arabic alphabets is "nun", which means "fish" in some of these languages. The sound value of the letter was —as in Greek, Etruscan, Latin and modern languages.

 represents a dental or alveolar nasal in virtually all languages that use the Latin alphabet, and in the International Phonetic Alphabet. A common digraph with is , which represents a velar nasal in a variety of languages, usually positioned word-finally in English. Often, before a velar plosive (as in "ink" or "jungle"), alone represents a velar nasal. In Italian and French, represents a palatal nasal . The Portuguese and Vietnamese spelling for this sound is , while Spanish, Breton, and a few other languages use the letter .

In English, is generally silent when it is preceded by an at the end of words, as in "hymn"; however, it is pronounced in this combination when occurring word medially, as in "hymnal".

On the other hand, other consonants are often silent when they precede an at the beginning of an English word. Examples include "gnome", "knife", "mnemonic", and "pneumonia".

In mathematics, the italic form "n" is a particularly common symbol for a variable quantity which represents a natural number. The set of natural numbers is referred to as .






</doc>
<doc id="21291" url="https://en.wikipedia.org/wiki?curid=21291" title="Nail (fastener)">
Nail (fastener)

In woodworking and construction, a nail is a small object made of metal (or wood, called a tree nail or "trunnel") which is used as a fastener, as a peg to hang something, or sometimes as a decoration. Generally, nails have a sharp point on one end and a flattened head on the other, but headless nails are available. Nails are made in a great variety of forms for specialized purposes. The most common is a "wire nail". Other types of nails include "pins", "tacks", "brads", "spikes", and "cleats."

Nails are typically driven into the workpiece by a hammer or pneumatic nail gun. A nail holds materials together by friction in the axial direction and shear strength laterally. The point of the nail is also sometimes bent over or "clinched" after driving to prevent pulling out.

The history of the nail is divided roughly into three distinct periods: 

The first nails were made of wrought iron. Nails date back at least to Ancient Egypt — bronze nails found in Egypt have been dated 3400 BC. The Bible provides a number of references to nails, including the story in Judges of Jael the wife of Heber, who drives a nail (or tent-peg) into the temple of a sleeping Canaanite commander; the provision of iron for nails by King David for what would become Solomon's Temple; and in connection with the crucifixion of Christ.

The Romans made extensive use of nails. The Roman army, for example, left behind seven tons of nails when it evacuated the fortress of Inchtuthil in Perthshire in the United Kingdom in 86 to 87 CE.

The term "penny", as it refers to nails, probably originated in medieval England to describe the price of a hundred nails. Nails themselves were sufficiently valuable and standardized to be used as an informal medium of exchange.
Until around 1800 artisans known as "nailers" or "nailors" made nails by hand – note the surname Naylor.

At the time of the American Revolution, England was the largest manufacturer of nails in the world. Nails were expensive and difficult to obtain in the American colonies, so that abandoned houses were sometimes deliberately burned down to allow recovery of used nails from the ashes. This became such a problem in Virginia that a law was created to stop people from burning their houses when they moved. Families often had small nail-manufacturing setups in their homes; during bad weather and at night, the entire family might work at making nails for their own use and for barter. Thomas Jefferson wrote in a letter: "In our private pursuits it is a great advantage that every honest employment is deemed honorable. I am myself a nail maker." The growth of the trade in the American colonies was theoretically held back by the prohibition of new slitting mills in America by the Iron Act of 1750, though there is no evidence that the Act was actually enforced.

The production of wrought-iron nails continued well into the 19th century, but ultimately was reduced to nails for purposes for which the softer cut nails were unsuitable, including horseshoe nails.

The slitting mill, introduced to England in 1590, simplified the production of nail rods, but the real first efforts to mechanise the nail-making process itself occurred between 1790 and 1820, initially in the United States and England, when various machines were invented to automate and speed up the process of making nails from bars of wrought iron. Also in Sweden in the early 1700s Christopher Polhem produced a nail cutting machine as part of his automated factory. These nails were known as "cut nails" or "square nails" because of their roughly rectangular cross section. Cut nails were one of the important factors in the increase in balloon framing beginning in the 1830s and thus the decline of timber framing with wooden joints. Though still used for historical renovations, and for heavy-duty applications, such as attaching boards to masonry walls, "cut nails" are much less common today than "wire nails".

The cut-nail process was patented in America by Jacob Perkins in 1795 and in England by Joseph Dyer, who set up machinery in Birmingham. The process was designed to cut nails from sheets of iron, while making sure that the fibres of the iron ran down the nails. The Birmingham industry expanded in the following decades, and reached its greatest extent in the 1860s, after which it declined due to competition from wire nails, but continued until the outbreak of World War I.

Wire nails are formed from wire. Usually coils of wire are drawn through a series of dies to reach a specific diameter, then cut into short rods that are then formed into nails. The nail tip is usually cut by a blade; the head is formed by reshaping the other end of the rod under high pressure. Other dies are used to cut grooves and ridges. Wire nails were also known as "French nails" for their country of origin. Belgian wire nails began to compete in England in 1863. Joseph Henry Nettlefold was making wire nails at Smethwick by 1875. Over the following decades, the nail-making process was almost completely automated. Eventually the industry had machines capable of quickly producing huge numbers of inexpensive nails with little or no human intervention.

With the introduction of cheap wire nails, the use of wrought iron for nail making quickly declined, as more slowly did the production of cut nails. In the United States, in 1892 more steel-wire nails were produced than cut nails. In 1913, 90% of manufactured nails were wire nails. Nails went from being rare and precious to being a cheap mass-produced commodity. Today almost all nails are manufactured from wire, but the term "wire nail" has come to refer to smaller nails, often available in a wider, more precise range of gauges than is typical for larger common and finish nails.

Nails were formerly made of bronze or wrought iron and were crafted by blacksmiths and nailors. These crafts people used a heated square iron rod that they forged before they hammered the sides which formed a point. After reheating and cutting off, the blacksmith or nailor inserted the hot nail into an opening and hammered it. Later new ways of making nails was created using machines to sheer the nails before wiggling the bar sideways to produce a shank. For example, the Type A cut nails were sheared from an iron bar type guillotine using early machinery. This method was slightly altered until the 1820s when new heads on the nails' ends were pounded via a separate mechanical nail heading machine. In the 1810s, iron bars were flipped over after each stroke while the cutter set was at an angle. Every nail was then sheared off of taper allowing for an automatic grip of each nail which also formed their heads. Type B nails were created this way. In 1886, 10 percent of the nails that were made in the United States were of the soft steel wire variety and by 1892, steel wire nails overtook iron cut nails as the main type of nails that were being produced. In 1913, wire nails were 90 percent of all nails that were produced.

Today's nails are typically made of steel, often dipped or coated to prevent corrosion in harsh conditions or to improve adhesion. Ordinary nails for wood are usually of a soft, low-carbon or "mild" steel (about 0.1% carbon, the rest iron and perhaps a trace of silicon or manganese). Nails for concrete are harder, with 0.5–0.75% carbon.

Types of nail include:

Most countries, except the United States, use a metric system for describing nail sizes. A "50 × 3.0" indicates a nail 50 mm long (not including the head) and 3 mm in diameter. Lengths are rounded to the nearest millimetre.

For example, finishing nail* sizes typically available from German suppliers are:

In the United States, the length of a nail is designated by its penny size.


Nails have been used in art, such as the Nail Men—a form of fundraising common in Germany and Austria during World War I.

Before the 1850s bocce and pétanque boules were wooden balls, sometimes partially reinforced with hand-forged nails. When cheap, plentiful machine-made nails became available, manufacturers began to produce the "boule cloutée"—a wooden core studded with nails to create an all-metal surface. Nails of different metals and colors (steel, brass, and copper) were used to create a wide variety of designs and patterns. Some of the old "boules cloutées" are genuine works of art and valued collector's items.

Once nails became cheap and widely available, they were often used in folk art and outsider art as a method of decorating a surface with metallic studs. Another common artistic use is the construction of sculpture from welded or brazed nails.

Nails were sometimes inscribed with incantations or signs intended for religious or mystical benefit, used at shrines or on the doors of houses for protection.




</doc>
