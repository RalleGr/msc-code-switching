<doc id="15379" url="https://en.wikipedia.org/wiki?curid=15379" title="Isle Royale National Park">
Isle Royale National Park

Isle Royale National Park is an American national park consisting of Isle Royale and hundreds of adjacent islands, as well as the surrounding waters of Lake Superior, in the state of Michigan. Isle Royale National Park was established on April 3, 1940, then additionally protected from development by wilderness area designation in 1976, and declared a UNESCO International Biosphere Reserve in 1980. The park covers , with of land and of surrounding waters. The park's northern boundary lies adjacent to the Canadian Lake Superior National Marine Conservation Area along the international border.

Isle Royale, the largest island in Lake Superior, is over in length and wide at its widest point. The park is made up of Isle Royale itself and approximately 400 smaller islands, along with any submerged lands within of the surrounding islands (16USC408g).

According to the Köppen climate classification system, Isle Royale National Park has a mild summer Humid continental climate ("Dfb"). According to the United States Department of Agriculture, the Plant Hardiness zone is 4b at 1178 ft (359 m) elevation with an average annual extreme minimum temperature of -24.2 °F (-31.2 °C).

Large quantities of copper artifacts found in indian mounds and settlements, some dating back to 3000 B.C., were most likely mined on Isle Royale and the nearby Keweenaw Peninsula. The island has hundreds of pits from these indigenous peoples, with most in the McCargoe Cove area. Carbon-14 testing of a charred log found at one of these pits yielded an age of 1,500 B.C. The Jesuit missionary Dablon published an account in 1669-70 of "an island called "Menong", celebrated for its copper." "Menong", or "Minong", was the native term for the island, and is the basis for Minong Ridge. Prospecting began in earnest when the Chippewas relinquished their claims to the island in 1843, starting with many of the original native pits. This activity had ended by 1855, when no economic deposits were found. The Minong Mine and Island Mine were the result of renewed but short-lived activity from 1873 to 1881.

In "Prehistoric Copper Mining in the Lake Superior Region", published in 1961, Drier and Du Temple estimated that over 1.5 billion pounds (630,400 t) of copper had been mined from the region. However, David Johnson and Susan Martin contend that their estimate was based on exaggerated and inaccurate assumptions.

In the mid-1840s, a report by Douglass Houghton, Michigan's first state geologist, set off a copper boom in the state, and the first modern copper mines were opened on the island. Evidence of the earlier mining efforts was everywhere, in the form of many stone hammers, some copper artifacts, and places where copper had been partially worked out of the rock but left in place. The ancient pits and trenches led to the discovery of many of the copper deposits that were mined in the 19th century.

The island was once the site of a resort community. The fishing industry has declined considerably, but continues at Edisen Fishery. Because numerous small islands surround Isle Royale, ships were once guided through the area by lighthouses at Passage Island, Rock Harbor, Rock of Ages, and Isle Royale Lighthouse on Menagerie Island.

Within the waters of Isle Royale National Park are several shipwrecks. The area’s notoriously harsh weather, dramatic underwater topography, the island’s central location on historic shipping routes, and the cold, fresh water have resulted in largely intact, well preserved wrecks throughout the park. These were documented in the 1980s, with follow up occurring in 2009, by the National Park Service Submerged Resources Center.

According to the A. W. Kuchler U.S. Potential natural vegetation Types, Isle Royale National Park has a Great Lakes Spruce/Fir ("93") potential vegetation type and a Northern Conifer Forest ("22") potential vegetation form.

The predominant floral habitats of Isle Royale are within the Laurentian Mixed Forest Province. The area is a temperate broadleaf and mixed forests biome transition zone between the true boreal forest to the north and Big Woods to the south, with characteristics of each. It has areas of both broadleaf and conifer forest cover, and bodies of water ranging from conifer bogs to swamps.

Conifers include jack pines ("Pinus banksiana"), black and white spruces ("Picea mariana" and "Picea glauca"), balsam firs ("Abies balsamea"), and eastern redcedars ("Juniperus virginiana").

Deciduous trees include quaking aspens ("Populus tremuloides"), red oaks ("Quercus rubra"), paper birches ("Betula papyrifera"), American mountain ash ("Sorbus americana"), red maples ("Acer rubrum"), sugar maples ("Acer saccharum"), and mountain maples ("Acer spicatum").

Isle Royale National Park is known for its timber wolf and moose populations which are studied by scientists investigating predator-prey relationships in a closed environment. This is made easier because Isle Royale has been colonized by roughly just one third of the mainland mammal species, because it is so remote. In addition, the environment is unique in that it is the only known place where wolves and moose coexist without the presence of bears.

Historically neither moose nor wolves inhabited Isle Royale. Just prior to becoming a national park the large mammals on Isle Royale were Canada lynx and the boreal woodland caribou. Archeological evidence indicates both of these species were present on Isle Royale for 3,500 years prior to being removed by direct human actions (hunting, trapping, mining, logging, fires, competition for resources from exotic species and possibly disease due to the introduction of invasive species). The last caribou documented on Isle Royale was in 1925. Though lynx were removed by the 1930s some have periodically crossed the ice bridge from neighboring Ontario, Canada, the most recent being an individual sighting in 1980. Although lynx are no longer present on
the island, their primary prey, snowshoe hares, remain. Before the appearance of wolves, coyotes were also predators on the island. Coyotes appeared around 1905 and disappeared shortly after wolves arrived in the 1950s.
Moose are believed to have colonized Isle Royale sometime between 1905 and 1912. It was initially believed that a small herd of moose (moose typically do not travel in herds) colonized the islands by crossing the ice from the adjacent mainland; later this theory was modified to a herd of moose swimming 20 miles across Lake Superior from the nearest mainland. The improbability of these theories received little scrutiny until recent years. Although no thorough scientific investigation to determine how moose arrived on Isle Royale has been carried out to date, both cultural and genetic evidence indicates they were likely introduced by humans to create a private hunting preserve in the early 1900s. The cultural evidence that moose were trapped in northwestern Minnesota and transported to Isle Royale sounded far fetched to many until decades later when genetic evidence revealed the moose on Isle Royale were more closely related to moose in the far northwestern Minnesota/Manitoba border area than the mainland adjacent to Isle Royale in far northeastern Minnesota bordering Ontario. Further evidence has also shown that the Washington Harbor Club, a group of well-to-do businessmen, owned various buildings on Isle Royale in addition to railroads that ran from Baudette to Duluth and Two Harbors and so had the means to transport moose from northwestern Minnesota to Two Harbors.

There are usually around 25 wolves and 1000 moose on the island, but the numbers change greatly year to year. In the 2006-2007 winter, 385 moose were counted, as well as 21 wolves, in three packs. In spring 2008, 23 wolves and approximately 650 moose were counted. However, recent reductions in winter pack ice had ended replenishment of the wolf population from the mainland. Due to genetic inbreeding, the wolf population had declined to two individuals in 2016, causing researchers to expect that the island's wolf population would eventually become extinct. At the same time, the island's moose population had exploded to an estimated 1600. By November 2017, the wolf population was down to one, a female.

In December 2016, the National Park Service (NPS) put forward an initial plan in which they would bring additional wolves to the island in order to prevent the pack from disappearing completely. As of June 7, 2018, the decision to relocate 20-30 wolves to the island has been approved and the NPS is actively developing specific implementation strategies. There was some debate as to whether or not the introduction was an unnatural intervention.

The two main rock assemblages found on the island include the Portage Lake Volcanics and the Copper Harbor Conglomerate, both Precambrian in age. The volcanics are mainly ophitic flood basalts, some 100 individual flows over an accumulated thickness of at least 10,000 feet. The conglomerate outcrops on the southwestern portion of the island and consists of sedimentary rock derived from volcanic rocks in present-day Minnesota. Glacial erosion accentuated the ridge and valley topography from pre-glacial stream erosion. Glacial striations indicate a generally westward movement of the glaciers as do the recessional moraines west of Lake Desor. Drumlins are found west of Siskiwit Lake.

Recent analyses by the USGS of both unmineralized basalt and copper-mineralized rock show that a small amount of naturally occurring mercury is associated with mineralization.

Native copper and chlorastrolite, the official state gem of Michigan, are secondary minerals filling pore spaces formed by vesicles and fractures within the volcanic rocks. Prehnite and agate amygdules are also plentiful island gemstones.

The Greenstone Ridge is a high ridge in the center of the island and carries the longest trail in the park, the Greenstone Ridge Trail, which runs from one end of the island to the other. This is generally done as a 4 or 5 day hike. A boat shuttle can carry hikers back to their starting point. In total there are of hiking trails. There are also canoe/kayak routes, many involving portages, along coastal bays and inland lakes.

The park has two developed areas:

Windigo, at the southwest end of the island (docking site for the ferries from Minnesota), with a campstore, showers, campsites, rustic camper cabins for those wanting to sleep off of the ground and a boat dock.

Rock Harbor on the south side of the northeast end (docking site for the ferries from Michigan), with a campstore, showers, restaurant, lodge, campsites, and a boat dock. Non-camping sleeping accommodations at the park are limited to the lodge at Rock Harbor and the camper cabins at Windigo.

The park has 36 designated wilderness campgrounds. Some campgrounds in the interior are accessible only by trail or by canoe/kayak on the island lakes. Other campgrounds are accessible only by private boat. The campsites vary in capacity but typically include a few three-sided wood shelters (the fourth wall is screened) with floors and roofs, and several individual sites suitable for pitching a small tent. Some tent sites with space for groups of up to 10 are available, and are used for overflow if all the individual sites are filled.

The only amenities at the campgrounds are pit toilets, picnic tables, and fire-rings at specific areas. Campfires are not permitted at most campgrounds; gas or alcohol camp stoves are recommended. Drinking and cooking water must be drawn from local water sources (Lake Superior and inland lakes) and filtered, treated, or boiled to avoid parasites. Hunting is not permitted, but fishing is, and edible berries (blueberries, thimbleberries) may be picked from the trail.

The park is accessible by ferries, floatplanes, and passenger ships during the summer months—from Houghton and Copper Harbor in Michigan; and Grand Portage in Minnesota. Private boats travel to the island from the coasts of Michigan, Minnesota, and Ontario. Isle Royale is quite popular with day-trippers in private boats, and day-trip ferry service is provided from Copper Harbor and Grand Portage to and from the park.

Isle Royale is the only American national park to entirely close in the winter months, from November 1 through April 15, due to extreme weather conditions and for the safety and protection of visitors. Isle Royale is the least-visited national park in the contiguous United States, due to the winter closing and the distance across Lake Superior to reach the park. The average annual visitation was about 19,000 in the period from 2009 to 2018, with 25,798 visiting in 2018. It is the least visited national park in the lower 48 states. Only three of the most remote Alaskan national parksLake Clark, Kobuk Valley and Gates of the Arcticreceive fewer visitors.

Scheduled ferry service operates from Grand Portage, Copper Harbor and Houghton.

The Grand Portage ferries reach the island in 1 1/2 hours, and stay 4 hours at the island, allowing time for hiking, a guided hike or program by the park staff, and picnics.

The "Isle Royale Queen" serves park visitors out of Copper Harbor, on the northern Upper Peninsula coast of Michigan. It arrives at Rock Harbor in the park in 3 to 3 1/2 hours, spends 3 1/2 hours before returning to Copper Harbor.

The "Sea Hunter" operates round-trips and offers day trips to the Windigo visitor center through much of the season, and less frequently in early summer and autumn; it will transport kayaks and canoes for visitors wanting to explore the park from the water. It is the fastest ferry serving the island and arrives in 1 1/2 hours, including some sightseeing points along the way out and back. Because of the relatively short boat ride, day visitors are able to get four hours on the island, and get back to the mainland earlier in the afternoon. This gives visitors on a tight schedule time to visit the Grand Portage National Monument or other attractions in the same day.

The "Ranger III" is a ship that serves park visitors from Houghton, Michigan to Rock Harbor. It is operated by the National Park Service, and is said to be the largest piece of equipment in the National Park system. It carries 125 passengers, along with canoes, kayaks, and even small powerboats. It is a six-hour voyage from Houghton to the park. The ship stays overnight at Rock Harbor before returning the next day, making two round trips each week from June to mid-September. Briefly in the 2008 season, the Ranger III carried visitors to and from Windigo. This was not continued after four trips, due to low interest and long crossing times. In 2012, Park Superintendent Phyllis Green required the "Ranger III" to purify its ballast water.

The "Voyageur II", out of Grand Portage, crosses up to three times a week, overnighting at Rock Harbor and providing transportation between popular lakeside campgrounds. In the fall season, in addition to carrying campers and hikers, it provides day-trip service to Windigo on weekends. The Voyageur transports kayaks and canoes for visitors wanting to explore the island from the water. The "Voyageur II" and other boat taxi services ferry hikers to points along the island, allowing a one-way hike back to Rock Harbor or Windigo. Visitors may land at Rock Harbor and depart from Windigo several days later, or vice versa. Hikers frequently ride it in one direction to do a cross-island hike and then get picked up at the other end.




</doc>
<doc id="15381" url="https://en.wikipedia.org/wiki?curid=15381" title="NATO Integrated Air Defense System">
NATO Integrated Air Defense System

The NATO Integrated Air Defense System (short: NATINADS) is a command and control network combining radars and other facilities spread throughout the NATO alliance's air defence forces. It formed in the mid-1950s and became operational in 1962 as NADGE. It has been constantly upgraded since its formation, notably with the integration of Airborne Early Warning aircraft in the 1970s. The United Kingdom maintained its own network, but was fully integrated with the network since the introduction of the Linesman/Mediator network in the 1970s. Similarly, the German network maintained an independent nature through GEADGE.

Development was approved by the NATO Military Committee in December 1955. The system was to be based on four air defense regions (ADRs) coordinated by SACEUR (Supreme Allied Commander Europe). Starting from 1956 early warning coverage was extended across Western Europe using 18 radar stations. This part of the system was completed by 1962. Linked to existing national radar sites the coordinated system was called the NATO Air Defence Ground Environment (NADGE).

From 1960 NATO countries agreed to place all their air defence forces under the command of SACEUR in the event of war. These forces included command & control (C2) systems, radar installations, and Surface-to-Air (SAM) missile units as well as interceptor aircraft.

By 1972 NADGE was converted into NATINADS consisting of 84 radar sites and associated Control Reporting Centers (CRC) and in the 1980s the Airborne Early Warning / Ground Environment Integration Segment (AEGIS) upgraded the NATINADS with the possibility to integrate the AWACS radar picture and all of its information into its visual displays. (NOTE: This AEGIS is not to be confused with the U.S.Navy AEGIS, a shipboard fire control radar and weapons system.) AEGIS processed the information through Hughes H5118ME computers, which replaced the H3118M computers installed at NADGE sites in the late 1960s and early 1970s.

NATINADS ability to handle data increased with faster clock rates. The H5118M computer had a staggering 1 megabyte of memory and could handle 1.2 million instructions per second while the former model had a memory of only 256 kilobytes and a clock speed of 150,000 instructions per seconds.

NATINADS/AEGIS were complemented, in West Germany by the German Air Defence Ground Environment (GEADGE), an updated radar network adding the southern part of Germany to the European system and Coastal Radar Integration System (CRIS), adding data links from Danish coastal radars.

In order to counter the hardware obsolescence, during the mid-1990s NATO started the AEGIS Site Emulator (ASE) program allowing the NATINADS/AEGIS sites to replace the proprietary hardware (the 5118ME computer and the various operator consoles IDM-2, HMD-22, IDM-80) with commercial-off-the-shelf (COTS) servers and workstations.

In the first years 2000, the initial ASE capability was expanded with the possibility to run, thanks to the new hardware power, multiple site emulators on the same hardware, so the system was renamed into Multi-AEGIS Site Emulator (MASE). The NATO system designed to replace MASE in the near future is the Air Command and Control System (ACCS).

Because of changing politics, NATO expanding and financial crises most European (NATO) countries are trying to cut defence budgets; as a direct result, many obsolete and outdated NATINADS facilities are phased out earlier. As of 2013, operational NATO radar sites in Europe are as follows:

Allied Air Command (AIRCOM) is the central command of all NATO air forces on the European continent. The command is based at Ramstein Air Base in Germany and has two subordinate commands in Germany and Spain. The Royal Canadian Air Force and United States Air Force fall under command of the Canadian/American North American Aerospace Defense Command.


The Albanian Air Force does not possess any fixed radar installations. Its air space is monitored by Italian Air Force and Greek Air Force radars. The Albanian Air Force's Air Surveillance Centre at Tirana International Airport reports to CAOC Torrejón in Spain, while the Italian Air Force's 36th Wing at Gioia del Colle Air Base is responsible for the air defense of Albania. Albania is buying two mobile Lockheed Martin AN/TPS-77 radars to provide its Armed Forces with its own radar capability.

The Belgian Air Component's Control and Reporting Centre was based at Glons, where also its main radar was located. The radar was deactivated in 2015 and the Centre moved to Beauvechain Air Base in 2018. The Belgian Control and Reporting Centre reports to CAOC Uedem in Germany and is also responsible for guarding the airspace of Luxembourg. At the new location the Control and Reporting Centre uses digital radar data of the civilian radars of Belgocontrol and the Marconi S-723 radar of the Air Component's Air Traffic Control Centre in Semmerzake.

The Bulgarian Air Force's Air Sovereignty Operations Centre is located in Sofia and reports to CAOC Torrejón. The Bulgarian Air Force fields three control and surveillance zones, which operate obsolete Soviet-era radars. The Bulgarian Air Force intends to replace these radars with fewer, but more capable Western 3-D radars as soon as possible. The future locations of the new radars are as of 2018 unknown.


The Royal Canadian Air Force's control centres and radar stations are part of the Canadian/American North American Aerospace Defense Command.

The Croatian Air Force and Air Defense's Airspace Surveillance Centre is headquartered in Podvornica and reports to CAOC Torrejón.


The Czech Air Force's Control and Reporting Centre is located in Hlavenec and reports to CAOC Uedem.


The Royal Danish Air Force's Combined Air Operations Centre (CAOC 1) in Finderup was deactivated in 2008 and replaced at the same location by the Combined Air Operations Centre Finderup (CAOC F), which had responsibility for the airspaces of Iceland, Norway, Denmark and the United Kingdom. CAOC F was deactivated in 2013 and its responsibilities were transferred to CAOC Uedem in Germany. The national Danish Control and Reporting Centre is located at Karup Air Base and it reports to CAOC Uedem.

The Thule Air Base in Greenland is a United States Air Force installation and its radars are part of the North American Aerospace Defense Command.


The Estonian Air Force's Air Operations Control Centre is located at Ämari Air Base and reports to the Baltic Air Surveillance Network's Regional Airspace Surveillance Coordination Centre (RASCC) in Karmėlava, Lithuania, which in turn reports to CAOC Uedem.


The French Air Force's Air Operations Centre is located at Mont Verdun Air Base and reports to CAOC Uedem. Most French radar sites use the PALMIER radar, which is being taken out of service. By 2022 all PALMIER radars will have been replaced with new radar stations using the GM 403 radar.


Additionally the French Air Force fields a GM 406 radar at the Cayenne-Rochambeau Air Base in French Guiana to protect the Guiana Space Centre in Kourou.

The German Air Force's Combined Air Operations Centre (CAOC 2) in Uedem was deactivated in 2008 and reactivated as CAOC Uedem in 2013. CAOC Uedem is responsible for the NATO airspace North of the Alps. The HADR radars are a variant of the HR-3000 radar, while the RRP-117 radars are a variant of the AN/FPS-117.


1st Area Control Centre, inside Mount Chortiatis, with Marconi S-743D
2nd Area Control Centre, inside Mount Parnitha, with Marconi S-743D
9th Control and Warning Station Squadron, on Mount Pelion, with Marconi S-743D
10th Control and Warning Station Squadron, on Mount Chortiatis, with Marconi S-743D

The Hellenic Air Force's Combined Air Operations Centre (CAOC 7) at Larissa Air Base was deactivated in 2013 and its responsibilities transferred to the CAOC Torrejón in Spain. The Hellenic Air Force fields two HR-3000, four AR-327 and six Marconi S-743D radar systems, however as of 2018 the air force is in the process of replacing some of its older systems with three RAT-31DL radars.


The Hungarian Air Force's Air Operations Centre is located in Veszprém and reports to CAOC Uedem. There are additional three radar companies with Soviet-era equipment subordinate to the 54th Radar Regiment ""Veszprém"", however it is unclear if they will remain in service once Hungary's newest radar at Medina reaches full operational capability.


The Iceland Air Defense System, which is part of the Icelandic Coast Guard, monitors Iceland's airspace. Air Defense is provided by fighter jets from NATO allies, which rotate units for the Icelandic Air Policing mission to Keflavik Air Base.
The Iceland Air Defense System's Control and Reporting Centre is at Keflavik Air Base and reports to CAOC Uedem in Germany.


The Italian Air Force's Combined Air Operations Centre (CAOC 5) in Poggio Renatico was deactivated in 2013 and replaced with the Mobile Command and Control Regiment (RMCC) at Bari Air Base, while the Centre's responsibilities were transferred to the CAOC Torrejón in Spain.


The Latvian Air Force's Air Operations Centre is located at Lielvārde Air Base and reports to the Baltic Air Surveillance Network's Regional Airspace Surveillance Coordination Centre (RASCC) in Karmėlava, Lithuania, which in turn reports to CAOC Uedem.


The Lithuanian Air Force's Air Operations Control Centre is located in Karmėlava and reports to the Baltic Air Surveillance Network's Regional Airspace Surveillance Coordination Centre (RASCC) co-located in Karmėlava, which in turn reports to CAOC Uedem.


Luxembourg's airspace is monitored and guarded by the Belgian Air Component's Control and Reporting Centre at Beauvechain Air Base.

The Armed Forces of Montenegro do not possess a modern air defense radar and the country's airspace is monitored by Italian Air Force radar sites. The Armed Forces Air Surveillance and Reporting Centre is located at Podgorica Airport in Golubovci and reports to CAOC Torrejón in Spain.

The Royal Netherlands Air Force's Air Operations Centre is located at Nieuw-Milligen and reports to CAOC Uedem. The air force's main radars are being replaced with two modern SMART-L GB radars.


The Royal Norwegian Air Force's Combined Air Operations Centre (CAOC 3) in Reitan was deactivated in 2008 and its responsibilities were transferred to the Combined Air Operations Centre Finderup (CAOC F). After CAOC F was deactivated in 2013 the responsibility for the air defense of Norway was transferred to CAOC Uedem in Germany and the Royal Norwegian Air Force's Control and Reporting Centre in Sørreisa reports to it. Until 2016 the Royal Norwegian Air Force's radar installations were distributed between two CRCs. That year the CRC Mågerø was disbanded. In its place a wartime mobilization back-up CRC has been formed with a reduction in personnel from the around active 170 duty to about 50 air force home guardsmen. The SINDRE I radars are a variant of the HR-3000 radar, which is also used in the German HADR radars. The newer RAT-31SL/N radars are sometimes designated SINDRE II.


The Polish Armed Forces Operational Command's Air Operations Centre is located in the Warsaw-Pyry neighborhood and reports to CAOC Uedem. The 3rd "Wrocław" Radiotechnical Brigade is responsible for the operation of the armed forces' radar equipment and fields mainly obsolete Soviet-era radars. As of 2018 the Polish Air Force possesses three modern RAT-31DL radars, which are listed below.


The Portuguese Air Force's Combined Air Operations Centre (CAOC 10) in Lisbon was deactivated in 2013 and its responsibilities were transferred to CAOC Torrejón in Spain.


The Romanian Air Force's Air Operations Centre is headquartered in Bucharest and reports to CAOC Torrejón. The radar station in Bârnova is officially designated and operated as a civilian radar station, however its data is fed into the military air surveillance system.


The Slovak Air Force's Air Operations Centre is located at Sliač Air Base and reports to CAOC Uedem. The Slovak Air Force still operates obsolete Soviet-era radars, which it intends to replace with fewer, but more capable Western 3-D radars as soon as possible. The future locations of the new radars are as of 2018 unknown.


The Slovenian Air Force and Air Defense's Airspace Surveillance and Control Centre is headquartered in Brnik and reports to CAOC Torrejón.

The Italian Air Force's 4th Wing at Grosseto Air Base and 36th Wing at Gioia del Colle Air Base rotate a QRA flight of Eurofighter Typhoons to Istrana Air Base, which are responsible for the air defense of Northern Italy and Slovenia.


The Spanish Air Force's Combined Air Operations Centre (CAOC 8) at Torrejón Air Base was deactivated in 2013 and replaced at same location by CAOC Torrejon, which took over the functions of CAOC 5, CAOC 7, CAOC 8 and CAOC 10. CAOC Torrejón is responsible for the NATO airspace South of the Alps.


The Turkish Air Force's Combined Air Operations Centre (CAOC 6) in Eskisehir was deactivated in 2013 and its responsibilities were transferred to CAOC Torrejón in Spain. Turkey's Air Force fields a mix of HR-3000, AN/FPS-117, RAT-31SL and RAT-31DL radars, however the exact number of each of these radar and their location in the Turkish radar system is unknown.


The Royal Air Force's Combined Air Operations Centre (CAOC 9) at RAF High Wycombe was deactivated in 2008 and its responsibilities were transferred to the Combined Air Operations Centre Finderup (CAOC F). After CAOC F was deactivated in 2013 the responsibility for the air defense of the United Kingdom was transferred to CAOC Uedem in Germany. The Royal Air Force's Control and Reporting Centres report to it.


The United States Air Force's control centres and radar stations are part of the Canadian/American North American Aerospace Defense Command.




</doc>
<doc id="15382" url="https://en.wikipedia.org/wiki?curid=15382" title="Invisible balance">
Invisible balance

The invisible balance or balance of trade on services is that part of the balance of trade that refers to services and other products that do not result in the transfer of physical objects. Examples include consulting services, shipping services, tourism, and patent license revenues. This figure is usually generated by tertiary industry. The term 'invisible balance' is especially common in the United Kingdom.

For countries that rely on service exports or on tourism, the invisible balance is particularly important. For instance the United Kingdom and Saudi Arabia receive significant international income from financial services, while Japan and Germany rely more on exports of manufactured goods.

Invisibles are both international payments for services (as opposed to goods), as well as movements of money without exchange for goods or services. These invisibles are called 'transfer payments' or 'remittances' and may include money sent from one country to another by an individual, business, government or non-governmental organisations (NGO) – often charities.

An individual remittance may include money sent to a relative overseas. Business transfers may include profits sent by a foreign subsidiary to a parent company or money invested by a business in a foreign country. Bank loans to foreign countries are also included in this category, as are license fees paid for the use of patents and trademarks. Government transfers may involve loans made or official aid given to foreign countries, while transfers made by NGO's include money designated for charitable work within foreign countries, respectively.

In many countries a useful distinction is drawn between the balance of trade and the balance of payments. 'Balance of trade' refers to the trade of both tangible (physical) objects as well as the trade in services – collectively known as exports and imports (in other words, 'visibles plus services') – while the 'balance of payments' also includes transfers of Capital in the form of loans, investments in shares or direct investment in projects.

A nation may have a visibles balance surplus but this can be offset by a larger deficit in the invisibles balance (creating a Balance of Trade deficit overall) – if, for example, there are large payments made to foreign businesses for invisibles such as shipping or tourism. On the other hand, a Visibles Balance deficit can be offset by a strong surplus on the invisibles balance if, for example, foreign aid is being provided.

In a similar way, a nation may also have a surplus 'balance of trade' because it exports more than it imports but a negative (or deficit) 'balance of payments' because, it has a much greater shortfall in transfers of capital. And, just as easily, a deficit in the 'balance of trade' may be offset by a larger surplus in capital transfers from overseas to produce a balance of payments surplus overall.

Problems with a country's balance of trade (or balance of payments) are often associated with an inappropriate valuation of its currency, its country's foreign exchange rate.

If a country's exchange rate is too high, its exports will become uncompetitive as buyers in foreign countries require more of their own currency to pay for them. In the meantime, it also becomes cheaper for the citizens of the country to buy goods from overseas, as opposed to buying locally produced goods), because an overvalued currency makes foreign products less expensive.

The simultaneous decline in currency inflows from decreased exports and the rise in outflows, due to increased imports, sends the balance of trade into deficit, which then needs to be paid for by a transfer of funds in some form, either invisible transfers (aid, etc.) or capital flows (loans, etc.). However, relying on funds like that to support a trade deficit, is unsustainable, and the country may eventually require its currency to be devalued.

If, on the other hand, a currency is undervalued, its exports will become cheaper and therefore more competitive internationally. At the same time, imports will also become more costly, stimulating the production of domestic substitutes to replace them. That will result in a growth of currency flowing into the country and a decline in currency flowing out of it, resulting in an improvement in the country's balance of trade.

Because a nation's exchange rate has a big impact on its 'balance of trade' and its 'balance of payments', many economists favour freely floating exchange rates over the older, fixed (or pegged) rates of foreign currency exchange. Floating exchange rates allow more regular adjustments in exchange rates to occur, allowing the greater opportunity for international payments to maintain equilibrium.


</doc>
<doc id="15387" url="https://en.wikipedia.org/wiki?curid=15387" title="Irreducible complexity">
Irreducible complexity

Irreducible complexity (IC) involves the idea that certain biological systems cannot have evolved by successive small modifications to pre-existing functional systems through natural selection, because no less complex system would function. Irreducible complexity has become central to the creationist concept of intelligent design, but the scientific community, which regards intelligent design as pseudoscience, rejects the concept of irreducible complexity. Irreducible complexity is one of two main arguments used by intelligent-design proponents, alongside specified complexity.

Creation science presented the theological argument from design with assertions that evolution could not explain complex molecular mechanisms, and in 1993 Michael Behe, a professor of biochemistry at Lehigh University, presented these arguments in a revised version of the school textbook "Of Pandas and People". In his 1996 book "Darwin's Black Box" he called this concept "irreducible complexity" and said it made evolution through natural selection of random mutations impossible. This was based on the mistaken assumption that evolution relies on improvement of existing functions, ignoring how complex adaptations originate from changes in function, and disregarding published research. Evolutionary biologists have published rebuttals showing how systems discussed by Behe can evolve, and examples documented through comparative genomics show that complex molecular systems are formed by the addition of components as revealed by different temporal origins of their proteins.

In the 2005 "Kitzmiller v. Dover Area School District" trial, Behe gave testimony on the subject of irreducible complexity. The court found that "Professor Behe's claim for irreducible complexity has been refuted in peer-reviewed research papers and has been rejected by the scientific community at large."

Michael Behe defined irreducible complexity in natural selection in his book "Darwin's Black Box":
... a single system which is composed of several well-matched, interacting parts that contribute to the basic function, and where the removal of any one of the parts causes the system to effectively cease functioning.

A second definition given by Behe (his "evolutionary definition") is as follows:
An irreducibly complex evolutionary pathway is one that contains one or more unselected steps (that is, one or more necessary-but-unselected mutations). The degree of irreducible complexity is the number of unselected steps in the pathway.

Intelligent design advocate William A. Dembski gives this definition:
A system performing a given basic function is irreducibly complex if it includes a set of well-matched, mutually interacting, nonarbitrarily individuated parts such that each part in the set is indispensable to maintaining the system's basic, and therefore original, function. The set of these indispensable parts is known as the irreducible core of the system.

The argument from irreducible complexity is a descendant of the teleological argument for God (the argument from design or from complexity). This states that because certain things in nature appear very complicated, they must have been designed. William Paley famously argued, in his 1802 watchmaker analogy, that complexity in nature implies a God for the same reason that the existence of a watch implies the existence of a watchmaker. This argument has a long history, and one can trace it back at least as far as Cicero's "De Natura Deorum" ii.34, written in 45 BC.

Galen (1st and 2nd centuries AD) wrote about the large number of parts of the body and their relationships, which observation was cited as evidence for creation. The idea that the interdependence between parts would have implications for the origins of living things was raised by writers starting with Pierre Gassendi in the mid-17th century and by John Wilkins (1614-1672), who wrote (citing Galen), "Now to imagine, that all these things, according to their several kinds, could be brought into this regular frame and order, to which such an infinite number of Intentions are required, without the contrivance of some wise Agent, must needs be irrational in the highest degree."

Chapter XV of Paley's "Natural Theology" discusses at length what he called "relations" of parts of living things as an indication of their design.

Georges Cuvier applied his principle of the "correlation of parts" to describe an animal from fragmentary remains. For Cuvier, this related to another principle of his, the "conditions of existence", which excluded the possibility of transmutation of species.

While he did not originate the term, Charles Darwin identified the argument as a possible way to falsify a prediction of the theory of evolution at the outset. In "The Origin of Species" (1859), he wrote, "If it could be demonstrated that any complex organ existed, which could not possibly have been formed by numerous, successive, slight modifications, my theory would absolutely break down. But I can find out no such case." Darwin's theory of evolution challenges the teleological argument by postulating an alternative explanation to that of an intelligent designer—namely, evolution by natural selection. By showing how simple unintelligent forces can ratchet up designs of extraordinary complexity without invoking outside design, Darwin showed that an intelligent designer was not the necessary conclusion to draw from complexity in nature. The argument from irreducible complexity attempts to demonstrate that certain biological features cannot be purely the product of Darwinian evolution.

In the late 19th century, in a dispute between supporters of the adequacy of natural selection and those who held for inheritance of acquired characteristics, one of the arguments made repeatedly by Herbert Spencer, and followed by others, depended on what Spencer referred to as "co-adaptation" of "co-operative" parts, as in: "We come now to Professor Weismann's endeavour to disprove my second thesis — that it is impossible to explain by natural selection alone the co-adaptation of co-operative parts. It is thirty years since this was set forth in "The Principles of Biology." In §166, I instanced the enormous horns of the extinct Irish elk, and contended that in this and in kindred cases, where for the efficient use of some one enlarged part many other parts have to be simultaneously enlarged, it is out of the question to suppose that they can have all spontaneously varied in the required proportions." Darwin responded to Spencer's objections in chapter XXV of "The Variation of Animals and Plants under Domestication" (1868). The history of this concept in the dispute has been characterized: "An older and more religious tradition of idealist thinkers were committed to the explanation of complex adaptive contrivances by intelligent design. ... Another line of thinkers, unified by the recurrent publications of Herbert Spencer, also saw co-adaptation as a composed, irreducible whole, but sought to explain it by the inheritance of acquired characteristics."

St. George Jackson Mivart raised the objection to natural selection that "Complex and simultaneous co-ordinations … until so far developed as to effect the requisite junctions, are useless" which "amounts to the concept of "irreducible complexity" as defined by … Michael Behe".

Hermann Muller, in the early 20th century, discussed a concept similar to irreducible complexity. However, far from seeing this as a problem for evolution, he described the "interlocking" of biological features as a consequence to be expected of evolution, which would lead to irreversibility of some evolutionary changes. He wrote, "Being thus finally woven, as it were, into the most intimate fabric of the organism, the once novel character can no longer be withdrawn with impunity, and may have become vitally necessary."

In 1974 the young Earth creationist Henry M. Morris introduced a similar concept in his book "Scientific Creationism", in which he wrote; "This issue can actually be attacked quantitatively, using simple principles of mathematical probability. The problem is simply whether a complex system, in which many components function unitedly together, and in which each component is uniquely necessary to the efficient functioning of the whole, could ever arise by random processes."

In 1975 Thomas H. Frazzetta published a book-length study of a concept similar to irreducible complexity, explained by gradual, step-wise, non-teleological evolution. Frazzetta wrote: "A complex adaptation is one constructed of "several" components that must blend together operationally to make the adaptation "work". It is analogous to a machine whose performance depends upon careful cooperation among its parts. In the case of the machine, no single part can greatly be altered without changing the performance of the entire machine." The machine that he chose as an analog is the Peaucellier–Lipkin linkage, and one biological system given extended description was the jaw apparatus of a python. The conclusion of this investigation, rather than that evolution of a complex adaptation was impossible, "awed by the adaptations of living things, to be stunned by their complexity and suitability", was "to accept the inescapable but not humiliating fact that much of mankind can be seen in a tree or a lizard."

In 1981, Ariel Roth, in defense of the creation-science position in the trial "McLean v. Arkansas", said of "complex integrated structures": "This system would not be functional until all the parts were there ... How did these parts survive during evolution ...?"

In 1985 Cairns-Smith wrote of "interlocking": "How can a complex collaboration between components evolve in small steps?" and used the analogy of the scaffolding called centering - used to build an arch then removed afterwards: "Surely there was 'scaffolding'. Before the multitudinous components of present biochemistry could come to lean together "they had to lean on something else."" However, neither Muller or Cairns-Smith claimed their ideas as evidence of something supernatural.

An essay in support of creationism published in 1994 referred to bacterial flagella as showing "multiple, integrated components", where "nothing about them works unless every one of their complexly fashioned and integrated components are in place". The author asked the reader to "imagine the effects of natural selection on those organisms that fortuitously evolved the flagella ... without the concommitant control mechanisms".

An early concept of irreducibly complex systems comes from Ludwig von Bertalanffy (1901-1972), an Austrian biologist. He believed that complex systems must be examined as complete, irreducible systems in order to fully understand how they work. He extended his work on biological complexity into a general theory of systems in a book titled "General Systems Theory".

After James Watson and Francis Crick published the structure of DNA in the early 1950s, General Systems Theory lost many of its adherents in the physical and biological sciences.
However, systems theory remained popular in the social sciences long after its demise in the physical and biological sciences.

Michael Behe developed his ideas on the concept around 1992, in the early days of the 'wedge movement', and first presented his ideas about "irreducible complexity" in June 1993 when the "Johnson-Behe cadre of scholars" met at Pajaro Dunes in California. He set out his ideas in the second edition of "Of Pandas and People" published in 1993, extensively revising Chapter 6 "Biochemical Similarities" with new sections on the complex mechanism of blood clotting and on the origin of proteins.

He first used the term "irreducible complexity" in his 1996 book "Darwin's Black Box", to refer to certain complex biochemical cellular systems. He posits that evolutionary mechanisms cannot explain the development of such "irreducibly complex" systems. Notably, Behe credits philosopher William Paley for the original concept (alone among the predecessors) and suggests that his application of the concept to biological systems is entirely original.

Intelligent design advocates argue that irreducibly complex systems must have been deliberately engineered by some form of intelligence.

In 2001, Michael Behe wrote: "[T]here is an asymmetry between my current definition of irreducible complexity and the task facing natural selection. I hope to repair this defect in future work." Behe specifically explained that the "current definition puts the focus on removing a part from an already functioning system", but the "difficult task facing Darwinian evolution, however, would not be to remove parts from sophisticated pre-existing systems; it would be to bring together components to make a new system in the first place". In the 2005 "Kitzmiller v. Dover Area School District" trial, Behe testified under oath that he "did not judge [the asymmetry] serious enough to [have revised the book] yet."

Behe additionally testified that the presence of irreducible complexity in organisms would not rule out the involvement of evolutionary mechanisms in the development of organic life. He further testified that he knew of no earlier "peer reviewed articles in scientific journals discussing the intelligent design of the blood clotting cascade," but that there were "probably a large number of peer reviewed articles in science journals that demonstrate that the blood clotting system is indeed a purposeful arrangement of parts of great complexity and sophistication." (The judge ruled that "intelligent design is not science and is essentially religious in nature".)

According to the theory of evolution, genetic variations occur without specific design or intent. The environment "selects" the variants that have the highest fitness, which are then passed on to the next generation of organisms. Change occurs by the gradual operation of natural forces over time, perhaps slowly, perhaps more quickly (see punctuated equilibrium). This process is able to adapt complex structures from simpler beginnings, or convert complex structures from one function to another (see spandrel). Most intelligent design advocates accept that evolution occurs through mutation and natural selection at the "micro level", such as changing the relative frequency of various beak lengths in finches, but assert that it cannot account for irreducible complexity, because none of the parts of an irreducible system would be functional or advantageous until the entire system is in place.

Behe uses the mousetrap as an illustrative example of this concept. A mousetrap consists of five interacting pieces: the base, the catch, the spring, the hammer, and the hold-down bar. All of these must be in place for the mousetrap to work, as the removal of any one piece destroys the function of the mousetrap. Likewise, he asserts that biological systems require multiple parts working together in order to function. Intelligent design advocates claim that natural selection could not create from scratch those systems for which science is currently unable to find a viable evolutionary pathway of successive, slight modifications, because the selectable function is only present when all parts are assembled.

In his 2008 book "Only A Theory", biologist Kenneth R. Miller challenges Behe's claim that the mousetrap is irreducibly complex. Miller observes that various subsets of the five components can be devised to form cooperative units, ones that have different functions from the mousetrap and so, in biological terms, could form functional spandrels before being adapted to the new function of catching mice. In an example taken from his high school experience, Miller recalls that one of his classmates...struck upon the brilliant idea of using an old, broken mousetrap as a spitball catapult, and it worked brilliantly... It had worked perfectly as something other than a mousetrap... my rowdy friend had pulled a couple of parts --probably the hold-down bar and catch-- off the trap to make it easier to conceal and more effective as a catapult... [leaving] the base, the spring, and the hammer. Not much of a mousetrap, but a helluva spitball launcher... I realized why [Behe's] mousetrap analogy had bothered me. It was wrong. The mousetrap is not irreducibly complex after all.

Other systems identified by Miller that include mousetrap components include the following:

The point of the reduction is that - in biology - most or all of the components were already at hand, by the time it became necessary to build a mousetrap. As such, it required far fewer steps to develop a mousetrap than to design all the components from scratch.

Thus, the development of the mousetrap, said to consist of five different parts which had no function on their own, has been reduced to one step: the assembly from parts that are already present, performing other functions.

Supporters of intelligent design argue that anything less than the complete form of such a system or organ would not work at all, or would in fact be a "detriment" to the organism, and would therefore never survive the process of natural selection. Although they accept that some complex systems and organs "can" be explained by evolution, they claim that organs and biological features which are "irreducibly complex" cannot be explained by current models, and that an intelligent designer must have created life or guided its evolution. Accordingly, the debate on irreducible complexity concerns two questions: whether irreducible complexity can be found in nature, and what significance it would have if it did exist in nature.

Behe's original examples of irreducibly complex mechanisms included the bacterial flagellum of "E. coli", the blood clotting cascade, cilia, and the adaptive immune system.

Behe argues that organs and biological features which are irreducibly complex cannot be wholly explained by current models of evolution. In explicating his definition of "irreducible complexity" he notes that:
An irreducibly complex system cannot be produced directly (that is, by continuously improving the initial function, which continues to work by the same mechanism) by slight, successive modifications of a precursor system, because any precursor to an irreducibly complex system that is missing a part is by definition nonfunctional.

Irreducible complexity is not an argument that evolution does not occur, but rather an argument that it is "incomplete". In the last chapter of "Darwin's Black Box", Behe goes on to explain his view that irreducible complexity is evidence for intelligent design. Mainstream critics, however, argue that irreducible complexity, as defined by Behe, can be generated by known evolutionary mechanisms. Behe's claim that no scientific literature adequately modeled the origins of biochemical systems through evolutionary mechanisms has been challenged by TalkOrigins. The judge in the "Dover" trial wrote "By defining irreducible complexity in the way that he has, Professor Behe attempts to exclude the phenomenon of exaptation by definitional fiat, ignoring as he does so abundant evidence which refutes his argument. Notably, the NAS has rejected Professor Behe's claim for irreducible complexity..."

Behe and others have suggested a number of biological features that they believed to be irreducibly complex.

The process of blood clotting or coagulation cascade in vertebrates is a complex biological pathway which is given as an example of apparent irreducible complexity.

The irreducible complexity argument assumes that the necessary parts of a system have always been necessary, and therefore could not have been added sequentially. However, in evolution, something which is at first merely advantageous can later become necessary. Natural selection can lead to complex biochemical systems being built up from simpler systems, or to existing functional systems being recombined as a new system with a different function. For example, one of the clotting factors that Behe listed as a part of the clotting cascade (Factor XII, also called Hageman factor) was later found to be absent in whales, demonstrating that it is not essential for a clotting system. Many purportedly irreducible structures can be found in other organisms as much simpler systems that utilize fewer parts. These systems, in turn, may have had even simpler precursors that are now extinct. Behe has responded to critics of his clotting cascade arguments by suggesting that homology is evidence for evolution, but not for natural selection.

The "improbability argument" also misrepresents natural selection. It is correct to say that a set of simultaneous mutations that form a complex protein structure is so unlikely as to be unfeasible, but that is not what Darwin advocated. His explanation is based on small accumulated changes that take place without a final goal. Each step must be advantageous in its own right, although biologists may not yet understand the reason behind all of them—for example, jawless fish accomplish blood clotting with just six proteins instead of the full ten.

The eye is an example of a supposedly irreducibly complex structure, due to its many elaborate and interlocking parts, seemingly all dependent upon one another. It is frequently cited by intelligent design and creationism advocates as an example of irreducible complexity. Behe used the "development of the eye problem" as evidence for intelligent design in "Darwin's Black Box". Although Behe acknowledged that the evolution of the larger anatomical features of the eye have been well-explained, he pointed out that the complexity of the minute biochemical reactions required at a molecular level for light sensitivity still defies explanation. Creationist Jonathan Sarfati has described the eye as evolutionary biologists' "greatest challenge as an example of superb 'irreducible complexity' in God's creation", specifically pointing to the supposed "vast complexity" required for transparency.

In an often misquoted passage from "On the Origin of Species", Charles Darwin appears to acknowledge the eye's development as a difficulty for his theory. However, the quote in context shows that Darwin actually had a very good understanding of the evolution of the eye (see fallacy of quoting out of context). He notes that "to suppose that the eye ... could have been formed by natural selection, seems, I freely confess, absurd in the highest possible degree". Yet this observation was merely a rhetorical device for Darwin. He goes on to explain that if gradual evolution of the eye could be shown to be possible, "the difficulty of believing that a perfect and complex eye could be formed by natural selection ... can hardly be considered real". He then proceeded to roughly map out a likely course for evolution using examples of gradually more complex eyes of various species.

Since Darwin's day, the eye's ancestry has become much better understood. Although learning about the construction of ancient eyes through fossil evidence is problematic due to the soft tissues leaving no imprint or remains, genetic and comparative anatomical evidence has increasingly supported the idea of a common ancestry for all eyes.

Current evidence does suggest possible evolutionary lineages for the origins of the anatomical features of the eye. One likely chain of development is that the eyes originated as simple patches of photoreceptor cells that could detect the presence or absence of light, but not its direction. When, via random mutation across the population, the photosensitive cells happened to have developed on a small depression, it endowed the organism with a better sense of the light's source. This small change gave the organism an advantage over those without the mutation. This genetic trait would then be "selected for" as those with the trait would have an increased chance of survival, and therefore progeny, over those without the trait. Individuals with deeper depressions would be able to discern changes in light over a wider field than those individuals with shallower depressions. As ever deeper depressions were advantageous to the organism, gradually, this depression would become a pit into which light would strike certain cells depending on its angle. The organism slowly gained increasingly precise visual information. And again, this gradual process continued as individuals having a slightly shrunken aperture of the eye had an advantage over those without the mutation as an aperture increases how collimated the light is at any one specific group of photoreceptors. As this trait developed, the eye became effectively a pinhole camera which allowed the organism to dimly make out shapes—the nautilus is a modern example of an animal with such an eye. Finally, via this same selection process, a protective layer of transparent cells over the aperture was differentiated into a crude lens, and the interior of the eye was filled with humours to assist in focusing images. In this way, eyes are recognized by modern biologists as actually a relatively unambiguous and simple structure to evolve, and many of the major developments of the eye's evolution are believed to have taken place over only a few million years, during the Cambrian explosion. Behe asserts that this is only an explanation of the gross anatomical steps, however, and not an explanation of the changes in discrete biochemical systems that would have needed to take place.

Behe maintains that the complexity of light sensitivity at the molecular level and the minute biochemical reactions required for those first "simple patches of photoreceptor[s]" still defies explanation, and that the proposed series of infinitesimal steps to get from patches of photoreceptors to a fully functional eye would actually be considered great, complex leaps in evolution if viewed on the molecular scale. Other intelligent design proponents claim that the evolution of the entire visual system would be difficult rather than the eye alone.

The flagella of certain bacteria constitute a molecular motor requiring the interaction of about 40 different protein parts. Behe presents this as a prime example of an irreducibly complex structure defined as "a single system composed of several well-matched, interacting parts that contribute to the basic function, wherein the removal of any one of the parts causes the system to effectively cease functioning", and argues that since "an irreducibly complex system that is missing a part is by definition nonfunctional", it could not have evolved gradually through natural selection.

Reducible complexity. In contrast to Behe's claims, many proteins can be deleted or mutated and the flagellum still works, even though sometimes at reduced efficiency. In fact, the composition of flagella is surprisingly diverse across bacteria with many proteins only found in some species but not others. Hence the flagellar apparatus is clearly very flexible in evolutionary terms and perfectly able to lose or gain protein components. Further studies have shown that, contrary to claims of "irreducible complexity", flagella and related protein transport mechanisms show evidence of evolution through Darwinian processes, providing case studies in how complex systems can evolve from simpler components. Multiple processes were involved in the evolution of the flagellum, including horizontal gene transfer.

Evolution from type three secretion systems. Scientists regard this argument as having been disproved in the light of research dating back to 1996 as well as more recent findings. They point out that the basal body of the flagella has been found to be similar to the Type III secretion system (TTSS), a needle-like structure that pathogenic germs such as "Salmonella" and "Yersinia pestis" use to inject toxins into living eucaryote cells. The needle's base has ten elements in common with the flagellum, but it is missing forty of the proteins that make a flagellum work. The TTSS system negates Behe's claim that taking away any one of the flagellum's parts would prevent the system from functioning. On this basis, Kenneth Miller notes that, "The parts of this supposedly irreducibly complex system actually have functions of their own." Studies have also shown that similar parts of the flagellum in different bacterial species can have different functions despite showing evidence of common descent, and that certain parts of the flagellum can be removed without completely eliminating its functionality.

Dembski has argued that phylogenetically, the TTSS is found in a narrow range of bacteria which makes it seem to him to be a late innovation, whereas flagella are widespread throughout many bacterial groups, and he argues that it was an early innovation. Against Dembski's argument, different flagella use completely different mechanisms, and publications show a plausible path in which bacterial flagella could have evolved from a secretion system.

The cilium construction of axoneme microtubules movement by the sliding of dynein protein was cited by Behe as an example of irreducible complexity. He further said that the advances in knowledge in the subsequent 10 years had shown that the complexity of intraflagellar transport for two hundred components cilium and many other cellular structures is substantially greater than was known earlier.

The bombardier beetle is able to defend itself by directing a spray of hot fluid at an attacker. The mechanism involves a system for mixing hydroquinones and hydrogen peroxide, which react violently to attain a temperature near boiling point, and in some species a nozzle which allows the spray to be directed accurately in any direction.

The unique combination of features of the bombardier beetle's defense mechanism—strongly exothermic reactions, boiling-hot fluids, and explosive release—have been claimed by creationists and proponents of intelligent design to be examples of irreducible complexity. Biologists such as the taxonomist Mark Isaak note however that step-by-step evolution of the mechanism could readily have occurred. In particular, quinones are precursors to sclerotin, used to harden the skeleton of many insects, while peroxide is a common by-product of metabolism.

Like intelligent design, the concept it seeks to support, irreducible complexity has failed to gain any notable acceptance within the scientific community.

Researchers have proposed potentially viable evolutionary pathways for allegedly irreducibly complex systems such as blood clotting, the immune system and the flagellum - the three examples Behe proposed. John H. McDonald even showed his example of a mousetrap to be reducible. If irreducible complexity is an insurmountable obstacle to evolution, it should not be possible to conceive of such pathways.

Niall Shanks and Karl H. Joplin, both of East Tennessee State University, have shown that systems satisfying Behe's characterization of irreducible biochemical complexity can arise naturally and spontaneously as the result of self-organizing chemical processes. They also assert that what evolved biochemical and molecular systems actually exhibit is "redundant complexity"—a kind of complexity that is the product of an evolved biochemical process. They claim that Behe overestimated the significance of irreducible complexity because of his simple, linear view of biochemical reactions, resulting in his taking snapshots of selective features of biological systems, structures, and processes, while ignoring the redundant complexity of the context in which those features are naturally embedded. They also criticized his over-reliance of overly simplistic metaphors, such as his mousetrap.

A computer model of the co-evolution of proteins binding to DNA in the peer-reviewed journal "Nucleic Acids Research" consisted of several parts (DNA binders and DNA binding sites) which contribute to the basic function; removal of either one leads immediately to the death of the organism. This model fits the definition of irreducible complexity exactly, yet it evolves. (The program can be run from Ev program.)

In addition, research published in the peer-reviewed journal "Nature" has shown that computer simulations of evolution demonstrate that it is possible for complex features to evolve naturally.

One can compare a mousetrap with a cat in this context. Both normally function so as to control the mouse population. The cat has many parts that can be removed leaving it still functional; for example, its tail can be bobbed, or it can lose an ear in a fight. Comparing the cat and the mousetrap, then, one sees that the mousetrap (which is not alive) offers better evidence, in terms of irreducible complexity, for intelligent design than the cat. Even looking at the mousetrap analogy, several critics have described ways in which the parts of the mousetrap could have independent uses or could develop in stages, demonstrating that it is not irreducibly complex.

Moreover, even cases where removing a certain component in an organic system will cause the system to fail do not demonstrate that the system could not have been formed in a step-by-step, evolutionary process. By analogy, stone arches are irreducibly complex—if you remove any stone the arch will collapse—yet humans build them easily enough, one stone at a time, by building over centering that is removed afterward. Similarly, naturally occurring arches of stone form by the weathering away of bits of stone from a large concretion that has formed previously.

Evolution can act to simplify as well as to complicate. This raises the possibility that seemingly irreducibly complex biological features may have been achieved with a period of increasing complexity, followed by a period of simplification.

A team led by Joseph Thornton, assistant professor of biology at the University of Oregon's Center for Ecology and Evolutionary Biology, using techniques for resurrecting ancient genes, reconstructed the evolution of an apparently irreducibly complex molecular system. The April 7, 2006 issue of "Science" published this research.

Irreducible complexity may not actually exist in nature, and the examples given by Behe and others may not in fact represent irreducible complexity, but can be explained in terms of simpler precursors. The theory of facilitated variation challenges irreducible complexity. Marc W. Kirschner, a professor and chair of Department of Systems Biology at Harvard Medical School, and John C. Gerhart, a professor in Molecular and Cell Biology, University of California, Berkeley, presented this theory in 2005. They describe how certain mutation and changes can cause apparent irreducible complexity. Thus, seemingly irreducibly complex structures are merely "very complex", or they are simply misunderstood or misrepresented.

The precursors of complex systems, when they are not useful in themselves, may be useful to perform other, unrelated functions. Evolutionary biologists argue that evolution often works in this kind of blind, haphazard manner in which the function of an early form is not necessarily the same as the function of the later form. The term used for this process is exaptation. The mammalian middle ear (derived from a jawbone) and the panda's thumb (derived from a wrist bone spur) provide classic examples. A 2006 article in "Nature" demonstrates intermediate states leading toward the development of the ear in a Devonian fish (about 360 million years ago). Furthermore, recent research shows that viruses play a heretofore unexpected role in evolution by mixing and matching genes from various hosts.

Arguments for irreducibility often assume that things started out the same way they ended up—as we see them now. However, that may not necessarily be the case. In the "Dover" trial an expert witness for the plaintiffs, Ken Miller, demonstrated this possibility using Behe's mousetrap analogy. By removing several parts, Miller made the object unusable as a mousetrap, but he pointed out that it was now a perfectly functional, if unstylish, tie clip.

Irreducible complexity can be seen as equivalent to an "uncrossable valley" in a fitness landscape. A number of mathematical models of evolution have explored the circumstances under which such valleys can, nevertheless, be crossed.

Some critics, such as Jerry Coyne (professor of evolutionary biology at the University of Chicago) and Eugenie Scott (a physical anthropologist and former executive director of the National Center for Science Education) have argued that the concept of irreducible complexity and, more generally, intelligent design is not falsifiable and, therefore, not scientific.

Behe argues that the theory that irreducibly complex systems could not have evolved can be falsified by an experiment where such systems are evolved. For example, he posits taking bacteria with no flagellum and imposing a selective pressure for mobility. If, after a few thousand generations, the bacteria evolved the bacterial flagellum, then Behe believes that this would refute his theory.

Other critics take a different approach, pointing to experimental evidence that they consider falsification of the argument for intelligent design from irreducible complexity. For example, Kenneth Miller describes the lab work of Barry G. Hall on E. coli as showing that "Behe is wrong".

Other evidence that irreducible complexity is not a problem for evolution comes from the field of computer science, which routinely uses computer analogues of the processes of evolution in order to automatically design complex solutions to problems. The results of such genetic algorithms are frequently irreducibly complex since the process, like evolution, both removes non-essential components over time as well as adding new components. The removal of unused components with no essential function, like the natural process where rock underneath a natural arch is removed, can produce irreducibly complex structures without requiring the intervention of a designer. Researchers applying these algorithms automatically produce human-competitive designs—but no human designer is required.

Intelligent design proponents attribute to an intelligent designer those biological structures they believe are irreducibly complex and therefore they say a natural explanation is insufficient to account for them. However, critics view irreducible complexity as a special case of the "complexity indicates design" claim, and thus see it as an argument from ignorance and as a God-of-the-gaps argument.

Eugenie Scott and Glenn Branch of the National Center for Science Education note that intelligent design arguments from irreducible complexity rest on the false assumption that a lack of knowledge of a natural explanation allows intelligent design proponents to assume an intelligent cause, when the proper response of scientists would be to say that we don't know, and further investigation is needed. Other critics describe Behe as saying that evolutionary explanations are not detailed enough to meet his standards, while at the same time presenting intelligent design as exempt from having to provide any positive evidence at all.

Irreducible complexity is at its core an argument against evolution. If truly irreducible systems are found, the argument goes, then intelligent design must be the correct explanation for their existence. However, this conclusion is based on the assumption that current evolutionary theory and intelligent design are the only two valid models to explain life, a false dilemma.

While testifying during the 2005 "Kitzmiller v. Dover Area School District" trial, Behe conceded that there are no peer-reviewed papers supporting his claims that complex molecular systems, like the bacterial flagellum, the blood-clotting cascade, and the immune system, were intelligently designed nor are there any peer-reviewed articles supporting his argument that certain complex molecular structures are "irreducibly complex."

In the final ruling of "Kitzmiller v. Dover Area School District", Judge Jones specifically singled out Behe and irreducible complexity:





</doc>
<doc id="15388" url="https://en.wikipedia.org/wiki?curid=15388" title="Religion in pre-Islamic Arabia">
Religion in pre-Islamic Arabia

Religion in pre-Islamic Arabia included indigenous animistic-polytheistic beliefs, as well as Christianity, Judaism, Mandaeism, and Iranian religions of Zoroastrianism, Mithraism, and Manichaeism. Arabian polytheism, the dominant form of religion in pre-Islamic Arabia, was based on veneration of deities and spirits. Worship was directed to various gods and goddesses, including Hubal and the goddesses al-Lāt, al-‘Uzzā, and Manāt, at local shrines and temples such as the Kaaba in Mecca. Deities were venerated and invoked through a variety of rituals, including pilgrimages and divination, as well as ritual sacrifice. Different theories have been proposed regarding the role of Allah in Meccan religion. Many of the physical descriptions of the pre-Islamic gods are traced to idols, especially near the Kaaba, which is said to have contained up to 360 of them.

Other religions were represented to varying, lesser degrees. The influence of the adjacent Roman, Aksumite, and Sasanian Empires resulted in Christian communities in the northwest, northeast, and south of Arabia. Christianity made a lesser impact, but secured some conversions, in the remainder of the peninsula. With the exception of Nestorianism in the northeast and the Persian Gulf, the dominant form of Christianity was Miaphysitism. The peninsula had been a destination for Jewish migration since Roman times, which had resulted in a diaspora community supplemented by local converts. Additionally, the influence of the Sasanian Empire resulted in Iranian religions being present in the peninsula. Zoroastrianism existed in the east and south, while there is evidence of Manichaeism or possibly Mazdakism being practiced in Mecca.

Until about the fourth century, almost all inhabitants of Arabia practiced polytheistic religions. Although significant Jewish and Christian minorities developed, polytheism remained the dominant belief system in pre-Islamic Arabia.

The contemporary sources of information regarding the pre-Islamic Arabian religion and pantheon include a small number of inscriptions and carvings, pre-Islamic poetry, external sources such as Jewish and Greek accounts, as well as the Muslim tradition, such as the Qur'an and Islamic writings. Nevertheless, information is limited.

One early attestation of Arabian polytheism was in Esarhaddon’s Annals, mentioning Atarsamain, Nukhay, Ruldaiu, and Atarquruma. Herodotus, writing in his "Histories", reported that the Arabs worshipped Orotalt (identified with Dionysus) and Alilat (identified with Aphrodite). Strabo stated the Arabs worshipped Dionysus and Zeus. Origen stated they worshipped Dionysus and Urania.

Muslim sources regarding Arabian polytheism include the eight-century "Book of Idols" by Hisham ibn al-Kalbi, which F.E. Peters argued to be the most substantial treatment of the religious practices of pre-Islamic Arabia, as well as the writings of the Yemeni historian al-Hasan al-Hamdani on south Arabian religious beliefs.

According to the "Book of Idols", descendants of the son of Abraham (Ishmael) who had settled in Mecca migrated to other lands. They carried holy stones from the Kaaba with them, erected them, and circumambulated them like the Kaaba. This, according to al-Kalbi led to the rise of idol worship. Based on this, it may be probable that Arabs originally venerated stones, later adopting idol-worship under foreign influences. The relationship between a god and a stone as his representation can be seen from the third-century work called the Syriac "Homily of Pseudo-Meliton" where he describes the pagan faiths of Syriac-speakers in northern Mesopotamia, who were mostly Arabs.

The pre-Islamic Arabian religions were polytheistic, with many of the deities' names known. Formal pantheons are more noticeable at the level of kingdoms, of variable sizes, ranging from simple city-states to collections of tribes. Tribes, towns, clans, lineages and families had their own cults too. Christian Julien Robin suggests that this structure of the divine world reflected the society of the time.

A large number of deities did not have proper names and were referred to by titles indicating a quality, a family relationship, or a locale preceded by "he who" or "she who" ("dhū" or "dhāt" respectively).

The religious beliefs and practices of the nomadic Bedouin were distinct from those of the settled tribes of towns such as Mecca. Nomadic religious belief systems and practices are believed to have included fetishism, totemism and veneration of the dead but were connected principally with immediate concerns and problems and did not consider larger philosophical questions such as the afterlife. Settled urban Arabs, on the other hand, are thought to have believed in a more complex pantheon of deities. While the Meccans and the other settled inhabitants of the Hejaz worshiped their gods at permanent shrines in towns and oases, the Bedouin practiced their religion on the move.

In south Arabia, "mndh’t" were anonymous guardian spirits of the community and the ancestor spirits of the family. They were known as ‘the sun ("shms") of their ancestors’.

In north Arabia, "ginnaye" were known from Palmyrene inscriptions as “the good and rewarding gods” and were probably related to the "jinn" of west and central Arabia. Unlike jinn, ginnaye could not hurt nor possess humans and were much more similar to the Roman genius. According to common Arabian belief, soothsayers, pre-Islamic philosophers, and poets were inspired by the jinn. However, jinn were also feared and thought to be responsible for causing various diseases and mental illnesses.

Aside from benevolent gods and spirits, there existed malevolent beings. These beings were not attested in the epigraphic record, but were alluded to in pre-Islamic Arabic poetry, and their legends were collected by later Muslim authors.

Commonly mentioned are ghouls. Etymologically, the English word “ghoul” was derived from the Arabic "ghul", from "ghala", “to seize”, related to the Sumerian "galla". They are said to have a hideous appearance, with feet like those of an ass. Arabs were said to utter the following couplet if they should encounter one: “Oh ass-footed one, just bray away, we won’t leave the desert plain nor ever go astray.”

Christian Julien Robin notes that all the known south Arabian divinities had a positive or protective role and that evil powers were only alluded to but were never personified.

Some scholars postulate that in pre-Islamic Arabia, including in Mecca, Allah was considered to be a deity, possibly a creator deity or a supreme deity in a polytheistic pantheon. The word "Allah" (from the Arabic "al-ilah" meaning "the god") may have been used as a title rather than a name. The concept of "Allah" may have been vague in the Meccan religion. According to Islamic sources, Meccans and their neighbors believed that the goddesses Al-lāt, Al-‘Uzzá, and Manāt were the daughters of Allah.

Regional variants of the word "Allah" occur in both pagan and Christian pre-Islamic inscriptions. References to Allah are found in the poetry of the pre-Islamic Arab poet Zuhayr bin Abi Sulma, who lived a generation before Muhammad, as well as pre-Islamic personal names. Muhammad's father's name was "ʿAbd-Allāh", meaning "the servant of Allah".

Charles Russell Coulter and Patricia Turner considered that Allah's name may be derived from a pre-Islamic god called Ailiah and is similar to El, Il, Ilah and Jehova. They also considered some of his characteristics to be seemingly based on lunar deities like Almaqah, Kahl, Shaker, Wadd and Warakh. Alfred Guillaume states that the connection between Ilah that came to form Allah and ancient Babylonian "Il" or "El" of ancient Israel is not clear. Wellhausen states that Allah was known from Jewish and Christian sources and was known to pagan Arabs as the supreme god. Winfried Corduan doubts the theory of Allah of Islam being linked to a moon god, stating that the term Allah functions as a generic term, like the term El-Elyon used as a title for the god Sin.

South Arabian inscriptions from the fourth century AD refer to a god called Rahman ("The Merciful One") who had a monotheistic cult and was referred to as the "Lord of heaven and Earth". Aaron W. Hughes states that scholars are unsure whether he developed from the earlier polytheistic systems or developed due to the increasing significance of the Christian and Jewish communities, and that it is difficult to establish whether Allah was linked to Rahmanan. Maxime Rodinson, however, considers one of Allah's names "Ar-Rahman" to have been used in the form of Rahmanan earlier.

Al-Lāt, Al-‘Uzzá and Manāt were common names used for multiple goddesses across Arabia. G.R. Hawting states that modern scholars have frequently associated the names of Arabian goddesses Al-lāt, Al-‘Uzzá and Manāt with cults devoted to celestial bodies, particularly Venus, drawing upon evidence external to the Muslim tradition as well as in relation to Syria, Mesopotamia and the Sinai Peninsula.

Allāt (Arabic: اللات‎) or al-Lāt was worshipped throughout the ancient Near East with various associations. Herodotus in the 5th century BC identifies "Alilat" (Greek: Ἀλιλάτ) as the Arabic name for Aphrodite (and, in another passage, for Urania), which is strong evidence for worship of Allāt in Arabia at that early date. Al-‘Uzzá (Arabic: العزى‎) was a fertility goddess or possibly a goddess of love. Manāt (Arabic: مناة‎) was the goddess of destiny.

Al-Lāt's cult was spread in Syria and northern Arabia. From Safaitic and Hismaic inscriptions, it is probable that she was worshiped as Lat ("lt"). F. V. Winnet saw al-Lat as a lunar deity due to the association of a crescent with her in 'Ayn esh-Shallāleh and a Lihyanite inscription mentioning the name of Wadd, the Minaean moon god, over the title of "'fkl lt". René Dussaud and Gonzague Ryckmans linked her with Venus while others have thought her to be a solar deity. John F. Healey considers that al-Uzza actually might have been an epithet of al-Lāt before becoming a separate deity in the Meccan pantheon. Paola Corrente, writing in "Redefining Dionysus", considers she might have been a god of vegetation or a celestial deity of atmospheric phenomena and a sky deity.

According to F.E. Peters, "one of the characteristics of Arab paganism as it has come down to us is the absence of a mythology, narratives that might serve to explain the origin or history of the gods." Many of the deities have epithets, but are lacking myths or narratives to decode the epithets, making them generally uninformative.

Cult images of a deity were most often an unworked stone block. The most common name for these stone blocks was derived from the Semitic "nsb" ("to be stood upright"), but other names were used, such as Nabataean "masgida" ("place of prostration") and Arabic "duwar" ("object of circumambulation", this term often occurs in pre-Islamic Arabic poetry). These god-stones were usually a free-standing slab, but Nabataean god-stones are usually carved directly on the rock face. Facial features may be incised on the stone (especially in Nabataea), or astral symbols (especially in south Arabia). Under Greco-Roman influence, an anthropomorphic statue might be used instead.

The "Book of Idols" describes two types of statues: idols ("sanam") and images ("wathan"). If a statue were made of wood, gold, or silver, after a human form, it would be an idol, but if the statue were made of stone, it would be an image.

Representation of deities in animal-form was common in south Arabia, such as the god Sayin from Hadhramaut, who was represented as either an eagle fighting a serpent or a bull.

Sacred places are known as "hima", "haram" or "mahram", and within these places, all living things were considered inviolable and violence was forbidden. In most of Arabia, these places would take the form of open-air sanctuaries, with distinguishing natural features such as springs and forests. Cities would contain temples, enclosing the sacred area with walls, and featuring ornate structures.

Sacred areas often had a guardian or a performer of cultic rites. These officials were thought to tend the area, receive offerings, and perform divination. They are known by many names, probably based on cultural-linguistic preference: "afkal" was used in the Hejaz, "kâhin" was used in the Sinai-Negev-Hisma region, and "kumrâ" was used in Aramaic-influenced areas. In south Arabia, "rsw" and "'fkl" were used to refer to priests, and other words include "qyn" ("administrator") and "mrtd" ("consecrated to a particular divinity"). A more specialized staff is thought to have existed in major sanctuaries.

Pilgrimages to sacred places would be made at certain times of the year. Pilgrim fairs of central and northern Arabia took place in specific months designated as violence-free, allowing several activities to flourish, such as trade, though in some places only exchange was permitted.

The most important pilgrimage in Saba' was probably the pilgrimage of Almaqah at Ma'rib, performed in the month of dhu-Abhi (roughly in July). Two references attest the pilgrimage of Almaqah dhu-Hirran at 'Amran. The pilgrimage of Ta'lab Riyam took place in Mount Tur'at and the Zabyan temple at Hadaqan, while the pilgrimage of Dhu-Samawi, the god of the Amir tribe, took place in Yathill. Aside from Sabaean pilgrimages, the pilgrimage of Sayin took place at Shabwa.

The pilgrimage of Mecca involved the stations of Mount Arafat, Muzdalifah, Mina and central Mecca that included Safa and Marwa as well as the Kaaba. Pilgrims at the first two stations performed "wuquf" or standing in adoration. At Mina, animals were sacrificed. The procession from Arafat to Muzdalifah, and from Mina to Mecca, in a pre-reserved route towards idols or an idol, was termed "ijaza" and "ifada", with the latter taking place before sunset. At Jabal Quzah, fires were started during the sacred month.

Nearby the Kaaba was located the betyl which was later called "Maqam Ibrahim"; a place called "al-Ḥigr" which Aziz al-Azmeh takes to be reserved for consecrated animals, basing his argument on a Sabaean inscription mentioning a place called "mḥgr" which was reserved for animals; and the Well of Zamzam. Both Safa and Marwa were adjacent to two sacrificial hills, one called Muṭ'im al Ṭayr and another Mujāwir al-Riḥ which was a pathway to Abu Kubais from where the Black Stone is reported to have originated.

Meccan pilgrimages differed according to the rites of different cult associations, in which individuals and groups joined together for religious purposes. The "Ḥilla" association performed the "hajj" in autumn season while the "Ṭuls" and "Ḥums" performed the "umrah" in spring.

The "Ḥums" were the Quraysh, Banu Kinanah, Banu Khuza'a and Banu 'Amir. They did not perform the pilgrimage outside the zone of Mecca's "haram", thus excluding Mount Arafat. They also developed certain dietary and cultural restrictions. According to "Kitab al-Muhabbar", the "Ḥilla" denoted most of the Banu Tamim, Qays, Rabi`ah, Qūḍa'ah, Ansar, Khath'am, Bajīlah, Banu Bakr ibn Abd Manat, Hudhayl, Asad, Tayy and Bariq. The "Ṭuls" comprised the tribes of Yemen and Hadramaut, 'Akk, Ujayb and Īyād. The "Basl" recognised at least eight months of the calendar as holy. There was also another group which didn't recognize the sanctity of Mecca's "haram" or holy months, unlike the other four.

In south Arabia, oracles were regarded as "ms’l", or "a place of asking", and that deities interacted by "hr’yhw" ("making them see") a vision, a dream, or even direct interaction. Otherwise deities interacted indirectly through a medium.

There were three methods of chance-based divination attested in pre-Islamic Arabia; two of these methods, making marks in the sand or on rocks and throwing pebbles are poorly attested. The other method, the practice of randomly selecting an arrow with instructions, was widely attested and was common throughout Arabia. A simple form of this practice was reportedly performed before the image of Dhu'l-Khalasa by a certain man, sometimes said to be the Kindite poet Imru al-Qays according to al-Kalbi. A more elaborate form of the ritual was performed in before the image of Hubal. This form of divination was also attested in Palmyra, evidenced by an honorific inscription in the temple of al-Lat.

The most common offerings were animals, crops, food, liquids, inscribed metal plaques or stone tablets, aromatics, edifices and manufactured objects. Camel-herding Arabs would devote some of their beasts to certain deities. The beasts would have their ears slit and would be left to pasture without a herdsman, allowing them to die a natural death.

Pre-Islamic Arabians, especially pastoralist tribes, sacrificed animals as an offering to a deity. This type of offering was common and involved domestic animals such as camels, sheep and cattle, while game animals and poultry were rarely or never mentioned. Sacrifice rites were not tied to a particular location though they were usually practiced in sacred places. Sacrifice rites could be performed by the devotee, though according to Hoyland, women were probably not allowed. The victim's blood, according to pre-Islamic Arabic poetry and certain south Arabian inscriptions, was also 'poured out' on the altar stone, thus forming a bond between the human and the deity. According to Muslim sources, most sacrifices were concluded with communal feasts.

In south Arabia, beginning with the Christian era, or perhaps a short while before, statuettes were presented before the deity, known as "slm" (male) or "slmt" (female).

Human sacrifice was sometimes carried out in Arabia. The victims were generally prisoners of war, who represented the god's part of the victory in booty, although other forms might have existed.

Blood sacrifice was definitely practiced in south Arabia, but few allusions to the practice are known, apart from some Minaean inscriptions.

In the Hejaz, menstruating women were not allowed to be near the cult images. The area where Isaf and Na'ila's images stood was considered out-of-bounds for menstruating women. This was reportedly the same with Manaf. According to the "Book of Idols", this rule applied to all the "idols". This was also the case in south Arabia, as attested in a south Arabian inscription from al-Jawf.

Sexual intercourse in temples was prohibited, as attested in two south Arabian inscriptions. One legend concerning Isaf and Na'ila, when two lovers made love in the Kaaba and were petrified, joining the idols in the Kaaba, echoes this prohibition.

The Dilmun civilization, which existed along the Persian Gulf coast and Bahrain until the 6th century BC, worshipped a pair of deities, Inzak and Meskilak. It is not known whether these were the only deities in the pantheon or whether there were others. The discovery of wells at the sites of a Dilmun temple and a shrine suggests that sweet water played an important part in religious practices.

In the subsequent Greco-Roman period, there is evidence that the worship of non-indigenous deities was brought to the region by merchants and visitors. These included Bel, a god popular in the Syrian city of Palmyra, the Mesopotamian deities Nabu and Shamash, the Greek deities Poseidon and Artemis and the west Arabian deities Kahl and Manat.

The main sources of religious information in pre-Islamic south Arabia are inscriptions, which number in the thousands, as well as the Quran, complemented by archaeological evidence. 

The civilizations of south Arabia are considered to have the most developed pantheon in the Arabian peninsula. In south Arabia, the most common god was 'Athtar, who was considered remote. The patron deity ("shym") was considered to be of much more immediate significance than 'Athtar. Thus, the kingdom of Saba' had Almaqah, the kingdom of Ma'in had Wadd, the kingdom of Qataban had 'Amm, and the kingdom of Hadhramaut had Sayin. Each people was termed the “children” of their respective patron deity. Patron deities played a vital role in sociopolitical terms, their cults serving as the focus of a person’s cohesion and loyalty.

Evidence from surviving inscriptions suggests that each of the southern kingdoms had its own pantheon of three to five deities, the major deity always being a god. For example, the pantheon of Saba comprised Almaqah, the major deity, together with 'Athtar, Haubas, Dhat-Himyam, and Dhat-Badan. The main god in Ma'in and Himyar was 'Athtar, in Qataban it was Amm, and in Hadhramaut it was Sayin. 'Amm was a lunar deity and was associated with the weather, especially lightning. One of the most frequent titles of the god Almaqah was "Lord of Awwam".

Anbay was an oracular god of Qataban and also the spokesman of Amm. His name was invoked in royal regulations regarding water supply. Anbay's name was related to that of the Babylonian deity Nabu. Hawkam was invoked alongside Anbay as god of "command and decision" and his name is derived from the root word "to be wise".

Each kingdom's central temple was the focus of worship for the main god and would be the destination for an annual pilgrimage, with regional temples dedicated to a local manifestation of the main god. Other beings worshipped included local deities or deities dedicated to specific functions as well as deified ancestors.

The encroachment of northern Arab tribes into south Arabia also introduced northern Arab deities into the region. The three goddesses al-Lat, al-Uzza and Manat became known as Lat/Latan, Uzzayan and Manawt. Uzzayan’s cult in particular was widespread in south Arabia, and in Qataban, she was invoked as a guardian of the final royal palace. Lat/Latan was not significant in south Arabia, but appears to be popular with the Arab tribes bordering Yemen. Other Arab deities include Dhu-Samawi, a god originally worshipped by the Amir tribe, and Kahilan, perhaps related to Kahl of Qaryat al-Faw.

Bordering Yemen, the Azd Sârat tribe of the Asir region was said to have worshipped Dhu'l-Shara, Dhu'l-Kaffayn, Dhu'l-Khalasa and A'im. According to the "Book of Idols", Dhu'l-Kaffayn originated from a clan of the Banu Daws. In addition to being worshipped among the Azd, Dushara is also reported to have a shrine amongst the Daws. Dhu’l-Khalasa was an oracular god and was also worshipped by the Bajila and Khatham tribes.

Before conversion to Christianity, the Aksumites followed a polytheistic religion that was similar to that of Southern Arabia. The lunar god Hawbas was worshiped in South Arabia and Aksum. The name of the god Astar, a sky-deity was related to that of 'Attar. The god Almaqah was worshiped at Hawulti-Melazo. The South Arabian gods in Aksum included Dhat-Himyam and Dhat-Ba'adan. A stone later reused for the church of Enda-Cerqos at Melazo mentions these gods. Hawbas is also mentioned on an altar and sphinx in Dibdib. The name of Nrw who is mentioned in Aksum inscriptions is related to that of the South Arabian god Nawraw, a deity of stars.

The Himyarite kings radically opposed polytheism in favor of Judaism, beginning officially in 380. The last trace of polytheism in south Arabia, an inscription commemorating a construction project with a polytheistic invocation, and another, mentioning the temple of Ta’lab, all date from just after 380 (the former dating to the rule of the king Dhara’amar Ayman, and the latter dating to the year 401–402). The rejection of polytheism from the public sphere did not mean the extinction of it altogether, as polytheism likely continued in the private sphere.

The Kindah tribe’s chief god was Kahl, whom their capital Qaryat Dhat Kahl (modern Qaryat al-Faw) was named for. His name appears in the form of many inscriptions and rock engravings on the slopes of the Tuwayq, on the walls of the souk of the village, in the residential houses and on the incense burners. An inscription in Qaryat Dhat Kahl invokes the gods Kahl, Athtar al-Shariq and Lah.

According to Islamic sources, the Hejaz region was home to three important shrines dedicated to al-Lat, al-’Uzza and Manat. The shrine and idol of al-Lat, according to the "Book of Idols", once stood in Ta'if, and was primarily worshipped by the Banu Thaqif tribe. Al-’Uzza’s principal shrine was in Nakhla and was the chief-goddess of the Quraysh tribe. Manāt’s idol, reportedly the oldest of the three, was erected on the seashore between Medina and Mecca, and was honored by the Aws and Khazraj tribes. Inhabitants of several areas venerated Manāt, performing sacrifices before her idol, and pilgrimages of some were not considered completed until they visited Manāt and shaved their heads.

In the Muzdalifah region near Mecca, the god Quzah, who is a god of rains and storms, was worshipped. In pre-Islamic times pilgrims used to halt at the "hill of Quzah" before sunrise. Qusai ibn Kilab is traditionally reported to have introduced the association of fire worship with him on Muzdalifah.

Various other deities were venerated in the area by specific tribes, such as the god Suwa' by the Banu Hudhayl tribe and the god Nuhm by the Muzaynah tribe.

The majority of extant information about Mecca during the rise of Islam and earlier times comes from the text of the Quran itself and later Muslim sources such as the prophetic biography literature dealing with the life of Muhammad and the "Book of Idols". Alternative sources are so fragmentary and specialized that writing a convincing history of this period based on them alone is impossible. Several scholars hold that the sīra literature is not independent of the Quran but has been fabricated to explain the verses of the Quran. There is evidence to support the contention that some reports of the sīras are of dubious validity, but there is also evidence to support the contention that the sīra narratives originated independently of the Quran. Compounding the problem is that the earliest extant Muslim historical works, including the sīras, were composed in their definitive form more than a century after the beginning of the Islamic era. Some of these works were based on subsequently lost earlier texts, which in their turn recorded a fluid oral tradition. Scholars do not agree as to the time when such oral accounts began to be systematically collected and written down, and they differ greatly in their assessment of the historical reliability of the available texts.

The Kaaba, whose environs were regarded as sacred ("haram"), became a national shrine under the custodianship of the Quraysh, the chief tribe of Mecca, which made the Hejaz the most important religious area in north Arabia. Its role was solidified by a confrontation with the Christian king Abraha, who controlled much of Arabia from a seat of power in Yemen in the middle of the sixth century. Abraha had recently constructed a splendid church in Sana'a, and he wanted to make that city a major center of pilgrimage, but Mecca's Kaaba presented a challenge to his plan. Abraha found a pretext for an attack on Mecca, presented by different sources alternatively as pollution of the church by a tribe allied to the Meccans or as an attack on Abraha's grandson in Najran by a Meccan party. The defeat of the army he assembled to conquer Mecca is recounted with miraculous details by the Islamic tradition and is also alluded to in the Quran and pre-Islamic poetry. After the battle, which probably occurred around 565, the Quraysh became a dominant force in western Arabia, receiving the title "God's people" ("ahl Allah") according to Islamic sources, and formed the cult association of "ḥums", which tied members of many tribes in western Arabia to the Kaaba.

According to tradition, the Kaaba was a cube-like, originally roofless structure housing a black stone venerated as a fetish. The sanctuary was dedicated to Hubal (Arabic: هبل‎), who, according to some sources, was worshiped as the greatest of the 360 idols the Kaaba contained, which probably represented the days of the year. Ibn Ishaq and Ibn Al-Kalbi both report that the human-shaped idol of Hubal made of precious stone came into the possession of the Quraysh with its right hand broken off and that the Quraysh made a hand of gold to replace it. A soothsayer performed divination in the shrine by drawing ritual arrows, and vows and sacrifices were made to assure success. Marshall Hodgson argues that relations with deities and fetishes in pre-Islamic Mecca were maintained chiefly on the basis of bargaining, where favors were expected in return for offerings. A deity's or oracle's failure to provide the desired response was sometimes met with anger.

Different theories have been proposed regarding the role of Allah in Meccan religion. According to one hypothesis, which goes back to Julius Wellhausen, Allah (the supreme deity of the tribal federation around Quraysh) was a designation that consecrated the superiority of Hubal (the supreme deity of Quraysh) over the other gods. However, there is also evidence that Allah and Hubal were two distinct deities. According to that hypothesis, the Kaaba was first consecrated to a supreme deity named Allah and then hosted the pantheon of Quraysh after their conquest of Mecca, about a century before the time of Muhammad. Some inscriptions seem to indicate the use of Allah as a name of a polytheist deity centuries earlier, but we know nothing precise about this use. Some scholars have suggested that Allah may have represented a remote creator god who was gradually eclipsed by more particularized local deities. There is disagreement on whether Allah played a major role in the Meccan religious cult. No iconic representation or idol of Allah is known to have existed.

The three chief goddesses of Meccan religion were al-Lat, Al-‘Uzzá, and Manāt, who were called the daughters of Allah. Egerton Sykes meanwhile states that Al-lāt was the female counterpart of Allah while Uzza was a name given by Banu Ghatafan to the planet Venus.

Other deities of the Quraysh in Mecca included Manaf, Isaf and Na’ila. Although the early Arab historian Al-Tabari calls Manaf (Arabic: مناف‎), "one of the greatest deities of Mecca," very little information is available about it. Women touched his idol as a token of blessing, and kept away from it during menstruation. Gonzague Ryckmans described this as a practice peculiar to Manaf, but according to the Encyclopedia of Islam, a report from Ibn Al-Kalbi indicates that it was common to all idols. Muhammad's great-great-grandfather's name was Abd Manaf which means "slave of Manaf". He is thought by some scholars to be a sun-god. The idols of Isāf and Nā'ila were located near the Black Stone with a "talbiyah" performed to Isāf during sacrifices. Various legends existed about the idols, including one that they were petrified after they committed adultery in the Kaaba.

The pantheon of the Quraysh was not identical with that of the tribes who entered into various cult and commercial associations with them, especially that of the "hums". Christian Julien Robin argues that the former was composed principally of idols that were in the sanctuary of Mecca, including Hubal and Manaf, while the pantheon of the associations was superimposed on it, and its principal deities included the three goddesses, who had neither idols nor a shrine in that city.

The second half of the sixth century was a period of political disorder in Arabia and communication routes were no longer secure. Religious divisions were an important cause of the crisis. Judaism became the dominant religion in Yemen while Christianity took root in the Persian Gulf area. In line with the broader trends of the ancient world, Arabia yearned for a more spiritual form of religion and began believing in afterlife, while the choice of religion increasingly became a personal rather than communal choice. While many were reluctant to convert to a foreign faith, those faiths provided intellectual and spiritual reference points, and the old pagan vocabulary of Arabic began to be replaced by Jewish and Christian loanwords from Aramaic everywhere, including Mecca. The distribution of pagan temples supports Gerald Hawting's argument that Arabian polytheism was marginalized in the region and already dying in Mecca on the eve of Islam. The practice of polytheistic cults was increasingly limited to the steppe and the desert, and in Yathrib (later known as Medina), which included two tribes with polytheistic majorities, the absence of a public pagan temple in the town or its immediate neighborhood indicates that polytheism was confined to the private sphere. Looking at the text of the Quran itself, Hawting has also argued that the criticism of idolaters and polytheists contained in Quran is in fact a hyperbolic reference to other monotheists, in particular the Arab Jews and Arab Christians, whose religious beliefs were considered imperfect. According to some traditions, the Kaaba contained no statues, but its interior was decorated with images of Mary and Jesus, prophets, angels, and trees.

To counter the effects of anarchy, the institution of sacred months, during which every act of violence was prohibited, was reestablished. During those months, it was possible to participate in pilgrimages and fairs without danger. The Quraysh upheld the principle of two annual truces, one of one month and the second of three months, which conferred a sacred character to the Meccan sanctuary. The cult association of "hums", in which individuals and groups partook in the same rites, was primarily religious, but it also had important economic consequences. Although, as Patricia Crone has shown, Mecca could not compare with the great centers of caravan trade on the eve of Islam, it was probably one of the most prosperous and secure cities of the peninsula, since, unlike many of them, it did not have surrounding walls. Pilgrimage to Mecca was a popular custom. Some Islamic rituals, including processions around the Kaaba and between the hills of al-Safa and Marwa, as well as the salutation "we are here, O Allah, we are here" repeated on approaching the Kaaba are believed to have antedated Islam. Spring water acquired a sacred character in Arabia early on and Islamic sources state that the well of Zamzam became holy long before the Islamic era.

According to Ibn Sa'd, the opposition in Mecca started when the prophet of Islam, Muhammad, delivered verses that "spoke shamefully of the idols they (the Meccans) worshiped other than Himself (God) and mentioned the perdition of their fathers who died in disbelief." According to William Montogomery Watt, as the ranks of Muhammad's followers swelled, he became a threat to the local tribes and the rulers of the city, whose wealth rested upon the Kaaba, the focal point of Meccan religious life, which Muhammad threatened to overthrow. Muhammad's denunciation of the Meccan traditional religion was especially offensive to his own tribe, the Quraysh, as they were the guardians of the Kaaba.

The conquest of Mecca around 629-630 AD led to the destruction of the idols around the Kaaba, including Hubal. Following the conquest, shrines and temples dedicated to deities were destroyed, such as the shrines to al-Lat, al-’Uzza and Manat in Ta’if, Nakhla and al-Qudayd respectively.

Less complex societies outside south Arabia often had smaller pantheons, with the patron deity having much prominence. The deities attested in north Arabian inscriptions include Ruda, Nuha, Allah, Dathan, and Kahl. Inscriptions in a North Arabian dialect in the region of Najd referring to Nuha describe emotions as a gift from him. In addition, they also refer to Ruda being responsible for all things good and bad. 

The Safaitic tribes in particular prominently worshipped the goddess al-Lat as a bringer of prosperity. The Syrian god Baalshamin was also worshipped by Safaitic tribes and is mentioned in Safaitic inscriptions.

Religious worship amongst the Qedarites, an ancient tribal confederation that was probably subsumed into Nabataea around the 2nd century AD, was centered around a polytheistic system in which women rose to prominence. Divine images of the gods and goddesses worshipped by Qedarite Arabs, as noted in Assyrian inscriptions, included representations of Atarsamain, Nuha, Ruda, Dai, Abirillu and Atarquruma. The female guardian of these idols, usually the reigning queen, served as a priestess ("apkallatu", in Assyrian texts) who communed with the other world. There is also evidence that the Qedar worshipped al-Lat to whom the inscription on a silver bowl from a king of Qedar is dedicated. In the Babylonian Talmud, which was passed down orally for centuries before being transcribed c. 500 AD, in tractate Taanis (folio 5b), it is said that most Qedarites worshiped pagan gods.

The Aramaic stele inscription discovered by Charles Hubert in 1880 at Tayma mentions the introduction of a new god called Salm of "hgm" into the city's pantheon being permitted by three local gods - Salm of Mahram who was the chief god, Shingala and Ashira. The name Salm means "image" or "idol". 

The Midianites, a people referred to in the Book of Genesis and located in north-western Arabia, may have worshipped Yahweh. Indeed, some scholars believe that Yahweh was originally a Midianite god and that he was subsequently adopted by the Israelites. An Egyptian temple of Hathor continued to be used during the Midianite occupation of the site, although images of Hathor were defaced suggesting Midianite opposition. They transformed it into a desert tent-shrine set up with a copper sculpture of a snake.

The Lihyanites worshipped the god Dhu-Ghabat and rarely turned to others for their needs. Dhu-Ghabat's name means "he of the thicket", based on the etymology of "gabah", meaning forest or thicket. The god al-Kutba', a god of writing probably related to a Babylonian deity and perhaps was brought into the region by the Babylonian king Nabonidus, is mentioned in Lihyanite inscriptions as well. The worship of the Hermonian gods Leucothea and Theandrios was spread from Phoenicia till Arabia.

According to the "Book of Idols", the Tayy tribe worshipped al-Fals, whose idol stood on Jabal Aja, while the Kalb tribe worshipped Wadd, who had an idol in Dumat al-Jandal.

The Nabataeans worshipped primarily northern Arabian deities. Under foreign influences, they also incorporated foreign deities and elements into their beliefs.

The Nabataeans’ chief-god is Dushara. In Petra, the only major goddess is al-’Uzza, assuming the traits of Isis, Tyche and Aphrodite. It is unknown if her worship and identity is related to her cult at Nakhla and others. The Nabatean inscriptions define Allāt and Al-Uzza as the "bride of Dushara". Al-Uzza may have been an epithet of Allāt in the Nabataean religion according to John F. Healey.

Outside Petra, other deities were worshipped; for example, Hubal and Manat were invoked in the Hejaz, and al-Lat was invoked in the Hauran and the Syrian desert. The Nabataean king Obodas I, who founded Obodat, was deified and worshipped as a god. They also worshipped Shay al-Qawm, al-Kutba', and various Greco-Roman deities such as Nike and Tyche. Maxime Rodinson suggests that Hubal, who was popular in Mecca, had a Nabataean origin.
The worship of Pakidas, a Nabataean god, is attested at Gerasa alongside Hera in an inscription dated to the first century A.D. while an Arabian god is also attested by three inscriptions dated to the second century.

The Nabataeans were known for their elaborate tombs, but they were not just for show; they were meant to be comfortable places for the dead. Petra has many "sacred high places" which include altars that have usually been interpreted as places of human sacrifice, although, since the 1960s, an alternative theory that they are "exposure platforms" for placing the corpses of the deceased as part of a funerary ritual has been put forward. However, there is, in fact, little evidence for either proposition.

Palmyra was a cosmopolitan society, with its population being a mix of Aramaeans and Arabs. The Arabs of Palmyra worshipped al-Lat, Rahim and Shamash. The temple of al-Lat was established by the Bene Ma'zin tribe, who were probably an Arab tribe. The nomads of the countryside worshipped a set of deities, bearing Arab names and attributes, most prominent of them was Abgal, who himself is not attested in Palmyra itself. Ma'n, an Arab god, was worshipped alongside Abgal in a temple dedicated in 195 AD at Khirbet Semrin in the Palmyrene region while an inscription dated 194 AD at Ras esh-Shaar calls him the "good and bountiful god". A stele at Ras esh-Shaar shows him riding a horse with a lance while the god Saad is riding a camel. Abgal, Ma'n and Sa'd were known as the "genii".

The god Ashar was represented on a stele in Dura-Europos alongside another god Sa'd. The former was represented on a horse with Arab dress while the other was shown standing on the ground. Both had Parthian hairstyle, large facial hair and moustaches as well as similar clothing. Ashar's name is found to have been used in a theophoric manner among the Arab-majority areas of the region of the Northwest Semitic languages, like Hatra, where names like "Refuge of Ashar", "Servant of Ashar" and "Ashar has given" are recorded on an inscription.

In Edessa, the solar deity was the primary god around the time of the Roman Emperor Julian and this worship was presumably brought in by migrants from Arabia. Julian's oration delivered to the denizens of the city mentioned that they worshipped the Sun surrounded by Azizos and Monimos whom Iamblichus identified with Ares and Hermes respectively. Monimos derived from "Mu'nim" or "the favourable one", and was another name of Ruda or Ruldaiu as apparent from spellings of his name in Sennacherib's Annals.

The idol of the god al-Uqaysir was, according to the "Book of Idols", located in Syria, and was worshipped by the tribes of Quda'a, Lakhm, Judham, Amela, and Ghatafan. Adherents would go on a pilgrimage to the idol and shave their heads, then mix their hair with wheat, "for every single hair a handful of wheat."

A shrine to Dushara has been discovered in the harbour of ancient Puteoli in Italy. The city was an important nexus for trade to the Near East, and it is known to have had a Nabataean presence during the mid 1st century BCE. A Minaean altar dedicated to Wadd evidently existed in Delos, containing two inscriptions in Minaean and Greek respectively.

The Bedouin were introduced to Meccan ritualistic practices as they frequented settled towns of the Hejaz during the four months of the "holy truce", the first three of which were devoted to religious observance, while the fourth was set aside for trade. Alan Jones infers from Bedouin poetry that the gods, even Allah, were less important to the Bedouins than Fate. They seem to have had little trust in rituals and pilgrimages as means of propitiating Fate, but had recourse to divination and soothsayers ("kahins"). The Bedouins regarded some trees, wells, caves and stones as sacred objects, either as fetishes or as means of reaching a deity. They created sanctuaries where people could worship fetishes.

The Bedouins had a code of honor which Fazlur Rahman Malik states may be regarded as their religious ethics. This code encompassed women, bravery, hospitality, honouring one's promises and pacts, and vengeance. They believed that the ghost of a slain person would cry out from the grave until their thirst for blood was quenched. Practices such as killing of infant girls were often regarded as having religious sanction. Numerous mentions of jinn in the Quran and testimony of both pre-Islamic and Islamic literature indicate that the belief in spirits was prominent in pre-Islamic Bedouin religion. However, there is evidence that the word jinn is derived from Aramaic, "ginnaye", which was widely attested in Palmyrene inscriptions. The Aramaic word was used by Christians to designate pagan gods reduced to the status of demons, and was introduced into Arabic folklore only late in the pre-Islamic era. Julius Wellhausen has observed that such spirits were thought to inhabit desolate, dingy and dark places and that they were feared. One had to protect oneself from them, but they were not the objects of a true cult.

Bedouin religious experience also included an apparently indigenous cult of ancestors. The dead were not regarded as powerful, but rather as deprived of protection and needing charity of the living as a continuation of social obligations beyond the grave. Only certain ancestors, especially heroes from which the tribe was said to derive its name, seem to have been objects of real veneration.

Iranian religions existed in pre-Islamic Arabia on account of Sasanian military presence along the Persian Gulf and South Arabia and on account of trade routes between the Hejaz and Iraq. Some Arabs in northeast of the peninsula converted to Zoroastrianism and several Zoroastrian temples were constructed in Najd. Some of the members from the tribe of Banu Tamim had converted to the religion. There is also evidence of existence of Manichaeism in Arabia as several early sources indicate a presence of "zandaqas" in Mecca, although the term could also be interpreted as referring to Mazdakism. However according to the most recent research by Tardieu, the prevalence of Manichaeism in Mecca during 6th & 7th-century, when Islam emerged, can not be proven. Similar reservations regarding the appearance of Manichaeism & Mazdakism in pre-islamic Mecca are offered by Trompf & Mikkelsen et al in their latest work (2018). There is evidence for the circulation of Iranian religious ideas in the form of Persian loan words in Quran such as "firdaws" (paradise).

Zoroastrianism was also present in Eastern Arabia and Persian-speaking Zoroastrians lived in the region. The religion was introduced in the region including modern-day Bahrain during the rule of Persian empires in the region starting from 250 B.C. It was mainly practiced in Bahrain by Persian settlers. Zoroastrianism was also practiced in the Persian-ruled area of modern-day Oman. The religion also existed in Persian-ruled area of modern Yemen. The descendants of Abna, the Persian conquerors of Yemen, were followers of Zoroastrianism. Yemen's Zoroastrians who had the jizya imposed on them after being conquered by Muhammad are mentioned by the Islamic historian al-Baladhuri. According to Serjeant, the Baharna people may be the Arabized descendants of converts from the original population of ancient Persians (majus) as well as other religions.

A thriving community of Jewish tribes existed in pre-Islamic Arabia and included both sedentary and nomadic communities. Jews had migrated into Arabia from Roman times onwards. Arabian Jews spoke Arabic as well as Hebrew and Aramaic and had contact with Jewish religious centers in Babylonia and Palestine. The Yemeni Himyarites converted to Judaism in the 4th century, and some of the Kindah, a tribe in central Arabia who were their vassals, were also converted in the 4th/5th century. Jewish tribes existed in all major Arabian towns during Muhammad's time including in Tayma and Khaybar as well as Medina with twenty tribes living in the peninsula. From tomb inscriptions, it is visible that Jews also lived in Mada'in Saleh and Al-`Ula.

There is evidence that Jewish converts in the Hejaz were regarded as Jews by other Jews, as well as by non-Jews, and have sought advice from Babylonian rabbis on matters of attire and kosher food. In at least one case, it is known that an Arab tribe agreed to adopting Judaism as a condition for settling in a town dominated by Jewish inhabitants. Some Arab women in Yathrib/Medina are said to have vowed to make their child a Jew if the child survived, since they considered the Jews to be people "of knowledge and the book" ("`ilmin wa-kitābin"). Philip Hitti infers from proper names and agricultural vocabulary that the Jewish tribes of Yathrib consisted mostly of Judaized clans of Arabian and Aramaean origin.

The key role played by Jews in the trade and markets of the Hejaz meant that market day for the week was the day preceding the Jewish Sabbath. This day, which was called "aruba" in Arabic, also provided occasion for legal proceedings and entertainment, which in turn may have influenced the choice of Friday as the day of Muslim congregational prayer. Toward the end of the sixth century, the Jewish communities in the Hejaz were in a state of economic and political decline, but they continued to flourish culturally in and beyond the region. They had developed their distinctive beliefs and practices, with a pronounced mystical and eschatological dimension. In the Islamic tradition, based on a phrase in the Quran, Arab Jews are said to have referred to Uzair as the son of Allah, although the historical accuracy of this assertion has been disputed.

Jewish agriculturalists lived in the region of Eastern Arabia. According to Robert Bertram Serjeant, the Baharna may be the Arabized "descendants of converts from Christians (Arameans), Jews and ancient Persians (Majus) inhabiting the island and cultivated coastal provinces of Eastern Arabia at the time of the Arab conquest". From the Islamic sources, it seems that Judaism was the religion most followed in Yemen. Ya'qubi claimed all Yemenites to be Jews, Ibn Hazm however states only Himyarites and some Kindites were Jews.

The main areas of Christian influence in Arabia were on the north eastern and north western borders and in what was to become Yemen in the south. The north west was under the influence of Christian missionary activity from the Roman Empire where the Ghassanids, a client kingdom of the Romans, were converted to Christianity. In the south, particularly at Najran, a centre of Christianity developed as a result of the influence of the Christian Kingdom of Axum based on the other side of the Red Sea in Ethiopia. Some of the Banu Harith had converted to Christianity. One family of the tribe built a large church at Najran called "Deir Najran", also known as the "Ka'ba of Najran". Both the Ghassanids and the Christians in the south adopted Monophysitism.

The third area of Christian influence was on the north eastern borders where the Lakhmids, a client tribe of the Sassanians, adopted Nestorianism, being the form of Christianity having the most influence in the Sassanian Empire. As the Persian Gulf region of Arabia increasingly fell under the influence of the Sassanians from the early third century, many of the inhabitants were exposed to Christianity following the eastward dispersal of the religion by Mesopotamian Christians. However, it was not until the fourth century that Christianity gained popularity in the region with the establishment of monasteries and a diocesan structure. 

In pre-Islamic times, the population of Eastern Arabia consisted of Christianized Arabs (including Abd al-Qays) and Aramean Christians among other religions. Syriac functioned as a liturgical language. Serjeant states that the Baharna may be the Arabized descendants of converts from the original population of Christians (Aramaeans), among other religions at the time of Arab conquests. Beth Qatraye which translates "region of the Qataris" in Syriac was the Christian name used for the region encompassing north-eastern Arabia. It included Bahrain, Tarout Island, Al-Khatt, Al-Hasa, and Qatar. Oman and the United Arab Emirates comprised the diocese known as Beth Mazunaye. The name was derived from 'Mazun', the Persian name for Oman and the United Arab Emirates. Sohar was the central city of the diocese.

In Nejd, in the centre of the peninsula, there is evidence of members of two tribes, Kindah and Taghlib, converting to Christianity in the 6th century. However, in the Hejaz in the west, whilst there is evidence of the presence of Christianity, it is not thought to have been significant amongst the indigenous population of the area.

Arabicized Christian names were fairly common among pre-Islamic Arabians, which has been attributed to the influence that Syrianized Christian Arabs had on Bedouins of the peninsula for several centuries before the rise of Islam.

Neal Robinson, based on verses in the Quran, believes that some Arab Christians may have held unorthodox beliefs such as the worshipping of a divine triad of God the father, Jesus the Son and Mary the Mother. Furthermore, there is evidence that unorthodox groups such as the Collyridians, whose adherents worshiped Mary, were present in Arabia, and it has been proposed that the Quran refers to their beliefs. However, other scholars, notably Mircea Eliade, William Montgomery Watt, G.R. Hawting and Sidney H. Griffith, cast doubt on the historicity or reliability of such references in the Quran. Their views are as follows:





</doc>
<doc id="15392" url="https://en.wikipedia.org/wiki?curid=15392" title="Imperial Conference">
Imperial Conference

Imperial Conferences (Colonial Conferences before 1907) were periodic gatherings of government leaders from the self-governing colonies and dominions of the British Empire between 1887 and 1937, before the establishment of regular Meetings of Commonwealth Prime Ministers in 1944. They were held in 1887, 1894, 1897, 1902, 1907, 1911, 1921, 1923, 1926, 1930, 1932 and 1937.

All the conferences were held in London, the seat of the Empire, except for the 1894 and 1932 conferences which were held in Ottawa, the capital of the senior Dominion of the Crown. The 1907 conference changed the name of the meetings to Imperial Conferences and agreed that the meetings should henceforth be regular rather than taking place while overseas statesmen were visiting London for royal occasions (e.g. jubilees and coronations).

Originally instituted to emphasise imperial unity, as time went on, the conferences became a key forum for dominion governments to assert the desire for removing the remaining vestiges of their colonial status. The conference of 1926 agreed to the Balfour Declaration, which acknowledged that the dominions would henceforth rank as equals to the United Kingdom, as members of the 'British Commonwealth of Nations'.

The conference of 1930 decided to abolish the legislative supremacy of the British Parliament as it was expressed through the Colonial Laws Validity Act and other Imperial Acts. The statesmen recommended that a declaratory enactment of Parliament, which became the Statute of Westminster 1931, be passed with the consent of the dominions, but some dominions did not ratify the statute until some years afterwards. The 1930 conference was notable, too, for the attendance of Southern Rhodesia, despite it being a self-governing colony, not a dominion.

As World War II drew to a close, Imperial Conferences were replaced by Commonwealth Prime Ministers' Conferences, with 17 such meetings occurring from 1944 until 1969, all but one of the meetings occurred in London. The gatherings were renamed Commonwealth Heads of Government Meetings (CHOGM) in 1971 and were henceforth held every two years with hosting duties rotating around the Commonwealth.




</doc>
<doc id="15395" url="https://en.wikipedia.org/wiki?curid=15395" title="International Refugee Organization">
International Refugee Organization

The International Refugee Organization (IRO) was an intergovernmental organization founded on 20 April 1946 to deal with the massive refugee problem created by World War II. A Preparatory Commission began operations fourteen months previously. In 1948, the treaty establishing the IRO formally entered into force and the IRO became a United Nations specialized agency. The IRO assumed most of the functions of the earlier United Nations Relief and Rehabilitation Administration. In 1952, operations of the IRO ceased, and it was replaced by the Office of the United Nations High Commissioner for Refugees (UNHCR).

The Constitution of the International Refugee Organization, adopted by the United Nations General Assembly on 15 December 1946, is the founding document of the IRO. The constitution specified the organization's field of operations. Controversially, the constitution defined "persons of German ethnic origin" who had been expelled, or were to be expelled from their countries of birth into the postwar Germany, as individuals who would "not be the concern of the Organization." This excluded from its purview a group that exceeded in number all the other European displaced persons put together. Also, because of disagreements between the Western allies and the Soviet Union, the IRO only worked in areas controlled by Western armies of occupation.

Twenty-six states became members of the IRO and it formally came into existence in 1948: Argentina, Australia, Belgium, Bolivia, Brazil, Canada, Republic of China, Chile, Denmark, the Dominican Republic, France, Guatemala, Honduras, Iceland, Italy, Liberia, Luxembourg, the Netherlands, New Zealand, Norway, Panama, Peru, the Philippines, Switzerland, the United Kingdom, the United States, and Venezuela. The U.S. provided about 40% of the IRO's $155 million annual budget. The total contribution by the members for the five years of operation was around $400 million. It had rehabilitated around 10 million people during this time, out of 15 million people who were stranded in Europe. The IRO's first Director-General was William Hallam Tuck, succeeded by J. Donald Kingsley on 31 July 1949.

IRO closed its operations on 31 January 1952 and after a liquidation period, went out of existence on 30 September 1953. By that time many of its responsibilities had been assumed by other agencies. Of particular importance was the Office of the High Commissioner for Refugees, established in January 1951 as a part of the United Nations, and the Intergovernmental Committee for European Migration (originally PICMME), set up in December 1951. 




</doc>
<doc id="15396" url="https://en.wikipedia.org/wiki?curid=15396" title="IRO">
IRO

IRO (or Iro, iro) may refer to:




</doc>
<doc id="15401" url="https://en.wikipedia.org/wiki?curid=15401" title="Isabella d'Este">
Isabella d'Este

Isabella d'Este (19 May 1474 – 13 February 1539) was Marchioness of Mantua and one of the leading women of the Italian Renaissance as a major cultural and political figure. She was a patron of the arts as well as a leader of fashion, whose innovative style of dressing was copied by women throughout Italy and at the French court. The poet Ariosto labeled her as the "liberal and magnanimous Isabella", while author Matteo Bandello described her as having been "supreme among women". Diplomat Niccolò da Correggio went even further by hailing her as "The First Lady of the world".

She served as the regent of Mantua during the absence of her husband, Francesco II Gonzaga, Marquess of Mantua, and the minority of her son, Federico, Duke of Mantua. In 1500 she met King Louis XII of France in Milan on a diplomatic mission to persuade him not to send his troops against Mantua.

She was a prolific letter-writer and maintained a lifelong correspondence with her sister-in-law Elisabetta Gonzaga. Lucrezia Borgia was another sister-in-law; she later became the mistress of Isabella's husband. 
She was described as having been physically attractive, albeit slightly plump; however, she also possessed "lively eyes" and was "of lively grace".

Isabella d'Este grew up in a cultured family in the city-state of Ferrara. She received a fine classical education and as a girl met many famous humanist scholars and artists.
Due to the vast amount of extant correspondence between Isabella and her family and friends, her life is unusually well documented. She was born on Tuesday 19 May 1474 at nine o'clock in the evening in Ferrara, to Ercole I d'Este, Duke of Ferrara, and Eleanor of Naples. Eleanor was the daughter of Ferdinand I, the Aragonese King of Naples, and Isabella of Clermont.

One year later on 29 June 1475, her sister Beatrice was born, and in 1476 and 1477 two brothers, Alfonso and Ferrante, arrived. In 1479 and 1480 two more brothers were born; they were Ippolito and Sigismondo. Of all the children, Isabella was considered to have been the favourite.

In the year of Ferrante's birth, Isabella travelled to Naples with her mother. When her mother returned to Ferrara, Isabella accompanied her, while the other children stayed behind with their grandfather for eight years. It was during the journey with her mother that Isabella acquired the art of diplomacy and statecraft.

Isabella was a very well educated young woman. As a child she studied Roman history and rapidly learned to translate Greek and Latin (the former would become her favourite language). Because of her outstanding intellect, she often discussed the classics and the affairs of state with ambassadors. Moreover, she was personally acquainted with the painters, musicians, writers, and scholars who lived in and around the court. Besides her knowledge of history and languages, she could also recite Virgil and Terence by heart. Isabella was also a talented singer and musician, and was taught to play the lute by Giovanni Angelo Testagrossa. In addition to all these admirable accomplishments, she was also an innovator of new dances, having been instructed in the art by Ambrogio, a Jewish dancing master.

In 1480, at the age of six, Isabella was betrothed to Francesco, the heir to the Marquess of Mantua, for a dowry of 25,000 ducats. Although he was not handsome, Isabella admired him for his strength and bravery; she also regarded him as a gentleman. After their first few encounters she found that she enjoyed his company and she spent the next few years getting to know him and preparing herself to be the Marchioness of Mantua. During their courtship, Isabella treasured the letters, poems, and sonnets he sent her as gifts.

Ten years later on 11 February 1490, at age 15, she married Francesco by proxy, who had by then succeeded to the marquisate. Isabella became his wife and marchioness amid a spectacular outpouring of popular acclamation and a grand celebration that took place on 15 February. Besides being the Marquess, Francesco was also Captain General of the armies of the Republic of Venice. She brought as her marriage portion the sum of 3,000 ducats as well as valuable jewellery, dishes, and a silver service. Prior to the magnificent banquet which followed the wedding ceremony, Isabella rode through the main streets of Ferrara astride a horse draped in gems and gold.
As the couple had known and admired one another for many years, their mutual attraction deepened into love; marriage to Francesco allegedly caused Isabella to "bloom". At the time of her wedding, Isabella was said to have been pretty, slim, graceful and well-dressed. Her long, fine hair was dyed pale blonde, and her eyes, "brown as fir cones in autumn, scattered laughter".

Francesco, in his capacity of Captain General of the Venetian armies, was often required to go to Venice for conferences which left Isabella in Mantua on her own at "La Reggia", the ancient palace which was the family seat of the Gonzagas. She did not lack company, however, as she passed the time with her mother and with her sister, Beatrice; and upon meeting Elisabetta Gonzaga, her 18-year-old sister-in-law, the two women became close friends. They enjoyed reading books, playing cards, and travelling about the countryside together. Once they journeyed as far as Lake Garda during one of Francesco's absences, and later travelled to Venice. They maintained a steady correspondence until Elisabetta's death in 1526.

Almost four years after her marriage in December 1493, Isabella gave birth to her first child out of an eventual total of eight; it was a daughter, Eleonora, whom they called Leonora for short.

Together Isabella and Francesco had eight children:

A year after her marriage to Isabella's brother Alfonso in 1502, the notorious Lucrezia Borgia became the mistress of Francesco. Isabella had given birth to a daughter, Ippolita, at about the same time, and she continued to bear him children throughout Francesco and Lucrezia's long, passionate affair, which was more sexual than romantic. Lucrezia had previously made overtures of friendship to Isabella which the latter had coldly and disdainfully ignored. From the time Lucrezia had first arrived in Ferrara as Alfonso's intended bride, Isabella, despite having acted as hostess during the wedding festivities, had regarded Lucrezia as a rival, whom she sought to outdo at every opportunity. Francesco's affair with Lucrezia, whose beauty was renowned, caused Isabella much jealous suffering and emotional pain. Their liaison ended when he contracted syphilis as a result of encounters with prostitutes.

Isabella played an important role in Mantua during the city's troubled times. When her husband was captured in 1509 and held hostage in Venice, she took control of Mantua's military forces and held off the invaders until his release in 1512. In the same year, 1512, she was the hostess at the Congress of Mantua, which was held to settle questions concerning Florence and Milan. As a ruler, she appeared to have been much more assertive and competent than her husband. When apprised of this fact upon his return, Francesco was furious and humiliated at being upstaged by his wife's superior political ability. This caused their marriage to break down irrevocably. As a result, Isabella began to travel freely and live independently from her husband until his death on 19 March 1519.

After the death of her husband, Isabella ruled Mantua as regent for her son Federico. She began to play an increasingly important role in Italian politics, steadily advancing Mantua's position. She was instrumental in promoting Mantua to a Duchy, which was obtained by wise diplomatic use of her son's marriage contracts. She also succeeded in obtaining a cardinalate for her son Ercole. She further displayed shrewd political acumen in her negotiations with Cesare Borgia, who had dispossessed Guidobaldo da Montefeltro, duke of Urbino, the husband of her sister-in-law and good friend Elisabetta Gonzaga in 1502.

Isabella d'Este is famous as the most important art patron of the Renaissance; her life is documented by her correspondence, which is still archived in Mantua (c. 28,000 letters received and copies of c. 12,000 letters written).

In painting she had numerous famous artists of the time work for her, including Giovanni Bellini, Giorgione, Leonardo da Vinci, Andrea Mantegna (court painter until 1506), Perugino, Raphael, Titian, Antonio da Correggio, Lorenzo Costa (court painter from 1509), Dosso Dossi, Francesco Francia, Giulio Romano and many others. For instance her 'Studiolo' in the Ducal Palace, Mantua, was decorated with allegories by Mantegna, Perugino, Costa and Correggio.

In parallel she contracted the most important sculptors and medallists of her time, i.e. Michelangelo, Pier Jacopo Alari Bonacolsi (L'Antico), Gian Cristoforo Romano and Tullio Lombardo, and collected ancient Roman art.

For what concerns writers, she was in contact with Pietro Aretino, Ludovico Ariosto, Pietro Bembo, Baldassare Castiglione, Mario Equicola, Gian Giorgio Trissino and others.

In music Isabella sponsored the composers Bartolomeo Tromboncino and Marco Cara and played the lute herself. Unusually, she employed women as professional singers at her court, including Giovanna Moreschi, the wife of Marchetto Cara.

In the architecture field, she could not afford new palaces, however she commissioned architects like Biagio Rossetti and Battista Covo.

Being a leader of fashion, she ordered the finest clothing, including furs as well as the newest distillations of perfume, which she concocted herself and sent as presents. Her style of dressing in caps ('capigliari') and plunging décolletage was imitated throughout Italy and at the French court.

Isabella d'Este has been proposed as a plausible candidate for Leonardo's "Mona Lisa" of c. 1503–1506, usually considered a portrait of Lisa del Giocondo. (Lisa was the wife of a merchant in Florence and Giorgio Vasari wrote of her portrait by Leonardo – it remains open whether this is the portrait now known as the 'Mona Lisa'.) Evidence in favor of Isabella as the subject of the famous work includes Leonardo's drawing 'Isabella d'Este' from 1499 and her letters of 1501–1506 requesting the promised painted portrait; further arguments are the mountains in the background and the armrest as a Renaissance symbol for a portrait of a sovereign.

Despite her significant art patronage, which included a number of portraits – no other person of her time was so often portrayed – there are very few surviving identified portraits of Isabella. These few identifications are known as inhomogeneous (i.e. differing eye and hair colours as well as divergent eyebrows in both Titian portraits) and there are no images of her between the ages of 26 and 54 (see picture). It is known that the elderly Isabella preferred idealized paintings and even waived sitting as a model. However, it could be assumed that she still insisted on seeing her personal characteristics in the outcome. Isabelle carefully managed her image. A portrait of her by Andrea Mantegna was rejected because it 'looked nothing at all like us'. The portrait most probably looked too much like Isabella, who was prone to corpulence.

In recent years several museums have withdrawn their few identifications of portraits as Isabella because of the risk of misidentification. The remaining three colourful portraits are still inhomogeneous (Kunsthistorisches Museum/KHM, Vienna):

"La Bella" (now in Palazzo Pitti, Florence) has been discussed as an alternative to Titian's 1536 portrait in Vienna, because the commission from the 60-year-old patron was for a rejuvenated portrait; if La Bella were Isabella, eye colour, hair colour, eyebrows and general appearance would homogenize in all known portraits, allowing potential links toward further identifications.

At present the 1495 medal by Gian Cristoforo Romano (several extant copies) is the only reliable identification because of the inscription created during Isabella's lifetime.

Isabella had met the French king in Milan in 1500 on a successful diplomatic mission which she had undertaken to protect Mantua from French invasion. Louis had been impressed by her alluring personality and keen intelligence. It was while she was being entertained by Louis, whose troops occupied Milan, that she offered asylum to Milanese refugees including Cecilia Gallerani, the refined mistress of her sister Beatrice's husband, Ludovico Sforza, Duke of Milan, who had been forced to leave his duchy in the wake of French occupation. Isabella presented Cecilia to King Louis, describing her as a "lady of rare gifts and charm".

Isabella was also an extreme example of the Renaissance European tendency to treat black African slaves in her household as exotic accessories. Isabella's fascination with black child servants is extensively documented. On 1 May 1491 Isabella asked Giorgio Brognolo, her agent in Venice, to procure a young black girl ('una moreta') between the ages of one-and-a-half and four, and twice in early June reminded him of the request, emphasizing that the girl should be 'as black as possible'. Isabella's household and financial records reflect she already had a significantly older black girl in her service when she inquired after a younger black child. Records also reflect that she obtained a little black girl from a Venetian orphanage, opened negotiations with a Venetian patrician household for the sale of a little black boy and purchased an enslaved little black girl from her sister. The commission for the purchase of a little girl "as black as possible" could be construed as a wish for maximum exoticism.

As a widow, Isabella at the age of 45 became a "devoted head of state". Her position as a Marquise required her serious attention, therefore she was required to study the problems faced by a ruler of a city-state. To improve the well-being of her subjects she studied architecture, agriculture, and industry, and followed the principles that Niccolò Machiavelli had set forth for rulers in his book "The Prince". In return, the people of Mantua respected and loved her.
Isabella left Mantua for Rome in 1527. She was present during the catastrophic Sack of Rome, when she converted her house into an asylum for about 2000 people fleeing the Imperial soldiers. Isabella's house was one of the very few which was not attacked, due to the fact that her son was a member of the invading army. When she left, she managed to acquire safe passage for all the refugees who had sought refuge in her home.

After Rome became stabilized following the sacking, she left the city and returned to Mantua. She made it a centre of culture, started a school for girls, and turned her ducal apartments into a museum containing the finest art treasures. This was not enough to satisfy Isabella, already in her mid-60s, so she returned to political life and ruled Solarolo, in Romagna until her death on 13 February 1539. She was buried beside her husband in the Church of San Francesco in Mantua.

During her lifetime and after her death, poets, popes, and statesmen paid tribute to Isabella. Pope Leo X invited her to treat him with "as much friendliness as you would your brother". The latter's secretary Pietro Bembo described her as "one of the wisest and most fortunate of women"; while the poet Ariosto deemed her the "liberal and magnanimous Isabella". Author Matteo Bandello wrote that she was "supreme among women", and the diplomat Niccolò da Correggio entitled her "The First Lady of the world".
The artwork "The Dinner Party" features a place setting for Isabella d'Este.

Isabella d'Este was portrayed by Belgian actress Alexandra Oppo in the TV show Borgia (2011–2014).



</doc>
<doc id="15402" url="https://en.wikipedia.org/wiki?curid=15402" title="International standard">
International standard

International standards may be used either by direct application or by a process of modifying an international standard to suit local conditions. The adoption of international standards results in the creation of equivalent, national standards that are substantially the same as international standards in technical content, but may have (i) editorial differences as to appearance, use of symbols and measurement units, substitution of a point for a comma as the decimal marker, and (ii) differences resulting from conflicts in governmental regulations or industry-specific requirements caused by fundamental climatic, geographical, technological, or infrastructural factors, or the stringency of safety requirements that a given standard authority considers appropriate.

International standards are one way of overcoming technical barriers in international commerce caused by differences among technical regulations and standards developed independently and separately by each nation, national standards organization, or company. Technical barriers arise when different groups come together, each with a large user base, doing some well established thing that between them is mutually incompatible. Establishing international standards is one way of preventing or overcoming this problem.

The implementation of standards in industry and commerce became highly important with the onset of the Industrial Revolution and the need for high-precision machine tools and interchangeable parts. Henry Maudslay developed the first industrially practical screw-cutting lathe in 1800, which allowed for the standardisation of screw thread sizes for the first time.

Maudslay's work, as well as the contributions of other engineers, accomplished a modest amount of industry standardization; some companies' in-house standards spread a bit within their industries. Joseph Whitworth's screw thread measurements were adopted as the first (unofficial) national standard by companies around the country in <time>1841</time>. It came to be known as the British Standard Whitworth, and was widely adopted in other countries.

By <time>the end of the 19th century</time> differences in standards between companies were making trade increasingly difficult and strained. The Engineering Standards Committee was established in London in <time>1901</time> as the world's first national standards body. After the First World War, similar national bodies were established in other countries. The Deutsches Institut für Normung was set up in Germany in <time>1917</time>, followed by its counterparts, the American National Standard Institute and the French Commission Permanente de Standardisation, both in <time>1918</time>.

By <time>the mid to late 19th century</time>, efforts were being made to standardize electrical measurement. An important figure was R. E. B. Crompton, who became concerned by the large range of different standards and systems used by electrical engineering companies and scientists in <time>the early 20th century</time>. Many companies had entered the market in <time>the 1890s</time> and all chose their own settings for voltage, frequency, current and even the symbols used on circuit diagrams. Adjacent buildings would have totally incompatible electrical systems simply because they had been fitted out by different companies. Crompton could see the lack of efficiency in this system and began to consider proposals for an international standard for electric engineering.

In <time>1904</time>, Crompton represented Britain at the Louisiana Purchase Exposition in St. Louis as part of a delegation by the Institute of Electrical Engineers. He presented a paper on standardisation, which was so well received that he was asked to look into the formation of a commission to oversee the process. By <time>1906</time> his work was complete and he drew up a permanent constitution for the first international standards organization, the International Electrotechnical Commission. The body held its first meeting that year in London, with representatives from 14 countries. In honour of his contribution to electrical standardisation, Lord Kelvin was elected as the body's first President.
The International Federation of the National Standardizing Associations (ISA) was founded in <time>1926</time> with a broader remit to enhance international cooperation for all technical standards and specifications. The body was suspended in <time>1942</time> during World.

After the war, ISA was approached by the recently formed United Nations Standards Coordinating Committee (UNSCC) with a proposal to form a new global standards body. In <time>October 1946</time>, ISA and UNSCC delegates from 25 countries met in London and agreed to join forces to create the new International Organization for Standardization (ISO); the new organization officially began operations in <time style="white-space:nowrap">February 1947</time>.



</doc>
<doc id="15403" url="https://en.wikipedia.org/wiki?curid=15403" title="ISO 4217">
ISO 4217

ISO 4217 is a standard first published by International Organization for Standardization in 1978, which delineates currency designators, country codes (alpha and numeric), and references to minor units in three tables:

The tables, history and ongoing discussion are maintained by SIX Interbank Clearing on behalf of ISO and the Swiss Association for Standardization.

The ISO 4217 code list is used in banking and business globally. In many countries the ISO codes for the more common currencies are so well known publicly that exchange rates published in newspapers or posted in banks use only these to delineate the currencies, instead of translated currency names or ambiguous currency symbols. ISO 4217 codes are used on airline tickets and international train tickets to remove any ambiguity about the price.

The first two letters of the code are the two letters of the ISO 3166-1 alpha-2 country codes (which are also used as the basis for national top-level domains on the Internet) and the third is usually the initial of the currency itself. So Japan's currency code is JPY—JP for Japan and Y for yen. This eliminates the problem caused by the names "dollar, franc, peso" and "pound" being used in dozens of countries, each having significantly differing values.

In some cases, the third letter of the code is not the initial letter of the currency name. There are two possible reasons for this to happen:

In addition to codes for most active national currencies ISO 4217 provides codes for "supranational" currencies, procedural purposes, and several things which are "similar to" currencies:

The use of an initial letter "X" for these purposes is facilitated by the ISO 3166 rule that no official country code beginning with X will ever be assigned. 

The inclusion of EU (denoting the European Union) in the ISO 3166-1 reserved codes list, allows the euro to be coded as EUR rather than assigned a code beginning with X, even though it is a supranational currency.

The ISO 4217 standard includes a crude mechanism for expressing the relationship between a major currency unit and its corresponding minor currency unit. This mechanism is called the currency "exponent" and assumes a base of 10. For example, USD (the United States dollar) is equal to 100 of its minor currency unit the "cent". So the USD has exponent 2 (10 to the power 2 is 100, which is the number of cents in a dollar). The code JPY (Japanese yen) is given the exponent 0, because its minor unit, the sen, although nominally valued at 1/100 of a yen, is of such negligible value that it is no longer used. Usually, as with the USD, the minor currency unit has a value that is 1/100 of the major unit, but in some cases (including most varieties of the dinar) 1/1000 is used, and sometimes ratios apply which are not integer powers of 10. Mauritania does not use a decimal division of units, setting 1 ouguiya (UM) equal to 5 khoums, and Madagascar has 1 ariary = 5 iraimbilanja. Some currencies do not have any minor currency unit at all and these are given an exponent of 0, as with currencies whose minor units are unused due to negligible value.

There is also a three-digit code number assigned to each currency, in the same manner as there is also a three-digit code number assigned to each country as part of ISO 3166. This numeric code is usually the same as the ISO 3166-1 numeric code. For example, USD (United States dollar) has code 840 which is also the numeric code for the US (United States).

The ISO standard does not regulate either the spacing, prefixing or suffixing in usage of currency codes. According however to the European Union's Publication Office, in English, Irish, Latvian and Maltese texts, the ISO 4217 code is to be followed by a hard space and the amount:

In Bulgarian, Croatian, Czech, Danish, Dutch, Estonian, Finnish, French, German, Greek, Hungarian, Italian, Lithuanian, Polish, Portuguese, Romanian, Slovak, Slovene, Spanish and Swedish the order is reversed; the amount is followed by a hard space and the ISO 4217 code:

Note that, as illustrated, the order is determined not by the currency, but by the native language of the document context.

In 1973, the ISO Technical Committee 68 decided to develop codes for the representation of currencies and funds for use in any application of trade, commerce or banking. At the 17th session (February 1978), the related UN/ECE Group of Experts agreed that the three-letter alphabetic codes for International Standard ISO 4217, "Codes for the representation of currencies and funds", would be suitable for use in international trade.

Over time, new currencies are created and old currencies are discontinued. Such changes usually originate from the formation of new countries, treaties between countries on shared currencies or monetary unions, or redenomination from an existing currency due to excessive inflation. As a result, the list of codes must be updated from time to time. The ISO 4217 maintenance agency (MA), SIX Interbank Clearing, is responsible for maintaining the list of codes.

The following is a list of active codes of official ISO 4217 currency names .
In the standard the values are called "alphabetic code", "numeric code", "minor unit", and "entity".
The US dollar has two codes assigned: USD and USN (next day). The USS (same day) code is not in use any longer, and was removed from the list of active ISO 4217 codes in March 2014.

According to UN/CEFACT recommendation 9, paragraphs 8–9 ECE/TRADE/203, 1996, available online:

As of August 2018, there are no new codes planned to be added to the standard.

A number of active currencies do not have an ISO 4217 code, because they may be: (1) a minor currency pegged at par (1:1) to a larger currency, even if independently regulated, (2) a currency only used for commemorative banknotes or coins, or (3) a currency of an unrecognized or partially recognized state. These currencies include:

See for a list of all currently pegged currencies.

Despite having no official recognition in ISO 4217, the following non-ISO codes are sometimes used locally or commercially.

The following non-ISO codes were used in the past.

Currency subdivisions (also known as currency subunits or minor currency units) are often used for pricing and trading stocks and other assets, such as energy, but are not assigned codes by ISO 4217. Two conventions for representing currency subdivisions are in widespread use:


A third convention is similar to the second one but uses an upper-case letter, eg ZAC for the South African Cent. This convention is not in widespread use as it would result in clashes, eg between GBP for Pound Sterling and GBP for Penny Sterling.

Recently, cryptocurrencies have unofficially used ISO-like codes on various cryptocurrency exchanges, for instance LTC for Litecoin, NMC for Namecoin and XRP for the XRP Ledger. SIX Interbank Clearing (a Maintenance Agency of ISO) is currently studying the impact and role of cryptocurrencies and other independent currencies on ISO 4217.

A number of currencies had official ISO 4217 currency codes and currency names until their replacement by another currency. The table below shows the ISO currency codes of former currencies and their common names (which do not always match the ISO 4217 names).





</doc>
<doc id="15406" url="https://en.wikipedia.org/wiki?curid=15406" title="Irgun">
Irgun

The Irgun (; full title: ', lit. "The National Military Organization in the Land of Israel") was a Zionist paramilitary organization that operated in Mandate Palestine between 1931 and 1948. The organization is also referred to as Etzel"' (), an acronym of the Hebrew initials, or by the abbreviation IZL. It was an offshoot of the older and larger Jewish paramilitary organization Haganah (Hebrew: , Defence). When the group broke from the Haganah it became known as the "Haganah Bet" (Hebrew: literally "Defense 'B' " or "Second Defense", ), or alternatively as haHaganah haLeumit () or Hama'amad (). Irgun members were absorbed into the Israel Defense Forces at the start of the 1948 Arab–Israeli war.

The Irgun policy was based on what was then called Revisionist Zionism founded by Ze'ev Jabotinsky. According to Howard Sachar, "The policy of the new organization was based squarely on Jabotinsky's teachings: every Jew had the right to enter Palestine; only active retaliation would deter the Arabs; only Jewish armed force would ensure the Jewish state".

Two of the operations for which the Irgun is best known are the bombing of the King David Hotel in Jerusalem on 22 July 1946 and the Deir Yassin massacre, carried out together with Lehi on 9 April 1948.

The Irgun has been viewed as a terrorist organization or organization which carried out terrorist acts. Specifically the organization "committed acts of terrorism and assassination against the British, whom it regarded as illegal occupiers, and it was also violently anti-Arab" according to the Encyclopædia Britannica. In particular the Irgun was described as a terrorist organization by the United Nations, British, and United States governments; in media such as "The New York Times" newspaper; as well as by the Anglo-American Committee of Inquiry, the 1946 Zionist Congress and the Jewish Agency. However, academics such as Bruce Hoffman and Max Abrahms have written that the Irgun went to considerable lengths to avoid harming civilians, such as issuing pre-attack warnings; according to Hoffman, Irgun leadership urged "targeting the physical manifestations of British rule while avoiding the deliberate infliction of bloodshed." Irgun's tactics appealed to many Jews who believed that any action taken in the cause of the creation of a Jewish state was justified, including terrorism.

The Irgun was a political predecessor to Israel's right-wing "Herut" (or "Freedom") party, which led to today's Likud party. Likud has led or been part of most Israeli governments since 1977.

Members of the Irgun came mostly from Betar and from the Revisionist Party both in Palestine and abroad. The Revisionist Movement made up a popular backing for the underground organization. Ze'ev Jabotinsky, founder of Revisionist Zionism, commanded the organization until he died in 1940. He formulated the general realm of operation, regarding "Restraint" and the end thereof, and was the inspiration for the organization overall. An additional major source of ideological inspiration was the poetry of Uri Zvi Greenberg. The symbol of the organization, with the motto רק כך (only thus), underneath a hand holding a rifle in the foreground of a map showing both Mandatory Palestine and the Emirate of Transjordan (at the time, both were administered under the terms of the British Mandate for Palestine), implied that force was the only way to "liberate the homeland."

The number of members of the Irgun varied from a few hundred to a few thousand. Most of its members were people who joined the organization's command, under which they carried out various operations and filled positions, largely in opposition to British law. Most of them were "ordinary" people, who held regular jobs, and only a few dozen worked full-time in the Irgun.

The Irgun disagreed with the policy of the Yishuv and with the World Zionist Organization, both with regard to strategy and basic ideology and with regard to PR and military tactics, such as use of armed force to accomplish the Zionist ends, operations against the Arabs during the riots, and relations with the British mandatory government. Therefore, the Irgun tended to ignore the decisions made by the Zionist leadership and the Yishuv's institutions. This fact caused the elected bodies not to recognize the independent organization, and during most of the time of its existence the organization was seen as irresponsible, and its actions thus worthy of thwarting. Accordingly, the Irgun accompanied its armed operations with public-relations campaigns aiming to convince the public of the Irgun's way and the problems with the official political leadership of the Yishuv. The Irgun put out numerous advertisements, an underground newspaper and even ran the first independent Hebrew radio station – Kol Zion HaLochemet.

As members of an underground armed organization, Irgun personnel did not normally call Irgun by its name, but rather used other names. In the first years of its existence it was known primarily as "Ha-Haganah Leumit"' (The National Defense), and also by names such as "Haganah Bet" ("Second Defense"), "Irgun Bet" ("Second Irgun"), the "Parallel Organization" and the "Rightwing Organization". Later on it became most widely known as המעמד (the Stand). The anthem adopted by the Irgun was "Anonymous Soldiers", written by Avraham (Yair) Stern who was at the time a commander in the Irgun. Later on Stern defected from the Irgun and founded Lehi, and the song became the anthem of the Lehi. The Irgun's new anthem then became the third verse of the "Betar Song", by Ze'ev Jabotinsky.

The Irgun gradually evolved from its humble origins into a serious and well-organized paramilitary organization. The movement developed a hierarchy of ranks and a sophisticated command-structure, and came to demand serious military training and strict discipline from its members. It developed clandestine networks of hidden arms-caches and weapons-production workshops, safe-houses, and training camps, along with a secret printing facility for propaganda posters. 

The ranks of the Irgun were (in ascending order):


The Irgun was led by a High Command, which set policy and gave orders. Directly underneath it was a General Staff, which oversaw the activities of the Irgun. The General Staff was divided into a military and a support staff. The military staff was divided into operational units that oversaw operations and support units in charge of planning, instruction, weapons caches and manufacture, and first aid. The military and support staff never met jointly; they communicated through the High Command. Beneath the General Staff were six district commands: Jerusalem, Tel Aviv, Haifa-Galilee, Southern, Sharon, and Shomron, each led by a district commander. A local Irgun district unit was called a "Branch". A "brigade" in the Irgun was made up of three sections. A section was made up of two groups, at the head of each was a "Group Head", and a deputy. Eventually, various units were established, which answered to a "Center" or "Staff".

The head of the Irgun High Command was the overall commander of the organization, but the designation of his rank varied. During the revolt against the British, Irgun commander Menachem Begin and the entire High Command held the rank of "Gundar Rishon". His predecessors, however, had held their own ranks. A rank of Military Commander (Seren) was awarded to the Irgun commander Yaakov Meridor and a rank of High Commander (Aluf) to David Raziel. Until his death in 1940, Jabotinsky was known as the "Military Commander of the Etzel" or the "Ha-Matzbi Ha-Elyon" ("Supreme Commander").

Under the command of Menachem Begin, the Irgun was divided into different corps:


The Irgun's commanders planned for it to have a regular combat force, a reserve, and shock units, but in practice there were not enough personnel for a reserve or for a shock force.

The Irgun emphasized that its fighters be highly disciplined. Strict drill exercises were carried out at ceremonies at different times, and strict attention was given to discipline, formal ceremonies and military relationships between the various ranks. The Irgun put out professional publications on combat doctrine, weaponry, leadership, drill exercises, etc. Among these publications were three books written by David Raziel, who had studied military history, techniques, and strategy:


A British analysis noted that the Irgun's discipline was "as strict as any army in the world."

The Irgun operated a sophisticated recruitment and military-training regime. Those wishing to join had to find and make contact with a member, meaning only those who personally knew a member or were persistent could find their way in. Once contact had been established, a meeting was set up with the three-member selection committee at a safe-house, where the recruit was interviewed in a darkened room, with the committee either positioned behind a screen, or with a flashlight shone into the recruit's eyes. The interviewers asked basic biographical questions, and then asked a series of questions designed to weed out romantics and adventurers and those who had not seriously contemplated the potential sacrifices. Those selected attended a four-month series of indoctrination seminars in groups of five to ten, where they were taught the Irgun's ideology and the code of conduct it expected of its members. These seminars also had another purpose - to weed out the impatient and those of flawed purpose who had gotten past the selection interview. Then, members were introduced to other members, were taught the locations of safe-houses, and given military training. Irgun recruits trained with firearms, hand grenades, and were taught how to conduct combined attacks on targets. Arms handling and tactics courses were given in clandestine training camps, while practice shooting took place in the desert or by the sea. Eventually, separate training camps were established for heavy-weapons training. The most rigorous course was the explosives course for bomb-makers, which lasted a year. The British authorities believed that some Irgun members enlisted in the Jewish section of the Palestine Police Force for a year as part of their training, during which they also passed intelligence. In addition to the Irgun's sophisticated training program, many Irgun members were veterans of the Haganah (including the Palmach), the British Armed Forces, and Jewish partisan groups that had waged guerrilla warfare in Nazi-occupied Europe, thus bringing significant military training and combat experience into the organization. The Irgun also operated a course for its intelligence operatives, in which recruits were taught espionage, cryptography, and analysis techniques.

Of the Irgun's members, almost all were part-time members. They were expected to maintain their civilian lives and jobs, dividing their time between their civilian lives and underground activities. There were never more than 40 full-time members, who were given a small expense stipend on which to live on. Upon joining, every member received an underground name. The Irgun's members were divided into cells, and worked with the members of their own cells. The identities of Irgun members in other cells were withheld. This ensured that an Irgun member taken prisoner could betray no more than a few comrades.

In addition to the Irgun's members in Palestine, underground Irgun cells composed of local Jews were established in Europe following World War II. An Irgun cell was also established in Shanghai, home to many European-Jewish refugees. The Irgun also set up a Swiss bank account. Eli Tavin, the former head of Irgun intelligence, was appointed commander of the Irgun abroad.

In November 1947, the Jewish insurgency came to an end as the UN approved of the partition of Palestine, and the British had announced their intention to withdraw the previous month. As the British left and the 1947-48 Civil War in Mandatory Palestine got underway, the Irgun came out of the underground and began to function more as a standing army rather an underground organization. It began openly recruiting, training, and raising funds, and established bases, including training facilities. It also introduced field communications and created a medical unit and supply service.

Until World War II the group armed itself with weapons purchased in Europe, primarily Italy and Poland, and smuggled to Palestine. The Irgun also established workshops that manufactured spare parts and attachments for the weapons. Also manufactured were land mines and simple hand grenades. Another way in which the Irgun armed itself was theft of weapons from the British Police and military.

The Irgun's first steps were in the aftermath of the Riots of 1929. In the Jerusalem branch of the Haganah there were feelings of disappointment and internal unrest towards the leadership of the movements and the Histadrut (at that time the organization running the Haganah). These feelings were a result of the view that the Haganah was not adequately defending Jewish interests in the region. Likewise, critics of the leadership spoke out against alleged failures in the number of weapons, readiness of the movement and its policy of restraint and not fighting back. On April 10, 1931, commanders and equipment managers announced that they refused to return weapons to the Haganah that had been issued to them earlier, prior to the Nebi Musa holiday. These weapons were later returned by the commander of the Jerusalem branch, Avraham Tehomi, a.k.a. "Gideon". However, the commanders who decided to rebel against the leadership of the Haganah relayed a message regarding their resignations to the Vaad Leumi, and thus this schism created a new independent movement.

The leader of the new underground movement was Avraham Tehomi, alongside other founding members who were all senior commanders in the Haganah, members of Hapoel Hatzair and of the Histadrut. Also among them was Eliyahu Ben Horin, an activist in the Revisionist Party. This group was known as the "Odessan Gang", because they previously had been members of the "Haganah Ha'Atzmit" of Jewish Odessa. The new movement was named "Irgun Tsvai Leumi", ("National Military Organization") in order to emphasize its active nature in contrast to the Haganah. Moreover, the organization was founded with the desire to become a true military organization and not just a militia as the Haganah was at the time.

In the autumn of that year the Jerusalem group merged with other armed groups affiliated with Betar. The Betar groups' center of activity was in Tel Aviv, and they began their activity in 1928 with the establishment of "Officers and Instructors School of Betar". Students at this institution had broken away from the Haganah earlier, for political reasons, and the new group called itself the "National Defense", הגנה הלאומית. During the riots of 1929 Betar youth participated in the defense of Tel Aviv neighborhoods under the command of Yermiyahu Halperin, at the behest of the Tel Aviv city hall. After the riots the Tel Avivian group expanded, and was known as "The Right Wing Organization".

After the Tel Aviv expansion another branch was established in Haifa. Towards the end of 1932 the Haganah branch of Safed also defected and joined the Irgun, as well as many members of the Maccabi sports association. At that time the movement's underground newsletter, "Ha'Metsudah" (the Fortress) also began publication, expressing the active trend of the movement. The Irgun also increased its numbers by expanding draft regiments of Betar – groups of volunteers, committed to two years of security and pioneer activities. These regiments were based in places that from which stemmed new Irgun strongholds in the many places, including the settlements of Yesod HaMa'ala, Mishmar HaYarden, Rosh Pina, Metula and Nahariya in the north; in the center – Hadera, Binyamina, Herzliya, Netanya and Kfar Saba, and south of there – Rishon LeZion, Rehovot and Ness Ziona. Later on regiments were also active in the Old City of Jerusalem ("the Kotel Brigades") among others. Primary training centers were based in Ramat Gan, Qastina (by Kiryat Mal'akhi of today) and other places.

In 1933 there were some signs of unrest, seen by the incitement of the local Arab leadership to act against the authorities. The strong British response put down the disturbances quickly. During that time the Irgun operated in a similar manner to the Haganah and was a guarding organization. The two organizations cooperated in ways such as coordination of posts and even intelligence sharing.

Within the Irgun, Tehomi was the first to serve as "Head of the Headquarters" or "Chief Commander". Alongside Tehomi served the senior commanders, or "Headquarters" of the movement. As the organization grew, it was divided into district commands.

In August 1933 a "Supervisory Committee" for the Irgun was established, which included representatives from most of the Zionist political parties. The members of this committee were Meir Grossman (of the Hebrew State Party), Rabbi Meir Bar-Ilan (of the Mizrachi Party, either Immanuel Neumann or Yehoshua Supersky (of the General Zionists) and Ze'ev Jabotinsky or Eliyahu Ben Horin (of Hatzohar).

In protest against, and with the aim of ending Jewish immigration to Palestine, the Great Arab Revolt of 1936–1939 broke out on April 19, 1936. The riots took the form of attacks by Arab rioters ambushing main roads, bombing of roads and settlements as well as property and agriculture vandalism. In the beginning, the Irgun and the Haganah generally maintained a policy of restraint, apart from a few instances. Some expressed resentment at this policy, leading up internal unrest in the two organizations. The Irgun tended to retaliate more often, and sometimes Irgun members patrolled areas beyond their positions in order to encounter attackers ahead of time. However, there were differences of opinion regarding what to do in the Haganah, as well. Due to the joining of many Betar Youth members, Jabotinsky (founder of Betar) had a great deal of influence over Irgun policy. Nevertheless, Jabotinsky was of the opinion that for moral reasons violent retaliation was not to be undertaken.

In November 1936 the Peel Commission was sent to inquire regarding the breakout of the riots and propose a solution to end the Revolt. In early 1937 there were still some in the Yishuv who felt the commission would recommend a partition of Mandatory Palestine (the land west of the Jordan River), thus creating a Jewish state on part of the land. The Irgun leadership, as well as the "Supervisory Committee" held similar beliefs, as did some members of the Haganah and the Jewish Agency. This belief strengthened the policy of restraint and led to the position that there was no room for defense institutions in the future Jewish state. Tehomi was quoted as saying: "We stand before great events: a Jewish state and a Jewish army. There is a need for a single military force". This position intensified the differences of opinion regarding the policy of restraint, both within the Irgun and within the political camp aligned with the organization. The leadership committee of the Irgun supported a merger with the Haganah. On April 24, 1937 a referendum was held among Irgun members regarding its continued independent existence. David Raziel and Avraham (Yair) Stern came out publicly in support for the continued existence of the Irgun:

In April 1937 the Irgun split after the referendum. Approximately 1,500–2,000 people, about half of the Irgun's membership, including the senior command staff, regional committee members, along with most of the Irgun's weapons, returned to the Haganah, which at that time was under the Jewish Agency's leadership. The Supervisory Committee's control over the Irgun ended, and Jabotinsky assumed command. In their opinion, the removal of the Haganah from the Jewish Agency's leadership to the national institutions necessitated their return. Furthermore, they no longer saw significant ideological differences between the movements. Those who remained in the Irgun were primarily young activists, mostly laypeople, who sided with the independent existence of the Irgun. In fact, most of those who remained were originally Betar people. Moshe Rosenberg estimated that approximately 1,800 members remained. In theory, the Irgun remained an organization not aligned with a political party, but in reality the supervisory committee was disbanded and the Irgun's continued ideological path was outlined according to Ze'ev Jabotinsky's school of thought and his decisions, until the movement eventually became Revisionist Zionism's military arm. One of the major changes in policy by Jabotinsky was the end of the policy of restraint.

On April 27, 1937 the Irgun founded a new headquarters, staffed by Moshe Rosenberg at the head, Avraham (Yair) Stern as secretary, David Raziel as head of the Jerusalem branch, Hanoch Kalai as commander of Haifa and Aharon Haichman as commander of Tel Aviv. On 20 Tammuz, (June 29) the day of Theodor Herzl's death, a ceremony was held in honor of the reorganization of the underground movement. For security purposes this ceremony was held at a construction site in Tel Aviv.

Ze'ev Jabotinsky placed Col. Robert Bitker at the head of the Irgun. Bitker had previously served as Betar commissioner in China and had military experience. A few months later, probably due to total incompatibility with the position, Jabotinsky replaced Bitker with Moshe Rosenberg. When the Peel Commission report was published a few months later, the Revisionist camp decided not to accept the commission's recommendations. Moreover, the organizations of Betar, Hatzohar and the Irgun began to increase their efforts to bring Jews to the land of Israel, illegally. This Aliyah was known as the עליית אף על פי "Af Al Pi (Nevertheless) Aliyah". As opposed to this position, the Jewish Agency began acting on behalf of the Zionist interest on the political front, and continued the policy of restraint. From this point onwards the differences between the Haganah and the Irgun were much more obvious.

According to Jabotinsky's "Evacuation Plan", which called for millions of European Jews to be brought to Palestine at once, the Irgun helped the illegal immigration of European Jews to the land of Israel. This was named by Jabotinsky the "National Sport". The most significant part of this immigration prior to World War II was carried out by the Revisionist camp, largely because the Yishuv institutions and the Jewish Agency shied away from such actions on grounds of cost and their belief that Britain would in the future allow widespread Jewish immigration.

The Irgun joined forces with Hatzohar and Betar in September 1937, when it assisted with the landing of a convoy of 54 Betar members at Tantura Beach (near Haifa.) The Irgun was responsible for discreetly bringing the Olim, or Jewish immigrants, to the beaches, and dispersing them among the various Jewish settlements. The Irgun also began participating in the organisation of the immigration enterprise and undertook the process of accompanying the ships. This began with the ship "Draga" which arrived at the coast of British Palestine in September 1938. In August of the same year, an agreement was made between Ari Jabotinsky (the son of Ze'ev Jabotinsky), the Betar representative and Hillel Kook, the Irgun representative, to coordinate the immigration (also known as Ha'apala). This agreement was also made in the "Paris Convention" in February 1939, at which Ze'ev Jabotinsky and David Raziel were present. Afterwards, the "Aliyah Center" was founded, made up of representatives of Hatzohar, Betar, and the Irgun, thereby making the Irgun a full participant in the process.

The difficult conditions on the ships demanded a high level of discipline. The people on board the ships were often split into units, led by commanders. In addition to having a daily roll call and the distribution of food and water (usually very little of either), organized talks were held to provide information regarding the actual arrival in Palestine. One of the largest ships was the "Sakaria", with 2,300 passengers, which equalled about 0.5% of the Jewish population in Palestine. The first vessel arrived on April 13, 1937, and the last on February 13, 1940. All told, about 18,000 Jews immigrated to Palestine with the help of the Revisionist organizations and private initiatives by other Revisionists. Most were not caught by the British.

Irgun members continued to defend settlements, but at the same time began attacks on Arab villages, thus ending the policy of restraint. These attacks were intended to instill fear in the Arab side, in order to cause the Arabs to wish for peace and quiet. In March 1938, David Raziel wrote in the underground newspaper "By the Sword" a constitutive article for the Irgun overall, in which he coined the term "Active Defense":

The first attacks began around April 1936, and by the end of World War II, more than 250 Arabs had been killed. Examples include:
During 1936, Irgun members carried out approximately ten attacks.

Throughout 1937 the Irgun continued this line of operation.

A more complete list can be found here.

At that time, however, these acts were not yet a part of a formulated policy of the Irgun. Not all of the aforementioned operations received a commander's approval, and Jabotinsky was not in favor of such actions at the time. Jabotinsky still hoped to establish a Jewish force out in the open that would not have to operate underground. However, the failure, in its eyes, of the Peel Commission and the renewal of violence on the part of the Arabs caused the Irgun to rethink its official policy.

14 November 1937 was a watershed in Irgun activity. From that date, the Irgun increased its reprisals. Following an increase in the number of attacks aimed at Jews, including the killing of five kibbutz members near Kiryat Anavim (today kibbutz Ma'ale HaHamisha), the Irgun undertook a series of attacks in various places in Jerusalem, killing five Arabs. Operations were also undertaken in Haifa (shooting at the Arab-populated Wadi Nisnas neighborhood) and in Herzliya. The date is known as the day the policy of restraint (Havlagah) ended, or as Black Sunday when operations resulted in the murder of 10 Arabs. This is when the organization fully changed its policy, with the approval of Jabotinsky and Headquarters to the policy of "active defense" in respect of Irgun actions.

The British responded with the arrest of Betar and Hatzohar members as suspected members of the Irgun. Military courts were allowed to act under "Time of Emergency Regulations" and even sentence people to death. In this manner Yehezkel Altman, a guard in a Betar battalion in the Nahalat Yizchak neighborhood of Tel Aviv, shot at an Arab bus, without his commanders' knowledge. Altman was acting in response to a shooting at Jewish vehicles on the Tel Aviv–Jerusalem road the day before. He turned himself in later and was sentenced to death, a sentence which was later commuted to a life sentence.

Despite the arrests, Irgun members continued fighting. Jabotinsky lent his moral support to these activities. In a letter to Moshe Rosenberg on 18 March 1938 he wrote:

Although the Irgun continued activities such as these, following Rosenberg's orders, they were greatly curtailed. Furthermore, in fear of the British threat of the death sentence for anyone found carrying a weapon, all operations were suspended for eight months. However, opposition to this policy gradually increased. In April, 1938, responding to the killing of six Jews, Betar members from the Rosh Pina Brigade went on a reprisal mission, without the consent of their commander, as described by historian Avi Shlaim:

Although the incident ended without casualties, the three were caught, and one of them – Shlomo Ben-Yosef was sentenced to death. Demonstrations around the country, as well as pressure from institutions and people such as Dr. Chaim Weizmann and the Chief Rabbi of Mandatory Palestine, Yitzhak HaLevi Herzog did not reduce his sentence. In Shlomo Ben-Yosef's writings in Hebrew were later found:

On 29 June 1938 he was executed, and was the first of the Olei Hagardom. The Irgun revered him after his death and many regarded him as an example.
In light of this, and due to the anger of the Irgun leadership over the decision to adopt a policy of restraint until that point, Jabotinsky relieved Rosenberg of his post and replaced him with David Raziel, who proved to be the most prominent Irgun commander until Menachem Begin. Jabotinsky simultaneously instructed the Irgun to end its policy of restraint, leading to armed offensive operations until the end of the Arab Revolt in 1939. In this time, the Irgun mounted about 40 operations against Arabs and Arab villages, for instance:

This action led the British Parliament to discuss the disturbances in Palestine. On 23 February 1939 the Secretary of State for the Colonies, Malcolm MacDonald revealed the British intention to cancel the mandate and establish a state that would preserve Arab rights. This caused a wave of riots and attacks by Arabs against Jews. The Irgun responded four days later with a series of attacks on Arab buses and other sites. The British used military force against the Arab rioters and in the latter stages of the revolt by the Arab community in Palestine, it deteriorated into a series of internal gang wars.

At the same time, the Irgun also established itself in Europe. The Irgun built underground cells that participated in organizing migration to Palestine. The cells were made up almost entirely of Betar members, and their primary activity was military training in preparation for emigration to Palestine. Ties formed with the Polish authorities brought about courses in which Irgun commanders were trained by Polish officers in advanced military issues such as guerrilla warfare, tactics and laying land mines. Avraham (Yair) Stern was notable among the cell organizers in Europe. In 1937 the Polish authorities began to deliver large amounts of weapons to the underground. According to Irgun activists Poland supplied the organization with 25,000 rifles, and additional material and weapons, by summer 1939 the Warsaw warehouses of Irgun held 5,000 rifles and 1,000 machine guns. The training and support by Poland would allow the organization to mobilize 30,000-40,000 men The transfer of handguns, rifles, explosives and ammunition stopped with the outbreak of World War II. Another field in which the Irgun operated was the training of pilots, so they could serve in the Air Force in the future war for independence, in the flight school in Lod.

Towards the end of 1938 there was progress towards aligning the ideologies of the Irgun and the Haganah. Many abandoned the belief that the land would be divided and a Jewish state would soon exist. The Haganah founded פו"מ, a special operations unit, (pronounced "poom"), which carried out reprisal attacks following Arab violence. These operations continued into 1939. Furthermore, the opposition within the Yishuv to illegal immigration significantly decreased, and the Haganah began to bring Jews to Palestine using rented ships, as the Irgun had in the past.

The publishing of the MacDonald White Paper of 1939 brought with it new edicts that were intended to lead to a more equitable settlement between Jews and Arabs. However, it was considered by some Jews to have an adverse effect on the continued development of the Jewish community in Palestine. Chief among these was the prohibition on selling land to Jews, and the smaller quotas for Jewish immigration. The entire Yishuv was furious at the contents of the White Paper. There were demonstrations against the "Treacherous Paper", as it was considered that it would preclude the establishment of a Jewish homeland in Palestine.

Under the temporary command of Hanoch Kalai, the Irgun began sabotaging strategic infrastructure such as electricity facilities, radio and telephone lines. It also started publicizing its activity and its goals. This was done in street announcements, newspapers, as well as the underground radio station Kol Zion HaLochemet. On August 26, 1939, the Irgun killed Ralph Cairns, a British police officer who, as head of the Jewish Department in the Palestine Police, had tortured a number of youths who were underground members. Cairns and Ronald Barker, another British police officer, were killed by an Irgun IED.

The British increased their efforts against the Irgun. As a result, on August 31 the British police arrested members meeting in the Irgun headquarters. On the next day, September 1, 1939, World War II broke out.

Following the outbreak of war, Ze'ev Jabotinsky and the New Zionist Organization voiced their support for Britain and France. In mid-September 1939 Raziel was moved from his place of detention in Tzrifin. This, among other events, encouraged the Irgun to announce a cessation of its activities against the British so as not to hinder Britain's effort to fight "the Hebrew's greatest enemy in the world – German Nazism". This announcement ended with the hope that after the war a Hebrew state would be founded "within the historical borders of the liberated homeland". After this announcement Irgun, Betar and Hatzohar members, including Raziel and the Irgun leadership, were gradually released from detention. The Irgun did not rule out joining the British army and the Jewish Brigade. Irgun members did enlist in various British units. Irgun members also assisted British forces with intelligence in Romania, Bulgaria, Morocco and Tunisia. An Irgun unit also operated in Syria and Lebanon. David Raziel later died during one of these operations.

During the Holocaust, Betar members revolted numerous times against the Nazis in occupied Europe. The largest of these revolts was the Warsaw Ghetto Uprising, in which an armed underground organization fought, formed by Betar and Hatzoar and known as the "Żydowski Związek Wojskowy (ŻZW)" (Jewish Military Union). Despite its political origins, the ŻZW accepted members without regard to political affiliation, and had contacts established before the war with elements of the Polish military. Because of differences over objectives and strategy, the ŻZW was unable to form a common front with the mainstream ghetto fighters of the Żydowska Organizacja Bojowa, and fought independently under the military leadership of Paweł Frenkiel and the political leadership of Dawid Wdowiński.

There were instances of Betar members enlisted in the British military smuggling British weapons to the Irgun.

From 1939 onwards, an Irgun delegation in the United States worked for the creation of a Jewish army made up of Jewish refugees and Jews from Palestine, to fight alongside the Allied Forces. In July 1943 the "Emergency Committee to Save the Jewish People in Europe" was formed, and worked until the end of the war to rescue the Jews of Europe from the Nazis and to garner public support for a Jewish state. However, it was not until January 1944 that US President Franklin Roosevelt established the War Refugee Board, which achieved some success in saving European Jews.

Throughout this entire period, the British continued enforcing the White Paper's provisions, which included a ban on the sale of land, restrictions on Jewish immigration and increased vigilance against illegal immigration. Part of the reason why the British banned land sales (to anyone) was the confused state of the post Ottoman land registry; it was difficult to determine who actually owned the land that was for sale.

Within the ranks of the Irgun this created much disappointment and unrest, at the center of which was disagreement with the leadership of the New Zionist Organization, David Raziel and the Irgun Headquarters. On June 18, 1939, Avraham (Yair) Stern and others of the leadership were released from prison and a rift opened between them the Irgun and Hatzohar leadership. The controversy centred on the issues of the underground movement submitting to public political leadership and fighting the British. On his release from prison Raziel resigned from Headquarters. To his chagrin, independent operations of senior members of the Irgun were carried out and some commanders even doubted Raziel's loyalty.

In his place, Stern was elected to the leadership. In the past, Stern had founded secret Irgun cells in Poland without Jabotinsky's knowledge, in opposition to his wishes. Furthermore, Stern was in favor of removing the Irgun from the authority of the New Zionist Organization, whose leadership urged Raziel to return to the command of the Irgun. He finally consented. Jabotinsky wrote to Raziel and to Stern, and these letters were distributed to the branches of the Irgun:

Stern was sent a telegram with an order to obey Raziel, who was reappointed. However, these events did not prevent the splitting of the organization. Suspicion and distrust were rampant among the members. Out of the Irgun a new organization was created on July 17, 1940, which was first named "The National Military Organization in Israel" (as opposed to the "National Military Organization in the Land of Israel") and later on changed its name to Lehi, an acronym for Lohamei Herut Israel, "Fighters for the Freedom of Israel", (לח"י – לוחמי חירות ישראל). Jabotinsky died in New York on August 4, 1940, yet this did not prevent the Lehi split. Following Jabotinsky's death, ties were formed between the Irgun and the New Zionist Organization. These ties would last until 1944, when the Irgun declared a revolt against the British.

The primary difference between the Irgun and the newly formed organization was its intention to fight the British in Palestine, regardless of their war against Germany. Later, additional operational and ideological differences developed that contradicted some of the Irgun's guiding principles. For example, the Lehi, unlike the Irgun, supported a population exchange with local Arabs.

The split damaged the Irgun both organizationally and from a morale point of view. As their spiritual leader, Jabotinsky's death also added to this feeling. Together, these factors brought about a mass abandonment by members. The British took advantage of this weakness to gather intelligence and arrest Irgun activists. The new Irgun leadership, which included Meridor, Yerachmiel Ha'Levi, Rabbi Moshe Zvi Segal and others used the forced hiatus in activity to rebuild the injured organization. This period was also marked by more cooperation between the Irgun and the Jewish Agency, however David Ben-Gurion's uncompromising demand that Irgun accept the Agency's command foiled any further cooperation.

In both the Irgun and the Haganah more voices were being heard opposing any cooperation with the British. Nevertheless, an Irgun operation carried out in the service of Britain was aimed at sabotaging pro-Nazi forces in Iraq, including the assassination of Haj Amin al-Husayni. Among others, Raziel and Yaakov Meridor participated. On April 20, 1941, during a Luftwaffe air raid on RAF Hannaniya near Baghdad, David Raziel, commander of the Irgun, was killed during the operation.

In late 1943 a joint Haganah – Irgun initiative was developed, to form a single fighting body, unaligned with any political party, by the name of עם לוחם ("Fighting Nation"). The new body's first plan was to kidnap the British High Commissioner of Palestine, Sir Harold MacMichael and take him to Cyprus. However, the Haganah leaked the planned operation and it was thwarted before it got off the ground. Nevertheless, at this stage the Irgun ceased its cooperation with the British. As Eliyahu Lankin tells in his book:

In 1943 the Polish II Corps, commanded by Władysław Anders, arrived in Palestine from Iraq. The British insisted that no Jewish units of the army be created. Eventually, many of the soldiers of Jewish origin that arrived with the army were released and allowed to stay in Palestine. One of them was Menachem Begin, whose arrival in Palestine created new-found expectations within the Irgun and Betar. Begin had served as head of the Betar movement in Poland, and was a respected leader. Yaakov Meridor, then the commander of the Irgun, raised the idea of appointing Begin to the post. In late 1943, when Begin accepted the position, a new leadership was formed. Meridor became Begin's deputy, and other members of the board were Aryeh Ben Eliezer, Eliyahu Lankin, and Shlomo Lev Ami.

On February 1, 1944 the Irgun put up posters all around the country, proclaiming a revolt against the British mandatory government. The posters began by saying that all of the Zionist movements stood by the Allied Forces and over 25,000 Jews had enlisted in the British military. The hope to establish a Jewish army had died. European Jewry was trapped and was being destroyed, yet Britain, for its part, did not allow any rescue missions. This part of the document ends with the following words:

The Irgun then declared that, for its part, the ceasefire was over and they were now at war with the British. It demanded the transfer of rule to a Jewish government, to implement ten policies. Among these were the mass evacuation of Jews from Europe, the signing of treaties with any state that recognized the Jewish state's sovereignty, including Britain, granting social justice to the state's residents, and full equality to the Arab population. The proclamation ended with:

The Irgun began this campaign rather weakly. At the time of the start of the revolt, it was only about 1,000 strong, including some 200 fighters. It possessed about 4 submachine guns, 40 rifles, 60 pistols, 150 hand grenades, and 2,000 kilograms of explosive material, and its funds were about £800.

The Irgun began a militant operation against the symbols of government, in an attempt to harm the regime's operation as well as its reputation. The first attack was on February 12, 1944 at the government immigration offices, a symbol of the immigration laws. The attacks went smoothly and ended with no casualties—as they took place on a Saturday night, when the buildings were empty—in the three largest cities: Jerusalem, Tel Aviv, and Haifa. On February 27 the income tax offices were bombed. Parts of the same cities were blown up, also on a Saturday night; prior warnings were put up near the buildings. On March 23 the national headquarters building of the British police in the Russian Compound in Jerusalem was attacked, and part of it was blown up. These attacks in the first few months were sharply condemned by the organized leadership of the Yishuv and by the Jewish Agency, who saw them as dangerous provocations.

At the same time the Lehi also renewed its attacks against the British. The Irgun continued to attack police stations and headquarters, and Tegart Fort, a fortified police station (today the location of Latrun). One relatively complex operation was the takeover of the radio station in Ramallah, on May 17, 1944.

One symbolic act by the Irgun happened before Yom Kippur of 1944. They plastered notices around town, warning that no British officers should come to the Western Wall on Yom Kippur, and for the first time since the mandate began no British police officers were there to prevent the Jews from the traditional Shofar blowing at the end of the fast. After the fast that year the Irgun attacked four police stations in Arab settlements. In order to obtain weapons, the Irgun carried out "confiscation" operations – they robbed British armouries and smuggled stolen weapons to their own hiding places. During this phase of activity the Irgun also cut all of its official ties with the New Zionist Organization, so as not to tie their fate in the underground organization.

Begin wrote in his memoirs, "The Revolt":

In October 1944 the British began expelling hundreds of arrested Irgun and Lehi members to detention camps in Africa. 251 detainees from Latrun were flown on thirteen planes, on October 19 to a camp in Asmara, Eritrea. Eleven additional transports were made. Throughout the period of their detention, the detainees often initiated rebellions and hunger strikes. Many escape attempts were made until July 1948 when the exiles were returned to Israel. While there were numerous successful escapes from the camp itself, only nine men actually made it back all the way. One noted success was that of Yaakov Meridor, who escaped nine times before finally reaching Europe in April 1948. These tribulations were the subject of his book "Long is the Path to Freedom: Chronicles of one of the Exiles".

On November 6, 1944, Lord Moyne, British Deputy Resident Minister of State in Cairo was assassinated by Lehi members Eliyahu Hakim and Eliyahu Bet-Zuri. This act raised concerns within the Yishuv from the British regime's reaction to the underground's violent acts against them. Therefore, the Jewish Agency decided on starting a "Hunting Season", known as the "saison", (from the French "la saison de chasse").

The Irgun's recuperation was noticeable when it began to renew its cooperation with the Lehi in May 1945, when it sabotaged oil pipelines, telephone lines and railroad bridges. All in all, over 1,000 members of the Irgun and Lehi were arrested and interned in British camps during the "Saison". Eventually the Hunting Season died out, and there was even talk of cooperation with the Haganah leading to the formation of the Jewish Resistance Movement.

Towards the end of July 1945 the Labour party in Britain was elected to power. The Yishuv leadership had high hopes that this would change the anti-Zionist policy that the British maintained at the time. However, these hopes were quickly dashed when the government limited Jewish immigration, with the intention that the population of Mandatory Palestine (the land west of the Jordan River) would not be more than one-third of the total. This, along with the stepping up of arrests and their pursuit of underground members and illegal immigration organizers led to the formation of the Jewish Resistance Movement. This body consolidated the armed resistance to the British of the Irgun, Lehi, and Haganah. For ten months the Irgun and the Lehi cooperated and they carried out nineteen attacks and defense operations. The Haganah and Palmach carried out ten such operations. The Haganah also assisted in landing 13,000 illegal immigrants.

Tension between the underground movements and the British increased with the increase in operations. On April 23, 1946, an operation undertaken by the Irgun to gain weapons from the Tegart fort at Ramat Gan resulted in a firefight with the police in which an Arab constable and two Irgun fighters were killed, including one who jumped on an explosive device to save his comrades. A third fighter, Dov Gruner, was wounded and captured. He stood trial and was sentenced to be death by hanging, refusing to sign a pardon request.

In 1946, British relations with the Yishuv worsened, building up to Operation Agatha of June 29. The authorities ignored the Anglo-American Committee of Inquiry's recommendation to allow 100,000 Jews into Palestine at once. As a result of the discovery of documents tying the Jewish Agency to the Jewish Resistance Movement, the Irgun was asked to speed up the plans for the King David Hotel bombing of July 22. The hotel was where the documents were located, the base for the British Secretariat, the military command and a branch of the Criminal Investigation Division of the police. The Irgun later claimed to have sent a warning that was ignored. Palestinian and U.S. sources confirm that the Irgun issued numerous warnings for civilians to evacuate the hotel prior to the bombing. 91 people were killed in the attack where a 350 kg bomb was placed in the basement of the hotel and caused a large section of it to collapse. Only 13 were British soldiers.

The King David Hotel bombing and the arrest of Jewish Agency and other Yishuv leaders as part of Operation Agatha caused the Haganah to cease their armed activity against the British. Yishuv and Jewish Agency leaders were released from prison. From then until the end of the British mandate, resistance activities were led by the Irgun and Lehi. In early September 1946 the Irgun renewed its attacks against civil structures, railroads, communication lines and bridges. One operation was the attack on the train station in Jerusalem, in which Meir Feinstein was arrested and later committed suicide awaiting execution. According to the Irgun these sort of armed attacks were legitimate, since the trains primarily served the British, for redeployment of their forces. The Irgun also publicized leaflets, in three languages, not to use specific trains in danger of being attacked. For a while, the British stopped train traffic at night. The Irgun also carried out repeated attacks against military and police traffic using disguised, electronically-detonated roadside mines which could be detonated by an operator hiding nearby as a vehicle passed, carried out arms raids against military bases and police stations (often disguised as British soldiers), launched bombing, shooting, and mortar attacks against military and police installations and checkpoints, and robbed banks to gain funds as a result of losing access to Haganah funding following the collapse of the Jewish Resistance Movement.

On October 31, 1946, in response to the British barring entry of Jews from Palestine, the Irgun blew up the British Embassy in Rome, a center of British efforts to monitor and stop Jewish immigration. The Irgun also carried out a few other operations in Europe: a British troop train was derailed and an attempt against another troop train failed. An attack on a British officers club in Vienna took place in 1947, and an attack on another British officer's club in Vienna and a sergeant's club in Germany took place in 1948.

In December 1946 a sentence of 18 years and 18 beatings was handed down to a young Irgun member for robbing a bank. The Irgun made good on a threat they made and after the detainee was whipped, Irgun members kidnapped British officers and beat them in public. The operation, known as the "Night of the Beatings" brought an end to British punitive beatings. The British, taking these acts seriously, moved many British families in Palestine into the confines of military bases, and some moved home.
On February 14, 1947, Ernest Bevin announced that the Jews and Arabs would not be able to agree on any British proposed solution for the land, and therefore the issue must be brought to the United Nations (UN) for a final decision. The Yishuv thought of the idea to transfer the issue to the UN as a British attempt to achieve delay while a UN inquiry commission would be established, and its ideas discussed, and all the while the Yishuv would weaken. Foundation for Immigration B increased the number of ships bringing in Jewish refugees. The British still strictly enforced the policy of limited Jewish immigration and illegal immigrants were placed in detention camps in Cyprus, which increased the anger of the Jewish community towards the mandate government.

The Irgun stepped up its activity and from February 19 until March 3 it attacked 18 British military camps, convoy routes, vehicles, and other facilities. The most notable of these attacks was the bombing of a British officer's club located in Goldsmith House in Jerusalem, which was in a heavily guarded security zone. Covered by machine-gun fire, an Irgun assault team in a truck penetrated the security zone and lobbed explosives into the building. Thirteen people, including two officers, were killed. As a result, martial law was imposed over much of the country, enforced by approximately 20,000 British soldiers. Despite this, attacks continued throughout the martial law period. The most notable one was an Irgun attack against the Royal Army Pay Corps base at the Schneller Orphanage, in which a British soldier was killed.

Throughout its struggle against the British, the Irgun sought to publicize its cause around the world. By humiliating the British, it attempted to focus global attention on Palestine, hoping that any British overreaction would be widely reported, and thus result in more political pressure against the British. Begin described this strategy as turning Palestine into a "glass house". The Irgun also re-established many representative offices internationally, and by 1948 operated in 23 states. In these countries, the Irgun sometimes acted against the local British representatives or led public relations campaigns against Britain. According to Bruce Hoffman: ""In an era long before the advent of 24/7 global news coverage and instantaneous satellite-transmitted broadcasts, the Irgun deliberately attempted to appeal to a worldwide audience far beyond the immediate confines of its local struggle, and beyond even the ruling regime's own homeland"."

On April 16, 1947, Irgun members Dov Gruner, Yehiel Dresner, Eliezer Kashani, and Mordechai Alkahi were hanged in Acre Prison, while singing Hatikvah. On April 21 Meir Feinstein and Lehi member Moshe Barazani blew themselves up, using a smuggled grenade, hours before their scheduled hanging. And on May 4 one of the Irgun's largest operations took place – the raid on Acre Prison. The operation was carried out by 23 men, commanded by Dov Cohen – AKA "Shimshon", along with the help of the Irgun and Lehi prisoners inside the prison. The Irgun had informed them of the plan in advance and smuggled in explosives. After a hole was blasted in the prison wall, the 41 Irgun and Lehi members who had been chosen to escape then ran to the hole, blasting through inner prison gates with the smuggled explosives. Meanwhile, Irgun teams mined roads and launched a mortar attack on a nearby British Army camp to delay the arrival of responding British forces. Although the 41 escapees managed to get out of the prison and board the escape trucks, some were rapidly recaptured and nine of the escapees and attackers were killed. Five Irgun men in the attacking party were also captured. Overall, 27 of the 41 designated escapees managed to escape. Along with the underground movement members, other criminals – including 214 Arabs – also escaped. Of the five attackers who were caught, three of them – Avshalom Haviv, Meir Nakar, and Yaakov Weiss, were sentenced to death.

After the death sentences of the three were confirmed, the Irgun tried to save them by kidnapping hostages — British sergeants Clifford Martin and Mervyn Paice — in the streets of Netanya. British forces closed off and combed the area in search of the two, but did not find them. On July 29, 1947, in the afternoon, Meir Nakar, Avshalom Haviv, and Yaakov Weiss were executed. Approximately thirteen hours later the hostages were hanged in retaliation by the Irgun and their bodies, booby-trapped with an explosive, afterwards strung up from trees in woodlands south of Netanya. This action caused an outcry in Britain and was condemned both there and by Jewish leaders in Palestine.

This episode has been given as a major influence on the British decision to terminate the Mandate and leave Palestine. The United Nations Special Committee on Palestine (UNSCOP) was also influenced by this and other actions. At the same time another incident was developing – the events of the ship "Exodus 1947". The 4,500 Holocaust survivors on board were not allowed to enter Palestine. UNSCOP also covered the events. Some of its members were even present at Haifa port when the putative immigrants were forcefully removed from their ship (later found to have been rigged with an IED by some of its passengers) onto the deportation ships, and later commented that this strong image helped them press for an immediate solution for Jewish immigration and the question of Palestine.

Two weeks later, the House of Commons convened for a special debate on events in Palestine, and concluded that their soldiers should be withdrawn as soon as possible.

UNSCOP's conclusion was a unanimous decision to end the British mandate, and a majority decision to divide Mandatory Palestine (the land west of the Jordan River) between a Jewish state and an Arab state. During the UN's deliberations regarding the committee's recommendations the Irgun avoided initiating any attacks, so as not to influence the UN negatively on the idea of a Jewish state. On November 29 the UN General Assembly voted in favor of ending the mandate and establishing two states on the land. That very same day the Irgun and the Lehi renewed their attacks on British targets. The next day the local Arabs began attacking the Jewish community, thus beginning the first stage of the 1948 Palestine War. The first attacks on Jews were in Jewish neighborhoods of Jerusalem, in and around Jaffa, and in Bat Yam, Holon, and the Ha'Tikvah neighborhood in Tel Aviv.

In the autumn of 1947, the Irgun had approximately 4,000 members. The goal of the organization at that point was the conquest of the land between the Jordan River and the Mediterranean Sea for the future Jewish state and preventing Arab forces from driving out the Jewish community. The Irgun became almost an overt organization, establishing military bases in Ramat Gan and Petah Tikva. It began recruiting openly, thus significantly increasing in size. During the war the Irgun fought alongside the Lehi and the Haganah in the front against the Arab attacks. At first the Haganah maintained a defensive policy, as it had until then, but after the Convoy of 35 incident it completely abandoned its policy of restraint: "Distinguishing between individuals is no longer possible, for now – it is a war, and even the innocent shall not be absolved."

The Irgun also began carrying out reprisal missions, as it had under David Raziel's command. At the same time though, it published announcements calling on the Arabs to lay down their weapons and maintain a ceasefire:

However, the mutual attacks continued. The Irgun attacked the Arab villages of Tira near Haifa, Yehudiya ('Abassiya) in the center, and Shuafat by Jerusalem. The Irgun also attacked in the Wadi Rushmiya neighborhood in Haifa and Abu Kabir in Jaffa. On December 29 Irgun units arrived by boat to the Jaffa shore and a gunfight between them and Arab gangs ensued. The following day a bomb was thrown from a speeding Irgun car at a group of Arab men waiting to be hired for the day at the Haifa oil refinery, resulting in seven Arabs killed, and dozens injured. In response, some Arab workers attacked Jews in the area, killing 41. This sparked a Haganah response in Balad al-Sheykh, which resulted in the deaths of 60 civilians. The Irgun's goal in the fighting was to move the battles from Jewish populated areas to Arab populated areas. On January 1, 1948 the Irgun attacked again in Jaffa, its men wearing British uniforms; later in the month it attacked in Beit Nabala, a base for many Arab fighters. On 5 January 1948 the Irgun detonated a lorry bomb outside Jaffa's Ottoman built Town Hall, killing 14 and injuring 19. In Jerusalem, two days later, Irgun members in a stolen police van rolled a barrel bomb into a large group of civilians who were waiting for a bus by the Jaffa Gate, killing around sixteen. In the pursuit that followed three of the attackers were killed and two taken prisoner.

On 6 April 1948, the Irgun raided the British Army camp at Pardes Hanna killing six British soldiers and their commanding officer.

The Deir Yassin massacre was carried out in a village west of Jerusalem that had signed a non-belligerency pact with its Jewish neighbors and the Haganah, and repeatedly had barred entry to foreign irregulars. On 9 April approximately 120 Irgun and Lehi members began an operation to capture the village. During the operation, the villagers fiercely resisted the attack, and a battle broke out. In the end, the Irgun and Lehi forces advanced gradually through house-to-house fighting. The village was only taken after the Irgun began systematically dynamiting houses, and after a Palmach unit intervened and employed mortar fire to silence the villagers' sniper positions. The operation resulted in five Jewish fighters dead and 40 injured. Some 100 to 120 villagers were also killed.

There are allegations that Irgun and Lehi forces committed war crimes during and after the capture of the village. These allegations include reports that fleeing individuals and families were fired at, and prisoners of war were killed after their capture. A Haganah report writes:

Some say that this incident was an event that accelerated the Arab exodus from Palestine.

The Irgun cooperated with the Haganah in the conquest of Haifa. At the regional commander's request, on April 21 the Irgun took over an Arab post above Hadar Ha'Carmel as well as the Arab neighborhood of Wadi Nisnas, adjacent to the Lower City.

The Irgun acted independently in the conquest of Jaffa (part of the proposed Arab State according to the UN Partition Plan). On April 25 Irgun units, about 600 strong, left the Irgun base in Ramat Gan towards Arab Jaffa. Difficult battles ensued, and the Irgun faced resistance from the Arabs as well as the British. Under the command of Amichai "Gidi" Paglin, the Irgun's chief operations officer, the Irgun captured the neighborhood of Manshiya, which threatened the city of Tel Aviv. Afterwards the force continued to the sea, towards the area of the port, and using mortars, shelled the southern neighborhoods.

On May 14, 1948 the establishment of the State of Israel was proclaimed. The declaration of independence was followed by the establishment of the Israel Defense Forces (IDF), and the process of absorbing all military organizations into the IDF started. On June 1, an agreement had been signed between Menachem Begin and Yisrael Galili for the absorption of the Irgun into the IDF. One of the clauses stated that the Irgun had to stop smuggling arms. Meanwhile, in France, Irgun representatives purchased a ship, renamed "Altalena" (a pseudonym of Ze'ev Jabotinsky), and weapons. The ship sailed on June 11 and arrived at the Israeli coast on June 20, during the first truce of the 1948 Arab–Israeli War. Despite United Nations Security Council Resolution 50 declared an arms embargo in the region, neither side respected it.

When the ship arrived the Israeli government, headed by Ben-Gurion, was adamant in its demand that the Irgun surrender and hand over all of the weapons. Ben-Gurion said: "We must decide whether to hand over power to Begin or to order him to cease his activities. If he does not do so, we will open fire! Otherwise, we must decide to disperse our own army."
There were two confrontations between the newly formed IDF and the Irgun: when "Altalena" reached Kfar Vitkin in the late afternoon of Sunday, June 20 many Irgun militants, including Begin, waited on the shore. A clash with the Alexandroni Brigade, commanded by Dan Even (Epstein), occurred. Fighting ensued and there were a number of casualties on both sides. The clash ended in a ceasefire and the transfer of the weapons on shore to the local IDF commander, and with the ship, now reinforced with local Irgun members, including Begin, sailing to Tel Aviv, where the Irgun had more supporters.
Many Irgun members, who joined the IDF earlier that month, left their bases and concentrated on the Tel Aviv beach. A confrontation between them and the IDF units started. In response, Ben-Gurion ordered Yigael Yadin (acting Chief of Staff) to concentrate large forces on the Tel Aviv beach and to take the ship by force. Heavy guns were transferred to the area and at four in the afternoon, Ben-Gurion ordered the shelling of the "Altalena". One of the shells hit the ship, which began to burn.
Sixteen Irgun fighters were killed in the confrontation with the army; six were killed in the Kfar Vitkin area and ten on Tel Aviv beach. Three IDF soldiers were killed: two at Kfar Vitkin and one in Tel Aviv.

After the shelling of the "Altalena", more than 200 Irgun fighters were arrested. Most of them were freed several weeks later. The Irgun militants were then fully integrated with the IDF and not kept in separate units.

The initial agreement for the integration of the Irgun into the IDF did not include Jerusalem, where a small remnant of the Irgun called the "Jerusalem Battalion", numbering around 400 fighters, and Lehi, continued to operate independently of the government. Following the assassination of UN Envoy for Peace Folke Bernadotte by Lehi in September 1948, the Israeli government determined to immediately dismantle the underground organizations. An ultimatum was issued to the Irgun to liquidate as an independent organization and integrate into the IDF or be destroyed, and Israeli troops surrounded the Irgun camp in the Katamon Quarter of Jerusalem. The Irgun accepted the ultimatum on September 22, 1948, and shortly afterward the remaining Irgun fighters in Jerusalem began enlisting in the IDF and turning over their arms. At Begin's orders, the Irgun in the diaspora formally disbanded on January 12, 1949, with the Irgun's former Paris headquarters becoming the European bureau of the Herut movement.

In order to increase the popularity of the Irgun organization and ideology, Irgun employed propaganda. This propaganda was mainly aimed at the British, and included the idea of Eretz Israel. According to Irgun , the Jewish state was not only to encompass all of Mandatory Palestine, but also The Emirate of Transjordan.

When the Labour party came into power in Britain in July 1945, Irgun published an announcement entitled, "We shall give the Labour Government a Chance to Keep Its Word." In this publication, Irgun stated, "Before it came to power, this Party undertook to return the Land of Israel to the people of Israel as a free state... Men and parties in opposition or in their struggle with their rivals, have, for twenty-five years, made us many promises and undertaken clear obligations; but, on coming to power, they have gone back on their words." Another publication, which followed a British counter-offensive against Jewish organizations in Palestine, Irgun released a document titled, "Mobilize the Nation!" Irgun used this publication to paint the British regime as hostile to the Jewish people, even comparing the British to the Nazis. In response to what was seen as British aggression, Irgun called for a Hebrew Provisional Government, and a Hebrew Liberation Army.

References to the Irgun as a terrorist organization came from sources including the Anglo-American Committee of Inquiry, newspapers and a number of prominent world and Jewish figures.
Leaders within the mainstream Jewish organizations, the Jewish Agency, Haganah and Histadrut, as well as the British authorities, routinely condemned Irgun operations as terrorism and branded it an illegal organization as a result of the group's attacks on civilian targets. However, privately at least the Haganah kept a dialogue with the dissident groups.
Ironically, in early 1947, "the British army in Mandate Palestine banned the use of the term 'terrorist' to refer to the Irgun zvai Leumi ... because it implied that British forces had reason to be terrified."

Irgun attacks prompted a formal declaration from the World Zionist Congress in 1946, which strongly condemned "the shedding of innocent blood as a means of political warfare."

The Israeli government, in September 1948, acting in response to the assassination of Count Folke Bernadotte, outlawed the Irgun and Lehi groups, declaring them terrorist organizations under the Prevention of Terrorism Ordinance.

In 1948, "The New York Times" published a letter signed by a number of prominent Jewish figures including Hannah Arendt, Albert Einstein, Sidney Hook, and Rabbi Jessurun Cardozo, which described Irgun as "a terrorist, right-wing, chauvinist organization in Palestine". The letter went on to state that Irgun and the Stern gang "inaugurated a reign of terror in the Palestine Jewish community. Teachers were beaten up for speaking against them, adults were shot for not letting their children join them. By gangster methods, beatings, window-smashing, and widespread robberies, the terrorists intimidated the population and exacted a heavy tribute."

Soon after World War II, Winston Churchill said "we should never have stopped immigration before the war", but that the Irgun were "the vilest gangsters" and that he would "never forgive the Irgun terrorists."

In 2006, Simon McDonald, the British ambassador in Tel Aviv, and John Jenkins, the Consul-General in Jerusalem, wrote in response to a pro-Irgun commemoration of the King David Hotel bombing: "We do not think that it is right for an act of terrorism, which led to the loss of many lives, to be commemorated." They also called for the removal of plaques at the site which presented as a fact that the deaths were due to the British ignoring warning calls. The plaques, in their original version, read:

McDonald and Jenkins said that no such warning calls were made, adding that even if they had, "this does not absolve those who planted the bomb from responsibility for the deaths."

Bruce Hoffman states: "Unlike many terrorist groups today, the Irgun's strategy was not deliberately to target or wantonly harm civilians." Max Abrahms writes that the Irgun "pioneered the practice of issuing pre-attack warnings to spare civilians", which was later emulated by the African National Congress (ANC) and other groups and proved "effective but not foolproof". In addition, Begin ordered attacks to take place at night and even during Shabbat to reduce the likelihood of civilian casualties. U.S. military intelligence found that "the Irgun Zvai Leumi is waging a general war against the government and at all times took special care not to cause damage or injury to persons". Although the King David Hotel bombing is widely considered a "prima facie" case of Irgun terrorism, Abrahms comments: "But this hotel wasn't a normal hotel. It served as the headquarters for the British Armed Forces in Palestine. By all accounts the intent wasn't to harm civilians."

"Ha'aretz" columnist and Israeli historian Tom Segev wrote of the Irgun: "In the second half of 1940, a few members of the Irgun Zvai Leumi (National Military Organization) – the anti-British terrorist group sponsored by the Revisionists and known by its acronym Etzel, and to the British simply as the Irgun – made contact with representatives of Fascist Italy, offering to cooperate against the British."

Clare Hollingworth, the "Daily Telegraph" and "The Scotsman" correspondent in Jerusalem during 1948 wrote several outspoken reports after spending several weeks in West Jerusalem:

A US military intelligence report, dated January 1948, described Irgun recruiting tactics amongst Displaced Persons (DP) in the camps across Germany:

Alan Dershowitz wrote in his book "The Case for Israel" that unlike the Haganah, the policy of the Irgun had been to encourage the flight of local Arabs.







</doc>
<doc id="15408" url="https://en.wikipedia.org/wiki?curid=15408" title="Isoroku Yamamoto">
Isoroku Yamamoto

Yamamoto held several important posts in the IJN, and undertook many of its changes and reorganizations, especially its development of naval aviation. He was the commander-in-chief during the early years of the Pacific War and oversaw major engagements including the attack on Pearl Harbor and the Battle of Midway. He was killed when American code breakers identified his flight plans, enabling the United States Army Air Forces to shoot down his plane. His death was a major blow to Japanese military morale during World War II.

Yamamoto was born in Nagaoka, Niigata. His father, Sadayoshi Takano (高野 貞吉), was an intermediate-rank "samurai" of the Nagaoka Domain. "Isoroku" is an old Japanese term meaning "56"; the name referred to his father's age at Isoroku's birth.

In 1916, Isoroku was adopted into the Yamamoto family (another family of former Nagaoka samurai) and took the Yamamoto name. It was a common practice for samurai families lacking sons to adopt suitable young men in this fashion to carry on the family name, the rank and the income that went with it. Isoroku married Reiko Mihashi in 1918; they had two sons and two daughters.

After graduating from the Imperial Japanese Naval Academy in 1904, Yamamoto served on the armored cruiser during the Russo-Japanese War. He was wounded at the Battle of Tsushima, losing two fingers (the index and middle fingers) on his left hand, as the cruiser was hit repeatedly by the Russian battle line. He returned to the Naval Staff College in 1914, emerging as a lieutenant commander in 1916. In December of 1919, he was promoted to Commander. 

Yamamoto was part of the Japanese Navy establishment, who were rivals of the more aggressive army establishment, especially the officers of the Kwantung Army. He promoted a policy of a strong fleet to project force through gunboat diplomacy, rather than a fleet used primarily for transport of invasion land forces, as some of his political opponents in the army wanted. This stance led him to oppose the invasion of China. He also opposed war against the United States, partly because of his studies at Harvard University (1919–1921) and his two postings as a naval attaché in Washington, D.C., where he learned to speak fluent English. Yamamoto traveled extensively in the United States during his tour of duty there, where he studied American customs and business practices.

He was promoted to captain in 1923. On February 13, 1924, at the rank of captain, he was part of the Japanese delegation visiting the US Naval War College. Later that year, he changed his specialty from gunnery to naval aviation. His first command was the cruiser in 1928, followed by the aircraft carrier .
He participated in the London Naval Conference 1930 as a rear admiral and the London Naval Conference 1935 as a vice admiral, as the growing military influence on the government at the time deemed that a career military specialist needed to accompany the diplomats to the arms limitations talks. Yamamoto was a strong proponent of naval aviation, and served as head of the Aeronautics Department before accepting a post as commander of the First Carrier Division. Yamamoto opposed the Japanese invasion of northeast China in 1931, the subsequent full-scale land war with China in 1937, and the Tripartite Pact with Nazi Germany and fascist Italy in 1940. As Deputy Navy Minister, he apologized to United States Ambassador Joseph C. Grew for the bombing of the gunboat in December 1937. These issues made him a target of assassination threats by pro-war militarists.

Throughout 1938, many young army and naval officers began to speak publicly against Yamamoto and certain other Japanese admirals such as Mitsumasa Yonai and Shigeyoshi Inoue for their strong opposition to a tripartite pact with Nazi Germany, which the admirals saw as inimical to "Japan's natural interests". Yamamoto received a steady stream of hate mail and death threats from Japanese nationalists. His reaction to the prospect of death by assassination was passive and accepting. The admiral wrote:
"To die for Emperor and Nation is the highest hope of a military man. After a brave hard fight the blossoms are scattered on the fighting field. But if a person wants to take a life instead, still the fighting man will go to eternity for Emperor and country. One man's life or death is a matter of no importance. All that matters is the Empire. As Confucius said, "They may crush cinnabar, yet they do not take away its color; one may burn a fragrant herb, yet it will not destroy the scent." They may destroy my body, yet they will not take away my will."

The Japanese Army, annoyed at Yamamoto's unflinching opposition to a Rome-Berlin-Tokyo treaty, dispatched military police to "guard" Yamamoto, a ruse by the army to keep an eye on him. He was later reassigned from the naval ministry to sea as the commander-in-chief of the Combined Fleet on August 30, 1939. This was done as one of the last acts of the acting Navy Minister Mitsumasa Yonai, under Baron Hiranuma's short-lived administration. It was done partly to make it harder for assassins to target Yamamoto. Yonai was certain that if Yamamoto remained ashore, he would be killed before the year [1939] ended.

Yamamoto was promoted to admiral on November 15, 1940. This, in spite of the fact that when Hideki Tōjō was appointed prime minister on October 18, 1941, many political observers thought that Yamamoto's career was essentially over. Tōjō had been Yamamoto's old opponent from the time when the latter served as Japan's deputy naval minister and Tōjō was the prime mover behind Japan's takeover of Manchuria. It was believed that Yamamoto would be appointed to command the Yokosuka Naval Base, "a nice safe demotion with a big house and no power at all". However, after a brief stint in the post, a new Japanese cabinet was announced, and Yamamoto found himself returned to his position of power despite his open conflicts with Tōjō and other members of the army's oligarchy who favored war with the European powers and the United States. 

Two of the main reasons for Yamamoto's political survival were his immense popularity within the fleet, where he commanded the respect of his men and officers, and his close relations with the imperial family. He also had the acceptance of Japan's naval hierarchy:
Consequently, Yamamoto stayed in his post. With Tōjō now in charge of Japan's highest political office, it became clear the army would lead the navy into a war about which Yamamoto had serious reservations. He wrote to an ultranationalist:

This quote was spread by the militarists, minus the last sentence, where it was interpreted in America as a boast that Japan would conquer the entire continental United States. The omitted sentence showed Yamamoto's counsel of caution towards a war that could cost Japan dearly. Nevertheless, Yamamoto accepted the reality of impending war and planned for a quick victory by destroying the United States Pacific Fleet at Pearl Harbor in a preventive strike while simultaneously thrusting into the oil and rubber resource-rich areas of Southeast Asia, especially the Dutch East Indies, Borneo, and Malaya. In naval matters, Yamamoto opposed the building of the super-battleships and as an unwise investment of resources.

Yamamoto was responsible for a number of innovations in Japanese naval aviation. Although remembered for his association with aircraft carriers, Yamamoto did more to influence the development of land-based naval aviation, particularly the Mitsubishi G3M and G4M medium bombers. His demand for great range and the ability to carry a torpedo was intended to conform to Japanese conceptions of bleeding the American fleet as it advanced across the Pacific. The planes did achieve long range, but long-range fighter escorts were not available. These planes were lightly constructed and when fully fueled, they were especially vulnerable to enemy fire. This earned the G4M the sardonic nickname the "flying cigarette lighter". Yamamoto would eventually die in one of these aircraft.

The range of the G3M and G4M contributed to a demand for great range in a fighter aircraft. This partly drove the requirements for the A6M Zero which was as noteworthy for its range as for its maneuverability. Both qualities were again purchased at the expense of light construction and flammability that later contributed to the A6M's high casualty rates as the war progressed.

As Japan moved toward war during 1940, Yamamoto gradually moved toward strategic as well as tactical innovation, again with mixed results. Prompted by talented young officers such as Lieutenant Commander Minoru Genda, Yamamoto approved the reorganization of Japanese carrier forces into the First Air Fleet, a consolidated striking force that gathered Japan's six largest carriers into one unit. This innovation gave great striking capacity, but also concentrated the vulnerable carriers into a compact target. Yamamoto also oversaw the organization of a similar large land-based organization in the 11th Air Fleet, which would later use the G3M and G4M to neutralize American air forces in the Philippines and sink the British "Force Z".

In January 1941, Yamamoto went even further and proposed a radical revision of Japanese naval strategy. For two decades, in keeping with the doctrine of Captain Alfred T. Mahan, the Naval General Staff had planned in terms of Japanese light surface forces, submarines, and land-based air units whittling down the American Fleet as it advanced across the Pacific until the Japanese Navy engaged it in a climactic "decisive battle" in the northern Philippine Sea (between the Ryukyu Islands and the Marianas), with battleships meeting in the traditional exchange between battle lines.

Correctly pointing out this plan had never worked even in Japanese war games, and painfully aware of American strategic advantages in military production capacity, Yamamoto proposed instead to seek parity with the Americans by first reducing their forces with a preventive strike, then following up with a "decisive battle" fought offensively, rather than defensively. Yamamoto hoped, but probably did not believe, that if the Americans could be dealt terrific blows early in the war, they might be willing to negotiate an end to the conflict. The Naval General Staff proved reluctant to go along and Yamamoto was eventually driven to capitalize on his popularity in the fleet by threatening to resign to get his way. Admiral Osami Nagano and the Naval General Staff eventually caved in to this pressure, but only insofar as approving the attack on Pearl Harbor.

The First Air Fleet commenced preparations for the Pearl Harbor raid, solving a number of technical problems along the way, including how to launch torpedoes in the shallow water of Pearl Harbor and how to craft armor-piercing bombs by machining down battleship gun projectiles.

As the U.S. and Japan were officially at peace, the First Air Fleet of six carriers attacked the U.S. on December 7, 1941, launching 353 aircraft against Pearl Harbor and other locations within Honolulu in two waves. The attack was a complete success according to the parameters of the mission, which sought to sink at least four American battleships and prevent the U.S. from interfering in Japan's southward advance for at least six months. Three American aircraft carriers were also considered a choice target, but these were not in port at the time of the attack.

In the end, four American battleships were sunk, four were damaged, and eleven other cruisers, destroyers, and auxiliaries were sunk or seriously damaged, 188 American aircraft were destroyed and 159 others damaged, and 2,403 people were killed and 1,178 others wounded. The Japanese lost 64 servicemen and only 29 aircraft, with 74 others damaged by anti-aircraft fire from the ground. The damaged aircraft were disproportionately dive and torpedo bombers, seriously impacting available firepower to exploit the first two waves' success, so the commander of the First Air Fleet, Naval Vice Admiral Chuichi Nagumo, withdrew. Yamamoto later lamented Nagumo's failure to seize the initiative to seek out and destroy the U.S. carriers, absent from the harbor, or further bombard various strategically important facilities on Oahu. 

Nagumo had absolutely no idea where the American carriers might be, and remaining on station while his forces cast about looking for them ran the risk of his own forces being found first and attacked while his aircraft were absent searching. In any case, insufficient daylight remained after recovering the aircraft from the first two waves for the carriers to launch and recover a third before dark, and Nagumo's escorting destroyers lacked the fuel capacity for him to loiter long. Much has been made of Yamamoto's hindsight, but, in keeping with Japanese military tradition not to criticize the commander on the spot, he did not punish Nagumo for his withdrawal.

On the strategic, moral, and political level, the attack was a disaster for Japan, rousing American passions for revenge due to what is now famously called a "sneak attack". The shock of the attack, coming in an unexpected place with devastating results and without a declaration of war, galvanized the U.S. public's determination to avenge the attack. When asked by Prime Minister Fumimaro Konoe in mid-1941 about the outcome of a possible war with the United States, Yamamoto made a well-known and prophetic statement: If ordered to fight, he said, "I shall run wild considerably for the first six months or a year, but I have utterly no confidence for the second and third years." His prediction would be vindicated, as Japan easily conquered territories and islands in Asia and the Pacific for the first six months of the war, before suffering a major defeat at the Battle of Midway on June 4–7, 1942, which ultimately tilted the balance of power in the Pacific towards the U.S.

As a strategic blow intended to prevent American interference in the Dutch East Indies for six months, the Pearl Harbor attack was a success, but unbeknownst to Yamamoto, it was a pointless one. In 1935, in keeping with the evolution of War Plan Orange, the U.S. Navy had abandoned any intention of attempting to charge across the Pacific towards the Philippines at the outset of a war with Japan. In 1937, the U.S. Navy had further determined even fully manning the fleet to wartime levels could not be accomplished in less than six months, and myriad other logistic assets needed to execute a trans-Pacific movement simply did not exist and would require two years to construct after the onset of war. 

In 1940, U.S. Chief of Naval Operations Admiral Harold Stark had penned a Plan Dog memorandum, which emphasized a defensive war in the Pacific while the US concentrated on defeating Nazi Germany first, and consigned Admiral Husband Kimmel's Pacific Fleet to merely keeping the Imperial Japanese Navy (IJN) out of the eastern Pacific and away from the shipping lanes to Australia. Moreover, it is questionable whether the US would have gone to war at all had Japan attacked only British and Dutch possessions in the Far East.

With the US fleet largely neutralized at Pearl Harbor, Yamamoto's Combined Fleet turned to the task of executing the larger Japanese war plan devised by the Imperial Japanese Army (IJA) and Navy General Staff. The First Air Fleet made a circuit of the Pacific, striking American, Australian, Dutch and British installations from Wake Island to Australia to Ceylon in the Indian Ocean. The 11th Air Fleet caught the US 5th Air Force on the ground in the Philippines hours after Pearl Harbor, and then sank the British Force Z battleship and battlecruiser underway at sea.

Under Yamamoto's able subordinates, Vice Admirals Jisaburō Ozawa, Nobutake Kondō, and Ibō Takahashi, the Japanese swept the inadequate remaining American, British, Dutch and Australian naval assets from the Dutch East Indies in a series of amphibious landings and surface naval battles culminating in the Battle of the Java Sea on February 27, 1942. Along with the occupation of the Dutch East Indies came the fall of Singapore on February 15, 1942, and the eventual reduction of the remaining American-Filipino defensive positions in the Philippines on the Bataan peninsula, April 9, 1942, and Corregidor Island on May 6, 1942. The Japanese had secured their oil- and rubber-rich "southern resources area".

By late-March, having achieved their initial aims with surprising speed and little loss, albeit against enemies ill-prepared to resist them, the Japanese paused to consider their next moves. Yamamoto and a few Japanese military leaders and officials waited, hoping that the United States or Great Britain would negotiate an armistice or a peace treaty to end the war. But when the British, as well as the Americans, expressed no interest in negotiating a ceasefire with Japan, Japanese thoughts turned to securing their newly seized territory and acquiring more with an eye to forcing one or more of their enemies out of the war.

Competing plans were developed at this stage, including thrusts to the west against India, the south against Australia, and east against the United States. Yamamoto was involved in this debate, supporting different plans at different times with varying degrees of enthusiasm and for varying purposes, including "horse-trading" for support of his own objectives.

Plans included ideas as ambitious as invading India or Australia, or seizing Hawaii. These grandiose ventures were inevitably set aside as the army could not spare enough troops from China for the first two, which would require a minimum of 250,000 men, nor shipping to support the latter two (transports were allocated separately to the IJN and IJA, and jealously guarded.) Instead, the Imperial General Staff supported an army thrust into Burma in hopes of linking up with Indian Nationalists revolting against British rule, and attacks in New Guinea and the Solomon Islands designed to imperil Australia's lines of communication with the United States. Yamamoto argued for a decisive offensive strike in the east to finish off the US fleet, but the more conservative Naval General Staff officers were unwilling to risk it.

On April 18, in the midst of these debates, the Doolittle Raid struck Tokyo and surrounding areas, demonstrating the threat posed by US aircraft carriers, and giving Yamamoto an event he could exploit to get his way as further debate over military strategy came to a quick end. The Naval General Staff agreed to Yamamoto's Midway Island (MI) Operation, subsequent to the first phase of the operations against Australia's link with America, and concurrent with its plan to seize positions in the Aleutian Islands.

Yamamoto rushed planning for the Midway and Aleutians missions, while dispatching a force under Vice Admiral Takeo Takagi, including the Fifth Carrier Division (the large, new carriers and ), to support the effort to seize the islands of Tulagi and Guadalcanal for seaplane and aeroplane bases, and the town of Port Moresby on Papua New Guinea's south coast facing Australia.

The Port Moresby (MO) Operation proved an unwelcome setback. Although Tulagi and Guadalcanal were taken, the Port Moresby invasion fleet was compelled to turn back when Takagi clashed with a US carrier task force in the Battle of the Coral Sea in early May. Although the Japanese sank the US carrier and damaged the , the Americans damaged the carrier "Shōkaku" so badly that she required dockyard repairs, and the Japanese lost the light carrier . Just as importantly, Japanese operational mishaps and US fighters and anti-aircraft fire devastated the dive bomber and torpedo plane formations of both "Shōkaku"s and "Zuikaku"s air groups. These losses sidelined "Zuikaku" while she awaited replacement aircraft and aircrews, and saw to tactical integration and training. These two ships would be sorely missed a month later at Midway.

Yamamoto's plan for Midway Island was an extension of his efforts to knock the US Pacific Fleet out of action long enough for Japan to fortify its defensive perimeter in the Pacific island chains. Yamamoto felt it necessary to seek an early, offensive decisive battle.

This plan was long believed to have been to draw American attention—and possibly carrier forces—north from Pearl Harbor by sending his Fifth Fleet (two light carriers, five cruisers, 13 destroyers, and four transports) against the Aleutians, raiding Dutch Harbor on Unalaska Island and invading the more distant islands of Kiska and Attu.

While Fifth Fleet attacked the Aleutians, First Mobile Force (four carriers, two battleships, three cruisers, and 12 destroyers) would raid Midway and destroy its air force. Once this was neutralized, Second Fleet (one light carrier, two battleships, 10 cruisers, 21 destroyers, and 11 transports) would land 5,000 troops to seize the atoll from the US Marines.

The seizure of Midway was expected to draw the US carriers west into a trap where the First Mobile Force would engage and destroy them. Afterwards, First Fleet (one light carrier, seven battleships, three cruisers and 13 destroyers), in conjunction with elements of Second Fleet, would mop up remaining US surface forces and complete the destruction of the US Pacific Fleet.

To guard against failure, Yamamoto initiated two security measures. The first was an aerial reconnaissance mission (Operation K) over Pearl Harbor to ascertain if the US carriers were there. The second was a picket line of submarines to detect the movement of US carriers toward Midway in time for First Mobile Force, First Fleet, and Second Fleet to combine against it. In the event, the first measure was aborted and the second delayed until after US carriers had already sortied.

The plan was a compromise and hastily prepared, apparently so it could be launched in time for the anniversary of Tsushima, but appeared well thought out, well organized, and finely timed when viewed from a Japanese viewpoint. Against four carriers, two light carriers, 11 battleships, 16 cruisers and 46 destroyers likely to be in the area of the main battle the US could field only three carriers, eight cruisers, and 15 destroyers. The disparity appeared crushing. Only in numbers of carrier decks, available aircraft, and submarines was there near parity between the two sides. Despite various mishaps developed in the execution, it appeared that—barring something unforeseen—Yamamoto held all the cards.

Unbeknownst to Admiral Yamamoto, the US had learned of Japanese plans thanks to the code breaking of Japanese naval code D (known to the US as JN-25). As a result, Admiral Chester Nimitz, the Pacific Fleet commander, was able to circumvent both of Yamamoto's security measures and place his outnumbered forces in a position to conduct an ambush. By Nimitz's calculation, his three available carrier decks, plus Midway, gave him rough parity with Nagumo's First Mobile Force.

Following a nuisance raid by Japanese flying boats in May, Nimitz dispatched a minesweeper to guard the intended refueling point for Operation K near French Frigate Shoals, causing the reconnaissance mission to be aborted and leaving Yamamoto ignorant of whether Pacific Fleet carriers were still at Pearl Harbor. It remains unclear why Yamamoto permitted the earlier attack, and why his submarines did not sortie sooner, as reconnaissance was essential to success at Midway. Nimitz also dispatched his carriers toward Midway early, and they passed the intended picket line force of submarines "en route" to their station, negating Yamamoto's back-up security measure. Nimitz's carriers positioned themselves to ambush the "Kidō Butai" (striking force) when it struck Midway. A token cruiser and destroyer force was sent toward the Aleutians, but otherwise Nimitz ignored them. On June 4, 1942, days before Yamamoto expected them to interfere in the Midway operation, US carrier-based aircraft destroyed the four carriers of the "Kidō Butai", catching the Japanese carriers at an especially vulnerable moment.

With his air power destroyed and his forces not yet concentrated for a fleet battle, Yamamoto maneuvered his remaining forces, still strong on paper, to trap the US forces. He was unable to do so because his initial dispositions had placed his surface combatants too far from Midway, and because Admiral Raymond Spruance prudently withdrew to the east in a position to further defend Midway Island, believing (based on a mistaken submarine report) the Japanese still intended to invade. Not knowing several battleships, including the powerful , were on the Japanese order of battle, he did not comprehend the severe risk of a night surface battle, in which his carriers and cruisers would be at a disadvantage. However, his move to the east did avoid the possibility of such a battle taking place. Correctly perceiving he had lost and could not bring surface forces into action, Yamamoto aborted the invasion of Midway and withdrew. The defeat marked the high tide of Japanese expansion.

Yamamoto's plan for Midway Island has been the subject of much criticism. Some historians state it violated the principle of concentration of force, and was overly complex. Others point to similarly complex Allied operations, such as Operation MB8, that were successful, and note the extent to which the US intelligence "coup" derailed the operation before it began. Had Yamamoto's dispositions not denied Nagumo adequate pre-attack reconnaissance assets, both the American cryptanalytic success and the unexpected appearance of the American carriers would have been irrelevant.

The Battle of Midway checked Japanese momentum, but the IJN was still a powerful force, capable of regaining the initiative. It planned to resume the thrust with Operation FS aimed at eventually taking Samoa and Fiji to cut the US lifeline to Australia.

Yamamoto remained in command as commander-in-chief, retained at least partly to avoid diminishing the morale of the Combined Fleet. However, he had lost face as a result of the Midway defeat and the Naval General Staff were disinclined to indulge in further gambles. This reduced Yamamoto to pursuing the classic defensive "decisive battle strategy" he had attempted to overturn.

Yamamoto committed Combined Fleet units to a series of small attrition actions across the south and central Pacific that stung the Americans, but suffered losses he could ill afford in return. Three major efforts to beat the Americans moving on Guadalcanal precipitated a pair of carrier battles that Yamamoto commanded personally: the Eastern Solomons and Santa Cruz Islands in September and October, and finally a wild pair of surface engagements in November, all timed to coincide with Japanese Army pushes. The effort was wasted when the army could not hold up its end of the operation. Yamamoto's naval forces won a few victories and inflicted considerable losses and damage to the US fleet in several naval battles around Guadalcanal which included the battles of Savo Island, Cape Esperance, and Tassafaronga, but he could never draw the US into a decisive fleet action. As a result, Japanese naval strength was reduced.

To boost morale following the defeat at Guadalcanal, Yamamoto decided to make an inspection tour throughout the South Pacific. 

On April 14, 1943, the US naval intelligence effort, code-named "Magic", intercepted and decrypted a message containing specifics of Yamamoto's tour, including arrival and departure times and locations, as well as the number and types of aircraft that would transport and accompany him on the journey. Yamamoto, the itinerary revealed, would be flying from Rabaul to Balalae Airfield, on an island near Bougainville in the Solomon Islands, on the morning of April 18, 1943.

US President Franklin D. Roosevelt may have authorized Secretary of the Navy Frank Knox to "get Yamamoto," however no official record of such an order exists and sources disagree whether he did so. Knox essentially let Admiral Chester W. Nimitz make the decision. Nimitz first consulted Admiral William Halsey Jr., Commander, South Pacific, and then authorized the mission on April 17 to intercept Yamamoto's flight "en route" and shoot it down. A squadron of USAAF Lockheed P-38 Lightning aircraft were assigned the task as only they possessed sufficient range to intercept and engage. Select pilots from three units were informed that they were intercepting an "important high officer" with no specific name given.

On the morning of April 18, despite urging by local commanders to cancel the trip for fear of ambush, Yamamoto's two Mitsubishi G4M bombers, used as fast transport aircraft without bombs, left Rabaul as scheduled for the trip. Sixteen P-38s intercepted the flight over Bougainville and a dogfight ensued between them and the six escorting Mitsubishi A6M Zeroes. First Lieutenant Rex T. Barber engaged the first of the two Japanese transports, which turned out to be "T1-323" (Yamamoto's aircraft). He fired on the aircraft until it began to spew smoke from its left engine. Barber turned away to attack the other transport as Yamamoto's plane crashed into the jungle.

Yamamoto's body, along with the crash site, was found the next day in the jungle of the island of Bougainville by a Japanese search and rescue party, led by army engineer Lieutenant Tsuyoshi Hamasuna. According to Hamasuna, Yamamoto had been thrown clear of the plane's wreckage, his white-gloved hand grasping the hilt of his katana, still upright in his seat under a tree. Hamasuna said Yamamoto was instantly recognizable, head dipped down as if deep in thought. A post-mortem disclosed that Yamamoto had received two 0.50-caliber bullet wounds, one to the back of his left shoulder and another to the left side of his lower jaw that exited above his right eye. The Japanese navy doctor examining the body determined that the head wound killed Yamamoto. The more violent details of Yamamoto's death were hidden from the Japanese public. The medical report was whitewashed, changed "on orders from above", according to biographer Hiroyuki Agawa.

Yamamoto's staff cremated his remains at Buin and his ashes were returned to Tokyo aboard the battleship , Yamamoto's last flagship. Yamamoto was given a full state funeral on June 5, 1943, where he received, posthumously, the title of Marshal Admiral and was awarded the Order of the Chrysanthemum (1st Class). He was also awarded Nazi Germany's Knight's Cross of the Iron Cross with Oak Leaves and Swords. Some of his ashes were buried in the public Tama Cemetery, Tokyo (多摩霊園) and the remainder at his ancestral burial grounds at the temple of Chuko-ji in Nagaoka City. He was succeeded as commander-in-chief of the Combined Fleet by Admiral Mineichi Koga.

Yamamoto practiced calligraphy. He and his wife, Reiko, had four children: two sons and two daughters. Yamamoto was an avid gambler, enjoying "Go", "shogi", billiards, bridge, mah jong, poker, and other games that tested his wits and sharpened his mind. He frequently made jokes about moving to Monaco and starting his own casino. He enjoyed the company of "geisha", and his wife Reiko revealed to the Japanese public in 1954 that Yamamoto was closer to his favorite "geisha" Kawai Chiyoko than to her, which stirred some controversy. His funeral procession passed by Kawai's quarters on the way to the cemetery. The claim that Yamamoto was a Catholic
is likely due to confusion with retired Admiral Shinjiro Stefano Yamamoto, who was a decade older than Isoroku, and died of natural causes in 1942.

"From the Japanese Wikipedia"


Since the end of the Second World War, a number of Japanese and American films have depicted the character of Isoroku Yamamoto.

One of the most notable films is the 1970 movie "Tora! Tora! Tora!", which stars Japanese actor Sô Yamamura as Yamamoto, who states after the attack on Pearl Harbor:

The first film to feature Yamamoto was Toho's 1953 film "", (later released in the United States as "Eagle of the Pacific"), in which Yamamoto was portrayed by Denjirô Ôkôchi.

The 1960 film "The Gallant Hours" depicts the battle of wits between Vice-Admiral William Halsey, Jr. and Yamamoto from the start of the Guadalcanal Campaign in August 1942 to Yamamoto's death in April 1943. The film, however, portrays Yamamoto's death as occurring in November 1942, the day after the Naval Battle of Guadalcanal, and the P-38 aircraft that killed him as coming from Guadalcanal.

In Daiei Studios's 1969 film "Aa, kaigun" (later released in the United States as "Gateway to Glory"), Yamamoto was portrayed by Shôgo Shimada.

Award-winning Japanese actor Toshiro Mifune (star of "The Seven Samurai") portrayed Yamamoto in three films:

In Shūe Matsubayashi's 1981 film "" (lit. "Combined Fleet", later released in the United States as "The Imperial Navy"), Yamamoto was portrayed by Keiju Kobayashi.

In the 2001 film "Pearl Harbor", Yamamoto was portrayed by Oscar-nominated Japanese-born American actor Mako Iwamatsu. Like "Tora! Tora! Tora!", this film also features the sleeping giant quote.

In Toei's 2011 war film "", Yamamoto was portrayed by Kōji Yakusho.

A fictionalized version of Yamamoto's death was portrayed in the "Baa Baa Black Sheep" episode "The Hawk Flies on Sunday", though only photos of Yamamoto were shown. In this episode, set much later in the war than in real life, the Black Sheep, a Marine Corsair squadron, joins an army squadron of P-51 Mustangs. The Marines intercepted fighter cover while the army shot down Yamamoto.

In the 2019 motion picture "Midway", Yamamoto is portrayed by Etsushi Toyokawa. This film also features Admiral Yamamoto speaking aloud the sleeping giant quote.

In the 1993 OVA series "Konpeki no Kantai" (lit. "Deep Blue Fleet"), instead of dying in the plane crash, Yamamoto blacks out and suddenly wakes up as his younger self, Isoroku Takano, after the Battle of Tsushima in 1905. His memory from the original timeline intact, Yamamoto uses his knowledge of the future to help Japan become a stronger military power, eventually launching a "coup d'état" against Hideki Tōjō's government. In the subsequent Pacific War, Japan's technologically advanced navy decisively defeats the United States, and grants all of the former European and American colonies in Asia full independence. Later on, Yamamoto convinces Japan to join forces with the United States and Britain to defeat Nazi Germany.

In the 2004 anime series "Zipang", Yamamoto (voiced by ) works to develop the uneasy partnership with the crew of the JMSDF "Mirai", which has been transported back sixty years through time to the year 1942.

In the Axis of Time trilogy by author John Birmingham, after a naval task force from the year 2021 is accidentally transported back through time to 1942, Yamamoto assumes a leadership role in the dramatic alteration of Japan's war strategy.

In Douglas Niles' 2007 book "MacArthur's War: A Novel of the Invasion of Japan" (written with Michael Dobson), which focuses on General Douglas MacArthur and an alternate history of the Pacific War (following a considerably different outcome of the Battle of Midway), Yamamoto is portrayed sympathetically, with much of the action in the Japanese government seen through his eyes, though he could not change the major decisions of Japan in World War II.

In Robert Conroy's 2011 book "Rising Sun", Yamamoto directs the IJN to launch a series of attacks on the American West Coast, in the hope the United States can be convinced to sue for peace and securing Japan's place as a world power; but cannot escape his lingering fear the war will ultimately doom Japan.

In Neal Stephenson's 1999 book "Cryptonomicon", Yamamoto's final moments are depicted, with him realising that Japan's naval codes have been broken and that he must inform headquarters.

In "The West Wing" episode "We Killed Yamamoto", the Chairman of the Joint Chiefs of Staff uses the assassination of Yamamoto to advocate for another assassination.





</doc>
<doc id="15412" url="https://en.wikipedia.org/wiki?curid=15412" title="Infrared spectroscopy">
Infrared spectroscopy

Infrared spectroscopy (IR spectroscopy or vibrational spectroscopy) is the measurement of the interaction of infrared radiation with matter by absorption, emission, or reflection. It is used to study and identify chemical substances or functional groups in solid, liquid, or gaseous forms. The method or technique of infrared spectroscopy is conducted with an instrument called an infrared spectrometer (or spectrophotometer) which produces an infrared spectrum. An IR spectrum can be visualized in a graph of infrared light absorbance (or transmittance) on the vertical axis vs. frequency or wavelength on the horizontal axis. Typical units of frequency used in IR spectra are reciprocal centimeters (sometimes called wave numbers), with the symbol cm. Units of IR wavelength are commonly given in micrometers (formerly called "microns"), symbol μm, which are related to wave numbers in a reciprocal way. A common laboratory instrument that uses this technique is a Fourier transform infrared (FTIR) spectrometer. Two-dimensional IR is also possible as discussed below.

The infrared portion of the electromagnetic spectrum is usually divided into three regions; the near-, mid- and far- infrared, named for their relation to the visible spectrum. The higher-energy near-IR, approximately 14000–4000 cm (0.7–2.5 μm wavelength) can excite overtone or combination modes of molecular vibrations. The mid-infrared, approximately 4000–400 cm (2.5–25 μm) is generally used to study the fundamental vibrations and associated rotational–vibrational structure. The far-infrared, approximately 400–10 cm (25–1000 μm) has low energy and may be used for rotational spectroscopy and low frequency vibrations. The region from 2–130 cm, bordering the microwave region, is considered the terahertz region and may probe intermolecular vibrations. The names and classifications of these subregions are conventions, and are only loosely based on the relative molecular or electromagnetic properties.

Infrared spectroscopy exploits the fact that molecules absorb frequencies that are characteristic of their structure. These absorptions occur at resonant frequencies, i.e. the frequency of the absorbed radiation matches the vibrational frequency. The energies are affected by the shape of the molecular potential energy surfaces, the masses of the atoms, and the associated vibronic coupling.

In order for a vibrational mode in a sample to be "IR active", it must be associated with changes in the dipole moment. A permanent dipole is not necessary, as the rule requires only a change in dipole moment.

A molecule can vibrate in many ways, and each way is called a "vibrational mode." For molecules with N number of atoms, linear molecules have 3N – 5 degrees of vibrational modes, whereas nonlinear molecules have 3N – 6 degrees of vibrational modes (also called vibrational degrees of freedom). As an example HO, a non-linear molecule, will have 3 × 3 – 6 = 3 degrees of vibrational freedom, or modes.

Simple diatomic molecules have only one bond and only one vibrational band. If the molecule is symmetrical, e.g. N, the band is not observed in the IR spectrum, but only in the Raman spectrum. Asymmetrical diatomic molecules, e.g. CO, absorb in the IR spectrum. More complex molecules have many bonds, and their vibrational spectra are correspondingly more complex, i.e. big molecules have many peaks in their IR spectra.

The atoms in a CHX group, commonly found in organic compounds and where X can represent any other atom, can vibrate in nine different ways. Six of these vibrations involve only the CH portion: symmetric (s) and antisymmetric (as) stretching (ν), scissoring (δ), rocking (ρ), wagging (ω) and twisting (τ), as shown below. Structures that do not have the two additional X groups attached have fewer modes because some modes are defined by specific relationships to those other attached groups. For example, in water, the rocking, wagging, and twisting modes do not exist because these types of motions of the H atoms represent simple rotation of the whole molecule rather than vibrations within it. In case of more complex molecules, out-of-plane (γ) vibrational modes can be also present.

These figures do not represent the "recoil" of the C atoms, which, though necessarily present to balance the overall movements of the molecule, are much smaller than the movements of the lighter H atoms.

The simplest and most important or "fundamental" IR bands arise from the excitations of normal modes, the simplest distortions of the molecule, from the ground state with vibrational quantum number v = 0 to the first excited state with vibrational quantum number v = 1. In some cases, overtone bands are observed. An overtone band arises from the absorption of a photon leading to a direct transition from the ground state to the second excited vibrational state (v = 2). Such a band appears at approximately twice the energy of the fundamental band for the same normal mode. Some excitations, so-called "combination modes", involve simultaneous excitation of more than one normal mode. The phenomenon of Fermi resonance can arise when two modes are similar in energy; Fermi resonance results in an unexpected shift in energy and intensity of the bands etc.

The infrared spectrum of a sample is recorded by passing a beam of infrared light through the sample. When the frequency of the IR is the same as the vibrational frequency of a bond or collection of bonds, absorption occurs. Examination of the transmitted light reveals how much energy was absorbed at each frequency (or wavelength). This measurement can be achieved by scanning the wavelength range using a monochromator. Alternatively, the entire wavelength range is measured using a Fourier transform instrument and then a transmittance or absorbance spectrum is generated using a dedicated procedure.

This technique is commonly used for analyzing samples with covalent bonds. Simple spectra are obtained from samples with few IR active bonds and high levels of purity. More complex molecular structures lead to more absorption bands and more complex spectra.

Gaseous samples require a sample cell with a long pathlength to compensate for the diluteness. The pathlength of the sample cell depends on the concentration of the compound of interest. A simple glass tube with length of 5 to 10 cm equipped with infrared-transparent windows at the both ends of the tube can be used for concentrations down to several hundred ppm. Sample gas concentrations well below ppm can be measured with a White's cell in which the infrared light is guided with mirrors to travel through the gas. White's cells are available with optical pathlength starting from 0.5 m up to hundred meters.

Liquid samples can be sandwiched between two plates of a salt (commonly sodium chloride, or common salt, although a number of other salts such as potassium bromide or calcium fluoride are also used).
The plates are transparent to the infrared light and do not introduce any lines onto the spectra.

Solid samples can be prepared in a variety of ways. One common method is to crush the sample with an oily mulling agent (usually mineral oil Nujol). A thin film of the mull is applied onto salt plates and measured. The second method is to grind a quantity of the sample with a specially purified salt (usually potassium bromide) finely (to remove scattering effects from large crystals). This powder mixture is then pressed in a mechanical press to form a translucent pellet through which the beam of the spectrometer can pass. A third technique is the "cast film" technique, which is used mainly for polymeric materials. The sample is first dissolved in a suitable, non-hygroscopic solvent. A drop of this solution is deposited on surface of KBr or NaCl cell. The solution is then evaporated to dryness and the film formed on the cell is analysed directly. Care is important to ensure that the film is not too thick otherwise light cannot pass through. This technique is suitable for qualitative analysis. The final method is to use microtomy to cut a thin (20–100 μm) film from a solid sample. This is one of the most important ways of analysing failed plastic products for example because the integrity of the solid is preserved.

In photoacoustic spectroscopy the need for sample treatment is minimal. The sample, liquid or solid, is placed into the sample cup which is inserted into the photoacoustic cell which is then sealed for the measurement. The sample may be one solid piece, powder or basically in any form for the measurement. For example, a piece of rock can be inserted into the sample cup and the spectrum measured from it.

It is typical to record spectrum of both the sample and a "reference". This step controls for a number of variables, e.g. infrared detector, which may affect the spectrum. The reference measurement makes it possible to eliminate the instrument influence.

The appropriate "reference" depends on the measurement and its goal. The simplest reference measurement is to simply remove the sample (replacing it by air). However, sometimes a different reference is more useful. For example, if the sample is a dilute solute dissolved in water in a beaker, then a good reference measurement might be to measure pure water in the same beaker. Then the reference measurement would cancel out not only all the instrumental properties (like what light source is used), but also the light-absorbing and light-reflecting properties of the water and beaker, and the final result would just show the properties of the solute (at least approximately).

A common way to compare to a reference is sequentially: first measure the reference, then replace the reference by the sample and measure the sample. This technique is not perfectly reliable; if the infrared lamp is a bit brighter during the reference measurement, then a bit dimmer during the sample measurement, the measurement will be distorted. More elaborate methods, such as a "two-beam" setup (see figure), can correct for these types of effects to give very accurate results. The Standard addition method can be used to statistically cancel these errors.

Nevertheless, among different absorption based techniques which are used for gaseous species detection, Cavity ring-down spectroscopy (CRDS) can be used as a calibration free method. The fact that CRDS is based on the measurements of photon life-times (and not the laser intensity) makes it needless for any calibration and comparison with a reference 

Fourier transform infrared (FTIR) spectroscopy is a measurement technique that allows one to record infrared spectra. Infrared light is guided through an interferometer and then through the sample (or vice versa). A moving mirror inside the apparatus alters the distribution of infrared light that passes through the interferometer. The signal directly recorded, called an "interferogram", represents light output as a function of mirror position. A data-processing technique called Fourier transform turns this raw data into the desired result (the sample's spectrum): Light output as a function of infrared wavelength (or equivalently, wavenumber). As described above, the sample's spectrum is always compared to a reference.

An alternate method for acquiring spectra is the "dispersive" or "scanning monochromator" method. In this approach, the sample is irradiated sequentially with various single wavelengths. The dispersive method is more common in UV-Vis spectroscopy, but is less practical in the infrared than the FTIR method. One reason that FTIR is favored is called "Fellgett's advantage" or the "multiplex advantage": The information at all frequencies is collected simultaneously, improving both speed and signal-to-noise ratio. Another is called "Jacquinot's Throughput Advantage": A dispersive measurement requires detecting much lower light levels than an FTIR measurement. There are other advantages, as well as some disadvantages, but virtually all modern infrared spectrometers are FTIR instruments.

Various forms of infrared microscopy exist. These include IR versions of sub-diffraction microscopy such as IR NSOM, photothermal microspectroscopy, Nano-FTIR and atomic force microscope based infrared spectroscopy (AFM-IR).

Infrared spectroscopy is not the only method of studying molecular vibrational spectra. Raman spectroscopy involves an inelastic scattering process in which only part of the energy of an incident photon is absorbed by the molecule, and the remaining part is scattered and detected. The energy difference corresponds to absorbed vibrational energy.

The selection rules for infrared and for Raman spectroscopy are different at least for some molecular symmetries, so that the two methods are complementary in that they observe vibrations of different symmetries.

Another method is electron energy loss spectroscopy (EELS), in which the energy absorbed is provided by an inelastically scattered electron rather than a photon. This method is useful for studying vibrations of molecules adsorbed on a solid surface. 

Recently, high-resolution EELS (HREELS) has emerged as a technique for performing vibrational spectroscopy in a transmission electron microscope (TEM). In combination with the high spatial resolution of the TEM, unprecedented experiments have been performed, such as nano-scale temperature measurements, mapping of isotopically labeled molecules, mapping of phonon modes in position- and momentum-space, vibrational surface and bulk mode mapping on nanocubes, and investigations of polariton modes in van der Waals crystals.
Analysis of vibrational modes that are IR-inactive but appear in Inelastic Neutron Scattering is also possible at high spatial resolution using EELS. Although the spatial resolution of HREELs is very high, the bands are extremely broad compared to other techniques.

IR spectroscopy is often used to identify structures because functional groups give rise to characteristic bands both in terms of intensity and position (frequency). The positions of these bands are summarized in correlation tables as shown below.

A spectrograph is often interpreted as having two regions.
In the functional region there are one to a few troughs per functional group.
In the fingerprint region there are many troughs which form an intricate pattern which can be used like a fingerprint to determine the compound.

For many kinds of samples, the assignments are known, i.e. which bond deformation(s) are associated with which frequency. In such cases further information can be gleaned about the strength on a bond, relying on the empirical guideline called Badger's Rule. Originally published by Richard Badger in 1934, this rule states that the strength of a bond correlates with the frequency of its vibrational mode. That is, increase in bond strength leads to corresponding frequency increase and vice versa.

Infrared spectroscopy is a simple and reliable technique widely used in both organic and inorganic chemistry, in research and industry. It is used in quality control, dynamic measurement, and monitoring applications such as the long-term unattended measurement of CO concentrations in greenhouses and growth chambers by infrared gas analyzers.

It is also used in forensic analysis in both criminal and civil cases, for example in identifying polymer degradation. It can be used in determining the blood alcohol content of a suspected drunk driver.

IR-spectroscopy has been successfully used in analysis and identification of pigments in paintings and other art objects such as illuminated manuscripts.

A useful way of analyzing solid samples without the need for cutting samples uses ATR or attenuated total reflectance spectroscopy. Using this approach, samples are pressed against the face of a single crystal. The infrared radiation passes through the crystal and only interacts with the sample at the interface between the two materials.

With increasing technology in computer filtering and manipulation of the results, samples in solution can now be measured accurately (water produces a broad absorbance across the range of interest, and thus renders the spectra unreadable without this computer treatment).

Some instruments also automatically identify the substance being measured from a store of thousands of reference spectra held in storage.

Infrared spectroscopy is also useful in measuring the degree of polymerization in polymer manufacture. Changes in the character or quantity of a particular bond are assessed by measuring at a specific frequency over time. Modern research instruments can take infrared measurements across the range of interest as frequently as 32 times a second. This can be done whilst simultaneous measurements are made using other techniques. This makes the observations of chemical reactions and processes quicker and more accurate.

Infrared spectroscopy has also been successfully utilized in the field of semiconductor microelectronics: for example, infrared spectroscopy can be applied to semiconductors like silicon, gallium arsenide, gallium nitride, zinc selenide, amorphous silicon, silicon nitride, etc.

Another important application of Infrared Spectroscopy is in the food industry to measure the concentration of various compounds in different food products

The instruments are now small, and can be transported, even for use in field trials.

Infrared Spectroscopy is also used in gas leak detection devices such as the DP-IR and EyeCGAs. These devices detect hydrocarbon gas leaks in the transportation of natural gas and crude oil.

In February 2014, NASA announced a greatly upgraded database, based on IR spectroscopy, for tracking polycyclic aromatic hydrocarbons (PAHs) in the universe. According to scientists, more than 20% of the carbon in the universe may be associated with PAHs, possible starting materials for the formation of life. PAHs seem to have been formed shortly after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.

Recent developments include a miniature IR-spectrometer that's linked to a cloud based database and suitable for personal everyday use, and NIR-spectroscopic chips that can be embedded in smartphones and various gadgets.

The different isotopes in a particular species may exhibit different fine details in infrared spectroscopy. For example, the O–O stretching frequency (in reciprocal centimeters) of oxyhemocyanin is experimentally determined to be 832 and 788 cm for ν(O–O) and ν(O–O), respectively.

By considering the O–O bond as a spring, the frequency of absorbance can be calculated as a wavenumber [= frequency/(speed of light)]

where "k" is the spring constant for the bond, "c" is the speed of light, and "μ" is the reduced mass of the A–B system:

(formula_5 is the mass of atom formula_6).

The reduced masses for O–O and O–O can be approximated as 8 and 9 respectively. Thus

The effect of isotopes, both on the vibration and the decay dynamics, has been found to be stronger than previously thought. In some systems, such as silicon and germanium, the decay of the anti-symmetric stretch mode of interstitial oxygen involves the symmetric stretch mode with a strong isotope dependence. For example, it was shown that for a natural silicon sample, the lifetime of the anti-symmetric vibration is 11.4 ps. When the isotope of one of the silicon atoms is increased to Si, the lifetime increases to 19 ps. In similar manner, when the silicon atom is changed to Si, the lifetime becomes 27 ps.

Two-dimensional infrared correlation spectroscopy analysis combines multiple samples of infrared spectra to reveal more complex properties. By extending the spectral information of a perturbed sample, spectral analysis is simplified and resolution is enhanced. The 2D synchronous and 2D asynchronous spectra represent a graphical overview of the spectral changes due to a perturbation (such as a changing concentration or changing temperature) as well as the relationship between the spectral changes at two different wavenumbers.

Nonlinear two-dimensional infrared spectroscopy is the infrared version of correlation spectroscopy. Nonlinear two-dimensional infrared spectroscopy is a technique that has become available with the development of femtosecond infrared laser pulses. In this experiment, first a set of pump pulses is applied to the sample. This is followed by a waiting time during which the system is allowed to relax. The typical waiting time lasts from zero to several picoseconds, and the duration can be controlled with a resolution of tens of femtoseconds. A probe pulse is then applied, resulting in the emission of a signal from the sample. The nonlinear two-dimensional infrared spectrum is a two-dimensional correlation plot of the frequency ω that was excited by the initial pump pulses and the frequency ω excited by the probe pulse after the waiting time. This allows the observation of coupling between different vibrational modes; because of its extremely fine time resolution, it can be used to monitor molecular dynamics on a picosecond timescale. It is still a largely unexplored technique and is becoming increasingly popular for fundamental research.

As with two-dimensional nuclear magnetic resonance (2DNMR) spectroscopy, this technique spreads the spectrum in two dimensions and allows for the observation of cross peaks that contain information on the coupling between different modes. In contrast to 2DNMR, nonlinear two-dimensional infrared spectroscopy also involves the excitation to overtones. These excitations result in excited state absorption peaks located below the diagonal and cross peaks. In 2DNMR, two distinct techniques, COSY and NOESY, are frequently used. The cross peaks in the first are related to the scalar coupling, while in the latter they are related to the spin transfer between different nuclei. In nonlinear two-dimensional infrared spectroscopy, analogs have been drawn to these 2DNMR techniques. Nonlinear two-dimensional infrared spectroscopy with zero waiting time corresponds to COSY, and nonlinear two-dimensional infrared spectroscopy with finite waiting time allowing vibrational population transfer corresponds to NOESY. The COSY variant of nonlinear two-dimensional infrared spectroscopy has been used for determination of the secondary structure content of proteins.





</doc>
<doc id="15414" url="https://en.wikipedia.org/wiki?curid=15414" title="Irenaeus">
Irenaeus

Irenaeus (; "Eirēnaios"; c. 130 – c. 202 AD) was a Greek bishop noted for his role in guiding and expanding Christian communities in what is now the south of France and, more widely, for the development of Christian theology by combating heresy and defining orthodoxy. Originating from Smyrna, now Izmir in Turkey, he had seen and heard the preaching of Polycarp, the last known living connection with the Apostles, who in turn was said to have heard John the Evangelist.

Chosen as bishop of Lugdunum, now Lyon, his best-known work is "Against Heresies", often cited as "Adversus Haereses", an attack on gnosticism, in particular that of Valentinus. To counter the doctrines of the gnostic sects claiming secret wisdom, he offered three pillars of orthodoxy: the scriptures, the tradition handed down from the apostles, and the teaching of the apostles' successors. Intrinsic to his writing is that the surest source of Christian guidance is the church of Rome, and he is the earliest surviving witness to regard all four of the now-canonical gospels as essential.

He is recognized as a saint in the Catholic Church, which celebrates his feast on 28 June, and in the Eastern Orthodox Churches, which celebrates the feast on 23 August.

Irenaeus was a Greek from Polycarp's hometown of Smyrna in Asia Minor, now İzmir, Turkey, born during the first half of the 2nd century. The exact date is thought to be between the years 120 and 140. Unlike many of his contemporaries, he was brought up in a Christian family rather than converting as an adult.

During the persecution of Marcus Aurelius, the Roman Emperor from 161–180, Irenaeus was a priest of the Church of Lyon. The clergy of that city, many of whom were suffering imprisonment for the faith, sent him in 177 to Rome with a letter to Pope Eleutherius concerning the heresy of Montanism, and that occasion bore emphatic testimony to his merits. While Irenaeus was in Rome, a persecution took place in Lyon. Returning to Gaul, Irenaeus succeeded the martyr Saint Pothinus and became the second bishop of Lyon.

During the religious peace which followed the persecution of Marcus Aurelius, the new bishop divided his activities between the duties of a pastor and of a missionary (as to which we have but brief data, late and not very certain). Almost all his writings were directed against Gnosticism. The most famous of these writings is "Adversus haereses" ("Against Heresies"). Irenaeus alludes to coming across Gnostic writings, and holding conversations with Gnostics, and this may have taken place in Asia Minor or in Rome. However, it also appears that Gnosticism was present near Lyon: he writes that there were followers of 'Marcus the Magician' living and teaching in the Rhone valley.

Little is known about the career of Irenaeus after he became bishop. The last action reported of him (by Eusebius, 150 years later) is that in 190 or 191, he exerted influence on Pope Victor I not to excommunicate the Christian communities of Asia Minor which persevered in the practice of the Quartodeciman celebration of Easter.

Nothing is known of the date of his death, which must have occurred at the end of the second or the beginning of the third century. He is regarded as a martyr by the Catholic Church and by some within the Orthodox Church. He was buried under the Church of Saint John in Lyon, which was later renamed St Irenaeus in his honour. The tomb and his remains were utterly destroyed in 1562 by the Huguenots.

Irenaeus wrote a number of books, but the most important that survives is the "Against Heresies" (or, in its Latin title, "Adversus haereses"). In Book I, Irenaeus talks about the Valentinian Gnostics and their predecessors, who he says go as far back as the magician Simon Magus. In Book II he attempts to provide proof that Valentinianism contains no merit in terms of its doctrines. In Book III Irenaeus purports to show that these doctrines are false, by providing counter-evidence gleaned from the Gospels. Book IV consists of Jesus' sayings, and here Irenaeus also stresses the unity of the Old Testament and the Gospel. In the final volume, Book V, Irenaeus focuses on more sayings of Jesus plus the letters of Paul the Apostle.

Irenaeus wrote: "One should not seek among others the truth that can be easily gotten from the Church. For in her, as in a rich treasury, the apostles have placed all that pertains to truth, so that everyone can drink this beverage of life. She is the door of life." (Irenaeus of Lyons, "Against Heresies", III.4) But he also said, "Christ came not only for those who believed from the time of Tiberius Caesar, nor did the Father provide only for those who are now, but for absolutely all men from the beginning, who, according to their ability, feared and loved God and lived justly. . . and desired to see Christ and to hear His voice Irenaeus recognized that all who feared and loved God, practiced justice and piety towards their neighbors, and desired to see Christ, insofar as they were able to do so, will be saved. Since many were not able to have an explicit desire to see Christ, but only implicit, it is clear that for Irenaeus this is enough.

The purpose of "Against Heresies" was to refute the teachings of various Gnostic groups; apparently, several Greek merchants had begun an oratorial campaign in Irenaeus' bishopric, teaching that the material world was the accidental creation of an evil god, from which we are to escape by the pursuit of "gnosis". Irenaeus argued that the true gnosis is in fact knowledge of Christ, which redeems rather than escapes from bodily existence.

Until the discovery of the Library of Nag Hammadi in 1945, "Against Heresies" was the best-surviving description of Gnosticism. Some religious scholars have argued the findings at Nag Hammadi have shown Irenaeus' description of Gnosticism to be inaccurate and polemic in nature. However, the general consensus among modern scholars is that Irenaeus was fairly accurate in his transmission of Gnostic beliefs, and that the Nag Hammadi texts have raised no substantial challenges to the overall accuracy of Irenaeus' information. Religious historian Elaine Pagels criticizes Irenaeus for describing Gnostic groups as sexual libertines, for example, when some of their own writings advocated chastity more strongly than did orthodox texts. However, the Nag Hammadi texts do not present a single, coherent picture of any unified Gnostc system of belief, but rather divergent beliefs of multiple Gnostic sects. Some of these sects were indeed libertine because they considered bodily existence meaningless; others praised chastity, and strongly prohibited any sexual activity, even within marriage.

Irenaeus also wrote "The Demonstration of the Apostolic Preaching" (also known as "Proof of the Apostolic Preaching"), an Armenian copy of which was discovered in 1904. This work seems to have been an instruction for recent Christian converts.

Eusebius attests to other works by Irenaeus, today lost, including "On the Ogdoad," an untitled letter to Blastus regarding schism, "On the Subject of Knowledge", "On the Monarchy" or "How God is not the Cause of Evil", "On Easter".

Irenaeus exercised wide influence on the generation which followed. Both Hippolytus and Tertullian freely drew on his writings. However, none of his works aside from "Against Heresies" and "The Demonstration of the Apostolic Preaching" survive today, perhaps because his literal hope of an earthly millennium may have made him uncongenial reading in the Greek East. Even though no complete version of "Against Heresies" in its original Greek exists, we possess the full ancient Latin version, probably of the third century, as well as thirty-three fragments of a Syrian version and a complete Armenian version of books 4 and 5.

Irenaeus' works were first translated into English by John Keble and published in 1872 as part of the Library of the Fathers series.

Irenaeus pointed to the public rule of faith, authoritatively articulated by the preaching of bishops and inculcated in Church practice, especially worship, as an authentic apostolic tradition by which to read Scripture truly against heresies. He classified as Scripture not only the Old Testament but most of the books now known as the New Testament, while excluding many works, a large number by Gnostics, that flourished in the 2nd century and claimed scriptural authority. Oftentimes, Irenaeus, as a student of Polycarp, who was a direct disciple of the Apostle John, believed that he was interpreting scriptures in the same hermeneutic as the Apostles. This connection to Jesus was important to Irenaeus because both he and the Gnostics based their arguments on Scripture. Irenaeus argued that since he could trace his authority to Jesus and the Gnostics could not, his interpretation of Scripture was correct. He also used "the Rule of Faith", a "proto-creed" with similarities to the Apostles' Creed, as a hermeneutical key to argue that his interpretation of Scripture was correct.

Before Irenaeus, Christians differed as to which gospel they preferred. The Christians of Asia Minor preferred the Gospel of John. The Gospel of Matthew was the most popular overall. Irenaeus asserted that four Gospels, Matthew, Mark, Luke, and John, were canonical scripture. Thus Irenaeus provides the earliest witness to the assertion of the four canonical Gospels, possibly in reaction to Marcion's edited version of the Gospel of Luke, which Marcion asserted was the one and only true gospel.

Based on the arguments Irenaeus made in support of only four authentic gospels, some interpreters deduce that the "fourfold Gospel" must have still been a novelty in Irenaeus' time. "Against Heresies" 3.11.7 acknowledges that many heterodox Christians use only one gospel while 3.11.9 acknowledges that some use more than four. The success of Tatian's Diatessaron in about the same time period is "... a powerful indication that the fourfold Gospel contemporaneously sponsored by Irenaeus was not broadly, let alone universally, recognized." (The apologist and ascetic Tatian had previously harmonized the four gospels into a single narrative, the "Diatesseron" circa 150–160)

Irenaeus is also the earliest attestation that the Gospel of John was written by John the Apostle, and that the Gospel of Luke was written by Luke, the companion of Paul.

Scholars contend that Irenaeus quotes from 21 of the 27 New Testament books, such as: 

He may refer to Hebrews 2:30 and James 4:16 and maybe even 2 Peter 5:28, but does not cite Philemon, 3 John or Jude.

Irenaeus cited the New Testament approximately 1,000 times. About one third of his citations are made to Paul's letters. Irenaeus considered all 13 letters belonging to the Pauline corpus to have been written by Paul himself.

Irenaeus is also known as one of the first theologians to use the principle of apostolic succession to refute his opponents.

In his writing against the Gnostics, who claimed to possess a secret oral tradition from Jesus himself, Irenaeus maintained that the bishops in different cities are known as far back as the Apostles and that the bishops provided the only safe guide to the interpretation of Scripture. In a passage that became a "locus classicus" of Catholic-Protestant polemics, he cited the Roman church as an example of the unbroken chain of authority, which text Western polemics would use to assert the primacy of Rome over Eastern churches by virtue of its "preeminent authority".

With the lists of bishops to which Irenaeus referred, the doctrine of the apostolic succession of the bishops, firmly established in the Church at this time, could be linked. This succession was important to establish a chain of custody for orthodoxy. He felt it important, however, also to speak of a succession of elders (presbyters).

Irenaeus' point when refuting the Gnostics was that all of the Apostolic churches had preserved the same traditions and teachings in many independent streams. It was the unanimous agreement between these many independent streams of transmission that proved the orthodox faith, current in those churches, to be true.

The central point of Irenaeus' theology is the unity and the goodness of God, in opposition to the Gnostics' theory of God; a number of divine emanations (Aeons) along with a distinction between the Monad and the Demiurge. Irenaeus uses the Logos theology he inherited from Justin Martyr. Irenaeus was a student of Polycarp, who was said to have been tutored by John the Apostle. (John had used Logos terminology in the Gospel of John and the letter of 1 John). Irenaeus prefers to speak of the Son and the Spirit as the "hands of God".

Irenaeus' emphasis on the unity of God is reflected in his corresponding emphasis on the unity of salvation history. Irenaeus repeatedly insists that God began the world and has been overseeing it ever since this creative act; everything that has happened is part of his plan for humanity. The essence of this plan is a process of maturation: Irenaeus believes that humanity was created immature, and God intended his creatures to take a long time to grow into or assume the divine likeness.

Everything that has happened since has therefore been planned by God to help humanity overcome this initial mishap and achieve spiritual maturity. The world has been intentionally designed by God as a difficult place, where human beings are forced to make moral decisions, as only in this way can they mature as moral agents. Irenaeus likens death to the big fish that swallowed Jonah: it was only in the depths of the whale's belly that Jonah could turn to God and act according to the divine will. Similarly, death and suffering appear as evils, but without them we could never come to know God.

According to Irenaeus, the high point in salvation history is the advent of Jesus. For Irenaeus, the Incarnation of Christ was intended by God before he determined that humanity would be created. Irenaeus develops this idea based on Rom. 5:14, saying "Forinasmuch as He had a pre-existence as a saving Being, it was necessary that what might be saved should also be called into existence, in order that the Being who saves should not exist in vain." Some theologians maintain that Irenaeus believed that Incarnation would have occurred even if humanity had never sinned; but the fact that they did sin determined his role as the savior.

Irenaeus sees Christ as the new Adam, who systematically "undoes" what Adam did: thus, where Adam was disobedient concerning God's edict concerning the fruit of the Tree of Knowledge of Good and Evil, Christ was obedient even to death on the wood of a tree. Irenaeus is the first to draw comparisons between Eve and Mary, contrasting the faithlessness of the former with the faithfulness of the latter. In addition to reversing the wrongs done by Adam, Irenaeus thinks of Christ as "recapitulating" or "summing up" human life.

Irenaeus conceives of our salvation as essentially coming about through the incarnation of God as a man. He characterizes the penalty for sin as death and corruption. God, however, is immortal and incorruptible, and simply by becoming united to human nature in Christ he conveys those qualities to us: they spread, as it were, like a benign infection. Irenaeus emphasizes that salvation occurs through Christ's Incarnation, which bestows incorruptibility on humanity, rather than emphasizing His Redemptive death in the crucifixion, although the latter event is an integral part of the former.

Part of the process of recapitulation is for Christ to go through every stage of human life, from infancy to old age, and simply by living it, sanctify it with his divinity. Although it is sometimes claimed that Irenaeus believed Christ did not die until he was older than is conventionally portrayed, the bishop of Lyon simply pointed out that because Jesus turned the permissible age for becoming a rabbi (30 years old and above), he recapitulated and sanctified the period between 30 and 50 years old, as per the Jewish custom of periodization on life, and so touches the beginning of old age when one becomes 50 years old. (see Adversus Haereses, book II, chapter 22).

In the passage of "Adversus Haereses" under consideration, Irenaeus is clear that after receiving baptism at the age of thirty, citing Luke 3:23, Gnostics then falsely assert that "He [Jesus] preached only one year reckoning from His baptism," and also, "On completing His thirtieth year He [Jesus] suffered, being in fact still a young man, and who had by no means attained to advanced age." Irenaeus argues against the Gnostics by using scripture to add several years after his baptism by referencing 3 distinctly separate visits to Jerusalem. The first is when Jesus makes wine out of water, he goes up to the Paschal feast-day, after which he withdraws and is found in Samaria. The second is when Jesus goes up to Jerusalem for Passover and cures the paralytic, after which he withdraws over the sea of Tiberias. The third mention is when he travels to Jerusalem, eats the Passover, and suffers on the following day.

Irenaeus quotes scripture, which we reference as John 8:57, to suggest that Jesus ministers while in his 40s. In this passage, Jesus' opponents want to argue that Jesus has not seen Abraham, because Jesus is too young. Jesus' opponents argue that Jesus is not yet 50 years old. Irenaeus argues that if Jesus was in his thirties, his opponents would've argued that He's not yet 40 years, since that would make Him even younger. Irenaeus' argument is that they would not weaken their own argument by adding years to Jesus' age. Irenaeus also writes that "The Elders witness to this, who in Asia conferred with John the Lord's disciple, to the effect that John had delivered these things unto them: for he abode with them until the times of Trajan. And some of them saw not only John, but others also of the Apostles, and had this same account from them, and witness to the aforesaid relation."

In Demonstration (74) Irenaeus notes "For Pontius Pilate was governor of Judæa, and he had at that time resentful enmity against Herod the king of the Jews. But then, when Christ was brought to him bound, Pilate sent Him to Herod, giving command to enquire of him, that he might know of a certainty what he should desire concerning Him; making Christ a convenient occasion of reconciliation with the king." Pilate was the prefect of the Roman province of Judaea from AD 26–36. He served under Emperor Tiberius Claudius Nero. Herod Antipas was tetrarch of Galilee and Perea, a client state of the Roman Empire. He ruled from 4 BC to 39 AD. In refuting Gnostic claims that Jesus preached for only one year after his baptism, Irenaeus used the "recapitulation" approach to demonstrate that by living beyond the age of thirty Christ sanctified even old age.

Many aspects of Irenaeus' presentation of salvation history depend on Paul's Epistles.

Irenaeus’ conception of salvation relies heavily on the understanding found in Paul's letters. Irenaeus first brings up the theme of victory over sin and evil that is afforded by Jesus's death. God's intervention has saved humanity from the Fall of Adam and the wickedness of Satan. Human nature has become joined with God's in the person of Jesus, thus allowing human nature to have victory over sin. Paul writes on the same theme, that Christ has come so that a new order is formed, and being under the Law, is being under the sin of Adam Rom. 6:14, Gal. 5:18.

Reconciliation is also a theme of Paul's that Irenaeus stresses in his teachings on Salvation. Irenaeus believes Jesus coming in flesh and blood sanctified humanity so that it might again reflect the perfection associated with the likeness of the Divine. This perfection leads to a new life, in the lineage of God, which is forever striving for eternal life and unity with the Father. This is a carryover from Paul, who attributes this reconciliation to the actions of Christ: "For since death came through a human being, the resurrection of the dead has also come through a human being; for as all die in Adam, so all will be made alive in Christ" 1 Cor. 15:21–22.

A third theme in both Paul's and Irenaeus's conceptions of salvation is the sacrifice of Christ being necessary for the new life given to humanity in the triumph over evil. It is in this obedient sacrifice that Jesus is victor and reconciler, thus erasing the marks that Adam left on human nature. To argue against the Gnostics on this point, Irenaeus uses Colossians Col. 2:13–4 in showing that the debt which came by a tree has been paid for us in another tree. Furthermore, the first chapter of Ephesians is picked up in Irenaeus's discussion of the topic when he asserts, "By His own blood He redeemed us, as also His apostle declares, 'In whom we have redemption through His blood, even the remission of sins.'"

Irenaeus does not simply parrot back the message of Paul in his understanding of salvation. One of the major changes that Irenaeus makes is when the Parousia will occur. Paul states that he believes that it was going to happen soon, probably in his own lifetime 1 Thess. 4:15 1 Cor. 15:51–52. However, the end times does not happen immediately and Christians begin to worry and have doubts about the faith. For Irenaeus, sin is seen as haste, just as Adam and Eve quickly ate from the tree of knowledge as they pleased. On the other hand, redemption restored to humanity through the Christ's submission to God's will. Thus, the salvation of man will also be restored to the original trajectory controlled by God forfeited in humanity's sinful in haste. This rather slower version of salvation is not something that Irenaeus received from Paul, but was a necessary construct given the delay of the second coming of Jesus.

The frequencies of quotations and allusions to the Pauline Epistles in "Against Heresies" are:
To counter his Gnostic opponents, Irenaeus significantly develops Paul's presentation of Christ as the Last Adam.

Irenaeus' presentation of Christ as the New Adam is based on Paul's Christ-Adam parallel in Romans 5:12–21. Irenaeus uses this parallel to demonstrate that Christ truly took human flesh. Irenaeus considered it important to emphasize this point because he understands the failure to recognize Christ's full humanity the bond linking the various strains of Gnosticism together, as seen in his statement that "according to the opinion of no one of the heretics was the Word of God made flesh." Irenaeus believes that unless the Word became flesh, humans were not fully redeemed. He explains that by becoming man, Christ restored humanity to being in the image and likeness of God, which they had lost in the Fall of man. Just as Adam was the original head of humanity through whom all sinned, Christ is the new head of humanity who fulfills Adam's role in the Economy of Salvation. Irenaeus calls this process of restoring humanity recapitulation.

For Irenaeus, Paul's presentation of the Old Law (the Mosaic covenant) in this passage indicates that the Old Law revealed humanity's sinfulness but could not save them. He explains that "For as the law was spiritual, it merely made sin to stand out in relief, but did not destroy it. For sin had no dominion over the spirit, but over man." Since humans have a physical nature, they cannot be saved by a spiritual law. Instead, they need a human Savior. This is why it was necessary for Christ to take human flesh. Irenaeus summarizes how Christ's taking human flesh saves humanity with a statement that closely resembles Romans 5:19, "For as by the disobedience of the one man who was originally moulded from virgin soil, the many were made sinners, and forfeited life; so was it necessary that, by the obedience of one man, who was originally born from a virgin, many should be justified and receive salvation." The physical creation of Adam and Christ is emphasized by Irenaeus to demonstrate how the Incarnation saves humanity's physical nature.

Irenaeus emphasizes the importance of Christ's reversal of Adam's action. Through His obedience, Christ undoes Adam's disobedience. Irenaeus presents the Passion as the climax of Christ's obedience, emphasizing how this obedience on the tree of the Cross Phil. 2:8 undoes the disobedience that occurred through a tree Gen. 3:17.
Irenaeus' interpretation of Paul's discussion of Christ as the New Adam is significant because it helped develop the recapitulation theory of atonement. Irenaeus emphasizes that it is through Christ's reversal of Adam's action that humanity is saved, rather than considering the Redemption to occur in a cultic or juridical way.

The biblical passage, "Death has been swallowed up in victory" (), implied for Irenaeus that the Lord will surely resurrect the first human, i.e. Adam, as one of the saved. According to Irenaeus, those who deny Adam's salvation are “shutting themselves out from life for ever” and the first one who did so was Tatian. The notion that the Second Adam saved the first Adam was advocated not only by Irenaeus, but also by Gregory Thaumaturgus, which suggests that it was popular in the Early Church.

Valentinian Gnosticism was one of the major forms of Gnosticism that Irenaeus opposed.

According to the Gnostic view of Salvation, creation was perfect to begin with; it did not need time to grow and mature. For the Valentinians, the material world is the result of the loss of perfection which resulted from Sophia's desire to understand the Forefather. Therefore, one is ultimately redeemed, through secret knowledge, to enter the pleroma of which the Achamoth originally fell.

According to the Valentinian Gnostics, there are three classes of human beings. They are the material, who cannot attain salvation; the psychic, who are strengthened by works and faith (they are part of the church); and the spiritual, who cannot decay or be harmed by material actions.
Essentially, ordinary humans—those who have faith but do not possess the special knowledge—will not attain salvation. Spirituals, on the other hand—those who obtain this great gift—are the only class that will eventually attain salvation.

In his article entitled "The Demiurge", J.P. Arendzen sums up the Valentinian view of the salvation of man. He writes, "The first, or carnal men, will return to the grossness of matter and finally be consumed by fire; the second, or psychic men, together with the Demiurge as their master, will enter a middle state, neither heaven (pleroma) nor hell (whyle); the purely spiritual men will be completely freed from the influence of the Demiurge and together with the Saviour and Achamoth, his spouse, will enter the pleroma divested of body (húle) and soul (psuché)."

In this understanding of salvation, the purpose of the Incarnation was to redeem the Spirituals from their material bodies. By taking a material body, the Son becomes the Savior and facilitates this entrance into the pleroma by making it possible for the Spirituals to receive his spiritual body. However, in becoming a body and soul, the Son Himself becomes one of those needing redemption. Therefore, the Word descends onto the Savior at His Baptism in the Jordan, which liberates the Son from his corruptible body and soul. His redemption from the body and soul is then applied to the Spirituals. In response to this Gnostic view of Christ, Irenaeus emphasized that the Word became flesh and developed a soteriology that emphasized the significance of Christ's material Body in saving humanity, as discussed in the sections above.

In his criticism of Gnosticism, Irenaeus made reference to a Gnostic gospel which portrayed Judas in a positive light, as having acted in accordance with Jesus' instructions. The recently discovered Gospel of Judas dates close to the period when Irenaeus lived (late 2nd century), and scholars typically regard this work as one of many Gnostic texts, showing one of many varieties of Gnostic beliefs of the period.

The first four books of "Against Heresies" constitute a minute analysis and refutation of the Gnostic doctrines. The fifth is a statement of positive belief contrasting the constantly shifting and contradictory Gnostic opinions with the steadfast faith of the church. He appeals to the Biblical prophecies to demonstrate the truthfulness of Christianity.

Irenaeus showed a close relationship between the predicted events of Daniel 2 and 7. Rome, the fourth prophetic kingdom, would end in a tenfold partition. The ten divisions of the empire are the "ten horns" of Daniel 7 and the "ten horns" in Revelation 17. A "little horn," which was to supplant three of Rome's ten divisions, was also the still future "eighth" in Revelation. Irenaeus concluded with the destruction of all kingdoms at the Second Advent, when Christ, the prophesied "stone," cut out of the mountain without hands, smote the image after Rome's division.

Irenaeus identified the Antichrist, another name of the apostate Man of Sin, with Daniel's Little Horn and John's Beast of Revelation 13. He sought to apply other expressions to the Antichrist, such as "the abomination of desolation," mentioned by Christ (Matt. 24:15) and the "king of a most fierce countenance," in Gabriel's explanation of the Little Horn of Daniel 8. But he is not very clear how "the sacrifice and the libation shall be taken away" during the "half-week," or three and one-half years of the Antichrist's reign.

Under the notion that the Antichrist, as a single individual, might be of Jewish origin, he fancies that the mention of "Dan," in Jeremiah 8:16, and the omission of that name from those tribes listed in Revelation 7, might indicate the Antichrist's tribe. This surmise became the foundation of a series of subsequent interpretations by other students of Bible prophecy.

Like the other early church fathers, Irenaeus interpreted the three and one-half "times" of the Little Horn of Daniel 7 as three and one-half literal years. Antichrist's three and a half years of sitting in the temple are placed immediately before the Second Coming of Christ. They are identified as the second half of the "one week" of Daniel 9. Irenaeus says nothing of the seventy weeks; we do not know whether he placed the "one week" at the end of the seventy or whether he had a gap.

Irenaeus is the first of the church fathers to consider the mystic number 666. While Irenaeus did propose some solutions of this numerical riddle, his interpretation was quite reserved. Thus, he cautiously states:

Although Irenaeus did speculate upon three names to symbolize this mystical number, namely Euanthas, Teitan, and Lateinos, nevertheless he was content to believe that the Antichrist would arise some time in the future after the fall of Rome and then the meaning of the number would be revealed.

Irenaeus declares that the Antichrist's future three-and-a-half-year reign, when he sits in the temple at Jerusalem, will be terminated by the second advent, with the resurrection of the just, the destruction of the wicked, and the millennial reign of the righteous. The general resurrection and the judgment follow the descent of the New Jerusalem at the end of the millennial kingdom.

Irenaeus calls those "heretics" who maintain that the saved are immediately glorified in the kingdom to come after death, before their resurrection. He avers that the millennial kingdom and the resurrection are actualities, not allegories, the first resurrection introducing this promised kingdom in which the risen saints are described as ruling over the renewed earth during the millennium, between the two resurrections.

Irenaeus held to the old Jewish tradition that the first six days of creation week were typical of the first six thousand years of human history, with Antichrist manifesting himself in the sixth period. And he expected the millennial kingdom to begin with the second coming of Christ to destroy the wicked and inaugurate, for the righteous, the reign of the kingdom of God during the seventh thousand years, the millennial Sabbath, as signified by the Sabbath of creation week.

In common with many of the fathers, Irenaeus did not distinguish between the new earth re-created in its eternal state—the thousand years of Revelation 20—when the saints are with Christ after His second advent, and the Jewish traditions of the Messianic kingdom. Hence, he applies Biblical and traditional ideas to his descriptions of this earth during the millennium, throughout the closing chapters of Book 5. This conception of the reign of resurrected and translated saints with Christ on this earth during the millennium-popularly known as chiliasm—was the increasingly prevailing belief of this time. Incipient distortions due to the admixture of current traditions, which figure in the extreme forms of chiliasm, caused a reaction against the earlier interpretations of Bible prophecies.

Irenaeus was not looking for a Jewish kingdom. He interpreted Israel as the Christian church, the spiritual seed of Abraham.

At times his expressions are highly fanciful. He tells, for instance, of a prodigious fertility of this earth during the millennium, after the resurrection of the righteous, "when also the creation, having been renovated and set free, shall fructify with an abundance of all kinds of food." In this connection, he attributes to Christ the saying about the vine with ten thousand branches, and the ear of wheat with ten thousand grains, and so forth, which he quotes from Papias of Hierapolis.

Often Irenaeus is grouped with other early church fathers as teaching historic premillennialism which maintain a belief in the earthly reign of Christ but differ from dispensational premillennialism in their view of the rapture as to when the translation of saints occurs. In Against Heresies (V.XXIX.1) he says "And therefore, when in the end the Church shall be suddenly caught up from this, it is said, 'There shall be tribulation such as has not been since the beginning, neither shall be.'"

Irenaeus' exegesis does not give complete coverage. On the seals, for example, he merely alludes to Christ as the rider on the white horse. He stresses five factors with greater clarity and emphasis than Justin:







</doc>
<doc id="15416" url="https://en.wikipedia.org/wiki?curid=15416" title="Involuntary commitment">
Involuntary commitment

Involuntary commitment or civil commitment (also known informally as sectioning or being sectioned in some jurisdictions, such as the United Kingdom) is a legal process through which an individual who is deemed by a qualified agent to have symptoms of severe mental disorder is detained in a psychiatric hospital (inpatient) where they can be treated involuntarily. This treatment may involve the administration of psychoactive drugs including involuntary administration. In many jurisdictions, people diagnosed with mental health disorders can also be forced to undergo treatment while in the community, this is sometimes referred to as outpatient commitment and shares legal processes with commitment. 

Criteria for civil commitment are established by laws which vary between nations. Commitment proceedings often follow a period of emergency hospitalization, during which an individual with acute psychiatric symptoms is confined for a relatively short duration (e.g. 72 hours) in a treatment facility for evaluation and stabilization by mental health professionals who may then determine whether further civil commitment is appropriate or necessary. Civil commitment procedures may take place in a court or only involve physicians. If commitment does not involve a court there is normally an appeal process that does involve the judiciary in some capacity, though potentially through a specialist court. 

Historically, until the mid-1960s in most jurisdictions in the United States, all committals to public psychiatric facilities and most committals to private ones were involuntary. Since then, there have been alternating trends towards the abolition or substantial reduction of involuntary commitment, a trend known as "deinstitutionalisation".

In most jurisdictions, involuntary commitment is applied to individuals believed to be experiencing a mental illness that impairs their ability to reason to such an extent that the agents of the law, state, or courts determine that decisions will be made for the individual under a legal framework. In some jurisdictions, this is a proceeding distinct from being found incompetent.

Involuntary commitment is used in some degree for each of the following although different jurisdictions have different criteria. Some jurisdictions limit involuntary treatment to individuals who meet statutory criteria for presenting a danger to self or others. Other jurisdictions have broader criteria.

The legal process by which commitment takes place varies between jurisdictions. Some jurisdictions have a formal court hearing where testimony and other evidence may also be submitted where subject of the hearing is typically entitled to legal counsel and may challenge a commitment order through habeas corpus. Other jurisdictions have delegated these power to physicians though may provide an appeal process that involves the judiciary but may also involve physicians.

Training is gradually becoming available in mental health first aid to equip community members such as teachers, school administrators, police officers, and medical workers with training in recognizing, and authority in managing, situations where involuntary evaluations of behavior are applicable under law. The extension of first aid training to cover mental health problems and crises is a quite recent development. A mental health first aid training course was developed in Australia in 2001 and has been found to improve assistance provided to persons with an alleged mental illness or mental health crisis. This form of training has now spread to a number of other countries (Canada, Finland, Hong Kong, Ireland, Singapore, Scotland, England, Wales, and the United States). Mental health triage may be used in an emergency room to make a determination about potential risk and apply treatment protocols.

Observation is sometimes used to determine whether a person warrants involuntary commitment. It is not always clear on a relatively brief examination whether a person is psychotic or otherwise warrants commitment.

Austria, Belgium, Germany, Israel, the Netherlands, Northern Ireland, Russia, Taiwan, Ontario (Canada), and the United States have adopted commitment criteria based on the presumed danger of the defendant to self or to others. 

People with suicidal thoughts may act on these impulses and harm or kill themselves. 

People with psychosis are occasionally driven by their delusions or hallucinations to harm themselves or others. Research has found that those who suffer from schizophrenia are between 3.4 and 7.4 times more likely to engage in violent behaviour than members of the general public. 

People with certain types of personality disorders can occasionally present a danger to themselves or others.

This concern has found expression in the standards for involuntary commitment in every US state and in other countries as the danger to self or others standard, sometimes supplemented by the requirement that the danger be imminent. In some jurisdictions, the danger to self or others standard has been broadened in recent years to include need-for-treatment criteria such as "gravely disabled".

Starting in the 1960s, there has been a worldwide trend toward moving psychiatric patients from hospital settings to less restricting settings in the community, a shift known as "deinstitutionalization". Because the shift was typically not accompanied by a commensurate development of community-based services, critics say that deinstitutionalization has led to large numbers of people who would once have been inpatients as instead being incarcerated or becoming homeless. In some jurisdictions, laws authorizing court-ordered outpatient treatment have been passed in an effort to compel individuals with chronic, untreated severe mental illness to take psychiatric medication while living outside the hospital (e.g. Laura's Law, Kendra's Law).

Before the 1960s deinstitutionalization there were earlier efforts to free psychiatric patients. Philippe Pinel (1745–1826) ordered the removal of chains from patients.

In a study of 269 patients from Vermont State Hospital done by Courtenay M. Harding and associates, about two-thirds of the ex-patients did well after deinstitutionalization.

United Nations General Assembly Resolution 46/119, "Principles for the Protection of Persons with Mental Illness and the Improvement of Mental Health Care", is a non-binding resolution advocating certain broadly drawn procedures for the carrying out of involuntary commitment. These principles have been used in many countries where local laws have been revised or new ones implemented. The UN runs programs in some countries to assist in this process.

Mental health professionals have, on occasion, wrongfully deemed individuals to have symptoms of a mental disorder, and thereby commit the individual for treatment in a psychiatric hospital. Claims of wrongful commitment are a common theme in the anti-psychiatry movement.

In 1860, the case of Elizabeth Packard, who was wrongfully committed that year and filed a lawsuit and won thereafter, highlighted the issue of wrongful involuntary commitment. In 1887, investigative journalist Nellie Bly went undercover at an asylum in New York City to expose the terrible conditions that mental patients at the time had to deal with. She published her findings and experiences as articles in "New York World", and later made the articles into one book called "Ten Days in a Mad-House".

In the first half of the twentieth century there were a few high-profile cases of wrongful commitment based on racism or punishment for political dissenters. In the former Soviet Union, psychiatric hospitals were used as prisons to isolate political prisoners from the rest of society. British playwright Tom Stoppard wrote "Every Good Boy Deserves Favour" about the relationship between a patient and his doctor in one of these hospitals. Stoppard was inspired by a meeting with a Russian exile. In 1927, after the execution of Sacco and Vanzetti in the United States, demonstrator Aurora D'Angelo was sent to a mental health facility for psychiatric evaluation after she participated in a rally in support of the anarchists. Throughout the 1940s and 1950s in Canada, 20,000 Canadian children, called the Duplessis orphans, were wrongfully certified as being mentally ill and as a result were wrongfully committed to psychiatric institutions where they were forced to take psychiatric medication that they did not need, and were abused. They were named after Maurice Duplessis, the premier of Quebec at the time, who deliberately committed these children to in order to misappropriate additional subsidies from the federal government. Decades later in the 1990s, several of the orphans sued Quebec and the Catholic Church for the abuse and wrongdoing. In 1958, black pastor and activist Clennon Washington King Jr. tried enrolling at the University of Mississippi, which at the time was white, for summer classes; the local police secretly arrested and involuntarily committed him to a mental hospital for 12 days.

Patients are able to sue if they believe that they have been wrongfully committed.



</doc>
<doc id="15417" url="https://en.wikipedia.org/wiki?curid=15417" title="Intermolecular force">
Intermolecular force

Intermolecular forces (IMF) are the forces which mediate interaction between molecules, including forces of attraction or repulsion which act between molecules and other types of neighboring particles, e.g. atoms or ions. Intermolecular forces are weak relative to intramolecular forces – the forces which hold a molecule together. For example, the covalent bond, involving sharing electron pairs between atoms, is much stronger than the forces present between neighboring molecules. Both sets of forces are essential parts of force fields frequently used in molecular mechanics.

The investigation of intermolecular forces starts from macroscopic observations which indicate the existence and action of forces at a molecular level. These observations include non-ideal-gas thermodynamic behavior reflected by virial coefficients, vapor pressure, viscosity, superficial tension, and absorption data.

The first reference to the nature of microscopic forces is found in Alexis Clairaut's work "Theorie de la Figure de la Terre". Other scientists who have contributed to the investigation of microscopic forces include: Laplace, Gauss, Maxwell and Boltzmann.

Attractive intermolecular forces are categorized into the following types:

Information on intermolecular forces is obtained by macroscopic measurements of properties like viscosity, pressure, volume, temperature (PVT) data. The link to microscopic aspects is given by virial coefficients and Lennard-Jones potentials.

A "hydrogen bond" is the attraction between the lone pair of an electronegative atom and a hydrogen atom that is bonded to an electronegative atom, usually nitrogen, oxygen, or fluorine. The hydrogen bond is often described as a strong electrostatic dipole–dipole interaction. However, it also has some features of covalent bonding: it is directional, stronger than a van der Waals force interaction, produces interatomic distances shorter than the sum of their van der Waals radii, and usually involves a limited number of interaction partners, which can be interpreted as a kind of valence. The number of Hydrogen bonds formed between molecules is equal to the number of active pairs. The molecule which donates its hydrogen is termed the donor molecule, while the molecule containing lone pair participating in H bonding is termed the acceptor molecule. The number of active pairs is equal to the common number between number of hydrogens the donor has and the number of lone pairs the acceptor has.

Though both not depicted in the diagram, water molecules have two active pairs, as the oxygen atom can interact with two hydrogens to form two hydrogen bonds. Intermolecular hydrogen bonding is responsible for the high boiling point of water (100 °C) compared to the other group 16 hydrides, which have little capability to hydrogen bond. Intramolecular hydrogen bonding is partly responsible for the secondary, tertiary, and quaternary structures of proteins and nucleic acids. It also plays an important role in the structure of polymers, both synthetic and natural.

The attraction between cationic and anionic sites is a noncovalent, or intermolecular interaction which is usually referred to as ion pairing or salt bridge.
It is essentially due to electrostatic forces, although in aqueous medium the association is driven by entropy and often even endothermic. Most salts form crystals with characteristic distances between the ions; in contrast to many other noncovalent interactions salt bridges are not directional and show in the solid state usually contact determined only by the van der Waals radii of the ions.
Inorganic as well as organic ions display in water at moderate ionic strength I similar salt bridge as association ΔG values around 5 to 6 kJ/mol for a 1:1 combination of anion and cation, almost independent of the nature (size, polarizability etc) of the ions. The ΔG values are additive and approximately a linear function of the charges, the interaction of e.g. a doubly charged phosphate anion with a single charged ammonium cation accounts for about 2x5 = 10 kJ/mol. The ΔG values depend on the ionic strength I of the solution, as described by the Debye-Hückel equation, at zero ionic strength one observes ΔG = 8 kJ/mol.

Dipole–dipole interactions are electrostatic interactions between molecules which have permanent dipoles.This interaction is stronger than the London forces but is weaker than ion-ion interaction because only partial charges are involved. These interactions tend to align the molecules to increase attraction (reducing potential energy). An example of a dipole–dipole interaction can be seen in hydrogen chloride (HCl): the positive end of a polar molecule will attract the negative end of the other molecule and influence its position. Polar molecules have a net attraction between them. Examples of polar molecules include hydrogen chloride (HCl) and chloroform (CHCl).

Often molecules contain dipolar groups of atoms, but have no overall dipole moment on the molecule as a whole. This occurs if there is symmetry within the molecule that causes the dipoles to cancel each other out. This occurs in molecules such as tetrachloromethane and carbon dioxide. The dipole–dipole interaction between two individual atoms is usually zero, since atoms rarely carry a permanent dipole. These forces are discussed further in the section about the Keesom interaction, below.

Ion–dipole and ion–induced dipole forces are similar to dipole–dipole and dipole–induced dipole interactions but involve ions, instead of only polar and non-polar molecules. Ion–dipole and ion–induced dipole forces are stronger than dipole–dipole interactions because the charge of any ion is much greater than the charge of a dipole moment. Ion–dipole bonding is stronger than hydrogen bonding.

An ion–dipole force consists of an ion and a polar molecule interacting. They align so that the positive and negative groups are next to one another, allowing maximum attraction. An important example of this interaction is hydration of ions in water which give rise to hydration enthalpy. The polar water molecules surround themselves around ions in water and the energy released during the process is known as hydration enthalpy. The interaction has its immense importance in justifying the stability of various ions (like Cu) in water.

An ion–induced dipole force consists of an ion and a non-polar molecule interacting. Like a dipole–induced dipole force, the charge of the ion causes distortion of the electron cloud on the non-polar molecule.

The van der Waals forces arise from interaction between uncharged atoms or molecules, leading not only to such phenomena as the cohesion of condensed phases and physical absorption of gases, but also to a universal force of attraction between macroscopic bodies.

The first contribution to van der Waals forces is due to electrostatic interactions between charges (in molecular ions), dipoles (for polar molecules), quadrupoles (all molecules with symmetry lower than cubic), and permanent multipoles. It is termed the "Keesom interaction", named after Willem Hendrik Keesom. These forces originate from the attraction between permanent dipoles (dipolar molecules) and are temperature dependent.

They consist of attractive interactions between dipoles that are ensemble averaged over different rotational orientations of the dipoles. It is assumed that the molecules are constantly rotating and never get locked into place. This is a good assumption, but at some point molecules do get locked into place. The energy of a Keesom interaction depends on the inverse sixth power of the distance, unlike the interaction energy of two spatially fixed dipoles, which depends on the inverse third power of the distance. The Keesom interaction can only occur among molecules that possess permanent dipole moments, i.e., two polar molecules. Also Keesom interactions are very weak van der Waals interactions and do not occur in aqueous solutions that contain electrolytes. The angle averaged interaction is given by the following equation:

where "m" = dipole moment, formula_2 = permitivity of free space, formula_3 = dielectric constant of surrounding material, "T" = temperature, formula_4 = Boltzmann constant, and "r" = distance between molecules.

The second contribution is the induction (also termed polarization) or Debye force, arising from interactions between rotating permanent dipoles and from the polarizability of atoms and molecules (induced dipoles). These induced dipoles occur when one molecule with a permanent dipole repels another molecule's electrons. A molecule with permanent dipole can induce a dipole in a similar neighboring molecule and cause mutual attraction. Debye forces cannot occur between atoms. The forces between induced and permanent dipoles are not as temperature dependent as Keesom interactions because the induced dipole is free to shift and rotate around the polar molecule. The Debye induction effects and Keesom orientation effects are termed polar interactions.

The induced dipole forces appear from the induction (also termed polarization), which is the attractive interaction between a permanent multipole on one molecule with an induced (by the former di/multi-pole) 31 on another. This interaction is called the "Debye force", named after Peter J. W. Debye.

One example of an induction interaction between permanent dipole and induced dipole is the interaction between HCl and Ar. In this system, Ar experiences a dipole as its electrons are attracted (to the H side of HCl) or repelled (from the Cl side) by HCl. The angle averaged interaction is given by the following equation:

where formula_6 = polarizability.

This kind of interaction can be expected between any polar molecule and non-polar/symmetrical molecule. The induction-interaction force is far weaker than dipole–dipole interaction, but stronger than the London dispersion force.

The third and dominant contribution is the dispersion or London force (fluctuating dipole–induced dipole), which arises due to the non-zero instantaneous dipole moments of all atoms and molecules. Such polarization can be induced either by a polar molecule or by the repulsion of negatively charged electron clouds in non-polar molecules. Thus, London interactions are caused by random fluctuations of electron density in an electron cloud. An atom with a large number of electrons will have a greater associated London force than an atom with fewer electrons. The dispersion (London) force is the most important component because all materials are polarizable, whereas Keesom and Debye forces require permanent dipoles. The London interaction is universal and is present in atom-atom interactions as well. For various reasons, London interactions (dispersion) have been considered relevant for interactions between macroscopic bodies in condensed systems. Hamaker developed the theory of van der Waals between macroscopic bodies in 1937 and showed that the additivity of these interactions renders them considerably more long-range.

This comparison is approximate. The actual relative strengths will vary depending on the molecules involved. Ionic bonding and covalent bonding will always be stronger than intermolecular forces in any given substance.

Intermolecular forces are repulsive at short distances and attractive at long distances (see the Lennard-Jones potential). In a gas, the repulsive force chiefly has the effect of keeping two molecules from occupying the same volume. This gives a real gas a tendency to occupy a larger volume than an ideal gas at the same temperature and pressure. The attractive force draws molecules closer together and gives a real gas a tendency to occupy a smaller volume than an ideal gas. Which interaction is more important depends on temperature and pressure (see compressibility factor).

In a gas, the distances between molecules are generally large, so intermolecular forces have only a small effect. The attractive force is not overcome by the repulsive force, but by the thermal energy of the molecules. Temperature is the measure of thermal energy, so increasing temperature reduces the influence of the attractive force. In contrast, the influence of the repulsive force is essentially unaffected by temperature.

When a gas is compressed to increase its density, the influence of the attractive force increases. If the gas is made sufficiently dense, the attractions can become large enough to overcome the tendency of thermal motion to cause the molecules to disperse. Then the gas can condense to form a solid or liquid, i.e., a condensed phase. Lower temperature favors the formation of a condensed phase. In a condensed phase, there is very nearly a balance between the attractive and repulsive forces.

Intermolecular forces observed between atoms and molecules can be described phenomenologically as occurring between permanent and instantaneous dipoles, as outlined above. Alternatively, one may seek a fundamental, unifying theory that is able to explain the various types of interactions such as hydrogen bonding, van der Waals forces and dipole–dipole interactions. Typically, this is done by applying the ideas of quantum mechanics to molecules, and Rayleigh–Schrödinger perturbation theory has been especially effective in this regard. When applied to existing quantum chemistry methods, such a quantum mechanical explanation of intermolecular interactions provides an array of approximate methods that can be used to analyze intermolecular interactions. One of the most helpful methods to visualize this kind of intermolecular interactions, that we can find in quantum chemistry, is the non-covalent interaction index, which is based on the electron density of the system. London Dispersion Forces (LDF) play a big role with this.


</doc>
<doc id="15420" url="https://en.wikipedia.org/wiki?curid=15420" title="IRQ">
IRQ

IRQ may refer to:



</doc>
<doc id="15422" url="https://en.wikipedia.org/wiki?curid=15422" title="List of Internet top-level domains">
List of Internet top-level domains

This list of Internet top-level domains (TLD) contains top-level domains, which are those domains in the DNS root zone of the Domain Name System of the Internet. A list of the top-level domains by the Internet Assigned Numbers Authority (IANA) is maintained at the Root Zone Database. IANA also oversees the approval process for new proposed top-level domains for ICANN. , their root domain contains 1511 top-level domains, with a number of TLDs that have been retired and are no longer functional. , the IANA root database includes 1,584 TLDs, including 55 that are not assigned (revoked), 8 that are retired and 11 test domains and are thus not represented in ICANN's listing and are not in root.zone file.

IANA distinguishes the following groups of top-level domains:


Seven generic top-level domains were created early in the development of the Internet, and predate the creation of ICANN in 1998.

As of 20 May 2017, there were 255 country-code top-level domains, purely in the Latin alphabet, using two-character codes. , this number is 316, with the addition of internationalized domains.


Internationalised domain names have been proposed for Japan and Libya.


All of these TLDs are internationalized domain names (IDN) and support second-level IDNs.



ICANN/IANA has created some Special-Use domain names which are meant for special technical purposes. ICANN/IANA owns all of the Special-Use domain names.




</doc>
<doc id="15428" url="https://en.wikipedia.org/wiki?curid=15428" title="Idealism">
Idealism

In philosophy, idealism is a diverse group of metaphysical views which all assert that "reality" is in some way indistinguishable or inseparable from human perception and/or understanding, that it is in some sense mentally constituted, or that it is otherwise closely connected to ideas. In contemporary scholarship, traditional idealist views are generally divided into two groups. Subjective idealism takes as its starting point that objects only exist to the extent that they are perceived by someone. Objective idealism posits the existence of an "objective" consciousness which exists before and, in some sense, independently of human consciousness, thereby bringing about the existence of objects independently of human minds. In the early modern period, George Berkeley was often considered the paradigmatic idealist, as he asserted that the essence of objects is to be perceived. By contrast, Immanuel Kant, a pioneer of modern idealist thought, held that his version of idealism does “not concern the existence of things”, but asserts only that our “modes of representation” of them, above all "space" and "time", are not “determinations that belong to things in themselves” but essential features of our own minds. Kant called this position “transcendental idealism” (or sometimes “critical idealism"), holding that the objects of experience relied for their existence on the mind, and that the way that things in themselves are outside of our experience cannot be thought without applying the categories which structure all of our experiences. However, since Kant's view affirms the existence of "some" things independently of experience (namely, "things in themselves"), it is very different from the more traditional idealism of Berkeley.

Epistemologically, idealism is accompanied by skepticism about the possibility of knowing any mind-independent thing. In its ontological commitments, idealism goes further, asserting that all entities rely for their existence on the mind. Ontological idealism thus rejects both physicalist and dualist views as failing to ascribe ontological priority to the mind. In contrast to materialism, idealism asserts the "primacy" of consciousness as the origin and prerequisite of phenomena. Idealism holds consciousness or mind to be the "origin" of the material world – in the sense that it is a necessary condition for our positing of a material world – and it aims to explain the existing world according to these principles. The earliest extant arguments that the world of experience is grounded in the mental derive from India and Greece. The Hindu idealists in India and the Greek neoplatonists gave panentheistic arguments for an all-pervading consciousness as the ground or true nature of reality. In contrast, the Yogācāra school, which arose within Mahayana Buddhism in India in the 4th century CE, based its "mind-only" idealism to a greater extent on phenomenological analyses of personal experience. This turn toward the subjective anticipated empiricists such as George Berkeley, who revived idealism in 18th-century Europe by employing skeptical arguments against materialism. Beginning with Immanuel Kant, German idealists such as Georg Wilhelm Friedrich Hegel, Johann Gottlieb Fichte, Friedrich Wilhelm Joseph Schelling, and Arthur Schopenhauer dominated 19th-century philosophy. This tradition, which emphasized the mental or "ideal" character of all phenomena, gave birth to idealistic and subjectivist schools ranging from British idealism to phenomenalism to existentialism.

Phenomenology, an influential strain of philosophy since the beginning of the 20th century, also draws on the lessons of idealism. In his "Being and Time", Martin Heidegger famously states: "If the term idealism amounts to the recognition that being can never be explained through beings, but, on the contrary, always is the transcendental in its relation to any beings, then the only right possibility of philosophical problematics lies with idealism. In that case, Aristotle was no less an idealist than Kant. If idealism means a reduction of all beings to a subject or a consciousness, distinguished by staying "undetermined" in its own being, and ultimately is characterised negatively as 'non-thingly', then this idealism is no less methodically naive than the most coarse-grained realism." Idealism as a philosophy came under heavy attack in the West at the turn of the 20th century. The most influential critics of both epistemological and ontological idealism were G. E. Moore and Bertrand Russell, but its critics also included the new realists. According to "Stanford Encyclopedia of Philosophy", the attacks by Moore and Russell were so influential that even more than 100 years later "any acknowledgment of idealistic tendencies is viewed in the English-speaking world with reservation". However, many aspects and paradigms of idealism did still have a large influence on subsequent philosophy.
"Idealism" is a term with several related meanings. It comes via Latin "idea" from the Ancient Greek "idea" (ἰδέα) from "idein" (ἰδεῖν), meaning "to see". The term entered the English language by 1743. It was first used in the abstract metaphysical sense "belief that reality is made up only of ideas" by Christian Wolff in 1747. The term re-entered the English language in this abstract sense by 1796.

In ordinary language, as when speaking of Woodrow Wilson's political idealism, it generally suggests the priority of ideals, principles, values, and goals over concrete realities. Idealists are understood to represent the world as it might or should be, unlike pragmatists, who focus on the world as it presently is. In the arts, similarly, idealism affirms imagination and attempts to realize a mental conception of beauty, a standard of perfection, juxtaposed to aesthetic naturalism and realism. The term "idealism" is also sometimes used in a sociological sense, which emphasizes how human ideas—especially beliefs and values—shape society. 

Any philosophy that assigns crucial importance to the ideal or spiritual realm in its account of human existence may be termed "idealist". Metaphysical idealism is an ontological doctrine that holds that reality itself is incorporeal or experiential at its core. Beyond this, idealists disagree on which aspects of the mental are more basic. Platonic idealism affirms that abstractions are more basic to reality than the things we perceive, while subjective idealists and phenomenalists tend to privilege sensory experience over abstract reasoning. Epistemological idealism is the view that reality can only be known through ideas, that only psychological experience can be apprehended by the mind.

Subjective idealists like George Berkeley are anti-realists in terms of a mind-independent world, whereas transcendental idealists like Immanuel Kant are strong skeptics of such a world, affirming epistemological and not metaphysical idealism. Thus Kant defines "idealism" as "the assertion that we can never be certain whether all of our putative outer experience is not mere imagining". He claimed that, according to "idealism", "the reality of external objects does not admit of strict proof. On the contrary, however, the reality of the object of our internal sense (of myself and state) is clear immediately through consciousness". However, not all idealists restrict the real or the knowable to our immediate subjective experience. Objective idealists make claims about a transempirical world, but simply deny that this world is essentially divorced from or ontologically prior to the mental. Thus, Plato and Gottfried Leibniz affirm an objective and knowable reality transcending our subjective awareness—a rejection of epistemological idealism—but propose that this reality is grounded in ideal entities, a form of metaphysical idealism. Nor do all metaphysical idealists agree on the nature of the ideal; for Plato, the fundamental entities were non-mental abstract forms, while for Leibniz they were proto-mental and concrete monads.

As a rule, transcendental idealists like Kant affirm idealism's epistemic side without committing themselves to whether reality is "ultimately" mental; objective idealists like Plato affirm reality's metaphysical basis in the mental or abstract without restricting their epistemology to ordinary experience; and subjective idealists like Berkeley affirm both metaphysical and epistemological idealism.

Idealism as a form of metaphysical monism holds that consciousness, not matter, is the ground of all being. It is monist because it holds that there is only one type of thing in the universe and idealist because it holds that one thing to be consciousness.

Anaxagoras (480 BC) taught that "all things" were created by "Nous" ("Mind"). He held that Mind held the cosmos together and gave human beings a connection to the cosmos or a pathway to the divine.

Plato's theory of forms or "ideas" describes ideal forms (for example the platonic solids in geometry or abstracts like Goodness and Justice), as universals existing independently of any particular instance. Arne Grøn calls this doctrine "the classic example of a metaphysical idealism as a "transcendent" idealism", while Simone Klein calls Plato "the earliest representative of metaphysical objective idealism". Nevertheless, Plato holds that matter is real, though transitory and imperfect, and is perceived by our body and its senses and given existence by the eternal ideas that are perceived directly by our rational soul. Plato was therefore a metaphysical and epistemological dualist, an outlook that modern idealism has striven to avoid: Plato's thought cannot therefore be counted as idealist in the modern sense.

With the neoplatonist Plotinus, wrote Nathaniel Alfred Boll "there even appears, probably for the first time in Western philosophy, "idealism" that had long been current in the East even at that time, for it taught... that the soul has made the world by stepping from eternity into time...". Similarly, in regard to passages from the Enneads, "The only space or place of the world is the soul" and "Time must not be assumed to exist outside the soul". Ludwig Noiré wrote: "For the first time in Western philosophy we find idealism proper in Plotinus". However, Plotinus does not address whether we know external objects, unlike Schopenhauer and other modern philosophers.

Christian theologians have held idealist views, often based on neoplatonism, despite the influence of Aristotelian scholasticism from the 12th century onward. Later western theistic idealism such as that of Hermann Lotze offers a theory of the "world ground" in which all things find their unity: it has been widely accepted by Protestant theologians. Several modern religious movements, for example the organizations within the New Thought Movement and the Unity Church, may be said to have a particularly idealist orientation. The theology of Christian Science includes a form of idealism: it teaches that all that truly exists is God and God's ideas; that the world as it appears to the senses is a distortion of the underlying spiritual reality, a distortion that may be corrected (both conceptually and in terms of human experience) through a reorientation (spiritualization) of thought.

Wang Yangming, a Ming Chinese neo-Confucian philosopher, official, educationist, calligraphist and general, held that objects do not exist entirely apart from the mind because the mind shapes them. It is not the world that shapes the mind but the mind that gives reason to the world, so the mind alone is the source of all reason, having an inner light, an innate moral goodness and understanding of what is good.

There are currents of idealism throughout Indian philosophy, ancient and modern. Hindu idealism often takes the form of monism or non-dualism, espousing the view that a unitary consciousness is the essence or meaning of the phenomenal reality and plurality.

Buddhist idealism on the other hand is more epistemic and is not a metaphysical monism, which Buddhists consider eternalistic and hence not the middle way between extremes espoused by the Buddha.

The oldest reference to Idealism in Vedic texts is in Purusha Sukta of the Rig Veda. This sukta espouses panentheism by presenting cosmic being Purusha as both pervading all universe and yet being transcendent to it. Absolute idealism can be seen in Chāndogya Upaniṣad, where things of the objective world like the five elements and the subjective world such as will, hope, memory etc. are seen to be emanations from the Self.

Idealist notions have been propounded by the Vedanta schools of thought, which use the Vedas, especially the Upanishads as their key texts. Idealism was opposed by dualists Samkhya, the atomists Vaisheshika, the logicians Nyaya, the linguists Mimamsa and the materialists Cārvāka. There are various sub schools of Vedanta, like Advaita Vedanta (non-dual), Vishishtadvaita and Bhedabheda Vedanta (difference and non-difference).

The schools of Vedanta all attempt to explain the nature and relationship of Brahman (universal soul or Self) and Atman (individual self), which they see as the central topic of the Vedas. One of the earliest attempts at this was Bādarāyaņa's Brahma Sutras, which is canonical for all Vedanta sub-schools. Advaita Vedanta is a major sub school of Vedanta which holds a non-dual Idealistic metaphysics. According to Advaita thinkers like Adi Shankara (788–820) and his contemporary Maṇḍana Miśra, Brahman, the single unitary consciousness or absolute awareness, appears as the diversity of the world because of "maya" or illusion, and hence perception of plurality is "mithya", error. The world and all beings or souls in it have no separate existence from Brahman, universal consciousness, and the seemingly independent soul ("jiva") is identical to Brahman. These doctrines are represented in verses such as "brahma satyam jagan mithya; jīvo brahmaiva na aparah" (Brahman is alone True, and this world of plurality is an error; the individual self is not different from Brahman). Other forms of Vedanta like the Vishishtadvaita of Ramanuja and the Bhedabheda of Bhāskara are not as radical in their non-dualism, accepting that there is a certain difference between individual souls and Brahman. Dvaita school of Vedanta by Madhvacharya maintains the opposing view that the world is real and eternal. It also argues that real atman fully depends and reflection of independent brahman.

The Tantric tradition of Kashmir Shaivism has also been categorized by scholars as a form of Idealism. The key thinker of this tradition is the Kashmirian Abhinavagupta (975–1025 CE).

Modern Vedic Idealism was defended by the influential Indian philosopher Sarvepalli Radhakrishnan in his 1932 "An Idealist View of Life" and other works, which espouse Advaita Vedanta. The essence of Hindu Idealism is captured by such modern writers as Sri Nisargadatta Maharaj, Sri Aurobindo, P. R. Sarkar, and Sohail Inayatullah.

Buddhist views which can be said to be similar to Idealism appear in Mahayana Buddhist texts such as the Samdhinirmocana sutra, Laṅkāvatāra Sūtra, Dashabhumika sutra, etc. These were later expanded upon by Indian Buddhist philosophers of the influential Yogacara school, like Vasubandhu, Asaṅga, Dharmakīrti, and Śāntarakṣita. Yogacara thought was also promoted in China by Chinese philosophers and translators like Xuanzang.

There is a modern scholarly disagreement about whether Yogacara Buddhism can be said to be a form of idealism. As Saam Trivedi notes: "on one side of the debate, writers such as Jay Garfield, Jeffrey Hopkins, Paul Williams, and others maintain the idealism label, while on the other side, Stefan Anacker, Dan Lusthaus, Richard King, Thomas Kochumuttom, Alex Wayman, Janice Dean Willis, and others have argued that Yogacara is not idealist." The central point of issue is what Buddhist philosophers like Vasubandhu who used the term "vijñapti-matra" ("representation-only" or "cognition-only") and formulated arguments to refute external objects actually meant to say.

Vasubandhu's works include a refutation of external objects or externality itself and argues that the true nature of reality is beyond subject-object distinctions. He views ordinary consciousness experience as deluded in its perceptions of an external world separate from itself and instead argues that all there is "Vijñapti" (representation or conceptualization). Hence Vasubandhu begins his "Vimsatika" with the verse: "All this is consciousness-only, because of the appearance of non-existent objects, just as someone with an optical disorder may see non-existent nets of hair."

Likewise, the Buddhist philosopher Dharmakirti's view of the apparent existence of external objects is summed up by him in the Pramānaṿārttika (‘Commentary on Logic and Epistemology’): "Cognition experiences itself, and nothing else whatsoever. Even the particular objects of perception, are by nature just consciousness itself."

While some writers like Jay Garfield hold that Vasubandhu is a metaphysical idealist, others see him as closer to an epistemic idealist like Kant who holds that our knowledge of the world is simply knowledge of our own concepts and perceptions of a transcendental world. Sean Butler upholding that Yogacara is a form of idealism, albeit its own unique type, notes the similarity of Kant's categories and Yogacara's "Vāsanās", both of which are simply phenomenal tools with which the mind interprets the noumenal realm. Unlike Kant however who holds that the noumenon or thing-in-itself is unknowable to us, Vasubandhu holds that ultimate reality is knowable, but only through non-conceptual yogic perception of a highly trained meditative mind.

Writers like Dan Lusthaus who hold that Yogacara is not a metaphysical idealism point out, for example, that Yogācāra thinkers did not focus on consciousness to assert it as ontologically real, but simply to analyze how our experiences and thus our suffering is created. As Lusthaus notes: "no Indian Yogācāra text ever claims that the world is created by mind. What they do claim is that we mistake our projected interpretations of the world for the world itself, i.e. we take our own mental constructions to be the world." Lusthaus notes that there are similarities to Western epistemic idealists like Kant and Husserl, enough so that Yogacara can be seen as a form of epistemological idealism. However he also notes key differences like the concepts of karma and nirvana. Saam Trivedi meanwhile notes the similarities between epistemic idealism and Yogacara, but adds that Yogacara Buddhism is in a sense its own theory.

Similarly, Thomas Kochumuttom sees Yogacara as "an explanation of experience, rather than a system of ontology" and Stefan Anacker sees Vasubandhu's philosophy as a form of psychology and as a mainly therapeutic enterprise.

Subjective idealism (also known as immaterialism) describes a relationship between experience and the world in which objects are no more than collections or bundles of sense data in the perceiver. Proponents include Berkeley, Bishop of Cloyne, an Anglo-Irish philosopher who advanced a theory he called "immaterialism," later referred to as "subjective idealism", contending that individuals can only know sensations and ideas of objects directly, not abstractions such as "matter", and that ideas also depend upon being perceived for their very existence - "esse est percipi"; "to be is to be perceived".

Arthur Collier published similar assertions though there seems to have been no influence between the two contemporary writers. The only knowable reality is the represented image of an external object. Matter as a cause of that image, is unthinkable and therefore nothing to us. An external world as absolute matter unrelated to an observer does not exist as far as we are concerned. The universe cannot exist as it appears if there is no perceiving mind. Collier was influenced by "An Essay Towards the Theory of the Ideal or Intelligible World" by Cambridge Platonist John Norris (1701).

Bertrand Russell's popular book "The Problems of Philosophy" highlights Berkeley's tautological premise for advancing idealism;

The Australian philosopher David Stove harshly criticized philosophical idealism, arguing that it rests on what he called "the worst argument in the world". Stove claims that Berkeley tried to derive a non-tautological conclusion from tautological reasoning. He argued that in Berkeley's case the fallacy is not obvious and this is because one premise is ambiguous between one meaning which is tautological and another which, Stove argues, is logically equivalent to the conclusion.

Alan Musgrave argues that conceptual idealists compound their mistakes with use/mention confusions;

and proliferation of hyphenated entities such as "thing-in-itself" (Immanuel Kant), "things-as-interacted-by-us" (Arthur Fine), "table-of-commonsense" and "table-of-physics" (Arthur Eddington) which are "warning signs" for conceptual idealism according to Musgrave because they allegedly do not exist but only highlight the numerous ways in which people come to know the world. This argument does not take into account the issues pertaining to hermeneutics, especially at the backdrop of analytic philosophy. Musgrave criticized Richard Rorty and postmodernist philosophy in general for confusion of use and mention.

A. A. Luce and John Foster are other subjectivists. Luce, in "Sense without Matter" (1954), attempts to bring Berkeley up to date by modernizing his vocabulary and putting the issues he faced in modern terms, and treats the Biblical account of matter and the psychology of perception and nature. Foster's "The Case for Idealism" argues that the physical world is the logical creation of natural, non-logical constraints on human sense-experience. Foster's latest defense of his views (phenomenalistic idealism) is in his book "A World for Us: The Case for Phenomenalistic Idealism".

Paul Brunton, a British philosopher, mystic, traveler, and guru, taught a type of idealism called "mentalism," similar to that of Bishop Berkeley, proposing a master world-image, projected or manifested by a world-mind, and an infinite number of individual minds participating. A tree does not cease to exist if nobody sees it because the world-mind is projecting the idea of the tree to all minds

John Searle, criticizing some versions of idealism, summarizes two important arguments for subjective idealism. The first is based on our perception of reality:

therefore;

Whilst agreeing with (2) Searle argues that (1) is false and points out that (3) does not follow from (1) and (2). The second argument runs as follows;

Searle contends that "Conclusion 2" does not follow from the premises.

Epistemological idealism is a subjectivist position in epistemology that holds that what one knows about an object exists only in one's mind. Proponents include Brand Blanshard.

Transcendental idealism, founded by Immanuel Kant in the eighteenth century, maintains that the mind shapes the world we perceive into the form of space-and-time.

The 2nd edition (1787) contained a "Refutation of Idealism" to distinguish his transcendental idealism from Descartes's Sceptical Idealism and Berkeley's anti-realist strain of Subjective Idealism. The section "Paralogisms of Pure Reason" is an implicit critique of Descartes' idealism. Kant says that it is not possible to infer the 'I' as an object (Descartes' "cogito ergo sum") purely from "the spontaneity of thought". Kant focused on ideas drawn from British philosophers such as Locke, Berkeley and Hume but distinguished his transcendental or critical idealism from previous varieties;

Kant distinguished between things as they appear to an observer and things in themselves, "that is, things considered without regard to whether and how they may be given to us". We cannot approach the "noumenon", the "thing in Itself" () without our own mental world. He added that the mind is not a blank slate, "tabula rasa" but rather comes equipped with categories for organising our sense impressions.

In the first volume of his "Parerga and Paralipomena", Schopenhauer wrote his "Sketch of a History of the Doctrine of the Ideal and the Real". He defined the ideal as being mental pictures that constitute subjective knowledge. The ideal, for him, is what can be attributed to our own minds. The images in our head are what comprise the ideal. Schopenhauer emphasized that we are restricted to our own consciousness. The world that appears is only a representation or mental picture of objects. We directly and immediately know only representations. All objects that are external to the mind are known indirectly through the mediation of our mind. He offered a history of the concept of the "ideal" as "ideational" or "existing in the mind as an image".

Charles Bernard Renouvier was the first Frenchman after Nicolas Malebranche to formulate a complete idealistic system, and had a vast influence on the development of French thought. His system is based on Immanuel Kant's, as his chosen term "néo-criticisme" indicates; but it is a transformation rather than a continuation of Kantianism.

Friedrich Nietzsche argued that Kant commits an agnostic tautology and does not offer a satisfactory answer as to the "source" of a philosophical right to such-or-other metaphysical claims; he ridicules his pride in tackling "the most difficult thing that could ever be undertaken on behalf of metaphysics." The famous "thing-in-itself" was called a product of philosophical habit, which seeks to introduce a grammatical subject: because wherever there is cognition, there must be a "thing" that is cognized and allegedly it must be added to ontology as a being (whereas, to Nietzsche, only the world as ever changing appearances can be assumed). Yet he attacks the idealism of Schopenhauer and Descartes with an argument similar to Kant's critique of the latter "(see above)".

Objective idealism asserts that the reality of experiencing combines and transcends the realities of the object experienced and of the mind of the observer. Proponents include Thomas Hill Green, Josiah Royce, Benedetto Croce and Charles Sanders Peirce.

Schelling (1775–1854) claimed that the Fichte's "I" needs the Not-I, because there is no subject without object, and vice versa. So there is no difference between the subjective and the objective, that is, the ideal and the real. This is Schelling's "absolute identity": the ideas or mental images in the mind are identical to the extended objects which are external to the mind.

Absolute idealism is G. W. F. Hegel's account of how existence is comprehensible as an all-inclusive whole. Hegel called his philosophy "absolute" idealism in contrast to the "subjective idealism" of Berkeley and the "transcendental idealism" of Kant and Fichte, which were not based on a critique of the finite and a dialectical philosophy of history as Hegel's idealism was. The exercise of reason and intellect enables the philosopher to know ultimate historical reality, the phenomenological constitution of self-determination, the dialectical development of self-awareness and personality in the realm of History.

In his "Science of Logic" (1812–1814) Hegel argues that finite qualities are not fully "real" because they depend on other finite qualities to determine them. Qualitative "infinity", on the other hand, would be more self-determining and hence more fully real. Similarly finite natural things are less "real"—because they are less self-determining—than spiritual things like morally responsible people, ethical communities and God. So any doctrine, such as materialism, that asserts that finite qualities or natural objects are fully real is mistaken.

Hegel certainly intends to preserve what he takes to be true of German idealism, in particular Kant's insistence that ethical reason can and does go beyond finite inclinations. For Hegel there must be some identity of thought and being for the "subject" (any human observer) to be able to know any observed "object" (any external entity, possibly even another human) at all. Under Hegel's concept of "subject-object identity," subject and object both have Spirit (Hegel's ersatz, redefined, nonsupernatural "God") as their "conceptual" (not metaphysical) inner reality—and in that sense are identical. But until Spirit's "self-realization" occurs and Spirit graduates from Spirit to "Absolute" Spirit status, subject (a human mind) mistakenly thinks every "object" it observes is something "alien," meaning something separate or apart from "subject." In Hegel's words, "The object is revealed to it [to "subject"] by [as] something alien, and it does not recognize itself." Self-realization occurs when Hegel (part of Spirit's nonsupernatural Mind, which is the collective mind of all humans) arrives on the scene and realizes that every "object" is "himself", because both subject and object are essentially Spirit. When self-realization occurs and Spirit becomes "Absolute" Spirit, the "finite" (man, human) becomes the "infinite" ("God," divine), replacing the imaginary or "picture-thinking" supernatural God of theism: man becomes God. Tucker puts it this way: "Hegelianism . . . is a religion of self-worship whose fundamental theme is given in Hegel's image of the man who aspires to be God himself, who demands 'something more, namely infinity.'" The picture Hegel presents is "a picture of a self-glorifying humanity striving compulsively, and at the end successfully, to rise to divinity."

Kierkegaard criticized Hegel's idealist philosophy in several of his works, particularly his claim to a comprehensive system that could explain the whole of reality. Where Hegel argues that an ultimate understanding of the logical structure of the world is an understanding of the logical structure of God's mind, Kierkegaard asserts that for God reality can be a system but it cannot be so for any human individual because both reality and humans are incomplete and all philosophical systems imply completeness. For Hegel, a logical system is possible but an existential system is not: "What is rational is actual; and what is actual is rational". Hegel's absolute idealism blurs the distinction between existence and thought: our mortal nature places limits on our understanding of reality;

So-called systems have often been characterized and challenged in the assertion that they abrogate the distinction between good and evil, and destroy freedom. Perhaps one would express oneself quite as definitely, if one said that every such system fantastically dissipates the concept existence. ... Being an individual man is a thing that has been abolished, and every speculative philosopher confuses himself with humanity at large; whereby he becomes something infinitely great, and at the same time nothing at all.

A major concern of Hegel's "Phenomenology of Spirit" (1807) and of the philosophy of Spirit that he lays out in his "Encyclopedia of the Philosophical Sciences" (1817–1830) is the interrelation between individual humans, which he conceives in terms of "mutual recognition." However, what Climacus means by the aforementioned statement, is that Hegel, in the "Philosophy of Right", believed the best solution was to surrender one's individuality to the customs of the State, identifying right and wrong in view of the prevailing bourgeois morality. Individual human will ought, at the State's highest level of development, to properly coincide with the will of the State. Climacus rejects Hegel's suppression of individuality by pointing out it is impossible to create a valid set of rules or system in any society which can adequately describe existence for any one individual. Submitting one's will to the State denies personal freedom, choice, and responsibility.

In addition, Hegel does believe we can know the structure of God's mind, or ultimate reality. Hegel agrees with Kierkegaard that both reality and humans are incomplete, inasmuch as we are in time, and reality develops through time. But the relation between time and eternity is outside time and this is the "logical structure" that Hegel thinks we can know. Kierkegaard disputes this assertion, because it eliminates the clear distinction between ontology and epistemology. Existence and thought are not identical and one cannot possibly think existence. Thought is always a form of abstraction, and thus not only is pure existence impossible to think, but all forms in existence are unthinkable; thought depends on language, which merely abstracts from experience, thus separating us from lived experience and the living essence of all beings. In addition, because we are finite beings, we cannot possibly know or understand anything that is universal or infinite such as God, so we cannot know God exists, since that which transcends time simultaneously transcends human understanding.

Bradley saw reality as a monistic whole apprehended through "feeling", a state in which there is no distinction between the perception and the thing perceived. Like Berkeley, Bradley thought that nothing can be known to exist unless it is known by a mind.

Bradley was the apparent target of G.E. Moore's radical rejection of idealism. Moore claimed that Bradley did not understand the statement that something is real. We know for certain, through common sense and prephilosophical beliefs, that some things are real, whether they are objects of thought or not, according to Moore. The 1903 article "The Refutation of Idealism" is one of the first demonstrations of Moore's commitment to analysis. He examines each of the three terms in the Berkeleian aphorism "esse est percipi", "to be is to be perceived", finding that it must mean that the object and the subject are "necessarily" connected so that "yellow" and "the sensation of yellow" are identical - "to be yellow" is "to be experienced as yellow". But it also seems there is a difference between "yellow" and "the sensation of yellow" and "that "esse" is held to be "percipi", solely because what is experienced is held to be identical with the experience of it". Though far from a complete refutation, this was the first strong statement by analytic philosophy against its idealist predecessors, or at any rate against the type of idealism represented by Berkeley.

Actual idealism is a form of idealism developed by Giovanni Gentile that grew into a "grounded" idealism contrasting Kant and Hegel. The idea is a version of Occam's razor; the simpler explanations are always correct. Actual idealism is the idea that reality is the ongoing act of thinking, or in Italian "pensiero pensante". Any action done by humans is classified as human thought because the action was done due to predisposed thought. He further believes that thoughts are the only concept that truly exist since reality is defined through the act of thinking. This idea was derived from Gentile's paper, "The Theory of Mind As Pure Act".

Since thoughts are actions, any conjectured idea can be enacted. This idea not only affects the individual's life, but everyone around them, which in turn affects the state since the people are the state. Therefore, thoughts of each person are subsumed within the state. The state is a composition of many minds that come together to change the country for better or worse.

Gentile theorizes that thoughts can only be conjectured within the bounds of known reality; abstract thinking does not exist. Thoughts cannot be formed outside our known reality because we are the reality that halt ourselves from thinking externally. With accordance to "The Act of Thought of Pure Thought", our actions comprise our thoughts, our thoughts create perception, perceptions define reality, thus we think within our created reality.

The present act of thought is reality but the past is not reality; it is history. The reason being, past can be rewritten through present knowledge and perspective of the event. The reality that is currently constructed can be completely changed through language (e.g. bias (omission, source, tone)). The unreliability of the recorded realty can skew the original concept and make the past remark unreliable. 
Actual idealism is regarded as a liberal and tolerant doctrine since it acknowledges that every being picturizes reality, in which their ideas remained hatched, differently. Even though, reality is a figment of thought.

Even though core concept of the theory is famous for its simplification, its application is regarded as extremely ambiguous. Over the years, philosophers have interpreted it numerously different ways: Holmes took it as metaphysics of the thinking act; Betti as a form of hermeneutics; Harris as a metaphysics of democracy; Fogu as a modernist philosophy of history.

Giovanni Gentile was a key supporter of fascism, regarded by many as the "philosopher of fascism". Gentile's philosophy was the key to understating fascism as it was believed by many who supported and loved it. They believed, if priori synthesis of subject and object is true, there is no difference between the individuals in society; they're all one. Which means that they have equal right, roles, and jobs. In fascist state, submission is given to one leader because individuals act as one body. In Gentile's view, far more can be accomplished when individuals are under a corporate body than a collection of autonomous individuals.

Pluralistic idealism such as that of Gottfried Leibniz takes the view that there are many individual minds that together underlie the existence of the observed world and make possible the existence of the physical universe. Unlike absolute idealism, pluralistic idealism does not assume the existence of a single ultimate mental reality or "Absolute". Leibniz' form of idealism, known as Panpsychism, views "monads" as the true atoms of the universe and as entities having perception. The monads are "substantial forms of being, "elemental, individual, subject to their own laws, non-interacting, each reflecting the entire universe. Monads are centers of force, which is substance while space, matter and motion are phenomenal and their form and existence is dependent on the simple and immaterial monads. There is a pre-established harmony by God, the central monad, between the world in the minds of the monads and the external world of objects. Leibniz's cosmology embraced traditional Christian theism. The English psychologist and philosopher James Ward inspired by Leibniz had also defended a form of pluralistic idealism. According to Ward the universe is composed of "psychic monads" of different levels, interacting for mutual self-betterment.

Personalism is the view that the minds that underlie reality are the minds of persons. Borden Parker Bowne, a philosopher at Boston University, a founder and popularizer of personal idealism, presented it as a substantive reality of persons, the only reality, as known directly in self-consciousness. Reality is a society of interacting persons dependent on the Supreme Person of God. Other proponents include George Holmes Howison and J. M. E. McTaggart.

Howison's personal idealism was also called "California Personalism" by others to distinguish it from the "Boston Personalism" which was of Bowne. Howison maintained that both impersonal, monistic idealism and materialism run contrary to the experience of moral freedom. To deny freedom to pursue truth, beauty, and "benignant love" is to undermine every profound human venture, including science, morality, and philosophy. Personalistic idealists Borden Parker Bowne and Edgar S. Brightman and realistic (in some senses of the term, though he remained influenced by neoplatonism) personal theist Saint Thomas Aquinas address a core issue, namely that of dependence upon an infinite personal God.

Howison, in his book "The Limits of Evolution and Other Essays Illustrating the Metaphysical Theory of Personal Idealism", created a democratic notion of personal idealism that extended all the way to God, who was no more the ultimate monarch but the ultimate democrat in eternal relation to other eternal persons. J. M. E. McTaggart's idealist atheism and Thomas Davidson's apeirotheism resemble Howisons personal idealism.

J. M. E. McTaggart argued that minds alone exist and only relate to each other through love. Space, time and material objects are unreal. In "The Unreality of Time" he argued that time is an illusion because it is impossible to produce a coherent account of a sequence of events. "The Nature of Existence" (1927) contained his arguments that space, time, and matter cannot possibly be real. In his "Studies in Hegelian Cosmology" (Cambridge, 1901, p196) he declared that metaphysics are not relevant to social and political action. McTaggart "thought that Hegel was wrong in supposing that metaphysics could show that the state is more than a means to the good of the individuals who compose it". For McTaggart "philosophy can give us very little, if any, guidance in action... Why should a Hegelian citizen be surprised that his belief as to the organic nature of the Absolute does not help him in deciding how to vote? Would a Hegelian engineer be reasonable in expecting that his belief that all matter is spirit should help him in planning a bridge?

Thomas Davidson taught a philosophy called "apeirotheism", a "form of pluralistic idealism...coupled with a stern ethical rigorism" which he defined as "a theory of Gods infinite in number." The theory was indebted to Aristotle's pluralism and his concepts of Soul, the rational, living aspect of a living substance which cannot exist apart from the body because it is not a substance but an essence, and "nous", rational thought, reflection and understanding. Although a perennial source of controversy, Aristotle arguably views the latter as both eternal and immaterial in nature, as exemplified in his theology of unmoved movers. Identifying Aristotle's God with rational thought, Davidson argued, contrary to Aristotle, that just as the soul cannot exist apart from the body, God cannot exist apart from the world.

Idealist notions took a strong hold among physicists of the early 20th century confronted with the paradoxes of quantum physics and the theory of relativity. In "The Grammar of Science", Preface to the 2nd Edition, 1900, Karl Pearson wrote, "There are many signs that a sound idealism is surely replacing, as a basis for natural philosophy, the crude materialism of the older physicists." This book influenced Einstein's regard for the importance of the observer in scientific measurements. In § 5 of that book, Pearson asserted that "...science is in reality a classification and analysis of the contents of the mind..." Also, "...the field of science is much more consciousness than an external world."

Arthur Eddington, a British astrophysicist of the early 20th century, wrote in his book "The Nature of the Physical World" that "The stuff of the world is mind-stuff": The mind-stuff of the world is, of course, something more general than our individual conscious minds... The mind-stuff is not spread in space and time; these are part of the cyclic scheme ultimately derived out of it... It is necessary to keep reminding ourselves that all knowledge of our environment from which the world of physics is constructed, has entered in the form of messages transmitted along the nerves to the seat of consciousness... Consciousness is not sharply defined, but fades into subconsciousness; and beyond that we must postulate something indefinite but yet continuous with our mental nature... It is difficult for the matter-of-fact physicist to accept the view that the substratum of everything is of mental character. But no one can deny that mind is the first and most direct thing in our experience, and all else is remote inference."
Ian Barbour in his book "Issues in Science and Religion" (1966), p. 133, cites Arthur Eddington's "The Nature of the Physical World" (1928) for a text that argues The Heisenberg Uncertainty Principles provides a scientific basis for "the defense of the idea of human freedom" and his "Science and the Unseen World" (1929) for support of philosophical idealism "the thesis that reality is basically mental".

Sir James Jeans wrote: "The stream of knowledge is heading towards a non-mechanical reality; the Universe begins to look more like a great thought than like a great machine. Mind no longer appears to be an accidental intruder into the realm of matter... we ought rather hail it as the creator and governor of the realm of matter."

Jeans, in an interview published in "The Observer" (London), when asked the question: "Do you believe that life on this planet is the result of some sort of accident, or do you believe that it is a part of some great scheme?" replied: I incline to the idealistic theory that consciousness is fundamental, and that the material universe is derivative from consciousness, not consciousness from the material universe... In general the universe seems to me to be nearer to a great thought than to a great machine. It may well be, it seems to me, that each individual consciousness ought to be compared to a brain-cell in a universal mind.

Addressing the British Association in 1934, Jeans said: What remains is in any case very different from the full-blooded matter and the forbidding materialism of the Victorian scientist. His objective and material universe is proved to consist of little more than constructs of our own minds. To this extent, then, modern physics has moved in the direction of philosophic idealism. Mind and matter, if not proved to be of similar nature, are at least found to be ingredients of one single system. There is no longer room for the kind of dualism which has haunted philosophy since the days of Descartes.

In "The Universe Around Us", Jeans writes: Finite picture whose dimensions are a certain amount of space and a certain amount of time; the protons and electrons are the streaks of paint which define the picture against its space-time background. Traveling as far back in time as we can, brings us not to the creation of the picture, but to its edge; the creation of the picture lies as much outside the picture as the artist is outside his canvas. On this view, discussing the creation of the universe in terms of time and space is like trying to discover the artist and the action of painting, by going to the edge of the canvas. This brings us very near to those philosophical systems which regard the universe as a thought in the mind of its Creator, thereby reducing all discussion of material creation to futility.

The chemist Ernest Lester Smith wrote a book "Intelligence Came First" (1975) in which he claimed that consciousness is a fact of nature and that the cosmos is grounded in and pervaded by mind and intelligence.

Bernard d'Espagnat, a French theoretical physicist best known for his work on the nature of reality, wrote a paper titled "The Quantum Theory and Reality". According to the paper: The doctrine that the world is made up of objects whose existence is independent of human consciousness turns out to be in conflict with quantum mechanics and with facts established by experiment.

In a "Guardian" article entitled "Quantum Weirdness: What We Call 'Reality' is Just a State of Mind", d'Espagnat wrote: What quantum mechanics tells us, I believe, is surprising to say the least. It tells us that the basic components of objects – the particles, electrons, quarks etc. – cannot be thought of as 'self-existent'.

He further writes that his research in quantum physics has led him to conclude that an "ultimate reality" exists, which is not embedded in space or time.


Further reading



</doc>
<doc id="15430" url="https://en.wikipedia.org/wiki?curid=15430" title="Inheritance">
Inheritance

Inheritance is the practice of passing on private property, titles, debts, rights, and obligations upon the death of an individual. The rules of inheritance differ among societies and have changed over time.

In law, an "heir" is a person who is entitled to receive a share of the deceased's (the person who died) property, subject to the rules of inheritance in the jurisdiction of which the deceased was a citizen or where the deceased (decedent) died or owned property at the time of death.

The inheritance may be either under the terms of a will or by intestate laws if the deceased had no will. However, the will must comply with the laws of the jurisdiction at the time it was created or it will be declared invalid (for example, some states do not recognise holographic wills as valid, or only in specific circumstances) and the intestate laws then apply.

A person does not become an heir before the death of the deceased, since the exact identity of the persons entitled to inherit is determined only then. Members of ruling noble or royal houses who are expected to become heirs are called heirs apparent if first in line and incapable of being displaced from inheriting by another claim; otherwise, they are heirs presumptive. There is a further concept of joint inheritance, pending renunciation by all but one, which is called coparceny.

In modern law, the terms "inheritance" and "heir" refer exclusively to succession to property by descent from a deceased dying intestate. Takers in property succeeded to under a will are termed generally "beneficiaries," and specifically "devises" for real property, "sequesters" for personal property (except money), or "legatees" for money.

Except in some jurisdictions where a person cannot be legally disinherited (such as the United States state of Louisiana, which allows disinheritance only under specifically enumerated circumstances), a person who would be an heir under intestate laws may be disinherited completely under the terms of a will (an example is that of the will of comedian Jerry Lewis; his will specifically disinherited his six children by his first wife, and their descendants, leaving his entire estate to his second wife).

Detailed anthropological and sociological studies have been made about customs of patrimonial inheritance, where only male children can inherit. Some cultures also employ matrilineal succession, where property can only pass along the female line, most commonly going to the sister's sons of the decedent; but also, in some societies, from the mother to her daughters. Some ancient societies and most modern states employ egalitarian inheritance, without discrimination based on gender and/or birth order.

The inheritance is patrimonial. The father —that is, the owner of the land— bequeaths only to his male descendants, so the Promised Land passes from one Jewish father to his sons.

If there were no living sons and no descendants of any previously living sons, daughters inherit. In , the daughters of Zelophehad (Mahlah, Noa, Hoglah, Milcah, and Tirzah) of the tribe of Manasseh come to Moses and ask for their father's inheritance, as they have no brothers. The order of inheritance is set out in : a man's sons inherit first, daughters if no sons, brothers if he has no children, and so on.

Later, in , some of the heads of the families of the tribe of Manasseh come to Moses and point out that, if a daughter inherits and then marries a man not from her paternal tribe, her land will pass from her birth-tribe's inheritance into her marriage-tribe's. So a further rule is laid down: if a daughter inherits land, she must marry someone within her father's tribe. (The daughters of Zelophehad marry the sons' of their father's brothers. There is no indication that this was not their choice.)

The tractate Baba Bathra, written during late Antiquity in Babylon, deals extensively with issues of property ownership and inheritance according to Jewish Law. Other works of Rabbinical Law, such as the Hilkhot naḥalot: mi-sefer Mishneh Torah leha-Rambam, and the Sefer ha-yerushot: ʻim yeter ha-mikhtavim be-divre ha-halakhah be-ʻAravit uve-ʻIvrit uve-Aramit also deal with inheritance issues. The first, often abbreviated to Mishneh Torah, was written by Maimonides and was very important in Jewish tradition.

All these sources agree that the firstborn son is entitled to a double portion of his father's estate: . This means that, for example, if a father left five sons, the firstborn receives a third of the estate and each of the other four receives a sixth. If he left nine sons, the firstborn receives a fifth and each of the other eight receive a tenth. If the eldest surviving son is not the firstborn son, he is not entitled to the double portion.

Philo of Alexandria and Josephus also comment on the Jewish laws of inheritance, praising them above other law codes of their time. They also agreed that the firstborn son must receive a double portion of his father's estate.

At first, Christianity did not have its own inheritance traditions distinct from Judaism. With the accession of Emperor Constantine in 306, Christians both began to distance themselves from Judaism and to have influence on the law and practices of secular institutions. From the beginning, this included inheritance. The Roman practice of adoption was a specific target, because it was perceived to be in conflict with the Judeo-Christian doctrine of primogeniture. As Stephanie Coontz documents in "Marriage, a History" (Penguin, 2006), not only succession but the whole constellation of rights and practices that included marriage, adoption, legitimacy, consanguinity, and inheritance changed in Western Europe from a Greco-Roman model to a Judeo-Christian pattern, based on Biblical and traditional Judeo-Christian principles. The transformation was essentially complete in the Middle Ages, although in English-speaking countries there was additional development under the influence of Protestantism. Even when Europe became secularized and Christianity faded into the background, the legal foundation Christendom had laid remained. Only in the era of modern jurisprudence have there been significant changes.

The Quran introduced a number of different rights and restrictions on matters of inheritance, including general improvements to the treatment of women and family life compared to the pre-Islamic societies that existed in the Arabian Peninsula at the time. Furthermore, the Quran introduced additional heirs that were not entitled to inheritance in pre-Islamic times, mentioning nine relatives specifically of which six were female and three were male. However, the inheritance rights of women remained inferior to those of men because in Islam someone always has a responsibility of looking after a woman's expenses. According to the Quran, for example, a son is entitled to twice as much inheritance as a daughter. The Quran also presented efforts to fix the laws of inheritance, and thus forming a complete legal system. This development was in contrast to pre-Islamic societies where rules of inheritance varied considerably. In addition to the above changes, the Quran imposed restrictions on testamentary powers of a Muslim in disposing his or her property. 
The Quran contains only three verses that give specific details of inheritance and shares, in addition to few other verses dealing with testamentary. But this information was used as a starting point by Muslim jurists who expounded the laws of inheritance even further using Hadith, as well as methods of juristic reasoning like Qiyas. Nowadays, inheritance is considered an integral part of Sharia law and its application for Muslims is mandatory, though many peoples (see Historical inheritance systems), despite being Muslim, have other inheritance customs.

The distribution of the inherited wealth has varied greatly among different cultures and legal traditions. In nations using civil law, for example, the right of children to inherit wealth from parents in pre-defined ratios is enshrined in law, as far back as the Code of Hammurabi (ca. 1750 BC). In the US State of Louisiana, the only US state where the legal system is derived from the Napoleonic Code, this system is known as "forced heirship" which prohibits disinheritance of adult children except for a few narrowly-defined reasons that a parent is obligated to prove. Other legal traditions, particularly in nations using common law, allow inheritances to be divided however one wishes, or to disinherit any child for any reason.

In cases of unequal inheritance, the majority might receive little while only a small number inherit a larger amount, with the lesser amount given to the daughter in the family. The amount of inheritance is often far less than the value of a business initially given to the son, especially when a son takes over a thriving multimillion-dollar business, yet the daughter is given the balance of the actual inheritance amounting to far less than the value of business that was initially given to the son. This is especially seen in old world cultures, but continues in many families to this day.

Arguments for eliminating forced heirship include the right to property and the merit of individual allocation of capital over government wealth confiscation and redistribution, but this does not resolve what some describe as the problem of unequal inheritance. In terms of inheritance inequality, some economists and sociologists focus on the inter generational transmission of income or wealth which is said to have a direct impact on one's mobility (or immobility) and class position in society. Nations differ on the political structure and policy options that govern the transfer of wealth.

According to the American federal government statistics compiled by Mark Zandi in 1985, the average US inheritance was $39,000. In subsequent years, the overall amount of total annual inheritance more than doubled, reaching nearly $200 billion. By 2050, there will be an estimated $25 trillion inheritance transmitted across generations.

Some researchers have attributed this rise to the baby boomer generation. Historically, the baby boomers were the largest influx of children conceived after WW2. For this reason, Thomas Shapiro suggests that this generation "is in the midst of benefiting from the greatest inheritance of wealth in history". Inherited wealth may help explain why many Americans who have become rich may have had a "substantial head start". In September 2012, according to the Institute for Policy Studies, "over 60 percent" of the Forbes richest 400 Americans "grew up in substantial privilege", and often (but not always) received substantial inheritances. .

Other research has shown that many inheritances, large or small, are rapidly squandered. Similarly, analysis shows that over two-thirds of high-wealth families lose their wealth within two generations, and almost 80% of high-wealth parents "feel the next generation is not financially responsible enough to handle inheritance".

It has been argued that inheritance plays a significant effect on social stratification. Inheritance is an integral component of family, economic, and legal institutions, and a basic mechanism of class stratification. It also affects the distribution of wealth at the societal level. The total cumulative effect of inheritance on stratification outcomes takes three forms, according to scholars who have examined the subject.

The first form of inheritance is the inheritance of cultural capital (i.e. linguistic styles, higher status social circles, and aesthetic preferences). The second form of inheritance is through familial interventions in the form of "inter vivos" transfers (i.e. gifts between the living), especially at crucial junctures in the life courses. Examples include during a child's milestone stages, such as going to college, getting married, getting a job, and purchasing a home. The third form of inheritance is the transfers of bulk estates at the time of death of the testators, thus resulting in significant economic advantage accruing to children during their adult years. The origin of the stability of inequalities is material (personal possessions one is able to obtain) and is also cultural, rooted either in varying child-rearing practices that are geared to socialization according to social class and economic position. Child-rearing practices among those who inherit wealth may center around favoring some groups at the expense of others at the bottom of the social hierarchy.

It is further argued that the degree to which economic status and inheritance is transmitted across generations determines one's life chances in society. Although many have linked one's social origins and educational attainment to life chances and opportunities, education cannot serve as the most influential predictor of economic mobility. In fact, children of well-off parents generally receive better schooling and benefit from material, cultural, and genetic inheritances. Likewise, schooling attainment is often persistent across generations and families with higher amounts of inheritance are able to acquire and transmit higher amounts of human capital. Lower amounts of human capital and inheritance can perpetuate inequality in the housing market and higher education. Research reveals that inheritance plays an important role in the accumulation of housing wealth. Those who receive an inheritance are more likely to own a home than those who do not regardless of the size of the inheritance.

Often, racial or religious minorities and individuals from socially disadvantaged backgrounds receive less inheritance and wealth. As a result, mixed races might be excluded in inheritance privilege and are more likely to rent homes or live in poorer neighborhoods, as well as achieve lower educational attainment compared with whites in America. Individuals with a substantial amount of wealth and inheritance often intermarry with others of the same social class to protect their wealth and ensure the continuous transmission of inheritance across generations; thus perpetuating a cycle of privilege.

Nations with the highest income and wealth inequalities often have the highest rates of homicide and disease (such as obesity, diabetes, and hypertension) which results in high mortality rates. A "The New York Times" article reveals that the U.S. is the world's wealthiest nation, but "ranks twenty-ninth in life expectancy, right behind Jordan and Bosnia" and "has the second highest mortality rate of the comparable OECD countries". This has been regarded as highly attributed to the significant gap of inheritance inequality in the country, although there are clearly other factors such as the affordability of healthcare.

When social and economic inequalities centered on inheritance are perpetuated by major social institutions such as family, education, religion, etc., these differing life opportunities are argued to be transmitted from each generation. As a result, this inequality is believed to become part of the overall social structure.

Dynastic wealth is monetary inheritance that is passed on to generations that didn't earn it. Dynastic wealth is linked to the term Plutocracy. Much has been written about the rise and influence of dynastic wealth including the bestselling book Capital in the Twenty-First Century by the French economist Thomas Piketty.Bill Gates uses the term in his article "Why Inequality Matters".

Many states have inheritance taxes or death duties, under which a portion of any estate goes to the government.




</doc>
<doc id="15435" url="https://en.wikipedia.org/wiki?curid=15435" title="Ignatius of Antioch">
Ignatius of Antioch

Ignatius of Antioch (; Greek: Ἰγνάτιος Ἀντιοχείας, "Ignátios Antiokheías"; died c. 108/140 AD), also known as Ignatius Theophorus (, "Ignátios ho Theophóros", "the God-bearing") or Ignatius Nurono ( "The fire-bearer"), was an early Christian writer and bishop of Antioch. While en route to Rome, where he met his martyrdom, Ignatius wrote a series of letters. This correspondence now forms a central part of a later collection of works known to be authored by the Apostolic Fathers. He is considered to be one of the three most important of these, together with Clement of Rome and Polycarp. His letters also serve as an example of early Christian theology. Important topics they address include ecclesiology, the sacraments, and the role of bishops.

Nothing is known of Ignatius' life apart from what may be inferred internally from his letters, except from later (sometimes spurious) traditions. It is said Ignatius converted to Christianity at a young age. Tradition identifies Ignatius, along with his friend Polycarp, as disciples of John the Apostle. Later in his life, Ignatius was chosen to serve as Bishop of Antioch; the fourth-century Church historian Eusebius writes that Ignatius succeeded Evodius. Theodoret of Cyrrhus claimed that St. Peter himself left directions that Ignatius be appointed to the episcopal see of Antioch. Ignatius called himself "Theophorus" (God Bearer). A tradition arose that he was one of the children whom Jesus Christ took in his arms and blessed, although if he was born around 50 AD, as supposed, then Jesus had been crucified approximately 20 years prior.

Ignatius' feast day was kept in his own Antioch on 17 October, the day on which he is now celebrated in the Catholic Church and generally in western Christianity, although from the 12th century until 1969 it was put at 1 February in the General Roman Calendar.

In the Eastern Orthodox Church it is observed on 20 December. The Synaxarium of the Coptic Orthodox Church of Alexandria places it on the 24th of the Coptic Month of Koiak (which is also the 24 day of the fourth month of Tahisas in the Synaxarium of The Ethiopian Orthodox Tewahedo Church), corresponding in three years out of every four to 20 December in the Julian Calendar, which currently falls on 2 January of the Gregorian Calendar.

Instead of being executed in his home town of Antioch, Ignatius was escorted to Rome by a company of ten Roman soldiers:

Scholars consider Ignatius' transport to Rome unusual, since those persecuted as Christians would be expected to be punished locally. Stevan Davies has pointed out that "no other examples exist from the Flavian age of any prisoners except citizens or prisoners of war being brought to Rome for execution."

If Ignatius were a Roman citizen, he could have appealed to the emperor, but then he would usually have been beheaded rather than tortured. Furthermore, the epistles of Ignatius state that he was put in chains during the journey to Rome, but it was illegal under Roman law for a citizen to be put in bonds during an appeal to the emperor.

Allen Brent argues that Ignatius was transferred to Rome at the request of the emperor in order to provide entertainment to the masses by being killed in the Colosseum. Brent insists, contrary to some, that "it was normal practice to transport condemned criminals from the provinces in order to offer spectator sport in the Colosseum at Rome."

Stevan Davies rejects the idea that Ignatius was transported to Rome for the games at the Colosseum. He reasons that "if Ignatius was in some way a donation by the Imperial Governor of Syria to the games at Rome, a single prisoner seems a rather miserly gift." Instead, Davies proposes that Ignatius may have been indicted by a legate, or representative, of the governor of Syria while the governor was away temporarily, and sent to Rome for trial and execution. Under Roman law, only the governor of a province or the emperor himself could impose capital punishment, so the legate would have faced the choice of imprisoning Ignatius in Antioch or sending him to Rome. Davies postulates that the legate may have decided to send Ignatius to Rome so as to minimize any further dissension among the Antiochene Christians.

Christine Trevett has called Davies' suggestion "entirely hypothetical" and concludes that no fully satisfactory solution to the problem can be found, writing, "I tend to take the bishop at his word when he says he is a condemned man. But the question remains, why is he going to Rome? The truth is that we do not know."

During the journey to Rome, Ignatius and his entourage of soldiers made a number of lengthy stops in Asia Minor, deviating from the most direct land route from Antioch to Rome. Scholars generally agree on the following reconstruction of Ignatius' route of travel:
During the journey, the soldiers seem to have allowed Ignatius to meet with entire congregations of Christians while in chains, at least while he was in Philadelphia (cf. Ign. Phil. 7), and numerous Christian visitors and messengers were allowed to meet with him on a one-on-one basis. These messengers allowed Ignatius to send six letters to nearby churches, and one to Polycarp, the bishop of Smyrna.

These aspects of Ignatius' martyrdom are also regarded by scholars as unusual. It is generally expected that a prisoner would be transported on the most direct, cost-effective route to their destination. Since travel by land in the Roman Empire was between five and fifty-two times more expensive than travel by sea, and Antioch was a major port city, the most efficient route would likely have been entirely by sea. Steven Davies argues that Ignatius' circuitous route to Rome can only be explained by positing that he was not the main purpose of the soldiers' trip, and that the various stops in Asia Minor were for other state business. He suggests that such a scenario would also explain the relative freedom that Ignatius was given to meet with other Christians during the journey.

Due to the sparse and fragmentary nature of the documentation of Ignatius' life and martyrdom, the date of his death is subject to a significant amount of uncertainty. Tradition places the martyrdom of Ignatius in the reign of Trajan, who was emperor of Rome from 98 to 117 AD. But the earliest source for this Trajanic date is the 4th century church historian Eusebius of Caesarea, who is regarded by some modern scholars as an unreliable source for chronological information regarding the early church. Eusebius had an ideological interest in dating church leaders as early as possible, and ensuring that there were no gaps in succession between the original apostles of Jesus and the leaders of the church in his day. Unfortunately, the epistles attributed to Ignatius provide no clear indication as to their date.

While many scholars accept the traditional dating of Ignatius' martyrdom under Trajan, others have argued for a somewhat later date. Richard Pervo dated Ignatius' death to 135-140 AD. British classicist Timothy Barnes has argued for a date in the 140s AD, on the grounds that Ignatius seems to have quoted a work of the Gnostic Ptolemy in one of his epistles, who only became active in the 130s.

Ignatius himself wrote that he would be thrown to the beasts, and in the fourth century Eusebius reports tradition that this came to pass, which is then repeated by Jerome who is the first to explicitly mention "lions." John Chrysostom is the first to allude to the Colosseum as the place of Ignatius' martyrdom. Contemporary scholars are uncertain that any of these authors had sources other than Ignatius' own writings.

According to a medieval Christian text titled "Martyrium Ignatii", Ignatius' remains were carried back to Antioch by his companions after his martyrdom. The sixth-century writings of Evagrius Scholasticus state that the reputed remains of Ignatius were moved by the Emperor Theodosius II to the Tychaeum, or Temple of Tyche, which had been converted into a church dedicated to Ignatius. In 637 the relics were transferred to the Basilica di San Clemente in Rome.

There is a purported eye-witness account of his martyrdom, named the "Martyrium Ignatii", of medieval date. It is presented as being an eye-witness account for the church of Antioch, attributed to Ignatius' companions, Philo of Cilicia, deacon at Tarsus, and Rheus Agathopus, a Syrian.

Although James Ussher regarded it as genuine, the authenticity of the account is seriously questioned. If there is any genuine nucleus of the "Martyrium", it has been so greatly expanded with interpolations that no part of it is without questions. Its most reliable manuscript is the 10th-century "Codex Colbertinus" (Paris), in which the "Martyrium" closes the collection. The "Martyrium" presents the confrontation of the bishop Ignatius with Trajan at Antioch, a familiar trope of "Acta" of the martyrs, and many details of the long, partly overland voyage to Rome. The Synaxarium of the Coptic Orthodox Church of Alexandria says that he was thrown to the wild beasts that devoured him and rent him to pieces.

The following seven epistles preserved under the name of Ignatius are generally considered authentic, since they were mentioned by the historian Eusebius in the first half of the fourth century.

Seven original epistles:

The text of these epistles is known in three different recensions, or editions: the Short Recension, found in a Syriac manuscript; the Middle Recension, found in Greek and Latin manuscripts; and the Long Recension, found in Latin manuscripts.

For some time, it was believed that the Long Recension was the only extant version of the Ignatian epistles, but around 1628 a Latin translation of the Middle Recension was discovered by Archbishop James Ussher, who published it in 1646. For around a quarter of a century after this, it was debated which recension represented the original text of the epistles. But ever since John Pearson's strong defense of the authenticity of the Middle Recension in the late 17th century, there has been a scholarly consensus that the Middle Recension is the original version of the text. The Long Recension is the product of a fourth-century Arian Christian, who interpolated the Middle Recension epistles in order to posthumously enlist Ignatius as an unwitting witness in theological disputes of that age. This individual also forged the six spurious epistles attributed to Ignatius (see below).

Manuscripts representing the Short Recension of the Ignatian epistles were discovered and published by William Cureton in the mid-19th century. For a brief period, there was a scholarly debate on the question of whether the Short Recension was earlier and more original than the Middle Recension. But by the end of the 19th century, Theodor Zahn and J. B. Lightfoot had established a scholarly consensus that the Short Recension is merely a summary of the text of the Middle Recension, and was therefore composed later.

Ever since the Protestant Reformation in the 16th century, the authenticity of all the Ignatian epistles has come under intense scrutiny. John Calvin called the epistles "rubbish published under Ignatius’ name." Some Protestants have tended to want to deny the authenticity of all the epistles attributed to Ignatius because they seem to attest to the existence of a monarchical episcopate in the second century. The Roman Catholic Church has long held up the authenticity of the letters from past to present. 

In 1886, Presbyterian minister and church historian William Dool Killen published an essay extensively arguing that none of the epistles attributed to Ignatius is authentic. Instead, he argued that Callixtus, bishop of Rome, forged the letters around AD 220 to garner support for a monarchical episcopate, modeling the renowned Saint Ignatius after his own life to give precedent for his own authority. Killen contrasted this episcopal polity with the presbyterian polity in the writings of Polycarp.

Some doubts about the authenticity of the original letters continued into the 20th century. In the late 1970s and 1980s, the scholars Robert Joly, Reinhard Hübner, Markus Vinzent, and Thomas Lenchner argued forcefully that the epistles of the Middle Recension were forgeries written during the reign of Marcus Aurelius (161-180 AD). Around the same time, the scholar Joseph Ruis-Camps published a study arguing that the Middle Recension letters were pseudepigraphically composed based on an original, smaller, authentic corpus of four letters (Romans, Magnesians, Trallians, and Ephesians). These publications stirred up tremendous, heated controversy in the scholarly community at the time, but today most scholars accept the authenticity of the seven original epistles.

The original text of six of the seven original letters are found in the Codex Mediceo Laurentianus written in Greek in the 11th century (which also contains the pseudepigraphical letters of the Long Recension, except that to the Philippians), while the letter to the Romans is found in the Codex Colbertinus.

Ignatius's letters bear signs of being written in great haste and without a proper plan, such as run-on sentences and an unsystematic succession of thought. Ignatius modeled his writings after Paul, Peter, and John, and even quoted or paraphrased their own works freely, such as when he quoted 1 Corinthians 1:18, in his letter to the Ephesians:
Ignatius is known to have taught the deity of Christ: 
The same section in text of the Long Recension says the following:

He stressed the value of the Eucharist, calling it a "medicine of immortality" ("Ignatius to the Ephesians" 20:2). The very strong desire for bloody martyrdom in the arena, which Ignatius expresses rather graphically in places, may seem quite odd to the modern reader. An examination of his theology of soteriology shows that he regarded salvation as one being free from the powerful fear of death and thus to bravely face martyrdom.

Ignatius is claimed to be the first known Christian writer to argue in favor of Christianity's replacement of the Sabbath with the Lord's Day:

Ignatius is the earliest known Christian writer to emphasize loyalty to a single bishop in each city (or diocese) who is assisted by both presbyters (priests) and deacons. Earlier writings only mention "either" bishops "or" presbyters.

For instance, his writings on bishops, presbyters and deacons:

He is also responsible for the first known use of the Greek word "katholikos" (καθολικός), meaning "universal", "complete" and "whole" to describe the Church, writing:

It is from the word "katholikos" ("according to the whole") that the word "catholic" comes. When Ignatius wrote the Letter to the Smyrnaeans in about the year 107 and used the word "catholic", he used it as if it were a word already in use to describe the Church. This has led many scholars to conclude that the appellation "Catholic Church" with its ecclesial connotation may have been in use as early as the last quarter of the first century. On the Eucharist, he wrote in his letter to the Smyrnaeans:

In his letter addressed to the Christians of Rome, he entreats to do nothing to prevent his martyrdom.

Several scholars have noted that there are striking similarities between Ignatius and the Christian-turned-Cynic philosopher Peregrinus Proteus, as described in Lucian's famous satire "The Passing of Peregrinus":
It is generally believed that these parallels are the result of Lucian intentionally copying traits from Ignatius and applying them to his satire of Peregrinus. If the dependence of Lucian on the Ignatian epistles is accepted, then this places an upper limit on the date of the epistles: around the 160s AD, just before "The Passing of Peregrinus" was written.

In 1892, Daniel Völter sought to explain the parallels by proposing that the Ignatian epistles were in fact "written" by Peregrinus, and later edited to conceal their provenance, but this speculative theory has failed to make a significant impact on the academic community.

Epistles attributed to Saint Ignatius but of spurious origin (their author is often called Pseudo-Ignatius in English) include:





</doc>
<doc id="15437" url="https://en.wikipedia.org/wiki?curid=15437" title="ITU prefix">
ITU prefix

The International Telecommunication Union (ITU) allocates call sign prefixes for radio and television stations of all types. They also form the basis for, but do not exactly match, aircraft registration identifiers. These prefixes are agreed upon internationally, and are a form of country code. A call sign can be any number of letters and numerals but each country must only use call signs that begin with the characters allocated for use in that country.

A few countries do not fully comply with these rules. Australian broadcast stations officially have—but do not use—the VL prefix, and Canada uses Chile's CB for its own Canadian Broadcasting Corporation stations. This is through a special agreement with the government of Chile, which is officially assigned the CB prefix.

With regard to the second and/or third letters in the prefixes in the list below, if the country in question is allocated all callsigns with A to Z in that position, then that country can also use call signs with the digits 0 to 9 in that position. For example, the United States is assigned KA–KZ, and therefore can also use prefixes like KW0 or K1.

Many large countries in turn have internal rules on how and where specific subsets of their callsigns can be used (such as Mexico's XE for AM and XH for FM radio and television broadcasting), which are not covered here.

Unallocated: The following call sign prefixes are available for future allocation by the ITU. ("x" represents any letter; "n" represents any digit from 2–9.)


Unavailable: Under present ITU guidelines the following call sign prefixes shall not be allocated. They are sometimes used unofficially – such as amateur radio operators operating in a disputed territory or in a nation state that has no official prefix (e.g. S0 in Western Sahara, station 1A0 at Knights of Malta headquarters in Rome, or station 1L in Liberland). ("x" represents any letter; "n" represents any digit from 2–9.)





</doc>
<doc id="15440" url="https://en.wikipedia.org/wiki?curid=15440" title="IBM PC keyboard">
IBM PC keyboard

The keyboard for IBM PC-compatible computers is standardized. However, during the more than 30 years of PC architecture being frequently updated, many keyboard layout variations have been developed.

A well-known class of IBM PC keyboards is the Model M. Introduced in 1984 and manufactured by IBM, Lexmark, Maxi-Switch and Unicomp, the vast majority of Model M keyboards feature a buckling spring key design and many have fully swappable keycaps.

The PC keyboard changed over the years, often at the launch of new IBM PC versions.

Common additions to the standard layouts include additional power management keys, volume controls, media player controls, and miscellaneous user-configurable shortcuts for email client, World Wide Web browser, etc.

The IBM PC layout, particularly the Model M, has been extremely influential, and today most keyboards use some variant of it. This has caused problems for applications developed with alternative layouts, which require keys that are in awkward positions on the Model M layout – often requiring the pinkie to operate – and thus require remapping for comfortable use. One notable example is the escape key, used by the vi editor: on the ADM-3A terminal this was located where the Tab key is on the IBM PC, but on the IBM PC the Escape key is in the corner; this is typically solved by remapping Caps Lock to Escape. Another example is the Emacs editor, which makes extensive use of modifier keys, and uses the Control key more than the meta key (IBM PC instead has the Alt key) – these date to the Knight keyboard, which had the Control key on the "inside" of the Meta key, opposite to the Model M, where it is on the "outside" of the Alt key; and to the space-cadet keyboard, where the four bucky bit keys (Control, Meta, Super, Hyper) are in a row, allowing easy chording to press several, unlike on the Model M layout. This results in the "Emacs pinky" problem.

Although "PC Magazine" praised most aspects of the 1981 IBM PC keyboard's hardware design, it questioned "how IBM, that ultimate pro of keyboard manufacture, could put the left-hand shift key at the awkward reach they did". The magazine reported in 1982 that it received more letters to its "Wish List" column asking for the ability to determine the status of the three lock keys than on any other topic. "Byte" columnist Jerry Pournelle described the keyboard as "infuriatingly excellent". He praised its feel but complained that the Shift and other keys' locations were "enough to make a saint weep", and denounced the trend of PC compatible computers to emulate the layout but not the feel. He reported that the layout "nearly drove" science-fiction editor Jim Baen "crazy", and that "many of [Baen's] authors refused to work with that keyboard" so could not submit manuscripts in a compatible format. The magazine's official review was more sanguine. It praised the keyboard as "bar none, the best ... on any microcomputer" and described the unusual Shift key locations as "minor [problems] compared to some of the gigantic mistakes made on almost every other microcomputer keyboard".

"I wasn't thrilled with the placement of [the left Shift and Return] keys, either", IBM's Don Estridge stated in 1983. He defended the layout, however, stating that "every place you pick to put them is not a good place for somebody ... there's no consensus", and claimed that "if we were to change it now we would be in hot water".

The PC keyboard with its various keys has a long history of evolution reaching back to teletypewriters. In addition to the 'old' standard keys, the PC keyboard has accumulated several special keys over the years. Some of the additions have been inspired by the opportunity or requirement for improving user productivity with general office application software, while other slightly more general keyboard additions have become the factory standards after being introduced by certain operating system or GUI software vendors such as Microsoft.







</doc>
<doc id="15441" url="https://en.wikipedia.org/wiki?curid=15441" title="Italian battleship Giulio Cesare">
Italian battleship Giulio Cesare

"Giulio Cesare (Julius Caesar)" was one of three dreadnought battleships built for the Royal Italian Navy () in the 1910s. Completed in 1914, she was little used and saw no combat during the First World War. The ship supported operations during the Corfu Incident in 1923 and spent much of the rest of the decade in reserve. She was rebuilt between 1933 and 1937 with more powerful guns, additional armor and considerably more speed than before.

During World War II, both "Giulio Cesare" and her sister ship, , participated in the Battle of Calabria in July 1940, when the former was lightly damaged. They were both present when British torpedo bombers attacked the fleet at Taranto in November 1940, but "Giulio Cesare" was not damaged. She escorted several convoys to North Africa and participated in the Battle of Cape Spartivento in late 1940 and the First Battle of Sirte in late 1941. She was designated as a training ship in early 1942, and escaped to Malta after the Italian armistice the following year. The ship was transferred to the Soviet Union in 1949 and renamed Novorossiysk (). The Soviets also used her for training until she was sunk in 1955, with the loss of 608 men, when an old German mine exploded. She was salvaged the following year and later scrapped.

The "Conte di Cavour" class was designed to counter the French dreadnoughts which caused them to be slower and more heavily armored than the first Italian dreadnought, . The ships were long at the waterline and overall. They had a beam of , and a draft of . The "Conte di Cavour"-class ships displaced at normal load, and at deep load. They had a crew of 31 officers and 969 enlisted men. The ships were powered by three sets of Parsons steam turbines, two sets driving the outer propeller shafts and one set the two inner shafts. Steam for the turbines was provided by 24 Babcock & Wilcox boilers, half of which burned fuel oil and the other half burning both oil and coal. Designed to reach a maximum speed of from , "Giulio Cesare" failed to reach this goal on her sea trials, reaching only from . The ships carried enough coal and oil to give them a range of at .

The main battery of the "Conte di Cavour" class consisted of thirteen 305-millimeter Model 1909 guns, in five centerline gun turrets, with a twin-gun turret superfiring over a triple-gun turret in fore and aft pairs, and a third triple turret amidships. Their secondary armament consisted of eighteen guns mounted in casemates on the sides of the hull. For defense against torpedo boats, the ships carried fourteen guns; thirteen of these could be mounted on the turret tops, but they could be positioned in 30 different locations, including some on the forecastle and upper decks. They were also fitted with three submerged torpedo tubes, one on each broadside and the third in the stern. 

The "Conte di Cavour"-class ships had a complete waterline armor belt that had a maximum thickness of amidships, which reduced to towards the stern and towards the bow. They had two armored decks: the main deck was thick on the flat that increased to on the slopes that connected it to the main belt. The second deck was thick. Frontal armor of the gun turrets was in thickness and the sides were thick. The armor protecting their barbettes ranged in thickness from . The walls of the forward conning tower were 280 millimeters thick.

Shortly after the end of World War I, the number of 76.2 mm guns was reduced to 13, all mounted on the turret tops, and six new 76.2-millimeter anti-aircraft (AA) guns were installed abreast the aft funnel. In addition two license-built 2-pounder () AA guns were mounted on the forecastle deck. In 1925–1926 the foremast was replaced by a four-legged (tetrapodal) mast, which was moved forward of the funnels, the rangefinders were upgraded, and the ship was equipped to handle a Macchi M.18 seaplane mounted on the amidships turret. Around that same time, either one or both of the ships was equipped with a fixed aircraft catapult on the port side of the forecastle.

"Giulio Cesare" began an extensive reconstruction in October 1933 at the Cantieri del Tirreno shipyard in Genoa that lasted until October 1937. A new bow section was grafted over the existing bow which increased her length by to and her beam increased to . The ship's draft at deep load increased to . All of the changes made increased her displacement to at standard load and at deep load. The ship's crew increased to 1,260 officers and enlisted men. Two of the propeller shafts were removed and the existing turbines were replaced by two Belluzzo geared steam turbines rated at . The boilers were replaced by eight Yarrow boilers. On her sea trials in December 1936, before her reconstruction was fully completed, "Giulio Cesare" reached a speed of from . In service her maximum speed was about and she had a range of at a speed of .

The main guns were bored out to and the center turret and the torpedo tubes were removed. All of the existing secondary armament and AA guns were replaced by a dozen 120 mm guns in six twin-gun turrets and eight AA guns in twin turrets. In addition the ship was fitted with a dozen Breda light AA guns in six twin-gun mounts and twelve Breda M31 anti-aircraft machine guns, also in twin mounts. In 1940 the 13.2 mm machine guns were replaced by AA guns in twin mounts. "Giulio Cesare" received two more twin mounts as well as four additional 37 mm guns in twin mounts on the forecastle between the two turrets in 1941. The tetrapodal mast was replaced with a new forward conning tower, protected with thick armor. Atop the conning tower there was a fire-control director fitted with two large stereo-rangefinders, with a base length of .

The deck armor was increased during the reconstruction to a total of over the engine and boiler rooms and over the magazines, although its distribution over three decks, meant that it was considerably less effective than a single plate of the same thickness. The armor protecting the barbettes was reinforced with plates. All this armor weighed a total of . The existing underwater protection was replaced by the Pugliese torpedo defense system that consisted of a large cylinder surrounded by fuel oil or water that was intended to absorb the blast of a torpedo warhead. It lacked, however, enough depth to be fully effective against contemporary torpedoes. A major problem of the reconstruction was that the ship's increased draft meant that their waterline armor belt was almost completely submerged with any significant load.

"Giulio Cesare", named after Julius Caesar, was laid down at the Gio. Ansaldo & C. shipyard in Genoa on 24 June 1910 and launched on 15 October 1911. She was completed on 14 May 1914 and served as a flagship in the southern Adriatic Sea during World War I. She saw no action, however, and spent little time at sea. Admiral Paolo Thaon di Revel, the Italian naval chief of staff, believed that Austro-Hungarian submarines and minelayers could operate too effectively in the narrow waters of the Adriatic. The threat from these underwater weapons to his capital ships was too serious for him to use the fleet in an active way. Instead, Revel decided to implement a blockade at the relatively safer southern end of the Adriatic with the battle fleet, while smaller vessels, such as the MAS torpedo boats, conducted raids on Austro-Hungarian ships and installations. Meanwhile, Revel's battleships would be preserved to confront the Austro-Hungarian battle fleet in the event that it sought a decisive engagement.

"Giulio Cesare" made port visits in the Levant in 1919 and 1920. Both "Giulio Cesare" and "Conte di Cavour" supported Italian operations on Corfu in 1923 after an Italian general and his staff were murdered at the Greek–Albanian frontier; Benito Mussolini, who had been looking for a pretext to seize Corfu, ordered Italian troops to occupy the island. "Cesare" became a gunnery training ship in 1928, after having been in reserve since 1926. She was reconstructed at Cantieri del Tirreno, Genoa, between 1933 and 1937. Both ships participated in a naval review by Adolf Hitler in the Bay of Naples in May 1938 and covered the invasion of Albania in May 1939.

Early in World War II, the ship took part in the Battle of Calabria (also known as the Battle of Punto Stilo), together with "Conte di Cavour", on 9 July 1940, as part of the 1st Battle Squadron, commanded by Admiral Inigo Campioni, during which she engaged major elements of the British Mediterranean Fleet. The British were escorting a convoy from Malta to Alexandria, while the Italians had finished escorting another from Naples to Benghazi, Libya. Admiral Andrew Cunningham, commander of the Mediterranean Fleet, attempted to interpose his ships between the Italians and their base at Taranto. Crew on the fleets spotted each other in the middle of the afternoon and the battleships opened fire at 15:53 at a range of nearly . The two leading British battleships, and , replied a minute later. Three minutes after she opened fire, shells from "Giulio Cesare" began to straddle "Warspite" which made a small turn and increased speed, to throw off the Italian ship's aim, at 16:00. Some rounds fired by "Giulio Cesare" overshot "Warspite" and near-missed the destroyers HMS "Decoy" and "Hereward", puncturing their superstructures with splinters. At that same time, a shell from "Warspite" struck "Giulio Cesare" at a distance of about . The shell pierced the rear funnel and detonated inside it, blowing out a hole nearly across. Fragments started several fires and their smoke was drawn into the boiler rooms, forcing four boilers off-line as their operators could not breathe. This reduced the ship's speed to . Uncertain how severe the damage was, Campioni ordered his battleships to turn away in the face of superior British numbers and they successfully disengaged. Repairs to "Giulio Cesare" were completed by the end of August and both ships unsuccessfully attempted to intercept British convoys to Malta in August and September.

On the night of 11 November 1940, "Giulio Cesare" and the other Italian battleships were at anchor in Taranto harbor when they were attacked by 21 Fairey Swordfish torpedo bombers from the British aircraft carrier , along with several other warships. One torpedo sank "Conte di Cavour" in shallow water, but "Giulio Cesare" was not hit during the attack. She participated in the Battle of Cape Spartivento on 27 November 1940, but never got close enough to any British ships to fire at them. The ship was damaged in January 1941 by splinters from a near miss during an air raid on Naples by Vickers Wellington bombers of the Royal Air Force; repairs at Genoa were completed in early February. On 8 February, she sailed from to the Straits of Bonifacio to intercept what the Italians thought was a Malta convoy, but was actually a raid on Genoa. She failed to make contact with any British forces. She participated in the First Battle of Sirte on 17 December 1941, providing distant cover for a convoy bound for Libya, and briefly engaging the escort force of a British convoy. She also provided distant cover for another convoy to North Africa in early January 1942. "Giulio Cesare" was reduced to a training ship afterwards at Taranto and later Pola. After the Italian surrender on 9 September 1943, she steamed to Taranto, putting down a mutiny and enduring an ineffective attack by five German aircraft en route. She then sailed for Malta where she arrived on 12 September to be interned. The ship remained there until 17 June 1944 when she returned to Taranto where she remained for the next four years.

After the war, "Giulio Cesare" was allocated to the Soviet Union as part of the war reparations. She was moved to Augusta, Sicily, on 9 December 1948, where an unsuccessful attempt was made at sabotage. The ship was stricken from the naval register on 15 December and turned over to the Soviets on 6 February 1949 under the temporary name of "Z11" in Vlorë, Albania. She was renamed "Novorossiysk", after the Soviet city of that name on the Black Sea. The Soviets used her as a training ship, and gave her eight refits. In 1953, all Italian light AA guns were replaced by eighteen 37 mm 70-K AA guns in six twin mounts and six singles. Also replaced were her fire-control systems and radars. The Soviets intended to rearm her with their own 305 mm guns, but this was forestalled by her loss. While at anchor in Sevastopol on the night of 28/29 October 1955, an explosion ripped a hole in the forecastle forward of 'A' turret. The flooding could not be controlled, and she capsized with the loss of 608 men, including men sent from other ships to assist.

The cause of the explosion is still unclear. The official cause, regarded as the most probable, was a magnetic RMH or LMB bottom mine, laid by the Germans during World War II and triggered by the dragging of the battleship's anchor chain before mooring for the last time. Subsequent searches located 32 mines of these types, some of them within of the explosion. The damage was consistent with an explosion of of TNT, and more than one mine may have detonated. Other explanations for the ship's loss have been proposed, and the most popular of these is that she was sunk by Italian frogmen of the wartime special operations unit "Decima Flottiglia MAS" who – more than ten years after the cessation of hostilities – were either avenging the transfer of the former Italian battleship to the USSR or sinking it on behalf of NATO. "Novorossiysk" was stricken from the naval register on 24 February 1956, salvaged on 4 May 1957, and subsequently scrapped.



</doc>
<doc id="15442" url="https://en.wikipedia.org/wiki?curid=15442" title="INS Vikrant (R11)">
INS Vikrant (R11)

INS "Vikrant (from Sanskrit "vikrānta", "courageous") was a of the Indian Navy. The ship was laid down as HMS "Hercules for the British Royal Navy during World War II, but construction was put on hold when the war ended. India purchased the incomplete carrier in 1957, and construction was completed in 1961. "Vikrant" was commissioned as the first aircraft carrier of the Indian Navy and played a key role in enforcing the naval blockade of East Pakistan during the Indo-Pakistani War of 1971.

In its later years, the ship underwent major refits to embark modern aircraft, before being decommissioned in January 1997. She was preserved as a museum ship in Cuffe Parade, Mumbai until 2012. In January 2014, the ship was sold through an online auction and scrapped in November 2014 after final clearance from the Supreme Court.

In 1943 the Royal Navy commissioned six light aircraft carriers in an effort to counter the German and Japanese navies. The 1942 Design Light Fleet Carrier, commonly referred to as the British Light Fleet Carrier, was the result. Serving with eight navies between 1944 and 2001, these ships were designed and constructed by civilian shipyards as an intermediate step between the full-sized fleet aircraft carriers and the less expensive but limited-capability escort carriers.

Sixteen light fleet carriers were ordered, and all were laid down as what became the "Colossus" class in 1942 and 1943. The final six ships were modified during construction to handle larger and faster aircraft, and were re-designated the "Majestic" class. The improvements from the "Colossus" class to the "Majestic" class included heavier displacement, armament, catapult, aircraft lifts and aircraft capacity. Construction on the ships was suspended at the end of World War II, as the ships were surplus to the Royal Navy's peacetime requirements.
Instead, the carriers were modernized and sold to several Commonwealth nations. The ships were similar, but each varied depending on the requirements of the country to which the ship was sold.

HMS "Hercules", the fifth ship in the "Majestic" class, was ordered on 7 August 1942 and laid down on 14 October 1943 by Vickers-Armstrongs on the River Tyne. After World War II ended with Japan's surrender on 2 September 1945, she was launched on 22 September, and her construction was suspended in May 1946. At the time of suspension, she was 75 per cent complete. Her hull was preserved, and in May 1947 she was laid up in Gareloch off the Clyde. In January 1957, she was purchased by India and was towed to Belfast to complete her construction and modifications by Harland and Wolff. Several improvements to the original design were ordered by the Indian Navy, including an angled deck, steam catapults, and a modified island.

"Vikrant" displaced at standard load and at deep load. She had an overall length of , a beam of and a mean deep draught of . She was powered by a pair of Parsons geared steam turbines, driving two propeller shafts, using steam provided by four Admiralty three-drum boilers. The turbines developed a total of which gave a maximum speed of . "Vikrant" carried about of fuel oil that gave her a range of at , and at . The air and ship crew comprised 1,110 officers and men.

The ship was armed with sixteen Bofors anti-aircraft guns, but these were later reduced to eight. At various times, its aircraft consisted of Hawker Sea Hawk and Sea Harrier (STOVL) jet fighters, Sea King Mk 42B and HAL Chetak helicopters, and Breguet Alizé Br.1050 anti-submarine aircraft. The carrier fielded between 21 and 23 aircraft of all types. "Vikrant"s flight decks were designed to handle aircraft up to , but remained the heaviest landing weight of an aircraft. Larger lifts were installed.

The ship was equipped with one LW-05 air-search radar, one ZW-06 surface-search radar, one LW-10 tactical radar and one Type 963 aircraft landing radar with other communication systems.

The Indian Navy's first aircraft carrier was commissioned as INS "Vikrant" on 4 March 1961 in Belfast by Vijaya Lakshmi Pandit, the Indian High Commissioner to the United Kingdom. The name "Vikrant" was derived from the Sanskrit word "vikrānta" meaning "stepping beyond", "courageous" or "bold". Captain Pritam Singh Mahindroo was the first commanding officer of the ship, which carried British Hawker Sea Hawk fighter-bombers and French Alizé anti-submarine aircraft. On 18 May 1961, the first jet landed on her deck. It was piloted by Lieutenant Radhakrishna Hariram Tahiliani, who later served as admiral and Chief of the Naval Staff of India from 1984 to 1987. "Vikrant" formally joined the Indian Navy's fleet in Bombay (now Mumbai) on 3 November 1961, when she was received at Ballard Pier by then Prime Minister Jawaharlal Nehru.

In December of that year, the ship was deployed for Operation Vijay (the code name for the annexation of Goa) off the coast of Goa with two destroyers, and . "Vikrant" did not see action, and patrolled along the coast to deter foreign interference. During the Indo-Pakistani War of 1965, "Vikrant" was in dry dock refitting, and did not see any action.

In June 1970, "Vikrant" was docked at the Naval Dockyard, Bombay, due to many internal fatigue cracks and fissures in the water drums of her boilers that could not be repaired by welding. As replacement drums were not available locally, four new ones were ordered from Britain, and Naval Headquarters issued orders not to use the boilers until further notice. On 26 February 1971 the ship was moved from Ballard Pier Extension to the anchorage, without replacement drums. The main objective behind this move was to light up the boilers at reduced pressure, and work up the main and flight deck machinery that had been idle for almost seven months. On 1 March, the boilers were ignited, and basin trials up to 40 revolutions per minute (RPM) were conducted. Catapult trials were conducted on the same day.

The ship began preliminary sea trials on 18 March and returned two days later. Trials were again conducted on 26–27 April. The navy decided to limit the boilers to a pressure of and the propeller revolutions to 120 RPM ahead and 80 RPM astern, reducing the ship's speed to . With the growing expectations of a war with Pakistan in the near future, the navy started to transfer its ships to strategically advantageous locations in Indian waters. The primary concern of Naval Headquarters about the operation was the serviceability of "Vikrant". When asked his opinion regarding the involvement of "Vikrant" in the war, Fleet Operations Officer Captain Gulab Mohanlal Hiranandani told the Chief of the Naval Staff Admiral Sardarilal Mathradas Nanda:

Nanda and Hiranandani proved to be instrumental in taking "Vikrant" to war. There were objections that the ship might have severe operational difficulties that would expose the carrier to increased danger on operations. In addition, the three s acquired by the Pakistan Navy posed a significant risk to the carrier. In June, extensive deep sea trials were carried out, with steel safety harnesses around the three boilers still operational. Observation windows were fitted as a precautionary measure, to detect any steam leaks. By the end of June, the trials were complete and "Vikrant" was cleared to participate on operations, with its speed restricted to 14 knots.

As a part of preparations for the war, "Vikrant" was assigned to the Eastern Naval Command, then to the Eastern Fleet. This fleet consisted of INS "Vikrant", the two s and , the two Petya III-class corvettes and , and one submarine, . The main reason behind strengthening the Eastern Fleet was to counter the Pakistani maritime forces deployed in support of military operations in East Bengal. A surveillance area of , confined by a triangle with a base of and sides of and , was set up in the Bay of Bengal. Any ship in this area was to be challenged and checked. If found to be neutral, it would be escorted to the nearest Indian port, otherwise, it would be captured, and taken as a war prize.

In the meantime, intelligence reports confirmed that Pakistan was to deploy a US-built , . "Ghazi" was considered as a serious threat to "Vikrant" by the Indian Navy, as "Vikrant"s approximate position would be known by the Pakistanis once she started operating aircraft. Of the four available surface ships, INS "Kavaratti" had no sonar, which meant that the other three had to remain in close vicinity of "Vikrant", without which the carrier would be completely vulnerable to attack by "Ghazi".

On 23 July, "Vikrant" sailed off to Cochin in company with the Western Fleet. En route, before reaching Cochin on 26 July, Sea King landing trials were carried out. After the completion of the radar and communication trials on 28 July, she departed for Madras, escorted by "Brahmaputra" and "Beas". The next major problem was operating aircraft from the carrier. The commanding officer of the ship, Captain (later Vice Admiral) S. Prakash, was seriously concerned about flight operations. He was concerned that aircrew morale would be adversely affected if flight operations were not undertaken, which could be disastrous. Naval Headquarters remained stubborn on the speed restrictions, and sought confirmation from Prakash whether it was possible to embark an Alizé without compromising the speed restrictions. The speed restrictions imposed by the headquarters meant that Alizé aircraft would have to land at close to stalling speed. Eventually the aircraft weight was reduced, which allowed several of the aircraft to embark, along with a Seahawk squadron.

By the end of September, "Vikrant" and her escorts reached Port Blair. En route to Visakhapatnam, tactical exercises were conducted in the presence of the Flag Officer Commanding-in-Chief of the Eastern Naval Command. From Vishakhapatnam, "Vikrant" set out for Madras for maintenance. Rear Admiral S. H. Sharma was appointed Flag Officer Commanding Eastern Fleet and arrived at Vishakhapatnam on 14 October. After receiving the reports that Pakistan might launch preemptive strikes, maintenance was stopped for another tactical exercise, which was completed during the night of 26–27 October at Vishakhapatnam. "Vikrant" then returned to Madras to resume maintenance. On 1 November, the Eastern Fleet was formally constituted, and on 13 November, all the ships set out for the Andaman and Nicobar Islands. To avoid misadventures, it was planned to sail "Vikrant" to a remote anchorage, isolating it from combat. Simultaneously, deception signals would give the impression that "Vikrant" was operating somewhere between Madras and Vishakhapatnam.

On 23 November, an emergency was declared in Pakistan after a clash of Indian and Pakistani troops in East Pakistan two days earlier. On 2 December, the Eastern Fleet proceeded to its patrol area in anticipation of an attack by Pakistan. The Pakistan Navy had deployed "Ghazi" on 14 November with the explicit goal of targeting and sinking "Vikrant", and "Ghazi" reached a location near Madras by the 23rd. In an attempt to deceive the Pakistan Navy and "Ghazi", India's Naval Headquarters deployed "Rajput" as a decoy—the ship sailed off the coast of Vishakhapatnam and broadcast a significant amount of radio traffic, making her appear to be "Vikrant".

"Ghazi", meanwhile, sank off the Visakhapatnam coast under mysterious circumstances. On the night of 3–4 December, a muffled underwater explosion was detected by a coastal battery. The next morning, a local fisherman observed flotsam near the coast, causing Indian naval officials to suspect a vessel had sunk off the coast. The next day, a clearance diving team was sent to search the area, and they confirmed that "Ghazi" had sunk in shallow waters.

The reason for "Ghazi"s fate is unclear. The Indian Navy's official historian, Hiranandani, suggests three possibilities, after having analysed the position of the rudder and extent of the damage suffered. The first was that "Ghazi" had come up to periscope depth to identify her position and may have seen an anti-submarine vessel that caused her to crash dive, which in turn may have led her to bury her bow in the bottom. The second possibility is closely related to the first: on the night of the explosion, "Rajput" was on patrol off Visakhapatnam and observed a severe disturbance in the water. Suspecting that it was a submarine, the ship dropped two depth charges on the spot, on a position that was very close to the wreckage. The third possibility is that there was a mishap when "Ghazi" was laying mines on the day before hostilities broke out.

"Vikrant" was redeployed towards Chittagong at the outbreak of hostilities. On 4 December, the ship's Sea Hawks struck shipping in Chittagong and Cox's Bazar harbours, sinking or incapacitating most of the ships present. Later strikes targeted Khulna and the Port of Mongla, which continued until 10 December, while other operations were flown to support a naval blockade of East Pakistan. On 14 December, the Sea Hawks attacked the cantonment area in Chittagong, destroying several Pakistani army barracks. Medium anti-aircraft fire was encountered during this strike. Simultaneous attacks by Alizés continued on Cox's Bazar. After this, "Vikrant"s fuel levels dropped to less than 25 per cent, and the aircraft carrier sailed to Paradip for refueling. The crew of INS "Vikrant" earned two Maha Vir Chakras and twelve Vir Chakra gallantry medals for their part in the war.

"Vikrant" did not see much service after the war, and was given two major modernisation refits—the first one from 1979 to 1981 and the second one from 1987 to 1989. In the first phase, her boilers, radars, communication systems and anti-aircraft guns were modernised, and facilities to operate Sea Harriers were installed. In the second phase, facilities to operate the new Sea Harrier Vertical/Short Take Off and Land (V/STOL) fighter aircraft and the new Sea King Mk 42B Anti-Submarine Warfare (ASW) helicopters were introduced. A 9.75-degree ski-jump ramp was fitted. The steam catapult was removed during this phase. Again in 1991, "Vikrant" underwent a six-month refit, followed by another fourteen-month refit in 1992–94. She remained operational thereafter, flying Sea Harriers, Sea Kings and Chetaks until her final sea outing on 23 November 1994. In the same year, a fire was also recorded aboard. In January 1995, the navy decided to keep "Vikrant" in "safe to float" state. She was laid up and formally decommissioned on 31 January 1997.

During her service, INS "Vikrant" embarked four squadrons of the Naval Air Arm of the Indian Navy:

Following decommissioning in 1997, the ship was earmarked for preservation as a museum ship in Mumbai. Lack of funding prevented progress on the ship's conversion to a museum and it was speculated that the ship would be made into a training ship. In 2001, the ship was opened to the public by the Indian Navy, but the Government of Maharashtra was unable to find a partner to operate the museum on a permanent, long-term basis and the museum was closed after it was deemed unsafe for the public in 2012.

In August 2013, Vice-Admiral Shekhar Sinha, chief of the Western Naval Command, said the Ministry of Defence would scrap the ship as she had become very difficult to maintain and no private bidders had offered to fund the museum's operations. On 3 December 2013, the Indian government decided to auction the ship. The Bombay High Court dismissed a public-interest lawsuit filed by Kiran Paigankar to stop the auction, stating the vessel's dilapidated condition did not warrant her preservation, nor were the necessary funds or government support available.

In January 2014, the ship was sold through an online auction to a Darukhana ship-breaker for . The Supreme Court of India dismissed another lawsuit challenging the ship's sale and scrapping on 14 August 2014. "Vikrant" remained beached off Darukhana in Mumbai Port while awaiting the final clearances of the Mumbai Port Trust. On 12 November 2014, the Supreme Court gave its final approval for the carrier to be scrapped, which commenced on 22 November 2014.

In memory of "Vikrant", the Vikrant Memorial was unveiled by Vice Admiral Surinder Pal Singh Cheema, Flag Officer Commanding-in-Chief of the Western Naval Command at K Subash Marg in the Naval Dockyard of Mumbai on 25 January 2016. The memorial is made from metal recovered from the ship.
In February 2016, Bajaj unveiled a new motorbike made with metal from "Vikrant"s scrap and named it Bajaj V in honour of "Vikrant".

The navy has named its first home-built carrier INS "Vikrant" in honour of INS "Vikrant" (R11). The new carrier is built by Cochin Shipyard Limited, and will displace . The keel was laid down in February 2009 and she was launched in August 2013. , the ship is being fitted out and is expected to be commissioned by the end of 2018.

The decommissioned ship featured prominently in the film "ABCD 2" as a backdrop while it was moored near Darukhana in Mumbai.





</doc>
<doc id="15443" url="https://en.wikipedia.org/wiki?curid=15443" title="Western imperialism in Asia">
Western imperialism in Asia

Western imperialism in Asia involves the influence of people from Western Europe and associated states (such as Russia, Japan and the United States) in Asian territories and waters. Much of this process stemmed from the 15th-century search for trade routes to China that led directly to the Age of Discovery, and the introduction of early modern warfare into what Europeans first called the East Indies and later the Far East. By the early 16th century, the Age of Sail greatly expanded Western European influence and development of the spice trade under colonialism. European-style colonial empires and imperialism operated in Asia throughout six centuries of colonialism, formally ending with the independence of the Portuguese Empire's last colony East Timor in 2002. The empires introduced Western concepts of nation and the multinational state. This article attempts to outline the consequent development of the Western concept of the nation state.

The thrust of European political power, commerce, and culture in Asia gave rise to growing trade in commodities—a key development in the rise of today's modern world free market economy. In the 16th century, the Portuguese broke the (overland) monopoly of the Arabs and Italians in trade between Asia and Europe by the discovery of the sea route to India around the Cape of Good Hope. The ensuing rise of the rival Dutch East India Company gradually eclipsed Portuguese influence in Asia. Dutch forces first established independent bases in the East (most significantly Batavia, the heavily fortified headquarters of the Dutch East India Company) and then between 1640 and 1660 wrested Malacca, Ceylon, some southern Indian ports, and the lucrative Japan trade from the Portuguese. Later, the English and the French established settlements in India and established trade with China and their acquisitions would gradually surpass those of the Dutch. Following the end of the Seven Years' War in 1763, the British eliminated French influence in India and established the British East India Company (founded in 1600) as the most important political force on the Indian Subcontinent.

Before the Industrial Revolution in the mid-to-late 19th century, demand for oriental goods such as (porcelain, silk, spices and tea) remained the driving force behind European imperialism, and (with the important exception of British East India Company rule in India) the Western European stake in Asia remained confined largely to trading stations and strategic outposts necessary to protect trade. Industrialization, however, dramatically increased European demand for Asian raw materials; and the severe Long Depression of the 1870s provoked a scramble for new markets for European industrial products and financial services in Africa, the Americas, Eastern Europe, and especially in Asia. This scramble coincided with a new era in global colonial expansion known as "the New Imperialism", which saw a shift in focus from trade and indirect rule to formal colonial control of vast overseas territories ruled as political extensions of their mother countries. Between the 1870s and the beginning of World War I in 1914, the United Kingdom, France, and the Netherlands—the established colonial powers in Asia—added to their empires vast expanses of territory in the Middle East, the Indian Subcontinent, and South East Asia. In the same period, the Empire of Japan, following the Meiji Restoration; the German Empire, following the end of the Franco-Prussian War in 1871; Tsarist Russia; and the United States, following the Spanish–American War in 1898, quickly emerged as new imperial powers in East Asia and in the Pacific Ocean area.

In Asia, World War I and World War II were played out as struggles among several key imperial powers—conflicts involving the European powers along with Russia and the rising American and Japanese powers. None of the colonial powers, however, possessed the resources to withstand the strains of both world wars and maintain their direct rule in Asia. Although nationalist movements throughout the colonial world led to the political independence of nearly all of Asia's remaining colonies, decolonization was intercepted by the Cold War; and South East Asia, South Asia, the Middle East, and East Asia remained embedded in a world economic, financial, and military system in which the great powers compete to extend their influence. However, the rapid post-war economic development and rise of the industrialized developed countries of Taiwan, Singapore, South Korea, Japan and the developing countries of India, the People's Republic of China and its autonomous territory of Hong Kong, along with the collapse of the Soviet Union, have greatly diminished Western European influence in Asia. The United States remains influential with trade and military bases in Asia.

European exploration of Asia started in ancient Roman times along the Silk Road. Knowledge of lands as distant as China were held by the Romans. Trade with India through the Roman Egyptian Red Sea ports was significant in the first centuries of the Common Era.

In the 13th and 14th centuries, a number of Europeans, many of them Christian missionaries, had sought to penetrate into China. The most famous of these travelers was Marco Polo. But these journeys had little permanent effect on east–west trade because of a series of political developments in Asia in the last decades of the 14th century, which put an end to further European exploration of Asia. The Yuan dynasty in China, which had been receptive to European missionaries and merchants, was overthrown, and the new Ming rulers were found to be unreceptive of religious proselytism. Meanwhile, the Turks consolidated control over the eastern Mediterranean, closing off key overland trade routes. Thus, until the 15th century, only minor trade and cultural exchanges between Europe and Asia continued at certain terminals controlled by Muslim traders.

Western European rulers determined to find new trade routes of their own. The Portuguese spearheaded the drive to find oceanic routes that would provide cheaper and easier access to South and East Asian goods. This chartering of oceanic routes between East and West began with the unprecedented voyages of Portuguese and Spanish sea captains. Their voyages were influenced by medieval European adventurers, who had journeyed overland to the Far East and contributed to geographical knowledge of parts of Asia upon their return.

In 1488, Bartolomeu Dias rounded the southern tip of Africa under the sponsorship of Portugal's John II, from which point he noticed that the coast swung northeast (Cape of Good Hope). While Dias' crew forced him to turn back, by 1497, Portuguese navigator Vasco da Gama made the first open voyage from Europe to India. In 1520, Ferdinand Magellan, a Portuguese navigator in the service of the Crown of Castile ('Spain'), found a sea route into the Pacific Ocean.

In 1509, the Portuguese under Francisco de Almeida won the decisive battle of Diu against a joint Mamluk and Arab fleet sent to expel the Portuguese of the Arabian Sea. The victory enabled Portugal to implement its strategy of controlling the Indian Ocean.

Early in the 16th century Afonso de Albuquerque (left) emerged as the Portuguese colonial viceroy most instrumental in consolidating Portugal's holdings in Africa and in Asia. He understood that Portugal could wrest commercial supremacy from the Arabs only by force, and therefore devised a plan to establish forts at strategic sites which would dominate the trade routes and also protect Portuguese interests on land. In 1510, he conquered Goa in India, which enabled him to gradually consolidate control of most of the commercial traffic between Europe and Asia, largely through trade; Europeans started to carry on trade from forts, acting as foreign merchants rather than as settlers. In contrast, early European expansion in the "West Indies", (later known to Europeans as a separate continent from Asia that they would call the "Americas") following the 1492 voyage of Christopher Columbus, involved heavy settlement in colonies that were treated as political extensions of the mother countries.

Lured by the potential of high profits from another expedition, the Portuguese established a permanent base in Cochin, south of the Indian trade port of Calicut in the early 16th century. In 1510, the Portuguese, led by Afonso de Albuquerque, seized Goa on the coast of India, which Portugal held until 1961, along with Diu and Daman (the remaining territory and enclaves in India from a former network of coastal towns and smaller fortified trading ports added and abandoned or lost centuries before). The Portuguese soon acquired a monopoly over trade in the Indian Ocean.

Portuguese viceroy Albuquerque (1509–1515) resolved to consolidate Portuguese holdings in Africa and Asia, and secure control of trade with the East Indies and China. His first objective was Malacca, which controlled the narrow strait through which most Far Eastern trade moved. Captured in 1511, Malacca became the springboard for further eastward penetration, starting with the voyage of António de Abreu and Francisco Serrão in 1512, ordered by Albuquerque, to the Moluccas. Years later the first trading posts were established in the Moluccas, or "Spice Islands", which was the source for some of the world's most hotly demanded spices, and from there, in Makassar and some others, but smaller, in the Lesser Sunda Islands. By 1513–1516, the first Portuguese ships had reached Canton on the southern coasts of China.
In 1513, after the failed attempt to conquer Aden, Albuquerque entered with an armada, for the first time for Europeans by the ocean via, on the Red Sea; and in 1515, Albuquerque consolidated the Portuguese hegemony in the Persian Gulf gates, already begun by him in 1507, with the domain of Muscat and Ormuz. Shortly after, other fortified bases and forts were annexed and built along the Gulf, and in 1521, through a military campaign, the Portuguese annexed Bahrain.

The Portuguese conquest of Malacca triggered the Malayan–Portuguese war. In 1521, Ming dynasty China defeated the Portuguese at the Battle of Tunmen and then defeated the Portuguese again at the Battle of Xicaowan. The Portuguese tried to establish trade with China by illegally smuggling with the pirates on the offshore islands off the coast of Zhejiang and Fujian, but they were driven away by the Ming navy in the 1530s-1540s.

In 1557, China decided to lease Macau to the Portuguese as a place where they could dry goods they transported on their ships, which they held until 1999. The Portuguese, based at Goa and Malacca, had now established a lucrative maritime empire in the Indian Ocean meant to monopolize the spice trade. The Portuguese also began a channel of trade with the Japanese, becoming the first recorded Westerners to have visited Japan. This contact introduced Christianity and firearms into Japan.

In 1505, (also possibly before, in 1501), the Portuguese, through Lourenço de Almeida, the son of Francisco de Almeida, reached Ceylon. The Portuguese founded a fort at the city of Colombo in 1517 and gradually extended their control over the coastal areas and inland. In a series of military conflicts and political maneuvers, the Portuguese extended their control over the Sinhalese kingdoms, including Jaffna (1591), Raigama (1593), Sitawaka (1593), and Kotte (1594)- However, the aim of unifying the entire island under Portuguese control faced the Kingdom of Kandy`s fierce resistance. The Portuguese, led by Pedro Lopes de Sousa, launched a full-scale military invasion of the kingdom of Kandy in the Campaign of Danture of 1594. The invasion was a disaster for the Portuguese, with their entire army, wiped out by Kandyan guerrilla warfare. Constantino de Sá, romantically celebrated in the 17th century Sinhalese Epic (also for its greater humanism and tolerance compared to other governors) led the last military operation that also ended in disaster. He died in the Battle of Randeniwela, refusing to abandon his troops in the face of total annihilation.

The energies of Castile (later, the "unified" Spain), the other major colonial power of the 16th century, were largely concentrated on the Americas, not South and East Asia, but the Spanish did establish a footing in the Far East in the Philippines. After fighting with the Portuguese by the Spice Islands since 1522 and the agreement between the two powers in 1529 (in the treaty of Zaragoza), the Spanish, led by Miguel López de Legazpi, settled and conquered gradually the Philippines since 1564. After the discovery of the return voyage to the Americas by Andres de Urdaneta in 1565, cargoes of Chinese goods were transported from the Philippines to Mexico and from there to Spain. By this long route, Spain reaped some of the profits of Far Eastern commerce. Spanish officials converted the islands to Christianity and established some settlements, permanently establishing the Philippines as the area of East Asia most oriented toward the West in terms of culture and commerce. The Moro Muslims fought against the Spanish for over three centuries in the Spanish–Moro conflict.

The lucrative trade was vastly expanded when the Portuguese began to export slaves from Africa in 1541; however, over time, the rise of the slave trade left Portugal over-extended, and vulnerable to competition from other Western European powers. Envious of Portugal's control of trade routes, other Western European nations—mainly the Netherlands, France, and England—began to send in rival expeditions to Asia. In 1642, the Dutch drove the Portuguese out of the Gold Coast in Africa, the source of the bulk of Portuguese slave laborers, leaving this rich slaving area to other Europeans, especially the Dutch and the English.

Rival European powers began to make inroads in Asia as the Portuguese and Spanish trade in the Indian Ocean declined primarily because they had become hugely over-stretched financially due to the limitations on their investment capacity and contemporary naval technology. Both of these factors worked in tandem, making control over Indian Ocean trade extremely expensive.

The existing Portuguese interests in Asia proved sufficient to finance further colonial expansion and entrenchment in areas regarded as of greater strategic importance in Africa and Brazil. Portuguese maritime supremacy was lost to the Dutch in the 17th century, and with this came serious challenges for the Portuguese. However, they still clung to Macau and settled a new colony on the island of Timor. It was as recent as the 1960s and 1970s that the Portuguese began to relinquish their colonies in Asia. Goa was invaded by India in 1961 and became an Indian state in 1987; Portuguese Timor was abandoned in 1975 and was then invaded by Indonesia. It became an independent country in 2002, and Macau was handed back to the Chinese as per a treaty in 1999.

The arrival of the Portuguese and Spanish and their holy wars against Muslim states in the Malayan–Portuguese war, Spanish–Moro conflict and Castilian War inflamed religious tensions and turned Southeast Asia into an arena of conflict between Muslims and Christians. The Brunei Sultanate's capital at Kota Batu was assaulted by Governor Sande who led the 1578 Spanish attack.

The word "savages" in Spanish, cafres, was from the word "infidel" in Arabic - Kafir, and was used by the Spanish to refer to their own "Christian savages" who were arrested in Brunei. It was said "Castilians are kafir, men who have no souls, who are condemned by fire when they die, and that too because they eat pork" by the Brunei Sultan after the term "accursed doctrine" was used to attack Islam by the Spaniards which fed into hatred between Muslims and Christians sparked by their 1571 war against Brunei. The Sultan's words were in response to insults coming from the Spanish at Manila in 1578, other Muslims from Champa, Java, Borneo, Luzon, Pahang, Demak, Aceh, and the Malays echoed the rhetoric of holy war against the Spanish and Iberian Portuguese, calling them kafir enemies which was a contrast to their earlier nuanced views of the Portuguese in the Hikayat Tanah Hitu and Sejarah Melayu. The war by Spain against Brunei was defended in an apologia written by Doctor De Sande. The British eventually partitioned and took over Brunei while Sulu was attacked by the British, Americans, and Spanish which caused its breakdown and downfall after both of them thrived from 1500-1900 for four centuries. Dar al-Islam was seen as under invasion by "kafirs" by the Atjehnese led by Zayn al-din and by Muslims in the Philippines as they saw the Spanish invasion, since the Spanish brought the idea of a crusader holy war against Muslim Moros just as the Portuguese did in Indonesia and India against what they called "Moors" in their political and commercial conquests which they saw through the lens of religion in the 16th century.

In 1578, an attack was launched by the Spanish against Jolo, and in 1875 it was destroyed at their hands, and once again in 1974 it was destroyed by the Philippines. The Spanish first set foot on Borneo in Brunei.

The Spanish war against Brunei failed to conquer Brunei but it totally cut off the Philippines from Brunei's influence, the Spanish then started colonizing Mindanao and building fortresses. In response, the Bisayas, where Spanish forces were stationed, were subjected to retaliatory attacks by the Magindanao in 1599-1600 due to the Spanish attacks on Mindanao.

The Brunei royal family was related to the Muslim Rajahs who in ruled the principality in 1570 of Manila (Kingdom of Maynila) and this was what the Spaniards came across on their initial arrival to Manila, Spain uprooted Islam out of areas where it was shallow after they began to force Christianity on the Philippines in their conquests after 1521 while Islam was already widespread in the 16th century Philippines. In the Philippines in the Cebu islands the natives killed the Spanish fleet leader Magellan. Borneo's western coastal areas at Landak, Sukadana, and Sambas saw the growth of Muslim states in the sixteenth century, in the 15th century at Nanking, the capital of China, the death and burial of the Borneo Bruneian king Maharaja Kama took place upon his visit to China with Zheng He's fleet.

The Spanish were expelled from Brunei in 1579 after they attacked in 1578. There were fifty thousand inhabitants before the 1597 attack by the Spanish in Brunei.

During first contact with China, numerous aggressions and provocations were undertaken by the Portuguese They believed they could mistreat the non-Christians because they themselves were Christians and acted in the name of their religion in committing crimes and atrocities. This resulted in the Battle of Xicaowan where the local Chinese navy defeated and captured a fleet of Portuguese caravels.

The Portuguese decline in Asia was accelerated by attacks on their commercial empire by the Dutch and the English, which began a global struggle over the empire in Asia that lasted until the end of the Seven Years' War in 1763. The Netherlands revolt against Spanish rule facilitated Dutch encroachment on the Portuguese monopoly over South and East Asian trade. The Dutch looked on Spain's trade and colonies as potential spoils of war. When the two crowns of the Iberian peninsula were joined in 1581, the Dutch felt free to attack Portuguese territories in Asia.

By the 1590s, a number of Dutch companies were formed to finance trading expeditions in Asia. Because competition lowered their profits, and because of the doctrines of mercantilism, in 1602 the companies united into a cartel and formed the Dutch East India Company, and received from the government the right to trade and colonize territory in the area stretching from the Cape of Good Hope eastward to the Strait of Magellan.

In 1605, armed Dutch merchants captured the Portuguese fort at Amboyna in the Moluccas, which was developed into the company's first secure base. Over time, the Dutch gradually consolidated control over the great trading ports of the East Indies. This control allowed the company to monopolise the world spice trade for decades. Their monopoly over the spice trade became complete after they drove the Portuguese from Malacca in 1641 and Ceylon in 1658.
Dutch East India Company colonies or outposts were later established in Atjeh (Aceh), 1667; Macassar, 1669; and Bantam, 1682. The company established its headquarters at Batavia (today Jakarta) on the island of Java. Outside the East Indies, the Dutch East India Company colonies or outposts were also established in Persia (Iran), Bengal (now Bangladesh and part of India), Mauritius (1638-1658/1664-1710), Siam (now Thailand), Guangzhou (Canton, China), Taiwan (1624–1662), and southern India (1616–1795).

Ming dynasty China defeated the Dutch East India Company in the Sino-Dutch conflicts. The Chinese first defeated and drove the Dutch out of the Pescadores in 1624. The Ming navy under Zheng Zhilong defeated the Dutch East India Company's fleet at the 1633 Battle of Liaoluo Bay. In 1662, Zheng Zhilong's son Zheng Chenggong (also known as Koxinga) expelled the Dutch from Taiwan after defeating them in the Siege of Fort Zeelandia. ("see" History of Taiwan) Further, the Dutch East India Company trade post on Dejima (1641–1857), an artificial island off the coast of Nagasaki, was for a long time the only place where Europeans could trade with Japan.

The Vietnamese Nguyễn lords defeated the Dutch in a naval battle in 1643.

The Cambodians defeated the Dutch in the Cambodian–Dutch War in 1644.

In 1652, Jan van Riebeeck established an outpost at the Cape of Good Hope (the southwestern tip of Africa, currently in South Africa) to restock company ships on their journey to East Asia. This post later became a fully-fledged colony, the Cape Colony (1652–1806). As Cape Colony attracted increasing Dutch and European settlement, the Dutch founded the city of Kaapstad (Cape Town).

By 1669, the Dutch East India Company was the richest private company in history, with a huge fleet of merchant ships and warships, tens of thousands of employees, a private army consisting of thousands of soldiers, and a reputation on the part of its stockholders for high dividend payments.

The company was in almost constant conflict with the English; relations were particularly tense following the Amboyna Massacre in 1623. During the 18th century, Dutch East India Company possessions were increasingly focused on the East Indies. After the fourth war between the United Provinces and England (1780–1784), the company suffered increasing financial difficulties. In 1799, the company was dissolved, commencing official colonisation of the East Indies. During the era of New Imperialism the territorial claims of the Dutch East India Company (VOC) expanded into a fully fledged colony named the Dutch East Indies. Partly driven by re-newed colonial aspirations of fellow European nation states the Dutch strived to establish unchallenged control of the archipelago now known as Indonesia.

Six years into formal colonisation of the East Indies, in Europe the Dutch Republic was occupied by the French forces of Napoleon. The Dutch government went into exile in England and formally ceded its colonial possessions to Great Britain. The pro-French Governor General of Java Jan Willem Janssens, resisted a British invasion force in 1811 until forced to surrender. British Governor Raffles, who the later founded the city of Singapore, ruled the colony the following 10 years of the British interregnum (1806–1816).

After the defeat of Napoleon and the Anglo-Dutch Treaty of 1814 colonial government of the East Indies was ceded back to the Dutch in 1817. The loss of South Africa and the continued scramble for Africa stimulated the Dutch to secure unchallenged dominion over its colony in the East Indies. The Dutch started to consolidate its power base through extensive military campaigns and elaborate diplomatic alliances with indigenous rulers ensuring the Dutch tricolor was firmly planted in all corners of the Archipelago. These military campaigns included: the Padri War (1821–1837), the Java War (1825–1830) and the Aceh War (1873–1904). This raised the need for a considerable military buildup of the colonial army (KNIL). From all over Europe soldiers were recruited to join the KNIL.

The Dutch concentrated their colonial enterprise in the Dutch East Indies (Indonesia) throughout the 19th century. The Dutch lost control over the East Indies to the Japanese during much of World War II. Following the war, the Dutch fought Indonesian independence forces after Japan surrendered to the Allies in 1945. In 1949, most of what was known as the Dutch East Indies was ceded to the independent Republic of Indonesia. In 1962, also Dutch New Guinea was annexed by Indonesia de facto ending Dutch imperialism in Asia.

The English sought to stake out claims in India at the expense of the Portuguese dating back to the Elizabethan era. In 1600, Queen Elizabeth I incorporated the English East India Company (later the British East India Company), granting it a monopoly of trade from the Cape of Good Hope eastward to the Strait of Magellan. In 1639, it acquired Madras on the east coast of India, where it quickly surpassed Portuguese Goa as the principal European trading centre on the Indian Subcontinent.

Through bribes, diplomacy, and manipulation of weak native rulers, the company prospered in India, where it became the most powerful political force, and outrivaled its Portuguese and French competitors. For more than one hundred years, English and French trading companies had fought one another for supremacy, and, by the middle of the 18th century, competition between the British and the French had heated up. French defeat by the British under the command of Robert Clive during the Seven Years' War (1756–1763) marked the end of the French stake in India.

The British East India Company, although still in direct competition with French and Dutch interests until 1763, was able to extend its control over almost the whole of India in the century following the subjugation of Bengal at the 1757 Battle of Plassey. The British East India Company made great advances at the expense of a Mughal dynasty.

The reign of Aurangzeb had marked the height of Mughal power. By 1690 Mughal territorial expansion reached its greatest extent encompassing the entire Indian Subcontinent. But this period of power was followed by one of decline. Fifty years after the death of Aurangzeb, the great Mughal empire had crumbled. Meanwhile, marauding warlords, nobles, and others bent on gaining power left the Subcontinent increasingly anarchic. Although the Mughals kept the imperial title until 1858, the central government had collapsed, creating a power vacuum.

Aside from defeating the French during the Seven Years' War, Robert Clive, the leader of the Company in India, defeated a key Indian ruler of Bengal at the decisive Battle of Plassey (1757), a victory that ushered in the beginning of a new period in Indian history, that of informal British rule. While still nominally the sovereign, the Mughal Indian emperor became more and more of a puppet ruler, and anarchy spread until the company stepped into the role of policeman of India. The transition to formal imperialism, characterised by Queen Victoria being crowned "Empress of India" in the 1870s was a gradual process. The first step toward cementing formal British control extended back to the late 18th century. The British Parliament, disturbed by the idea that a great business concern, interested primarily in profit, was controlling the destinies of millions of people, passed acts in 1773 and 1784 that gave itself the power to control company policies and to appoint the highest company official in India, the Governor-General. (This system of dual control lasted until 1858.) By 1818, the East India Company was master of all of India. Some local rulers were forced to accept its overlordship; others were deprived of their territories. Some portions of India were administered by the British directly; in others native dynasties were retained under British supervision.
Until 1858, however, much of India was still officially the dominion of the Mughal emperor. Anger among some social groups, however, was seething under the governor-generalship of James Dalhousie (1847–1856), who annexed the Punjab (1849) after victory in the Second Sikh War, annexed seven princely states using the doctrine of lapse, annexed the key state of Oudh on the basis of misgovernment, and upset cultural sensibilities by banning Hindu practices such as sati.

The 1857 Sepoy Rebellion, or Indian Mutiny, an uprising initiated by Indian troops, called sepoys, who formed the bulk of the company's armed forces, was the key turning point. Rumour had spread among them that their bullet cartridges were lubricated with pig and cow fat. The cartridges had to be bit open, so this upset the Hindu and Muslim soldiers. The Hindu religion held cows sacred, and for Muslims pork was considered haraam. In one camp, 85 out of 90 sepoys would not accept the cartridges from their garrison officer. The British harshly punished those who would not by jailing them. The Indian people were outraged, and on May 10, 1857, sepoys marched to Delhi, and, with the help of soldiers stationed there, captured it. Fortunately for the British, many areas remained loyal and quiescent, allowing the revolt to be crushed after fierce fighting. One important consequence of the revolt was the final collapse of the Mughal dynasty. The mutiny also ended the system of dual control under which the British government and the British East India Company shared authority. The government relieved the company of its political responsibilities, and in 1858, after 258 years of existence, the company relinquished its role. Trained civil servants were recruited from graduates of British universities, and these men set out to rule India. Lord Canning (created earl in 1859), appointed Governor-General of India in 1856, became known as "Clemency Canning" as a term of derision for his efforts to restrain revenge against the Indians during the Indian Mutiny. When the Government of India was transferred from the company to the Crown, Canning became the first viceroy of India.

The Company initiated the first of the Anglo-Burmese wars in 1824, which led to total annexation of Burma by the Crown in 1885. The British ruled Burma as a province of British India until 1937, then administered her separately under the Burma Office except during the Japanese occupation of Burma, 1942–1945, until granted independence on 4 January 1948. (Unlike India, Burma opted not to join the Commonwealth of Nations.)

The denial of equal status to Indians was the immediate stimulus for the formation in 1885 of the Indian National Congress, initially loyal to the Empire but committed from 1905 to increased self-government and by 1930 to outright independence. The "Home charges", payments transferred from India for administrative costs, were a lasting source of nationalist grievance, though the flow declined in relative importance over the decades to independence in 1947.

Although majority Hindu and minority Muslim political leaders were able to collaborate closely in their criticism of British policy into the 1920s, British support for a distinct Muslim political organisation, the Muslim League from 1906 and insistence from the 1920s on separate electorates for religious minorities, is seen by many in India as having contributed to Hindu-Muslim discord and the country's eventual Partition.

France, which had lost its empire to the British by the end of the 18th century, had little geographical or commercial basis for expansion in Southeast Asia. After the 1850s, French imperialism was initially impelled by a nationalistic need to rival the United Kingdom and was supported intellectually by the notion that French culture was superior to that of the people of Annam (Vietnam), and its "mission civilisatrice"—or its "civilizing mission" of the Annamese through their assimilation to French culture and the Catholic religion. The pretext for French expansionism in Indochina was the protection of French religious missions in the area, coupled with a desire to find a southern route to China through Tonkin, the European name for a region of northern Vietnam.

French religious and commercial interests were established in Indochina as early as the 17th century, but no concerted effort at stabilizing the French position was possible in the face of British strength in the Indian Ocean and French defeat in Europe at the beginning of the 19th century. A mid-19th century religious revival under the Second Empire provided the atmosphere within which interest in Indochina grew. Anti-Christian persecutions in the Far East provided the pretext for the bombardment of Tourane (Danang) in 1847, and invasion and occupation of Danang in 1857 and Saigon in 1858. Under Napoleon III, France decided that French trade with China would be surpassed by the British, and accordingly the French joined the British against China in the Second Opium War from 1857 to 1860, and occupied parts of Vietnam as its gateway to China.

By the Treaty of Saigon in 1862, on June 5, the Vietnamese emperor ceded France three provinces of southern Vietnam to form the French colony of Cochinchina; France also secured trade and religious privileges in the rest of Vietnam and a protectorate over Vietnam's foreign relations. Gradually French power spread through exploration, the establishment of protectorates, and outright annexations. Their seizure of Hanoi in 1882 led directly to war with China (1883–1885), and the French victory confirmed French supremacy in the region. France governed Cochinchina as a direct colony, and central and northern Vietnam under the protectorates of Annam and Tonkin, and Cambodia as protectorates in one degree or another. Laos too was soon brought under French "protection".

By the beginning of the 20th century, France had created an empire in Indochina nearly 50 percent larger than the mother country. A Governor-General in Hanoi ruled Cochinchina directly and the other regions through a system of residents. Theoretically, the French maintained the precolonial rulers and administrative structures in Annam, Tonkin, Cochinchina, Cambodia, and Laos, but in fact the governor-generalship was a centralised fiscal and administrative regime ruling the entire region. Although the surviving native institutions were preserved in order to make French rule more acceptable, they were almost completely deprived of any independence of action. The ethnocentric French colonial administrators sought to assimilate the upper classes into France's "superior culture." While the French improved public services and provided commercial stability, the native standard of living declined and precolonial social structures eroded. Indochina, which had a population of over eighteen million in 1914, was important to France for its tin, pepper, coal, cotton, and rice. It is still a matter of debate, however, whether the colony was commercially profitable.

Tsarist Russia is not often regarded as a colonial power such as the United Kingdom or France because of the manner of Russian expansions: unlike the United Kingdom, which expanded overseas, the Russian empire grew from the centre outward by a process of accretion, like the United States. In the 19th century, Russian expansion took the form of a struggle of an effectively landlocked country for access to a warm water port.

Qing China defeated Russia in the Sino-Russian border conflicts.

While the British were consolidating their hold on India, Russian expansion had moved steadily eastward to the Pacific, then toward the Middle East. In the early 19th century it succeeded in conquering the South Caucasus and Dagestan from Qajar Iran following the Russo-Persian War (1804–13), the Russo-Persian War (1826–28) and the out coming treaties of Gulistan and Turkmenchay, giving Russia direct borders with both Persia's as well as Ottoman Turkey's heartlands. Later, they eventually reached the frontiers of Afghanistan as well (which had the largest foreign border adjacent to British holdings in India). In response to Russian expansion, the defense of India's land frontiers and the control of all sea approaches to the Subcontinent via the Suez Canal, the Red Sea, and the Persian Gulf became preoccupations of British foreign policy in the 19th century.

Anglo-Russian rivalry in the Middle East and Central Asia led to a brief confrontation over Afghanistan in the 1870s. In Persia (Iran), both nations set up banks to extend their economic influence. The United Kingdom went so far as to invade Tibet, a land subordinate to the Chinese empire, in 1904, but withdrew when it became clear that Russian influence was insignificant and when Chinese resistance proved tougher than expected.

In 1907, the United Kingdom and Russia signed an agreement which — on the surface —ended their rivalry in Central Asia. ("see" Anglo-Russian Entente) As part of the entente, Russia agreed to deal with the sovereign of Afghanistan only through British intermediaries. In turn, the United Kingdom would not annex or occupy Afghanistan. Chinese suzerainty over Tibet also was recognised by both Russia and the United Kingdom, since nominal control by a weak China was preferable to control by either power. Persia was divided into Russian and British spheres of influence and an intervening "neutral" zone. The United Kingdom and Russia chose to reach these uneasy compromises because of growing concern on the part of both powers over German expansion in strategic areas of China and Africa.

Following the entente, Russia increasingly intervened in Persian domestic politics and suppressed nationalist movements that threatened both St. Petersburg and London. After the Russian Revolution, Russia gave up its claim to a sphere of influence, though Soviet involvement persisted alongside the United Kingdom's until the 1940s.

In the Middle East, in Persia (Iran) and the Ottoman Empire, a German company built a railroad from Constantinople to Baghdad and the Persian Gulf in the latter, while it built a railroad from the north of the country to the south, connecting the Caucasus with the Persian Gulf in the former. Germany wanted to gain economic influence in the region and then, perhaps, move on to India. This was met with bitter resistance by the United Kingdom, Russia, and France who divided the region among themselves.

The 16th century brought many Jesuit missionaries to China, such as Matteo Ricci, who established missions where Western science was introduced, and where Europeans gathered knowledge of Chinese society, history, culture, and science. During the 18th century, merchants from Western Europe came to China in increasing numbers. However, merchants were confined to Guangzhou and the Portuguese colony of Macau, as they had been since the 16th century. European traders were increasingly irritated by what they saw as the relatively high customs duties they had to pay and by the attempts to curb the growing import trade in opium. By 1800, its importation was forbidden by the imperial government. However, the opium trade continued to boom.

Early in the 19th century, serious internal weaknesses developed in the Qing dynasty that left China vulnerable to Western, Meiji period Japanese, and Russian imperialism. In 1839, China found itself fighting the First Opium War with Britain. China was defeated, and in 1842, signed the provisions of the Treaty of Nanking which were first of the unequal treaties signed during the Qing Dynasty. Hong Kong Island was ceded to Britain, and certain ports, including Shanghai and Guangzhou, were opened to British trade and residence. In 1856, the Second Opium War broke out. The Chinese were again defeated, and now forced to the terms of the 1858 Treaty of Tientsin. The treaty opened new ports to trade and allowed foreigners to travel in the interior. In addition, Christians gained the right to propagate their religion. The United States Treaty of Wanghia and Russia later obtained the same prerogatives in separate treaties.

Toward the end of the 19th century, China appeared on the way to territorial dismemberment and economic vassalage—the fate of India's rulers that played out much earlier. Several provisions of these treaties caused long-standing bitterness and humiliation among the Chinese: extraterritoriality (meaning that in a dispute with a Chinese person, a Westerner had the right to be tried in a court under the laws of his own country), customs regulation, and the right to station foreign warships in Chinese waters, including its navigable rivers.

Jane E. Elliott criticized the allegation that China refused to modernize or was unable to defeat Western armies as simplistic, noting that China embarked on a massive military modernization in the late 1800s after several defeats, buying weapons from Western countries and manufacturing their own at arsenals, such as the Hanyang Arsenal during the Boxer Rebellion. In addition, Elliott questioned the claim that Chinese society was traumatized by the Western victories, as many Chinese peasants (90% of the population at that time) living outside the concessions continued about their daily lives, uninterrupted and without any feeling of "humiliation".

Historians have judged the Qing dynasty's vulnerability and weakness to foreign imperialism in the 19th century to be based mainly on its maritime naval weakness while it achieved military success against westerners on land, the historian Edward L. Dreyer said that "China’s nineteenth-century humiliations were strongly related to her weakness and failure at sea. At the start of the Opium War, China had no unified navy and no sense of how vulnerable she was to attack from the sea; British forces sailed and steamed wherever they wanted to go...In the Arrow War (1856-60), the Chinese had no way to prevent the Anglo-French expedition of 1860 from sailing into the Gulf of Zhili and landing as near as possible to Beijing. Meanwhile, new but not exactly modern Chinese armies suppressed the midcentury rebellions, bluffed Russia into a peaceful settlement of disputed frontiers in Central Asia, and defeated the French forces on land in the Sino-French War (1884-85). But the defeat of the fleet, and the resulting threat to steamship traffic to Taiwan, forced China to conclude peace on unfavorable terms."

During the Sino-French War, Chinese forces defeated the French at the Battle of Cầu Giấy (Paper Bridge), Bắc Lệ ambush, Battle of Phu Lam Tao, Battle of Zhenhai, the Battle of Tamsui in the Keelung Campaign and in the last battle which ended the war, the Battle of Bang Bo (Zhennan Pass), which triggered the French Retreat from Lạng Sơn and resulted in the collapse of the French Jules Ferry government in the Tonkin Affair.

The Qing dynasty forced Russia to hand over disputed territory in Ili in the Treaty of Saint Petersburg (1881), in what was widely seen by the west as a diplomatic victory for the Qing. Russia acknowledged that Qing China potentially posed a serious military threat. Mass media in the west during this era portrayed China as a rising military power due to its modernization programs and as a major threat to the western world, invoking fears that China would successfully conquer western colonies like Australia.

The British observer Demetrius Charles de Kavanagh Boulger suggested a British-Chinese alliance to check Russian expansion in Central Asia.

During the Ili crisis when Qing China threatened to go to war against Russia over the Russian occupation of Ili, the British officer Charles George Gordon was sent to China by Britain to advise China on military options against Russia should a potential war break out between China and Russia.

The Russians observed the Chinese building up their arsenal of modern weapons during the Ili crisis, the Chinese bought thousands of rifles from Germany. In 1880, massive amounts of military equipment and rifles were shipped via boats to China from Antwerp as China purchased torpedoes, artillery, and 260,260 modern rifles from Europe.

The Russian military observer D. V. Putiatia visited China in 1888 and found that in Northeastern China (Manchuria) along the Chinese-Russian border, the Chinese soldiers were potentially able to become adept at "European tactics" under certain circumstances, and the Chinese soldiers were armed with modern weapons like Krupp artillery, Winchester carbines, and Mauser rifles.

Compared to Russian controlled areas, more benefits were given to the Muslim Kirghiz on the Chinese controlled areas. Russian settlers fought against the Muslim nomadic Kirghiz, which led the Russians to believe that the Kirghiz would be a liability in any conflict against China. The Muslim Kirghiz were sure that in an upcoming war, that China would defeat Russia.

Russian sinologists, the Russian media, threat of internal rebellion, the pariah status inflicted by the Congress of Berlin, the negative state of the Russian economy all led Russia to concede and negotiate with China in St Petersburg, and return most of Ili to China.
The rise of Japan since the Meiji Restoration as an imperial power led to further subjugation of China. In a dispute over China's longstanding claim of suzerainty in Korea, war broke out between China and Japan, resulting in humiliating defeat for the Chinese. By the Treaty of Shimonoseki (1895), China was forced to recognize effective Japanese rule of Korea and Taiwan was ceded to Japan until its recovery in 1945 at the end of the WWII by the Republic of China.

China's defeat at the hands of Japan was another trigger for future aggressive actions by Western powers. In 1897, Germany demanded and was given a set of exclusive mining and railroad rights in Shandong province. Russia obtained access to Dairen and Port Arthur and the right to build a railroad across Manchuria, thereby achieving complete domination over a large portion of northwestern China. The United Kingdom and France also received a number of concessions. At this time, much of China was divided up into "spheres of influence": Germany had influence in Jiaozhou (Kiaochow) Bay, Shandong, and the Yellow River valley; Russia had influence in the Liaodong Peninsula and Manchuria; the United Kingdom had influence in Weihaiwei and the Yangtze Valley; and France had influence in the Guangzhou Bay and the provinces of Yunnan, Guizhou and Guangxi
China continued to be divided up into these spheres until the United States, which had no sphere of influence, grew alarmed at the possibility of its businessmen being excluded from Chinese markets. In 1899, Secretary of State John Hay asked the major powers to agree to a policy of equal trading privileges. In 1900, several powers agreed to the U.S.-backed scheme, giving rise to the "Open Door" policy, denoting freedom of commercial access and non-annexation of Chinese territory. In any event, it was in the European powers' interest to have a weak but independent Chinese government. The privileges of the Europeans in China were guaranteed in the form of treaties with the Qing government. In the event that the Qing government totally collapsed, each power risked losing the privileges that it already had negotiated.

The erosion of Chinese sovereignty and seizures of land from Chinese by foreigners contributed to a spectacular anti-foreign outbreak in June 1900, when the "Boxers" (properly the society of the "righteous and harmonious fists") attacked foreigners around Beijing. The Imperial Court was divided into anti-foreign and pro-foreign factions, with the pro-foreign faction led by Ronglu and Prince Qing hampering any military effort by the anti-foreign faction led by Prince Duan and Dong Fuxiang. The Qing Empress Dowager ordered all diplomatic ties to be cut off and all foreigners to leave the legations in Beijing to go to Tianjin. The foreigners refused to leave. Fueled by entirely false reports that the foreigners in the legations were massacred, the Eight-Nation Alliance decided to launch an expedition on Beijing to reach the legations but they underestimated the Qing military. The Qing and Boxers defeated the foreigners at the Seymour Expedition, forcing them to turn back at the Battle of Langfang. In response to the foreign attack on Dagu Forts the Qing responded by declaring war against the foreigners. the Qing forces and foreigners fought a fierce battle at the Battle of Tientsin before the foreigners could launch a second expedition. On their second try Gaselee Expedition, with a much larger force, the foreigners managed to reach Beijing and fight the Battle of Peking (1900). British and French forces looted, plundered and burned the Old Summer Palace to the ground for the second time (the first time being in 1860, following the Second Opium War). German forces were particularly severe in exacting revenge for the killing of their ambassador due to the orders of Kaiser Wilhelm II, who held anti-Asian sentiments, while Russia tightened its hold on Manchuria in the northeast until its crushing defeat by Japan in the war of 1904–1905. The Qing court evacuated to Xi'an and threatened to continue the war against foreigners, until the foreigners tempered their demands in the Boxer Protocol, promising that China would not have to give up any land and gave up the demands for the execution of Dong Fuxiang and Prince Duan.

The correspondent Douglas Story observed Chinese troops in 1907 and praised their abilities and military skill.

Extraterritorial jurisdiction was abandoned by the United Kingdom and the United States in 1943. Chiang Kai-shek forced the French to hand over all their concessions back to China control after World War II. Foreign political control over leased parts of China ended with the incorporation of Hong Kong and the small Portuguese territory of Macau into the People's Republic of China in 1997 and 1999 respectively.

Some Americans in the Nineteenth Century advocated for the annexation of Taiwan from China. Aboriginals on Taiwan often attacked and massacred shipwrecked western sailors. In 1867, during the Rover incident, Taiwanese aborigines attacked shipwrecked American sailors, killing the entire crew. They subsequently defeated a retaliatory expedition by the American military and killed another American during the battle.

As the United States emerged as a new imperial power in the Pacific and Asia, one of the two oldest Western imperialist powers in the regions, Spain, was finding it increasingly difficult to maintain control of territories it had held in the regions since the 16th century. In 1896, a widespread revolt against Spanish rule broke out in the Philippines. Meanwhile, the recent string of U.S. territorial gains in the Pacific posed an even greater threat to Spain's remaining colonial holdings.

As the U.S. continued to expand its economic and military power in the Pacific, it declared war against Spain in 1898. During the Spanish–American War, U.S. Admiral Dewey destroyed the Spanish fleet at Manila and U.S. troops landed in the Philippines. Spain later agreed by treaty to cede the Philippines in Asia and Guam in the Pacific. In the Caribbean, Spain ceded Puerto Rico to the U.S. The war also marked the end of Spanish rule in Cuba, which was to be granted nominal independence but remained heavily influenced by the U.S. government and U.S. business interests. One year following its treaty with Spain, the U.S. occupied the small Pacific outpost of Wake Island.

The Filipinos, who assisted U.S. troops in fighting the Spanish, wished to establish an independent state and, on June 12, 1898, declared independence from Spain. In 1899, fighting between the Filipino nationalists and the U.S. broke out; it took the U.S. almost fifteen years to fully subdue the insurgency. The U.S. sent 70,000 troops and suffered thousands of casualties. The Filipinos insurgents, however, suffered considerably higher casualties than the Americans. Most casualties in the war were civilians dying primarily from disease.

U.S. attacks into the countryside often included scorched earth campaigns where entire villages were burned and destroyed, and concentrated civilians into camps known as "protected zones". Most of these civilian casualties resulted from disease and famine. Reports of the execution of U.S. soldiers taken prisoner by the Filipinos led to disproportionate reprisals by American forces.

The Moro Muslims fought against the Americans in the Moro Rebellion.

In 1914, Dean C. Worcester, U.S. Secretary of the Interior for the Philippines (1901–1913) described "the regime of civilisation and improvement which started with American occupation and resulted in developing naked savages into cultivated and educated men". Nevertheless, some Americans, such as Mark Twain, deeply opposed American involvement/imperialism in the Philippines, leading to the abandonment of attempts to construct a permanent U.S. naval base and using it as an entry point to the Chinese market. In 1916, Congress guaranteed the independence of the Philippines by 1945.

World War I brought about the fall of several empires in Europe. This had repercussions around the world. The defeated Central Powers included Germany and the Turkish Ottoman Empire. Germany lost all of its colonies in Asia. German New Guinea, a part of Papua New Guinea, became administered by Australia. German possessions and concessions in China, including Qingdao, became the subject of a controversy during the Paris Peace Conference when the Beiyang government in China agreed to cede these interests to Japan, to the anger of many Chinese people. Although the Chinese diplomats refused to sign the agreement, these interests were ceded to Japan with the support of the United States and the United Kingdom.

Turkey gave up her provinces; Syria, Palestine, and Mesopotamia (now Iraq) came under French and British control as League of Nations Mandates. The discovery of petroleum first in Iran and then in the Arab lands in the interbellum provided a new focus for activity on the part of the United Kingdom, France, and the United States.

In 1641, all Westerners were thrown out of Japan. For the next two centuries, Japan was free from Western contact, except for at the port of Nagasaki, which Japan allowed Dutch merchant vessels to enter on a limited basis.

Japan's freedom from Western contact ended on 8 July 1853, when Commodore Matthew Perry of the U.S. Navy sailed a squadron of black-hulled warships into Edo (modern Tokyo) harbor. The Japanese told Perry to sail to Nagasaki but he refused. Perry sought to present a letter from U.S. President Millard Fillmore to the emperor which demanded concessions from Japan. Japanese authorities responded by stating that they could not present the letter directly to the emperor, but scheduled a meeting on 14 July with a representative of the emperor. On 14 July, the squadron sailed towards the shore, giving a demonstration of their cannon's firepower thirteen times. Perry landed with a large detachment of Marines and presented the emperor's representative with Fillmore's letter. Perry said he would return, and did so, this time with even more war ships. The U.S. show of force led to Japan's concession to the Convention of Kanagawa on 31 March 1854. This treaty conferred extraterritoriality on American nationals, as well as, opening up further treaty ports beyond Nagasaki. This treaty was followed up by similar treaties with the United Kingdom, the Netherlands, Russia and France. These events made Japanese authorities aware that the country was lacking technologically and needed the strength of industrialism in order to keep their power. This realisation eventually led to a civil war and political reform known the Meiji Restoration.

The Meiji Restoration of 1868 led to administrative overhaul, deflation and subsequent rapid economic development. Japan had limited natural resources of her own and sought both overseas markets and sources of raw materials, fuelling a drive for imperial conquest which began with the defeat of China in 1895.
Taiwan, ceded by Qing dynasty China, became the first Japanese colony. In 1899, Japan won agreements from the great powers' to abandon extraterritoriality for their citizens, and an alliance with the United Kingdom established it in 1902 as an international power. Its spectacular defeat of Russia's navy in 1905 gave it the southern half of the island of Sakhalin; exclusive Japanese influence over Korea (propinquity); the former Russian lease of the Liaodong Peninsula with Port Arthur (Lüshunkou); and extensive rights in Manchuria (see the Russo-Japanese War).

The Empire of Japan and the Joseon Dynasty in Korea formed bilateral diplomatic relations in 1876. China lost its suzerainty of Korea after defeat in the Sino-Japanese War in 1894. Russia also lost influence on the Korean peninsula with the Treaty of Portsmouth as a result of the Russo-Japanese war in 1904. The Joseon Dynasty became increasingly dependent on Japan. Korea became a protectorate of Japan with the Japan–Korea Treaty of 1905. Korea was then "de jure" annexed to Japan with the Japan–Korea Treaty of 1910.

Japan was now one of the most powerful forces in the Far East, and in 1914, it entered World War I on the side of the Allies, seizing German-occupied Kiaochow and subsequently demanding Chinese acceptance of Japanese political influence and territorial acquisitions (Twenty-One Demands, 1915). Mass protests in Peking in 1919 coupled with Allied (and particularly U.S.) opinion led to Japan's abandonment of most of the demands and Joseon's 1922 return to China. Japan received the German territory from the Treaty of Versailles, 1919, sparking widespread Chinese nationalism.

Tensions with China increased over the 1920s, and in 1931 Japanese army units based in Manchuria seized control of the region without direction from Tokyo. Intermittent conflict with China led to full-scale war in mid-1937, drawing Japan toward an overambitious bid for Asian hegemony (Greater East Asia Co-Prosperity Sphere), which ultimately led to defeat and the loss of all its overseas territories after World War II (see Japanese expansionism and Japanese nationalism).

In the aftermath of World War II, European colonies, controlling more than one billion people throughout the world, still ruled most of the Middle East, South East Asia, and the Indian Subcontinent. However, the image of European pre-eminence was shattered by the wartime Japanese occupations of large portions of British, French, and Dutch territories in the Pacific. The destabilisation of European rule led to the rapid growth of nationalist movements in Asia—especially in Indonesia, Malaya, Burma, and French Indochina (Vietnam, Cambodia, and Laos).
The war, however, only accelerated forces already in existence undermining Western imperialism in Asia. Throughout the colonial world, the processes of urbanisation and capitalist investment created professional merchant classes that emerged as new Westernised elites. While imbued with Western political and economic ideas, these classes increasingly grew to resent their unequal status under European rule.

In India, the westward movement of Japanese forces towards Bengal during World War II had led to major concessions on the part of British authorities to Indian nationalist leaders. In 1947, the United Kingdom, devastated by war and embroiled in economic crisis at home, granted British India its independence as two nations: India and Pakistan. Myanmar (Burma) and Sri Lanka (Ceylon), which is also part of British India, also gained their independence from the United Kingdom the following year, in 1948. In the Middle East, the United Kingdom granted independence to Jordan in 1946 and two years later, in 1948, ended its mandate of Palestine becoming the independent nation of Israel.
Following the end of the war, nationalists in Indonesia demanded complete independence from the Netherlands. A brutal conflict ensued, and finally, in 1949, through United Nations mediation, the Dutch East Indies achieved independence, becoming the new nation of Indonesia. Dutch imperialism moulded this new multi-ethnic state comprising roughly 3,000 islands of the Indonesian archipelago with a population at the time of over 100 million.

The end of Dutch rule opened up latent tensions between the roughly 300 distinct ethnic groups of the islands, with the major ethnic fault line being between the Javanese and the non-Javanese.

Netherlands New Guinea was under the Dutch administration until 1962 (see also West New Guinea dispute).

In the Philippines, the U.S. remained committed to its previous pledges to grant the islands their independence, and the Philippines became the first of the Western-controlled Asian colonies to be granted independence post-World War II. However, the Philippines remained under pressure to adopt a political and economic system similar to the U.S.

This aim was greatly complicated by the rise of new political forces. During the war, the "Hukbalahap" (People's Army), which had strong ties to the Communist Party of the Philippines (PKP), fought against the Japanese occupation of the Philippines and won strong popularity among many sectors of the Filipino working class and peasantry. In 1946, the PKP participated in elections as part of the Democratic Alliance. However, with the onset of the Cold War, its growing political strength drew a reaction from the ruling government and the United States, resulting in the repression of the PKP and its associated organisations. In 1948, the PKP began organizing an armed struggle against the government and continued U.S. military presence. In 1950, the PKP created the People's Liberation Army ("Hukbong Mapagpalaya ng Bayan"), which mobilised thousands of troops throughout the islands. The insurgency lasted until 1956, when the PKP gave up armed struggle.

In 1968, the PKP underwent a split, and in 1969 the Maoist faction of the PKP created the New People's Army. Maoist rebels re-launched an armed struggle against the government and the U.S. military presence in the Philippines, which continues to this day.

France remained determined to retain its control of Indochina. However, in Hanoi, in 1945, a broad front of nationalists and communists led by Ho Chi Minh declared an independent Republic of Vietnam, commonly referred to as the Viet Minh regime by Western outsiders. France, seeking to regain control of Vietnam, countered with a vague offer of self-government under French rule. France's offers were unacceptable to Vietnamese nationalists; and in December 1946 the Việt Minh launched a rebellion against the French authority governing the colonies of French Indochina. The first few years of the war involved a low-level rural insurgency against French authority. However, after the Chinese communists reached the Northern border of Vietnam in 1949, the conflict turned into a conventional war between two armies equipped with modern weapons supplied by the United States and the Soviet Union. Meanwhile, the France granted the State of Vietnam based in Saigon independence in 1949 while Laos and Cambodia received independence in 1953. The US recognized the regime in Saigon, and provided the French military effort with military aid.

Meanwhile, in Vietnam, the French war against the Viet Minh continued for nearly eight years. The French were gradually worn down by guerrilla and jungle fighting. The turning point for France occurred at Dien Bien Phu in 1954, which resulted in the surrender of ten thousand French troops. Paris was forced to accept a political settlement that year at the Geneva Conference, which led to a precarious set of agreements regarding the future political status of Laos, Cambodia, and Vietnam.

British colonies in South Asia, East Asia, And Southeast Asia:


French colonies in Southeast Asia:


Dutch, British, Portuguese colonies and Russian territories in Asia:
















</doc>
<doc id="15445" url="https://en.wikipedia.org/wiki?curid=15445" title="Entropy (information theory)">
Entropy (information theory)

In information theory, the entropy of a random variable is the average level of "information", "surprise", or "uncertainty" inherent in the variable's possible outcomes. The concept of information entropy was introduced by Claude Shannon in his 1948 paper "A Mathematical Theory of Communication". As an example, consider a biased coin with probability of landing on heads and probability of landing on tails. The maximum surprise is for , when there is no reason to expect one outcome over another, and in this case a coin flip has an entropy of one bit. The minimum surprise is when or , when the event is known and the entropy is zero bits. Other values of "p" give different entropies between zero and one bits.

Given a discrete random variable formula_1, with possible outcomes formula_2, which occur with probability formula_3, the entropy of formula_1 is formally defined as:

where formula_5 denotes the sum over the variable's possible values and formula_6 is the logarithm, the choice of base varying between different applications. Base 2 gives the unit of bits (or "shannons"), while base "e" gives the "natural units" nat, and base 10 gives a unit called "dits", "bans", or "hartleys". An equivalent definition of entropy is the expected value of the self-information of a variable.

The entropy was originally created by Shannon as part of his theory of communication, in which a data communication system is composed of three elements: a source of data, a communication channel, and a receiver. In Shannon's theory, the "fundamental problem of communication" – as expressed by Shannon – is for the receiver to be able to identify what data was generated by the source, based on the signal it receives through the channel. Shannon considered various ways to encode, compress, and transmit messages from a data source, and proved in his famous source coding theorem that the entropy represents an absolute mathematical limit on how well data from the source can be losslessly compressed onto a perfectly noiseless channel. Shannon strengthened this result considerably for noisy channels in his noisy-channel coding theorem.

Entropy in information theory is directly analogous to the entropy in statistical thermodynamics. Entropy has relevance to other areas of mathematics such as combinatorics. The definition can be derived from a set of axioms establishing that entropy should be a measure of how "surprising" the average outcome of a variable is. For a continuous random variable, differential entropy is analogous to entropy.

The basic idea of information theory is that the "informational value" of a communicated message depends on the degree to which the content of the message is surprising. If an event is very probable, it is no surprise (and generally uninteresting) when that event happens as expected; hence transmission of such a message carries very little new information. However, if an event is unlikely to occur, it is much more informative to learn that the event happened or will happen. For instance, the knowledge that some particular number "will not" be the winning number of a lottery provides very little information, because any particular chosen number will almost certainly not win. However, knowledge that a particular number "will" win a lottery has high value because it communicates the outcome of a very low probability event.

The information content (also called the "surprisal") of an event formula_7 is a function which decreases as the probability formula_8 of an event increases, defined by formula_9 or equivalently formula_10, where formula_6 is the logarithm. Entropy measures the expected (i.e., average) amount of information conveyed by identifying the outcome of a random trial. This implies that casting a die has higher entropy than tossing a coin because each outcome of a die toss has smaller probability (about formula_12) than each outcome of a coin toss (formula_13).

Consider the example of a coin toss. If the probability of heads is the same as the probability of tails, then the entropy of the coin toss is as high as it could be for a two-outcome trial. There is no way to predict the outcome of the coin toss ahead of time: if one has to choose, there is no average advantage to be gained by predicting that the toss will come up heads or tails, as either prediction will be correct with probability formula_14. Such a coin toss has entropy formula_15 (in bits) since there are two possible outcomes that occur with equal probability, and learning the actual outcome contains one bit of information. In contrast, a coin toss using a coin that has two heads and no tails has entropy formula_16 since the coin will always come up heads, and the outcome can be predicted perfectly. Similarly, one trit with equiprobable values contains formula_17 (about 1.58496) bits of information because it can have one of three values.

English text, treated as a string of characters, has fairly low entropy, i.e., is fairly predictable. If we do not know exactly what is going to come next, we can be fairly certain that, for example, 'e' will be far more common than 'z', that the combination 'qu' will be much more common than any other combination with a 'q' in it, and that the combination 'th' will be more common than 'z', 'q', or 'qu'. After the first few letters one can often guess the rest of the word. English text has between 0.6 and 1.3 bits of entropy per character of the message.

If a compression scheme is lossless – one in which you can always recover the entire original message by decompression – then a compressed message has the same quantity of information as the original, but communicated in fewer characters. It has more information (higher entropy) per character. A compressed message has less redundancy. Shannon's source coding theorem states a lossless compression scheme cannot compress messages, on average, to have "more" than one bit of information per bit of message, but that any value "less" than one bit of information per bit of message can be attained by employing a suitable coding scheme. The entropy of a message per bit multiplied by the length of that message is a measure of how much total information the message contains.

If one were to transmit sequences comprising the 4 characters 'A', 'B', 'C', and 'D', a transmitted message might be 'ABADDCAB'. Information theory gives a way of calculating the smallest possible amount of information that will convey this. If all 4 letters are equally likely (25%), one can't do better (over a binary channel) than to have 2 bits encode (in binary) each letter: 'A' might code as '00', 'B' as '01', 'C' as '10', and 'D' as '11'. If 'A' occurs with 70% probability, 'B' with 26%, and 'C' and 'D' with 2% each, one could assign variable length codes, so that receiving a '1' says to look at another bit unless 2 bits of sequential 1s have already been received. In this case, 'A' would be coded as '0' (one bit), 'B' as '10', and 'C' and 'D' as '110' and '111'. It is easy to see that 70% of the time only one bit needs to be sent, 26% of the time two bits, and only 4% of the time 3 bits. On average, fewer than 2 bits are required since the entropy is lower (owing to the high prevalence of 'A' followed by 'B' – together 96% of characters). The calculation of the sum of probability-weighted log probabilities measures and captures this effect.

Shannon's theorem also implies that no lossless compression scheme can shorten "all" messages. If some messages come out shorter, at least one must come out longer due to the pigeonhole principle. In practical use, this is generally not a problem, because one is usually only interested in compressing certain types of messages, such as a document in English, as opposed to gibberish text, or digital photographs rather than noise, and it is unimportant if a compression algorithm makes some unlikely or uninteresting sequences larger.

Named after Boltzmann's Η-theorem, Shannon defined the entropy (Greek capital letter eta) of a discrete random variable formula_18 with possible values formula_19 and probability mass function formula_20 as:

Here formula_22 is the expected value operator, and is the information content of .
formula_23 is itself a random variable.

The entropy can explicitly be written as:

where is the base of the logarithm used. Common values of are 2, Euler's number, and 10, and the corresponding units of entropy are the bits for , nats for , and bans for .

In the case of for some , the value of the corresponding summand is taken to be , which is consistent with the limit:

One may also define the conditional entropy of two events formula_1 and formula_26 taking values formula_27 and formula_28 respectively, as:

where formula_30 is the probability that formula_31 and formula_32. This quantity should be understood as the amount of randomness in the random variable formula_1 given the random variable formula_26.

Consider tossing a coin with known, not necessarily fair, probabilities of coming up heads or tails; this can be modelled as a Bernoulli process.

The entropy of the unknown result of the next toss of the coin is maximized if the coin is fair (that is, if heads and tails both have equal probability 1/2). This is the situation of maximum uncertainty as it is most difficult to predict the outcome of the next toss; the result of each toss of the coin delivers one full bit of information. This is because

However, if we know the coin is not fair, but comes up heads or tails with probabilities and , where , then there is less uncertainty. Every time it is tossed, one side is more likely to come up than the other. The reduced uncertainty is quantified in a lower entropy: on average each toss of the coin delivers less than one full bit of information. For example, if =0.7, then

Uniform probability yields maximum uncertainty and therefore maximum entropy. Entropy, then, can only decrease from the value associated with uniform probability. The extreme case is that of a double-headed coin that never comes up tails, or a double-tailed coin that never results in a head. Then there is no uncertainty. The entropy is zero: each toss of the coin delivers no new information as the outcome of each coin toss is always certain.

Entropy can be normalized by dividing it by information length. This ratio is called metric entropy and is a measure of the randomness of the information.

To understand the meaning of , first define an information function in terms of an event with probability . The amount of information acquired due to the observation of event follows from Shannon's solution of the fundamental properties of information:

Given two independent events, if the first event can yield one of equiprobable outcomes and another has one of equiprobable outcomes then there are equiprobable outcomes of the joint event. This means that if bits are needed to encode the first value and to encode the second, one needs to encode both.

Shannon discovered that a suitable choice of formula_37 is given by:

In fact, the only possible values of formula_37 are formula_40 for formula_41. Additionally, choosing a value for is equivalent to choosing a value formula_42 for formula_43, so that corresponds to the base for the logarithm. Thus, entropy is characterized by the above four properties.

The different units of information (bits for the binary logarithm , nats for the natural logarithm , bans for the decimal logarithm and so on) are constant multiples of each other. For instance, in case of a fair coin toss, heads provides bit of information, which is approximately 0.693 nats or 0.301 decimal digits. Because of additivity, tosses provide bits of information, which is approximately nats or decimal digits.

The "meaning" of the events observed (the meaning of "messages") does not matter in the definition of entropy. Entropy only takes into account the probability of observing a specific event, so the information it encapsulates is information about the underlying probability distribution, not the meaning of the events themselves.

Another characterization of entropy uses the following properties. We denote and .


The rule of additivity has the following consequences: for positive integers where ,

Choosing , this implies that the entropy of a certain outcome is zero: . This implies that the efficiency of a source alphabet with symbols can be defined simply as being equal to its -ary entropy. See also Redundancy (information theory).

The Shannon entropy satisfies the following properties, for some of which it is useful to interpret entropy as the amount of information learned (or uncertainty eliminated) by revealing the value of a random variable :


The inspiration for adopting the word "entropy" in information theory came from the close resemblance between Shannon's formula and very similar known formulae from statistical mechanics.

In statistical thermodynamics the most general formula for the thermodynamic entropy of a thermodynamic system is the Gibbs entropy,
where is the Boltzmann constant, and is the probability of a microstate. The Gibbs entropy was defined by J. Willard Gibbs in 1878 after earlier work by Boltzmann (1872).

The Gibbs entropy translates over almost unchanged into the world of quantum physics to give the von Neumann entropy, introduced by John von Neumann in 1927,
where ρ is the density matrix of the quantum mechanical system and Tr is the trace.

At an everyday practical level, the links between information entropy and thermodynamic entropy are not evident. Physicists and chemists are apt to be more interested in "changes" in entropy as a system spontaneously evolves away from its initial conditions, in accordance with the second law of thermodynamics, rather than an unchanging probability distribution. As the minuteness of Boltzmann's constant indicates, the changes in for even tiny amounts of substances in chemical and physical processes represent amounts of entropy that are extremely large compared to anything in data compression or signal processing. In classical thermodynamics, entropy is defined in terms of macroscopic measurements and makes no reference to any probability distribution, which is central to the definition of information entropy.

The connection between thermodynamics and what is now known as information theory was first made by Ludwig Boltzmann and expressed by his famous equation:

where formula_69 is the thermodynamic entropy of a particular macrostate (defined by thermodynamic parameters such as temperature, volume, energy, etc.), "W" is the number of microstates (various combinations of particles in various energy states) that can yield the given macrostate, and "k" is Boltzmann's constant. It is assumed that each microstate is equally likely, so that the probability of a given microstate is "p = 1/W". When these probabilities are substituted into the above expression for the Gibbs entropy (or equivalently "k" times the Shannon entropy), Boltzmann's equation results. In information theoretic terms, the information entropy of a system is the amount of "missing" information needed to determine a microstate, given the macrostate.

In the view of Jaynes (1957), thermodynamic entropy, as explained by statistical mechanics, should be seen as an "application" of Shannon's information theory: the thermodynamic entropy is interpreted as being proportional to the amount of further Shannon information needed to define the detailed microscopic state of the system, that remains uncommunicated by a description solely in terms of the macroscopic variables of classical thermodynamics, with the constant of proportionality being just the Boltzmann constant. Adding heat to a system increases its thermodynamic entropy because it increases the number of possible microscopic states of the system that are consistent with the measurable values of its macroscopic variables, making any complete state description longer. (See article: "maximum entropy thermodynamics"). Maxwell's demon can (hypothetically) reduce the thermodynamic entropy of a system by using information about the states of individual molecules; but, as Landauer (from 1961) and co-workers have shown, to function the demon himself must increase thermodynamic entropy in the process, by at least the amount of Shannon information he proposes to first acquire and store; and so the total thermodynamic entropy does not decrease (which resolves the paradox). Landauer's principle imposes a lower bound on the amount of heat a computer must generate to process a given amount of information, though modern computers are far less efficient.

Entropy is defined in the context of a probabilistic model. Independent fair coin flips have an entropy of 1 bit per flip. A source that always generates a long string of B's has an entropy of 0, since the next character will always be a 'B'.

The entropy rate of a data source means the average number of bits per symbol needed to encode it. Shannon's experiments with human predictors show an information rate between 0.6 and 1.3 bits per character in English; the PPM compression algorithm can achieve a compression ratio of 1.5 bits per character in English text.

Shannon's definition of entropy, when applied to an information source, can determine the minimum channel capacity required to reliably transmit the source as encoded binary digits. Shannon's entropy measures the information contained in a message as opposed to the portion of the message that is determined (or predictable). Examples of the latter include redundancy in language structure or statistical properties relating to the occurrence frequencies of letter or word pairs, triplets etc.

The minimum channel capacity can be realized in theory by using the typical set or in practice using Huffman, Lempel–Ziv or arithmetic coding. See also Kolmogorov complexity. In practice, compression algorithms deliberately include some judicious redundancy in the form of checksums to protect against errors.

A 2011 study in "Science" estimates the world's technological capacity to store and communicate optimally compressed information normalized on the most effective compression algorithms available in the year 2007, therefore estimating the entropy of the technologically available sources. 

The authors estimate humankind technological capacity to store information (fully entropically compressed) in 1986 and again in 2007. They break the information into three categories—to store information on a medium, to receive information through one-way broadcast networks, or to exchange information through two-way telecommunication networks.

Entropy is one of several ways to measure diversity. Specifically, Shannon entropy is the logarithm of , the true diversity index with parameter equal to 1.

There are a number of entropy-related concepts that mathematically quantify information content in some way:
(The "rate of self-information" can also be defined for a particular sequence of messages or symbols generated by a given stochastic process: this will always be equal to the entropy rate in the case of a stationary process.) Other quantities of information are also used to compare or relate different sources of information.

It is important not to confuse the above concepts. Often it is only clear from context which one is meant. For example, when someone says that the "entropy" of the English language is about 1 bit per character, they are actually modeling the English language as a stochastic process and talking about its entropy "rate". Shannon himself used the term in this way.

If very large blocks are used, the estimate of per-character entropy rate may become artificially low because the probability distribution of the sequence is not known exactly; it is only an estimate. If one considers the text of every book ever published as a sequence, with each symbol being the text of a complete book, and if there are published books, and each book is only published once, the estimate of the probability of each book is , and the entropy (in bits) is . As a practical code, this corresponds to assigning each book a unique identifier and using it in place of the text of the book whenever one wants to refer to the book. This is enormously useful for talking about books, but it is not so useful for characterizing the information content of an individual book, or of language in general: it is not possible to reconstruct the book from its identifier without knowing the probability distribution, that is, the complete text of all the books. The key idea is that the complexity of the probabilistic model must be considered. Kolmogorov complexity is a theoretical generalization of this idea that allows the consideration of the information content of a sequence independent of any particular probability model; it considers the shortest program for a universal computer that outputs the sequence. A code that achieves the entropy rate of a sequence for a given model, plus the codebook (i.e. the probabilistic model), is one such program, but it may not be the shortest.

The Fibonacci sequence is 1, 1, 2, 3, 5, 8, 13, ... treating the sequence as a message and each number as a symbol, there are almost as many symbols as there are characters in the message, giving an entropy of approximately . The first 128 symbols of the Fibonacci sequence has an entropy of approximately 7 bits/symbol, but the sequence can be expressed using a formula [ for , , ] and this formula has a much lower entropy and applies to any length of the Fibonacci sequence.

In cryptanalysis, entropy is often roughly used as a measure of the unpredictability of a cryptographic key, though its real uncertainty is unmeasurable. For example, a 128-bit key that is uniformly and randomly generated has 128 bits of entropy. It also takes (on average) formula_70 guesses to break by brute force. Entropy fails to capture the number of guesses required if the possible keys are not chosen uniformly. Instead, a measure called "guesswork" can be used to measure the effort required for a brute force attack.

Other problems may arise from non-uniform distributions used in cryptography. For example, a 1,000,000-digit binary one-time pad using exclusive or. If the pad has 1,000,000 bits of entropy, it is perfect. If the pad has 999,999 bits of entropy, evenly distributed (each individual bit of the pad having 0.999999 bits of entropy) it may provide good security. But if the pad has 999,999 bits of entropy, where the first bit is fixed and the remaining 999,999 bits are perfectly random, the first bit of the ciphertext will not be encrypted at all.

A common way to define entropy for text is based on the Markov model of text. For an order-0 source (each character is selected independent of the last characters), the binary entropy is:

where is the probability of . For a first-order Markov source (one in which the probability of selecting a character is dependent only on the immediately preceding character), the entropy rate is:

where is a state (certain preceding characters) and formula_73 is the probability of given as the previous character.

For a second order Markov source, the entropy rate is

A source alphabet with non-uniform distribution will have less entropy than if those symbols had uniform distribution (i.e. the "optimized alphabet"). This deficiency in entropy can be expressed as a ratio called efficiency:

Applying the basic properties of the logarithm, this quantity can also be expressed as:

Efficiency has utility in quantifying the effective use of a communication channel. This formulation is also referred to as the normalized entropy, as the entropy is divided by the maximum entropy formula_77. Furthermore, the efficiency is indifferent to choice of (positive) base , as indicated by the insensitivity within the final logarithm above thereto.

The Shannon entropy is restricted to random variables taking discrete values. The corresponding formula for a continuous random variable with probability density function with finite or infinite support formula_78 on the real line is defined by analogy, using the above form of the entropy as an expectation:

This is the differential entropy (or continuous entropy). A precursor of the continuous entropy is the expression for the functional in the H-theorem of Boltzmann.

Although the analogy between both functions is suggestive, the following question must be set: is the differential entropy a valid extension of the Shannon discrete entropy? Differential entropy lacks a number of properties that the Shannon discrete entropy has – it can even be negative – and corrections have been suggested, notably limiting density of discrete points.

To answer this question, a connection must be established between the two functions:

In order to obtain a generally finite measure as the bin size goes to zero. In the discrete case, the bin size is the (implicit) width of each of the (finite or infinite) bins whose probabilities are denoted by . As the continuous domain is generalised, the width must be made explicit.

To do this, start with a continuous function discretized into bins of size formula_80.

By the mean-value theorem there exists a value in each bin such that

the integral of the function can be approximated (in the Riemannian sense) by

where this limit and "bin size goes to zero" are equivalent.

We will denote

and expanding the logarithm, we have

As Δ → 0, we have

Note; as , requires a special definition of the differential or continuous entropy:

which is, as said before, referred to as the differential entropy. This means that the differential entropy "is not" a limit of the Shannon entropy for . Rather, it differs from the limit of the Shannon entropy by an infinite offset (see also the article on information dimension).

It turns out as a result that, unlike the Shannon entropy, the differential entropy is "not" in general a good measure of uncertainty or information. For example, the differential entropy can be negative; also it is not invariant under continuous co-ordinate transformations. This problem may be illustrated by a change of units when "x" is a dimensioned variable. "f(x)" will then have the units of "1/x". The argument of the logarithm must be dimensionless, otherwise it is improper, so that the differential entropy as given above will be improper. If "Δ" is some "standard" value of "x" (i.e. "bin size") and therefore has the same units, then a modified differential entropy may be written in proper form as:

and the result will be the same for any choice of units for "x". In fact, the limit of discrete entropy as formula_88 would also include a term of formula_89, which would in general be infinite. This is expected, continuous variables would typically have infinite entropy when discretized. The limiting density of discrete points is really a measure of how much easier a distribution is to describe than a distribution that is uniform over its quantization scheme.

Another useful measure of entropy that works equally well in the discrete and the continuous case is the relative entropy of a distribution. It is defined as the Kullback–Leibler divergence from the distribution to a reference measure as follows. Assume that a probability distribution is absolutely continuous with respect to a measure , i.e. is of the form for some non-negative -integrable function with -integral 1, then the relative entropy can be defined as

In this form the relative entropy generalises (up to change in sign) both the discrete entropy, where the measure is the counting measure, and the differential entropy, where the measure is the Lebesgue measure. If the measure is itself a probability distribution, the relative entropy is non-negative, and zero if as measures. It is defined for any measure space, hence coordinate independent and invariant under co-ordinate reparameterizations if one properly takes into account the transformation of the measure . The relative entropy, and implicitly entropy and differential entropy, do depend on the "reference" measure .

Entropy has become a useful quantity in combinatorics.

A simple example of this is an alternate proof of the Loomis–Whitney inequality: for every subset , we have
where is the orthogonal projection in the th coordinate:

The proof follows as a simple corollary of Shearer's inequality: if are random variables and are subsets of } such that every integer between 1 and lies in exactly of these subsets, then
where formula_94 is the Cartesian product of random variables with indexes in (so the dimension of this vector is equal to the size of ).

We sketch how Loomis–Whitney follows from this: Indeed, let be a uniformly distributed random variable with values in and so that each point in occurs with equal probability. Then (by the further properties of entropy mentioned above) , where denotes the cardinality of . Let }. The range of formula_95 is contained in and hence formula_96. Now use this to bound the right side of Shearer's inequality and exponentiate the opposite sides of the resulting inequality you obtain.

For integers let . Then
where 

A nice interpretation of this is that the number of binary strings of length with exactly many 1's is approximately formula_99.




</doc>
<doc id="15446" url="https://en.wikipedia.org/wiki?curid=15446" title="Ithaca College">
Ithaca College

Ithaca College is a private liberal arts college in Ithaca, New York. The college was founded by William Egbert in 1892 as a conservatory of music and is set against the backdrop of the city of Ithaca, Cayuga Lake, waterfalls, and gorges. The college is best known for its large list of alumni who have played substantial roles in the media and entertainment industries.

Ithaca College is internationally known for the Roy H. Park School of Communications, which is ranked by several organizations as a top school for journalism, film, media and entertainment. The college has a strong liberal arts core, and offers several pre-professional programs, along with some graduate programs.

Ithaca College has been ranked among the Top 10 masters universities in the "Regional Universities North" category by "U.S. News & World Report," every year since 1996, and was ranked 9th in 2019. Ithaca College is consistently named among the best colleges in the nation by "Princeton Review", with the 2018 guide ranking the college number 3 for theater, number 3 for newspaper, and number 6 for Radio, and is among the top schools producing Fulbright scholarship recipients.

Ithaca College was founded as the Ithaca Conservatory of Music in 1892 when a local violin teacher, William Grant Egbert, rented four rooms and arranged for the instruction of eight students. For nearly seven decades the institution flourished in the city of Ithaca, adding to its music curriculum the study of elocution, dance, physical education, speech correction, radio, business, and the liberal arts. In 1931 the conservatory was chartered as a private college. The college was originally housed in the Boardman House, that later became the Ithaca College Museum of Art, and it was listed on the National Register of Historic Places in 1971.

By 1960, some 2,000 students were in attendance. A modern campus was built on South Hill in the 1960s, and students were shuttled between the old and new during the construction. The hillside campus continued to grow in the ensuing 30 years to accommodate more than 6,000 students.

As the campus expanded, the college also began to expand its curriculum. By the 1990s, some 2,000 courses in more than 100 programs of study were available in the college's five schools. The school attracts a multicultural student body with representatives from almost every state and from 78 foreign countries.

Ithaca College's current campus was built in the 1960s on South Hill. The college's final academic department moved from downtown to the South Hill campus in 1968, making the move complete.

Besides its Ithaca campus, Ithaca College has also operated satellite campuses in other cities. The Ithaca College London Center has been in existence since 1972. Ithaca runs the Ithaca College Los Angeles Program at the James B. Pendleton Center. Additionally, there is an Ithaca College Washington Semester Program, and a recently launched Ithaca College New York City Center.

Former programs include the Ithaca College Antigua Program and the Ithaca College Walkabout Down Under Program in Australia.

Ithaca College also operates direct enrollment exchange programs with several universities, including Griffith University, La Trobe University, Murdoch University, and University of Tasmania (Australia); Chengdu Sport University and Beijing Sport University (China); University of Hong Kong (Hong Kong); Masaryk University (Czech Republic); Akita International University and University of Tsukuba (Japan); Hanyang University (Korea); Nanyang Technological University (Singapore); University of Valencia (Spain); and Jönköping University (Sweden). Ithaca College is also affiliated with study abroad programs such as IES Abroad and offers dozens of exchange or study abroad options to students.

The college offers a curriculum with more than 100 degree programs in its five schools.

Until recently, several cross-disciplinary degree programs, along with the Center for the Study of Culture, Race, and Ethnicity, were housed in the Division of Interdisciplinary and International Studies; however, starting in the spring of 2011, the division was eliminated and its programs, centers and institutes were absorbed into other schools.

, the most popular majors included visual and performing arts, health professions and related programs, business, management, marketing, and related support services and biological and biomedical Sciences.

Ithaca College is well known for its several prominent student-run media vehicles, including:


Historically, various independent and national fraternities and sororities had active chapters at Ithaca College. However, due to a series of highly publicized hazing incidents in the 1980s, including one that was responsible for the death of a student, the college administration reevaluated their Greek life policy and only professional music fraternities were allowed to remain affiliated with the school.

, three recognized Greek organizations remain on campus, all of which are music-oriented:


A fourth house, performing arts fraternity Kappa Gamma Psi (Iota Chapter) became inactive in 2008. Although there are potentially plans to reactivate the chapter, it is unclear whether this will be permitted or not due to the college's policy on Greek Life.

However, there are various Greek letter organizations at Ithaca College that are unaffiliated with the school, and therefore not subject to the same housing privileges or rules that contribute to the safety of their members such as non-hazing and non-drinking policies. Additionally, while not particularly common, Ithaca College students may rush for Greek houses affiliated with Cornell University, subject to the rules of each individual fraternity or sorority. Some Cornell-affiliated Greek organizations actively recruit Ithaca College students.

There are a few unaffiliated fraternities that some Ithaca College students join - ΔΚΕ (Delta Kappa Epsilon), ΑΕΠ (Alpha Epsilon Pi), and ΚΣ (Kappa Sigma). There is one unaffiliated sorority - ΓΔΠ (Gamma Delta Pi).

Ithaca competes in athletics at the NCAA Division III level as a members of the Liberty League and the Eastern College Athletic Conference (ECAC). Ithaca has one of Division III's strongest athletic programs, with the Bombers winning a total of 15 national titles in seven team sports and five individual sports. Ithaca was previously a member of the Empire 8.

The Ithaca athletics nickname "Bombers" is unique in NCAA athletics, and the origins of the nickname are obscure. Ithaca College's sports teams were originally named the Cayugas, but the name was changed to the Bombers sometime in the 1930s. Some other names that have been used for Ithaca College's teams include: Blue Team, Blues, Blue and Gold, Collegians, and the Seneca Streeters. Several possibilities for the change to the "Bombers" have been posited. The most common explanation is that the school's baseball uniforms—white with navy blue pinstripes and an interlocking "IC" on the left chest—bear a striking resemblance to the distinctive home uniforms of the New York Yankees, who are known as the Bronx Bombers. It may also have referred to the Ithaca basketball team of that era and its propensity for half-court "bombs". Grumman Aircraft also manufactured airplanes including bombers in Ithaca for many years. The first “Bombers” reference on record was in the December 17, 1938 issue of the "Rochester Times-Union" in a men's basketball article.

The name has at times sparked controversy for its perceived martial connotations. It is an occasional source of umbrage from Ithaca's prominent pacifist community, but the athletics department has consistently stated it has no interest in changing the name. The athletics logo has in the past incorporated World War II era fighter planes, but currently does not, and the school does not currently have a physical mascot to personify the name. In 2010 the school launched a contest to choose one. It received over 250 suggestions and narrowed the field down to three: a phoenix, a flying squirrel, and a Lake Beast. In June 2011, President Rochon announced that the school would discontinue the search due to opposition in the alumni community.

Ithaca College remodeled the Hill Center in 2013. The building features hardwood floors (Ben Light Gymnasium) as well as coaches offices. The building is home to Ithaca's men's and women's basketball teams, women's volleyball team, wrestling, and gymnastics. Ithaca also opened the Athletics & Events Center in 2011, a $65.5 million facility funded by donors. The facility is mainly used by the school's varsity athletes. It has a 47,000 square foot, 9-lane 50 meter Olympic-size pool. The building also has Glazer Arena, a 130,000 square foot event space. It is a track and field center that doubles as a practice facility for lacrosse, field hockey, soccer, baseball, tennis, and football. The facility was designed by the architectural firm Moody-Nolan and began construction in June 2009.

Coached by Jim Butterfield for 27 years, the football team has won three NCAA Division III National Football Championships in 1979, 1988 and 1991 (a total surpassed only by Augustana, Mount Union and Wisconsin-Whitewater). Bomber football teams made a record seven appearances in the Division III national championship game, the Amos Alonzo Stagg Bowl, which has since been surpassed by Mount Union in 2003. The Bombers play the SUNY Cortland Red Dragons for the Cortaca Jug, which was added in 1959 to an already competitive rivalry. The match-up is one of the most prominent in Division III college football. The game alternates locations between Ithaca and Cortland. Cortland had won the Cortaca Jug six years in a row until Ithaca broke the streak in 2017.

Women's soccer has won two national championships in Division III and is consistently ranked in the top 20 nationally.

Gymnastics won the NCAA Division III national championships in 1998.

The men's wrestling team won NCAA Division III National Championships in 1989, 1990 and 1994.

Women's field hockey won the 1982 NCAA Division III Field Hockey Championship.

In 2013, Paula Miller, head of the women's swimming team completed her 30th year as head coach of the Ithaca Bombers. She has led the team to many victories. In the previous four years, the Bombers were undefeated throughout their season defeating tough competition. Ithaca has finished first or second at 25 of the past 29 state meets. The Bombers have also won the Empire 8 crown in each of the past nine seasons.

The 2013–2014 season ended with regaining the NCAA Division III Championship trophy.

During the 2015–2016 season the Bombers swimming and diving team held the UNYSCSA Empire 8 state champion meet in the Athletic and Events center at Ithaca College. The men's swimming and diving team scored 616.5 points, finishing 4th in states under coach Kevin Markwardt. The men's team was led by captain Addison Hebert, who was injured the first day of the meet and was able to overcome it by the last day helping the rest of the bombers get 3rd place in the 400 freestyle relay by .01 seconds. The girls' swimming and diving team scored 1227 points, winning states under Paula Miller. The bombers were to bring two women divers to South Carolina, to compete in nationals in March. During the 2017-2018 season the Bombers' Veronica Griesemer won the diving national championships.

The Men’s and Women’s Crew programs are housed in the Robert B. Tallman Rowing Center, a $2.6 million boathouse dedicated in 2012. The new boathouse replaced the Haskell Davidson Boathouse, which was constructed in 1974 on Cayuga Inlet. The old boathouse was razed to make room for the new facility. At 8,500 square feet, the Tallman boathouse is almost twice the size of the previous structure.

The women's crew won back-to-back NCAA Division III Rowing Championships in 2004 and 2005. The men's crew received 4 medals at the New York State Collegiate Championships in 2008.

Ithaca is also home to more than 60 club sports, many of which compete regularly against other colleges in leagues and tournaments. The Men's Rugby team is of particular note, consistently earning a top-25 ranking under NSCRO. Repeating as Upstate Small College Rugby Conference champions in 2019, they earned a trip to the Northeast Regional playoff. The team finished 3rd in the region for a second year in a row.

Along with Intercollegiate athletics, Ithaca College has a large intramural sport program. This extracurricular program serves approximately 25% of the undergraduate population yearly. Fourteen traditional team activities are offered throughout the year and include basketball, flag football, kickball, soccer, softball, ultimate frisbee, ski racing, and volleyball.

For most activities, divisions are offered for men's, women's, and co-recreational teams. Throughout the year usually two or more activities run concurrently and participants are able to play on a single sex team and co-recreational team for each activity.

Ithaca's School of Business was the first college or university business school in the world to achieve LEED Platinum Certification alongside Yale University, which had the second. Ithaca's Peggy Ryan Williams Center is also LEED Platinum certified. It makes extensive use of day light in occupied spaces. There are sensors that regulate lighting and ventilation based on occupancy and natural light. Over 50% of the building energy comes from renewable sources such as wind power. The college also has a LEED Gold Certified building, the Athletics & Events Center. The college composts its dining hall waste, runs a "Take It or Leave It" Green move-out program, and offers a sustainable living option. It also operates an office supply collection and reuse program, as well as a sustainability education program during new student orientation. Ithaca received a B- grade on the Sustainable Endowments Institute's 2009 College Sustainability Report Card and an A- for 2010.

In 2017, Ithaca College was listed as one of Princeton Review's top "green colleges" for being environmentally responsible.

In the spring of 2007, then-President Peggy R. Williams signed the American College and University President's Climate Commitment (ACUPCC), pledging Ithaca College to the task of developing a strategy and long-range plan to achieve "carbon neutrality" at some point in the future. In 2009 the Ithaca College Board of Trustees approved the Ithaca College Climate Action Plan, which calls for 100% carbon neutrality by 2050. In 2009, the Ithaca College Board of Trustees approved the Ithaca College Climate Action Plan, which calls for 100% carbon neutrality by 2050 and offers a 40-year action plan to work toward that ambitious goal.

The college purchases 100 percent of its electricity from renewable sources. Including offsets from a solar farm, the college's overall energy usage is 45 percent carbon neutral.

The college aims to optimize investment returns and does not invest the endowment in on-campus sustainability projects, renewable energy funds, or community development loan funds. The college's investment policy reserves the right of the investment committee to restrict investments for any reason, which could include environmental and sustainability factors.

While the Ithaca College Natural Lands has issued a statement that Ithaca College should join efforts calling for a moratorium on horizontal drilling and high volume (“slick water”) hydraulic fracturing, or fracking, the college as a whole has refused to issue a statement regarding the issue.

Ithaca's current president is Shirley M. Collado. She was named the ninth president of Ithaca College on February 22, 2017, and assumed the presidency on July 1, 2017. She was previously executive vice chancellor and chief operating officer at Rutgers University–Newark and vice president of student affairs and dean of the college at Middlebury College. She is the first Dominican American to be named president of a college in the United States.

Collado succeeds Thomas Rochon, who was named eighth president of Ithaca College on April 11, 2008. Rochon took over as president of the college following Peggy Williams, who had announced on July 12, 2007, that she would retire from the presidency post effective May 31, 2009, following a one-year sabbatical. During the fall 2015 semester, multiple protests focusing on campus climate and Rochon's leadership were led by students and faculty. After multiple racially charged events including student house party themes and racially tinged comments at administration led-programs, students, faculty and staff all decided to hold votes of "no confidence" in Rochon. Students voted "no confidence" by a count of 72% no confidence, 27% confidence, and 1% abstaining. The faculty voted 77.8% no confidence to 22.2% confidence. Rochon retired on July 1, 2017.

Ithaca College has over 70,000 alumni, with clubs in Boston, Chicago, Connecticut, Los Angeles, Metro New York, National Capital, North and South Carolina, Philadelphia, Rochester (NY), San Diego, and Southern Florida. Alumni events are hosted in cooperation with city-specific clubs and through a program called "IC on the Road".

Following is a brief list of noteworthy Ithaca College alumni.
For a more extensive list, refer to the List of Ithaca College alumni.

Following is a brief list of current and former noteworthy Ithaca College faculty.


</doc>
<doc id="15447" url="https://en.wikipedia.org/wiki?curid=15447" title="Differential psychology">
Differential psychology

Differential psychology studies the ways in which individuals differ in their behavior and the processes that underlie it. This is a discipline that develops classifications (taxonomies) of psychological individual differences. This is distinguished from other aspects of psychology in that although psychology is ostensibly a study of individuals, modern psychologists often study groups, or attempt to discover general psychological processes that apply to all individuals. This particular area of psychology has been named first named “differential psychology” at Breslau University and kept the name as Vidkunn Coucheron Jarl quotes William Stern when he calls this field “a new and fast growing off-shoot of experimental psychology.”(Jarl, 2013). 

While prominent psychologists, including Stern, have been widely credited for the concept of individual differences, historical records show that it was Charles Darwin (1859) who first spurred the scientific interest in the study of individual differences. His interest was further pursued by his half-cousin Francis Galton in his attempt to quantify individual differences among people. 

For example, in evaluating the effectiveness of a new therapy, the mean performance of the therapy in one treatment group might be compared to the mean effectiveness of a placebo (or a well-known therapy) in a second, control group. In this context, differences between individuals in their reaction to the experimental and control manipulations are actually treated as errors rather than as interesting phenomena to study. This approach is applied because psychological research depends upon statistical controls that are only defined upon groups of people. 

Importantly, individuals can also differ not only in their
current state, but in the magnitude or even direction of response to a
given stimulus. Such phenomena, often
explained in terms of inverted-U response curves,
place differential psychology at an important location in such
endeavours as personalized medicine, in which diagnoses are
customised for an individual's response profile.

Individual differences research typically includes personality, temperament (neuro-chemically-based behavioural traits), motivation, intelligence, ability, IQ, interests, values, self-concept, self-efficacy, and self-esteem (to name just a few). There are few remaining "differential psychology" programs in the United States, although research in this area is very active. Current researchers are found in a variety of applied and experimental programs, including clinical psychology, educational psychology, Industrial and organizational psychology, personality psychology, social psychology, behavioral genetics, and developmental psychology programs, in the neo-Piagetian theories of cognitive development in particular.

To study individual differences, psychologists use a variety of methods. Psychophysiological experiments on both humans and other mammals include EEG, PET-scans, MRI, functional MRI, neurochemistry experiments with neurotransmitter and hormonal systems, caffeine and controlled drug challenges. These methods can be used for a search of biomarkers of consistent, biologically-based behavioural patterns (temperament traits and symptoms of psychiatric disorders). Other sets of methods include behavioural experiments, to see how different people behave in similar settings. Behavioural experiments are often used in personality and social psychology, and include lexical and self-report methods where people are asked to complete paper-based and computer-based forms prepared by psychologists.




</doc>
<doc id="15450" url="https://en.wikipedia.org/wiki?curid=15450" title="Industrial and organizational psychology">
Industrial and organizational psychology

Industrial and organizational psychology, which is also known as occupational psychology, organizational psychology, or work and organizational psychology; is an applied discipline within psychology. Industrial, work and organizational psychology (IWO) is the broader global term for the field internationally. 

The discipline is the science of human behavior relating to work and applies psychological theories and principles to organizations and individuals in their places of work as well as the individual's work-life more generally. IO psychologists are trained in the scientist–practitioner model. They contribute to an organization's success by improving the performance, motivation, job satisfaction, and occupational safety and health as well as the overall health and well-being of its employees. An IO psychologist conducts research on employee behaviors and attitudes, and how these can be improved through hiring practices, training programs, feedback, and management systems. 

IO psychology was ranked the fastest growing occupation over the next decade according to the US Bureau of Labor Statistics's "Occupational Outlook Handbook" in 2014. It is estimated to grow 53% with a mean salary of US$109,030, with those at the top 10 percentile earning $192,150 for 2018. 

As of 2020, IO psychology is one of the 17 recognized professional specialties by the American Psychological Association (APA) in the United States. It is represented by Division 14 of the APA and is formally known as the Society for Industrial and Organizational Psychology (SIOP).

In the United Kingdom, industrial and organizational psychologists are referred to as occupational psychologists. Occupational psychology in the UK is one of nine "protected titles" within the profession "practitioner psychologist" regulated by the Health and Care Professions Council. In the UK, graduate programs in psychology, including occupational psychology, are accredited by the British Psychological Society.

In Australia, the title organizational psychologist is protected by law, and regulated by the Australian Health Practitioner Regulation Agency (AHPRA). Organizational psychology is one of nine areas of specialist endorsement for psychology practice in Australia.

In Europe, someone with a specialist EuroPsy Certificate in Work and Organisational Psychology is a fully qualified psychologist and a specialist in the work psychology field. Industrial and organizational psychologists reaching the EuroPsy standard are recorded in the Register of European Psychologists and industrial and organizational psychology is one of the three main psychology specializations in Europe.

In South Africa, industrial psychology is a registration category for the profession of psychologist as regulated by the Health Professions Council of South Africa (HPCSA).

The historical development of IO psychology was paralleled in the US, the UK, Australia, Germany, the Netherlands, and Eastern European countries such as Romania. The roots of IO psychology trace back nearly to the beginning of psychology as a science, when Wilhelm Wundt founded one of the first psychological laboratories in 1879 in Leipzig, Germany. In the mid 1880s, Wundt trained two psychologists, Hugo Münsterberg and James McKeen Cattell, who had a major influence on the emergence of IO psychology.

Instead of viewing performance differences as human "errors", Cattell was one of the first to recognize the importance of differences among individuals as a way of better understanding work behavior. Walter Dill Scott, who was a contemporary of Cattell, was elected President of the American Psychological Association (APA) in 1919, was arguably the most prominent IO psychologist of his time. Scott, along with Walter Van Dyke Bingham, worked at the Carnegie Institute of Technology, developing methods for selecting and training sales personnel.

The "industrial" side of IO psychology originated in research on individual differences, assessment, and the prediction of work performance. Industrial psychology crystallized during World War I. In response to the need to rapidly assign new troops to duty. Scott and Bingham volunteered to help with the testing and placement of more than a million army recruits. In 1917, together with other prominent psychologists, they adapted a well-known intelligence test the Stanford–Binet, which was designed for testing one individual at a time, to make it suitable for group testing. The new test was called the Army Alpha.

After the war, the growing industrial base in the US was a source of momentum for what was then called industrial psychology. Private industry set out to emulate the successful testing of army personnel. Mental ability testing soon became commonplace in the work setting.

Elton Mayo found that rest periods improved morale and reduced turnover in a Philadelphia textile factory. He later joined the ongoing Hawthorne studies, where he became interested in how workers' emotions and informal relationships affected productivity. The results of these studies ushered in the human relations movement.

World War II brought renewed interest in ability testing (to accurately place recruits in new technologically advanced military jobs), the introduction of the assessment center, and concern with morale and fatigue in war industry workers.

The industrial psychology division of the former American Association of Applied Psychology became a division within APA, becoming Division 14 of APA. It was initially called the Industrial and Business Psychology Division. In 1962, the name was changed to the Industrial Psychology Division. In 1973, it was renamed again, this time to the Division of Industrial and Organizational Psychology. In 1982, the unit become more independent of APA, and its name was changed again, this time to the Society for Industrial and Organizational Psychology.

The name change of the division from "industrial psychology" to "industrial and organizational psychology" reflected the shift in the work of industrial psychologists who had originally addressed work behavior from the individual perspective, examining performance and attitudes of individual workers. Their work became broader. Group behavior in the workplace became a worthy subject of study. The emphasis on "organizational" underlined the fact that when an individual joins an organization (e.g., the organization that hired him or her), he or she will be exposed to a common goal and a common set of operating procedures. In the 1970s in the UK, references to occupational psychology became more common than IO psychology.

According to Bryan and Vinchur, "while organizational psychology increased in popularity through [the 1960s and 1970s], research and practice in the traditional areas of industrial psychology continued, primarily driven by employment legislation and case law". There was a focus on fairness and validity in selection efforts as well as in the job analyses that undergirded selection instruments. For example, IO psychology showed increased interest in behaviorally anchored rating scales. What critics there were of IO psychology accused the discipline of being responsive only to the concerns of managements.

From the 1980s to 2010s, other changes in IO psychology took place. Researchers increasingly adopted a multi-level approach, attempting to understand behavioral phenomena from both the level of the organization and the level of the individual worker. There was also an increased interest in the needs and expectations of employees as individuals. For example, an emphasis on organizational justice and the psychological contract took root, as well as the more traditional concerns of selection and training. Methodological innovations (e.g., meta-analyses, structural equation modeling) were adopted. With the passage of the American with Disabilities Act in 1990 and parallel legislation elsewhere in the world, IO psychology saw an increased emphasis on "fairness in personnel decisions." Training research relied increasingly on advances in educational psychology and cognitive science.

As described above, IO psychologists are trained in the scientist–practitioner model. IO psychologists rely on a variety of methods to conduct organizational research. Study designs employed by IO psychologists include surveys, experiments, quasi-experiments, and observational studies. IO psychologists rely on diverse data sources including human judgments, historical databases, objective measures of work performance (e.g., sales volume), and questionnaires and surveys.

IO researchers employ quantitative statistical methods. Quantitative methods used in IO psychology include correlation, multiple regression, and analysis of variance. More advanced statistical methods employed in IO research include logistic regression, structural equation modeling, and hierarchical linear modeling (HLM; also known as multilevel modeling). IO research has also employed meta-analysis. IO psychologists also employ psychometric methods including methods associated with classical test theory, generalizability theory, and item response theory (IRT).

IO psychologists have also employed qualitative methods, which largely involve focus groups, interviews, and case studies. IO research on organizational culture research has employed ethnographic techniques and participant observation. A qualitative technique associated with IO psychology is Flanagan's Critical Incident Technique. IO psychologists sometimes use quantitative and qualitative methods in concert. OHP researchers have also combined and coordinated quantitative and qualitative methods within a single study.

Job analysis encompasses a number of different methods including, but not limited to, interviews, questionnaires, task analysis, and observation. It primarily involves the systematic collection of information about a job. A task-oriented job analysis involves an examination of the duties, tasks, and/or competencies required by the job being assessed. By contrast, a worker-oriented job analysis involves an examination of the knowledge, skills, abilities, and other characteristics (KSAOs) required to successfully perform the work. Information obtained from job analyses are used for many purposes, including the creation of job-relevant selection procedures, performance appraisals and the criteria they require, and the development of training programs.

IO psychologists typically work with human resource specialists to design (a) recruitment processes and (b) personnel selection systems. Personnel recruitment is the process of identifying qualified candidates in the workforce and getting them to apply for jobs within an organization. Personnel recruitment processes include developing job announcements, placing ads, defining key qualifications for applicants, and screening out unqualified applicants.

Personnel selection is the systematic process of hiring and promoting personnel. Personnel selection systems employ evidence-based practices to determine the most qualified candidates. Personnel selection involves both the newly hired and individuals who can be promoted from within the organization. Common selection tools include ability tests (e.g., cognitive, physical, or psycho-motor), knowledge tests, personality tests, structured interviews, the systematic collection of biographical data, and work samples. IO psychologists must evaluate evidence regarding the extent to which selection tools predict job performance.

Personnel selection procedures are usually validated, i.e., shown to be job relevant to personnel selection, using one or more of the following types of validity: content validity, construct validity, and/or criterion-related validity. IO psychologists must adhere to professional standards in personnel selection efforts. SIOP (e.g., "Principles for validation and use of personnel selection procedures") and APA together with the National Council on Measurement in Education (e.g., "Standards for educational and psychological testing" are sources of those standards. The Equal Employment Opportunity Commission's "Uniform guidelines" are also influential in guiding personnel selection decisions.

A meta-analysis of selection methods found that general mental ability was the best overall predictor of job performance and attainment in training.

Performance appraisal or performance evaluation is the process in which an individual's or a group's work behaviors and outcomes are assessed against managers' and others' expectations for the job. Performance appraisal is frequently used in promotion and compensation decisions, to help design and validate personnel selection procedures, and for performance management. Performance management is the process of providing performance feedback relative to expectations, and information relevant to improvement (e.g., coaching, mentoring). Performance management may also include documenting and tracking performance information for organizational evaluation purposes.

An IO psychologist would typically use information from the job analysis to determine a job's performance dimensions, and then construct a rating scale to describe each level of performance for the job. Often, the IO psychologist would be responsible for training organizational personnel how to use the performance appraisal instrument, including ways to minimize bias when using the rating scale, and how to provide effective performance feedback.

Individual assessment involves the measurement of individual differences. IO psychologists perform individual assessments in order to evaluate differences among candidates for employment as well as differences among employees. The constructs measured pertain to job performance. With candidates for employment, individual assessment is often part of the personnel selection process. These assessments can include written tests, aptitude tests, physical tests, psycho-motor tests, personality tests, integrity and reliability tests, work samples, simulations, and assessment centres.

IO psychologists are concerned with occupational health and well-being for well over a century. Developments early in the 20th century occurred in both the UK and the U.S. During World War I Charles Myers in the U.K. studied worker fatigue and other aspects of well-being, discussed in his 1920 IO psychology textbook. In the U.S. Arthur Kornhauser examined the impact on productivity of hiring mentally unstable workers. Kornhauser also examined the link between industrial working conditions and mental health as well as the spillover into a worker's personal life of having an unsatisfying job.

More recently, IO researchers have found that staying vigorous during working hours is associated with better work-related behaviour and subjective well-being as well as more effective functioning in the family domain. Trait vigor and recovery experiences after work were related to vigor at work. Job satisfaction has also been found to be associated with life satisfaction, happiness, well-being and positive affect, and the absence of negative affect. Other research indicates that among older workers activities such as volunteering and participating in social clubs was related to a decrease in depressive symptoms over the next two years. Research on job changing indicates that mobility between, but not within, organizations is associated with burnout. In the late 1980s and early 1990s, a new discipline, occupational health psychology, emerged out of i/o psychology and both health psychology, and occupational medicine.

IO psychologists are concerned with the related topics of workplace bullying, aggression, and violence. For example, IO research found that exposure to workplace violence elicited ruminative thinking. Ruminative thinking is associated with poor well-being. IO research has found that interpersonal aggressive behaviour is associated with worse team performance.

Compensation includes wages or salary, bonuses, pension/retirement contributions, and employee benefits that can be converted to cash or replace living expenses. IO psychologists may be asked to conduct a job evaluation for the purpose of determining compensation levels and ranges. IO psychologists may also serve as expert witnesses in pay discrimination cases, when disparities in pay for similar work are alleged by employees.

Training involves the systematic teaching of skills, concepts, or attitudes that results in improved performance in another environment. Because many people hired for a job are not already versed in all the tasks the job requires, training may be needed to help the individual perform the job effectively. Evidence indicates that training is often effective, and that it succeeds in terms of higher net sales and gross profitability per employee.

Similar to performance management (see above), an IO psychologist would employ a job analysis in concert with the application of the principles of instructional design to create an effective training program. A training program is likely to include a summative evaluation at its conclusion in order to ensure that trainees have met the training objectives and can perform the target work tasks at an acceptable level. Training programs often include formative evaluations to assess the effect of the training as the training proceeds. Formative evaluations can be used to locate problems in training procedures and help IO psychologists make corrective adjustments while training is ongoing.

The foundation for training programs is learning. Learning outcomes can be organized into three broad categories: cognitive, skill-based, and affective outcomes. Cognitive training is aimed at instilling declarative knowledge or the knowledge of rules, facts, and principles (e.g., police officer training covers laws and court procedures). Skill-based training aims to impart procedural knowledge (e.g., skills needed to use a special tool) or technical skills (e.g., understanding the workings of software program). Affective training concerns teaching individuals to develop specific attitudes or beliefs that predispose trainees to behave a certain way (e.g., show commitment to the organization, appreciate diversity).

A needs assessment, an analysis of corporate and individual goals, is often undertaken prior to the development of a training program. In addition, a careful needs analysis is required in order to develop a systematic understanding of where training is needed, what should be taught, and who will be trained. A training needs analysis typically involves a three-step process that includes organizational analysis, task analysis and person analysis.

An organizational analysis is an examination of organizational goals and resources as well as the organizational environment. The results of an organizational analysis help to determine where training should be directed. The analysis identifies the training needs of different departments or subunits. It systematically assesses manager, peer, and technological support for transfer of training. An organizational analysis also takes into account the climate of the organization and its subunits. For example, if a climate for safety is emphasized throughout the organization or in subunits of the organization (e.g., production), then training needs will likely reflect an emphasis on safety. A task analysis uses the results of a job analysis to determine what is needed for successful job performance, contributing to training content. With organizations increasingly trying to identify "core competencies" that are required for all jobs, task analysis can also include an assessment of competencies. A person analysis identifies which individuals within an organization should receive training and what kind of instruction they need. Employee needs can be assessed using a variety of methods that identify weaknesses that training can address.

Work motivation reflects the energy an individual applies "to initiate work-related behavior, and to determine its form, direction, intensity, and duration" Understanding what motivates an organization's employees is central to IO psychology. Motivation is generally thought of as a theoretical construct that fuels behavior. An incentive is an anticipated reward that is thought to incline a person to behave a certain way. Motivation varies among individuals. Studying its influence on behavior, it must be examined together with ability and environmental influences. Because of motivation's role in influencing workplace behavior and performance, many organizations structure the work environment to encourage productive behaviors and discourage unproductive behaviors.

Motivation involves three psychological processes: arousal, direction, and intensity. Arousal is what initiates action. It is often fueled by a person's need or desire for something that is missing from his or her life, either totally or partially. Direction refers to the path employees take in accomplishing the goals they set for themselves. Intensity is the amount of energy employees put into goal-directed work performance. The level of intensity often reflects the importance and difficulty of the goal. These psychological processes involve four factors. First, motivation serves to direct attention, focusing on particular issues, people, tasks, etc. Second, it serves to stimulate effort. Third, motivation influences persistence. Finally, motivation influences the choice and application of task-related strategies.

IO psychologists have since the 1960s been at the forefront of research and the practice of occupational stress and design of individual and organizational interventions to manage and reduce the stress levels and increase productivity, performance, health and wellbeing. Occupational stress can have implications for organizational performance because of the emotions job stress evokes. For example, a job stressor such as conflict with a supervisor can precipitate anger that in turn motivates counterproductive workplace behaviors. IO research has examined the association between work stressors and aggression, theft, substance abuse, and depressive symptoms. A number of models have been developed to explain the job stress process, including the person-environment (P-E) fit model and the demand-control model. 

IO research has also examined occupational stress in specific occupations, including police, general practitioners, and dentists. Another concern has been the relation of occupational stress to family life. Other IO researchers have examined gender differences in leadership style and job stress and strain in the context of male- and female-dominated industries, and unemployment-related distress. IO psychology is also concerned with the relation of occupational stress to career advancement.

Accidents and safety in the workplace are significant areas of interest to IO psychology and IO psychologists. Examples of psychosocial injury hazards of interest to IO psychology include fatigue, workplace violence, workplace bullying, and working night shifts. IO researchers conduct "stress audits" that can help organizations remain compliant with various occupational safety regulations. Psychosocial hazards can affect musculoskeletal disorders. A psychosocial factor related to accident risk is safety climate, which refers to employees' perceptions of the extent to which their work organization prioritizes safety. By contrast, psychosocial safety climate refers to management's "policies, practices, and procedures" aimed at protecting workers' psychological health. Research on safety leadership is also relevant to IO psychology. Research suggests that safety-oriented transformational leadership is associated with a positive safety climate and safe worker practices.

While there is no universal definition for organizational culture, a collective understanding shares the following assumptions:
Organizational culture has been shown to affect important organizational outcomes such as performance, attraction, recruitment, retention, employee satisfaction, and employee well-being. There are three levels of organizational culture: artifacts, shared values, and basic beliefs and assumptions. Artifacts comprise the physical components of the organization that relay cultural meaning. Shared values are individuals' preferences regarding certain aspects of the organization's culture (e.g., loyalty, customer service). Basic beliefs and assumptions include individuals' impressions about the trustworthiness and supportiveness of an organization, and are often deeply ingrained within the organization's culture.

In addition to an overall culture, organizations also have subcultures. Subcultures can be departmental (e.g. different work units) or defined by geographical distinction. While there is no single "type" of organizational culture, some researchers have developed models to describe different organizational cultures.

Group behavior involves the interactions among individuals in a collective. The individuals' opinions, attitudes, and adaptations affect group behavior and group behavior. In turn, affects those opinions, etc. The interactions are thought to fulfill some need satisfaction in an individual who is part of the collective. A specific area of IO research in group behavior is the team dynamics and team effectiveness.

Organizations often organize teams because teams can accomplish a much greater amount of work in a short period of time than an individual can accomplish. IO research has examined the harm workplace aggression does to team performance.

Team composition, or the configuration of team member knowledge, skills, abilities, and other characteristics, fundamentally influences teamwork. Team composition can be considered in the selection and management of teams to increase the likelihood of team success. To achieve high-quality results, teams built with members having higher skill levels are more likely to be effective than teams built around members having lesser skills; teams that include a members with a diversity of skills are also likely to show improved team performance. Team members should also be compatible in terms of personality traits, values, and work styles. There is substantial evidence that personality traits and values can shape the nature of teamwork, and influence team performance.

A fundamental question in team task design is whether or not a task is even appropriate for a team. Those tasks that require predominantly independent work are best left to individuals, and team tasks should include those tasks that consist primarily of interdependent work. When a given task is appropriate for a team, task design can play a key role in team effectiveness.

Job characteristic theory identifies core job dimensions that affect motivation, satisfaction, performance, etc. These dimensions include skill variety, task identity, task significance, autonomy and feedback. The dimensions map well to the team environment. Individual contributors who perform team tasks that are challenging, interesting, and engaging are more likely to be motivated to exert greater effort and perform better than team members who are working on tasks that lack those characteristics.

Organizational support systems affect the team effectiveness and provide resources for teams operating in the multi-team environment. During the chartering of new teams, organizational enabling resources are first identified. Examples of enabling resources include facilities, equipment, information, training, and leadership. Team-specific resources (e.g., budgetary resources, human resources) are typically made available. Team-specific human resources represent the individual contributors who are selected to be team members. Intra-team processes (e.g., task design, task assignment) involve these team-specific resources.

Teams also function in dynamic multi-team environments. Teams often must respond to shifting organizational contingencies. Contingencies affecting teams include constraints arising from conditions in which organizational resources are not exclusively earmarked for certain teams. When resources are scarce, they must be shared by multiple teams.

Organizational reward systems drive the strengthening and enhancing of individual team member efforts; such efforts contribute towards reaching team goals. In other words, rewards that are given to individual team members should be contingent upon the performance of the entire team.

Several design elements are needed to enable organizational reward systems to operate successfully. First, for a collective assessment to be appropriate for individual team members, the group's tasks must be highly interdependent. If this is not the case, individual assessment is more appropriate than team assessment. Second, individual-level reward systems and team-level reward systems must be compatible. For example, it would be unfair to reward the entire team for a job well done if only one team member did most of the work. That team member would most likely view teams and teamwork negatively, and would not want to work on a team in the future. Third, an organizational culture must be created such that it supports and rewards employees who believe in the value of teamwork and who maintain a positive attitude towards team-based rewards.

Goals potentially motivate team members when goals contain three elements: difficulty, acceptance, and specificity. Under difficult goal conditions, teams with more committed members tend to outperform teams with less committed members. When team members commit to team goals, team effectiveness is a function of how supportive members are with each other. The goals of individual team members and team goals interact. Team and individual goals must be coordinated. Individual goals must be consistent with team goals in order for a team to be effective.

Job satisfaction is often thought to reflect the extent to which a worker likes his or her job, or individual aspects or facets of jobs. It is one of the most heavily researched topics in IO psychology. Job satisfaction has theoretical and practical utility for the field. It has been linked to important job outcomes including attitudinal variables (e.g., job involvement, organizational commitment), absenteeism, turnover intentions, actual turnover, job performance, and tension. A meta-analyses found job satisfaction to be related to life satisfaction, happiness, positive affect, and the absence of negative affect.

Productive behavior is defined as employee behavior that contributes positively to the goals and objectives of an organization. When an employee begins a new job, there is a transition period during which he or she may not contribute significantly. To assist with this transition an employee typically requires job-related training. In financial terms, productive behavior represents the point at which an organization begins to achieve some return on the investment it has made in a new employee. IO psychologists are ordinarily more focused on productive behavior than job or task performance, including in-role "and" extra-role performance. In-role performance tells managers how well an employee performs the required aspects of the job; extra-role performance includes behaviors not necessarily required by job but nonetheless contribute to organizational effectiveness. By taking both in-role and extra-role performance into account, an IO psychologist is able to assess employees' effectiveness (how well they do what they were hired to do), efficiency (outputs to relative inputs), and productivity (how much they help the organization reach its goals). Three forms of productive behavior that IO psychologists often evaluate include job performance, organizational citizenship behavior (see below), and innovation.

Job performance represents behaviors employees engage in while at work which contribute to organizational goals. These behaviors are formally evaluated by an organization as part of an employee's responsibilities. In order to understand and ultimately predict job performance, it is important to be precise when defining the term. Job performance is about behaviors that are within the control of the employee and not about results (effectiveness), the costs involved in achieving results (productivity), the results that can be achieved in a period of time (efficiency), or the value an organization places on a given level of performance, effectiveness, productivity or efficiency (utility).

To model job performance, researchers have attempted to define a set of dimensions that are common to all jobs. Using a common set of dimensions provides a consistent basis for assessing performance and enables the comparison of performance across jobs. Performance is commonly broken into two major categories: in-role (technical aspects of a job) and extra-role (non-technical abilities such as communication skills and being a good team member). While this distinction in behavior has been challenged it is commonly made by both employees and management. A model of performance by Campbell breaks performance into in-role and extra-role categories. Campbell labeled job-specific task proficiency and non-job-specific task proficiency as in-role dimensions, while written and oral communication, demonstrating effort, maintaining personal discipline, facilitating peer and team performance, supervision and leadership and management and administration are labeled as extra-role dimensions. Murphy's model of job performance also broke job performance into in-role and extra-role categories. However, task-orientated behaviors composed the in-role category and the extra-role category included interpersonally-oriented behaviors, down-time behaviors and destructive and hazardous behaviors. However, it has been challenged as to whether the measurement of job performance is usually done through pencil/paper tests, job skills tests, on-site hands-on tests, off-site hands-on tests, high-fidelity simulations, symbolic simulations, task ratings and global ratings. These various tools are often used to evaluate performance on specific tasks and overall job performance. Van Dyne and LePine developed a measurement model in which overall job performance was evaluated using Campbell's in-role and extra-role categories. Here, in-role performance was reflected through how well "employees met their performance expectations and performed well at the tasks that made up the employees' job." Dimensions regarding how well the employee assists others with their work for the benefit of the group, if the employee voices new ideas for projects or changes to procedure and whether the employee attends functions that help the group composed the extra-role category.

To assess job performance, reliable and valid measures must be established. While there are many sources of error with performance ratings, error can be reduced through rater training and through the use of behaviorally-anchored rating scales. Such scales can be used to clearly define the behaviors that constitute poor, average, and superior performance. Additional factors that complicate the measurement of job performance include the instability of job performance over time due to forces such as changing performance criteria, the structure of the job itself and the restriction of variation in individual performance by organizational forces. These factors include errors in job measurement techniques, acceptance and the justification of poor performance and lack of importance of individual performance.

The determinants of job performance consist of factors having to do with the individual worker as well as environmental factors in the workplace. According to Campbell's Model of The Determinants of Job Performance, job performance is a result of the interaction between declarative knowledge (knowledge of facts or things), procedural knowledge (knowledge of what needs to be done and how to do it), and motivation (reflective of an employee's choices regarding whether to expend effort, the level of effort to expend, and whether to persist with the level of effort chosen). The interplay between these factors show that an employee may, for example, have a low level of declarative knowledge, but may still have a high level of performance if the employee has high levels of procedural knowledge and motivation.

Regardless of the job, three determinants stand out as predictors of performance: (1) general mental ability (especially for jobs higher in complexity); (2) job experience (although there is a law of diminishing returns); and (3) the personality trait of conscientiousness (people who are dependable and achievement-oriented, who plan well). These determinants appear to influence performance largely through the acquisition and usage of job knowledge and the motivation to do well. Further, an expanding area of research in job performance determinants includes emotional intelligence.

Organizational citizenship behaviors (OCBs) are another form of workplace behavior that IO psychologists are involved with. OCBs tend to be beneficial to both the organization and other workers. Dennis Organ (1988) defines OCBs as "individual behavior that is discretionary, not directly or explicitly recognized by the formal reward system, and that in the aggregate promotes the effective functioning of the organization." Behaviors that qualify as OCBs can fall into one of the following five categories: altruism, courtesy, sportsmanship, conscientiousness, and civic virtue. OCBs have also been categorized in other ways too, for example, by their intended targets (individuals, supervisors, and the organization as a whole. Other alternative ways of categorizing OCBs include "compulsory OCBs", which are engaged in owing to coercive persuasion or peer pressure rather than out of good will. The extent to which OCBs are voluntary has been the subject of some debate.

Other research suggests that some employees perform OCBs to influence how they are viewed within the organization. While these behaviors are not formally part of the job description, performing them can influence performance appraisals. Researchers have advanced the view that employees engage in OCBs as a form of "impression management," a term coined by Erving Goffman. Goffman defined impression management as "the way in which the individual ... presents himself and his activity to others, the ways in which he guides and controls the impression they form of him, and the kinds of things he may and may not do while sustaining his performance before them. Some researchers have hypothesized that OCBs are not performed out of good will, positive affect, etc., but instead as a way of being noticed by others, including supervisors.

Four qualities are generally linked to creative and innovative behaviour by individuals: 

At the organizational level, a study by Damanpour identified four specific characteristics that may predict innovation:


Counterproductive work behavior (CWB) can be defined as employee behavior that goes against the goals of an organization. These behaviors can be intentional or unintentional and result from a wide range of underlying causes and motivations. Some CWBs have instrumental motivations (e.g., theft). It has been proposed that a person-by-environment interaction can be utilized to explain a variety of counterproductive behaviors. For instance, an employee who sabotages another employee's work may do so because of lax supervision (environment) and underlying psychopathology (person) that work in concert to result in the counterproductive behavior. There is evidence that an emotional response (e.g., anger) to job stress (e.g., unfair treatment) can motivate CWBs.

The forms of counterproductive behavior with the most empirical examination are ineffective job performance, absenteeism, job turnover, and accidents. Less common but potentially more detrimental forms of counterproductive behavior have also been investigated including violence and sexual harassment.

In IO psychology, leadership can be defined as a process of influencing others to agree on a shared purpose, and to work towards shared objectives. A distinction should be made between leadership and management. Managers process administrative tasks and organize work environments. Although leaders may be required to undertake managerial duties as well, leaders typically focus on inspiring followers and creating a shared organizational culture and values. Managers deal with complexity, while leaders deal with initiating and adapting to change. Managers undertake the tasks of planning, budgeting, organizing, staffing, controlling and problem solving. In contrast, leaders undertake the tasks of setting a direction or vision, aligning people to shared goals, communicating, and motivating.

Approaches to studying leadership in IO psychology can be broadly classified into three categories: Leader-focused approaches, contingency-focused approaches, and follower-focused approaches.

Leader-focused approaches look to organizational leaders to determine the characteristics of effective leadership. According to the trait approach, more effective leaders possess certain traits that less effective leaders lack. More recently, this approach is being used to predict leader emergence. The following traits have been identified as those that predict leader emergence when there is no formal leader: high intelligence, high needs for dominance, high self-motivation, and socially perceptive. Another leader-focused approached is the "behavioral approach", which focuses on the behaviors that distinguish effective from ineffective leaders. There are two categories of leadership behaviors: "consideration and initiating structure". Behaviors associated with the category of consideration include showing subordinates they are valued and that the leader cares about them. An example of a consideration behavior is showing compassion when problems arise in or out of the office. Behaviors associated with the category of initiating structure include facilitating the task performance of groups. One example of an initiating structure behavior is meeting one-on-one with subordinates to explain expectations and goals. The final leader-focused approach is "power and influence". To be most effective, a leader should be able to influence others to behave in ways that are in line with the organization's mission and goals. How influential a leader can be depends on their social power – their potential to influence their subordinates. There are six bases of power: French and Raven's classic five bases of coercive, reward, legitimate, expert, and referent power, plus informational power. A leader can use several different tactics to influence others within an organization. These include: rational persuasion, inspirational appeal, consultation, ingratiation, exchange, personal appeal, coalition, legitimating, and pressure.

Of the 3 approaches to leadership, contingency-focused approaches have been the most prevalent over the past 30 years. Contingency-focused theories base a leader's effectiveness on their ability to assess a situation and adapt their behavior accordingly. These theories assume that an effective leader can accurately "read" a situation and skillfully employ a leadership style that meets the needs of the individuals involved and the task at hand. A brief introduction to the most prominent contingency-focused theories will follow.

The Fiedler contingency model holds that a leader's effectiveness depends on the interaction between their characteristics and the characteristics of the situation. Path–goal theory asserts that the role of the leader is to help his or her subordinates achieve their goals. To effectively do this, leaders must skillfully select from four different leadership styles to meet the situational factors. The situational factors are a product of the characteristics of subordinates and the characteristics of the environment. The leader–member exchange theory (LMX) focuses on how leader–subordinate relationships develop. Generally speaking, when a subordinate performs well or when there are positive exchanges between a leader and a subordinate, their relationship is strengthened, performance and job satisfaction are enhanced, and the subordinate will feel more commitment to the leader and the organization as a whole. Vroom-Yetton-Jago model focuses on decision-making with respect to a "feasibility set" which is composed of the situational attributes.

In addition to the contingency-focused approaches mentioned, there has been a high degree of interest paid to three novel approaches that have recently emerged. The first is transformational leadership, which posits that there are certain leadership traits that inspire subordinates to perform beyond their capabilities. The second is transactional leadership, which is most concerned with keeping subordinates in-line with deadlines and organizational policy. This type of leader fills more of a managerial role and lacks qualities necessary to inspire subordinates and induce meaningful change. And the third is authentic leadership which is centered around empathy and a leader's values or character. If the leader understands their followers, they can inspire subordinates by cultivating a personal connection and leading them to share in the vision and goals of the team. Although there has been a limited amount of research conducted on these theories, they are sure to receive continued attention as the field of IO psychology matures.

Follower-focused approaches look at the processes by which leaders motivate followers, and lead teams to achieve shared goals. Understandably, the area of leadership motivation draws heavily from the abundant research literature in the domain of motivation in IO psychology. Because leaders are held responsible for their followers' ability to achieve the organization's goals, their ability to motivate their followers is a critical factor of leadership effectiveness. Similarly, the area of team leadership draws heavily from the research in teams and team effectiveness in IO psychology. Because organizational employees are frequently structured in the form of teams, leaders need to be aware of the potential benefits and pitfalls of working in teams, how teams develop, how to satisfy team members' needs, and ultimately how to bring about team effectiveness and performance.

An emerging area of IO research in the area of team leadership is in leading virtual teams, where people in the team are geographically-distributed across various distances and sometimes even countries. While technological advances have enabled the leadership process to take place in such virtual contexts, they present new challenges for leaders as well, such as the need to use technology to build relationships with followers, and influencing followers when faced with limited (or no) face-to-face interaction.

IO psychologists are also concerned with organizational change. This effort, called organizational development (OD). Tools used to advance organization development include the survey feedback technique. The technique involves the periodic assessment (with surveys) of employee attitudes and feelings. The results are conveyed to organizational stakeholders, who may want to take the organization in a particular direction. Another tool is the team building technique. Because many if not most tasks within the organization are completed by small groups and/or teams, team building is important to organizational success. In order to enhance a team's morale and problem-solving skills, IO psychologists help the groups to build their self-confidence, group cohesiveness, and working effectiveness.

The IO psychology and organizational behavior have manifested some overlap. The overlap has led to some confusion regarding how the two disciplines differ. There is also much confusion about the differences between IO psychology and human resources, or human resource management

The minimum requirement for working as an IO psychologist is a master's degree. Normally, this degree requires about two to three years of postgraduate work to complete. Of all the degrees granted in IO psychology each year, approximately two thirds are at the master's level.
A comprehensive list of US and Canadian master's and doctoral programs can be found at the web site of the Society for Industrial and Organizational Psychology (SIOP). Admission into IO psychology PhD programs is highly competitive; many programs accept only a small number of applicants each year.
There are graduate degree programs in IO psychology outside of the US and Canada. The SIOP web site also provides a comprehensive list of IO programs in many other countries.

In Australia, organizational psychologists must be accredited by the Australian Psychological Society (APS). To become an organizational psychologist, one must meet the criteria for a general psychologist's licence: three years studying bachelor's degree in psychology, 4th year honours degree or postgraduate diploma in psychology, and two-year full-time supervised practice plus 80 hours of professional development. There are other avenues available, such as a two-year supervised training program after honours (i.e. 4+2 pathway), or one year of postgraduate coursework and practical placements followed by a one-year supervised training program (i.e. 5+1 pathway). After this, psychologists can elect to specialize as Organizational Psychologists.

There are many different sets of competencies for different specializations within IO psychology and IO psychologists are versatile behavioral scientists. For example, an IO psychologist specializing in selection and recruiting should have expertise in finding the best talent for the organization and getting everyone on board while he or she might not need to know much about executive coaching. Some IO psychologists specialize in specific areas of consulting whereas others tend to generalize their areas of expertise. There are basic skills and knowledge an individual needs in order to be an effective IO psychologist, which include being an independent learner, interpersonal skills (e.g., listening skills), and general consultation skills (e.g., skills and knowledge in the problem area).

According to the United States Department of Labor's Bureau of Labor Statistics, IO psychology is the fastest growing occupation in the United States, based on projections between 2012 and 2022. 
In a 2006 salary survey, the median salary for a PhD in IO psychology was $98,000; for a master's level IO psychologist was $72,000. The highest paid PhD IO psychologists in private industry worked in pharmaceuticals and averaged approximately $151,000 per year; the earnings median of self-employed consultants was $150,000; those employed in retail, energy, and manufacturing followed closely behind, averaging approximately $133,000. The lowest earners were found in state and local government positions, averaging approximately $77,000. In 2005, IO psychologists whose primary responsibility is teaching at private and public colleges and universities often earn additional income from consulting with government and industry.

An IO psychologist, whether an academic, a consultant, or an employee, is expected to maintain high ethical standards. The APA's ethical principles apply to IO psychologists. For example, ethically, the IO psychologist should only accept projects for which he or she is qualified. With more organizations becoming global, it is important that when an IO psychologist works outside her or his home country, the psychologist is aware of rules, regulations, and cultures of the organizations and countries in which the psychologist works, while also adhering to the ethical standards set at home.



</doc>
<doc id="15451" url="https://en.wikipedia.org/wiki?curid=15451" title="International Council of Unitarians and Universalists">
International Council of Unitarians and Universalists

The International Council of Unitarians and Universalists (ICUU) is an umbrella organization founded in 1995 comprising many Unitarian, Universalist, and Unitarian Universalist organizations. Some groups represent only a few hundred people; while the largest, the Unitarian Universalist Association, had more than 160,000 members —including over 150,000 in the United States.

The original initiative for its establishment was contained in a resolution of the General Assembly of Unitarian and Free Christian Churches (British Unitarians) in 1987. This led to the establishment of the Advocates for the Establishment of an International Organization of Unitarians (AEIOU), which worked towards creating the council. However, the General Assembly resolution provided no funding.

The Unitarian Universalist Association (UUA) became particularly interested in the establishment of a council when it had to deal with an increasing number of applications for membership from congregations outside North America. It had already granted membership to congregations in Adelaide, Auckland, the Philippines and Pakistan, and congregations in Sydney, Russia and Spain had applied for membership. Rather than admit congregations from all over the world, the UUA hoped that they would join a world council instead. The UUA thus became willing to provide funding for the council's establishment.

As a result, the council was finally established at a meeting in Essex, Massachusetts, United States on 23–26 March 1995.

The Preamble to the Constitution of the International Council of Unitarians and Universalists reads:

We, the member groups of the International Council of Unitarians and Universalists, affirming our belief in religious community based on:


declare our purposes to be:



Polish Unitarians have reported a need for a period of reorganization, and that at this time they are unable to maintain the level of activity needed to be full Council members, be it moved that membership of these groups be suspended. This action is taken with regret and the ICUU looks forward to welcoming Poland back into membership at the earliest possible date.

Churches and religious associations which have expressed their will to become members of the Council may be admitted as "Provisional Members" for a period of time (generally two or four years), until the Council decides that they have shown their organizational stability, affinity with the ICUU principles and commitment to deserve becoming Full Members of the Council. Provisional Members are invited to Council meetings through a delegate but cannot vote.


According to the Bylaws of the ICUU, Emerging Groups are ""applicants that are deemed to be reasonable prospects for membership, but do not fulfil the conditions of either Provisional membership or Full Membership"". These groups may be designated as Emerging Groups by the Executive Committee upon its sole discretion. Emerging Groups may be invited as observers to General Meetings.

The current list of Emerging Groups after the last meeting of the Executive Committee (London, 22–25 November 2008) is as follows:

Organizations with beliefs and purposes closely akin to those of ICUU but which by nature of their constitution are not eligible for full membership or which do not wish to become full members now or in the foreseeable future, may become Associates of the ICUU. The application must be approved by the ICUU Council Meeting.





</doc>
<doc id="15454" url="https://en.wikipedia.org/wiki?curid=15454" title="Itanium">
Itanium

Itanium ( ) is a family of 64-bit Intel microprocessors that implement the Intel Itanium architecture (formerly called IA-64). Intel marketed the processors for enterprise servers and high-performance computing systems. The Itanium architecture originated at Hewlett-Packard (HP), and was later jointly developed by HP and Intel.

Itanium-based systems have been produced by HP (the HP Integrity Servers line) and several other manufacturers. In 2008, Itanium was the fourth-most deployed microprocessor architecture for enterprise-class systems, behind x86-64, Power ISA, and SPARC.

In February 2017, Intel released the final generation, , to test customers, and in May began shipping in volume. It is used exclusively in mission-critical servers from Hewlett Packard Enterprise.

Intel officially announced the discontinuation of the Itanium CPU family on January 30, 2019.

In 1989, HP determined that the Reduced Instruction Set Computing (RISC) architectures were approaching the processing limit at one instruction per cycle. HP researchers investigated a new architecture, later named Explicitly Parallel Instruction Computing (EPIC), that allows the processor to execute multiple instructions in each clock cycle. EPIC implements a form of very long instruction word (VLIW) architecture, in which a single instruction word contains multiple instructions. With EPIC, the compiler determines in advance which instructions can be executed at the same time, so the microprocessor simply executes the instructions and does not need elaborate mechanisms to determine which instructions to execute in parallel.
The goal of this approach is twofold: to enable deeper inspection of the code at compile time to identify additional opportunities for parallel execution, and to simplify the processor design and reduce energy consumption by eliminating the need for runtime scheduling circuitry.

HP believed that it was no longer cost-effective for individual enterprise systems companies such as itself to develop proprietary microprocessors, so it partnered with Intel in 1994 to develop the IA-64 architecture, derived from EPIC. Intel was willing to undertake the very large development effort on IA-64 in the expectation that the resulting microprocessor would be used by the majority of enterprise systems manufacturers. HP and Intel initiated a large joint development effort with a goal of delivering the first product, Merced, in 1998.

During development, Intel, HP, and industry analysts predicted that IA-64 would dominate in servers, workstations, and high-end desktops, and eventually supplant RISC and complex instruction set computing (CISC) architectures for all general-purpose applications.
Compaq and Silicon Graphics decided to abandon further development of the Alpha and MIPS architectures respectively in favor of migrating to IA-64.

Several groups ported operating systems for the architecture, including Microsoft Windows, OpenVMS, Linux, HP-UX, Solaris,
Tru64 UNIX, and Monterey/64.
The latter three were canceled before reaching the market. By 1997, it was apparent that the IA-64 architecture and the compiler were much more difficult to implement than originally thought, and the delivery timeframe of Merced began slipping.

Intel announced the official name of the processor, "Itanium", on October 4, 1999.
Within hours, the name Itanic had been coined on a Usenet newsgroup, a reference to the RMS "Titanic", the "unsinkable" ocean liner that sank on her maiden voyage in 1912. "Itanic" has since often been used by "The Register", and others, to imply that the multibillion-dollar investment in Itanium—and the early hype associated with it—would be followed by its relatively quick demise.

By the time Itanium was released in June 2001, its performance was not superior to competing RISC and CISC processors.
Itanium competed at the low-end (primarily four-CPU and smaller systems) with servers based on x86 processors, and at the high-end with IBM POWER and Sun Microsystems SPARC processors. Intel repositioned Itanium to focus on the high-end business and HPC computing markets, attempting to duplicate the x86's successful "horizontal" market (i.e., single architecture, multiple systems vendors). The success of this initial processor version was limited to replacing the PA-RISC in HP systems, Alpha in Compaq systems and MIPS in SGI systems, though IBM also delivered a supercomputer based on this processor.
POWER and SPARC remained strong, while the 32-bit x86 architecture continued to grow into the enterprise space, building on the economies of scale fueled by its enormous installed base.

Only a few thousand systems using the original "Merced" Itanium processor were sold, due to relatively poor performance, high cost and limited software availability. Recognizing that the lack of software could be a serious problem for the future, Intel made thousands of these early systems available to independent software vendors (ISVs) to stimulate development. HP and Intel brought the next-generation Itanium 2 processor to the market a year later.

The Itanium 2 processor was released in 2002, and was marketed for enterprise servers rather than for the whole gamut of high-end computing. The first Itanium 2, code-named "McKinley", was jointly developed by HP and Intel. It relieved many of the performance problems of the original Itanium processor, which were mostly caused by an inefficient memory subsystem. "McKinley" contains 221 million transistors (of which 25 million are for logic), measured 19.5 mm by 21.6 mm (421 mm) and was fabricated in a 180 nm, bulk CMOS process with six layers of aluminium metallization.

In 2003, AMD released the Opteron CPU, which implements its own 64-bit architecture called AMD64. The Opteron gained rapid acceptance in the enterprise server space because it provided an easy upgrade from x86. Under the influence of Microsoft, Intel responded by implementing AMD's x86-64 instruction set architecture instead of IA-64 in its Xeon microprocessors in 2004, resulting in a new industry-wide "de facto" standard.

Intel released a new Itanium 2 family member, codenamed "Madison", in 2003. Madison used a 130 nm process and was the basis of all new Itanium processors until Montecito was released in June 2006.

In March 2005, Intel announced that it was working on a new Itanium processor, codenamed "Tukwila", to be released in 2007. The Tukwila would have four processor cores and would replace the Itanium bus with a new Common System Interface, which would also be used by a new Xeon processor.
Later that year, Intel revised Tukwila's delivery date to late 2008.

In November 2005, the major Itanium server manufacturers joined with Intel and a number of software vendors to form the Itanium Solutions Alliance to promote the architecture and accelerate the software porting effort.
The Alliance announced that its members would invest $10 billion in the Itanium Solutions Alliance by the end of the decade.

In 2006, Intel delivered "Montecito" (marketed as the Itanium 2 9000 series), a dual-core processor that roughly doubled the performance and decreased the energy consumption by about 20 percent.

Intel released the Itanium 2 9100 series, codenamed "Montvale", in November 2007.
In May 2009, the schedule for Tukwila, its follow-on, was revised again, with the release to OEMs planned for the first quarter of 2010.

The Itanium 9300 series processor, codenamed "Tukwila", was released on February 8, 2010, with greater performance and memory capacity.

The device uses a 65 nm process, includes two to four cores, up to 24 MB on-die caches, Hyper-Threading technology and integrated memory controllers. It implements double-device data correction, which helps to fix memory errors. Tukwila also implements Intel QuickPath Interconnect (QPI) to replace the Itanium bus-based architecture. It has a peak interprocessor bandwidth of 96 GB/s and a peak memory bandwidth of 34 GB/s. With QuickPath, the processor has integrated memory controllers and interfaces the memory directly, using QPI interfaces to directly connect to other processors and I/O hubs. QuickPath is also used on Intel processors using the "Nehalem" microarchitecture, which possibly enabled Tukwila and Nehalem to use the same chipsets.
Tukwila incorporates four memory controllers, each of which supports multiple DDR3 DIMMs via a separate memory controller,
much like the Nehalem-based Xeon processor code-named "Beckton".

The Itanium 9500 series processor, codenamed "Poulson", is the follow-on processor to Tukwila and was released on November 8, 2012.
According to Intel, it skips the 45 nm process technology and uses a 32 nm process technology. It features eight cores and has a 12-wide issue architecture, multithreading enhancements, and new instructions to take advantage of parallelism, especially in virtualization.
The Poulson L3 cache size is 32 MB. L2 cache size is 6 MB, 512 I KB, 256 D KB per core. Die size is 544 mm², less than its predecessor Tukwila (698.75 mm²).

At ISSCC 2011, Intel presented a paper called "A 32nm 3.1 Billion Transistor 12-Wide-Issue Itanium Processor for Mission Critical Servers."
Given Intel's history of disclosing details about Itanium microprocessors at ISSCC, this paper most likely referred to Poulson. Analyst David Kanter speculated that Poulson would use a new microarchitecture, with a more advanced form of multithreading that uses up to two threads, to improve performance for single threaded and multithreaded workloads.
Some information was also released at the Hot Chips conference.

Information presented improvements in multithreading, resiliency improvements (Intel Instruction Replay RAS) and few new instructions (thread priority, integer instruction, cache prefetching, and data access hints).

Intel's Product Change Notification (PCN) 111456-01 lists four models of Itanium 9500 series CPU, which was later removed in a revised document. The parts were later listed in Intel's Material Declaration Data Sheets (MDDS) database. Intel later posted Itanium 9500 reference manual.

The models are the following:

During the 2012 "Hewlett-Packard Co. v. Oracle Corp." support lawsuit, court documents unsealed by a Santa Clara County Court judge revealed that in 2008, Hewlett-Packard had paid Intel around $440 million to keep producing and updating Itanium microprocessors from 2009 to 2014. In 2010, the two companies signed another $250 million deal, which obliged Intel to continue making Itanium CPUs for HP's machines until 2017. Under the terms of the agreements, HP has to pay for chips it gets from Intel, while Intel launches Tukwila, Poulson, Kittson, and Kittson+ chips in a bid to gradually boost performance of the platform.

Rumors of a successor to Poulson (code named "Kittson") began to circulate in 2012–2013. This was at first associated with a forthcoming 22 nm process shrink, and later revised in the face of declining Itanium sales to a less-ambitious 32 nm node. In April 2015, Intel, although it had not yet confirmed formal specifications, did confirm that it continued to work on the project. Meanwhile, the aggressively multicore Xeon E7 platform displaced Itanium-based solutions in the Intel roadmap.

In July 2016, the HP spin-off Hewlett Packard Enterprise (HPE) announced in "Computer World" that Kittson would be released mid-2017. In February 2017, Intel reported that it was shipping Kittson to test customers, with plans to ship in volume later that year.

Intel officially launched the Itanium 9700 series processor family on May 11, 2017. Notably, Kittson has no microarchitecture improvements over Poulson, only higher clock speeds.

Intel has announced that the 9700 series will be the last Itanium chips produced.

The models are:

In comparison with its Xeon family of server processors, Itanium has never been a high-volume product for Intel. Intel does not release production numbers. One industry analyst estimated that the production rate was 200,000 processors per year in 2007.

According to Gartner Inc., the total number of Itanium servers (not processors) sold by all vendors in 2007, was about 55,000. (It is unclear whether clustered servers counted as a single server or not.) This compares with 417,000 RISC servers (spread across all RISC vendors) and 8.4 million x86 servers. IDC reports that a total of 184,000 Itanium-based systems were sold from 2001 through 2007. For the combined POWER/SPARC/Itanium systems market, IDC reports that POWER captured 42% of revenue and SPARC captured 32%, while Itanium-based system revenue reached 26% in the second quarter of 2008.
According to an IDC analyst, in 2007, HP accounted for perhaps 80% of Itanium systems revenue.
According to Gartner, in 2008, HP accounted for 95% of Itanium sales. HP's Itanium system sales were at an annual rate of $4.4Bn at the end of 2008, and declined to $3.5Bn by the end of 2009,
compared to a 35% decline in UNIX system revenue for Sun and an 11% drop for IBM, with an x86-64 server revenue increase of 14% during this period.

In December 2012, IDC released a research report stating that Itanium server shipments would remain flat through 2016, with annual shipment of 26,000 systems (a decline of over 50% compared to shipments in 2008).

By 2006, HP manufactured at least 80% of all Itanium systems, and sold 7,200 in the first quarter of 2006.
The bulk of systems sold were enterprise servers and machines for large-scale technical computing, with an average selling price per system in excess of US$200,000. A typical system uses eight or more Itanium processors.

By 2012, only a few manufacturers offered Itanium systems, including HP, Bull, NEC, Inspur and Huawei. In addition, Intel offered a chassis that could be used by system integrators to build Itanium systems.

By 2015, only HP supplied Itanium-based systems. With HP split in late 2015, Itanium systems (branded as Integrity) are handled by Hewlett-Packard Enterprise (HPE), with recent major update in 2017 (Integrity i6, and HP-UX 11i v3 Update 16). HPE also supports a few other operating systems, including Windows up to Server 2008 R2, Linux, OpenVMS and NonStop. Itanium is not affected by Spectre and Meltdown.

The Itanium bus interfaces to the rest of the system via a chipset. Enterprise server manufacturers differentiate their systems by designing and developing chipsets that interface the processor to memory, interconnections, and peripheral controllers. The chipset is the heart of the system-level architecture for each system design. Development of a chipset costs tens of millions of dollars and represents a major commitment to the use of the Itanium. IBM created a chipset in 2003, and Intel in 2002, but neither of them developed chipsets to support newer technologies such as DDR2 or PCI Express.
Before "Tukwila" moved away from the FSB, chipsets supporting such technologies were manufactured by all Itanium server vendors, such as HP, Fujitsu, SGI, NEC, and Hitachi.

The "Tukwila" Itanium processor model had been designed to share a common chipset with the Intel Xeon processor EX (Intel's Xeon processor designed for four processor and larger servers). The goal was to streamline system development and reduce costs for server OEMs, many of which develop both Itanium- and Xeon-based servers. However, in 2013, this goal was pushed back to be "evaluated for future implementation opportunities".

Itanium is currently supported by the following operating systems:


Previously supported: 


GNU Compiler Collection deprecated support for IA-64 in GCC 10, after Intel announced the planned phase-out of this ISA. LLVM (Clang) supports Itanium.

HP sells a virtualization technology for Itanium called Integrity Virtual Machines.

Emulation is a technique that allows a computer to execute binary code that was compiled for a different type of computer. Before IBM's acquisition of QuickTransit in 2009, application binary software for IRIX/MIPS and Solaris/SPARC could run via type of emulation called "dynamic binary translation" on Linux/Itanium. Similarly, HP implemented a method to execute PA-RISC/HP-UX on the Itanium/HP-UX via emulation, to simplify migration of its PA-RISC customers to the radically different Itanium instruction set. Itanium processors can also run the mainframe environment GCOS from Groupe Bull and several x86 operating systems via instruction set simulators.

Itanium is aimed at the enterprise server and high-performance computing (HPC) markets. Other enterprise- and HPC-focused processor lines include Oracle's and Fujitsu's SPARC processors and IBM's POWER microprocessors. Measured by quantity sold, Itanium's most serious competition comes from x86-64 processors including Intel's own Xeon line and AMD's Opteron line. Since 2009, most servers were being shipped with x86-64 processors.

In 2005, Itanium systems accounted for about 14% of HPC systems revenue, but the percentage has declined as the industry shifted to x86-64 clusters for this application.

An October 2008 Gartner report on the Tukwila processor, stated that "...the future roadmap for Itanium looks as strong as that of any RISC peer like Power or SPARC."

An Itanium-based computer first appeared on the list of the TOP500 supercomputers in November 2001. The best position ever achieved by an "Itanium 2" based system in the list was #2, achieved in June 2004, when Thunder (Lawrence Livermore National Laboratory) entered the list with an Rmax of 19.94 Teraflops. In November 2004, Columbia entered the list at #2 with 51.8 Teraflops, and there was at least one Itanium-based computer in the top 10 from then until June 2007. The peak number of Itanium-based machines on the list occurred in the November 2004 list, at 84 systems (16.8%); by June 2012, this had dropped to one system (0.2%), and no Itanium system remained on the list in November 2012.

The Itanium processors show a progression in capability. Merced was a proof of concept. McKinley dramatically improved the memory hierarchy and allowed Itanium to become reasonably competitive. Madison, with the shift to a 130 nm process, allowed for enough cache space to overcome the major performance bottlenecks. Montecito, with a 90 nm process, allowed for a dual-core implementation and a major improvement in performance per watt. Montvale added three new features: core-level lockstep, demand-based switching and front-side bus frequency of up to 667 MHz.

When first released in 2001, Itanium's performance was disappointing compared to better-established RISC and CISC processors. Emulation to run existing x86 applications and operating systems was particularly poor, with one benchmark in 2001 reporting that it was equivalent at best to a 100 MHz Pentium in this mode (1.1 GHz Pentiums were on the market at that time).
Itanium failed to make significant inroads against IA-32 or RISC, and suffered further following the arrival of x86-64 systems which offered greater compatibility with older x86 applications.

In a 2009 article on the history of the processor — "How the Itanium Killed the Computer Industry" — journalist John C. Dvorak reported "This continues to be one of the great fiascos of the last 50 years". Tech columnist Ashlee Vance commented that the delays and underperformance "turned the product into a joke in the chip industry". In an interview, Donald Knuth said "The Itanium approach...was supposed to be so terrific—until it turned out that the wished-for compilers were basically impossible to write."

Both Red Hat and Microsoft announced plans to drop Itanium support in their operating systems due to lack of market interest; however, other Linux distributions such as Gentoo and Debian remain available for Itanium. On March 22, 2011, Oracle Corporation announced that it would no longer develop new products for HP-UX on Itanium, although it would continue to provide support for existing products. Following this announcement, HP sued Oracle for breach of contract, arguing that Oracle had violated conditions imposed during settlement over Oracle's hiring of former HP CEO Mark Hurd as its co-CEO, requiring the vendor to support Itanium on its software "until such time as HP discontinues the sales of its Itanium-based servers", and that the breach had harmed its business. In 2012, a court ruled in favor of HP, and ordered Oracle to resume its support for Itanium. In June 2016, Hewlett-Packard Enterprise (the corporate successor to HP's server business) was awarded $3 billion in damages from the lawsuit.

A former Intel official reported that the Itanium business had become profitable for Intel in late 2009. By 2009, the chip was almost entirely deployed on servers made by HP, which had over 95% of the Itanium server market share, making the main operating system for Itanium HP-UX. On March 22, 2011, Intel reaffirmed its commitment to Itanium with multiple generations of chips in development and on schedule.

Although Itanium did attain limited success in the niche market of high-end computing, Intel had originally hoped it would find broader acceptance as a replacement for the original x86 architecture.

AMD chose a different direction, designing the less radical x86-64, a 64-bit extension to the existing x86 architecture, which Microsoft then supported, forcing Intel to introduce the same extensions in its own x86-based processors. These designs can run existing 32-bit applications at native hardware speed, while offering support for 64-bit memory addressing and other enhancements to new applications. This architecture has now become the predominant 64-bit architecture in the desktop and portable market. Although some Itanium-based workstations were initially introduced by companies such as SGI, they are no longer available.

1989
1994
1995
1996
1997
1998
1999
2000
2001
2002
2003
2004
2005
2006
2007
2009
2010
2011
2012
2013
2014
2017
2019
2020




</doc>
<doc id="15459" url="https://en.wikipedia.org/wiki?curid=15459" title="International Classification of Diseases">
International Classification of Diseases

The International Classification of Diseases (ICD) is a globally used diagnostic tool for epidemiology, health management and clinical purposes. The ICD is maintained by the World Health Organization (WHO), which is the directing and coordinating authority for health within the United Nations System. The ICD is originally designed as a health care classification system, providing a system of diagnostic codes for classifying diseases, including nuanced classifications of a wide variety of signs, symptoms, abnormal findings, complaints, social circumstances, and external causes of injury or disease. This system is designed to map health conditions to corresponding generic categories together with specific variations, assigning for these a designated code, up to six characters long. Thus, major categories are designed to include a set of similar diseases.

The ICD is published by the WHO and used worldwide for morbidity and mortality statistics, reimbursement systems, and automated decision support in health care. This system is designed to promote international comparability in the collection, processing, classification, and presentation of these statistics. Like the analogous "Diagnostic and Statistical Manual of Mental Disorders" (which is limited to psychiatric disorders and almost exclusive to the United States), the ICD is a major project to statistically classify all health disorders, and provide diagnostic assistance. The ICD is a core statistically based classificatory diagnostic system for health care related issues of the WHO Family of International Classifications (WHO-FIC).

The ICD is revised periodically and is currently in its 10th revision. The ICD-10, as it is therefore known, was first released in 1992, and the WHO publishes annual minor updates and triennial major updates. The eleventh revision of the ICD, the ICD-11, was accepted by WHO's World Health Assembly (WHA) on 25 May 2019 and will officially come into effect on 1 January 2022. The version for preparation of approval at the WHA was released on 18 June 2018.

The ICD is part of a "family" of international classifications (WHOFIC) that complement each other, also including the International Classification of Functioning, Disability and Health (ICF) which focuses on the domains of functioning (disability) associated with health conditions, from both medical and social perspectives, and the International Classification of Health Interventions (ICHI) that classifies the whole range of medical, nursing, functioning and public health interventions.

In 1860, during the international statistical congress held in London, Florence Nightingale made a proposal that was to result in the development of the first model of systematic collection of hospital data. In 1893, a French physician, Jacques Bertillon, introduced the "Bertillon Classification of Causes of Death" at a congress of the International Statistical Institute in Chicago.

A number of countries adopted Bertillon's system, which was based on the principle of distinguishing between general diseases and those localized to a particular organ or anatomical site, as used by the City of Paris for classifying deaths. Subsequent revisions represented a synthesis of English, German, and Swiss classifications, expanding from the original 44 titles to 161 titles. In 1898, the American Public Health Association (APHA) recommended that the registrars of Canada, Mexico, and the United States also adopt it. The APHA also recommended revising the system every 10 years to ensure the system remained current with medical practice advances. As a result, the first international conference to revise the International Classification of Causes of Death took place in 1900, with revisions occurring every ten years thereafter. At that time, the classification system was contained in one book, which included an Alphabetic Index as well as a Tabular List. The book was small compared with current coding texts.

The revisions that followed contained minor changes, until the sixth revision of the classification system. With the sixth revision, the classification system expanded to two volumes. The sixth revision included morbidity and mortality conditions, and its title was modified to reflect the changes: International Statistical Classification of Diseases, Injuries and Causes of Death (ICD). Prior to the sixth revision, responsibility for ICD revisions fell to the Mixed Commission, a group composed of representatives from the International Statistical Institute and the Health Organization of the League of Nations. In 1948, the WHO assumed responsibility for preparing and publishing the revisions to the ICD every ten years. WHO sponsored the seventh and eighth revisions in 1957 and 1968, respectively. It later became clear that the established ten year interval between revisions was too short.

The ICD is currently the most widely used statistical classification system for diseases in the world. In addition, some countries—including Australia, Canada, and the United States—have developed their own adaptations of ICD, with more procedure codes for classification of operative or diagnostic procedures.

The ICD-6, published in 1949, was the first to be shaped to become suitable for morbidity reporting. Accordingly, the name changed from International List of Causes of Death to International Statistical Classification of Diseases. The combined code section for injuries and their associated accidents was split into two, a chapter for injuries, and a chapter for their external causes. With use for morbidity there was a need for coding mental conditions, and for the first time a section on mental disorders was added .

The international Conference for the Seventh Revision of the International Classification of Diseases was held in Paris under the auspices of WHO in February 1955. In accordance with a recommendation of the WHO Expert Committee on Health Statistics, this revision was limited to essential changes and amendments of errors and inconsistencies.

The 8th Revision Conference convened by WHO met in Geneva, from 6 to 12 July 1965. This revision was more radical than the Seventh but left unchanged the basic structure of the Classification and the general philosophy of classifying diseases, whenever possible, according to their etiology rather than a particular manifestation.
During the years that the Seventh and Eighth Revisions of the ICD were in force, the use of the ICD for indexing hospital medical records increased rapidly and some countries prepared national adaptations which provided the additional detail needed for this application of the ICD. 
In the US, a group of consultants was asked to study the 8th revision of ICD (ICD-8a) for its applicability to various users in the United States. This group recommended that further detail be provided for coding hospital and morbidity data. The American Hospital Association's "Advisory Committee to the Central Office on ICDA" developed the needed adaptation proposals, resulting in the publication of the International Classification of Diseases, Adapted (ICDA). In 1968, the United States Public Health Service published the International Classification of Diseases, Adapted, 8th Revision for use in the United States (ICDA-8a). Beginning in 1968, ICDA-8a served as the basis for coding diagnostic data for both official morbidity [and mortality] statistics in the United States.

The International Conference for the Ninth Revision of the International Statistical Classification of Diseases, Injuries, and Causes of Death, convened by WHO, met in Geneva from 30 September to 6 October 1975. In the discussions leading up to the conference, it had originally been intended that there should be little change other than updating of the classification. This was mainly because of the expense of adapting data processing systems each time the classification was revised.

There had been an enormous growth of interest in the ICD and ways had to be found of responding to this, partly by modifying the classification itself and partly by introducing special coding provisions. A number of representations were made by specialist bodies which had become interested in using the ICD for their own statistics. Some subject areas in the classification were regarded as inappropriately arranged and there was considerable pressure for more detail and for adaptation of the classification to make it more relevant for the evaluation of medical care, by classifying conditions to the chapters concerned with the part of the body affected rather than to those dealing with the underlying generalized disease.

At the other end of the scale, there were representations from countries and areas where a detailed and sophisticated classification was irrelevant, but which nevertheless needed a classification based on the ICD in order to assess their progress in health care and in the control of disease. A field test with a bi-axial classification approach—one axis (criterion) for anatomy, with another for etiology—showed the impracticability of such approach for routine use.

The final proposals presented to and accepted by the Conference in 1978 retained the basic structure of the ICD, although with much additional detail at the level of the four digit subcategories, and some optional five digit subdivisions. For the benefit of users not requiring such detail, care was taken to ensure that the categories at the three digit level were appropriate.

For the benefit of users wishing to produce statistics and indexes oriented towards medical care, the 9th Revision included an optional alternative method of classifying diagnostic statements, including information about both an underlying general disease and a manifestation in a particular organ or site. This system became known as the 'dagger and asterisk system' and is retained in the Tenth Revision. A number of other technical innovations were included in the Ninth Revision, aimed at increasing its flexibility for use in a variety of situations.

It was eventually replaced by ICD-10, the version currently in use by the WHO and most countries. Given the widespread expansion in the tenth revision, it is not possible to convert ICD-9 data sets directly into ICD-10 data sets, although some tools are available to help guide users.
Publication of ICD-9 without IP restrictions in a world with evolving electronic data systems led to a range of products based on ICD-9, such as MeDRA or the Read directory.

ICPM

When ICD-9 was published by the World Health Organization (WHO), the International Classification of Procedures in Medicine (ICPM) was also developed (1975) and published (1978). The ICPM surgical procedures fascicle was originally created by the United States, based on its adaptations of ICD (called ICDA), which had contained a procedure classification since 1962. ICPM is published separately from the ICD disease classification as a series of supplementary documents called fascicles (bundles or groups of items). Each fascicle contains a classification of modes of laboratory, radiology, surgery, therapy, and other diagnostic procedures. Many countries have adapted and translated the ICPM in parts or as a whole and are using it with amendments since then.

ICD-9-CM

"International Classification of Diseases, Clinical Modification" (ICD-9-CM) is an adaption created by the U.S. National Center for Health Statistics (NCHS) and used in assigning diagnostic and procedure codes associated with inpatient, outpatient, and physician office utilization in the United States. The ICD-9-CM is based on the ICD-9 but provides for additional morbidity detail. It is updated annually on October 1.

It consists of two or three volumes: 

The NCHS and the Centers for Medicare and Medicaid Services are the U.S. governmental agencies responsible for overseeing all changes and modifications to the ICD-9-CM.

Work on ICD-10 began in 1983, and the new revision was endorsed by the Forty-third World Health Assembly in May 1990. The latest version came into use in WHO Member States starting in 1994. The classification system allows more than 155,000 different codes and permits tracking of many new diagnoses and procedures, a significant expansion on the 17,000 codes available in ICD-9.
Adoption was relatively swift in most of the world. Several materials are made available online by WHO to facilitate its use, including a manual, training guidelines, a browser, and files for download. Some countries have adapted the international standard, such as the "ICD-10-AM" published in Australia in 1998 (also used in New Zealand), and the "ICD-10-CA" introduced in Canada in 2000.

ICD-10-CM

Adoption of ICD-10-CM was slow in the United States. Since 1979, the US had required ICD-9-CM codes for Medicare and Medicaid claims, and most of the rest of the American medical industry followed suit. On 1 January 1999 the ICD-10 (without clinical extensions) was adopted for reporting mortality, but ICD-9-CM was still used for morbidity. Meanwhile, NCHS received permission from the WHO to create a clinical modification of the ICD-10, and has production of all these systems:


On 21 August 2008, the US Department of Health and Human Services (HHS) proposed new code sets to be used for reporting diagnoses and procedures on health care transactions. Under the proposal, the ICD-9-CM code sets would be replaced with the ICD-10-CM code sets, effective 1 October 2013. On 17 April 2012 the Department of Health and Human Services (HHS) published a proposed rule that would delay, from 1 October 2013 to 1 October 2014, the compliance date for the ICD-10-CM and PCS. Once again, Congress delayed implementation date to 1 October 2015, after it was inserted into "Doc Fix" Bill without debate over objections of many.

Revisions to ICD-10-CM Include:
ICD-10-CA

ICD-10-CA is a clinical modification of ICD-10 developed by the Canadian Institute for Health Information for morbidity classification in Canada. ICD-10-CA applies beyond acute hospital care, and includes conditions and situations that are not diseases but represent risk factors to health, such as occupational and environmental factors, lifestyle and psycho-social circumstances.

The eleventh revision of the International Classification of Diseases, or the ICD-11, is almost five times as big as the ICD-10. It was created following a decade of development involving over 300 specialists from 55 countries. Following an alpha version in May 2011 and a beta draft in May 2012, a stable version of the ICD-11 was released on 18 June 2018, and officially endorsed by all WHO members during the 72nd World Health Assembly on 25 May 2019.

For the ICD-11, the WHO decided to differentiate between the core of the system and its derived specialty versions, such as the ICD-O for oncology. As such, the collection of all ICD entities is called the Foundation Component. From this common core, subsets can be derived. The primary derivative of the Foundation is called the ICD-11 MMS, and it is this system that is commonly referred to and recognized as "the ICD-11". MMS stands for Mortality and Morbidity Statistics.

ICD-11 comes with an implementation package that includes transition tables from and to ICD-10, a translation tool, a coding tool, web-services, a manual, training material, and more. All tools are accessible after self-registration from the Maintenance Platform.

The ICD-11 will "officially" come into effect on 1 January 2022, although the WHO admitted that "not many countries are likely to adapt that quickly". In the United States, the advisory body of the Secretary of Health and Human Services has given an expected release year of 2025, but if a clinical modification is determined to be needed (similar to the ICD-10-CM), this could become 2027.

In the United States, the U.S. Public Health Service published "The International Classification of Diseases, Adapted for Indexing of Hospital Records and Operation Classification (ICDA)," completed in 1962 and expanding the ICD-7 in a number of areas to more completely meet the indexing needs of hospitals. The U.S. Public Health Service later published the "Eighth Revision, International Classification of Diseases, Adapted for Use in the United States," commonly referred to as ICDA-8, for official national morbidity and mortality statistics. This was followed by the "ICD, 9th Revision, Clinical Modification", known as ICD-9-CM, published by the U.S. Department of Health and Human Services and used by hospitals and other healthcare facilities to better describe the clinical picture of the patient. The diagnosis component of ICD-9-CM is completely consistent with ICD-9 codes, and remains the data standard for reporting morbidity. National adaptations of the ICD-10 progressed to incorporate both clinical code (ICD-10-CM) and procedure code (ICD-10-PCS) with the revisions completed in 2003. In 2009, the U.S. Centers for Medicare and Medicaid Services announced that it would begin using ICD-10 on April 1, 2010, with full compliance by all involved parties by 2013.
However, the US extended the deadline twice and did not formally require transitioning to ICD-10-CM (for most clinical encounters) until October 1, 2015.

The years for which causes of death in the United States have been classified by each revision as follows:

Cause of death on United States death certificates, statistically compiled by the Centers for Disease Control and Prevention (CDC), are coded in the ICD, which does not include codes for human and system factors commonly called medical errors.

The ICD includes a section classifying mental and behavioral disorders (). This has developed alongside the Diagnostic and Statistical Manual of Mental Disorders (DSM) of the American Psychiatric Association and the two manuals seek to use the same codes. The WHO is revising their classifications in these sections as part the development of the ICD-11 (scheduled for 2018), and an "International Advisory Group" has been established to guide this. Section F66 of the ICD-10 deals with classifications of psychological and behavioural disorders that are associated with sexual development and orientation. It explicitly states that "sexual orientation by itself is not to be considered a disorder," in line with the DSM and other classifications that recognise homosexuality as a normal variation in human sexuality. The Working Group has reported that there is "no evidence that [these classifications] are clinically useful" and recommended that section F66 be deleted for the ICD-11.

An international survey of psychiatrists in 66 countries comparing use of the ICD-10 and DSM-IV found that the former was more often used for clinical diagnosis while the latter was more valued for research. The ICD is actually the official system for the US, although many mental health professionals do not realize this due to the dominance of the DSM. A psychologist has stated: "Serious problems with the clinical utility of both the ICD and the DSM are widely acknowledged."


Note: Since adoption of ICD-10 CM in the US, several online tools have been mushrooming. They all refer to that particular modification and thus are not linked here.


</doc>
<doc id="15462" url="https://en.wikipedia.org/wiki?curid=15462" title="Integral domain">
Integral domain

In mathematics, specifically abstract algebra, an integral domain is a nonzero commutative ring in which the product of any two nonzero elements is nonzero. Integral domains are generalizations of the ring of integers and provide a natural setting for studying divisibility. In an integral domain, every nonzero element "a" has the cancellation property, that is, if , an equality implies .

"Integral domain" is defined almost universally as above, but there is some variation. This article follows the convention that rings have a multiplicative identity, generally denoted 1, but some authors do not follow this, by not requiring integral domains to have a multiplicative identity. Noncommutative integral domains are sometimes admitted. This article, however, follows the much more usual convention of reserving the term "integral domain" for the commutative case and using "domain" for the general case including noncommutative rings.

Some sources, notably Lang, use the term entire ring for integral domain.

Some specific kinds of integral domains are given with the following chain of class inclusions:

An "integral domain" is basically defined as a nonzero commutative ring in which the product of any two nonzero elements is nonzero. This definition may be reformulated in a number of equivalent definitions :

A fundamental property of integral domains is that every subring of a field is an integral domain, and that, conversely, given any integral domain, one may construct a field that contains it as a subring, the field of fractions. This characterization may be viewed as a further equivalent definition:








The following rings are "not" integral domains.









In this section, "R" is an integral domain.

Given elements "a" and "b" of "R", one says that "a" "divides" "b", or that "a" is a "divisor" of "b", or that "b" is a "multiple" of "a", if there exists an element "x" in "R" such that "ax" = "b".

The "units" of "R" are the elements that divide 1; these are precisely the invertible elements in "R". Units divide all other elements.

If "a" divides "b" and "b" divides "a", then "a" and "b" are associated elements or associates. Equivalently, "a" and "b" are associates if "a" = "ub" for some unit "u".

An "irreducible element" is a nonzero non-unit that cannot be written as a product of two non-units.

A nonzero non-unit "p" is a "prime element" if, whenever "p" divides a product "ab", then "p" divides "a" or "p" divides "b". Equivalently, an element "p" is prime if and only if the principal ideal ("p") is a nonzero prime ideal. 

Both notions of irreducible elements and prime elements generalize the ordinary definition of prime numbers in the ring formula_58 if one considers as prime the negative primes.

Every prime element is irreducible. The converse is not true in general: for example, in the quadratic integer ring formula_59 the element 3 is irreducible (if it factored nontrivially, the factors would each have to have norm 3, but there are no norm 3 elements since formula_60 has no integer solutions), but not prime (since 3 divides formula_61 without dividing either factor). In a unique factorization domain (or more generally, a GCD domain), an irreducible element is a prime element.

While unique factorization does not hold in formula_59, there is unique factorization of ideals. See Lasker–Noether theorem.


The field of fractions "K" of an integral domain "R" is the set of fractions "a"/"b" with "a" and "b" in "R" and "b" ≠ 0 modulo an appropriate equivalence relation, equipped with the usual addition and multiplication operations. It is "the smallest field containing "R" " in the sense that there is an injective ring homomorphism such that any injective ring homomorphism from "R" to a field factors through "K". The field of fractions of the ring of integers formula_1 is the field of rational numbers formula_66 The field of fractions of a field is isomorphic to the field itself.

Integral domains are characterized by the condition that they are reduced (that is "x" = 0 implies "x" = 0) and irreducible (that is there is only one minimal prime ideal). The former condition ensures that the nilradical of the ring is zero, so that the intersection of all the ring's minimal primes is zero. The latter condition is that the ring have only one minimal prime. It follows that the unique minimal prime ideal of a reduced and irreducible ring is the zero ideal, so such rings are integral domains. The converse is clear: an integral domain has no nonzero nilpotent elements, and the zero ideal is the unique minimal prime ideal.

This translates, in algebraic geometry, into the fact that the coordinate ring of an affine algebraic set is an integral domain if and only if the algebraic set is an algebraic variety.

More generally, a commutative ring is an integral domain if and only if its spectrum is an integral affine scheme.

The characteristic of an integral domain is either 0 or a prime number.

If "R" is an integral domain of prime characteristic "p", then the Frobenius endomorphism "f"("x") = "x" is injective.




</doc>
<doc id="15466" url="https://en.wikipedia.org/wiki?curid=15466" title="Infundibulum">
Infundibulum

An infundibulum (Latin for "funnel"; plural, "infundibula") is a funnel-shaped cavity or organ.






</doc>
<doc id="15467" url="https://en.wikipedia.org/wiki?curid=15467" title="Interrupt latency">
Interrupt latency

In computing, interrupt latency is the time that elapses from when an interrupt is generated to when the source of the interrupt is serviced. For many operating systems, devices are serviced as soon as the device's interrupt handler is executed. Interrupt latency may be affected by microprocessor design, interrupt controllers, interrupt masking, and the operating system's (OS) interrupt handling methods.

There is usually a trade-off between interrupt latency, throughput, and processor utilization. Many of the techniques of CPU and OS design that improve interrupt latency will decrease throughput and increase processor utilization. Techniques that increase throughput may increase interrupt latency and increase processor utilization. Lastly, trying to reduce processor utilization may increase interrupt latency and decrease throughput.

Minimum interrupt latency is largely determined by the interrupt controller circuit and its configuration. They can also affect the jitter in the interrupt latency, which can drastically affect the real-time schedulability of the system. The Intel APIC architecture is well known for producing a huge amount of interrupt latency jitter.

Maximum interrupt latency is largely determined by the methods an OS uses for interrupt handling. For example, most processors allow programs to disable interrupts, putting off the execution of interrupt handlers, in order to protect critical sections of code. During the execution of such a critical section, all interrupt handlers that cannot execute safely within a critical section are blocked (they save the minimum amount of information required to restart the interrupt handler after all critical sections have exited). So the interrupt latency for a blocked interrupt is extended to the end of the critical section, plus any interrupts with equal and higher priority that arrived while the block was in place.

Many computer systems require low interrupt latencies, especially embedded systems that need to control machinery in real-time. Sometimes these systems use a real-time operating system (RTOS). An RTOS makes the promise that no more than a specified maximum amount of time will pass between executions of subroutines. In order to do this, the RTOS must also guarantee that interrupt latency will never exceed a predefined maximum.

Advanced interrupt controllers implement a multitude of hardware features in order to minimize the overhead during context switches and the effective interrupt latency. These include features like:


Also, there are many other methods hardware may use to help lower the requirements for shorter interrupt latency in order to make a given interrupt latency tolerable in a situation. These include buffers, and flow control. For example, most network cards implement transmit and receive ring buffers, interrupt rate limiting, and hardware flow control. Buffers allow data to be stored until it can be transferred, and flow control allows the network card to pause communications without having to discard data if the buffer is full.

Modern hardware also implements interrupt rate limiting. This helps prevent interrupt storms or live-locks by having the hardware wait a programmable minimum amount of time between each interrupt it generates. Interrupt rate limiting reduces the amount of time spent servicing interrupts, allowing the processor to spend more time doing useful work. Exceeding this time results in a soft (recoverable) or hard (non-recoverable) error.



</doc>
<doc id="15468" url="https://en.wikipedia.org/wiki?curid=15468" title="İskender kebap">
İskender kebap

İskender kebap is one of the best-known dishes of northwestern Turkey. It takes its name from its inventor, İskender Efendi (Master Alexander), who lived in Bursa in the late 19th century Ottoman Empire.

The dish consists of döner kebab prepared from thinly cut grilled lamb topped with hot tomato sauce over pieces of pita bread and generously slathered with melted sheep butter and yogurt. Tomato sauce and melted butter are generally poured over the dish, at the table.

"Kebapçı İskender" is trademarked by the İskenderoğlu family, who still run the restaurant in Bursa. This dish is available in many restaurants throughout the country mostly under the name "İskender kebap", "Bursa kebabı", or at times with an alternative one made up by the serving restaurant such as "Uludağ kebabı".




</doc>
<doc id="15471" url="https://en.wikipedia.org/wiki?curid=15471" title="LGBT in Islam">
LGBT in Islam

Attitudes toward lesbian, gay, bisexual, and transgender (LGBT) people and their experiences in the Muslim world have been influenced by its religious, legal, social, political, and cultural history.

The Quran narrates the story of the "people of Lot" destroyed by the wrath of God because the men engaged in lustful carnal acts between themselves. Some hadith collections also condemn homosexual and transgender acts, prescribing death penalty for male homosexual intercourse.

Homosexual acts are forbidden in traditional Islamic jurisprudence and are liable to different punishments, including the death penalty, depending on the situation and legal school. However, homosexual relationships were generally tolerated in pre-modern Islamic societies, and historical records suggest that these laws were invoked infrequently, mainly in cases of rape or other "exceptionally blatant infringement on public morals". There is little evidence of homosexual practice in Islamic societies for the first century and a half of the Islamic era. Homoerotic themes were cultivated in poetry and other literary genres written in major languages of the Muslim world from the eighth century into the modern era. The conceptions of homosexuality found in classical Islamic texts resemble the traditions of Graeco-Roman antiquity rather than the modern understanding of sexual orientation. Public attitudes toward homosexuality in the Ottoman Empire and elsewhere in the Muslim world underwent a marked negative change starting from the 19th century through the gradual spread of Islamic fundamentalism such as Wahhabism and under the influence of the sexual notions and restrictive norms prevalent in Europe at the time: a number of Muslim-majority countries have retained criminal penalties for homosexual acts enacted under British and Soviet rule.

In recent times, extreme prejudice against homosexuals persists, both socially and legally, in much of the Islamic world, exacerbated by increasingly conservative attitudes and the rise of Islamist movements. In Afghanistan, Brunei, Iran, Mauritania, Nigeria, Saudi Arabia, parts of Somalia, Sudan, United Arab Emirates, and Yemen, homosexual activity carries the death penalty or prison sentences. In other countries, such as Algeria, Bangladesh, Chad, Malaysia, Maldives, Pakistan, Qatar, Somalia, and Syria, it is illegal, and penalties may be imposed. Same-sex sexual intercourse is legal in Albania, Azerbaijan, Bahrain, Bosnia and Herzegovina, Burkina Faso, Djibouti, Guinea-Bissau, Iraq, Jordan, Kazakhstan, Kosovo, Kyrgyzstan, Lebanon, Mali, Niger, Tajikistan, Turkey, most of Indonesia, the West Bank (State of Palestine), and Northern Cyprus. Homosexual relations between females are legal in Kuwait, Turkmenistan, and Uzbekistan, but homosexual acts between males are illegal.

Most Muslim-majority countries and the Organisation of Islamic Cooperation (OIC) have opposed moves to advance LGBT rights at the United Nations, in the General Assembly or the UNHRC. In May 2016, a group of 51 Muslim majority states blocked 11 gay and transgender organizations from attending the 2016 High Level Meeting on Ending AIDS. However, Albania and Sierra Leone have signed a UN Declaration supporting LGBT rights. LGBT anti-discrimination laws have been enacted in Albania, Kosovo, and Northern Cyprus. There are also several organizations for LGBT Muslims which support LGBT rights, and others which attempt conversion therapy. 

The Quran contains several allusions to homosexual activity, which has prompted considerable exegetical and legal commentary over the centuries. The subject is most clearly addressed in the story of Sodom and Gomorrah (seven verses) after the city inhabitants demand sexual access to the messengers sent by God to the prophet Lot (or Lut). The Quranic narrative largely conforms to that found in Genesis. In one passage the Quran says that the men "solicited his guests of him" (), using an expression that parallels phrasing used to describe the attempted seduction of Joseph, and in multiple passages they are accused of "coming with lust" to men instead of women (or their wives). The Quran terms this an abomination or fahisha () unprecedented in the history of the world:

Later exegetical literature built on these verses as writers attempted to give their own views as to what went on; and there was general agreement among exegetes that the "abomination" alluded to by the Quranic passages was attempted sodomy (specifically anal intercourse) between men. Some Muslim academics disagree with this interpretation, arguing that the people of Lot were destroyed not because of participation in same-sex acts, but because of misdeeds which included refusing to worship one God, disregarding the authority of the Prophets, and attempting to rape the travelers, a crime made even worse by the fact that the travelers were under Lot's protection and hospitality.

The sins of the people of Lut () subsequently became proverbial and the Arabic words for the act of anal sex between men such as liwat () and for a person who performs such acts () both derive from his name, although Lut was not the one demanding sex.

Only one passage in the Quran prescribes a strictly legal position. It is not restricted to homosexual behaviour, however, and deals more generally with "zina" (illicit sexual intercourse):
Most exegetes hold that these verses refer to illicit heterosexual relationships, although a minority view attributed to the Mu'tazilite scholar Abu Muslim al-Isfahani interpreted them as referring to homosexual relations. This view was widely rejected by medieval scholars, but has found some acceptance in modern times.

Some Quranic verses describing the paradise refer to "immortal boys" (56:17, 76:19) or "young men" (52:24) who serve wine to the blessed. Although the "tafsir" literature does not interpret this as a homoerotic allusion, the connection was made in other literary genres, mostly humorously. For example, the Abbasid-era poet Abu Nuwas wrote:

Jurists of the Hanafi school took up the question seriously, considering, but ultimately rejecting the suggestion that homosexual pleasures were, like wine, forbidden in this world but enjoyed in the afterlife.

The hadith (sayings and actions attributed to Muhammad) show that homosexual behaviour was not unknown in seventh-century Arabia. However, given that the Quran did not specify the punishment of homosexual sodomy, Islamic jurists increasingly turned to several "more explicit" hadiths in an attempt to find guidance on appropriate punishment.

While there are no reports relating to homosexuality in the best known hadith collections of Bukhari and Muslim, other canonical collections record a number of condemnations of the "act of the people of Lot" (male-to-male anal intercourse). For example, Abu `Isa Muhammad ibn `Isa at-Tirmidhi (compiling the Sunan al-Tirmidhi around C.E.884) wrote that Muhammad had indeed prescribed the death penalty for both the active and also the passive partner:

Ibn al-Jawzi (1114–1200) writing in the 12th century claimed that Muhammad had cursed "sodomites" in several hadith, and had recommended the death penalty for both the active and passive partners in homosexual acts.

Al-Nuwayri (1272–1332) in his "Nihaya" reports that Muhammad is "alleged to have said what he feared most for his community were the practices of the people of Lot (he seems to have expressed the same idea in regard to wine and female seduction)."

Other hadiths seem to permit homoerotic feelings as long as they are not translated into action. One hadith acknowledges homoerotic temptation and warns against it: "Do not gaze at the beardless youths, for verily they have eyes more tempting than the "houris"" or "... for verily they resemble the "houris"". These beardless youths are also described as wearing sumptuous robes and having perfumed hair.

In addition, there is a number of "purported (but mutually inconsistent) reports" ("athar") of punishments of sodomy ordered by early caliphs. Abu Bakr apparently recommended toppling a wall on the culprit, or else burning him alive, while Ali bin Abi Talib is said to have ordered death by stoning for one sodomite and had another thrown head-first from the top of a minaret—according to Ibn Abbas, the latter punishment must be followed by stoning.

There are, however, fewer hadith mentioning homosexual behavior in women;
but punishment (if any) for lesbianism was not clarified.

In Islam, the term mukhannathun is used to describe gender-variant people, usually male-to-female transgender. Neither this term nor the equivalent for "eunuch" occurs in the Quran, but the term does appear in the Hadith, the sayings of Muhammad, which have a secondary status to the central text. Moreover, within Islam, there is a tradition of the elaboration and refinement of extended religious doctrines through scholarship. This doctrine contains a passage by the scholar and hadith collector An-Nawawi:A mukhannath is the one ("male") who carries in his movements, in his appearance and in his language the characteristics of a woman. There are two types; the first is the one in whom these characteristics are innate, he did not put them on by himself, and therein is no guilt, no blame and no shame, as long as he does not perform any (illicit) act or exploit it for money (prostitution etc.). The second type acts like a woman out of immoral purposes and he is the sinner and blameworthy.The hadith collection of Bukhari (compiled in the 9th century from earlier oral traditions) includes a report regarding "mukhannathun", effeminate men who were granted access to secluded women's quarters and engaged in other non-normative gender behavior: This hadiths attributed to Muhammad's wives, a "mukhannath" in question expressed his appreciation of a woman's body and described it for the benefit of another man. According to one hadith, this incident was prompted by a "mukhannath" servant of Muhammad's wife Umm Salama commenting upon the body of a woman and following that, Muhammad cursed the "mukhannathun" and their female equivalents, "mutarajjilat" and ordered his followers to remove them from their homes.

According to Everett Rowson, none of the sources state that Muhammad banished more than two "mukhannathun", and it is not clear to what extent the action was taken because of their breaking of gender rules in itself or because of the "perceived damage to social institutions from their activities as matchmakers and their corresponding access to women".

The paucity of concrete prescriptions to be derived from hadith and the contradictory nature of information about the actions of early authorities resulted in lack of agreement among classical jurists as to how homosexual activity should be treated.
Classical Islamic jurists did not deal with homosexuality as a sexual orientation, since the latter concept is modern and has no match in traditional law, which dealt with it under the technical terms of "liwat" and "zina". 

Broadly, traditional Islamic law took the view that homosexual activity could not be legally sanctioned because it takes place outside religiously-recognised marriages. All major schools of law consider liwat (anal sex) as a punishable offence. Most legal schools treat homosexual intercourse with penetration similarly to unlawful heterosexual intercourse under the rubric of "zina", but there are differences of opinion with respect to methods of punishment. Some legal schools "prescribed capital punishment for sodomy, but others opted only for a relatively mild discretionary punishment." The Hanbalites are the most severe among Sunni schools, insisting on capital punishment for anal sex in all cases, while the other schools generally restrict punishment to flagellation with or without banishment, unless the culprit is "muhsan" (Muslim free married adult), and Hanafis often suggest no physical punishment at all, leaving the choice to the judge's discretion. The founder of the Hanafi school Abu Hanifa refused to recognize the analogy between sodomy and "zina", although his two principal students disagreed with him on this point. The Hanafi scholar Abu Bakr Al-Jassas (d. 981 AD/370 AH) argued that the two hadiths on killing homosexuals "are not reliable by any means and no legal punishment can be prescribed based on them". Where capital punishment is prescribed and a particular method is recommended, the methods range from stoning (Hanbali, Maliki), to the sword (some Hanbalites and Shafi'ites), or leaving it to the court to choose between several methods, including throwing the culprit off a high building (Shi'ite).

For unclear reasons, the treatment of homosexuality in Twelver Shia jurisprudence is generally harsher than in Sunni fiqh, while Zaydi and Isma'ili Shia jurists took positions similar to the Sunnis. Where flogging is prescribed, there is a tendency for indulgence and some recommend that the prescribed penalty should not be applied in full, with Ibn Hazm reducing the number of strokes to 10. There was debate as to whether the active and passive partners in anal sex should be punished equally. Beyond penetrative anal sex, there was "general agreement" that "other homosexual acts (including any between females) were lesser offenses, subject only to discretionary punishment." Some jurists viewed sexual intercourse as possible only for an individual who possesses a phallus; hence those definitions of sexual intercourse that rely on the entry of as little of the corona of the phallus into a partner's orifice. Since women do not possess a phallus and cannot have intercourse with one another, they are, in this interpretation, physically incapable of committing zinā.

Since a "hadd" punishment for "zina" requires testimony from four witnesses to the actual act of penetration or a confession from the accused repeated four times, the legal criteria for the prescribed harsh punishments of homosexual acts were very difficult to fulfill. The debates of classical jurists are "to a large extent theoretical, since homosexual relations have always been tolerated" in pre-modern Islamic societies. While it is difficult to ascertain to what extent the legal sanctions were enforced in different times and places, historical record suggests that the laws were invoked mainly in cases of rape or other "exceptionally blatant infringement on public morals". Documented instances of prosecution for homosexual acts are rare, and those which followed legal procedure prescribed by Islamic law are even rarer.

In Kecia Ali's book, she cites that "contemporary scholars disagree sharply about the Qur'anic perspective on same-sex intimacy." One scholar represents the conventional perspective by arguing that the Qur'an "is very explicit in its condemnation of homosexuality leaving scarcely any loophole for a theological accommodation of homosexuality in Islam." Another scholar argues that "the Qur'an does not address homosexuality or homosexuals explicitly." Overall, Ali says that "there is no one Muslim perspective on anything."

Many Muslim scholars have followed a "don't ask, don't tell" policy in regards to homosexuality in Islam, by treating the subject with passivity.

Mohamed El-Moctar El-Shinqiti, director of the Islamic Center of South Plains in Texas, has argued that "[even though] homosexuality is a grievous sin...[a] no legal punishment is stated in the Qur'an for homosexuality...[b] it is not reported that Prophet Muhammad has punished somebody for committing homosexuality...[c] there is no authentic hadith reported from the Prophet prescribing a punishment for the homosexuals..." Classical hadith scholars such as Al-Bukhari, Yahya ibn Ma'in, Al-Nasa'i, Ibn Hazm, Al-Tirmidhi, and others have impugned the authenticity of hadith reporting these statements.

Egyptian Islamist journalist Muhammad Jalal Kishk also found no punishment for homosexual acts prescribed in the Quran, regarding the hadith that mentioned it as poorly attested. He did not approve of such acts, but believed that Muslims who abstained from sodomy would be rewarded by sex with youthful boys in paradise.

Faisal Kutty, a professor of Islamic law at Indiana-based Valparaiso University Law School and Toronto-based Osgoode Hall Law School, commented on the contemporary same-sex marriage debate in a March 27, 2014, essay in the Huffington Post. He acknowledged that while Islamic law iterations prohibits pre- and extra-marital as well as same-sex sexual activity, it does not attempt to "regulate feelings, emotions and urges, but only its translation into action that authorities had declared unlawful". Kutty, who teaches comparative law and legal reasoning, also wrote that many Islamic scholars have "even argued that homosexual tendencies themselves were not haram [prohibited] but had to be suppressed for the public good". He claimed that this may not be "what the LGBTQ community wants to hear", but that, "it reveals that even classical Islamic jurists struggled with this issue and had a more sophisticated attitude than many contemporary Muslims". Kutty, who in the past wrote in support of allowing Islamic principles in dispute resolution, also noted that "most Muslims have no problem extending full human rights to those—even Muslims—who live together 'in sin'". He argued that it therefore seems hypocritical to deny fundamental rights to same-sex couples. Moreover, he concurred with Islamic legal scholar Mohamed Fadel in arguing that this is not about changing Islamic marriage (nikah), but about making "sure that all citizens have access to the same kinds of public benefits".

Some modern day Muslim scholars, such as Scott Siraj al-Haqq Kugle, argue for a different interpretation of the Lot narrative focusing not on the sexual act but on the infidelity of the tribe and their rejection of Lot's Prophethood. According to Kugle, "where the Qur'an treats same-sex acts, it condemns them only so far as they are exploitive or violent." More generally, Kugle notes that the Quran refers to four different levels of personality. One level is "genetic inheritance." The Qur'an refers to this level as one's "physical stamp" that "determines one's temperamental nature" including one's sexuality. One the basis of this reading of the Qur'an, Kugle asserts that homosexuality is "caused by divine will," so "homosexuals have no rational choice in their internal disposition to be attracted to same-sex mates." Kugle argues that if the classical commentators had seen "sexual orientation as an integral aspect of human personality," they would have read the narrative of Lot and his tribe "as addressing male rape of men in particular" and not as "addressing homosexuality in general." Kugle furthermore reads the Qur'an as holding "a positive assessment of diversity." Under this reading, Islam can be described as "a religion that positively assesses diversity in creation and in human societies," allowing gay and lesbian Muslims to view homosexuality as representing the "natural diversity in sexuality in human societies." A critique of Kugle's approach, interpretations and conclusions was published in 2016 by Mobeen Vaid.

In a 2012 book, Aisha Geissinger writes that there are "apparently irreconcilable Muslim standpoints on same-sex desires and acts," all of which claim "interpretative authenticity." One of these standpoints results from "queer-friendly" interpretations of the Lot story and the Quran. The Lot story is interpreted as condemning "rape and inhospitality rather than today's consensual same-sex relationships."

In their book "Islamic Law and Muslim Same-Sex Unions", Junaid Jahangir and Hussein Abdullatif argue that interpretations which view the Quranic narrative of the people of Lot and the derived classical notion of "liwat" as applying to same-sex relationships reflect the sociocultural norms and medical knowledge of societies that produced those interpretations. They further argue that the notion of "liwat" is compatible with the Quranic narrative, but not with the contemporary understanding of same-sex relationships based on love and shared responsibilities.

Abdessamad Dialmy in his 2010 article, "Sexuality and Islam," addressed "sexual norms defined by the sacred texts (Koran and Sunna)." He wrote that "sexual standards in Islam are paradoxical." The sacred texts "allow and actually are an enticement to the exercise of sexuality." However, they also "discriminate . . . between heterosexuality and homosexuality." Islam's paradoxical standards result in "the current back and forth swing of sexual practices between repression and openness." Dialmy sees a solution to this back and forth swing by a "reinterpretation of repressive holy texts."

Some Islamic & Western scholars argue that in the course of the Quranic Lot story, homosexuality in the modern sense is not addressed, but that the destruction of the "people of Lot" was a result of breaking the ancient hospitality law and sexual violence, in this case the attempted rape of men.

Societies in Islam have recognized "both erotic attraction and sexual behavior between members of the same sex". However, their attitudes about them have often been contradictory: "severe religious and legal sanctions" against homosexual behavior and at the same time "celebratory expressions" of erotic attraction. Homoeroticism was idealized in the form of poetry or artistic declarations of love from one man to another. Accordingly, the Arabic language had an appreciable vocabulary of homoerotic terms, with dozens of words just to describe types of male prostitutes. Schmitt (1992) identifies some twenty words in Arabic, Persian and Turkish to identify those who are penetrated. Other related Arabic words includes "Mukhannathun", "ma'bûn", "halaqī", "baghghā".

There is little evidence of homosexual practice in Islamic societies for the first century and a half of the Islamic era. Homoerotic poetry appears suddenly at the end of the 8th century CE, particularly in Baghdad in the work of Abu Nuwas (756–814), who became a master of all the contemporary genres of Arabic poetry. The famous author Jahiz tried to explain the abrupt change in attitudes toward homosexuality after the Abbasid Revolution by the arrival of the Abbasid army from Khurasan, who are said to have consoled themselves with male pages when they were forbidden to take their wives with them. The increased prosperity following the early conquests was accompanied by a "corruption of morals" in the two holy cities of Mecca and Medina, and it can be inferred that homosexual practice became more widespread during this time as a result of acculturation to foreign customs, such as the music and dance practiced by "mukhannathun", who were mostly foreign in origin. The Abbasid ruler Al-Amin (809–813) was said to have required slave women to be dressed in masculine clothing so he could be persuaded to have sex with them, and a broader fashion for "ghulamiyyat" (boy-like girls) is reflected in literature of the period. The same was said of Andalusian caliph al-Hakam II (915–976).

The conceptions of homosexuality found in classical Islamic texts resemble the traditions of classical Greece and those of ancient Rome, rather than the modern understanding of sexual orientation. It was expected that many mature men would be sexually attracted to both women and adolescent boys (with different views about the appropriate age range for the latter), and such men were expected to wish to play only an active role in homosexual intercourse once they reached adulthood. However, any confident assessment of the actual incidence of homosexual behavior remains elusive. Preference for homosexual over heterosexual relations was regarded as a matter of personal taste rather than a marker of homosexual identity in a modern sense. While playing an active role in homosexual relations carried no social stigma beyond that of licentious behavior, seeking to play a passive role was considered both unnatural and shameful for a mature man. Following Greek precedents, the Islamic medical tradition regarded as pathological only this latter case, and showed no concern for other forms of homosexual behavior.
During the early period, growth of a beard was considered to be the conventional age when an adolescent lost his homoerotic appeal, as evidenced by poetic protestations that the author still found his lover beautiful despite the growing beard. During later periods, the age of the stereotypical beloved became more ambiguous, and this prototype was often represented in Persian poetry by Turkish soldiers. This trend is illustrated by the story of Mahmud of Ghazni (971–1030), the ruler of the Ghaznavid Empire, and his cupbearer Malik Ayaz. Their relationship, which was sketchily attested in contemporary sources, became a staple of Persian literature comparable to the story of Layla and Majnun. Poets used it to illustrate the power of love, pointing to Mahmud as an example of a man who becomes "a slave to his slave", while Malik Ayaz served as an example of the ideal beloved, and a model for purity in Sufi literature.

Other famous examples of homosexuality include the Aghlabid Emir Ibrahim II of Ifriqiya (ruled 875–902), who was said to have been surrounded by some sixty catamites, yet whom he was said to have treated in a most horrific manner. Caliph al-Mutasim in the 9th century and some of his successors were accused of homosexuality. The Christian martyr Pelagius of Córdoba was executed by Andalusian ruler Abd al-Rahman III because the boy refused his advances.

The 14th-century Iranian poet Obeid Zakani, in his scores of satirical stories and poems, has ridiculed the contradiction between the strict prohibitions of homosexuality on the one hand and its common practice on the other. Following is just an example from his Ressaleh Delgosha: “Two old men, who used to exchange sex since their very childhood, were making love on the top of a mosque’s minaret in the holy city of Qom. When both finished their turns, one told the other: “shameless practices have ruined our city.” The other man nodded and said, “You and I are the city’s blessed seniors, what then do you expect from others?”

Mehmed the Conqueror, the Ottoman sultan living in the 15th century, European sources say "who was known to have ambivalent sexual tastes, sent a eunuch to the house of Notaras, demanding that he supply his good-looking fourteen-year-old son for the Sultan's pleasure. When he refused, the Sultan instantly ordered the decapitation of Notaras, together with that of his son and his son-in-law; and their three heads … were placed on the banqueting table before him". Another youth Mehmed found attractive, and who was presumably more accommodating, was Radu III the Fair, the brother of the famous Vlad the Impaler, "Radu, a hostage in Istanbul whose good looks had caught the Sultan's fancy, and who was thus singled out to serve as one of his most favored pages." After the defeat of Vlad, Mehmed placed Radu on the throne of Wallachia as a vassal ruler. However, Turkish sources deny these stories.

According to the "Encyclopedia of Islam and the Muslim World":
Whatever the legal strictures on sexual activity, the positive expression of male homoerotic sentiment in literature was accepted, and assiduously cultivated, from the late eighth century until modern times. First in Arabic, but later also in Persian, Turkish and Urdu, love poetry by men about boys more than competed with that about women, it overwhelmed it. Anecdotal literature reinforces this impression of general societal acceptance of the public celebration of male-male love (which hostile Western caricatures of Islamic societies in medieval and early modern times simply exaggerate).
European travellers remarked on the taste that Shah Abbas of Iran (1588-1629) had for wine and festivities, but also for attractive pages and cup-bearers. A painting by Riza Abbasi with homo-erotic qualities shows the ruler enjoying such delights.

"Homosexuality was a key symbolic issue throughout the Middle Ages in [Islamic] Iberia. As was customary everywhere until the nineteenth century, homosexuality was not viewed as a congenital disposition or 'identity'; the focus was on nonprocreative sexual practices, of which sodomy was the most controversial." For example, in "al-Andalus homosexual pleasures were much indulged by the intellectual and political elite. Evidence includes the behavior of rulers . . . who kept male harems." Although early Islamic writings such as the Quran expressed a mildly negative attitude towards homosexuality, laypersons usually apprehended the idea with indifference, if not admiration. Few literary works displayed hostility towards non-heterosexuality, apart from partisan statements and debates about types of love (which also occurred in heterosexual contexts). Khaled el-Rouayheb (2014) maintain that "much if not most of the extant love poetry of the period [16th to 18th century] is pederastic in tone, portraying an adult male poet's passionate love for a teenage boy".

El-Rouayheb suggests that even though religious scholars considered sodomy as an abhorrent sin, most of them did not genuinely believe that it was illicit to merely fall in love with a boy or expressing this love via poetry. In the secular society however, a male's desire to penetrate a desirable youth was seen as understandable, even if not lawful. On the other hand, men adopting the passive role were more subjected to stigma. The medical term "ubnah" qualified the pathological desire of a male to exclusively be on the receiving end of anal intercourse. Physician that theorized on "ubnah" includes Rhazes, who thought that it was correlated with small genitals and that a treatment was possible provided that the subject was deemed to be not too effeminate and the behavior not "prolonged". Dawud al-Antaki advanced that it could have been caused by an acidic substance embedded in the veins of the anus, causing itchiness and thus the need to seek relief.

In mystic writings of the medieval era, such as Sufi texts, it is "unclear whether the beloved being addressed is a teenage boy or God." European chroniclers censured "the indulgent attitudes to gay sex in the Caliphs' courts." Mustafa Akyol writes that "The Ottoman sultans, arguably, were social liberals compared with the contemporary Islamists of Turkey, let alone the Arab World."

The 18th and 19th centuries saw the rise of Islamic fundamentalism such as Wahhabism, which came to call for stricter adherence to the Hadith. In 1744, Muhammad bin Saud, the tribal ruler of the town of Diriyah, endorsed ibn Abd al-Wahhab’s mission and the two swore an oath to establish together a state run according to true Islamic principles. For the next seventy years, until the dismantlement of the first state in 1818, the Wahhabis dominated from Damascus to Baghdad. Homosexuality, which had been largely tolerated in the Ottoman Empire, also became criminalized, and those found guilty were thrown to their deaths from the top of the minarets.

Homosexuality in the Ottoman Empire was decriminalized in 1858, as part of wider reforms during the Tanzimat. However, authors Lapidus and Salaymeh write that before the 19th century Ottoman society had been open and welcoming to homosexuals and that by the 1850s via European influence they began censoring homosexuality in their society. In Iran, several hundred political opponents were executed in the aftermath of the 1979 Islamic Revolution on accusations of homosexuality, and homosexual intercourse is declared a capital offense in Iran's "Islamic Penal Code", enacted in 1991. Though the grounds for execution in Iran are difficult to track, there is evidence that several people were hanged for homosexual behavior in 2005-2006 and in 2016, in some cases on dubious charges of rape. In some countries like Iran and Iraq the dominant discourse is that Western imperialism has spread homosexuality. In Egypt, though homosexuality is not explicitly criminalized, it has been widely prosecuted under vaguely formulated "morality" laws, and under the current rule of Abdel Fattah el-Sisi arrests of LGBT individuals have risen fivefold, apparently reflecting an effort to appeal to conservatives. In Uzbekistan, an anti-sodomy law, passed after World War II with the goal of increasing the birth rate, was invoked in 2004 against a gay rights activist, who was imprisoned and subjected to extreme abuse. In Iraq, where homosexuality is legal, the breakdown of law and order following the Second Gulf War allowed Islamist militias and vigilantes to act on their prejudice against gays, with ISIS gaining particular notoritety for the gruesome acts of anti-LGBT violence committed under its rule of parts of Syria and Iraq. Scott Siraj al-Haqq Kugle has argued that, while "Muslims commemorate the early days of Islam when they were oppressed as a marginalized few," many of them now forget their history and fail to protect "Muslims who are gay, transgender and lesbian."

According to Georg Klauda, in the 19th and early 20th century, homosexual sexual contact was viewed as relatively commonplace in parts of the Middle East, owing in part to widespread sex segregation, which made heterosexual encounters outside marriage more difficult. Klauda states that "Countless writers and artists such as André Gide, Oscar Wilde, Edward M. Forster, and Jean Genet made pilgrimages in the 19th and 20th centuries from homophobic Europe to Algeria, Morocco, Egypt, and various other Arab countries, where homosexual sex was not only met without any discrimination or subcultural ghettoization whatsoever, but rather, additionally as a result of rigid segregation of the sexes, seemed to be available on every corner." Views about homosexuality have never been universal all across the Islamic world. With reference to the Muslim world more broadly, Tilo Beckers writes that "Besides the endogenous changes in the interpretation of scriptures having a deliberalizing influence that came from within Islamic cultures, the rejection of homosexuality in Islam gained momentum through the exogenous effects of European colonialism, that is, the import of Western cultural understandings of homosexuality as a perversion." University of Münster professor Thomas Bauer points that even though there were many orders of stoning for homosexuality, there is not a single proven case of it being carried out. Bauer continues that "Although contemporary Islamist movements decry homosexuality as a form of Western decadence, the current prejudice against it among Muslim publics stems from an amalgamation of traditional Islamic legal theory with popular notions that were imported from Europe during the colonial era, when Western military and economic superiority made Western notions of sexuality particularly influential in the Muslim world."

In some Muslim-majority countries, current anti-LGBT laws were enacted by United Kingdom or Soviet organs and retained following independence. The 1860 Indian Penal Code, which included an anti-sodomy statute, was used as a basis of penal laws in other parts of the British empire. However, as Dynes and Donaldson point out, North African countries under French colonial tutelage lacked antihomosexual laws which were only born afterwards, with the full weight of Islamic opinion descending on those who, on the model of the gay liberationists of the West, would seek to make "homosexuality" (above all, adult men taking passive roles) publicly respectable. Among former British colonies, only Jordan, Bahrain, and (more recently) India abolished the criminal penalties for consensual homosexual acts introduced under colonial rule. Persecution of homosexuals has been exacerbated in recent decades by a rise in Islamic fundamentalism and the emergence of the gay-rights movement in the West, which allowed Islamists to paint homosexuality as a noxious Western import.

While friendship between men and boys is often described in sexual ways in classical Islamic literature, Khaled El-Rouayheb and Oliver Leaman have argued that it would be misleading to conclude from this that homosexuality was widespread in practice. Such literature tended to use transgressive motifs alluding to what is forbidden, in particular homosexuality and wine. Greek homoerotic motifs may have accurately described practices in ancient Greece, but in their Islamic adaptations they tended to play a satirical or metaphorical rather than descriptive role. At the same time, many miniatures, especially from Ottoman Turkey, contain explicit depictions of pederasty, suggesting that the practice enjoyed a certain degree of popularity. A number of pre-modern texts discuss the possibility of sexual exploitation faced by young boys in educational institutions and warn teachers to take precautions against it.

In modern times, despite the formal disapproval of religious authority, the segregation of women in Muslim societies and the strong emphasis on male virility leads adolescent males and unmarried young men to seek sexual outlets with boys younger than themselves—in one study in Morocco, with boys in the age-range 7 to 13. Men have sex with other males so long as they are the penetrators and their partners are boys, or in some cases effeminate men.

"Liwat" can therefore be regarded as "temptation", and anal intercourse is not seen as repulsively unnatural so much as dangerously attractive. They believe "one has to avoid getting buggered precisely in order not to acquire a taste for it and thus become addicted." Not all sodomy is homosexual: one Moroccan sociologist, in a study of sex education in his native country, notes that for many young men heterosexual sodomy is considered better than vaginal penetration, and female prostitutes likewise report the demand for anal penetration from their (male) clients.

In regards to homosexual intercourse, it is the enjoyment that is considered bad, rather than simply the penetration. Deep shame attaches to the passive partner. Similar sexual sociologies are reported for other Muslim societies from North Africa to Pakistan and the Far East. In Afghanistan in 2009, the British Army was forced to commission a report into the sexuality of the local men after British soldiers reported the discomfort at witnessing adult males involved in sexual relations with boys. The report stated that though illegal, there was a tradition of such relationships in the country, known as "bacha bazi" or "boy play", and that it was especially strong around North Afghanistan.

According to the International Lesbian and Gay Association (ILGA) seven countries still retain capital punishment for homosexual behavior: Saudi Arabia, Yemen, Iran, Afghanistan, Mauritania, Sudan, and northern Nigeria. In United Arab Emirates it is a capital offense. In Qatar, Algeria, Uzbekistan, and the Maldives, homosexuality is punished with time in prison or a fine. This has led to controversy regarding Qatar, which is due to stage the 2022 FIFA World Cup. Human rights groups have questioned the awarding in 2010 of the right to host the competition, due to the possibility that gay football fans may be jailed. In response, Sepp Blatter, head of FIFA, joked that they would have to "refrain from sexual activity" while in Qatar. He later withdrew the remarks after condemnation from rights groups.

Same-sex sexual activity is illegal in Chad since August 1, 2017 under a new penal code. Before that, homosexuality between consenting adults had not been criminalized ever prior to this law.<ref name="Penal Code 2017/1"></ref><ref name="Penal Code 2017/2"></ref>

In Egypt, openly gay men have been prosecuted under general public morality laws. (See Cairo 52.) "Sexual relations between consenting adult persons of the same sex in private are not prohibited as such. However, the Law on the Combating of Prostitution, and the law against debauchery have been used to imprison gay men in recent years." An Egyptian TV host was recently sentenced to a year in prison for interviewing a gay man in January 2019.

Islamic state has decreed capital punishment for gay people. They have executed more than two dozen men and women for suspected homosexual activity, including several thrown off the top of buildings in highly publicized executions.

In India, which has the third-largest Muslim population in the world, and where Muslims form a large minority, the largest Islamic seminary (Darul Uloom Deoband) has vehemently opposed recent government moves to abrogate and liberalize laws from the British Raj era that banned homosexuality. As of September 2018, homosexuality is no longer a criminal act in India, and most of the religious groups withdrew their opposing claims against it in the Supreme Court.

In Iraq, homosexuality is allowed by the government, but terrorist groups often carry out illegal executions of gay people. Saddam Hussein was "unbothered by sexual mores." Ali Hili reports that "since the 2003 invasion more than 700 people have been killed because of their sexuality." He calls Iraq the "most dangerous place in the world for sexual minorities."

In Jordan, where homosexuality is legal, "gay hangouts have been raided or closed on bogus charges, such as serving alcohol illegally." Despite this legality, social attitudes towards homosexuality are still hostile and hateful.

In Pakistan, its law is a mixture of both Anglo-Saxon colonial law as well as Islamic law, both which proscribe criminal penalties for same-sex sexual acts. The Pakistan Penal Code of 1860, originally developed under colonialism, punishes sodomy with a possible prison sentence and has other provisions that impact the human rights of LGBT Pakistanis, under the guise of protecting public morality and order. Yet, the more likely situation for gay and bisexual men is sporadic police blackmail, harassment, fines, and jail sentences.

In Bangladesh, homosexual acts are illegal and punishable according to section 377. Due to the traditional mentality of the predominantly conservative Bangladeshi society, negative attitudes towards those in the LGBT community are high. In 2009 and 2013, the Bangladeshi Parliament refused to overturn Section 377.

In Saudi Arabia, the maximum punishment for homosexual acts is public execution by beheading or being thrown off of roofs.

In Malaysia, homosexual acts are illegal and punishable with jail, fine, deportation, whipping or castration. In October 2018, Prime Minister Mahathir Mohamad stated that Malaysia would not "copy" Western nations' approach towards LGBT rights, indicating that these countries were exhibiting a disregard for the institutions of the traditional family and marriage, as the value system in Malaysia is good. In May 2019, in response to the warning of George Clooney about intending to impose death penalty for homosexuals like Brunei, the Deputy Foreign Minister Marzuki Yahya pointed out that Malaysia does not kill gay people, and will not resort to killing sexual minorities. He also said, although such lifestyles deviate from Islam, the government would not impose such a punishment on the group.

In Indonesia, most parts of the country do not have a sodomy law and do not currently criminalize private, non-commercial homosexual acts among consenting adults, except in Aceh province and for Muslims in Palembang, the capital of South Sumatra province, where homosexuality is illegal for Muslims under Islamic Sharia law, and punishable by flogging. While not criminalising homosexuality, the country does not recognise same-sex marriage. In July 2015, the Minister of Religious Affairs stated that it is unacceptable in Indonesia, because strongly held religious norms speak strongly against it. According to some jurists, there should be death stoning penalty for homosexuals. While another group consider flogging with 100 lashes is the correct punishment.

In Turkey, homosexuality is legal, but "official censure can be fierce". A former interior minister, İdris Naim Şahin, called homosexuality an example of "dishonour, immorality and inhuman situations". Turkey held its 16th Gay Pride Parade in Istanbul on June 30, 2019.

As the latest addition in the list of criminalizing Muslim counties, Brunei's has implemented penalty for homosexuals within "Sharia Penal Code" in stages since 2014. It prescribes death by stoning as punishment for sex between men, and sex between women is punishable by caning or imprisonment. The sultanate currently has a moratorium in effect on death penalty.

In 2016, the International Lesbian, Gay, Bisexual, Trans and Intersex Association (ILGA) released its most recent "State Sponsored Homophobia Report". The report found that thirteen countries or regions impose the death penalty for "same-sex sexual acts" with reference to sharia-based laws. In Iran, according to article 129 and 131 there are up to 100 lashes of whip first three times and fourth time death penalty for lesbians. The death penalty is implemented nationwide in Brunei, Iran, Saudi Arabia, Sudan and Yemen; implemented locally in Nigeria (12 northern states), United Arab Emirates, and Somalia (southern parts); allowed by the law but not implemented in Afghanistan, Mauritania, and Pakistan; and was back then implemented through non-state courts by ISIS in parts of Iraq and Syria (now no longer existing).

Due to Brunei's law dictating that gay sex be punishable by stoning, many lesbian citizens fled to Canada in hopes of finding refuge. The law is also set to impose the same punishment for adultery among heterosexual couples. Despite pushback from citizens in the LGBTQ+ community, Brunei prime minister's office produced a statement explaining Brunei's intention for carrying through with the law. It has been suggested that this is part of a plan to separate Brunei from the western world and towards a Muslim one.

In Algeria, Bangladesh, Chad, Aceh province and Palembang city of South Sumatra province of Indonesia,
Malaysia, Maldives, Pakistan, Qatar, Somalia and Syria, it is illegal, and penalties may be imposed. In Kuwait, Turkmenistan and Uzbekistan, homosexual acts between males are illegal, but homosexual relations between females are legal.

The Ottoman Empire (predecessor of Turkey) decriminalized homosexuality in 1858. In Turkey, where 99.8% of the population is officially registered as Muslim, homosexuality has never been criminalized since the day it was founded in 1923. LGBT people also have the right to seek asylum in Turkey under the Geneva Convention since 1951.

Same-sex sexual intercourse is legal in Albania, Azerbaijan, Bahrain, Bosnia and Herzegovina, Burkina Faso, Djibouti, Guinea-Bissau, Iraq, Jordan, Kazakhstan, Kosovo, Kyrgyzstan, Mali, Niger, Tajikistan, Turkey, West Bank (State of Palestine), most of Indonesia, and in Northern Cyprus. In Albania and Turkey, there have been discussions about legalizing same-sex marriage. Albania, Northern Cyprus, and Kosovo also protect LGBT people with anti-discrimination laws.

Same-sex relations between females are legal in Kuwait, Turkmenistan, and Uzbekistan, but homosexual acts between males are illegal.

In Lebanon, courts have ruled that the country's penal code must not be used to target homosexuals, but the law has yet to be changed by parliament.

In 2007 there was a gay party in the Moroccan town of al-Qasr al-Kabir. Rumours spread that this was a gay marriage and more than 600 people took to the streets, condemning the alleged event and protesting against leniency towards homosexuals. Several persons who attended the party were detained and eventually six Moroccan men were sentenced to between four and ten months in prison for "homosexuality".

In France there was an Islamic same-sex marriage on February 18, 2012. In Paris in November 2012 a room in a Buddhist prayer hall was used by gay Muslims and called a "gay-friendly mosque", and a French Islamic website is supporting religious same-sex marriage.

The first American Muslim in the United States Congress, Keith Ellison (D-MN) said in 2010 that all discrimination against LGBT people is wrong. He further expressed support for gay marriage stating:

I believe that the right to marry someone who you please is so fundamental it should not be subject to popular approval any more than we should vote on whether blacks should be allowed to sit in the front of the bus.

In 2014 eight men were jailed for three years by a Cairo court after the circulation of a video of them allegedly taking part in a private wedding ceremony between two men on a boat on the Nile.

While Iran has outlawed homosexuality, Iranian Shi'a thinkers such as Ayatollah Khomeini have allowed for transgender people to change their sex so that they can enter heterosexual relationships. This position has been confirmed by the Supreme Leader of Iran, Ayatollah Ali Khamenei, and is also supported by many other Iranian clerics.

Iran carries out more sex change operations than any other nation in the world except for Thailand. It is regarded as a cure for homosexuality, which is punishable by death under Iranian law. The government even provides up to half the cost for those needing financial assistance and a sex change is recognized on the birth certificate.

On 26 June 2016, clerics affiliated to the Pakistan-based organization Tanzeem Ittehad-i-Ummat issued a fatwa on transgender people where a trans woman (born male) with "visible signs of being a woman" is allowed to marry a man, and a trans man (born female) with "visible signs of being a man" is allowed to marry a woman. Pakistani transsexuals can also change their gender. Muslim ritual funerals also apply. Depriving transgender people of their inheritance, humiliating, insulting or teasing them were also declared haraam.

In Pakistan, transgender people make up 0.005 percent of the total population. Previously, transgender people were isolated from society and had no legal rights or protections. They also suffered discrimination in healthcare services. For example, in 2016 a transgender individual died in a hospital while doctors were trying to decide which ward the patient should be placed in. Transgender people also faced discrimination in finding employment resulting from incorrect Identification Cards and incongruous legal status. Many were forced into poverty, dancing, singing, and begging on the streets to scrape by. However, in May 2018, the Pakistani parliament passed a bill giving transgender individuals the right to choose their identity and correct their official documents, such as ID cards, driver license, and passports. Today, transgender people in Pakistan have the right to vote and to search for a job free from discrimination. As of 2018, one transgender woman became a news anchor, and two others were appointed to the Supreme Court.

In Lebanon, transgender women are not given any rights. Discrimination starts from their own family members when trans women are forced to leave their house. After that, trans women are not allowed to have any connections with their family members or with their neighbors. Trans women can't access educational institutions and medical services. Moreover, trans women face employment discrimination due to their wrong identification cards that are not being corrected by the government agencies. To support themselves financially, the only option often open to trans women is sex work, which is not safe for them either. Doing sex work, trans women are at higher risk of sexual abuse and violence. No laws are in existence to protect trans women. Instead, trans women are being arrested and put in jail for up to one year for having same sex intercourse.

Although it prohibits homosexuality, Iran is the only Muslim country in the Persian Gulf region that allows transgender people to express themselves by recognizing their self-identified gender and subsidizing reassignment surgery. Despite this, those who do not commit to reassignment surgery are often seen as freaks, and due to their refusal to conform they are treated as outcasts.

In 2011, the UN Human Rights Council passed its first resolution recognizing LGBT rights, which was followed up with a report from the UN Human Rights Commission documenting violations of the rights of LGBT people. The two world maps of the percentage of Muslims per country and the countries that support LGBT rights at the UN give an impression of the attitude towards homosexuality on the part of many Muslim-majority governments.
The Muslim community as a whole, worldwide, has become polarized on the subject of homosexuality. Some Muslims say that "no good Muslim can be gay," and "traditional schools of Islamic law consider homosexuality a grave sin." At the opposite pole, "some Muslims . . . are welcoming what they see as an opening within their communities to address anti-gay attitudes." Especially, it is "young Muslims" who are "increasingly speaking out in support of gay rights".

According to the Albert Kennedy Trust, one in four young homeless people identify as LGBT due to their religious parents disowning them. The Trust suggests that the majority of individuals who are homeless due to religious out casting are either Christian or Muslim. Many young adults who come out to their parents are often forced out of the house to find refuge in a more accepting place. This leads many individual to be homeless – or worse – attempt suicide.

In 2013, the Pew Research Center conducted a study on the global acceptance of homosexuality and found a widespread rejection of homosexuality in many nations that are predominantly Muslim. In some countries, views were becoming more conservative among younger people.




There are a number of Islamic ex-gay organizations, that is, those composed of people claiming to have experienced a basic change in sexual orientation from exclusive homosexuality to exclusive heterosexuality. These groups, like those based in socially conservative Christianity, are aimed at attempting to guide homosexuals towards heterosexuality.
One of the leading LGBT reformatory Muslim organization is StraightWay Foundation, which was established in the United Kingdom in 2004 as an organization that provides information and advice for Muslims who struggle with homosexual attraction. They believe "that through following God's guidance", one may "cease to be" gay. They teach that the male-female pair is the "basis for humanity's growth" and that homosexual acts "are forbidden by God". NARTH has written favourably of the group. In 2004, Straightway entered into a controversy with the contemporary Mayor of London, Ken Livingstone, and the controversial Islamic cleric Yusuf al-Qaradawi. It was suggested that Livingstone was giving a platform to Islamic fundamentalists, and not liberal and progressive Muslims. Straightway responded to this by compiling a document to clarify what they regarded as fundamental issues concerning Islam and homosexuality. They sent Livingstone a letter thanking him for his support of al-Qaradawi. Livingstone then ignited controversy when he thanked Straightway for the letter.

Since February 2017, over 100 male residents of the Chechen Republic (part of the Russian Federation) assumed to be gay or bisexual have been rounded up, detained and tortured by authorities on account of their sexual orientation. These crackdowns have been described as part of a systemic anti-LGBT "purge" in the region. The men are held and allegedly tortured in concentration camps.

Allegations were initially reported in "Novaya Gazeta" on April 1, 2017 a Russian-language opposition newspaper, which reported that over 100 men have allegedly been detained and tortured and at least three people have died in an extrajudicial killing. The paper, citing its sources in the Chechen special services, called the wave of detentions a "prophylactic sweep." The journalist who first reported on the subject has gone into hiding, There have been calls for reprisals for journalists reporting on the situation.

In response, the Russian LGBT Network is attempting to assist those who are threatened to evacuate from Chechnya. Human rights groups and foreign governments have called upon Russia and Chechnya to put an end to the internments.

On 11 January 2019, it was reported that another 'gay purge' had begun in the country in December 2018, with several gay men and women being detained. The Russian LGBT Network believes that around 40 persons were detained and two killed.

Several anti-LGBT incidents have occurred:

The coming together of "human rights discourses and sexual orientation struggles" has resulted in an abundance of "social movements and organizations concerned with gender and sexual minority oppression and discrimination."

The Al-Fatiha Foundation was an organization which tried to advance the cause of gay, lesbian, and transgender Muslims. It was founded in 1998 by Faisal Alam, a Pakistani American, and was registered as a nonprofit organization in the United States. The organization was an offshoot of an internet listserve that brought together many gay, lesbian and questioning Muslims from various countries. The Foundation accepted and considered homosexuality as natural, either regarding Qur'anic verses as obsolete in the context of modern society, or stating that the Qu'ran speaks out against homosexual lust and is silent on homosexual love. After Alam stepped down, subsequent leaders failed to sustain the organization and it began a process of legal dissolution in 2011.

In 2001, Al-Muhajiroun, an international organization which sought the establishment of a global Islamic caliphate, but which is now a banned and defunct, issued a "fatwa" (ruling) declaring that all members of Al-Fatiha were "murtadd" (apostates), and condemning them to death. Because of this threat and their conservative familial backgrounds, many Al-Fatiha members chose anonymity to protect their identity. Al-Fatiha had fourteen chapters in the United States, as well as offices in England, Canada, Spain, Turkey, and South Africa.

The Al-Fitrah Foundation, previously known as The Inner Circle was one of the first queer Muslim organizations founded in 1996 when its founder Imam Muhsin Hendricks publicly revealed his sexual orientation. Imam Muhsin Hendricks is also considered as the world's first openly queer Imam. His activism grew since then and the organization became public in 1998. It soon grew into an international organization with annual international retreats of up to 120 international delegates meeting annually in Cape Town to discuss issues pertaining to LGBTIQ Muslims. In 2018 after having served the organization for 20 years her resigned after detecting corruption in the organization and being maliciously and wrongfully accused of mismanagement of funds. He, along with other queer Muslims who left the old Al-Fitrah Foundation founded a new organization in 2018 called Al-Ghurbaah Foundation. Imam Muhsin Hendricks also administers the Compassion-centred Islamic Network (CCI Network) which is a global network that seeks to connect and create a stronger voice for queer Muslims amongst activists, academics, Islamic scholars and religious leaders (Imams). Currently, the organization's main focus is working with religious leaders (Imams) while serving the needs of the queer Muslim community globally. Al-Ghurbaah Foundation also runs an inclusive mosque called Masjidul Ghurbaah which is open to anyone who wants to connect spiritually regardless of sexual orientation, gender identity, religion or belief.



There are numbers of Muslim LGBT activists from different parts of the world. Some of them are listed below:


In 2010, an anthology "Islam and Homosexuality" was published. In the Forward, Parvez Sharma sounded a pessimistic note about the future: "In my lifetime I do not see Islam drafting a uniform edict that homosexuality is permissible." Following is material from two chapters dealing with the present:

Rusmir Musić in a chapter "Queer Visions of Islam" said that "Queer Muslims struggle daily to reconcile their sexuality and their faith." Musić began to study in college "whether or not my love for somebody of the same gender disgusts God and whether it will propel me to hell. The answer, for me, is an unequivocal "no". Furthermore, Musić wrote, "my research and reflection helped me to imagine my sexuality as a gift from a loving, not hateful, God."

Marhuq Fatima Khan in a chapter "Queer, American, and Muslim: Cultivating Identities and Communities of Affirmation," says that "Queer Muslims employ a few narratives to enable them to reconcile their religious and sexual identities." They "fall into three broad categories: (1) God Is Merciful; (2) That Is Just Who I Am; and (3) It's Not Just Islam."

In Chapter Eight of the 2003 book, "Progressive Muslims: On Justice, Gender, and Pluralism", Professor Scott Siraj al-Haqq Kugle asserts "that Islam does not address homosexuality." In Kugle's reading, the Quran holds "a positive assessment of diversity." It "respects diversity in physical appearance, constitution, stature, and color of human beings as a natural consequence of Divine wisdom in creation." Therefore, Islam can be described as "a religion that positively assesses diversity in creation and in human societies." Furthermore, in Kugle's reading, the Quran "implies that some people are different in their sexual desires than others." Thus, homosexuality can be seen as part of the "natural diversity in sexuality in human societies." This is the way "gay and lesbian Muslims" view their homosexuality.

In addition to the Qur'an, Kugle refers to the benediction of Imam Al-Ghazali (the 11th-century Muslim theologian) which says. "praise be to God, the marvels of whose creation are not subject to the arrows of accident." For Kugle, this benediction implies that "if sexuality is inherent in a person's personality, then sexual diversity is a part of creation, which is never accidental but is always marvelous." Kugle also refers to "a rich archive of same-sex sexual desires and expressions, written by or reported about respected members of society: literati, educated elites, and religious scholars." Given these writings, Kugle concludes that "one might consider Islamic societies (like classical Greece) to provide a vivid illustration of a 'homosexual-friendly' environment." This evoked from "medieval and early modern Christian Europeans" accusations that Muslim were "engaging openly in same-sex practices."

Kugle goes a step further in his argument and asserts that "if some Muslims find it necessary to deny that sexual diversity is part of the natural created world, then the burden of proof rests on their shoulders to illustrate their denial from the Qur'anic discourse itself."

Kecia Ali in her 2016 book "Sexual Ethics and Islam" says that "there is no one Muslim perspective on anything." Regarding the Quran, Ali says that modern scholars disagree about what it says about "same-sex intimacy." Some scholars argue that "the Qur'an does not address homosexuality or homosexuals explicitly."

Regarding homosexuality, Ali, says that the belief that "exclusively homosexual desire is innate in some individuals" has been adopted "even among some relatively conservative Western Muslim thinkers."100 Homosexual Muslims believe their homosexuality to be innate and view "their sexual orientation as God-given and immutable." She observes that "queer and trans people are sometimes treated as defective or deviant," and she adds that it is "vital not to assume that variation implies imperfection or disability."

Regarding "medieval Muslim culture," Ali says that "male desire to penetrate desirable youth . . . was perfectly normal." Even if same-sex relations were not lawful, there was "an unwillingness to seek out and condemn instances of same-sex activity, but rather to let them pass by . . . unpunished." Ali states that some scholars claim that Islamic societies were 'homosexual-friendly' in history.

In an article "Same-sex Sexual Activity and Lesbian and Bisexual Women" Ali elaborates on homosexuality as an aspect of medieval Muslim culture. She says that "same-sex sexual expression has been a more or less recognized aspect of Muslim societies for many centuries." There are many explicit discussions of "same-sex sexual activity" in medieval Arabic literature. Ali states there is a lack of focus in medieval tradition on female same-sex sexual activity, where the Qur'an mainly focuses male/male sex. With female same-sex sexual activity there is more focus on the punishment for the acts and the complications with the dower, compare to men where there is a focus on punishment but also the needs to ablutions and the effect of the act on possible marriage decisions.








</doc>
<doc id="15474" url="https://en.wikipedia.org/wiki?curid=15474" title="Infanticide">
Infanticide

Infanticide (or infant homicide) is the intentional killing of infants. Now universally illegal, infanticide was a widespread practice throughout human history that was mainly used to dispose of unwanted children. Its main purposes were controlling population growth and saving resources from being spent on weak or disabled offspring. Unwanted infants were normally abandoned to die of exposure, but in some societies they were manually killed. 

Most Stone Age human societies routinely practiced infanticide, and estimates of children killed by infanticide in the Mesolithic and Neolithic eras vary from 15 to 50 percent. Infanticide continued to be common in most societies after the historical era began, including ancient Greece, ancient Rome, the Phoenicians, ancient China, ancient Japan, Aboriginal Australia, Native Americans, and Native Alaskans. A few ancient societies did not practice infanticide, such as ancient Egypt and the ancient Jews.

Infanticide became forbidden in Europe and the Near East during the 1st millennium AD. Christianity forbade infanticide from its earliest times, which led Constantine the Great and Valentinian I to ban it across the Roman Empire in the 4th century AD. The practice ceased in Arabia in the 7th century after the founding of Islam, since the Quran prohibits infanticide. Infanticide of male babies had become uncommon in China by the Ming dynasty (1368–1644), though female infanticide remained common until the 19th century. During British rule of India the British attempted to ban infanticide but were only partially successful, and female infanticide in some parts of India still continues. Infanticide is now very rare in Western and other developed countries, but may persist in some less developed countries.

Parental infanticide researchers have found that mothers are far more likely than fathers to be the perpetrators of neonaticide and slightly more likely to commit infanticide in general.

The practice of infanticide has taken many forms over time. Child sacrifice to supernatural figures or forces, such as that believed to have been practiced in ancient Carthage, may be only the most notorious example in the ancient world.

A frequent method of infanticide in ancient Europe and Asia was simply to abandon the infant, leaving it to die by exposure (i.e., hypothermia, hunger, thirst, or animal attack).

On at least one island in Oceania, infanticide was carried out until the 20th century by suffocating the infant, while in pre-Columbian Mesoamerica and in the Inca Empire it was carried out by sacrifice (see below).

Many Neolithic groups routinely resorted to infanticide in order to control their numbers so that their lands could support them. Joseph Birdsell believed that infanticide rates in prehistoric times were between 15% and 50% of the total number of births, while Laila Williamson estimated a lower rate ranging from 15% to 20%. Both anthropologists believed that these high rates of infanticide persisted until the development of agriculture during the Neolithic Revolution. Comparative anthropologists have calculated that 50% of female newborn babies were killed by their parents during the Paleolithic era. From the infants hominid skulls (e.g. Taung child skull) that had been traumatized, has been proposed cannibalism by Raymond A. Dart. The children were not necessarily actively killed, but neglect and intentional malnourishment may also have occurred, as proposed by Vicente Lull as an explanation for an apparent surplus of men and the below average height of women in prehistoric Menorca.

Archaeologists have uncovered physical evidence of child sacrifice at several locations. Some of the best attested examples are the diverse rites which were part of the religious practices in Mesoamerica and the Inca Empire.

Three thousand bones of young children, with evidence of sacrificial rituals, have been found in Sardinia. Pelasgians offered a sacrifice of every tenth child during difficult times. Syrians sacrificed children to Jupiter and Juno. Many remains of children have been found in Gezer excavations with signs of sacrifice. Child skeletons with the marks of sacrifice have been found also in Egypt dating 950–720 BCE. In Carthage "[child] sacrifice in the ancient world reached its infamous zenith". Besides the Carthaginians, other Phoenicians, and the Canaanites, Moabites and Sepharvites offered their first-born as a sacrifice to their gods.

In Egyptian households, at all social levels, children of both sexes were valued and there is no evidence of infanticide. The religion of the Ancient Egyptians forbade infanticide and during the Greco-Roman period they rescued abandoned babies from manure heaps, a common method of infanticide by Greeks or Romans, and were allowed to either adopt them as foundling or raise them as slaves, often giving them names such as "copro -" to memorialize their rescue. Strabo considered it a peculiarity of the Egyptians that every child must be reared. Diodorus indicates infanticide was a punishable offence. Egypt was heavily dependent on the annual flooding of the Nile to irrigate the land and in years of low inundation, severe famine could occur with breakdowns in social order resulting, notably between 930–1070 AD and 1180–1350 AD. Instances of cannibalism are recorded during these periods but it is unknown if this happened during the pharaonic era of Ancient Egypt. Beatrix Midant-Reynes describes human sacrifice as having occurred at Abydos in the early dynastic period (c. 3150–2850 BCE), while Jan Assmann asserts there is no clear evidence of human sacrifice ever happening in Ancient Egypt.

According to Shelby Brown, Carthaginians, descendants of the Phoenicians, sacrificed infants to their gods. Charred bones of hundreds of infants have been found in Carthaginian archaeological sites. One such area harbored as many as 20,000 burial urns. Skeptics suggest that the bodies of children found in Carthaginian and Phoenician cemeteries were merely the cremated remains of children that died naturally.

Plutarch (c. 46–120 AD) mentions the practice, as do Tertullian, Orosius, Diodorus Siculus and Philo. The Hebrew Bible also mentions what appears to be child sacrifice practiced at a place called the Tophet (from the Hebrew "taph" or "toph", to burn) by the Canaanites. Writing in the 3rd century BCE, Kleitarchos, one of the historians of Alexander the Great, described that the infants rolled into the flaming pit. Diodorus Siculus wrote that babies were roasted to death inside the burning pit of the god Baal Hamon, a bronze statue.

The historical Greeks considered the practice of adult and child sacrifice barbarous, however, the exposure of newborns was widely practiced in ancient Greece, it was even advocated by Aristotle in the case of congenital deformity — "As to the exposure of children, let there be a law that no deformed child shall live.” In Greece, the decision to expose a child was typically the father's, although in Sparta the decision was made by a group of elders. Exposure was the preferred method of disposal, as that act in itself was not considered to be murder; moreover, the exposed child technically had a chance of being rescued by the gods or any passersby. This very situation was a recurring motif in Greek mythology.
To notify the neighbors of a birth of a child, a woolen strip was hung over the front door to indicate a female baby and an olive branch to indicate a boy had been born. Families did not always keep their new child. After a woman had a baby, she would show it to her husband. If the husband accepted it, it would live, but if he refused it, it would die. Babies would often be rejected if they were illegitimate, unhealthy or deformed, the wrong sex, or too great a burden on the family. These babies would not be directly killed, but put in a clay pot or jar and deserted outside the front door or on the roadway. In ancient Greek religion, this practice took the responsibility away from the parents because the child would die of natural causes, for example, hunger, asphyxiation or exposure to the elements.

The practice was prevalent in ancient Rome, as well. Philo was the first philosopher to speak out against it. A letter from a Roman citizen to his sister, or a pregnant wife from her husband, dating from 1 BC, demonstrates the casual nature with which infanticide was often viewed:

In some periods of Roman history it was traditional for a newborn to be brought to the "pater familias", the family patriarch, who would then decide whether the child was to be kept and raised, or left to die by exposure. The Twelve Tables of Roman law obliged him to put to death a child that was visibly deformed. The concurrent practices of slavery and infanticide contributed to the "background noise" of the crises during the Republic.

Infanticide became a capital offense in Roman law in 374 AD, but offenders were rarely if ever prosecuted.

According to mythology, Romulus and Remus, twin infant sons of the war god Mars, survived near-infanticide after being tossed into the Tiber River. According to the myth, they were raised by wolves, and later founded the city of Rome.

Whereas theologians and clerics preached sparing their lives, newborn abandonment continued as registered in both the literature record and in legal documents. According to William Lecky, exposure in the early Middle Ages, as distinct from other forms of infanticide, "was practiced on a gigantic scale with absolute impunity, noticed by writers with most frigid indifference and, at least in the case of destitute parents, considered a very venial offence". The first foundling house in Europe was established in Milan in 787 on account of the high number of infanticides and out-of-wedlock births. The Hospital of the Holy Spirit in Rome was founded by Pope Innocent III because women were throwing their infants into the Tiber river.

Unlike other European regions, in the Middle Ages the German mother had the right to expose the newborn.

In the High Middle Ages, abandoning unwanted children finally eclipsed infanticide. Unwanted children were left at the door of church or abbey, and the clergy was assumed to take care of their upbringing. This practice also gave rise to the first orphanages.

However, very high sex ratios were common in even late medieval Europe, which may indicate sex-selective infanticide.

Judaism prohibits infanticide, and has for some time, dating back to at least early Common Era. Roman historians wrote about the ideas and customs of other peoples, which often diverged from their own. Tacitus recorded that the Jews "take thought to increase their numbers, for they regard it as a crime to kill any late-born children". Josephus, whose works give an important insight into 1st-century Judaism, wrote that God "forbids women to cause abortion of what is begotten, or to destroy it afterward".

In his book "Germania", Tacitus wrote in 98 AD that the ancient Germanic tribes enforced a similar prohibition. He found such mores remarkable and commented: ""[The Germani] hold it shameful to kill any unwanted child."" It has become clear over the millennia, though, that Tacitus' description was inaccurate; the consensus of modern scholarship significantly differs. John Boswell believed that in ancient Germanic tribes unwanted children were exposed, usually in the forest. "It was the custom of the [Teutonic] pagans, that if they wanted to kill a son or daughter, they would be killed before they had been given any food." Usually children born out of wedlock were disposed of that way.

In his highly influential "Pre-historic Times", John Lubbock described burnt bones indicating the practice of child sacrifice in pagan Britain.

The last canto, "Marjatan poika" (Son of Marjatta), of Finnish national epic Kalevala describes assumed infanticide. Väinämöinen orders the infant bastard son of Marjatta to be drowned in a marsh.

The Íslendingabók, the main source for the early history of Iceland, recounts that on the Conversion of Iceland to Christianity in 1000 it was provided – in order to make the transition more palatable to Pagans – that "the old laws allowing exposure of newborn children will remain in force".
However, this provision – like other concessions made at the time to the Pagans – was abolished some years later.

Christians have explicitly rejected infanticide. The "Teachings of the Apostles" or "Didache" said "thou shalt not kill a child by abortion, neither shalt thou slay it when born". The "Epistle of Barnabas" stated an identical command, both thus conflating abortion and infanticide. Apologists Tertullian, Athenagoras, Minucius Felix, Justin Martyr and Lactantius also maintained that exposing a baby to death was a wicked act. In 318 AD, Constantine I considered infanticide a crime, and in 374 AD, Valentinian I mandated the rearing of all children (exposing babies, especially girls, was still common). The Council of Constantinople declared that infanticide was homicide, and in 589 AD, the Third Council of Toledo took measures against the custom of killing their own children.

Some Muslim sources allege that pre-Islamic Arabian society practiced infanticide as a form of "post-partum birth control". The word "waʾd" was used to describe the practice. These sources state that infanticide was practiced either out of destitution (thus practiced on males and females alike), or as "disappointment and fear of social disgrace felt by a father upon the birth of a daughter".

Some authors believe that there is little evidence that infanticide was prevalent in pre-Islamic Arabia or early Muslim history, except for the case of the Tamim tribe, who practiced it during severe famine according to Islamic sources. Others state that "female infanticide was common all over Arabia during this period of time" (pre-Islamic Arabia), especially by burying alive a female newborn. A tablet discovered in Yemen, forbidding the people of a certain town from engaging in the practice, is the only written reference to infanticide within the peninsula in pre-Islamic times.

Infanticide is explicitly prohibited by the Qur'an. ""And do not kill your children for fear of poverty; We give them sustenance and yourselves too; surely to kill them is a great wrong.""
Together with polytheism and homicide, infanticide is regarded as a grave sin (see and ). Infanticide is also implicitly denounced in the story of Pharaoh's slaughter of the male children of Israelites (see ; ; ; ; ; ).

Infanticide may have been practiced as human sacrifice, as part of the pagan cult of Perun. Ibn Fadlan describes sacrificial practices at the time of his trip to Kiev Rus (present-day Ukraine) in 921–922, and describes an incident of a woman voluntarily sacrificing her life as part of a funeral rite for a prominent leader, but makes no mention of infanticide. The Primary Chronicle, one of the most important literary sources before the 12th century, indicates that human sacrifice to idols may have been introduced by Vladimir the Great in 980. The same Vladimir the Great formally converted Kiev Rus into Christianity just 8 years later, but pagan cults continued to be practiced clandestinely in remote areas as late as the 13th century.

American explorer George Kennan noted that among the Koryaks, a Mongoloid people of north-eastern Siberia, infanticide was still common in the nineteenth century. One of a pair of twins was always sacrificed.

Infanticide (as a crime) gained both popular and bureaucratic significance in Victorian Britain. By the mid 19th century, in the context of criminal lunacy and the insanity defence, killing one's own child(ren) attracted ferocious debate, as the role of women in society was defined by motherhood, and it was thought that any woman who murdered her own child was by definition insane and could not be held responsible for her actions. Several cases were subsequently highlighted during the Royal Commission on Capital Punishment (1864-66), as a particular felony where an effective avoidance of the death penalty had informally begun.
The New Poor Law Act of 1834 ended parish relief for unmarried mothers and allowed fathers of illegitimate children to avoid paying for "child support". Unmarried mothers then received little assistance and the poor were left with the option either entering the workhouse, prostitution, infanticide or abortion. By the middle of the century infanticide was common for social reasons, such as illegitimacy, and the introduction of child life insurance additionally encouraged some women to kill their children for gain. Examples are Mary Ann Cotton, who murdered many of her 15 children as well as 3 husbands, Margaret Waters, the 'Brixton Baby Farmer', a professional baby-farmer who was found guilty of infanticide in 1870, Jessie King hanged in 1889, Amelia Dyer, the 'Angel Maker', who murdered over 400 babies in her care, and Ada Chard-Williams, a baby farmer who was later hanged at Newgate prison.

The Times reported that 67 infants were murdered in London in 1861 and 150 more recorded as "found dead", many of which were found on the streets. Another 250 were suffocated, half of them not recorded as accidental deaths. The report noted that "infancy in London has to creep into life in the midst of foes."

Recording a birth as a still-birth was also another way of concealing infanticide because still-births did not need to be registered until 1926 and they did not need to be buried in public cemeteries. In 1895 the Sun (London) published an article "Massacre of the Innocents" highlighting the dangers of baby-farming, in the recording of stillbirths and quoting Braxton-Hicks, the London Coroner, on lying-in houses: "I have not the slightest doubt that a large amount of crime is covered by the expression `still-birth’. There are a large number of cases of what are called newly-born children, which are found all over England, more especially in London and large towns, abandoned in streets, rivers, on commons, and so on." He continued "a great deal of that crime is due to what are called lying-in houses, which are not registered, or under the supervision of that sort, where the people who act as midwives constantly, as soon as the child is born, either drop it into a pail of water or smother it with a damp cloth. It is a very common thing, also, to find that they bash their heads on the floor and break their skulls."

The last British woman to be executed for infanticide of her own child was Rebecca Smith, who was hanged in Wiltshire in 1849.

The Infant Life Protection Act of 1897 required local authorities to be notified within 48 hours of changes in custody or the death of children under seven years. Under the Children’s Act of 1908 "no infant could be kept in a home that was so unfit and so overcrowded as to endanger its health, and no infant could be kept by an unfit nurse who threatened, by neglect or abuse, its proper care, and maintenance."

Short of execution, the harshest penalties were imposed on practitioners of infanticide by the legal codes of the Qin dynasty and Han dynasty of ancient China.

Marco Polo, the explorer, saw newborns exposed in Manzi. China's society practiced sex selective infanticide. Philosopher Han Fei Tzu, a member of the ruling aristocracy of the 3rd century BC, who developed a school of law, wrote: "As to children, a father and mother when they produce a boy congratulate one another, but when they produce a girl they put it to death." Among the Hakka people, and in Yunnan, Anhui, Sichuan, Jiangxi and Fujian a method of killing the baby was to put her into a bucket of cold water, which was called "baby water".

Infanticide was known in China as early as the 3rd century BC, and, by the time of the Song dynasty (960–1279 AD), it was widespread in some provinces. Belief in transmigration allowed poor residents of the country to kill their newborn children if they felt unable to care for them, hoping that they would be reborn in better circumstances. Furthermore, some Chinese did not consider newborn children fully "human" and saw "life" beginning at some point after the sixth month after birth.

Contemporary writers from the Song dynasty note that, in Hubei and Fujian provinces, residents would only keep three sons and two daughters (among poor farmers, two sons, and one daughter), and kill all babies beyond that number at birth. Initially the sex of the child was only one factor to consider. By the time of the Ming Dynasty, however (1368–1644), male infanticide was becoming increasingly uncommon. The prevalence of female infanticide remained high much longer. The magnitude of this practice is subject to some dispute; however, one commonly quoted estimate is that, by late Qing, between one fifth and one-quarter of all newborn girls, across the entire social spectrum, were victims of infanticide. If one includes excess mortality among female children under 10 (ascribed to gender-differential neglect), the share of victims rises to one third.

Scottish Physician John Dudgeon, who worked in Beijing, China, during the Qing Dynasty said that in China, "Infanticide does not prevail to the extent so generally believed among us, and in the north, it does not exist at all."
Gender-selected abortion or sex identification (without medical uses), abandonment, and infanticide are illegal in present-day Mainland China. Nevertheless, the US State Department, and the human rights organization Amnesty International have all declared that Mainland China's family planning programs, called the one child policy (which has since changed to a two-child policy), contribute to infanticide. The sex gap between males and females aged 0–19 years old was estimated to be 25 million in 2010 by the United Nations Population Fund. But in some cases, in order to avoid Mainland China's family planning programs, parents will not report to government when a child is born (in most cases a girl), so she or he will not have an identity in the government and they can keep on giving birth until they are satisfied, without fines or punishment. In 2017, the government announced that all children without an identity can now have an identity legally, known as family register.

Since feudal Edo era Japan the common slang for infanticide was ""mabiki"" (間引き) which means to pull plants from an overcrowded garden. A typical method in Japan was smothering through wet paper on the baby's mouth and nose. It became common as a method of population control. Farmers would often kill their second or third sons. Daughters were usually spared, as they could be married off, sold off as servants or prostitutes, or sent off to become geishas. Mabiki persisted in the 19th century and early 20th century. To bear twins was perceived as barbarous and unlucky and efforts were made to hide or kill one or both twins.

Female infanticide of newborn girls was systematic in feudatory Rajputs in South Asia for illegitimate female children during the Middle Ages. According to Firishta, as soon as the illegitimate female child was born she was held "in one hand, and a knife in the other, that any person who wanted a wife might take her now, otherwise she was immediately put to death". The practice of female infanticide was also common among the Kutch, Kehtri, Nagar, Bengal, Miazed, Kalowries in India inhabitants, and also among the Sindh in British India.

It was not uncommon that parents threw a child to the sharks in the Ganges River as a sacrificial offering. The British colonists were unable to outlaw the custom until the beginnings of the 19th century.

According to social activists, female infanticide has remained a problem in India into the 21st century, with both NGOs and the government conducting awareness campaigns to combat it.
In India female infanticide is more common than the killing of male offspring, due to sex-selective infanticide.

In some African societies some neonates were killed because of beliefs in evil omens or because they were considered unlucky. Twins were usually put to death in Arebo; as well as by the Nama people of South West Africa; in the Lake Victoria Nyanza region; by the Tswana in Portuguese East Africa; in some parts of Igboland, Nigeria twins were sometimes abandoned in a forest at birth (as depicted in "Things Fall Apart"), oftentimes one twin was killed or hidden by midwives of wealthier mothers; and by the !Kung people of the Kalahari Desert. The Kikuyu, Kenya's most populous ethnic group, practiced ritual killing of twins.

Infanticide is rooted in the old traditions and beliefs prevailing all over the country. A survey conducted by Disability Rights International found that 45% of women interviewed by them in Kenya were pressured to kill their children born with disabilities. The pressure is much higher in the rural areas, with every second mother being forced out of three.

Literature suggests infanticide may have occurred reasonably commonly among Indigenous Australians, in all areas of Australia prior to European settlement. Infanticide may have continued to occur quite often up until the 1960s. An 1866 issue of "The Australian News for Home Readers" informed readers that "the crime of infanticide is so prevalent amongst the natives that it is rare to see an infant".

Author Susanna de Vries in 2007 told a newspaper that her accounts of Aboriginal violence, including infanticide, were censored by publishers in the 1980s and 1990s. She told reporters that the censorship "stemmed from guilt over the stolen children question". Keith Windschuttle weighed in on the conversation, saying this type of censorship started in the 1970s. In the same article Louis Nowra suggested that infanticide in customary Aboriginal law may have been because it was difficult to keep an abundant number of Aboriginal children alive; there were life-and-death decisions modern-day Australians no longer have to face.

According to William D. Rubinstein, "Nineteenth-century European observers of Aboriginal life in South Australia and Victoria reported that about 30% of Aboriginal infants were killed at birth."

James Dawson wrote a passage about infanticide among Indigenous people in the western district of Victoria, which stated that "Twins are as common among them as among Europeans; but as food is occasionally very scarce, and a large family troublesome to move about, it is lawful and customary to destroy the weakest twin child, irrespective of sex.
It is usual also to destroy those which are malformed."

He also wrote "When a woman has children too rapidly for the convenience and necessities of the parents, she makes up her mind to let one be killed, and consults with her husband which it is to be. As the strength of a tribe depends more on males than females, the girls are generally sacrificed.
The child is put to death and buried, or burned without ceremony; not, however, by its father or mother, but by relatives. No one wears mourning for it.
Sickly children are never killed on account of their bad health, and are allowed to die naturally."

In 1937, a reverend in the Kimberley offered a "baby bonus" to Aboriginal families as a deterrent against infanticide and to increase the birthrate of the local Indigenous population.

A Canberran journalist in 1927 wrote of the "cheapness of life" to the Aboriginal people local to the Canberra area 100 years before. "If drought or bush fires had devastated the country and curtailed food supplies, babies got a short shift. Ailing babies, too would not be kept" he wrote.

A bishop wrote in 1928 that it was common for Aboriginal Australians to restrict the size of their tribal groups, including by infanticide, so that the food resources of the tribal area may be sufficient for them.

Annette Hamilton, a professor of anthropology at Macquarie University who carried out research in the Aboriginal community of Maningrida in Arnhem Land during the 1960s wrote that prior to that time part-European babies born to Aboriginal mothers had not been allowed to live, and that 'mixed-unions are frowned on by men and women alike as a matter of principle'.

There is no agreement about the actual estimates of the frequency of newborn female infanticide in the Inuit population. Carmel Schrire mentions diverse studies ranging from 15–50% to 80%.

Polar Inuit (Inughuit) killed the child by throwing him or her into the sea. There is even a legend in Inuit mythology, "The Unwanted Child", where a mother throws her child into the fjord.

The Yukon and the Mahlemuit tribes of Alaska exposed the female newborns by first stuffing their mouths with grass before leaving them to die. In Arctic Canada the Inuit exposed their babies on the ice and left them to die.

Female Inuit infanticide disappeared in the 1930s and 1940s after contact with the Western cultures from the South.

The "Handbook of North American Indians" reports infanticide among the Dene Natives and those of the Mackenzie Mountains.

In the Eastern Shoshone there was a scarcity of Indian women as a result of female infanticide. For the Maidu Native Americans twins were so dangerous that they not only killed them, but the mother as well. In the region known today as southern Texas, the Mariame Indians practiced infanticide of females on a large scale. Wives had to be obtained from neighboring groups.

Bernal Díaz recounted that, after landing on the Veracruz coast, they came across a temple dedicated to Tezcatlipoca. "That day they had sacrificed two boys, cutting open their chests and offering their blood and hearts to that accursed idol". In "The Conquest of New Spain" Díaz describes more child sacrifices in the towns before the Spaniards reached the large Aztec city Tenochtitlan.

Although academic data of infanticides among the indigenous people in South America is not as abundant as that of North America, the estimates seem to be similar.

The Tapirapé indigenous people of Brazil allowed no more than three children per woman, and no more than two of the same sex. If the rule was broken infanticide was practiced. The Bororo killed all the newborns that did not appear healthy enough. Infanticide is also documented in the case of the Korubo people in the Amazon.

The Yanomami men killed children while raiding enemy villages. Helena Valero, a Brazilian woman kidnapped by Yanomami warriors in the 1930s, witnessed a Karawetari raid on her tribe:

While "qhapaq hucha" was practiced in the Peruvian large cities, child sacrifice in the pre-Columbian tribes of the region is less documented. However, even today studies on the Aymara Indians reveal high incidences of mortality among the newborn, especially female deaths, suggesting infanticide. The Abipones, a small tribe of Guaycuruan stock, of about 5,000 by the end of the 18th century in Paraguay, practiced systematic infanticide; with never more than two children being reared in one family. The Machigenga killed their disabled children. Infanticide among the Chaco in Paraguay was estimated as high as 50% of all newborns in that tribe, who were usually buried. The infanticidal custom had such roots among the Ayoreo in Bolivia and Paraguay that it persisted until the late 20th century.

Infanticide has become less common in the Western world. The frequency has been estimated to be 1 in approximately 3000 to 5000 children of all ages and 2.1 per 100,000 newborns per year. It is thought that infanticide today continues at a much higher rate in areas of extremely high poverty and overpopulation, such as parts of China and India. Female infants, then and even now, are particularly vulnerable, a factor in sex-selective infanticide. Recent estimates suggest that over 100 million girls and women are 'missing' in Asia.

In spite of the fact that it is illegal, in Benin, West Africa, parents secretly continue with infanticidal customs.

According to "The Hidden Gulag" published by the Committee for Human Rights in North Korea, Mainland China returns all illegal immigrants from North Korea which usually imprisons them in a short term facility. Korean women who are suspected of being impregnated by Chinese fathers are subjected to forced abortions; babies born alive are killed, sometimes by exposure or being buried alive.

There have been some accusations that infanticide occurs in Mainland China due to the one-child policy. In the 1990s, a certain stretch of the Yangtze River was known to be a common site of infanticide by drowning, until government projects made access to it more difficult. Recent studies suggest that over 40 million girls and women are 'missing' in Mainland China (Klasen and Wink 2003).

The practice has continued in some rural areas of India. Infanticide is illegal in India but still has the highest infanticide rate in the world.

According to a recent report by the United Nations Children's Fund (UNICEF) up to 50 million girls and women are missing in India's population as a result of systematic sex discrimination and sex selective abortions.

Killings of newborn babies have been on the rise in Pakistan, corresponding to an increase in poverty across the country. More than 1,000 infants, mostly girls, were killed or abandoned to die in Pakistan in 2009 according to a Pakistani charity organization.

The Edhi Foundation found 1,210 dead babies in 2010. Many more are abandoned and left at the doorsteps of mosques. As a result, Edhi centers feature signs "Do not murder, lay them here." Though female infanticide is punishable by life in prison, such crimes are rarely prosecuted.

In November 2008 it was reported that in Agibu and Amosa villages of Gimi region of Eastern Highlands province of Papua New Guinea where tribal fighting in the region of Gimi has been going on since 1986 (many of the clashes arising over claims of sorcery) women had agreed that if they stopped producing males, allowing only female babies to survive, their tribe's stock of boys would go down and there would be no men in the future to fight. They agreed to have all newborn male babies killed. It is not known how many male babies were killed by being smothered, but it had reportedly happened to all males over a 10-year period and probably was still happening.

In England and Wales there were typically 30 to 50 homicides per million children less than 1 year old between 1982 and 1996. The younger the infant, the higher the risk. The rate for children 1 to 5 years was around 10 per million children. The homicide rate of infants less than 1 year is significantly higher than for the general population.

In English law infanticide is established as a distinct offence by the Infanticide Acts. Defined as the killing of a child under 12 months of age by their mother, the effect of the Acts are to establish a partial defence to charges of murder.

In the United States the infanticide rate during the first hour of life outside the womb dropped from 1.41 per 100,000 during 1963 to 1972 to 0.44 per 100,000 for 1974 to 1983; the rates during the first month after birth also declined, whereas those for older infants rose during this time. The legalization of abortion, which was completed in 1973, was the most important factor in the decline in neonatal mortality during the period from 1964 to 1977, according to a study by economists associated with the National Bureau of Economic Research.

While legislation regarding infanticide in the majority of Western countries focuses on rehabilitation, believing that treatment and education will prevent repetitive action, the United States remains focused on delivering punishment. One justification for punishment is the difficulty of implementing rehabilitation services. With an overcrowded prison system, the United States can not provide the necessary treatment and services.

In Canada 114 cases of infanticide by a parent were reported during 1964–1968. There is ongoing debate in the Canadian legal and political fields about whether section 237 of the Criminal Code, which creates the specific offence and partial defence of infanticide in Canadian law, should be amended or abolished altogether.

From 2013 to March 2018, 28 infanticides cases done by 22 mothers and three stepmothers were reported in Spain. The most famous case was the murder of Bernardo González Parra in 1910 perpetrated by Francisco Leona Romero, Julio Hernández Rodríguez, Francisco Ortega el Moruno and Agustina Rodríguez.

There are various reasons for infanticide. Neonaticide typically has different patterns and causes than for the killing of older infants. Traditional neonaticide is often related to economic necessity - the inability to provide for the infant.

In the United Kingdom and the United States, older infants are typically killed for reasons related to child abuse, domestic violence or mental illness. For infants older than one day, younger infants are more at risk, and boys are more at risk than girls. Risk factors for the parent include: Family history of violence, violence in a current relationship, history of abuse or neglect of children, and personality disorder and/or depression.

In the late 17th and early 18th centuries, "loopholes" were invented by Protestants who wanted to avoid the damnation that was promised by most Christian doctrine as a penalty of suicide. One famous example of someone who wished to end their life but avoid the eternity in hell was Christina Johansdotter (died 1740). She was a Swedish murderer who killed a child in Stockholm with the sole purpose of being executed. She is an example of those who seek suicide through execution by committing a murder. It was a common act, frequently targeting young children or infants as they were believed to be free from sin, thus believing to go "straight to heaven".

On the contrary, most mainstream denominations view the murder of an innocent as being condemned in the Fifth Commandment. The Roman Catholic Congregation of the Doctrine of Faith, in Donum Vitæ, is instructive. "Human life is sacred because from its beginning it involves the creative action of God and it remains forever in a special relationship with the Creator, who is its sole end. God alone is the Lord of life from its beginning until its end: no one can under any circumstance claim for himself the right directly to destroy an innocent human being." 

In 1888, Lieut. F. Elton reported that Ugi beach people in the Solomon Islands killed their infants at birth by burying them, and women were also said to practice abortion. They reported that it was too much trouble to raise a child, and instead preferred to buy one from the bush people.

Many historians believe the reason to be primarily economic, with more children born than the family is prepared to support. In societies that are patrilineal and patrilocal, the family may choose to allow more sons to live and kill some daughters, as the former will support their birth family until they die, whereas the latter will leave economically and geographically to join their husband's family, possibly only after the payment of a burdensome dowry price. Thus the decision to bring up a boy is more economically rewarding to the parents. However, this does not explain why infanticide would occur equally among rich and poor, nor why it would be as frequent during decadent periods of the Roman Empire as during earlier, less affluent, periods.

Before the appearance of effective contraception, infanticide was a common occurrence in ancient brothels. Unlike usual infanticide - where historically girls have been more likely to be killed - prostitutes in certain areas preferred to kill their male offspring.

Instances of infanticide in Britain in 18th and 19th centuries is often attributed to the economic position of the women, with juries committing “pious perjury” in many subsequent murder cases. The knowledge of the difficulties faced in the 18th century by those women who attempted to keep their children can be seen as a reason for juries to show compassion. If the woman chose to keep the child, society was not set up to ease the pressure placed upon the woman, legally, socially or economically.

In mid-18th century Britain there was assistance available for women who were not able to raise their children. The Foundling Hospital opened in 1756 and was able to take in some of the illegitimate children. However, the conditions within the hospital caused Parliament to withdraw funding and the governors to live off of their own incomes. This resulted in a stringent entrance policy, with the committee requiring that the hospital:

Once a mother had admitted her child to the hospital, the hospital did all it could to ensure that the parent and child were not re-united.

MacFarlane argues in "Illegitimacy and Illegitimates in Britain" (1980) that English society greatly concerned itself with the burden that a bastard child places upon its communities and had gone to some lengths to ensure that the father of the child is identified in order to maintain its well-being. Assistance could be gained through maintenance payments from the father, however, this was capped "at a miserable 2 s and 6 d a week". If the father fell behind with the payments he could only be asked "to pay a maximum of 13 weeks arrears".

Despite the accusations of some that women were getting a free hand-out, there is evidence that many women were far from receiving adequate assistance from their parish. "Within Leeds in 1822 ... relief was limited to 1 s per week". Sheffield required women to enter the workhouse, whereas Halifax gave no relief to the women who required it. The prospect of entering the workhouse was certainly something to be avoided. Lionel Rose quotes Dr Joseph Rogers in "Massacre of the Innocents ..." (1986). Rogers, who was employed by a London workhouse in 1856 stated that conditions in the nursery were ‘wretchedly damp and miserable ... [and] ... overcrowded with young mothers and their infants’.

The loss of social standing for a servant girl was a particular problem in respect of producing a bastard child as they relied upon a good character reference in order to maintain their job and more importantly, to get a new or better job. In a large number of trials for the crime of infanticide, it is the servant girl that stood accused. The disadvantage of being a servant girl is that they had to live to the social standards of their superiors or risk dismissal and no references. Whereas within other professions, such as in the factory, the relationship between employer and employee was much more anonymous and the mother would be better able to make other provisions, such as employing a minder. The result of the lack of basic social care in Britain in the 18th and 19th century is the numerous accounts in court records of women, particularly servant girls, standing trial for the murder of their child.

There may have been no specific offense of infanticide in England before about 1623 because infanticide was a matter for the by ecclesiastical courts, possibly because infant mortality from natural causes was high (about 15% or one in six).

Thereafter the accusation of the suppression of bastard children by lewd mothers was a crime incurring the presumption of guilt.

The Infanticide Acts are several laws. That of 1922 made the killing of an infant child by its mother during the early months of life as a lesser crime than murder. The acts of 1938 and 1939 abolished the earlier act, but introduced the idea that postpartum depression was legally to be regarded as a form of diminished responsibility.

Marvin Harris estimated that among Paleolithic hunters 23–50% of newborn children were killed. He argued that the goal was to preserve the 0.001% population growth of that time. He also wrote that female infanticide may be a form of population control. Population control is achieved not only by limiting the number of potential mothers; increased fighting among men for access to relatively scarce wives would also lead to a decline in population. For example, on the Melanesian island of Tikopia infanticide was used to keep a stable population in line with its resource base. Research by Marvin Harris and William Divale supports this argument, it has been cited as an example of environmental determinism.

Evolutionary psychology has proposed several theories for different forms of infanticide. Infanticide by stepfathers, as well as child abuse in general by stepfathers, has been explained by spending resources on not genetically related children reducing reproductive success (See the Cinderella effect and Infanticide (zoology)). Infanticide is one of the few forms of violence more often done by women than men. Cross-cultural research has found that this is more likely to occur when the child has deformities or illnesses as well as when there are lacking resources due to factors such as poverty, other children requiring resources, and no male support. Such a child may have a low chance of reproductive success in which case it would decrease the mother's inclusive fitness, in particular since women generally have a greater parental investment than men, to spend resources on the child.

A minority of academics subscribe to an alternate school of thought, considering the practice as "early infanticidal childrearing". They attribute parental infanticidal wishes to massive projection or displacement of the parents' unconscious onto the child, because of intergenerational, ancestral abuse by their own parents. Clearly, an infanticidal parent may have multiple motivations, conflicts, emotions, and thoughts about their baby and their relationship with their baby, which are often colored both by their individual psychology, current relational context and attachment history, and, perhaps most saliently, their psychopathology (See also Psychiatric section below) Almeida, Merminod, and Schechter suggest that parents with fantasies, projections, and delusions involving infanticide need to be taken seriously and assessed carefully, whenever possible, by an interdisciplinary team that includes infant mental health specialists or mental health practitioners who have experience in working with parents, children, and families.

In addition to debates over the morality of infanticide itself, there is some debate over the effects of infanticide on surviving children, and the effects of childrearing in societies that also sanction infanticide. Some argue that the practice of infanticide in any widespread form causes enormous psychological damage in children. Conversely, studying societies that practice infanticide Géza Róheim reported that even infanticidal mothers in New Guinea, who ate a child, did not affect the personality development of the surviving children; that "these are good mothers who eat their own children". Harris and Divale's work on the relationship between female infanticide and warfare suggests that there are, however, extensive negative effects.

Postpartum psychosis is also a causative factor of infanticide. Stuart S. Asch, MD, a Professor of Psychiatry at Cornell University established the connections between some cases of infanticide and post-partum depression. The books, "From Cradle to Grave", and "The Death of Innocents", describe selected cases of maternal infanticide and the investigative research of Professor Asch working in concert with the New York City Medical Examiner's Office.
Stanley Hopwood wrote that childbirth and lactation entail severe stress on the female sex, and that under certain circumstances attempts at infanticide and suicide are common. A study published in the "American Journal of Psychiatry" revealed that 44% of filicidal fathers had a diagnosis of psychosis. In addition to postpartum psychosis, dissociative psychopathology and sociopathy have also been found to be associated with neonaticide in some cases

In addition, severe postpartum depression can lead to infanticide.

Sex selection may be one of the contributing factors of infanticide. In the absence of sex-selective abortion, sex-selective infanticide can be deduced from very skewed birth statistics. The biologically normal sex ratio for humans at birth is approximately 105 males per 100 females; normal ratios hardly ranging beyond 102–108. When a society has an infant male to female ratio which is significantly higher or lower than the biological norm, and biased data can be ruled out, sex selection can usually be inferred.

In New South Wales, infanticide is defined in Section 22A(1) of the Crimes Act 1900 (NSW) as follows:

Because Infanticide is punishable as manslaughter, as per s24, the maximum penalty for this offence is therefore 25 years imprisonment.

In Victoria, infanticide is defined by Section 6 of the Crimes Act of 1958 with a maximum penalty of five years .

In Canada, a mother commits infanticide, a lesser offense than homicide, if she killed her child while "not fully recovered from the effects of giving birth to the child and by reason thereof or of the effect of lactation consequent on the birth of the child her mind is then disturbed".

In England and Wales, the Infanticide Act 1938 describes the offense of infanticide as one which would otherwise amount to murder (by his/her mother) if the victim was older than 12 months and the mother was not suffering from an imbalance of mind due to the effects of childbirth or lactation. Where a mother who has killed such an infant has been charged with murder rather than infanticide s.1(3) of the Act confirms that a jury has the power to find alternative verdicts of Manslaughter in English law or guilty but insane.

Infanticide is illegal in the Netherlands, although the maximum sentence is lower than for homicide. The Groningen Protocol regulates euthanasia for infants who are believed to "suffer hopelessly and unbearably" under strict conditions.

Article 200 of the Penal Code of Romania stipulates that the killing of a newborn during the first 24 hours, by the mother who is in a state of mental distress, shall be punished with imprisonment of one to five years. The previous Romanian Penal Code also defined infanticide ("pruncucidere") as a distinct criminal offense, providing for punishment of two to seven years imprisonment, recognizing the fact that a mother's judgment may be impaired immediately after birth but did not define the term "infant", and this had led to debates regarding the precise moment when infanticide becomes homicide. This issue was resolved by the new Penal Code, which came into force in 2014.

In 2009, Texas state representative Jessica Farrar proposed legislation that would define infanticide as a distinct and lesser crime than homicide. Under the terms of the proposed legislation, if jurors concluded that a mother's "judgment was impaired as a result of the effects of giving birth or the effects of lactation following the birth", they would be allowed to convict her of the crime of infanticide, rather than murder. The maximum penalty for infanticide would be two years in prison. Farrar's introduction of this bill prompted liberal bioethics scholar Jacob M. Appel to call her "the bravest politician in America".

The "MOTHERS Act" (Moms Opportunity To access Health, Education, Research and Support), precipitated by the death of a Chicago woman with postpartum psychosis was introduced in 2009. The act was ultimately incorporated into the "Patient Protection and Affordable Care Act" which passed in 2010. The act requires screening for postpartum mood disorders at any time of the adult lifespan as well as expands research on postpartum depression. Provisions of the act also authorize grants to support clinical services for women who have, or are at risk for, postpartum psychosis.

Since infanticide, especially neonaticide, is often a response to an unwanted birth, preventing unwanted pregnancies through improved sex education and increased contraceptive access are advocated as ways of preventing infanticide. Increased use of contraceptives and access to safe legal abortions have greatly reduced neonaticide in many developed nations. Some say that where abortion is illegal, as in Pakistan, infanticide would decline if safer legal abortions were available.

Cases of infanticide have also garnered increasing attention and interest from advocates for the mentally ill as well as organizations dedicated to postpartum disorders. Following the trial of Andrea Yates, a mother from the United States who garnered national attention for drowning her 5 children, representatives from organizations such as the Postpartum Support International and the Marcé Society for Treatment and Prevention of Postpartum Disorders began requesting clarification of diagnostic criteria for postpartum disorders and improved guidelines for treatments. While accounts of postpartum psychosis have dated back over 2,000 years ago, perinatal mental illness is still largely under-diagnosed despite postpartum psychosis affecting 1 to 2 per 1000 women. However, with clinical research continuing to demonstrate the large role of rapid neurochemical fluctuation in postpartum psychosis, prevention of infanticide points ever strongly towards psychiatric intervention.

Screening for psychiatric disorders or risk factors, and providing treatment or assistance to those at risk may help prevent infanticide. Current diagnostic considerations include symptoms, psychological history, thoughts of self-harm or harming one's children, physical and neurological examination, laboratory testing, substance abuse, and brain imaging. As psychotic symptoms may fluctuate, it is important that diagnostic assessments cover a wide range of factors.

While studies on the treatment of postpartum psychosis are scarce, a number of case and cohort studies have found evidence describing the effectiveness of lithium monotherapy for both acute and maintenance treatment of postpartum psychosis, with the majority of patients achieving complete remission. Adjunctive treatments include electroconvulsive therapy, antipsychotic medication, or benzodiazepines. Electroconvulsive therapy, in particular, is the primary treatment for patients with catatonia, severe agitation, and difficulties eating or drinking. Antidepressants should be avoided throughout the acute treatment of postpartum psychosis due to risk of worsening mood instability.

Though screening and treatment may help prevent infanticide, in the developed world, significant proportions of neonaticides that are detected occur in young women who deny their pregnancy and avoid outside contacts, many of who may have limited contact with these health care services.

In some areas baby hatches or "safe surrender sites", safe places for a mother to anonymously leave an infant, are offered, in part to reduce the rate of infanticide. In other places, like the United States, safe-haven laws allow mothers to anonymously give infants to designated officials; they are frequently located at hospitals and police and fire stations. Additionally, some countries in Europe have the laws of anonymous birth and confidential birth that allow mothers to give up an infant after birth. In anonymous birth, the mother does not attach her name to the birth certificate. In confidential birth, the mother registers her name and information, but the document containing her name is sealed until the child comes to age. Typically such babies are put up for adoption, or cared for in orphanages.

Granting women employment raises their status and autonomy. Having a gainful employment can raise the perceived worth of females. This can lead to an increase in the number of women getting an education and a decrease in the number of female infanticide. As a result, the infant mortality rate will decrease and economic development will increase.

The practice has been observed in many other species of the animal kingdom since it was first seriously studied by . These include from microscopic rotifers and insects, to fish, amphibians, birds and mammals, including primates such as chacma baboons.

According to studies carried out by Kyoto University in primates, including certain types of gorillas and chimpanzees, several conditions favor the tendency to kill their offspring in some species (to be performed only by males), among them are: Nocturnal life, the absence of nest construction, the marked sexual dimorphism in which the male is much larger than the female, the mating in a specific season and the high period of lactation without resumption of the estrus state in the female.



</doc>
<doc id="15476" url="https://en.wikipedia.org/wiki?curid=15476" title="Internet protocol suite">
Internet protocol suite

The Internet protocol suite is the conceptual model and set of communications protocols used in the Internet and similar computer networks. It is commonly known as TCP/IP because the foundational protocols in the suite are the Transmission Control Protocol (TCP) and the Internet Protocol (IP). During its development, versions of it were known as the Department of Defense (DoD) model because the development of the networking method was funded by the United States Department of Defense through DARPA. Its implementation is a protocol stack.

The Internet protocol suite provides end-to-end data communication specifying how data should be packetized, addressed, transmitted, routed, and received. This functionality is organized into four abstraction layers, which classify all related protocols according to the scope of networking involved. From lowest to highest, the layers are the link layer, containing communication methods for data that remains within a single network segment (link); the internet layer, providing internetworking between independent networks; the transport layer, handling host-to-host communication; and the application layer, providing process-to-process data exchange for applications.

The technical standards underlying the Internet protocol suite and its constituent protocols are maintained by the Internet Engineering Task Force (IETF). The Internet protocol suite predates the OSI model, a more comprehensive reference framework for general networking systems.

The Internet protocol suite resulted from research and development conducted by the Defense Advanced Research Projects Agency (DARPA) in the late 1960s. After initiating the pioneering ARPANET in 1969, DARPA started work on a number of other data transmission technologies. In 1972, Robert E. Kahn joined the DARPA Information Processing Technology Office, where he worked on both satellite packet networks and ground-based radio packet networks, and recognized the value of being able to communicate across both. In the spring of 1973, Vinton Cerf, who helped develop the existing ARPANET Network Control Program (NCP) protocol, joined Kahn to work on open-architecture interconnection models with the goal of designing the next protocol generation for the ARPANET. They drew on the experience from the ARPANET research community and the International Networking Working Group, which Cerf chaired.

By the summer of 1973, Kahn and Cerf had worked out a fundamental reformulation, in which the differences between local network protocols were hidden by using a common internetwork protocol, and, instead of the network being responsible for reliability, as in the existing ARPANET protocols, this function was delegated to the hosts. Cerf credits Hubert Zimmermann and Louis Pouzin, designer of the CYCLADES network, with important influences on this design. The new protocol was implemented as the Transmission Control Program in 1974.

Initially, the Transmission Control Program managed both datagram transmissions and routing, but as experience with the protocol grew, collaborators recommended division of functionality into layers of distinct protocols. Advocates included Jonathan Postel of the University of Southern California's Information Sciences Institute, who edited the Request for Comments (RFCs), the technical and strategic document series that has both documented and catalyzed Internet development, and the research group of Robert Metcalfe at Xerox PARC. Postel stated, "We are screwing up in our design of Internet protocols by violating the principle of layering." Encapsulation of different mechanisms was intended to create an environment where the upper layers could access only what was needed from the lower layers. A monolithic design would be inflexible and lead to scalability issues. In version 3 of TCP, written in 1978, the Transmission Control Program was split into two distinct protocols, the Internet Protocol as connectionless layer and the Transmission Control Protocol as a reliable connection-oriented service.

The design of the network included the recognition that it should provide only the functions of efficiently transmitting and routing traffic between end nodes and that all other intelligence should be located at the edge of the network, in the end nodes. This design is known as the end-to-end principle. Using this design, it became possible to connect other networks to the ARPANET that used the same principle, irrespective of other local characteristics, thereby solving Kahn's initial internetworking problem. A popular expression is that TCP/IP, the eventual product of Cerf and Kahn's work, can run over "two tin cans and a string." Years later, as a joke, the IP over Avian Carriers formal protocol specification was created and successfully tested.

DARPA contracted with BBN Technologies, Stanford University, and the University College London to develop operational versions of the protocol on several hardware platforms. During development of the protocol the version number of the packet routing layer progressed from version 1 to version 4, the latter of which was installed in the ARPANET in 1983. It became known as "Internet Protocol version 4" (IPv4) as the protocol that is still in use in the Internet, alongside its current successor, Internet Protocol version 6 (IPv6).

In 1975, a two-network TCP/IP communications test was performed between Stanford and University College London. In November 1977, a three-network TCP/IP test was conducted between sites in the US, the UK, and Norway. Several other TCP/IP prototypes were developed at multiple research centers between 1978 and 1983.
A computer called a router is provided with an interface to each network. It forwards network packets back and forth between them. Originally a router was called "gateway", but the term was changed to avoid confusion with other types of gateways.

In March 1982, the US Department of Defense declared TCP/IP as the standard for all military computer networking. In the same year, NORSAR and Peter Kirstein's research group at University College London adopted the protocol. The migration of the ARPANET to TCP/IP was officially completed on flag day January 1, 1983, when the new protocols were permanently activated.

In 1985, the Internet Advisory Board (later Internet Architecture Board) held a three-day TCP/IP workshop for the computer industry, attended by 250 vendor representatives, promoting the protocol and leading to its increasing commercial use. In 1985, the first Interop conference focused on network interoperability by broader adoption of TCP/IP. The conference was founded by Dan Lynch, an early Internet activist. From the beginning, large corporations, such as IBM and DEC, attended the meeting.

IBM, AT&T and DEC were the first major corporations to adopt TCP/IP, this despite having competing proprietary protocols. In IBM, from 1984, Barry Appelman's group did TCP/IP development. They navigated the corporate politics to get a stream of TCP/IP products for various IBM systems, including MVS, VM, and OS/2. At the same time, several smaller companies, such as FTP Software and the Wollongong Group, began offering TCP/IP stacks for DOS and Microsoft Windows. The first VM/CMS TCP/IP stack came from the University of Wisconsin.

Some of the early TCP/IP stacks were written single-handedly by a few programmers. Jay Elinsky and of IBM Research wrote TCP/IP stacks for VM/CMS and OS/2, respectively. In 1984 Donald Gillies at MIT wrote a "ntcp" multi-connection TCP which ran atop the IP/PacketDriver layer maintained by John Romkey at MIT in 1983–4. Romkey leveraged this TCP in 1986 when FTP Software was founded. Starting in 1985, Phil Karn created a multi-connection TCP application for ham radio systems (KA9Q TCP).

The spread of TCP/IP was fueled further in June 1989, when the University of California, Berkeley agreed to place the TCP/IP code developed for BSD UNIX into the public domain. Various corporate vendors, including IBM, included this code in commercial TCP/IP software releases. Microsoft released a native TCP/IP stack in Windows 95. This event helped cement TCP/IP's dominance over other protocols on Microsoft-based networks, which included IBM's Systems Network Architecture (SNA), and on other platforms such as Digital Equipment Corporation's DECnet, Open Systems Interconnection (OSI), and Xerox Network Systems (XNS).

Nonetheless, for a period in the late 1980s and early 1990s, engineers, organizations and nations were polarized over the issue of which standard, the OSI model or the Internet protocol suite would result in the best and most robust computer networks.

The technical standards underlying the Internet protocol suite and its constituent protocols have been delegated to the Internet Engineering Task Force (IETF).

The characteristic architecture of the Internet Protocol Suite is its broad division into operating scopes for the protocols that constitute its core functionality. The defining specification of the suite is RFC 1122, which broadly outlines four abstraction layers. These have stood the test of time, as the IETF has never modified this structure. As such a model of networking, the Internet Protocol Suite predates the OSI model, a more comprehensive reference framework for general networking systems.

The end-to-end principle has evolved over time. Its original expression put the maintenance of state and overall intelligence at the edges, and assumed the Internet that connected the edges retained no state and concentrated on speed and simplicity. Real-world needs for firewalls, network address translators, web content caches and the like have forced changes in this principle.

The robustness principle states: "In general, an implementation must be conservative in its sending behavior, and liberal in its receiving behavior. That is, it must be careful to send well-formed datagrams, but must accept any datagram that it can interpret (e.g., not object to technical errors where the meaning is still clear)." "The second part of the principle is almost as important: software on other hosts may contain deficiencies that make it unwise to exploit legal but obscure protocol features."

Encapsulation is used to provide abstraction of protocols and services. Encapsulation is usually aligned with the division of the protocol suite into layers of general functionality. In general, an application (the highest level of the model) uses a set of protocols to send its data down the layers. The data is further encapsulated at each level.

An early architectural document, , emphasizes architectural principles over layering. RFC 1122, titled "Host Requirements", is structured in paragraphs referring to layers, but the document refers to many other architectural principles and does not emphasize layering. It loosely defines a four-layer model, with the layers having names, not numbers, as follows:

The protocols of the link layer operate within the scope of the local network connection to which a host is attached. This regime is called the "link" in TCP/IP parlance and is the lowest component layer of the suite. The link includes all hosts accessible without traversing a router. The size of the link is therefore determined by the networking hardware design. In principle, TCP/IP is designed to be hardware independent and may be implemented on top of virtually any link-layer technology. This includes not only hardware implementations, but also virtual link layers such as virtual private networks and networking tunnels.

The link layer is used to move packets between the Internet layer interfaces of two different hosts on the same link. The processes of transmitting and receiving packets on the link can be controlled in the device driver for the network card, as well as in firmware or by specialized chipsets. These perform functions, such as framing, to prepare the Internet layer packets for transmission, and finally transmit the frames to the physical layer and over a transmission medium. The TCP/IP model includes specifications for translating the network addressing methods used in the Internet Protocol to link-layer addresses, such as media access control (MAC) addresses. All other aspects below that level, however, are implicitly assumed to exist, and are not explicitly defined in the TCP/IP model.

The link layer in the TCP/IP model has corresponding functions in Layer 2 of the OSI model.

Internetworking requires sending data from the source network to the destination network. This process is called routing and is supported by host addressing and identification using the hierarchical IP addressing system. The internet layer provides an unreliable datagram transmission facility between hosts located on potentially different IP networks by forwarding datagrams to an appropriate next-hop router for further relaying to its destination. The internet layer has the responsibility of sending packets across potentially multiple networks. With this functionality, the internet layer makes possible internetworking, the interworking of different IP networks, and it essentially establishes the Internet. 

The internet layer does not distinguish between the various transport layer protocols. IP carries data for a variety of different upper layer protocols. These protocols are each identified by a unique protocol number: for example, Internet Control Message Protocol (ICMP) and Internet Group Management Protocol (IGMP) are protocols 1 and 2, respectively.

The Internet Protocol is the principal component of the internet layer, and it defines two addressing systems to identify network hosts and to locate them on the network. The original address system of the ARPANET and its successor, the Internet, is Internet Protocol version 4 (IPv4). It uses a 32-bit IP address and is therefore capable of identifying approximately four billion hosts. This limitation was eliminated in 1998 by the standardization of Internet Protocol version 6 (IPv6) which uses 128-bit addresses. IPv6 production implementations emerged in approximately 2006.

The transport layer establishes basic data channels that applications use for task-specific data exchange. The layer establishes host-to-host connectivity in the form of end-to-end message transfer services that are independent of the underlying network and independent of the structure of user data and the logistics of exchanging information. Connectivity at the transport layer can be categorized as either connection-oriented, implemented in TCP, or connectionless, implemented in UDP. The protocols in this layer may provide error control, segmentation, flow control, congestion control, and application addressing (port numbers).
For the purpose of providing process-specific transmission channels for applications, the layer establishes the concept of the network port. This is a numbered logical construct allocated specifically for each of the communication channels an application needs. For many types of services, these port numbers have been standardized so that client computers may address specific services of a server computer without the involvement of service announcements or directory services.

Because IP provides only a best effort delivery, some transport layer protocols offer reliability. However, IP can run over a reliable data link protocol such as the High-Level Data Link Control (HDLC).

For example, the TCP is a connection-oriented protocol that addresses numerous reliability issues in providing a reliable byte stream:

The newer Stream Control Transmission Protocol (SCTP) is also a reliable, connection-oriented transport mechanism. It is message-stream-oriented—not byte-stream-oriented like TCP—and provides multiple streams multiplexed over a single connection. It also provides multi-homing support, in which a connection end can be represented by multiple IP addresses (representing multiple physical interfaces), such that if one fails, the connection is not interrupted. It was developed initially for telephony applications (to transport SS7 over IP), but can also be used for other applications.

The User Datagram Protocol is a connectionless datagram protocol. Like IP, it is a best effort, "unreliable" protocol. Reliability is addressed through error detection using a weak checksum algorithm. UDP is typically used for applications such as streaming media (audio, video, Voice over IP etc.) where on-time arrival is more important than reliability, or for simple query/response applications like DNS lookups, where the overhead of setting up a reliable connection is disproportionately large. Real-time Transport Protocol (RTP) is a datagram protocol that is designed for real-time data such as streaming audio and video.

The applications at any given network address are distinguished by their TCP or UDP port. By convention certain "well known ports" are associated with specific applications.

The TCP/IP model's transport or host-to-host layer corresponds roughly to the fourth layer in the OSI model, also called the transport layer.

The application layer includes the protocols used by most applications for providing user services or exchanging application data over the network connections established by the lower level protocols. This may include some basic network support services such as protocols for routing and host configuration. Examples of application layer protocols include the Hypertext Transfer Protocol (HTTP), the File Transfer Protocol (FTP), the Simple Mail Transfer Protocol (SMTP), and the Dynamic Host Configuration Protocol (DHCP). Data coded according to application layer protocols are encapsulated into transport layer protocol units (such as TCP or UDP messages), which in turn use lower layer protocols to effect actual data transfer.

The TCP/IP model does not consider the specifics of formatting and presenting data, and does not define additional layers between the application and transport layers as in the OSI model (presentation and session layers). Such functions are the realm of libraries and application programming interfaces.

Application layer protocols generally treat the transport layer (and lower) protocols as black boxes which provide a stable network connection across which to communicate, although the applications are usually aware of key qualities of the transport layer connection such as the end point IP addresses and port numbers. Application layer protocols are often associated with particular client-server applications, and common services have "well-known" port numbers reserved by the Internet Assigned Numbers Authority (IANA). For example, the HyperText Transfer Protocol uses server port 80 and Telnet uses server port 23. Clients connecting to a service usually use ephemeral ports, i.e., port numbers assigned only for the duration of the transaction at random or from a specific range configured in the application.

The transport layer and lower-level layers are unconcerned with the specifics of application layer protocols. Routers and switches do not typically examine the encapsulated traffic, rather they just provide a conduit for it. However, some firewall and bandwidth throttling applications must interpret application data. An example is the Resource Reservation Protocol (RSVP). It is also sometimes necessary for network address translator (NAT) traversal to consider the application payload.

The application layer in the TCP/IP model is often compared as equivalent to a combination of the fifth (Session), sixth (Presentation), and the seventh (Application) layers of the OSI model.

Furthermore, the TCP/IP model distinguishes between "user protocols" and "support protocols". Support protocols provide services to a system of network infrastructure. User protocols are used for actual user applications. For example, FTP is a user protocol and DNS is a support protocol.

The following table shows various networking models. The number of layers varies between three and seven.

Some of the networking models are from textbooks, which are secondary sources that may conflict with the intent of RFC 1122 and other IETF primary sources.

The three top layers in the OSI model, i.e. the application layer, the presentation layer and the session layer, are not distinguished separately in the TCP/IP model which only has an application layer above the transport layer. While some pure OSI protocol applications, such as X.400, also combined them, there is no requirement that a TCP/IP protocol stack must impose monolithic architecture above the transport layer. For example, the NFS application protocol runs over the eXternal Data Representation (XDR) presentation protocol, which, in turn, runs over a protocol called Remote Procedure Call (RPC). RPC provides reliable record transmission, so it can safely use the best-effort UDP transport.

Different authors have interpreted the TCP/IP model differently, and disagree whether the link layer, or the entire TCP/IP model, covers OSI layer 1 (physical layer) issues, or whether a hardware layer is assumed below the link layer.

Several authors have attempted to incorporate the OSI model's layers 1 and 2 into the TCP/IP model, since these are commonly referred to in modern standards (for example, by IEEE and ITU). This often results in a model with five layers, where the link layer or network access layer is split into the OSI model's layers 1 and 2.

The IETF protocol development effort is not concerned with strict layering. Some of its protocols may not fit cleanly into the OSI model, although RFCs sometimes refer to it and often use the old OSI layer numbers. The IETF has repeatedly stated that Internet protocol and architecture development is not intended to be OSI-compliant. RFC 3439, referring to the Internet architecture, contains a section entitled: "Layering Considered Harmful".

For example, the session and presentation layers of the OSI suite are considered to be included to the application layer of the TCP/IP suite. The functionality of the session layer can be found in protocols like HTTP and SMTP and is more evident in protocols like Telnet and the Session Initiation Protocol (SIP). Session layer functionality is also realized with the port numbering of the TCP and UDP protocols, which cover the transport layer in the TCP/IP suite. Functions of the presentation layer are realized in the TCP/IP applications with the MIME standard in data exchange.

Conflicts are apparent also in the original OSI model, ISO 7498, when not considering the annexes to this model, e.g., the ISO 7498/4 Management Framework, or the ISO 8648 Internal Organization of the Network layer (IONL). When the IONL and Management Framework documents are considered, the ICMP and IGMP are defined as layer management protocols for the network layer. In like manner, the IONL provides a structure for "subnetwork dependent convergence facilities" such as ARP and RARP.

IETF protocols can be encapsulated recursively, as demonstrated by tunneling protocols such as Generic Routing Encapsulation (GRE). GRE uses the same mechanism that OSI uses for tunneling at the network layer.

The Internet protocol suite does not presume any specific hardware or software environment. It only requires that hardware and a software layer exists that is capable of sending and receiving packets on a computer network. As a result, the suite has been implemented on essentially every computing platform. A minimal implementation of TCP/IP includes the following: Internet Protocol (IP), Address Resolution Protocol (ARP), Internet Control Message Protocol (ICMP), Transmission Control Protocol (TCP), User Datagram Protocol (UDP), and Internet Group Management Protocol (IGMP). In addition to IP, ICMP, TCP, UDP, Internet Protocol version 6 requires Neighbor Discovery Protocol (NDP), ICMPv6, and IGMPv6 and is often accompanied by an integrated IPSec security layer.

Application programmers are typically concerned only with interfaces in the application layer and often also in the transport layer, while the layers below are services provided by the TCP/IP stack in the operating system. Most IP implementations are accessible to programmers through sockets and APIs.

Unique implementations include Lightweight TCP/IP, an open source stack designed for embedded systems, and KA9Q NOS, a stack and associated protocols for amateur packet radio systems and personal computers connected via serial lines.

Microcontroller firmware in the network adapter typically handles link issues, supported by driver software in the operating system. Non-programmable analog and digital electronics are normally in charge of the physical components below the link layer, typically using an application-specific integrated circuit (ASIC) chipset for each network interface or other physical standard. High-performance routers are to a large extent based on fast non-programmable digital electronics, carrying out link level switching.





</doc>
<doc id="15477" url="https://en.wikipedia.org/wiki?curid=15477" title="Ibn al-Shaykh al-Libi">
Ibn al-Shaykh al-Libi

Ibn al-Shaykh al-Libi (; "Ḁbnʋ ălŞɑỉƈ alLibi"; born Ali Mohamed Abdul Aziz al-Fakheri, 1963 – May 10, 2009) was a Libyan national captured in Afghanistan in November 2001 after the fall of the Taliban; he was interrogated by the American and Egyptian forces. The information he gave under torture to Egyptian authorities was cited by the George W. Bush Administration in the months preceding its 2003 invasion of Iraq as evidence of a connection between Saddam Hussein and al-Qaeda. That information was frequently repeated by members of the Bush Administration, although reports from both the Central Intelligence Agency (CIA) and the Defense Intelligence Agency (DIA) strongly questioned its credibility, suggesting that al-Libi was "intentionally misleading" interrogators.

In 2006, the United States transferred al-Libi to Libya, where he was imprisoned by the government. He was reported to have tuberculosis. On May 19, 2009, the government reported that he had recently committed suicide in prison. Human Rights Watch, whose representatives had recently visited him, called for an investigation into the circumstances of his death; "The New York Times" reported that Ayman al-Zawahiri had asserted that Libya had tortured al-Libi to death.

In Afghanistan, al-Libi led the Al Khaldan training camp, where Zacarias Moussaoui and Ahmed Ressam trained for attacks in the United States. An associate of Abu Zubaydah, al-Libi had his assets frozen by the U.S. government following the September 11 attacks; on September 26, 2002, the U.S. government published a list of terrorists who were covered by this restriction.

The Uyghur Turkistan Islamic Party's "Islamic Turkistan" magazine in its 5th edition published an obituary of its member Turghun (Ibn Umar al Turkistani) speaking of his time training at the Al Khaldan training camp and his meeting with Ibn al-Shaykh al-Libi. The Uyghurs in Afghanistan fought against the American bombing and the Northern Alliance after the September 11, 2001, attacks. Ibn Umar died fighting against Americans at the Qalai Jangi prison riot.

Al-Libi was captured by Pakistani officials in November 2001, as he attempted to flee Afghanistan following the collapse of the Taliban after the 2001 U.S. invasion of Afghanistan, and was transferred to the US military in January 2002.

Department of Defense spokesmen used to routinely describe the Khaldan training camp as an al-Qaeda training camp, and Al-Libi and Abu Zubaydah as senior members of al-Qaeda. But, during testimony at their Combatant Status Review Tribunals, several Guantanamo captives, including Zubaydah, described the Khaldan camp as having been run by a rival jihadist organizationone that did not support attacking civilians.

Al-Libi was turned over to the FBI and held at Bagram Air Base. When talking to the FBI interrogators Russell Fincher and Marty Mahon, he seemed "genuinely friendly" and spoke chiefly in English, calling for a translator only when necessary. He seemed to bond with Fincher, a devout Christian, and the two prayed together and discussed religion at length.

Al-Libi told the interrogators details about Richard Reid, a British citizen who had joined al-Qaeda and trained to carry out a suicide bombing of an airliner, which he unsuccessfully attempted on December 22, 2001. Al-Libi agreed to continue cooperating if the United States would allow his wife and her family to emigrate, while he was prosecuted within the American legal system.

The CIA asked President Bush for permission to take al-Libi into their own custody and rendition him to a foreign country for more "tough guy" questioning, and were granted permission. They "simply came and took al-Libi away from the FBI." One CIA officer was heard telling their new prisoner that "You know where you are going. Before you get there, I am going to find your mother and fuck her".

In the second week of January 2002, al-Libi was flown to the USS "Bataan" in the northern Arabian Sea, a ship being used to hold eight other notable prisoners, including John Walker Lindh. He was subsequently transferred to Egyptian interrogators.

According to "The Washington Post",

On September 15, 2002, "Time" published an article that detailed the CIA interrogations of Omar al-Faruq. It said,

On Sept. 9, according to a secret CIA summary of the interview, al-Faruq confessed that he was, in fact, al-Qaeda's senior representative in Southeast Asia. Then came an even more shocking confession: according to the CIA document, al-Faruq said two senior al-Qaeda officials, Abu Zubaydah and Ibn al-Shaykh al-Libi, had ordered him to 'plan large-scale attacks against U.S. interests in Indonesia, Malaysia, the Philippines, Singapore, Thailand, Taiwan, Vietnam and Cambodia.'

Al-Libi has been identified as a principal source of faulty prewar intelligence regarding chemical weapons training between Iraq and al-Qaeda that was used by the Bush Administration to justify the invasion of Iraq. Specifically, he told interrogators that Iraq provided training to al-Qaeda in the area of "chemical and biological weapons". In Cincinnati in October 2002, Bush informed the public: "Iraq has trained al Qaeda members in bomb making and poisons and gases."

This claim was repeated several times in the run-up to the war, including in then-Secretary of State Colin Powell's speech to the U.N Security Council on February 5, 2003, which concluded with a long recitation of the information provided by al-Libi. Powell's speech was made less than a month after a then-classified CIA report concluded that the information provided by al-Libi was unreliable, and about a year after a DIA report concluded the same thing.

Al-Libi recanted these claims in January 2004 after U.S. interrogators presented "new evidence from other detainees that cast doubt on his claims", according to "Newsweek". The DIA concluded in February 2002 that al-Libi deliberately misled interrogators, in what the CIA called an "attempt to exaggerate his importance". Some speculate that his reason for giving disinformation was in order to draw the U.S. into an attack on Iraq—Islam's "weakest" state; a remark attributed to al-Libi—which al-Qaeda believes will lead to a global jihad. Others, including al-Libi himself, have insisted that he gave false information due to the use of torture (so-called "enhanced interrogation techniques").

An article published in the November 5, 2005, "The New York Times" quoted two paragraphs of a Defense Intelligence Agency report, declassified upon request by Senator Carl Levin, that expressed doubts about the results of al-Libi's interrogation in February 2002.

Al-Libi told a foreign intelligence service that:

Iraq — acting on the request of al-Qa'ida militant Abu Abdullah, who was Muhammad Atif's emissary — agreed to provide unspecified chemical or biological weapons training for two al-Qa'ida associates beginning in December 2000. The two individuals departed for Iraq but did not return, so al-Libi was not in a position to know if any training had taken place. 

The September 2002 version of "Iraqi Support for Terrorism" stated that al-Libi said Iraq had "provided" chemical and biological weapons training for two al-Qaeda associates in 2000, but also stated that al-Libi "did not know the results of the training."

The 2006 Senate Report on Pre-war Intelligence on Iraq stated that "Although DIA coordinated on CIA's "Iraqi Support for Terrorism" paper, DIA analysis preceding that assessment was more skeptical of the al-Libi reporting." In July 2002, DIA assessed

It is plausible al-Qa'ida attempted to obtain CB assistance from Iraq and Ibn al-Shaykh is sufficiently senior to have access to such sensitive information. However, Ibn al-Shaykh's information lacks details concerning the individual Iraqis involved, the specific CB materials associated with the assistance and the location where the alleged training occurred. The information is also second hand, and not derived from Ibn al-Shaykh's personal experience.

The Senate report also states "According to al-Libi, after his decision to fabricate information for debriefers, he 'lied about being a member of al-Qa'ida. Although he considered himself close to, but not a member of, al-Qa'ida, he knew enough about the senior members, organization and operations to claim to be a member.'"

On September 8, 2006, the United States Senate Select Committee on Intelligence released "Phase II" of its report on prewar intelligence on Iraq. Conclusion 3 of the report states the following:

On June 11, 2008, "Newsweek" published an account of material from a "previously undisclosed CIA report written in the summer of 2002". The article reported that on August 7, 2002, CIA analysts had drafted a high-level report that expressed serious doubts about the information flowing from al-Libi's interrogation. The information that al-Libi acknowledged being a member of al-Qaeda's executive council was not supported by other sources. According to al-Libi, in Egypt he was locked in a tiny box less than 20 inches high and held for 17 hours and after being let out he was thrown to the floor and punched for 15 minutes. According to CIA operational cables, only then did he tell his "fabricated" story about al-Qaeda members being dispatched to Iraq.

In November 2006, a Moroccan using the pseudonym Omar Nasiri, having infiltrated al-Qaeda in the 1990s, wrote the book, "". In the book, Nasiri claims that al-Libi deliberately planted information to encourage the U.S. to invade Iraq. In an interview with BBC2's "Newsnight", Nasiri said Libi "needed the conflict in Iraq because months before I heard him telling us when a question was asked in the mosque after the prayer in the evening, where is the best country to fight the jihad?" Nasiri said that Libi had identified Iraq as the "weakest" Muslim country. He suggested to "Newsnight" that al-Libi wanted to overthrow Saddam and use Iraq as a jihadist base. Nasiri describes al-Libi as one of the leaders at the Afghan camp, and characterizes him as "brilliant in every way." He said that learning how to withstand interrogations and supply false information was a key part of the training in the camps. Al-Libi "knew what his interrogators wanted, and he was happy to give it to them. He wanted to see Saddam toppled even more than the Americans did."

In April 2007, former Director of Central Intelligence George Tenet released his memoir titled "". With regard to al-Libi, Tenet writes the following:

In 2006, the Bush Administration announced that it was transferring high-value al-Qaeda detainees from CIA secret prisons so they could be put on trial by military commissions. However, the Administration was conspicuously silent about al-Libi. In December 2014, it was revealed that he had been transferred to the Guantanamo Bay detention camp in 2003 and transferred to Morocco on March 27, 2004.

Noman Benotman, a former Mujahideen who knew Libi, told "Newsweek" that during a recent trip to Tripoli, he met with a senior Libyan government official who confirmed to him that Libi had been transferred to Libya and was being held in prison there. He was suffering from tuberculosis.

On May 10, 2009, the English language edition of the Libyan newspaper "Ennahar" reported that the government said that Al-Libi had been repatriated to Libyan custody in 2006, and had recently committed suicide by hanging. It attributed the information to another newspaper, "Oea". "Ennahar" reported Al-Libi's real name was Ali Mohamed Abdul Aziz Al-Fakheri. It stated he was 46 years old, and had been allowed visits with international human rights workers from Human Rights Watch. The story was widely reported by other media outlets.

Al-Libi had been visited in April 2009 by a team from Human Rights Watch. His sudden death so soon after this visit has led human rights organisations and Islamic groups to question whether it was truly a suicide. Clive Stafford Smith, Legal Director of the UK branch of the human rights group Reprieve, said, "We are told that al-Libi committed suicide in his Libyan prison. If this is true it would be because of his torture and abuse. If false, it may reflect a desire to silence one of the greatest embarrassments to the Bush administration." Hafed Al-Ghwell, a Libya expert and director of communications at the Dubai campus of Harvard's Kennedy School of Government, commented,

This is a regime with a long history of killing people in jail and then claiming it was suicide. My guess is Libya has seen the winds of change in America and wanted to bury this man before international organisations start demanding access to him. 
On June 19, 2009, Andy Worthington published new information on al-Libi's death. Worthington gave a detailed timeline of Al Libi's last years.

The head of the Washington office of Human Rights Watch said al-Libi was "Exhibit A" in hearings on the relationship between pre-Iraq War false intelligence and torture. Confirmation of al-Libi's location came two weeks prior to his death. An independent investigation of his death has been requested by Human Rights Watch.

On October 4, 2009, the "Reuters" reported that Ayman Al Zawahiri, the head of al-Qaeda, had asserted that Libya had caused al-Libi's death through torture.




</doc>
<doc id="15478" url="https://en.wikipedia.org/wiki?curid=15478" title="IDF">
IDF

IDF or idf may refer to:







</doc>
<doc id="15487" url="https://en.wikipedia.org/wiki?curid=15487" title="International Red Cross and Red Crescent Movement">
International Red Cross and Red Crescent Movement

The International Red Cross and Red Crescent Movement is an international humanitarian movement with approximately 97 million volunteers, members and staff worldwide which was founded to protect human life and health, to ensure respect for all human beings, and to prevent and alleviate human suffering.

The movement consists of several distinct organizations that are legally independent from each other, but are united within the movement through common basic principles, objectives, symbols, statutes and governing organisations. The movement's parts are:

Until the middle of the 19th century, there were no organized and/or well-established army nursing systems for casualties and no safe and protected institutions to accommodate and treat those who were wounded on the battlefield. A devout Reformed Christian, the Swiss businessman Jean-Henri Dunant, in June 1859, traveled to Italy to meet French emperor Napoléon III with the intention of discussing difficulties in conducting business in Algeria, at that time occupied by France. He arrived in the small town of Solferino on the evening of 24 June after the Battle of Solferino, an engagement in the Austro-Sardinian War. In a single day, about 40,000 soldiers on both sides died or were left wounded on the field. Jean-Henri Dunant was shocked by the terrible aftermath of the battle, the suffering of the wounded soldiers, and the near-total lack of medical attendance and basic care. He completely abandoned the original intent of his trip and for several days he devoted himself to helping with the treatment and care for the wounded. He took point in organizing an overwhelming level of relief assistance with the local villagers to aid without discrimination.
Back in his home in Geneva, he decided to write a book entitled "A Memory of Solferino" which he published using his own money in 1862. He sent copies of the book to leading political and military figures throughout Europe, and people he thought could help him make a change. In addition to penning a vivid description of his experiences in Solferino in 1859, he explicitly advocated the formation of national voluntary relief organizations to help nurse wounded soldiers in the case of war, an idea that was inspired by Christian teaching regarding social responsibility, as well as his experience after the battlefield of Solferino. In addition, he called for the development of an international treaty to guarantee the protection of medics and field hospitals for soldiers wounded on the battlefield.

In 1863, Gustave Moynier, a Geneva lawyer and president of the Geneva Society for Public Welfare, received a copy of Dunant's book and introduced it for discussion at a meeting of that society. As a result of this initial discussion the society established an investigatory commission to examine the feasibility of Dunant's suggestions and eventually to organize an international conference about their possible implementation. The members of this committee, which has subsequently been referred to as the "Committee of the Five," aside from Dunant and Moynier were physician Louis Appia, who had significant experience working as a field surgeon; Appia's friend and colleague Théodore Maunoir, from the Geneva Hygiene and Health Commission; and Guillaume-Henri Dufour, a Swiss Army general of great renown. Eight days later, the five men decided to rename the committee to the "International Committee for Relief to the Wounded". In October (26–29) 1863, the international conference organized by the committee was held in Geneva to develop possible measures to improve medical services on the battlefield. The conference was attended by 36 individuals: eighteen official delegates from national governments, six delegates from other non-governmental organizations, seven non-official foreign delegates, and the five members of the International Committee. The states and kingdoms represented by official delegates were: Austrian Empire, Grand Duchy of Baden, Kingdom of Bavaria, French Empire, Kingdom of Hanover, Grand Duchy of Hesse, Kingdom of Italy, Kingdom of the Netherlands, Kingdom of Prussia, Russian Empire, Kingdom of Saxony, Kingdom of Spain, United Kingdoms of Sweden and Norway, and United Kingdom of Great Britain and Ireland.
Among the proposals written in the final resolutions of the conference, adopted on 29 October 1863, were:

Only one year later, the Swiss government invited the governments of all European countries, as well as the United States of America, the Empire of Brazil, and the Mexican Empire, to attend an official diplomatic conference. Sixteen countries sent a total of twenty-six delegates to Geneva. On 22 August 1864, the conference adopted the first Geneva Convention "for the Amelioration of the Condition of the Wounded in Armies in the Field". Representatives of 12 states and kingdoms signed the convention:

The convention contained ten articles, establishing for the first time legally binding rules guaranteeing neutrality and protection for wounded soldiers, field medical personnel, and specific humanitarian institutions in an armed conflict.

Directly following the establishment of the Geneva Convention, the first national societies were founded in Belgium, Denmark, France, Oldenburg, Prussia, Spain, and Württemberg. Also in 1864, Louis Appia and Charles van de Velde, a captain of the Dutch Army, became the first independent and neutral delegates to work under the symbol of the Red Cross in an armed conflict. Three years later in 1867, the first International Conference of National Aid Societies for the Nursing of the War Wounded was convened.

Also in 1867, Jean-Henri Dunant was forced to declare bankruptcy due to business failures in Algeria, partly because he had neglected his business interests during his tireless activities for the International Committee. The controversy surrounding Dunant's business dealings and the resulting negative public opinion, combined with an ongoing conflict with Gustave Moynier, led to Dunant's expulsion from his position as a member and secretary. He was charged with fraudulent bankruptcy and a warrant for his arrest was issued. Thus, he was forced to leave Geneva and never returned to his home city.

In the following years, national societies were founded in nearly every country in Europe. The project resonated well with patriotic sentiments that were on the rise in the late-nineteenth-century, and national societies were often encouraged as signifiers of national moral superiority. In 1876, the committee adopted the name "International Committee of the Red Cross" (ICRC), which is still its official designation today. Five years later, the American Red Cross was founded through the efforts of Clara Barton. More and more countries signed the Geneva Convention and began to respect it in practice during armed conflicts. In a rather short period of time, the Red Cross gained huge momentum as an internationally respected movement, and the national societies became increasingly popular as a venue for volunteer work.

When the first Nobel Peace Prize was awarded in 1901, the Norwegian Nobel Committee opted to give it jointly to Jean-Henri Dunant and Frédéric Passy, a leading international pacifist. More significant than the honor of the prize itself, this prize marked the overdue rehabilitation of Jean-Henri Dunant and represented a tribute to his key role in the formation of the Red Cross. Dunant died nine years later in the small Swiss health resort of Heiden. Only two months earlier his long-standing adversary Gustave Moynier had also died, leaving a mark in the history of the committee as its longest-serving president ever.

In 1906, the 1864 Geneva Convention was revised for the first time. One year later, the Hague Convention X, adopted at the Second International Peace Conference in The Hague, extended the scope of the Geneva Convention to naval warfare. Shortly before the beginning of the First World War in 1914, 50 years after the foundation of the ICRC and the adoption of the first Geneva Convention, there were already 45 national relief societies throughout the world. The movement had extended itself beyond Europe and North America to Central and South America (Argentine Republic, the United States of Brazil, the Republic of Chile, the Republic of Cuba, the United Mexican States, the Republic of Peru, the Republic of El Salvador, the Oriental Republic of Uruguay, the United States of Venezuela), Asia (the Republic of China, the Empire of Japan and the Kingdom of Siam), and Africa (Union of South Africa).

With the outbreak of World War I, the ICRC found itself confronted with enormous challenges that it could handle only by working closely with the national Red Cross societies. Red Cross nurses from around the world, including the United States and Japan, came to support the medical services of the armed forces of the European countries involved in the war. On 15 August 1914, immediately after the start of the war, the ICRC set up its International Prisoners-of-War (POW) Agency, which had about 1,200 mostly volunteer staff members by the end of 1914. By the end of the war, the Agency had transferred about 20 million letters and messages, 1.9 million parcels, and about 18 million Swiss francs in monetary donations to POWs of all affected countries. Furthermore, due to the intervention of the Agency, about 200,000 prisoners were exchanged between the warring parties, released from captivity and returned to their home country. The organizational card index of the Agency accumulated about 7 million records from 1914 to 1923. The card index led to the identification of about 2 million POWs and the ability to contact their families. The complete index is on loan today from the ICRC to the International Red Cross and Red Crescent Museum in Geneva. The right to access the index is still strictly restricted to the ICRC.

During the entire war, the ICRC monitored warring parties' compliance with the Geneva Conventions of the 1907 revision and forwarded complaints about violations to the respective country. When chemical weapons were used in this war for the first time in history, the ICRC vigorously protested against this new type of warfare. Even without having a mandate from the Geneva Conventions, the ICRC tried to ameliorate the suffering of civil populations. In territories that were officially designated as "occupied territories", the ICRC could assist the civilian population on the basis of the Hague Convention's "Laws and Customs of War on Land" of 1907. This convention was also the legal basis for the ICRC's work for prisoners of war. In addition to the work of the International Prisoner-of-War Agency as described above this included inspection visits to POW camps. A total of 524 camps throughout Europe were visited by 41 delegates from the ICRC until the end of the war.

Between 1916 and 1918, the ICRC published a number of postcards with scenes from the POW camps. The pictures showed the prisoners in day-to-day activities such as the distribution of letters from home. The intention of the ICRC was to provide the families of the prisoners with some hope and solace and to alleviate their uncertainties about the fate of their loved ones. After the end of the war, between 1920 and 1922, the ICRC organized the return of about 500,000 prisoners to their home countries. In 1920, the task of repatriation was handed over to the newly founded League of Nations, which appointed the Norwegian diplomat and scientist Fridtjof Nansen as its "High Commissioner for Repatriation of the War Prisoners". His legal mandate was later extended to support and care for war refugees and displaced persons when his office became that of the League of Nations "High Commissioner for Refugees". Nansen, who invented the Nansen passport for stateless refugees and was awarded the Nobel Peace Prize in 1922, appointed two delegates from the ICRC as his deputies.

A year before the end of the war, the ICRC received the 1917 Nobel Peace Prize for its outstanding wartime work. It was the only Nobel Peace Prize awarded in the period from 1914 to 1918. In 1923, the International Committee of the Red Cross adopted a change in its policy regarding the selection of new members. Until then, only citizens from the city of Geneva could serve in the committee. This limitation was expanded to include Swiss citizens. As a direct consequence of World War I, a treaty was adopted in 1925 which outlawed the use of suffocating or poisonous gases and biological agents as weapons. Four years later, the original Convention was revised and the second Geneva Convention "relative to the Amelioration of the Condition of Wounded, Sick and Shipwrecked Members of Armed Forces at Sea" was established. The events of World War I and the respective activities of the ICRC significantly increased the reputation and authority of the Committee among the international community and led to an extension of its competencies.

As early as in 1934, a draft proposal for an additional convention for the protection of the civil population in occupied territories during an armed conflict was adopted by the International Red Cross Conference. Unfortunately, most governments had little interest in implementing this convention, and it was thus prevented from entering into force before the beginning of World War II.

The Red Cross' response to the Holocaust has been the subject of significant controversy and criticism. As early as May 1944, the ICRC was criticized for its indifference to Jewish suffering and death—criticism that intensified after the end of the war, when the full extent of the Holocaust became undeniable. One defense to these allegations is that the Red Cross was trying to preserve its reputation as a neutral and impartial organization by not interfering with what was viewed as a German internal matter. The Red Cross also considered its primary focus to be prisoners of war whose countries had signed the Geneva Convention.

The legal basis of the work of the ICRC during World War II were the Geneva Conventions in their 1929 revision. The activities of the committee were similar to those during World War I: visiting and monitoring POW camps, organizing relief assistance for civilian populations, and administering the exchange of messages regarding prisoners and missing persons. By the end of the war, 179 delegates had conducted 12,750 visits to POW camps in 41 countries. The Central Information Agency on Prisoners-of-War ("Agence centrale des prisonniers de guerre") had a staff of 3,000, the card index tracking prisoners contained 45 million cards, and 120 million messages were exchanged by the Agency. One major obstacle was that the Nazi-controlled German Red Cross refused to cooperate with the Geneva statutes including blatant violations such as the deportation of Jews from Germany and the mass murders conducted in the Nazi concentration camps. Moreover, two other main parties to the conflict, the Soviet Union and Japan, were not party to the 1929 Geneva Conventions and were not legally required to follow the rules of the conventions.

During the war, the ICRC was unable to obtain an agreement with Nazi Germany about the treatment of detainees in concentration camps, and it eventually abandoned applying pressure in order to avoid disrupting its work with POWs. The ICRC was also unable to obtain a response to reliable information about the extermination camps and the mass killing of European Jews, Roma, et al. After November 1943, the ICRC achieved permission to send parcels to concentration camp detainees with known names and locations. Because the notices of receipt for these parcels were often signed by other inmates, the ICRC managed to register the identities of about 105,000 detainees in the concentration camps and delivered about 1.1 million parcels, primarily to the camps Dachau, Buchenwald, Ravensbrück, and Sachsenhausen.

Maurice Rossel was sent to Berlin as a delegate of the International Red Cross; he visited Theresienstadt in 1944. The choice of the inexperienced Rossel for this mission has been interpreted as indicative of his organization's indifference to the "Jewish problem", while his report has been described as "emblematic of the failure of the ICRC" to advocate for Jews during the Holocaust. Rossel's report was noted for its uncritical acceptance of Nazi propaganda. He erroneously stated that Jews were not deported from Theresienstadt. Claude Lanzmann recorded his experiences in 1979, producing a documentary entitled "A Visitor from the Living".

On 12 March 1945, ICRC president Jacob Burckhardt received a message from SS General Ernst Kaltenbrunner allowing ICRC delegates to visit the concentration camps. This agreement was bound by the condition that these delegates would have to stay in the camps until the end of the war. Ten delegates, among them Louis Haefliger (Mauthausen-Gusen), Paul Dunant (Theresienstadt) and Victor Maurer (Dachau), accepted the assignment and visited the camps. Louis Haefliger prevented the forceful eviction or blasting of Mauthausen-Gusen by alerting American troops.

Friedrich Born (1903–1963), an ICRC delegate in Budapest who saved the lives of about 11,000 to 15,000 Jewish people in Hungary. Marcel Junod (1904–1961), a physician from Geneva was one of the first foreigners to visit Hiroshima after the atomic bomb was dropped.

In 1944, the ICRC received its second Nobel Peace Prize. As in World War I, it received the only Peace Prize awarded during the main period of war, 1939 to 1945. At the end of the war, the ICRC worked with national Red Cross societies to organize relief assistance to those countries most severely affected. In 1948, the Committee published a report reviewing its war-era activities from 1 September 1939 to 30 June 1947. The ICRC opened its archives from World War II in 1996.

On 12 August 1949, further revisions to the existing two Geneva Conventions were adopted. An additional convention "for the Amelioration of the Condition of Wounded, Sick and Shipwrecked Members of Armed Forces at Sea", now called the second Geneva Convention, was brought under the Geneva Convention umbrella as a successor to the 1907 Hague Convention X. The 1929 Geneva convention "relative to the Treatment of Prisoners of War" may have been the second Geneva Convention from a historical point of view (because it was actually formulated in Geneva), but after 1949 it came to be called the third Convention because it came later chronologically than the Hague Convention. Reacting to the experience of World War II, the Fourth Geneva Convention, a new Convention "relative to the Protection of Civilian Persons in Time of War", was established. Also, the additional protocols of 8 June 1977 were intended to make the conventions apply to internal conflicts such as civil wars. Today, the four conventions and their added protocols contain more than 600 articles, a remarkable expansion when compared to the mere 10 articles in the first 1864 convention.

In celebration of its centennial in 1963, the ICRC, together with the League of Red Cross Societies, received its third Nobel Peace Prize. Since 1993, non-Swiss individuals have been allowed to serve as Committee delegates abroad, a task which was previously restricted to Swiss citizens. Indeed, since then, the share of staff without Swiss citizenship has increased to about 35%.

On 16 October 1990, the UN General Assembly decided to grant the ICRC observer status for its assembly sessions and sub-committee meetings, the first observer status given to a private organization. The resolution was jointly proposed by 138 member states and introduced by the Italian ambassador, Vieri Traxler, in memory of the organization's origins in the Battle of Solferino. An agreement with the Swiss government signed on 19 March 1993, affirmed the already long-standing policy of full independence of the committee from any possible interference by Switzerland. The agreement protects the full sanctity of all ICRC property in Switzerland including its headquarters and archive, grants members and staff legal immunity, exempts the ICRC from all taxes and fees, guarantees the protected and duty-free transfer of goods, services, and money, provides the ICRC with secure communication privileges at the same level as foreign embassies, and simplifies Committee travel in and out of Switzerland.

At the end of the Cold War, the ICRC's work actually became more dangerous. In the 1990s, more delegates lost their lives than at any point in its history, especially when working in local and internal armed conflicts. These incidents often demonstrated a lack of respect for the rules of the Geneva Conventions and their protection symbols. Among the slain delegates were:

ICRC is active in the Afghanistan conflict areas and has set up six physical rehabilitation centers to help land mine victims. Their support extends to the national and international armed forces, civilians and the armed opposition. They regularly visit detainees under the custody of the Afghan government and the international armed forces, but have also occasionally had access since 2009 to people detained by the Taliban. They have provided basic first aid training and aid kits to both the Afghan security forces and Taliban members because, according to an ICRC spokesperson, "ICRC's constitution stipulates that all parties harmed by warfare will be treated as fairly as possible".

In 1919, representatives from the national Red Cross societies of Britain, France, Italy, Japan, and the US came together in Paris to found the "League of Red Cross Societies". The original idea was Henry Davison's, then president of the American Red Cross. This move, led by the American Red Cross, expanded the international activities of the Red Cross movement beyond the strict mission of the ICRC to include relief assistance in response to emergency situations which were not caused by war (such as man-made or natural disasters). The ARC already had great disaster relief mission experience extending back to its foundation.

The formation of the League, as an additional international Red Cross organization alongside the ICRC, was not without controversy for a number of reasons. The ICRC had, to some extent, valid concerns about a possible rivalry between both organizations. The foundation of the League was seen as an attempt to undermine the leadership position of the ICRC within the movement and to gradually transfer most of its tasks and competencies to a multilateral institution. In addition to that, all founding members of the League were national societies from countries of the Entente or from associated partners of the Entente. The original statutes of the League from May 1919 contained further regulations which gave the five founding societies a privileged status and, due to the efforts of Henry P. Davison, the right to permanently exclude the national Red Cross societies from the countries of the Central Powers, namely Germany, Austria, Hungary, Bulgaria and Turkey, and in addition to that the national Red Cross society of Russia. These rules were contrary to the Red Cross principles of universality and equality among all national societies, a situation which furthered the concerns of the ICRC.

The first relief assistance mission organized by the League was an aid mission for the victims of a famine and subsequent typhus epidemic in Poland. Only five years after its foundation, the League had already issued 47 donation appeals for missions in 34 countries, an impressive indication of the need for this type of Red Cross work. The total sum raised by these appeals reached 685 million Swiss francs, which were used to bring emergency supplies to the victims of famines in Russia, Germany, and Albania; earthquakes in Chile, Persia, Japan, Colombia, Ecuador, Costa Rica, and Turkey; and refugee flows in Greece and Turkey. The first large-scale disaster mission of the League came after the 1923 earthquake in Japan which killed about 200,000 people and left countless more wounded and without shelter. Due to the League's coordination, the Red Cross society of Japan received goods from its sister societies reaching a total worth of about $100 million. Another important new field initiated by the League was the creation of youth Red Cross organizations within the national societies.

A joint mission of the ICRC and the League in the Russian Civil War from 1917 to 1922 marked the first time the movement was involved in an internal conflict, although still without an explicit mandate from the Geneva Conventions. The League, with support from more than 25 national societies, organized assistance missions and the distribution of food and other aid goods for civil populations affected by hunger and disease. The ICRC worked with the Russian Red Cross society and later the society of the Soviet Union, constantly emphasizing the ICRC's neutrality. In 1928, the "International Council" was founded to coordinate cooperation between the ICRC and the League, a task which was later taken over by the "Standing Commission". In the same year, a common statute for the movement was adopted for the first time, defining the respective roles of the ICRC and the League within the movement.

During the Abyssinian war between Ethiopia and Italy from 1935 to 1936, the League contributed aid supplies worth about 1.7 million Swiss francs. Because the Italian fascist regime under Benito Mussolini refused any cooperation with the Red Cross, these goods were delivered solely to Ethiopia. During the war, an estimated 29 people lost their lives while being under explicit protection of the Red Cross symbol, most of them due to attacks by the Italian Army. During the civil war in Spain from 1936 to 1939 the League once again joined forces with the ICRC with the support of 41 national societies. In 1939 on the brink of the Second World War, the League relocated its headquarters from Paris to Geneva to take advantage of Swiss neutrality.

In 1952, the 1928 common statute of the movement was revised for the first time. Also, the period of decolonization from 1960 to 1970 was marked by a huge jump in the number of recognized national Red Cross and Red Crescent societies. By the end of the 1960s, there were more than 100 societies around the world. On December 10, 1963, the Federation and the ICRC received the Nobel Peace Prize. In 1983, the League was renamed to the "League of Red Cross and Red Crescent Societies" to reflect the growing number of national societies operating under the Red Crescent symbol. Three years later, the seven basic principles of the movement as adopted in 1965 were incorporated into its statutes. The name of the League was changed again in 1991 to its current official designation the "International Federation of Red Cross and Red Crescent Societies". In 1997, the ICRC and the IFRC signed the Seville Agreement which further defined the responsibilities of both organizations within the movement. In 2004, the IFRC began its largest mission to date after the tsunami disaster in South Asia. More than 40 national societies have worked with more than 22,000 volunteers to bring relief to the countless victims left without food and shelter and endangered by the risk of epidemics.

Altogether, there are about 97 million people worldwide who serve with the ICRC, the International Federation, and the National Societies, the majority with the latter.

The 1965 International Conference in Vienna adopted seven basic principles which should be shared by all parts of the Movement, and they were added to the official statutes of the Movement in 1986.

At the 20th International Conference in Neue Hofburg, Vienna, from 2–9 October 1965, delegates "proclaimed" seven fundamental principles which are shared by all components of the Movement, and they were added to the official statutes of the Movement in 1986. The durability and universal acceptance is a result of the process through which they came into being in the form they have. Rather than an effort to arrive at agreement, it was an attempt to discover what successful operations and organisational units, over the past 100 years, had in common. As a result, the Fundamental Principles of the Red Cross and Red Crescent were not revealed, but "found" – through a deliberate and participative process of discovery.

That makes it even more important to note that the text that appears under each "heading" is an integral part of the Principle in question and not an interpretation that can vary with time and place.

Humanity

The International Red Cross and Red Crescent Movement, born of a desire to bring assistance without discrimination to the wounded on the battlefield, endeavours, in its international and national capacity, to prevent and alleviate human suffering wherever it may be found. Its purpose is to protect life and health and to ensure respect for the human being. It promotes mutual understanding, friendship, cooperation and lasting peace amongst all peoples.

Impartiality

It makes no discrimination as to nationality, race, religious beliefs, class or political opinions. It endeavours to relieve the suffering of individuals, being guided solely by their needs, and to give priority to the most urgent cases of distress.

Neutrality

In order to continue to enjoy the confidence of all, the Movement may not take sides in hostilities or engage at any time in controversies of a political, racial, religious or ideological nature.

Independence

The Movement is independent. The National Societies, while auxiliaries in the humanitarian services of their governments and subject to the laws of their respective countries, must always maintain their autonomy so that they may be able at all times to act in accordance with the principles of the Movement.

Voluntary Service

It is a voluntary relief movement not prompted in any manner by desire for gain.

Unity

There can be only one Red Cross or one Red Crescent Society in any one country. It must be open to all. It must carry on its humanitarian work throughout its territory.

Universality

The International Red Cross and Red Crescent Movement, in which all Societies have equal status and share equal responsibilities and duties in helping each other, is worldwide.

The International Conference of the Red Cross and Red Crescent, which occurs once every four years, is the highest institutional body of the Movement. It gathers delegations from all of the national societies as well as from the ICRC, the IFRC and the signatory states to the Geneva Conventions. In between the conferences, the Standing Commission of the Red Cross and Red Crescent acts as the supreme body and supervises implementation of and compliance with the resolutions of the conference. In addition, the Standing Commission coordinates the cooperation between the ICRC and the IFRC. It consists of two representatives from the ICRC (including its president), two from the IFRC (including its president), and five individuals who are elected by the International Conference. The Standing Commission convenes every six months on average. Moreover, a convention of the Council of Delegates of the Movement takes place every two years in the course of the conferences of the General Assembly of the International Federation. The Council of Delegates plans and coordinates joint activities for the Movement.

The official mission of the ICRC as an impartial, neutral, and independent organization is to stand for the protection of the life and dignity of victims of international and internal armed conflicts. According to the 1997 Seville Agreement, it is the "Lead Agency" of the Movement in conflicts. The core tasks of the committee, which are derived from the Geneva Conventions and its own statutes, are the following:

The ICRC is headquartered in the Swiss city of Geneva and has external offices in about 80 countries. It has about 12,000 staff members worldwide, about 800 of them working in its Geneva headquarters, 1,200 expatriates with about half of them serving as delegates managing its international missions and the other half being specialists like doctors, agronomists, engineers or interpreters, and about 10,000 members of individual national societies working on site.

According to Swiss law, the ICRC is defined as a private association. Contrary to popular belief, the ICRC is not a non-governmental organization in the most common sense of the term, nor is it an international organization. As it limits its members (a process called cooptation) to Swiss nationals only, it does not have a policy of open and unrestricted membership for individuals like other legally defined NGOs. The word "international" in its name does not refer to its membership but to the worldwide scope of its activities as defined by the Geneva Conventions. The ICRC has special privileges and legal immunities in many countries, based on national law in these countries or through agreements between the committee and respective national governments.

According to its statutes it consists of 15 to 25 Swiss-citizen members, which it coopts for a period of four years. There is no limit to the number of terms an individual member can have although a three-quarters majority of all members is required for re-election after the third term.

The leading organs of the ICRC are the Directorate and the Assembly. The Directorate is the executive body of the committee. It consists of a general director and five directors in the areas of "Operations", "Human Resources", "Resources and Operational Support", "Communication", and "International Law and Cooperation within the Movement". The members of the Directorate are appointed by the Assembly to serve for four years. The Assembly, consisting of all of the members of the committee, convenes on a regular basis and is responsible for defining aims, guidelines, and strategies and for supervising the financial matters of the committee. The president of the Assembly is also the president of the committee as a whole. Furthermore, the Assembly elects a five-member Assembly Council which has the authority to decide on behalf of the full Assembly in some matters. The council is also responsible for organizing the Assembly meetings and for facilitating communication between the Assembly and the Directorate.

Due to Geneva's location in the French-speaking part of Switzerland, the ICRC usually acts under its French name "Comité international de la Croix-Rouge" (CICR). The official symbol of the ICRC is the Red Cross on white background with the words "COMITE INTERNATIONAL GENEVE" circling the cross.

The 2009 budget of the ICRC amounts to more than 1 billion Swiss francs. Most of that money comes from the States, including Switzerland in its capacity as the depositary state of the Geneva Conventions, from national Red Cross societies, the signatory states of the Geneva Conventions, and from international organizations like the European Union. All payments to the ICRC are voluntary and are received as donations based on two types of appeals issued by the committee: an annual "Headquarters Appeal" to cover its internal costs and "Emergency Appeals" for its individual missions.

The ICRC is asking donors for more than 1.1 billion Swiss francs to fund its work in 2010. Afghanistan is projected to become the ICRC's biggest humanitarian operation (at 86 million Swiss francs, an 18% increase over the initial 2009 budget), followed by Iraq (85 million francs) and Sudan (76 million francs). The initial 2010 field budget for medical activities of 132 million francs represents an increase of 12 million francs over 2009.

The IFRC coordinates cooperation between national Red Cross and Red Crescent societies throughout the world and supports the foundation of new national societies in countries where no official society exists. On the international stage, the IFRC organizes and leads relief assistance missions after emergencies such as natural disasters, manmade disasters, epidemics, mass refugee flights, and other emergencies. As per the 1997 Seville Agreement, the IFRC is the Lead Agency of the Movement in any emergency situation which does not take place as part of an armed conflict. The IFRC cooperates with the national societies of those countries affected – each called the "Operating National Society" (ONS) – as well as the national societies of other countries willing to offer assistance – called "Participating National Societies" (PNS). Among the 187 national societies admitted to the General Assembly of the International Federation as full members or observers, about 25–30 regularly work as PNS in other countries. The most active of those are the American Red Cross, the British Red Cross, the German Red Cross, and the Red Cross societies of Sweden and Norway. Another major mission of the IFRC which has gained attention in recent years is its commitment to work towards a codified, worldwide ban on the use of land mines and to bring medical, psychological, and social support for people injured by land mines.

The tasks of the IFRC can therefore be summarized as follows:

The IFRC has its headquarters in Geneva. It also runs five zone offices (Africa, Americas, Asia Pacific, Europe, Middle East-North Africa), 14 permanent regional offices and has about 350 delegates in more than 60 delegations around the world. The legal basis for the work of the IFRC is its constitution. The executive body of the IFRC is a secretariat, led by a secretary general. The secretariat is supported by five divisions including "Programme Services", "Humanitarian values and humanitarian diplomacy", "National Society and Knowledge Development" and "Governance and Management Services".

The highest decision making body of the IFRC is its General Assembly, which convenes every two years with delegates from all of the national societies. Among other tasks, the General Assembly elects the secretary general. Between the convening of General Assemblies, the Governing Board is the leading body of the IFRC. It has the authority to make decisions for the IFRC in a number of areas. The Governing Board consists of the president and the vice presidents of the IFRC, the chairpersons of the Finance and Youth Commissions, and twenty elected representatives from national societies.

The symbol of the IFRC is the combination of the Red Cross (left) and Red Crescent (right) on a white background surrounded by a red rectangular frame.

The main parts of the budget of the IFRC are funded by contributions from the national societies which are members of the IFRC and through revenues from its investments. The exact amount of contributions from each member society is established by the Finance Commission and approved by the General Assembly. Any additional funding, especially for unforeseen expenses for relief assistance missions, is raised by ""appeals"" published by the IFRC and comes for voluntary donations by national societies, governments, other organizations, corporations, and individuals.

National Red Cross and Red Crescent societies exist in nearly every country in the world. Within their home country, they take on the duties and responsibilities of a national relief society as defined by International Humanitarian Law. Within the Movement, the ICRC is responsible for legally recognizing a relief society as an official national Red Cross or Red Crescent society. The exact rules for recognition are defined in the statutes of the Movement. Article 4 of these statutes contains the ""Conditions for recognition of National Societies.""


Once a National Society has been recognized by the ICRC as a component of the International Red Cross and Red Crescent Movement (the Movement), it is in principle admitted to the International Federation of Red Cross and Red Crescent Societies in accordance with the terms defined in the Constitution and Rules of Procedure of the International Federation.

There are today 190 National Societies recognized within the Movement and which are members of the International Federation.

The most recent National Societies to have been recognized within the Movement are the Maldives Red Crescent Society (9 November 2011), the Cyprus Red Cross Society, the South Sudan Red Cross Society (12 November 2013) and, the last, the Tuvalu Red Cross Society (on 1 March 2016).

Despite formal independence regarding its organizational structure and work, each national society is still bound by the laws of its home country. In many countries, national Red Cross and Red Crescent societies enjoy exceptional privileges due to agreements with their governments or specific "Red Cross Laws" granting full independence as required by the International Movement. The duties and responsibilities of a national society as defined by International Humanitarian Law and the statutes of the Movement include humanitarian aid in armed conflicts and emergency crises such as natural disasters through activities such as Restoring Family Links.

Depending on their respective human, technical, financial, and organizational resources, many national societies take on additional humanitarian tasks within their home countries such as blood donation services or acting as civilian Emergency Medical Service (EMS) providers. The ICRC and the International Federation cooperate with the national societies in their international missions, especially with human, material, and financial resources and organizing on-site logistics.

The Red Cross emblem was officially approved in Geneva in 1863.

The Red Cross flag is not to be confused with the Saint George's Cross depicted on the flag of England, Barcelona, Georgia, Freiburg im Breisgau, and several other places. In order to avoid this confusion the protected symbol is sometimes referred to as the "Greek Red Cross" (now Hellenic Red Cross); that term is also used in United States law to describe the Red Cross. The red cross of the Saint George cross extends to the edge of the flag, whereas the red cross on the Red Cross flag does not.

The Red Cross flag is the colour-switched version of the Flag of Switzerland. In 1906, to put an end to the argument of the Ottoman Empire that the flag took its roots from Christianity, it was decided to promote officially the idea that the Red Cross flag had been formed by reversing the federal colours of Switzerland, although no clear evidence of this origin had ever been found.

The Red Crescent emblem was first used by ICRC volunteers during the armed conflict of 1876–8 between the Ottoman Empire and the Russian Empire. The symbol was officially adopted in 1929, and so far 33 states in the Muslim world have recognized it. In common with the official promotion of the red cross symbol as a colour-reversal of the Swiss flag (rather than a religious symbol), the red crescent is similarly presented as being derived from a colour-reversal of the flag of the Ottoman Empire.

On 8 December 2005, in response to growing pressure to accommodate Magen David Adom (MDA), Israel's national emergency medical, disaster, ambulance and blood bank service, as a full member of the Red Cross and Red Crescent movement, a new emblem (officially the Third Protocol Emblem, but more commonly known as the Red Crystal) was adopted by an amendment of the Geneva Conventions known as Protocol III.

The Red Lion and Sun Society of Iran was established in 1922 and admitted to the Red Cross and Red Crescent movement in 1923. However, some report the symbol was introduced at Geneva in 1864 as a counter example to the crescent and cross used by two of Iran's rivals, the Ottoman and the Russian empires. Although that claim is inconsistent with the Red Crescent's history, that history also suggests that the Red Lion and Sun, like the Red Crescent, may have been conceived during the 1877–1878 war between Russia and Turkey.

Due to the emblem's association with the Iranian monarchy, the Islamic Republic of Iran replaced the Red Lion and Sun with the Red Crescent in 1980, consistent with two existing Red Cross and Red Crescent symbols. Although the Red Lion and Sun has now fallen into disuse, Iran has in the past reserved the right to take it up again at any time; the Geneva Conventions continue to recognize it as an official emblem, and that status was confirmed by Protocol III in 2005 even as it added the Red Crystal.

For over 50 years, Israel requested the addition of a red Star of David, arguing that since Christian and Muslim emblems were recognized, the corresponding Jewish emblem should be as well. This emblem has been used by Magen David Adom (MDA), or Red Star of David, but it is not recognized by the Geneva Conventions as a protected symbol. The first use of the ″Magen David Adom″ was during the Anglo Boer War in South Africa (1899–1902) when it was used by the Ambulance Corps founded by Ben Zion Aaron in Johannesburg as a first aid corps to assist the Boer forces. Permission was given by President Paul Kruger of the South African Republic for the Star of David to be used as its insignia, rather than the conventional red cross.

The Red Cross and Red Crescent movement repeatedly rejected Israel's request over the years, stating that the Red Cross and Red Crescent emblems were not meant to represent Christianity and Islam but were colour reversals of the Swiss and the Ottoman flags, and also that if Jews (or another group) were to be given another emblem, there would be no end to the number of religious or other groups claiming an emblem for themselves. They reasoned that a proliferation of red symbols would detract from the original intention of the Red Cross emblem, which was to be a single emblem to mark vehicles and buildings protected on humanitarian grounds.

Certain Arab nations, such as Syria, also protested against the entry of MDA into the Red Cross movement, making consensus impossible for a time.
However, from 2000 to 2006 the American Red Cross withheld its dues (a total of $42 million) to the International Federation of Red Cross and Red Crescent Societies (IFRC) because of IFRC's refusal to admit MDA; this ultimately led to the creation of the Red Crystal emblem and the admission of MDA on June 22, 2006.

The Red Star of David is not recognized as a protected symbol outside Israel; instead the MDA uses the Red Crystal emblem during international operations in order to ensure protection. Depending on the circumstances, it may place the Red Star of David inside the Red Crystal, or use the Red Crystal alone.

The Australian TV network ABC and the indigenous rights group Rettet die Naturvölker released a documentary called "Blood on the Cross" in 1999. It alleged the involvement of the Red Cross with the British and Indonesian military in a massacre in the Southern Highlands of Western New Guinea during the World Wildlife Fund's Mapenduma hostage crisis of May 1996, when Western and Indonesian activists were held hostage by separatists.

Following the broadcast of the documentary, the Red Cross announced publicly that it would appoint an individual outside the organization to investigate the allegations made in the film and any responsibility on its part. Piotr Obuchowicz was appointed to investigate the matter. The report categorically states that the Red Cross personnel accused of involvement were proven not to have been present; that a white helicopter was probably used in a military operation, but the helicopter was not a Red Cross helicopter, and must have been painted by one of several military organizations operating in the region at the time. Perhaps the Red Cross logo itself was also used, although no hard evidence was found for this; that this was part of the military operation to free the hostages, but was clearly intended to achieve surprise by deceiving the local people into thinking that a Red Cross helicopter was landing; and that the Red Cross should have responded more quickly and thoroughly to investigate the allegations than it did.







</doc>
<doc id="15489" url="https://en.wikipedia.org/wiki?curid=15489" title="Ira Gershwin">
Ira Gershwin

Ira Gershwin (born Israel Gershowitz, December 6, 1896 – August 17, 1983) was an American lyricist who collaborated with his younger brother, composer George Gershwin, to create some of the most memorable songs in the English language of the 20th century.

With George he wrote more than a dozen Broadway shows, featuring songs such as "I Got Rhythm", "Embraceable You", "The Man I Love" and "Someone to Watch Over Me". He was also responsible, along with DuBose Heyward, for the libretto to George's opera "Porgy and Bess".

The success the Gershwin brothers had with their collaborative works has often overshadowed the creative role that Ira played. His mastery of songwriting continued, however, after the early death of George. He wrote additional hit songs with composers Jerome Kern, Kurt Weill, Harry Warren and Harold Arlen.

His critically acclaimed 1959 book "Lyrics on Several Occasions", an amalgam of autobiography and annotated anthology, is an important source for studying the art of the lyricist in the golden age of American popular song.

Gershwin was born at 242 Snediker Avenue in Brooklyn, the oldest of four children of Morris (Moishe) and Rose Gershovitz (née Rosa Bruskin), who were Russian Jews, born in St Petersburg, who had emigrated to the US in 1891. Ira's siblings were George (Jacob, b. 1898), Arthur (b. 1900) and Frances (b. 1906). Morris changed the family name to "Gershwine" (or alternatively "Gershvin") well before their children rose to fame; it was not spelled "Gershwin" until later. Shy in his youth, Ira spent much of his time at home reading, but from grammar school through college he played a prominent part in several school newspapers and magazines.

He graduated in 1914 from Townsend Harris High School, a public school for intellectually gifted students, where he met Yip Harburg, with whom he enjoyed a lifelong friendship and a love of Gilbert and Sullivan. He attended the City College of New York but dropped out.

The childhood home of Ira and George Gershwin was in the center of the Yiddish Theater District, on the second floor at 91 Second Avenue, between East 5th Street and East 6th Street. They frequented the local Yiddish theaters.

While George began composing and "plugging" in Tin Pan Alley from the age of 18, Ira worked as a cashier in his father's Turkish baths. It was not until 1921 that Ira became involved in the music business. Alex Aarons signed Ira to write the songs for his next show, "Two Little Girls in Blue", ultimately produced by Abraham Erlanger, along with co-composers Vincent Youmans and Paul Lannin. So as not to appear to trade off George's growing reputation, Ira wrote under the pseudonym "Arthur Francis", after his youngest two siblings. His lyrics were well received, allowing him successfully to enter the show-business world with just one show. Later the same year, the Gershwins collaborated for the first time on a score; this was for "A Dangerous Maid", which played in Atlantic City and on tour.

It was not until 1924 that Ira and George teamed up to write the music for what became their first Broadway hit "Lady, Be Good". Once the brothers joined forces, their combined talents became one of the most influential forces in the history of American Musical Theatre. "When the Gershwins teamed up to write songs for "Lady, Be Good", the American musical found its native idiom." Together, they wrote the music for more than 12 shows and four films. Some of their more famous works include "The Man I Love", "Fascinating Rhythm", "Someone to Watch Over Me", "I Got Rhythm" and "They Can't Take That Away from Me". Their partnership continued until George's sudden death from a brain tumor in 1937. Following his brother's death, Ira waited nearly three years before writing again.

After this temporary retirement, Ira teamed up with accomplished composers such as Jerome Kern ("Cover Girl"); Kurt Weill ("Where Do We Go from Here?"; "Lady in the Dark"); and Harold Arlen (""; "A Star Is Born"). Over the next 14 years, Gershwin continued to write the lyrics for many film scores and a few Broadway shows. But the failure of "Park Avenue" in 1946 (a "smart" show about divorce, co-written with composer Arthur Schwartz) was his farewell to Broadway. As he wrote at the time, "Am reading a couple of stories for possible musicalization (if there is such a word) but I hope I don't like them as I think I deserve a long rest."

In 1947, he took 11 songs George had written but never used, provided them with new lyrics, and incorporated them into the Betty Grable film "The Shocking Miss Pilgrim". He later wrote comic lyrics for Billy Wilder's 1964 movie "Kiss Me, Stupid", although most critics believe his final major work was for the 1954 Judy Garland film "A Star Is Born".

American singer, pianist and musical historian Michael Feinstein worked for Gershwin in the lyricist's latter years, helping him with his archive. Several lost musical treasures were unearthed during this period, and Feinstein performed some of the material. Feinstein's book "The Gershwins and Me: A Personal History in Twelve Songs" about working for Ira, and George and Ira's music was published in 2012.

According to a 1999 story in Vanity Fair, Ira Gershwin's love for loud music was as great as his wife's loathing of it. When Debby Boone—daughter-in-law of his neighbor Rosemary Clooney—returned from Japan with one of the first Sony Walkmans (utilizing cassette tape), Clooney gave it to Michael Feinstein to give to Ira, "so he could crank it in his ears, you know. And he said, 'This is absolutely wonderful!' And he called his broker and bought Sony stock!"

Three of Ira Gershwin's songs ("They Can't Take That Away From Me" (1937), "Long Ago (And Far Away)" (1944) and "The Man That Got Away" (1954)) were nominated for an Academy Award for Best Original Song, though none won.

Along with George S Kaufman and Morrie Ryskind, he was a recipient of the 1932 Pulitzer Prize for Drama for "Of Thee I Sing".

In 1988 UCLA established The George and Ira Gershwin Lifetime Musical Achievement Award in recognition of the brothers' contribution to music, and for their gift to UCLA of the fight song "Strike Up the Band for UCLA". Recipients include Angela Lansbury (1988), Ray Charles (1991), Mel Tormé (1994), Bernadette Peters (1995), Frank Sinatra (2000), Stevie Wonder (2002), k.d. lang (2003), James Taylor (2004), Babyface (2005), Burt Bacharach (2006), Quincy Jones (2007), Lionel Richie (2008) and Julie Andrews (2009).

Ira Gershwin was a joyous listener to the sounds of the modern world. "He had a sharp eye and ear for the minutiae of living." He noted in a diary: "Heard in a day: An elevator's purr, telephone's ring, telephone's buzz, a baby's moans, a shout of delight, a screech from a 'flat wheel', hoarse honks, a hoarse voice, a tinkle, a match scratch on sandpaper, a deep resounding boom of dynamiting in the impending subway, iron hooks on the gutter."

In 1987, Ira's widow, Leonore, established the Ira Gershwin Literacy Center at University Settlement, a century-old institution at 185 Eldridge Street on the Lower East Side, New York City. The center is designed to give English-language programs to primarily Hispanic and Chinese Americans. Ira and his younger brother George spent many after-school hours at the Settlement.

The George and Ira Gershwin Collection and the Ira Gershwin Files from the Law Office of Leonard Saxe are both at the Library of Congress Music Division. The Edward Jablonski and Lawrence D. Stewart Gershwin Collection at the Harry Ransom Humanities Research Center at the University of Texas at Austin holds a number of Ira's manuscripts and other material.

In 2007, the United States Library of Congress named its Prize for Popular Song after him and his brother George. Recognizing the profound and positive effect of American popular music on the world's culture, the prize will be given annually to a composer or performer whose lifetime contributions exemplify the standard of excellence associated with the Gershwins.

He married Leonore (née Strunsky) in 1926. He died in Beverly Hills, California, on 17 August 1983 at the age of 86. He is interred at Westchester Hills Cemetery, Hastings-on-Hudson, New York. Leonore died in 1991.





</doc>
<doc id="15490" url="https://en.wikipedia.org/wiki?curid=15490" title="Indus River">
Indus River

The Indus River (also called the Sindhū) is one of the longest rivers in Asia. It flows through China (western Tibet), India (Ladakh) and Pakistan. Originating in the Tibetan Plateau in the vicinity of Lake Manasarovar, the river runs a course through the Ladakh region of India, towards Gilgit-Baltistan and then flows in a southerly direction along the entire length of Pakistan to merge into the Arabian Sea near the port city of Karachi in Sindh. It is the longest river of Pakistan.

The river has a total drainage area exceeding . Its estimated annual flow stands at around , twice that of the Nile River and three times that of the Tigris and Euphrates rivers combined, making it one of the largest rivers in the world in terms of annual flow. The Zanskar is its left bank tributary in Ladakh. In the plains, its left bank tributary is the Panjnad which itself has five major tributaries, namely, the Chenab, Jhelum, the Ravi, the Beas, and the Sutlej. Its principal right bank tributaries are the Shyok, the Gilgit, the Kabul, the Gomal, and the Kurram. Beginning in a mountain spring and fed with glaciers and rivers in the Himalayan, Karakoram and Hindu Kush ranges, the river supports ecosystems of temperate forests, plains and arid countryside.

The northern part of the Indus Valley, with its tributaries, forms the Punjab region, while the lower course of the river is known as Sindh and ends in a large delta. The river has historically been important to many cultures of the region. The 3rd millennium BC saw the rise of a major urban civilization of the Bronze Age. During the 2nd millennium BC, the Punjab region was mentioned in the hymns of the Hindu Rigveda as "Sapta Sindhu" and the Zoroastrian Avesta as "Hapta Hindu" (both terms meaning "seven rivers"). Early historical kingdoms that arose in the Indus Valley include Gandhāra, and the Ror dynasty of Sauvīra. The Indus River came into the knowledge of the West early in the Classical Period, when King Darius of Persia sent his Greek subject Scylax of Caryanda to explore the river, c. 515 BC.

This river was known to the ancient Indians in Sanskrit as "Sindhu" and the Persians as "Hindu" which was regarded by both of them as "the border river". The variation between the two names is explained by the Old Iranian sound change "*s" > "h", which occurred between 850–600 BCE according to Asko Parpola. From the Persian Achaemenid Empire, the name passed to the Greeks as "Indós" (Ἰνδός). It was adopted by the Romans as "Indus".

Southworth suggests that the name "Sindhu" is derived from "Cintu", the Proto-Dravidian word for date palm, a tree commonly found in Sindh.

The meaning of "Sindhu" as a "large body of water, sea, or ocean" is a later meaning in Classical Sanskrit. A later Persian name for the river was "Darya", which similarly has the connotations of large body of water and sea. Other variants of the name "Sindhu" include Assyrian "Sinda" (as early as the 7th century BC), Persian "Ab-e-sind", Pashto "Abasind", Arab "Al-Sind", Chinese "Sintow", and Javanese "Santri".

In other languages of the region, the river is known as (Darya-ī Sindh) in Urdu सिन्धु ("Sindhu") in Hindi, سنڌو ("Sindhu") in Sindhi, ("Sindh") in Shahmukhi Punjabi, ਸਿੰਧ ਨਦੀ ("Sindh Nadī") in Gurmukhī Punjabi, اباسين ("Abāsin" lit. "Father of Rivers") in Pashto, نهر السند ("Nahar al-Sind") in Arabic, སེང་གེ་གཙང་པོ། ("singi khamban" lit. "Lion River" or "Lion Spring") in Tibetan, ("Yìndù") in Chinese, "Nilab" in Turki and සින්දු නදී ("Sindhu Nadi") in Sinhalese.

The Indus River provides key water resources for Pakistan's economy – especially the "breadbasket" of Punjab province, which accounts for most of the nation's agricultural production, and Sindh. The word Punjab means "land of five rivers" and the five rivers are Jhelum, Chenab, Ravi, Beas and Sutlej, all of which finally flow into the Indus. The Indus also supports many heavy industries and provides the main supply of potable water in Pakistan.

The ultimate source of the Indus is in Tibet; the river begins at the confluence of the Sengge Zangbo and Gar Tsangpo rivers that drain the Nganglong Kangri and Gangdise Shan (Gang Rinpoche, Mt. Kailash) mountain ranges. The Indus then flows northwest through Ladakh, India, and Baltistan into Gilgit, just south of the Karakoram range. The Shyok, Shigar and Gilgit rivers carry glacial waters into the main river. It gradually bends to the south and descends into the Punjab plains at Kalabagh, Pakistan. The Indus passes gigantic gorges deep near the Nanga Parbat massif. It flows swiftly across Hazara and is dammed at the Tarbela Reservoir. The Kabul River joins it near Attock. The remainder of its route to the sea is in the plains of the Punjab and Sindh, where the flow of the river becomes slow and highly braided. It is joined by the Panjnad at Mithankot. Beyond this confluence, the river, at one time, was named the "Satnad River" ("sat" = "seven", "nadī" = "river"), as the river now carried the waters of the Kabul River, the Indus River and the five Punjab rivers. Passing by Jamshoro, it ends in a large delta to the South of Thatta in the Sindh province of Pakistan

The Indus is one of the few rivers in the world to exhibit a tidal bore. The Indus system is largely fed by the snows and glaciers of the Himalayas, Karakoram and the Hindu Kush ranges of Tibet, the Indian states and union territories of Ladakh and Himachal Pradesh and the Gilgit-Baltistan region of Pakistan. The flow of the river is also determined by the seasons – it diminishes greatly in the winter, while flooding its banks in the monsoon months from July to September. There is also evidence of a steady shift in the course of the river since prehistoric times – it deviated westwards from flowing into the Rann of Kutch and adjoining Banni grasslands after the 1816 earthquake. Presently, Indus water flows in to the Rann of Kutch during its floods breaching flood banks.

The traditional source of the river is the "Sênggê Kanbab" (a.k.a. Sênggê Zangbo, Senge Khabab) or "Lion's Mouth", a perennial spring, not far from the sacred Mount Kailash marked by a long low line of Tibetan chortens. There are several other tributaries nearby, which may possibly form a longer stream than Sênggê Kanbab, but unlike the Sênggê Kanbab, are all dependent on snowmelt. The Zanskar River, which flows into the Indus in Ladakh, has a greater volume of water than the Indus itself before that point.

The major cities of the Indus Valley Civilisation, such as Harappa and Mohenjo-daro, date back to around 3300 BC, and represent some of the largest human habitations of the ancient world. The Indus Valley Civilisation extended from across northeast Afghanistan to Pakistan and northwest India, with an upward reach from east of Jhelum River to Ropar on the upper Sutlej. The coastal settlements extended from Sutkagan Dor at the Pakistan, Iran border to Kutch in modern Gujarat, India. There is an Indus site on the Amu Darya at Shortughai in northern Afghanistan, and the Indus site Alamgirpur at the Hindon River is located only from Delhi. To date, over 1,052 cities and settlements have been found, mainly in the general region of the Ghaggar-Hakra River and its tributaries. Among the settlements were the major urban centres of Harappa and Mohenjo-daro, as well as Lothal, Dholavira, Ganeriwala, and Rakhigarhi. Only 90–96 of more than 800 known Indus Valley sites have been discovered on the Indus and its tributaries. The Sutlej, now a tributary of the Indus, in Harappan times flowed into the Ghaggar-Hakra River, in the watershed of which were more Harappan sites than along the Indus.

Most scholars believe that settlements of Gandhara grave culture of the early Indo-Aryans flourished in Gandhara from 1700 BC to 600 BC, when Mohenjo-daro and Harappa had already been abandoned.

The Rigveda describes several rivers, including one named "Sindhu". The Rigvedic "Sindhu" is thought to be the present-day Indus river. It is attested 176 times in its text, 94 times in the plural, and most often used in the generic sense of "river". In the Rigveda, notably in the later hymns, the meaning of the word is narrowed to refer to the Indus river in particular, e.g. in the list of rivers mentioned in the hymn of "Nadistuti sukta". The Rigvedic hymns apply a feminine gender to all the rivers mentioned therein, except for the Brahmaputra.

The word "India" is derived from the Indus River. In ancient times, "India" initially referred to those regions immediately along the east bank of the Indus, but by 300 BC, Greek writers including Herodotus and Megasthenes were applying the term to the entire subcontinent that extends much farther eastward.

The lower basin of the Indus forms a natural boundary between the Iranian Plateau and the Indian subcontinent; this region embraces all or parts of the Pakistani provinces Balochistan, Khyber Pakhtunkhwa, Punjab and Sindh and the countries Afghanistan and India. The first West Eurasian empire to annex the Indus Valley was the Persian Empire, during the reign of Darius the Great. During his reign, the Greek explorer Scylax of Caryanda was commissioned to explore the course of the Indus. It was crossed by the invading armies of Alexander, but after his Macedonians conquered the west bank—joining it to the Hellenic world, they elected to retreat along the southern course of the river, ending Alexander's Asian campaign. Alexander's admiral Nearchus set out from the Indus Delta to explore the Persian Gulf, until reaching the Tigris River. The Indus Valley were later dominated by the Mauryan and Kushan Empires, Indo-Greek Kingdoms, Indo-Scythians and Hepthalites. Over several centuries Muslim armies of Muhammad bin Qasim, Mahmud of Ghazni, Mohammed Ghori, Tamerlane and Babur crossed the river to invade Sindh and Punjab, providing a gateway to the Indian subcontinent.

The Indus river feeds the Indus submarine fan, which is the second largest sediment body on the Earth. It consists of around 5 million cubic kilometres of material eroded from the mountains. Studies of the sediment in the modern river indicate that the Karakoram Mountains in northern Pakistan and India are the single most important source of material, with the Himalayas providing the next largest contribution, mostly via the large rivers of the Punjab (Jhelum, Ravi, Chenab, Beas and Sutlej). Analysis of sediments from the Arabian Sea has demonstrated that prior to five million years ago the Indus was not connected to these Punjab rivers which instead flowed east into the Ganga and were captured after that time. Earlier work showed that sand and silt from western Tibet was reaching the Arabian Sea by 45 million years ago, implying the existence of an ancient Indus River by that time. The delta of this proto-Indus river has subsequently been found in the Katawaz Basin, on the Afghan-Pakistan border.

In the Nanga Parbat region, the massive amounts of erosion due to the Indus river following the capture and rerouting through that area is thought to bring middle and lower crustal rocks to the surface.

In November 2011, satellite images showed that the Indus river had re-entered India, feeding Great Rann of Kutch, Little Rann of Kutch and a lake near Ahmedabad known as Nal Sarovar. Heavy rains had left the river basin along with the Lake Manchar, Lake Hemal and Kalri Lake (all in modern-day Pakistan) inundated. This happened two centuries after the Indus river shifted its course westwards following the 1819 Rann of Kutch earthquake.

The Induan Age at start of the Triassic Period of geological time is named for the Indus region.

Accounts of the Indus valley from the times of Alexander's campaign indicate a healthy forest cover in the region, which has now considerably receded. The Mughal Emperor Babur writes of encountering rhinoceroses along its bank in his memoirs (the Baburnama). Extensive deforestation and human interference in the ecology of the Shivalik Hills has led to a marked deterioration in vegetation and growing conditions. The Indus valley regions are arid with poor vegetation. Agriculture is sustained largely due to irrigation works.
The Indus river and its watershed has a rich biodiversity. It is home to around 25 amphibian species.

The Indus river dolphin ("Platanista indicus minor") is found only in the Indus River. It is subspecies of the South Asian river dolphin. The Indus river dolphin formerly also occurred in the tributaries of the Indus river. According to the World Wildlife Fund it is one of the most threatened cetaceans with only about 1,000 still existing.

There are two otter species in the Indus River basin: the Eurasian otter in the northeastern highland sections and the smooth-coated otter elsewhere in the river basin. The smooth-coated otters in the Indus River represent a subspecies found nowhere else, the Sindh otter ("Lutrogale perspicillata sindica").

The Indus River basin has a high diversity, being the home of more than 180 freshwater fish species, including 22 which are found nowhere else. Fish also played a major role in earlier cultures of the region, including the ancient Indus Valley Civilisation where depictions of fish were frequent. The Indus script has a commonly used fish sign, which in its various forms may simply have meant "fish", or referred to stars or gods.

In the uppermost, highest part of the Indus River basin there are relatively few genera and species: "Diptychus", "Ptychobarbus", "Schizopyge", "Schizopygopsis" and "Schizothorax" snowtrout, "Triplophysa" loaches, and the catfish "Glyptosternon reticulatum". Going downstream these are soon joined by the golden mahseer "Tor putitora" (alternatively "T. macrolepis", although it often is regarded as a synonym of "T. putitora") and "Schistura" loaches. Downriver from around Thakot, Tarbela, the Kabul–Indus river confluence, Attock Khurd and Peshawar the diversity rises strongly, including many cyprinids ("Amblypharyngodon", "Aspidoparia", "Barilius", "Chela", "Cirrhinus", "Crossocheilus", "Cyprinion", "Danio", "Devario", "Esomus", "Garra", "Labeo", "Naziritor", "Osteobrama", "Pethia", "Puntius", "Rasbora", "Salmophasia", "Securicula" and "Systomus"), true loaches ("Botia" and "Lepidocephalus"), stone loaches ("Acanthocobitis" and "Nemacheilus"), ailiid catfish ("Clupisoma"), bagridae catfish ("Batasio", "Mystus", "Rita" and "Sperata"), airsac catfish ("Heteropneustes"), schilbid catfish ("Eutropiichthys"), silurid catfish ("Ompok" and "Wallago"), sisorid catfish ("Bagarius", "Gagata", "Glyptothorax" and "Sisor"), gouramis ("Trichogaster"), nandid leaffish ("Nandus"), snakeheads ("Channa"), spiny eel ("Macrognathus" and "Mastacembelus"), knifefish ("Notopterus"), glassfish ("Chanda" and "Parambassis"), clupeids ("Gudusia"), needlefish ("Xenentodon") and gobies ("Glossogobius"), as well as a few introduced species. As the altitude further declines the Indus basin becomes overall quite slow-flowing as it passes through the Punjab Plain. Major carp become common, and chameleonfish ("Badis"), mullet ("Sicamugil") and swamp eel ("Monopterus") appear. In some upland lakes and tributaries of the Punjab region snowtrout and mahseer are still common, but once the Indus basin reaches its lower plain the former group is entirely absent and the latter are rare. Many of the species of the middle sections of the Indus basin are also present in the lower. Notable examples of genera that are present in the lower plain but generally not elsewhere in the Indus River basin are the "Aphanius" pupfish, "Aplocheilus" killifish, palla fish ("Tenualosa ilisha"), catla ("Labeo catla"), rohu ("Labeo rohita") and "Cirrhinus mrigala". The lowermost part of the river and its delta are home to freshwater fish, but also a number of brackish and marine species. This includes including pomfret and prawns. The large delta has been recognized by conservationists as an important ecological region. Here, the river turns into many marshes, streams and creeks and meets the sea at shallow levels.

Palla fish ("Tenualosa ilisha") of the river is a delicacy for people living along the river. The population of fish in the river is moderately high, with Sukkur, Thatta, and Kotri being the major fishing centres – all in the lower Sindh course. As a result, damming and irrigation has made fish farming an important economic activity.

The Indus is the most important supplier of water resources to the Punjab and Sindh plains – it forms the backbone of agriculture and food production in Pakistan. The river is especially critical since rainfall is meagre in the lower Indus valley. Irrigation canals were first built by the people of the Indus Valley Civilisation, and later by the engineers of the Kushan Empire and the Mughal Empire. Modern irrigation was introduced by the British East India Company in 1850 – the construction of modern canals accompanied with the restoration of old canals. The British supervised the construction of one of the most complex irrigation networks in the world. The Guddu Barrage is long – irrigating Sukkur, Jacobabad, Larkana and Kalat. The Sukkur Barrage serves over .

After Pakistan came into existence, a water control treaty signed between India and Pakistan in 1960 guaranteed that Pakistan would receive water from the Indus River and its two tributaries the Jhelum River & the Chenab River independently of upstream control by India.

The Indus Basin Project consisted primarily of the construction of two main dams, the Mangla Dam built on the Jhelum River and the Tarbela Dam constructed on the Indus River, together with their subsidiary dams. The Pakistan Water and Power Development Authority undertook the construction of the Chashma-Jhelum link canal – linking the waters of the Indus and Jhelum rivers – extending water supplies to the regions of Bahawalpur and Multan. Pakistan constructed the Tarbela Dam near Rawalpindi – standing long and high, with an long reservoir. It supports the Chashma Barrage near Dera Ismail Khan for irrigation use and flood control and the Taunsa Barrage near Dera Ghazi Khan which also produces 100,000 kilowatts of electricity. The Kotri Barrage near Hyderabad is long and provides additional water supplies for Karachi. The extensive linking of tributaries with the Indus has helped spread water resources to the valley of Peshawar, in the Khyber Pakhtunkhwa. The extensive irrigation and dam projects provide the basis for Pakistan's large production of crops such as cotton, sugarcane and wheat. The dams also generate electricity for heavy industries and urban centers.

Indus river is sacred to Hindus.The inhabitants of the regions are mainly Muslim as Pakistan is an Islamic country through which the Indus river passes and forms a major natural feature and resource are diverse in ethnicity, religion, national and linguistic backgrounds. On the northern course of the river in the union territory of Ladakh in India, live the Buddhist people of Ladakh, of Tibetan stock, and the Dards of Indo-Aryan or Dardic stock and practising Islam. Then it descends into Baltistan, northern Pakistan passing the main Balti city of Skardu. A river from Dubair Bala also drains into it at Dubair Bazar. People living in this area are mainly Kohistani and speak the Kohistani language. Major areas through which the Indus river passes in Kohistan are Dasu, Pattan and Dubair. As it continues through Pakistan, the Indus river forms a distinctive boundary of ethnicity and cultures – upon the western banks the population is largely Pashtun, Baloch, and of other Iranian stock. The eastern banks are largely populated by people of Indo-Aryan stock, such as the Punjabis and the Sindhis. In northern Punjab and the Khyber Pakhtunkhwa, ethnic Pashtun tribes live alongside Dardic people in the hills (Khowar, Kalash, Shina, etc.), Burushos (in Hunza), and Punjabi people.

The people living along the Indus river speak Punjabi and Sindhi on the eastern side (in Punjab and Sindh provinces respectively), Pushto plus Balochi as well as Barohi (in Khyber Pakhtoonkha and Baluchistan provinces). In the province of Sindh, the upper third of the river is inhabited by people speaking Saraiki; which is a somewhat transitional dialect of the Punjabi and Sindhi languages.

The ethnicities of the Indus Valley (Pakistan and Northwest India) have a greater amount of ANI (or West Eurasian) admixture than other South Asians, including inputs from Western Steppe Herders, with evidence of more sustained and multi-layered migrations from the west.

The Indus is a strategically vital resource for Pakistan's economy and society. After Pakistan and India declared Independence from the British Raj, the use of the waters of the Indus and its five eastern tributaries became a major dispute between India and Pakistan. The irrigation canals of the Sutlej valley and the Bari Doab were split – with the canals lying primarily in Pakistan and the headwork dams in India disrupting supply in some parts of Pakistan. The concern over India building large dams over various Punjab rivers that could undercut the supply flowing to Pakistan, as well as the possibility that India could divert rivers in the time of war, caused political consternation in Pakistan. Holding diplomatic talks brokered by the World Bank, India and Pakistan signed the Indus Waters Treaty in 1960. The treaty gave India control of the three easternmost rivers of the Punjab, the Sutlej, the Beas and the Ravi, while Pakistan gained control of the three western rivers, the Jhelum, the Chenab and the Indus. India retained the right to use of the western rivers for non-irrigation projects.

Large-scale diversion of the river's water for irrigation has raised far-reaching issues. Sediment clogging from poor maintenance of canals has affected agricultural production and vegetation on numerous occasions. Irrigation itself is increasing soil salinization, reducing crop yields and in some cases rendering farmland useless for cultivation. And ecologically, the reduced flow of fresh water and silt into the Indus delta is threatening the area's mangrove forests.

There are also concerns that the Indus River may be shifting its course westwards, although the progression spans centuries.

Originally, the delta used to receive almost all of the water from the Indus river, which has an annual flow of approximately , and is accompanied by of silt. Since the 1940s, dams, barrages and irrigation works have been constructed on the river Indus. The Indus Basin Irrigation System is the "largest contiguous irrigation system developed over the past 140 years" anywhere in the world. This has reduced the flow of water and by 2018, the average annual flow of water below the Kotri barrage was , and annual amount of silt discharged was estimated at . As a result, the 2010 Pakistan floods were considered "good news" for the ecosystem and population of the river delta as they brought much needed fresh water. Any further utilization of the river basin water is not economically feasible.

Vegetation and wildlife of the Indus delta are threatened by the reduced inflow of fresh water, along with extensive deforestation, industrial pollution and global warming. Damming has also isolated the delta population of Indus river dolphins from those further upstream.

The Tibetan Plateau contains the world's third-largest store of ice. Qin Dahe, the former head of the China Meteorological Administration, said the recent fast pace of melting and warmer temperatures will be good for agriculture and tourism in the short term, but issued a strong warning:

"There is insufficient data to say what will happen to the Indus," says David Grey, the World Bank's senior water advisor in South Asia. "But we all have very nasty fears that the flows of the Indus could be severely, severely affected by glacier melt as a consequence of climate change," and reduced by perhaps as much as 50 percent. "Now what does that mean to a population that lives in a desert [where], without the river, there would be no life? I don't know the answer to that question," he says. "But we need to be concerned about that. Deeply, deeply concerned."

U.S. diplomat Richard Holbrooke said, shortly before his death in 2010, that he believed that falling water levels in the Indus River "could very well precipitate World War III."

Over the years factories on the banks of the Indus River have increased levels of water pollution in the river and the atmosphere around it. High levels of pollutants in the river have led to the deaths of endangered Indus river dolphin. The Sindh Environmental Protection Agency has ordered polluting factories around the river to shut down under the Pakistan Environmental Protection Act, 1997. Death of the Indus river dolphin has also been attributed to fishermen using poison to kill fish and scooping them up. As a result, the government banned fishing from Guddu Barrage to Sukkur.

The Indus is second among a group of ten rivers responsible for about 90% of all the plastic that reaches the oceans. The Yangtze is the only river contributing more plastic.

Frequently, Indus river is prone to moderate to severe flooding. In July 2010, following abnormally heavy monsoon rains, the Indus River rose above its banks and started flooding. The rain continued for the next two months, devastating large areas of Pakistan. In Sindh, the Indus burst its banks near Sukkur on 8 August, submerging the village of Mor Khan Jatoi. In early August, the heaviest flooding moved southward along the Indus River from severely affected northern regions toward western Punjab, where at least of cropland was destroyed, and the southern province of Sindh. , over two thousand people had died and over a million homes had been destroyed since the flooding began.

The 2011 Sindh floods began during the Pakistani monsoon season in mid-August 2011, resulting from heavy monsoon rains in Sindh, eastern Balochistan, and southern Punjab. The floods caused considerable damage; an estimated 434 civilians were killed, with 5.3 million people and 1,524,773 homes affected. Sindh is a fertile region and often called the "breadbasket" of the country; the damage and toll of the floods on the local agrarian economy was said to be extensive. At least of arable land were inundated. The flooding followed the previous year's floods, which devastated a large part of the country. Unprecedented torrential monsoon rains caused severe flooding in 16 districts of Sindh.

In Pakistan currently there are six barrages on the Indus: Guddu barrage, Sukkur Barrage, Kotri barrage (also called Ghulam Muhammad barrage), Taunsa Barrage, Chashma Barrage and Jinnah Barrage. Another new barrage called "Sindh barrage" is planned as terminal barrage on Indus River. There are some bridges on river Indus, such as, Dadu Moro Bridge, Larkana Khairpur Indus River Bridge, Thatta-Sujawal bridge, Jhirk-Mula Katiar bridge and recently planned Kandhkot-Ghotki bridge.

The entire left bank of Indus river in Sind province is protected from river flooding by constructing around 600 km long levees. The right bank side is also leveed from Guddu barrage to Lake Manchar. In response to the levees construction, the river has been aggrading rapidly over the last 20 years leading to breaches upstream of barrages and inundation of large areas.

Tarbela Dam in Pakistan is constructed on the Indus River, while the controversial Kalabagh dam is also being constructed on Indus river. Pakistan is also building Munda Dam.



</doc>
<doc id="15491" url="https://en.wikipedia.org/wiki?curid=15491" title="Integer factorization">
Integer factorization

In number theory, integer factorization is the decomposition of a composite number into a product of smaller integers. If these factors are further restricted to prime numbers, the process is called prime factorization.

When the numbers are sufficiently large, no efficient, non-quantum integer factorization algorithm is known. In 2019, Fabrice Boudot, Pierrick Gaudry, Aurore Guillevic, Nadia Heninger, Emmanuel Thomé and Paul Zimmermann factored a 240-digit number (RSA-240) utilizing approximately 900 core-years of computing power. The researchers estimated that a 1024-bit RSA modulus would take about 500 times as long. However, it has not been proven that no efficient algorithm exists. The presumed difficulty of this problem is at the heart of widely used algorithms in cryptography such as RSA. Many areas of mathematics and computer science have been brought to bear on the problem, including elliptic curves, algebraic number theory, and quantum computing.

Not all numbers of a given length are equally hard to factor. The hardest instances of these problems (for currently known techniques) are semiprimes, the product of two prime numbers. When they are both large, for instance more than two thousand bits long, randomly chosen, and about the same size (but not too close, for example, to avoid efficient factorization by Fermat's factorization method), even the fastest prime factorization algorithms on the fastest computers can take enough time to make the search impractical; that is, as the number of digits of the primes being factored increases, the number of operations required to perform the factorization on any computer increases drastically.

Many cryptographic protocols are based on the difficulty of factoring large composite integers or a related problem—for example, the RSA problem. An algorithm that efficiently factors an arbitrary integer would render RSA-based public-key cryptography insecure.

By the fundamental theorem of arithmetic, every positive integer has a unique prime factorization. (By convention, 1 is the empty product.) Testing whether the integer is prime can be done in polynomial time, for example, by the AKS primality test. If composite, however, the polynomial time tests give no insight into how to obtain the factors.

Given a general algorithm for integer factorization, any integer can be factored into its constituent prime factors by repeated application of this algorithm. The situation is more complicated with special-purpose factorization algorithms, whose benefits may not be realized as well or even at all with the factors produced during decomposition. For example, if where are very large primes, trial division will quickly produce the factors 3 and 19 but will take "p" divisions to find the next factor. As a contrasting example, if "N" is the product of the primes 13729, 1372933, and 18848997161, where , Fermat's factorization method will begin with which immediately yields and hence the factors and . While these are easily recognized as composite and prime respectively, Fermat's method will take much longer to factor the composite number because the starting value of for "a" is nowhere near 1372933.

Among the "b"-bit numbers, the most difficult to factor in practice using existing algorithms are those that are products of two primes of similar size. For this reason, these are the integers used in cryptographic applications. The largest such semiprime yet factored was RSA-250, a 829-bit number with 250 decimal digits, in February 2020. The total computation time was roughly 2700 core-years of computing using Intel Xeon Gold 6130 at 2.1 GHz. Like all recent factorization records, this factorization was completed with a highly optimized implementation of the general number field sieve run on hundreds of machines.

No algorithm has been published that can factor all integers in polynomial time, that is, that can factor "b"-bit numbers in time O("b") for some constant "k". Neither the existence nor non-existence of such algorithms has been proved, but it is generally suspected that they do not exist and hence that the problem is not in class P. The problem is clearly in class NP but has not been proved to be or not be NP-complete. It is generally suspected not to be NP-complete.

There are published algorithms that are faster than O((1 + "ε")) for all positive "ε", that is, sub-exponential. The best published asymptotic running time is for the general number field sieve (GNFS) algorithm, which, for a "b"-bit number "n", is

For current computers, GNFS is the best published algorithm for large "n" (more than about 400 bits). For a quantum computer, however, Peter Shor discovered an algorithm in 1994 that solves it in polynomial time. This will have significant implications for cryptography if quantum computation becomes scalable. Shor's algorithm takes only time and O("b") space on "b"-bit number inputs. In 2001, Shor's algorithm was implemented for the first time, by using NMR techniques on molecules that provide 7 qubits.

When discussing what complexity classes the integer factorization problem falls into, it is necessary to distinguish two slightly different versions of the problem:

For , the decision problem is equivalent to asking whether "N" is not prime. 

An algorithm for either version provides one for the other. Repeated application of the function problem (applied to "d" and "N"/"d", and their factors, if needed) will eventually provide either a factor of "N" no larger than "M" or a factorization into primes all greater than "M". All known algorithms for the decision problem work in this way. Hence it is only of theoretical interest that, with at most log "N" queries using an algorithm for the decision problem, one would isolate a factor of "N" (or prove it prime) by binary search. 

It is not known exactly which complexity classes contain the decision version of the integer factorization problem. It is known to be in both NP and co-NP. This is because both "yes" and "no" answers can be verified in polynomial time. An answer of "yes" can be certified by exhibiting a factorization with . An answer of "no" can be certified by exhibiting the factorization of "N" into distinct primes, all larger than "M". We can verify their primality using the AKS primality test and that their product is "N" by multiplication. The fundamental theorem of arithmetic guarantees that there is only one possible string that will be accepted (providing the factors are required to be listed in order), which shows that the problem is in both UP and co-UP. It is known to be in BQP because of Shor's algorithm. It is suspected to be outside of all three of the complexity classes P, NP-complete, and co-NP-complete. It is therefore a candidate for the NP-intermediate complexity class. If it could be proved that it is in either NP-complete or co-NP-complete, that would imply NP = co-NP. That would be a very surprising result, and therefore integer factorization is widely suspected to be outside both of those classes. Many people have tried to find classical polynomial-time algorithms for it and failed, and therefore it is widely suspected to be outside P.

In contrast, the decision problem "is "N" a composite number?" (or equivalently: "is "N" a prime number?") appears to be much easier than the problem of actually finding the factors of "N". Specifically, the former can be solved in polynomial time (in the number "n" of digits of "N") with the AKS primality test. In addition, there are a number of probabilistic algorithms that can test primality very quickly in practice if one is willing to accept the vanishingly small possibility of error. The ease of primality testing is a crucial part of the RSA algorithm, as it is necessary to find large prime numbers to start with.

A special-purpose factoring algorithm's running time depends on the properties of the number to be factored or on one of its unknown factors: size, special form, etc. Exactly what the running time depends on varies between algorithms.

An important subclass of special-purpose factoring algorithms is the "Category 1" or "First Category" algorithms, whose running time depends on the size of smallest prime factor. Given an integer of unknown form, these methods are usually applied before general-purpose methods to remove small factors. For example, trial division is a Category 1 algorithm.


A general-purpose factoring algorithm, also known as a "Category 2", "Second Category", or "Kraitchik family" algorithm (after Maurice Kraitchik), has a running time which depends solely on the size of the integer to be factored. This is the type of algorithm used to factor RSA numbers. Most general-purpose factoring algorithms are based on the congruence of squares method.



In number theory, there are many integer factoring algorithms that heuristically have expected running time

in big O and L-notation.
Some examples of those algorithms are the elliptic curve method and the quadratic sieve.
Another such algorithm is the class group relations method proposed by Schnorr, Seysen, and Lenstra, that is proved under the assumption of the Generalized Riemann Hypothesis (GRH).

The Schnorr–Seysen–Lenstra probabilistic algorithm has been rigorously proven by Lenstra and Pomerance to have expected running time formula_3 by replacing the GRH assumption with the use of multipliers.
The algorithm uses the class group of positive binary quadratic forms of discriminant Δ denoted by "G".
"G" is the set of triples of integers ("a", "b", "c") in which those integers are relative prime.

Given an integer "n" that will be factored, where "n" is an odd positive integer greater than a certain constant. In this factoring algorithm the discriminant Δ is chosen as a multiple of "n", , where "d" is some positive multiplier. The algorithm expects that for one "d" there exist enough smooth forms in "G". Lenstra and Pomerance show that the choice of "d" can be restricted to a small set to guarantee the smoothness result.

Denote by "P" the set of all primes "q" with Kronecker symbol formula_4. By constructing a set of generators of "G" and prime forms "f" of "G" with "q" in "P" a sequence of relations between the set of generators and "f" are produced.
The size of "q" can be bounded by formula_5 for some constant formula_6.

The relation that will be used is a relation between the product of powers that is equal to the neutral element of "G". These relations will be used to construct a so-called ambiguous form of "G", which is an element of "G" of order dividing 2. By calculating the corresponding factorization of Δ and by taking a gcd, this ambiguous form provides the complete prime factorization of "n". This algorithm has these main steps:

Let "n" be the number to be factored.

To obtain an algorithm for factoring any positive integer, it is necessary to add a few steps to this algorithm such as trial division, and the Jacobi sum test.

The algorithm as stated is a probabilistic algorithm as it makes random choices. Its expected running time is at most formula_3.





</doc>
<doc id="15492" url="https://en.wikipedia.org/wiki?curid=15492" title="Imperial units">
Imperial units

The imperial system of units, imperial system or imperial units (also known as British Imperial or Exchequer Standards of 1825) is the system of units first defined in the British Weights and Measures Act 1824 and continued to be developed through a series of Weights and Measures Acts and amendments. The imperial units replaced the Winchester Standards, which were in effect from 1588 to 1825. The system came into official use across the British Empire in 1826. By the late 20th century, most nations of the former empire had officially adopted the metric system as their main system of measurement but imperial units are still used in the United Kingdom, Canada and other countries formerly part of the British Empire. The imperial system developed from what were first known as English units, as did the related system of United States customary units.

The modern legislation defining the imperial system of units is given in the Weights and Measures Act 1985 (as amended).

The Weights and Measures Act of 1824 was initially scheduled to go into effect on 1 May 1825. The Weights and Measures Act of 1825 pushed back the date to 1 January 1826. The 1824 Act allowed the continued use of pre-imperial units provided that they were customary, widely known, and clearly marked with imperial equivalents.

Apothecaries' units are mentioned neither in the act of 1824 nor 1825. At the time, apothecaries' weights and measures were regulated "in England, Wales, and Berwick-upon-Tweed" by the London College of Physicians, and in Ireland by the Dublin College of Physicians. In Scotland, apothecaries' units were unofficially regulated by the Edinburgh College of Physicians. The three colleges published, at infrequent intervals, pharmacopœiae, the London and Dublin editions having the force of law.

Imperial apothecaries' measures, based on the imperial pint of 20 fluid ounces, were introduced by the publication of the London Pharmacopoeia of 1836, the Edinburgh Pharmacopoeia of 1839, and the Dublin Pharmacopoeia of 1850. The Medical Act of 1858 transferred to The Crown the right to publish the official pharmacopoeia and to regulate apothecaries' weights and measures.

Metric equivalents in this article usually assume the latest official definition. Before this date, the most precise measurement of the imperial Standard Yard was metres.

The Weights and Measures Act 1824 invalidated the various different gallons in use in the British Empire, declaring them to be replaced by the statute gallon (which became known as the Imperial Gallon), a unit close in volume to the ale gallon. The 1824 Act defined as the volume of a gallon to be that of of distilled water weighed in air with brass weights with the barometer standing at at a temperature of . The 1824 Act went on to give this volume as . The Weights and Measures Act 1963 refined this definition to be the volume of 10 pounds of distilled water of density weighed in air of density against weights of density , which works out to . The Weights and Measures Act 1985 defined a gallon to be exactly (approximately ).

These measurements were in use from 1826, when the new imperial gallon was defined, but were officially abolished in the United Kingdom on 1 January 1971. In the US, though no longer recommended, the apothecaries' system is still used occasionally in medicine, especially in prescriptions for older medications.
In the 19th and 20th centuries, the UK used three different systems for mass and weight.


The distinction between mass and weight is not always clearly drawn. Strictly a pound is a unit of mass, but it is commonly referred to as a weight. When a distinction is necessary, the term "pound-force" may refer to a unit of force rather than mass. The troy pound () was made the primary unit of mass by the 1824 Act and its use was abolished in the UK on 1 January 1879, with only the troy ounce () and its decimal subdivisions retained. The "Weights and Measures Act 1855" (18 & 19 Victoria C72) made the avoirdupois pound the primary unit of mass. In all the systems, the fundamental unit is the pound, and all other units are defined as fractions or multiples of it.

The 1824 Act of Parliament defined the yard and pound by reference to the prototype standards, and it also defined the values of certain physical constants, to make provision for re-creation of the standards if they were to be damaged. For the yard, the length of a pendulum beating seconds at the latitude of Greenwich at Mean Sea Level "in vacuo" was defined as inches. For the pound, the mass of a cubic inch of distilled water at an atmospheric pressure of 30 inches of mercury and a temperature of 62° Fahrenheit was defined as 252.458 grains, with there being 7,000 grains per pound.

Following the destruction of the original prototypes in the 1834 Houses of Parliament fire, it proved impossible to recreate the standards from these definitions, and a new Weights and Measures Act (18 & 19 Victoria. Cap. 72) was passed in 1855 which permitted the recreation of the prototypes from recognized secondary standards.

The imperial system is one of many systems of English units. Most of the units are defined in more than one system, and some subsidiary units were used to a much greater extent, or for different purposes, in one area rather than another. The distinctions between the systems are often not drawn precisely.

One such distinction is that between the Imperial system and older British/English units/systems or newer additions. The term "imperial" should not be applied to English units that were outlawed by the Weights and Measures Act 1824 or earlier, or which had fallen out of use by that time, nor to post-imperial inventions, such as the slug or poundal.

The US customary system is derived from those English units which were in use at the time of the original settlement of North America from England in the 17th century. And because the United States had achieved independence from Britain prior to the 19th Century, US customary units were unaffected by Britain's introduction of the Imperial system in 1834.

Since the Weights and Measures Act 1985, British law defines base imperial units in terms of their metric equivalent and declares that none may be used "for trade" (except that milk, beer and cider may also be sold by the pint). The metric system is routinely used in business and technology within the United Kingdom, with Imperial units remaining in widespread use amongst the public. All UK roads use the imperial system except for weight limits, and newer height or width restriction signs give metric alongside imperial.

Units of measurement regulations require all measuring devices used in trade or retail to display measurements in metric quantities. Almost all traders in the UK will accept requests from customers specified in imperial units, and scales which display in both unit systems are commonplace in the retail trade. Metric price signs may be accompanied by imperial price signs provided that the imperial signs are no larger and no more prominent than the metric ones.

The United Kingdom completed its official partial transition to the metric system in 1995, with imperial units still legally mandated for certain applications such as draught beer and cider, and road-signs. Therefore, the speedometers on vehicles sold in the UK must be capable of displaying miles per hour. Even though the troy pound was outlawed in the UK in the Weights and Measures Act of 1878, the "troy ounce" "may" still be used for the weights of precious stones and metals. The original railways (many built in the Victorian era) are a big user of imperial units, with distances officially measured in miles and yards or miles and chains, and also feet and inches, and speeds are in miles per hour. More recent systems like tram networks and the London Underground use metric.

Most British people still use imperial units in everyday life for distance (miles, yards, feet and inches) and volume in some cases (especially milk and beer in pints) but rarely for canned or bottled soft drinks or petrol. Though use of kilograms is increasing, many British people also still use imperial units in everyday life for body weight (stones and pounds for adults, pounds and ounces for babies). Government documents aimed at the public may give body weight and height in imperial units as well as in metric. A survey in 2015 found that many people did not know their body weight or height in one system or the other. People under the age of 40 preferred the metric system but people aged 40 and over preferred the imperial system. The height of horses in English-speaking countries, including Australia, Canada, the United Kingdom and the United States is usually measured in hands, standardized to 4 inches (101.6 mm). Fuel consumption for vehicles is commonly stated in miles per gallon (mpg), though official figures always include litres per 100 km equivalents and fuel is sold in litres. When sold draught in licensed premises, beer and cider must be sold in pints, half-pints and third-pints. Cow's milk is available in both litre- and pint-based containers in supermarkets and shops. Areas of land associated with farming, forestry and real estate are commonly advertised in acres and square feet but, for contracts and land registration purposes, the units are always hectares and square metres.

Office space and industrial units are usually advertised in square feet. Steel pipe sizes are sold in increments of inches, while copper pipe is sold in increments of millimetres. Road bicycles have their frames measured in centimetres, while off-road bicycles have their frames measured in inches. The size (diagonal) of television and computer monitor screens is always denominated in inches. Food sold by length or width, e.g. pizzas or sandwiches, is generally sold in inches. Clothing is always sized in inches, with the metric equivalent often shown as a small supplementary indicator. Gas is usually measured by the cubic foot or cubic metre, but is billed like electricity by the kilowatt hour.

Pre-packaged products can show both metric and imperial measures, and it is also common to see imperial pack sizes with metric only labels, e.g. a tin of Lyle's Golden Syrup is always labelled 454 g with no imperial indicator. Similarly most jars of jam and packs of sausages are labelled 454 g with no imperial indicator. But, whatever the label says, the tins or packets are invariably still packaged in Imperial quantities (typically 8 or 16 ounces).

India's conversion to the metric system from the imperial system occurred in stages between 1955 and 1962. The metric system in weights and measures was adopted by the Indian Parliament in December 1956 with the "Standards of Weights and Measures Act", which took effect beginning 1 October 1958. For the next five years, both the previous and new system were legal. In April 1962, all other systems were banned.

Today all official measurements are made in the metric system. In common usage some older Indians may still refer to imperial units. Some measurements, such as the heights of mountains, are still recorded in feet. Tyre rim diameters are still measured in inches, as used worldwide. Industries like the construction and the real estate industry still use both the metric and the imperial system though it is more common for sizes of homes to be given in square feet and land in acres.

In Standard Indian English, as in Australian, Singaporean, and British English, metric units such as the litre, metre, and metric tonne utilise the traditional spellings brought over from French, which differ from those used in the United States and the Philippines. The imperial long ton is invariably spelt with one 'n'.

Hong Kong has three main systems of units of measurement in current use:


In 1976 the Hong Kong Government started the conversion to the metric system, and as of 2012 measurements for government purposes, such as road signs, are almost always in metric units. All three systems are officially permitted for trade, and in the wider society a mixture of all three systems prevails.

The Chinese system's most commonly used units for length are ("lei"), ("zoeng"), ("cek"), ("cyun"), ("fan") in descending scale order. These units are now rarely used in daily life, the imperial and metric systems being preferred. The imperial equivalents are written with the same basic Chinese characters as the Chinese system. In order to distinguish between the units of the two systems, the units can be prefixed with "Ying" (, "jing") for the Imperial system and "Wa" (, "waa") for the Chinese system. In writing, derived characters are often used, with an additional (mouth) radical to the left of the original Chinese character, for writing imperial units. The most commonly used units are the mile or "li" (, "li"), the yard or "ma" (, "maa"), the foot or "chek" (, "cek"), and the inch or "tsun" (, "cyun").

The traditional measure of flat area is the square foot (, "fong cek, ping fong cek") of the imperial system, which is still in common use for real estate purposes. The measurement of agricultural plots and fields is traditionally conducted in ("mau") of the Chinese system.

For the measurement of volume, Hong Kong officially uses the metric system, though the gallon (, "gaa leon") is also occasionally used.

During the 1970s, the metric system and SI units were introduced in Canada to replace the imperial system. Within the government, efforts to implement the metric system were extensive; almost any agency, institution, or function provided by the government uses SI units exclusively. Imperial units were eliminated from all public road signs and both systems of measurement will still be found on privately owned signs, such as the height warnings at the entrance of a parkade. In the 1980s, momentum to fully convert to the metric system stalled when the government of Brian Mulroney was elected. There was heavy opposition to metrication and as a compromise the government maintains legal definitions for and allows use of imperial units as long as metric units are shown as well.
The law requires that measured products (such as fuel and meat) be priced in metric units and an imperial price can be shown if a metric price is present. There tends to be leniency in regards to fruits and vegetables being priced in imperial units only.
Environment Canada still offers an imperial unit option beside metric units, even though weather is typically measured and reported in metric units in the Canadian media. Some radio stations near the United States border (such as CIMX and CIDR) primarily use imperial units to report the weather. Railways in Canada also continue to use Imperial units.

Imperial units are still used in ordinary conversation. Today, Canadians typically use a mix of metric and imperial measurements in their daily lives. The use of the metric and imperial systems varies by age. The older generation mostly uses the imperial system, while the younger generation more often uses the metric system. Quebec has implemented metrication more fully. Newborns are measured in SI at hospitals, but the birth weight and length is also announced to family and friends in imperial units. Drivers' licences use SI units, though many English-speaking Canadians give their height and weight in imperial. In livestock auction markets, cattle are sold in dollars per hundredweight (short), whereas hogs are sold in dollars per hundred kilograms. Imperial units still dominate in recipes, construction, house renovation and gardening. Land is now surveyed and registered in metric units whist initial surveys used imperial units. For example, partitioning of farm land on the prairies in the late 19th and early 20th centuries was done in imperial units; this accounts for imperial units of distance and area retaining wide use in the Prairie Provinces. In English-speaking Canada commercial and residential spaces are mostly (but not exclusively) constructed using square feet, while in French-speaking Quebec commercial and residential spaces are constructed in metres and advertised using both square metres and square feet as equivalents. Carpet or flooring tile is purchased by the square foot, but less frequently also in square metres. Motor-vehicle fuel consumption is reported in both litres per 100 km and statute miles per imperial gallon, leading to the erroneous impression that Canadian vehicles are 20% more fuel-efficient than their apparently identical American counterparts for which fuel economy is reported in statute miles per US gallon (neither country specifies which gallon is used). Canadian railways maintain exclusive use of imperial measurements to describe train length (feet), train height (feet), capacity (tons), speed (mph), and trackage (miles).

Imperial units also retain common use in firearms and ammunition. Imperial measures are still used in the description of cartridge types, even when the cartridge is of relatively recent invention (e.g., .204 Ruger, .17 HMR, where the calibre is expressed in decimal fractions of an inch). Ammunition that is already classified in metric is still kept metric (e.g., 9×19mm). In the manufacture of ammunition, bullet and powder weights are expressed in terms of grains for both metric and imperial cartridges.

As in most of the western world, air navigation is based on "nautical" units, e.g., the nautical mile, which is neither imperial nor metric, though altitude is still measured in imperial feet in keeping with the international standard.

Metrication in Australia has largely ended the official use of imperial units, though for particular measurements, international use of imperial units is still followed. In licensed venues, draught beer and cider is sold in glasses and jugs with sizes based on the imperial fluid ounce, though rounded to the nearest 5 mL.

Newborns are measured in metric at hospitals, but the birth weight and length is also announced to family and friends in imperial units.

Screen sizes, are frequently advertised in inches as well as centimetres.

Property size is frequently advertised in acres, but is mostly as square metres.

Navigation is done in nautical miles, and water-based speed limits are in nautical miles per hour.

In New Zealand, which completed metrication in the 1970s, a study of university students undertaken in 1992 found a continued use of imperial units for birth weight and human height alongside metric units.

In aviation, altitude and airport elevation are measured in feet whilst navigation is done in nautical miles; all other aspects (fuel quantity, aircraft weight, runway length, etc.) use metric units.

Screen sizes for devices such as televisions, monitors and phones, and wheel rim sizes for vehicles, are stated in inches, as is the convention in the rest of the world.

Ireland has officially changed over to the metric system since entering the European Union, with distances on new road signs being metric since 1997 and speed limits being metric since 2005. The imperial system remains in limited use – for sales of beer in pubs (traditionally sold by the pint). All other goods are required by law to be sold in metric units with traditional quantities being retained for goods like butter and sausages, which are sold in 454-gram (1 lb) packaging. The majority of cars sold pre-2005 feature speedometers with miles per hour as the primary unit, but with a kilometres per hour display as well. Often signs such as those for bridge height can display both metric and imperial units. Imperial measurements continue to be used colloquially by the general population especially with height and distance measurements such as feet, inches, and acres as well as for weight with pounds and stones still in common use among people of all ages. Measurements such as yards have fallen out of favour with younger generations. All of Ireland's railways still operate on imperial measurements and property is usually listed in square feet as well as metres also.

Some imperial measurements remain in limited use in Malaysia, the Philippines, Sri Lanka and South Africa. Measurements in feet and inches, especially for a person's height, are frequently encountered in conversation and non-governmental publications.

Prior to metrication, it was a common practice in Malaysia for people to refer to unnamed locations and small settlements along major roads by referring to how many miles the said locations were from the nearest major town. In some cases, these eventually became the official names of the locations; in other cases, such names have been largely or completely superseded by new names. An example of the former is Batu 32 (literally "Mile 32" in Malay), which refers to the area surrounding the intersection between Federal Route 22 (the Tamparuli-Sandakan highway) and Federal Route 13 (the Sandakan-Tawau highway). The area is so named because it is 32 miles west of Sandakan, the nearest major town.

Petrol is still sold by the imperial gallon in Anguilla, Antigua and Barbuda, Belize, Myanmar, the Cayman Islands, Dominica, Grenada, Montserrat, St Kitts and Nevis and St. Vincent and the Grenadines. The United Arab Emirates Cabinet in 2009 issued the Decree No. (270 / 3) specifying that, from 1 January 2010, the new unit sale price for petrol will be the litre and not the gallon. This in line with the UAE Cabinet Decision No. 31 of 2006 on the national system of measurement, which mandates the use of International System of units as a basis for the legal units of measurement in the country. Sierra Leone switched to selling fuel by the litre in May 2011.

In October 2011, the Antigua and Barbuda government announced the re-launch of the Metrication Programme in accordance with the Metrology Act 2007, which established the International System of Units as the legal system of units. The Antigua and Barbuda government has committed to a full conversion from the imperial system by the first quarter of 2015.




</doc>
<doc id="15494" url="https://en.wikipedia.org/wiki?curid=15494" title="Incompatible-properties argument">
Incompatible-properties argument

The incompatible-properties argument is the idea that no description of God is consistent with reality. For example, if one takes the definition of God to be described fully from the Bible, then the claims of what properties God has described therein might be argued to lead to a contradiction.

The problem of evil is the argument that the existence of evil is incompatible with the concept of an omnipotent and perfectly good God.

A variation does not depend on the existence of evil. A truly omnipotent God could create all possible worlds. A "good" God can create only "good" worlds. A God that created all possible worlds would have no moral qualities whatsoever, and could be replaced by a random generator. The standard response is to argue a distinction between "could create" and "would create." In other words, God "could" create all possible worlds but that is simply not in God's nature. This has been argued by theologians for centuries. However, the result is that a "good" God is incompatible with some possible worlds, thus incapable of creating them without losing the property of being a totally different God. Yet, it is not necessary for God to be "good". He simply is good, but is capable of evil.

One argument based on incompatible properties rests on a definition of God that includes a will, plan or purpose and an existence outside of time. To say that a being possesses a purpose implies an inclination or tendency to steer events toward some state that does not yet exist. This, in turn, implies a privileged direction, which we may call "time". It may be one direction of causality, the direction of increasing entropy, or some other emergent property of a world. These are not identical, but one must exist in order to progress toward a goal. 

In general, God's time would not be related to our time. God might be able to operate within our time without being constrained to do so. However, God could then step outside this game for any purpose. Thus God's time must be aligned with our time if human activities are relevant to God's purpose. (In a relativistic universe, presumably this means—at any point in spacetime—time measured from t=0 at the Big Bang or end of inflation.)

A God existing outside of any sort of time could not create anything because creation substitutes one thing for another, or for nothing. Creation requires a creator that existed, by definition, prior to the thing created.

Another pair of alleged incompatible properties is omniscience and either indeterminacy or free will. Omniscience concerning the past and present (properly defined relative to Earth) is not a problem, but there is an argument that omniscience regarding the future implies it has been determined, what seems possible only in a deterministic world.

Another pair is divine simplicity and omniscience. An omniscient God must necessarily encompass all information in the universe. Information is not "ineffable" and cannot be reduced to something simpler.




</doc>
<doc id="15495" url="https://en.wikipedia.org/wiki?curid=15495" title="International Society of Olympic Historians">
International Society of Olympic Historians

The International Society of Olympic Historians (ISOH) is a non-profit organization founded in 1991 with the purpose of promoting and studying the Olympic Movement and the Olympic Games. The majority of recent books on the Olympic Games have been written by ISOH members. The ISOH publishes the Journal of Olympic History (JOH, formerly "Citius, Altius, Fortius") three times a year.

The International Society of Olympic Historians (ISOH) was formed as the result of a meeting in London, England in December 1991. The idea of forming an Olympic historical society had been the subject of correspondence – mainly between Bill Mallon (United States) and Ture Widlund (Sweden) – for many years. On Thursday, 5 December 1991, a group of potential members met at the Duke of Clarence, a small pub in the Kensington section of London. Those present were Ian Buchanan (Great Britain), Stan Greenberg (Great Britain), Ove Karlsson (Sweden), Bill Mallon (United States), Peter Matthews (Great Britain), David Wallechinsky (United States), and Ture Widlund (Sweden). The invited guests who sent regrets were: Anthony Bijkerk (Netherlands), Peter Diamond (United States), Pim Huurman (Netherlands), Erich Kamper (Austria), Volker Kluge (Germany), John Lucas (United States), and Wolf Lyberg (Sweden).

ISOH was formed with the purpose of promoting and studying the Olympic Movement and the Olympic Games. This purpose is achieved primarily through research into their history, through the gathering of historical and statistical data concerning the Olympic Movement and Olympic Games, through the publication of the research via journals and other publications, and through the cooperation of the membership.

From its inception to 2000, Ian Buchanan has been the president of the ISOH. In 2000, this function was taken over by Bill Mallon. From 2004 to 2012 Dr. Karl Lennartz (Germany) served as president and since 2012 David Wallechinsky (United States) has been president.

The ISOH publishes the "Journal of Olympic History" (formerly "Citius, Altius, Fortius").

, the ISOH has about 340 members from 48 nations. The membership includes well-known Olympic historians and researchers on Olympic topics. The majority of recent books on the Olympic Games have been written by ISOH members. Over 20 ISOH members have received the Olympic Order for their contributions to the Olympic Movement, and several members of the IOC and several Olympians are members. Other members are collectors of Olympic memorabilia, such as Raleigh DeGeer Amyx.






</doc>
<doc id="15496" url="https://en.wikipedia.org/wiki?curid=15496" title="Serie A">
Serie A

Serie A (), also called Serie A TIM due to sponsorship by TIM, is a professional league competition for football clubs located at the top of the Italian football league system and the winner is awarded the Scudetto and the Coppa Campioni d'Italia. It has been operating as a round-robin tournament for over ninety years since the 1929–30 season. It had been organized by the Direttorio Divisioni Superiori until 1943 and the Lega Calcio until 2010, when the Lega Serie A was created for the 2010–11 season. Serie A is regarded as one of the best football leagues in the world and it is often depicted as the most tactical and defensively sound national league. Serie A was the world's second-strongest national league in 2014 according to IFFHS. Serie A is ranked fourth among European leagues according to UEFA's league coefficient, behind La Liga, the Premier League and the Bundesliga, and ahead of Ligue 1, which is based on the performance of Italian clubs in the Champions League and the Europa League during the last five years. Serie A led the UEFA ranking from 1986 to 1988 and from 1990 to 1999.

In its current format, the Italian Football Championship was revised from having regional and interregional rounds, to a single-tier league from the 1929–30 season onwards. The championship titles won prior to 1929 are officially recognised by FIGC with the same weighting as titles that were subsequently awarded. Similarly, the 1945–46 season, when the round-robin was suspended and the league was played over two geographical groups due to the ravages of WWII, is not statistically considered, even if its title is fully official. All the winning teams are recognised with the title of "Campione d'Italia" ("Champion of Italy"), which is ratified by the Lega Serie A before the start of the next edition of the championship.

The league hosts three of the world's most famous clubs as Juventus, Milan and Internazionale, all founding members of the G-14, a group which represented the largest and most prestigious European football clubs from 2000 to 2008, with the first two also being founding members of its successive organisation, European Club Association (ECA). More players have won the coveted Ballon d'Or award while playing at a Serie A club than any league in the world other than Spain's La Liga, although Spain's La Liga has the highest total number of Ballon d'Or winners. Juventus, Italy's most successful club of the 20th century and the most successful Italian team, is tied for fifth in Europe and eleventh in the world with the most official international titles. The club is also the only one in the world to have won all possible official confederation competitions. Milan is joint third club for official international titles won in the world, with 18. Internazionale, following their achievements in the 2009–10 season, became the first Italian team to have achieved a treble. Inter are also the only team in Italian football history to have never been relegated. Juventus, Milan and Inter, along with Lazio, Fiorentina, Roma and Napoli, are known as the Seven Sisters of Italian football.

Serie A is one of the most storied football leagues in the world. Of the 100 greatest footballers in history chosen by "FourFourTwo" magazine in 2017, 42 players have played in Serie A, more than any other league in the world. Juventus is the team that has produced the most World Cup champions (25), with Inter (19), Roma (15) and Milan (10), being respectively third, fourth and ninth in that ranking.

Serie A, as it is structured today, began during the 1929–30 season. From 1898 to 1922, the competition was organised into regional groups. Because of ever growing teams attending regional championships, the Italian Football Federation (FIGC) split the CCI (Italian Football Confederation) in 1921, which founded in Milan the Lega Nord (Northern Football League), ancestor of present-day Lega Serie A. When CCI teams rejoined the FIGC created two interregional divisions renaming Categories into Divisions and splitting FIGC sections into two north–south leagues. In 1926, due to internal crises and fascist pressures, the FIGC changed internal settings, adding southern teams to the national division, ultimately leading to the 1929–30 final settlement. Torino were declared champions in the 1948–49 season following a plane crash near the end of the season in which the entire team was killed.

The Serie A Championship title is often referred to as the "scudetto" ("small shield") because since the 1923–24 season, the winning team will bear a small coat of arms with the Italian tricolour on their strip in the following season. The most successful club is Juventus with 36 championships, followed by both Milan and Internazionale, with 18 championships apiece. From the 2004–05 season onwards, an actual trophy was awarded to club on the pitch after the last turn of the championship. The trophy, called the Coppa Campioni d'Italia, has officially been used since the 1960–61 season, but between 1961 and 2004 was consigned to the winning clubs at the head office of the Lega Nazionale Professionisti.

In April 2009, Serie A announced a split from Serie B. Nineteen of the twenty clubs voted in favour of the move in an argument over television rights; the relegation-threatened Lecce had voted against the decision. Maurizio Beretta, the former head of Italy's employers' association, became president of the new league.

In April 2016, it was announced that Serie A was selected by the International Football Association Board to test video replays, which were initially private for the 2016–17 season, allowing them to become a live pilot phase, with replay assistance implemented in the 2017–18 season. On the decision, FIGC President Carlo Tavecchio said, "We were among the first supporters of using technology on the pitch and we believe we have everything required to offer our contribution to this important experiment."

For most of Serie A's history, there were 16 or 18 clubs competing at the top level. Since 2004–05, however, there have been 20 clubs altogether. One season (1947–48) was played with 21 teams for political reasons, following post-war tensions with Yugoslavia. Below is a complete record of how many teams played in each season throughout the league's history;
During the season, which runs from August to May, each club plays each of the other teams twice; once at home and once away, totalling 38 games for each team by the end of the season. Thus, in Italian football a true round-robin format is used. In the first half of the season, called the "andata", each team plays once against each league opponent, for a total of 19 games. In the second half of the season, called the "ritorno", the teams play in exactly the same order that they did in the first half of the season, the only difference being that home and away situations are switched. Since the 1994–95 season, teams are awarded three points for a win, one point for a draw and no points for a loss.

The top four teams in the Serie A qualify straight to the UEFA Champions League. Team finishing fifth, with the winner of the Coppa Italia, qualify for the UEFA Europa League tournament. The sixth or the seventh ranked club, depending from the Coppa Italia winner’s performance, joins the preliminary round of the UEFA Europa Conference League. The three lowest-placed teams are relegated to Serie B.

From 2005–06 season, if two or more teams are tied in points (for any place), the deciding tie-breakers are as follows:


Until 2004–05 season, a playoff would be used to determine the champions, European spots or relegation, if the two teams were tied on points. Any play-off was held after the end of regular season. The last championship playoff occurred in the 1963–64 season when Bologna and Inter both finished on 54 points. Bologna won the play-off 2–0.

Prior to 1929, many clubs competed in the top level of Italian football as the earlier rounds were competed up to 1922 on a regional basis then interregional up to 1929. Below is a list of Serie A clubs who have competed in the competition since it has been a league format (66 in total).

The following 20 clubs will compete in the Serie A during the 2020–21 season.

There are 66 teams that have taken part in 89 Serie A championships in a single round that was played from the 1929–30 season until the 2020–21 season. The teams in bold compete in Serie A currently. Internazionale is the only team that has played Serie A football in every season.

Serie A had logos that featured its sponsor Telecom Italia (TIM). The logo that was introduced in 2010, had minor change in 2016 due to the change of the logo of Telecom Italia itself. In August 2018, a new logo was announced, and another one in August 2019.

In the past, individual clubs competing in the league had the rights to sell their broadcast rights to specific channels throughout Italy, unlike in most other European countries. Currently, the two broadcasters in Italy are the satellite broadcaster Sky Italia and streaming platform DAZN for its own pay television networks; RAI is allowed to broadcast only highlights (in exclusive from 13:30 to 22:30 CET).
This is a list of television rights in Italy (since 2018–19):

Since the 2010–11 season, Serie A clubs have negotiated television rights collectively rather than on an individual club basis, having previously abandoned collective negotiation at the end of the 1998–99 season.

In the 1990s, Serie A was at its most popular in the United Kingdom when it was shown on "Football Italia" on Channel 4, although it has actually appeared on more UK channels than any other league, rarely staying in one place for long since 2002. Serie A has appeared in the UK on BSB's The Sports Channel (1990–91), Sky Sports (1991–92), Channel 4 (1992–2002), Eurosport (2002–04), Setanta Sports and Bravo (2004–07), Channel 5 (2007–08), ESPN (2009–13), BT Sport (2013–2018), Eleven Sports Network (2018), Premier and FreeSports (2019–present).

Bold indicates clubs which play in the 2019–20 Serie A.


Boldface indicates a player still active in Serie A. "Italics" indicates a player active outside Serie A.

Unlike La Liga, which imposed a quota on the number of non-EU players on each club, Serie A clubs could sign as many non-EU players as available on domestic transfer.

During the 1980s and 1990s, most Serie A clubs signed a large number of players from foreign nations (both EU and non-EU members). Notable foreign players to play in Serie A during this era included Irish international Liam Brady, England internationals Paul Gascoigne and David Platt, France's Michel Platini and Laurent Blanc, Lothar Matthäus and Jürgen Klinsmann from Germany, Dutchmen Ruud Gullit and Dennis Bergkamp, and Argentina's Diego Maradona.

But since the 2003–04 season, a quota has been imposed on each of the clubs limiting the number of non-EU, non-EFTA and non-Swiss players who may be signed from abroad each season, following provisional measures introduced in the 2002–03 season, which allowed Serie A and B clubs to sign only one non-EU player in the 2002 summer transfer window.

In the middle of the 2000–01 season, the old quota system was abolished, which no longer limited each team to having more than five non-EU players and using no more than three in each match. Concurrent with the abolishment of the quota, the FIGC had investigated footballers that used fake passports. Alberto and Warley, Alejandro Da Silva and Jorginho Paulista of Udinese; Fábio Júnior and Gustavo Bartelt of Roma; Dida of Milan; Álvaro Recoba of Inter; Thomas Job, Francis Zé, Jean Ondoa of Sampdoria; and Jeda and Dede of Vicenza were all banned in July 2001 for lengths ranging from six months to one year. However, most of the bans were subsequently reduced.

The number of non-EU players was reduced from 265 in 2002–03 season to 166 in 2006–07 season. It also included players who received EU status after their respective countries joined the EU (see 2004 and 2007 enlargement), which made players such as Adrian Mutu, Valeri Bojinov, Marek Jankulovski and Marius Stankevičius EU players.

The rule underwent minor changes in August 2004, June 2005, June 2006. and June 2007.

Since the 2008–09 season, three quotas have been awarded to clubs that do not have non-EU players in their squad (previously only newly promoted clubs could have three quotas); clubs that have one non-EU player have two quotas. Those clubs that have two non-EU players, are awarded one quota and one conditional quota, which is awarded after: 1) Transferred 1 non-EU player abroad, or 2) Release 1 non-EU player as free agent, or 3) A non-EU player received EU nationality. Clubs with three or more non-EU players, have two conditional quotas, but releasing two non-EU players as free agent, will only have one quota instead of two. Serie B and Lega Pro clubs cannot sign non-EU player from abroad, except those followed the club promoted from Serie D.

Large clubs with many foreigners usually borrow quotas from other clubs that have few foreigners or no foreigners in order to sign more non-EU players. For example, Adrian Mutu joined Juventus via Livorno in 2005, as at the time Romania was not a member of the EU. Other examples include Júlio César, Victor Obinna and Maxwell, who joined Internazionale from Chievo (first two) and Empoli respectively.

On 2 July 2010, the above conditional quota reduced back to one, though if a team did not have any non-EU players, that team could still sign up to three non-EU players. In 2011 the signing quota reverted to two.

Serie A also imposed Homegrown players rule, a modification of Homegrown Player Rule (UEFA). Unlike UEFA, Serie A at first did not cap the number of players in first team squad at 25, meaning the club could employ more foreigners by increasing the size of the squad. However, a cap of 25 (under-21 players were excluded) was introduced to 2015–16 season (in 2015–16 season, squad simply require 8 homegrown players but not require 4 of them from their own youth team). In the 2016–17 season, the FIGC sanctioned Sassuolo for fielding ineligible player, Antonino Ragusa. Although the club did not exceed the capacity of 21 players that were not from their own youth team (only Domenico Berardi was eligible as youth product of their own) as well as under 21 of age (born 1995 or after, of which four players were eligible) in their 24-men call-up, It was reported that on Lega Serie A side the squad list was not updated.

In 2015–16 season, the following quota was announced.



 



</doc>
<doc id="15501" url="https://en.wikipedia.org/wiki?curid=15501" title="Inhalant">
Inhalant

Inhalants are a broad range of household and industrial chemicals whose volatile vapors or pressurized gases can be concentrated and breathed in via the nose or mouth to produce intoxication, in a manner not intended by the manufacturer. They are inhaled at room temperature through volatilization (in the case of gasoline or acetone) or from a pressurized container (e.g., nitrous oxide or butane), and do not include drugs that are sniffed after burning or heating. For example, amyl nitrite (poppers), nitrous oxide and toluene – a solvent widely used in contact cement, permanent markers, and certain types of glue – are considered inhalants, but smoking tobacco, cannabis, and crack are not, even though these drugs are inhaled as smoke.

While a small number of inhalants are prescribed by medical professionals and used for medical purposes, as in the case of inhaled anesthetics and nitrous oxide (an anxiolytic and pain relief agent prescribed by dentists), this article focuses on inhalant use of household and industrial propellants, glues, fuels and other products in a manner not intended by the manufacturer, to produce intoxication or other psychoactive effects. These products are used as recreational drugs for their intoxicating effect. According to a 1995 report by the National Institute on Drug Abuse, the most serious inhalant abuse occurs among homeless children and teens who "... live on the streets completely without family ties." Inhalants are the only substance which is used more by younger teens than by older teens. Inhalant users inhale vapor or aerosol propellant gases using plastic bags held over the mouth or by breathing from a solvent-soaked rag or an open container. The practices are known colloquially as "sniffing", "huffing" or "bagging".

The effects of inhalants range from an alcohol-like intoxication and intense euphoria to vivid hallucinations, depending on the substance and the dose. Some inhalant users are injured due to the harmful effects of the solvents or gases or due to other chemicals used in the products that they are inhaling. As with any recreational drug, users can be injured due to dangerous behavior while they are intoxicated, such as driving under the influence. In some cases, users have died from hypoxia (lack of oxygen), pneumonia, cardiac failure or arrest, or aspiration of vomit. Brain damage is typically seen with chronic long-term use of solvents as opposed to short-term exposure.

Even though many inhalants are legal, there have been legal actions taken in some jurisdictions to limit access by minors. While solvent glue is normally a legal product, a Scottish court has ruled that supplying glue to children is illegal if the store knows the children intend to abuse the glue. In the US, thirty-eight of 50 states have enacted laws making various inhalants unavailable to those under the age of 18, or making inhalant use illegal.

Inhalants can be classified by the intended function. Most inhalant drugs that are used non-medically are ingredients in household or industrial chemical products that are not intended to be concentrated and inhaled. A small number of recreational inhalant drugs are pharmaceutical products that are used illicitly.

Another way to categorize inhalants is by their product category. There are three main product categories: solvents; gases; and medical drugs which are used illicitly.

A wide range of volatile solvents intended for household or industrial use are inhaled as recreational drugs. This includes petroleum products (gasoline and kerosene), toluene (used in paint thinner, permanent markers, contact cement and model glue), and acetone (used in nail polish remover). These solvents vaporize at room temperature. Ethanol (the alcohol which is normally drunk) is sometimes inhaled, but this cannot be done at room temperature. The ethanol must be converted from liquid into gaseous state (vapor) or aerosol (mist), in some cases using a nebulizer, a machine that agitates the liquid into an aerosol. The sale of nebulizers for inhaling ethanol was banned in some US states due to safety concerns.

A number of gases intended for household or industrial use are inhaled as recreational drugs. This includes chlorofluorocarbons used in aerosols and propellants (e.g., aerosol hair spray, aerosol deodorant). A gas used as a propellant in whipped cream aerosol containers, nitrous oxide, is used as a recreational drug. Pressurized canisters of propane and butane gas, both of which are intended for use as fuels, are used as inhalants.

Several medical anesthetics are used as recreational drugs, including diethyl ether (a drug that is no longer used medically, due to its high flammability and the development of safer alternatives) and nitrous oxide, which is widely used in the 2010s by dentists as an anti-anxiety drug during dental procedures. Diethyl ether has a long history of use as a recreational drug. The effects of ether intoxication are similar to those of alcohol intoxication, but more potent. Also, due to NMDA antagonism, the user may experience all the psychedelic effects present in classical dissociatives such as ketamine in forms of thought loops and feeling of mind being disconnected from one's body. Nitrous oxide is a dental anesthetic which is used as a recreational drug, either by users who have access to medical-grade gas canisters (e.g., dental hygienists or dentists) or by using the gas contained in whipped cream aerosol containers. Nitrous oxide inhalation can cause pain relief, depersonalisation, derealisation, dizziness, euphoria, and some sound distortion.

It is also possible to classify inhalants by the effect they have on the body. Some solvents act as depressants, causing users to feel relaxed or drowsy. Many inhalants act primarily as asphyxiant gases, with their primary effect due to oxygen deprivation. Nitrous oxide can be categorized as a dissociative drug, as it can cause visual and auditory hallucinations. Other agents may have more direct effects at receptors, as inhalants exhibit a variety of mechanisms of action. The mechanisms of action of many non-medical inhalants have not been well elucidated. Anesthetic gases used for surgery, such as nitrous oxide or enflurane, are believed to induce anesthesia primarily by acting as NMDA receptor antagonists, open channel blockers that bind to the inside of the calcium channels on the outer surface of the neuron, and provide high levels of NMDA receptor blockade for a short period of time.

This makes inhaled anesthetic gases different from other NMDA antagonists, such as ketamine, which bind to a regulatory site on the NMDA-sensitive calcium transporter complex and provide slightly lower levels of NMDA blockade, but for a longer and much more predictable duration. This makes a deeper level of anesthesia achievable more easily using anesthetic gases but can also make them more dangerous than other drugs used for this purpose.

Inhalants can also be classified by chemical structure. Classes include:
Inhalant users inhale vapors or aerosol propellant gases using plastic bags held over the mouth or by breathing from an open container of solvents, such as gasoline or paint thinner. Nitrous oxide gases from whipped cream aerosol cans, aerosol hairspray or non-stick frying spray are sprayed into plastic bags. Some nitrous oxide users spray the gas into balloons. When inhaling non-stick cooking spray or other aerosol products, some users may filter the aerosolized particles out with a rag. Some gases, such as propane and butane gases, are inhaled directly from the canister. Once these solvents or gases are inhaled, the extensive capillary surface of the lungs rapidly absorb the solvent or gas, and blood levels peak rapidly. The intoxication effects occur so quickly that the effects of inhalation can resemble the intensity of effects produced by intravenous injection of other psychoactive drugs.

Ethanol is also inhaled, either by vaporizing it by pouring it over dry ice in a narrow container and inhaling with a straw or by pouring alcohol in a corked bottle with a pipe, and then using a bicycle pump to make a spray. Alcohol can be vaporized using a simple container and open-flame heater. Medical devices such as asthma nebulizers and inhalers were also reported as means of application. The practice gained popularity in 2004, with marketing of the device dubbed AWOL (Alcohol without liquid), a play on the military term AWOL (Absent Without Leave). AWOL, created by British businessman Dominic Simler, was first introduced in Asia and Europe, and then in United States in August 2004. AWOL was used by nightclubs, at gatherings and parties, and it garnered attraction as a novelty, as people 'enjoyed passing it around in a group'. AWOL uses a nebulizer, a machine that agitates the liquid into an aerosol. AWOL's official website states that "AWOL and AWOL 1 are powered by "Electrical Air Compressors" while AWOL 2 and AWOL 3 are powered by "electrical oxygen generators"", which refer to a couple of mechanisms used by the nebulizer drug delivery device for inhalation. Although the AWOL machine is marketed as having no downsides, such as the lack of calories or hangovers, Amanda Shaffer of "Slate" describes these claims as "dubious at best". Although inhaled alcohol does reduce the caloric content, the savings are minimal. After expressed safety and health concerns, sale or use of AWOL machines was banned in a number of American states.

The effects of solvent intoxication can vary widely depending on the dose and what type of solvent or gas is inhaled. A person who has inhaled a small amount of rubber cement or paint thinner vapor may be impaired in a manner resembling alcohol inebriation. A person who has inhaled a larger quantity of solvents or gases, or a stronger chemical, may experience stronger effects such as distortion in perceptions of time and space, hallucinations, and emotional disturbances. The effects of inhalant use are also modified by the combined use of inhalants and alcohol or other drugs.

In the short term, many users experience headache, nausea and vomiting, slurred speech, loss of motor coordination, and wheezing. A characteristic "glue sniffer's rash" around the nose and mouth is sometimes seen after prolonged use. An odor of paint or solvents on clothes, skin, and breath is sometimes a sign of inhalant abuse, and paint or solvent residues can sometimes emerge in sweat.

According to NIH, even a single session of inhalant abuse "can disrupt heart rhythms and lower oxygen levels", which can lead to death. "Regular abuse can result in serious harm to the brain, heart, kidneys and liver."

Statistics on deaths caused by inhalant abuse are difficult to determine. It may be severely under-reported, because death is often attributed to a discrete event such as a stroke or a heart attack, even if the event happened because of inhalant abuse. Inhalant use or abuse was mentioned on 144 death certificates in Texas during the period 1988–1998 and was reported in 39 deaths in Virginia between 1987 and 1996 from acute voluntary exposure to abused inhalants.

Regardless of which inhalant is used, inhaling vapours or gases can lead to injury or death. One major risk is hypoxia (lack of oxygen), which can occur due to inhaling fumes from a plastic bag, or from using proper inhalation mask equipment (e.g., a medical mask for nitrous oxide) but not adding oxygen or room air. Another danger is freezing the throat. When a gas that was stored under high pressure is released, it cools abruptly and can cause frostbite if it is inhaled directly from the container. This can occur, for example, with inhaling nitrous oxide. When nitrous oxide is used as an automotive power adder, its cooling effect is used to make the fuel-air charge denser. In a person, this effect is potentially lethal. Many inhalants are volatile organic chemicals and can catch fire or explode, especially when combined with smoking. As with many other drugs, users may also injure themselves due to loss of coordination or impaired judgment, especially if they attempt to drive.

Solvents have many potential risks in common, including pneumonia, cardiac failure or arrest, and aspiration of vomit. The inhaling of some solvents can cause hearing loss, limb spasms, and damage to the central nervous system and brain. Serious but potentially reversible effects include liver and kidney damage and blood-oxygen depletion. Death from inhalants is generally caused by a very high concentration of fumes. Deliberately inhaling solvents from an attached paper or plastic bag or in a closed area greatly increases the chances of suffocation. Brain damage is typically seen with chronic long-term use as opposed to short-term exposure. Parkinsonism (see: Signs and symptoms of Parkinson's disease) has been associated with huffing.
Female inhalant users who are pregnant may have adverse effects on the fetus, and the baby may be smaller when it is born and may need additional health care (similar to those seen with alcohol – fetal alcohol syndrome). There is some evidence of birth defects and disabilities in babies born to women who sniffed solvents such as gasoline.

In the short term, death from solvent abuse occurs most commonly from aspiration of vomit while unconscious or from a combination of respiratory depression and hypoxia, the second cause being especially a risk with heavier-than-air vapors such as butane or gasoline vapor. Deaths typically occur from complications related to excessive sedation and vomiting. Actual overdose from the drug does occur, however, and inhaled solvent abuse is statistically more likely to result in life-threatening respiratory depression than intravenous use of opiates such as heroin. Most deaths from solvent abuse could be prevented if individuals were resuscitated quickly when they stopped breathing and their airway cleared if they vomited. However, most inhalant abuse takes place when people inhale solvents by themselves or in groups of people who are intoxicated. Certain solvents are more hazardous than others, such as gasoline.

In contrast, a few inhalants like amyl nitrate and diethyl ether have medical applications and are not toxic in the same sense as solvents, though they can still be dangerous when used recreationally. Nitrous oxide is thought to be particularly non-toxic, though heavy long-term use can lead to a variety of serious health problems linked to destruction of vitamin B12 and folic acid.

The hypoxic effect of inhalants can cause damage to many organ systems (particularly the brain, which has a very low tolerance for oxygen deprivation), but there can also be additional toxicity resulting from either the physical properties of the compound itself or additional ingredients present in a product. Organochlorine solvents are particularly hazardous; many of these are now restricted in developed countries due to their environmental impact.


Toxicity may also result from the pharmacological properties of the drug; excess NMDA antagonism can completely block calcium influx into neurons and provoke cell death through apoptosis, although this is more likely to be a long-term result of chronic solvent abuse than a consequence of short-term use.

Sudden sniffing death syndrome is commonly known as SSDS.

Inhaling butane gas can cause drowsiness, unconsciousness, asphyxia, and cardiac arrhythmia. Butane is the most commonly misused volatile solvent in the UK and caused 52% of solvent-related deaths in 2000. When butane is sprayed directly into the throat, the jet of fluid can cool rapidly to −20 °C by adiabatic expansion, causing prolonged laryngospasm.

Some inhalants can also indirectly cause sudden death by cardiac arrest, in a syndrome known as "sudden sniffing death". The anaesthetic gases present in the inhalants appear to sensitize the user to adrenaline and, in this state, a sudden surge of adrenaline (e.g., from a frightening hallucination or run-in with aggressors), may cause fatal cardiac arrhythmia.

Furthermore, the inhalation of any gas that is capable of displacing oxygen in the lungs (especially gases heavier than oxygen) carries the risk of hypoxia as a result of the very mechanism by which breathing is triggered. Since reflexive breathing is prompted by elevated carbon dioxide levels (rather than diminished blood oxygen levels), breathing a concentrated, relatively inert gas (such as computer-duster tetrafluoroethane or nitrous oxide) that removes carbon dioxide from the blood without replacing it with oxygen will produce no outward signs of suffocation even when the brain is experiencing hypoxia. Once full symptoms of hypoxia appear, it may be too late to breathe without assistance, especially if the gas is heavy enough to lodge in the lungs for extended periods. Even completely inert gases, such as argon, can have this effect if oxygen is largely excluded.

Even though solvent glue is normally a legal product, there is a case where a court has ruled that supplying glue to children is illegal. Khaliq v HM Advocate was a Scottish criminal case decided by the High Court of Justiciary on appeal, in which it was decided that it was an offence at common law to supply glue sniffing materials that were otherwise legal in the knowledge that they would be used recreationally by children. Two shopkeepers in Glasgow were arrested and charged with supplying to children "glue-sniffing kits" consisting of a quantity of petroleum-based glue in a plastic bag. They argued there was nothing illegal about the items that they had supplied. On appeal, the High Court took the view that, even though glue and plastic bags might be perfectly legal, everyday items, the two shopkeepers knew perfectly well that the children were going to use the articles as inhalants and the charge on the indictment should stand. When the case came to trial at Glasgow High Court the two were sentenced to three years' imprisonment.

"Thirty-eight of 50 [US] states have enacted laws making various inhalants unavailable to those under the age of 18. Other states prohibit the sale of these items to anyone without recognition of purpose for purchase. Some states mandate laws against using these products for purposes of getting high, while some states have laws about possessing certain inhalants. Nearly every state imposes fines and jail terms for violation of their specific laws."

"Connecticut law bans the unauthorized manufacture or compounding, possession, control, sale, delivery, or administration of any "restricted substance". It defines restricted substances as... specific volatile substances if they are sold, compounded, possessed or controlled, or delivered or administered to another person for breathing, inhaling, sniffing, or drinking to induce a stimulant, depressant, or hallucinogenic effect. Violators can be fined up to $100." As well, 24 states "ban the use, possession, or sale or other distribution of inhalants... like glue and solvents."

"Louisiana prohibits the sale, transfer, or possession of model glue and inhalable toluene substances to minors. In Ohio, it is illegal to inhale certain compounds for intoxication—a common, general prohibition other states have enacted.
Some states draw their prohibitions more narrowly... In Massachusetts, retailers must ask minors for identification before selling them glue or cement that contains a solvent that can release toxic vapors."

"New Jersey... prohibits selling or offering to sell minors products containing chlorofluorocarbon that is used in refrigerant."

The sale of alkyl nitrite-based poppers was banned in Canada in 2013. Although not considered a narcotic and not illegal to possess or use, they are considered a drug. Sales that are not authorized can now be punished with fines and prison. Since 2007, reformulated poppers containing isopropyl nitrite are sold in Europe because only isobutyl nitrite is prohibited. In France, the sale of products containing butyl nitrite, pentyl nitrite, or isomers thereof, has been prohibited since 1990 on grounds of danger to consumers. In 2007, the government extended this prohibition to all alkyl nitrites that were not authorized for sale as drugs. After litigation by sex shop owners, this extension was quashed by the Council of State on the grounds that the government had failed to justify such a blanket prohibition: according to the court, the risks cited, concerning rare accidents often following abnormal usage, rather justified compulsory warnings on the packaging.

In the United Kingdom, poppers are widely available and frequently (legally) sold in gay clubs/bars, sex shops, drug paraphernalia head shops, over the Internet and on markets. It is illegal under Medicines Act 1968 to sell them advertised for human consumption, and to bypass this, they are usually sold as odorizers. In the U.S., originally marketed as a prescription drug in 1937, amyl nitrite remained so until 1960, when the Food and Drug Administration removed the prescription requirement due to its safety record. This requirement was reinstated in 1969, after observation of an increase in recreational use. Other alkyl nitrites were outlawed in the U.S. by Congress through the Anti-Drug Abuse Act of 1988. The law includes an exception for commercial purposes. The term "commercial purpose" is defined to mean any use other than for the production of consumer products containing volatile alkyl nitrites meant for inhaling or otherwise introducing volatile alkyl nitrites into the human body for euphoric or physical effects. The law came into effect in 1990. Visits to retail outlets selling these products reveal that some manufacturers have since reformulated their products to abide by the regulations, through the use of the legal cyclohexyl nitrite as the primary ingredient in their products, which are sold as video head cleaners, polish removers, or room odorants.

In the United States, possession of nitrous oxide is legal under federal law and is not subject to DEA purview. It is, however, regulated by the Food and Drug Administration under the Food Drug and Cosmetics Act; prosecution is possible under its "misbranding" clauses, prohibiting the sale or distribution of nitrous oxide for the purpose of human consumption as a recreational drug. Many states have laws regulating the possession, sale, and distribution of nitrous oxide. Such laws usually ban distribution to minors or limit the amount of nitrous oxide that may be sold without special license. For example, in the state of California, possession for recreational use is prohibited and qualifies as a misdemeanour. In New Zealand, the Ministry of Health has warned that nitrous oxide is a prescription medicine, and its sale or possession without a prescription is an offence under the Medicines Act. This statement would seemingly prohibit all non-medicinal uses of the chemical, though it is implied that only recreational use will be legally targeted. In India, for general anaesthesia purposes, nitrous oxide is available as Nitrous Oxide IP. India's gas cylinder rules (1985) permit the transfer of gas from one cylinder to another for breathing purposes. Because India's Food & Drug Authority (FDA-India) rules state that transferring a drug from one container to another (refilling) is equivalent to manufacturing, anyone found doing so must possess a drug manufacturing license.

Inhalant drugs are often used by children, teenagers, incarcerated or institutionalized people, and impoverished people, because these solvents and gases are ingredients in hundreds of legally available, inexpensive products, such as deodorant sprays, hair spray, contact cement and aerosol air fresheners. However, most users tend to be "... adolescents (between the ages of 12 and 17)." In some countries, chronic, heavy inhalant use is concentrated in marginalized, impoverished communities. Young people who become chronic, heavy inhalant abusers are also more likely to be those who are isolated from their families and community. The article "Epidemiology of Inhalant Abuse: An International Perspective" notes that "[t]he most serious form of obsession with inhalant use probably occurs in countries other than the United States where young children live on the streets completely without family ties. These groups almost always use inhalants at very high levels (Leal et al. 1978). This isolation can make it harder to keep in touch with the sniffer and encourage him or her to stop sniffing."

The article also states that "... high [inhalant use] rates among barrio Hispanics almost undoubtedly are related to the poverty, lack of opportunity, and social dysfunction that occur in barrios" and states that the "... same general tendency appears for Native-American youth" because "... Indian reservations are among the most disadvantaged environments in the United States; there are high rates of unemployment, little opportunity, and high rates of alcoholism and other health problems." There are a wide range of social problems associated with inhalant use, such as feelings of distress, anxiety and grief for the community; violence and damage to property; violent crime; stresses on the juvenile justice system; and stresses on youth agencies and support services.

Glue and gasoline (petrol) sniffing is also a problem in parts of Africa, especially with street children. In India and South Asia, three of the most widely abused inhalants are the Dendrite brand and other forms of contact adhesives and rubber cements manufactured in Kolkata, and toluenes in paint thinners. Genkem is a brand of glue which had become the generic name for all the glues used by glue-sniffing children in Africa before the manufacturer replaced n-hexane in its ingredients in 2000.

The United Nations Office on Drugs and Crime has reported that glue sniffing is at the core of "street culture" in Nairobi, Kenya, and that the majority of street children in the city are habitual solvent users. Research conducted by Cottrell-Boyce for the African Journal of Drug and Alcohol Studies found that glue sniffing amongst Kenyan street children was primarily functional – dulling the senses against the hardship of life on the street – but it also provided a link to the support structure of the "street family" as a potent symbol of shared experience.

Similar incidents of glue sniffing among destitute youth in the Philippines have also been reported, most commonly from groups of street children and teenagers collectively known as "Rugby" boys, which were named after a brand of toluene-laden contact cement. Other toluene-containing substances have also been subject to abuse, most notably the Vulca Seal brand of roof sealants. Bostik Philippines, which currently owns the Rugby and Vulca Seal brands, has since responded to the issue by adding bitterants such as mustard oil to their Rugby line, as well as reformulating it by replacing toluene with xylene. Several other manufacturers have also followed suit.

Another very common inhalant is Erase-X, a correction fluid that contains toluene. It has become very common for school and college students to use it, because it is easily available in stationery shops in India. This fluid is also used by street and working children in Delhi.

In the UK, marginalized youth use a number of inhalants, such as solvents and propellants. In Russia and Eastern Europe, gasoline sniffing became common on Russian ships following attempts to limit the supply of alcohol to ship crews in the 1980s. The documentary "Children Underground" depicts the huffing of a solvent called Aurolac (a product used in chroming) by Romanian homeless children. During the Interbellum the inhalation of ether (etheromania) was widespread in some regions of Poland, especially in Upper Silesia—tens of thousands of people were affected by this problem.

In Canada, Native children in the isolated Northern Labrador community of Davis Inlet were the focus of national concern in 1993, when many were found to be sniffing gasoline. The Canadian and provincial Newfoundland and Labrador governments intervened on a number of occasions, sending many children away for treatment. Despite being moved to the new community of Natuashish in 2002, serious inhalant abuse problems have continued. Similar problems were reported in Sheshatshiu in 2000 and also in Pikangikum First Nation. In 2012, the issue once again made the news media in Canada. In Mexico, the inhaling of a mixture of gasoline and industrial solvents, known locally as "Activo" or "Chemo", has risen in popularity among the homeless and among the street children of Mexico City in recent years. The mixture is poured onto a handkerchief and inhaled while held in one's fist.

In the US, ether was used as a recreational drug during the 1930s Prohibition era, when alcohol was made illegal. Ether was either sniffed or drunk and, in some towns, replaced alcohol entirely. However, the risk of death from excessive sedation or overdose is greater than that with alcohol, and ether drinking is associated with damage to the stomach and gastrointestinal tract. Use of glue, paint and gasoline became more common after the 1950s. Model airplane glue-sniffing as problematic behavior among youth was first reported in 1959, and increased in the 1960s. Abuse of aerosol sprays became more common in the 1980s, as older propellants such as CFCs were phased out and replaced by more environmentally friendly compounds such as propane and butane. Most inhalant solvents and gases are not regulated under drug laws such as the United States' Controlled Substances Act. However, many US states and Canadian cities have placed restrictions on the sale of some solvent-containing products to minors, particularly for products widely associated with sniffing, such as model cement. The practice of inhaling such substances is sometimes colloquially referred to as huffing, sniffing (or glue sniffing), dusting, or chroming.

Australia has long faced a petrol (gasoline) sniffing problem in isolated and impoverished aboriginal communities. Although some sources argue that sniffing was introduced by United States servicemen stationed in the nation's Top End during World War II or through experimentation by 1940s-era Cobourg Peninsula sawmill workers, other sources claim that inhalant abuse (such as glue inhalation) emerged in Australia in the late 1960s. Chronic, heavy petrol sniffing appears to occur among remote, impoverished indigenous communities, where the ready accessibility of petrol has helped to make it a common substance for abuse.

In Australia, petrol sniffing now occurs widely throughout remote Aboriginal communities in the Northern Territory, Western Australia, northern parts of South Australia and Queensland. The number of people sniffing petrol goes up and down over time as young people experiment or sniff occasionally. "Boss", or chronic, sniffers may move in and out of communities; they are often responsible for encouraging young people to take it up.

A 1983 survey of 4,165 secondary students in New South Wales showed that solvents and aerosols ranked just after analgesics (e.g., codeine pills) and alcohol for drugs that were abused. This 1983 study did not find any common usage patterns or social class factors. The causes of death for inhalant users in Australia included pneumonia, cardiac failure/arrest, aspiration of vomit, and burns. In 1985, there were 14 communities in Central Australia reporting young people sniffing. In July 1997, it was estimated that there were around 200 young people sniffing petrol across 10 communities in Central Australia. Approximately 40 were classified as chronic sniffers. There have been reports of young Aboriginal people sniffing petrol in the urban areas around Darwin and Alice Springs.

In 2005, the Government of Australia and BP Australia began the usage of opal fuel in remote areas prone to petrol sniffing. Opal is a non-sniffable fuel (which is much less likely to cause a high) and has made a difference in some indigenous communities.

One of the early musical references to inhalant use occurs in the 1974 Elton John song "The Bitch Is Back", in the line "I get high in the evening sniffing pots of glue." Inhalant use, especially glue sniffing, is widely associated with the late-1970s punk youth subculture in the UK and North America. Raymond Cochrane and Douglas Carroll claim that when glue sniffing became widespread in the late 1970s, it was "adopted by punks because public [negative] perceptions of sniffing fitted in with their self-image" as rebels against societal values. While punks at first used inhalants "experimentally and as a cheap high, adult disgust and hostility [to the practice] encouraged punks to use glue sniffing as a way of shocking society." As well, using inhalants was a way of expressing their anti-corporatist DIY (do it yourself) credo; by using inexpensive household products as inhalants, punks did not have to purchase industrially manufactured liquor or beer.

One history of the punk subculture argues that "substance abuse was often referred to in the music and did become synonymous with the genre, glue sniffing especially" because the youths' "faith in the future had died and that the youth just didn't care anymore" due to the "awareness of the threat of nuclear war and a pervasive sense of doom." In a BBC interview with a person who was a punk in the late 1970s, they said that "there was a real fear of imminent nuclear war—people were sniffing glue knowing that it could kill them, but they didn't care because they believed that very soon everybody would be dead anyway."

A number of 1970s punk rock and 1980s hardcore punk songs refer to inhalant use. The Ramones, an influential early US punk band, referred to inhalant use in several of their songs. The song "Now I Wanna Sniff Some Glue" describes adolescent boredom, and the song "Carbona not Glue" states, "My brain is stuck from shooting glue." An influential punk fanzine about the subculture and music took its name ("Sniffin' Glue") from the Ramones song. The 1980s punk band The Dead Milkmen wrote a song, "Life is Shit" from their album "Beelzebubba", about two friends hallucinating after sniffing glue. Punk-band-turned-hip-hop group the Beastie Boys penned a song "Hold it Now – Hit It", which includes the line "cause I'm beer drinkin, breath stinkin, sniffing glue." Pop punk band Sum 41 wrote a song, "Fat Lip", which refers to a character who does not "make sense from all the gas you be huffing..." The song "Lança-perfume", written and performed by Brazilian popstar Rita Lee, became a national hit in 1980. The song is about chloroethane and its widespread recreational sale and use during the rise of Brazil's carnivals.

Inhalants are referred to by bands from other genres, including several grunge bands—an early 1990s genre that was influenced by punk rock. The 1990s grunge band Nirvana, which was influenced by punk music, penned a song, "Dumb", in which Kurt Cobain sings "my heart is broke / But I have some glue/help me inhale / And mend it with you". L7, an all-female grunge band, penned a song titled "Scrap" about a skinhead who inhales spray-paint fumes until his mind "starts to gel". Also in the 1990s, the Britpop band Suede had a UK hit with their song "Animal Nitrate" whose title is a thinly veiled reference to amyl nitrite. The Beck song "Fume" from his "Fresh Meat and Old Slabs" release is about inhaling nitrous oxide. Another Beck song, "Cold Ass Fashion", contains the line "O.G. – Original Gluesniffer!" Primus's 1998 song "Lacquer Head" is about adolescents who use inhalants to get high. Hip hop performer Eminem wrote a song, "Bad Meets Evil", which refers to breathing "... ether in three lethal amounts." The Brian Jonestown Massacre, a retro-rock band from the 1990s, has a song "Hyperventilation", which is about sniffing model-airplane cement. Frank Zappa's song "Teenage Wind" from 1981 has a reference to glue sniffing: "Nothing left to do but get out the 'ol glue; Parents, parents; Sniff it good now..."

A number of films have depicted or referred to the use of solvent inhalants. In the 1980 comedy film "Airplane!", the character of McCroskey (Lloyd Bridges) refers to his inhalant use when he states, "I picked the wrong week to quit sniffing glue." In the 1996 film "Citizen Ruth", the character Ruth (Laura Dern), a homeless drifter, is depicted inhaling patio sealant from a paper bag in an alleyway. In the tragicomedy "Love Liza", the main character, played by Philip Seymour Hoffman, plays a man who takes up building remote-controlled airplanes as a hobby to give him an excuse to sniff the fuel in the wake of his wife's suicide.

Harmony Korine's 1997 "Gummo" depicts adolescent boys inhaling contact cement for a high. Edet Belzberg's 2001 documentary "Children Underground" chronicles the lives of Romanian street children addicted to inhaling paint. In "The Basketball Diaries", a group of boys are huffing carbona cleaning liquid at 3 minutes and 27 seconds into the movie; further on, a boy is reading a diary describing the experience of sniffing the cleaning liquid.

In the David Lynch film "Blue Velvet", the bizarre and manipulative character played by Dennis Hopper uses a mask to inhale amyl nitrite. In "Little Shop of Horrors", Steve Martin's character dies from nitrous oxide inhalation. The 1999 independent film "Boys Don't Cry" depicts two young low-income women inhaling aerosol computer cleaner (compressed gas) for a buzz. In "The Cider House Rules", Michael Caine's character is addicted to inhaling ether vapors.

In "Thirteen", the main character, a teen, uses a can of aerosol computer cleaner to get high. In the action movie "Shooter", an ex-serviceman on the run from the law (Mark Wahlberg) inhales nitrous oxide gas from a number of Whip-It! whipped cream canisters until he becomes unconscious. The South African film "The Wooden Camera" also depicts the use of inhalants by one of the main characters, a homeless teen, and their use in terms of socio-economic stratification. The title characters in "Samson and Delilah" sniff petrol; in Samson's case, possibly causing brain damage.

In the 2004 film "Taxi", Queen Latifah and Jimmy Fallon are trapped in a room with a burst tank containing nitrous oxide. Queen Latifah's character curses at Fallon while they both laugh hysterically. Fallon's character asks if it is possible to die from nitrous oxide, to which Queen Latifah's character responds with "It's laughing gas, stupid!" Neither of them suffered any side effects other than their voices becoming much deeper while in the room.

In the French horror film "Them", (2006) a French couple living in Romania are pursued by a gang of street children who break into their home at night. Olivia Bonamy's character is later tortured and forced to inhale aurolac from a silver-colored bag. During a flashback scene in the 2001 film "Hannibal", Hannibal Lecter gets Mason Verger high on amyl nitrite poppers, then convinces Verger to cut off his own face and feed it to his dogs.

The science fiction story "Waterspider" by Philip K. Dick (first published in January 1964 in "If" magazine) contains a scene in which characters from the future are discussing the culture of the early 1950s. One character says: "You mean he sniffed what they called 'airplane dope'? He was a 'glue-sniffer'?", to which another character replies: "Hardly. That was a mania among adolescents and did not become widespread in fact until a decade later. No, I am speaking about imbibing alcohol."

The book "Fear and Loathing in Las Vegas" describes how the two main characters inhale diethyl ether and amyl nitrite.

In the comedy series "Newman and Baddiel in Pieces", Rob Newman's inhaling gas from a foghorn was a running joke in the series. One episode of the "Jeremy Kyle Show" featured a woman with a 20-year butane gas addiction. In the series "It's Always Sunny in Philadelphia", Charlie Kelly has an addiction to huffing glue. Additionally, season nine episode 8 shows Dennis, Mac and Dee getting a can of gasoline to use as a solvent, but instead end up taking turns huffing from the canister.

A 2008 episode of the reality show "Intervention" (season 5, episode 9) featured Allison, who was addicted to huffing computer duster for the short-lived, psychoactive effects. Allison has since achieved a small but significant cult following among bloggers and YouTube users. Several remixes of scenes from Allison's episode can be found online. Since 2009, Allison has worked with drug and alcohol treatment centers in Los Angeles County. In the third episode of season 5 of "American Dad!", titled "Home Adrone", Roger asks an airline stewardess to bring him industrial adhesive and a plastic bag. In the seventh episode of the fourteenth season of South Park, Towelie, an anthropomorphic towel, develops an addiction to inhaling computer duster. In the show "Squidbilles", the main character Early Cuyler is often seen inhaling gas or other substances.





</doc>
